
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 15 papers. April 16.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ</span> | <span id="title-articles-count">15 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-04-15.html">â¬…ï¸ <span id="prev-date">15.04</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-04-17.html">â¡ï¸ <span id="next-date">17.04</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-04.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 16', 'zh': '4æœˆ16æ—¥'};
        let feedDateNext = {'ru': '17.04', 'en': '04/17', 'zh': '4æœˆ17æ—¥'};
        let feedDatePrev = {'ru': '15.04', 'en': '04/15', 'zh': '4æœˆ15æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2504.10481', 'title': 'xVerify: Efficient Answer Verifier for Reasoning Model Evaluations', 'url': 'https://huggingface.co/papers/2504.10481', 'abstract': 'With the release of the o1 model by OpenAI, reasoning models adopting slow thinking strategies have gradually emerged. As the responses generated by such models often include complex reasoning, intermediate steps, and self-reflection, existing evaluation methods are often inadequate. They struggle to determine whether the LLM output is truly equivalent to the reference answer, and also have difficulty identifying and extracting the final answer from long, complex responses. To address this issue, we propose xVerify, an efficient answer verifier for reasoning model evaluations. xVerify demonstrates strong capability in equivalence judgment, enabling it to effectively determine whether the answers produced by reasoning models are equivalent to reference answers across various types of objective questions. To train and evaluate xVerify, we construct the VAR dataset by collecting question-answer pairs generated by multiple LLMs across various datasets, leveraging multiple reasoning models and challenging evaluation sets designed specifically for reasoning model assessment. A multi-round annotation process is employed to ensure label accuracy. Based on the VAR dataset, we train multiple xVerify models of different scales. In evaluation experiments conducted on both the test set and generalization set, all xVerify models achieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest variant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o, while xVerify-3B-Ib surpasses GPT-4o in overall performance. These results validate the effectiveness and generalizability of xVerify.', 'score': 18, 'issue_id': 3258, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': '72678fc1ff453072', 'authors': ['Ding Chen', 'Qingchen Yu', 'Pengyuan Wang', 'Wentao Zhang', 'Bo Tang', 'Feiyu Xiong', 'Xinchi Li', 'Minchuan Yang', 'Zhiyu Li'], 'affiliations': ['Center for Data Science, Peking University', 'MemTensor (Shanghai) Technology Co., Ltd.', 'Research Institute of China Telecom, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.10481.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#dataset', '#reasoning', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'xVerify: Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ xVerify - ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. xVerify ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ xVerify Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VAR, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ¿Ğ°Ñ€Ñ‹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¾Ñ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ xVerify Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ F1-Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹ÑˆĞµ 95% Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'xVerify: Elevating Reasoning Model Evaluation with Precision', 'desc': 'This paper introduces xVerify, a novel answer verification tool designed to evaluate reasoning models that utilize slow thinking strategies. Traditional evaluation methods struggle with complex outputs from large language models (LLMs), particularly in assessing the equivalence of answers and extracting final responses. xVerify addresses these challenges by leveraging a specially constructed VAR dataset, which includes diverse question-answer pairs generated by various LLMs. The results show that xVerify models achieve high accuracy and F1 scores, demonstrating their effectiveness in evaluating reasoning models compared to existing methods.'}, 'zh': {'title': 'xVerifyï¼šæ¨ç†æ¨¡å‹è¯„ä¼°çš„æ–°æ ‡å‡†', 'desc': 'éšç€OpenAIå‘å¸ƒo1æ¨¡å‹ï¼Œé‡‡ç”¨æ…¢æ€ç»´ç­–ç•¥çš„æ¨ç†æ¨¡å‹é€æ¸å‡ºç°ã€‚è¿™äº›æ¨¡å‹ç”Ÿæˆçš„å“åº”é€šå¸¸åŒ…å«å¤æ‚çš„æ¨ç†ã€ä¸­é—´æ­¥éª¤å’Œè‡ªæˆ‘åæ€ï¼Œç°æœ‰çš„è¯„ä¼°æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆåˆ¤æ–­LLMè¾“å‡ºæ˜¯å¦çœŸæ­£ç­‰åŒäºå‚è€ƒç­”æ¡ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†xVerifyï¼Œä¸€ä¸ªé«˜æ•ˆçš„ç­”æ¡ˆéªŒè¯å™¨ï¼Œç”¨äºæ¨ç†æ¨¡å‹çš„è¯„ä¼°ã€‚xVerifyåœ¨ç­‰ä»·åˆ¤æ–­æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ¤æ–­æ¨ç†æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦ä¸å‚è€ƒç­”æ¡ˆç­‰ä»·ï¼Œå¹¶åœ¨å¤šä¸ªå®¢è§‚é—®é¢˜ç±»å‹ä¸­è¡¨ç°ä¼˜å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10337', 'title': 'Heimdall: test-time scaling on the generative verification', 'url': 'https://huggingface.co/papers/2504.10337', 'abstract': 'An AI system can create and maintain knowledge only to the extent that it can verify that knowledge itself. Recent work on long Chain-of-Thought reasoning has demonstrated great potential of LLMs on solving competitive problems, but their verification ability remains to be weak and not sufficiently investigated. In this paper, we propose Heimdall, the long CoT verification LLM that can accurately judge the correctness of solutions. With pure reinforcement learning, we boost the verification accuracy from 62.5% to 94.5% on competitive math problems. By scaling with repeated sampling, the accuracy further increases to 97.5%. Through human evaluation, Heimdall demonstrates impressive generalization capabilities, successfully detecting most issues in challenging math proofs, the type of which is not included during training. Furthermore, we propose Pessimistic Verification to extend the functionality of Heimdall to scaling up the problem solving. It calls Heimdall to judge the solutions from a solver model and based on the pessimistic principle, selects the most likely correct solution with the least uncertainty. Taking DeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification improves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute budget and to 83.3% with more compute budget. With the stronger solver Gemini 2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge discovery system, a ternary system where one poses questions, another provides solutions, and the third verifies the solutions. Using the data synthesis work NuminaMath for the first two components, Heimdall effectively identifies problematic records within the dataset and reveals that nearly half of the data is flawed, which interestingly aligns with the recent ablation studies from NuminaMath.', 'score': 17, 'issue_id': 3258, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': '6da5db970a101d21', 'authors': ['Wenlei Shi', 'Xing Jin'], 'affiliations': ['bytedance.com'], 'pdf_title_img': 'assets/pdf/title_img/2504.10337.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#long_context', '#optimization', '#training', '#rl', '#math'], 'emoji': 'ğŸ”', 'ru': {'title': 'Heimdall: Ğ˜Ğ˜-Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Heimdall - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought). Ğ¡ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ±Ñ‹Ğ»Ğ° Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ° Ñ 62.5% Ğ´Ğ¾ 94.5% Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑÑĞ¸Ğ¼Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. Heimdall Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ´Ğ°Ğ¶Ğµ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°Ñ….'}, 'en': {'title': 'Heimdall: Elevating AI Verification with Chain-of-Thought Reasoning', 'desc': 'This paper introduces Heimdall, a long Chain-of-Thought (CoT) verification model designed to enhance the accuracy of solution verification in AI systems. By employing pure reinforcement learning, Heimdall significantly improves verification accuracy on competitive math problems from 62.5% to 94.5%, and with repeated sampling, it reaches 97.5%. The paper also presents Pessimistic Verification, which optimizes solution selection by minimizing uncertainty, leading to improved accuracy in problem-solving tasks. Additionally, Heimdall is part of an automatic knowledge discovery system that identifies flaws in datasets, revealing that a substantial portion of the data is incorrect, which is consistent with findings from previous studies.'}, 'zh': {'title': 'æå‡AIçŸ¥è¯†éªŒè¯èƒ½åŠ›çš„Heimdallæ¨¡å‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºHeimdallçš„é•¿é“¾æ€ç»´éªŒè¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæ—¨åœ¨æé«˜è§£å†³ç«äº‰æ€§æ•°å­¦é—®é¢˜çš„è§£ç­”å‡†ç¡®æ€§ã€‚é€šè¿‡çº¯å¼ºåŒ–å­¦ä¹ ï¼ŒHeimdallçš„éªŒè¯å‡†ç¡®ç‡ä»62.5%æå‡è‡³94.5%ï¼Œå¹¶é€šè¿‡é‡å¤é‡‡æ ·è¿›ä¸€æ­¥æé«˜è‡³97.5%ã€‚æ­¤å¤–ï¼ŒHeimdallè¿˜å±•ç¤ºäº†å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ£€æµ‹å‡ºè®­ç»ƒä¸­æœªåŒ…å«çš„å¤æ‚æ•°å­¦è¯æ˜ä¸­çš„å¤§å¤šæ•°é—®é¢˜ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†æ‚²è§‚éªŒè¯æ–¹æ³•ï¼Œä»¥æ‰©å±•Heimdallçš„åŠŸèƒ½ï¼Œæ˜¾è‘—æé«˜äº†è§£å†³æ–¹æ¡ˆçš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10766', 'title': 'How Instruction and Reasoning Data shape Post-Training: Data Quality\n  through the Lens of Layer-wise Gradients', 'url': 'https://huggingface.co/papers/2504.10766', 'abstract': "As the post-training of large language models (LLMs) advances from instruction-following to complex reasoning tasks, understanding how different data affect finetuning dynamics remains largely unexplored. In this paper, we present a spectral analysis of layer-wise gradients induced by low/high-quality instruction and reasoning data for LLM post-training. Our analysis reveals that widely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and Reward, can be explained and unified by spectral properties computed from gradients' singular value decomposition (SVD). Specifically, higher-quality data are usually associated with lower nuclear norms and higher effective ranks. Notably, effective rank exhibits better robustness and resolution than nuclear norm in capturing subtle quality differences. For example, reasoning data achieves substantially higher effective ranks than instruction data, implying richer gradient structures on more complex tasks. Our experiments also highlight that models within the same family share similar gradient patterns regardless of their sizes, whereas different model families diverge significantly. Providing a unified view on the effects of data quality across instruction and reasoning data, this work illuminates the interplay between data quality and training stability, shedding novel insights into developing better data exploration strategies for post-training.", 'score': 14, 'issue_id': 3258, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': 'b95bec819bad5307', 'authors': ['Ming Li', 'Yanhong Li', 'Ziyue Li', 'Tianyi Zhou'], 'affiliations': ['University of Chicago', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2504.10766.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#data', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞµĞºÑ€ĞµÑ‚Ñ‹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº IFD, InsTag, Difficulty Ğ¸ Reward, Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ñ‚ÑŒ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ², Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· ÑĞ¸Ğ½Ğ³ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ ÑĞ²ÑĞ·Ğ°Ğ½Ñ‹ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼Ğ¸ ÑĞ´ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ¼Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ½Ğ³Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Ğ¸Ğ¼ĞµÑÑ‚ ÑÑ…Ğ¾Ğ¶Ğ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ÑÑ.'}, 'en': {'title': 'Unlocking the Secrets of Data Quality in LLM Fine-Tuning', 'desc': 'This paper investigates how the quality of data influences the fine-tuning of large language models (LLMs) during post-training, particularly for complex reasoning tasks. It employs spectral analysis of layer-wise gradients to understand the effects of low and high-quality instruction and reasoning data. The study finds that traditional metrics for data evaluation can be unified through the spectral properties derived from the singular value decomposition (SVD) of gradients. Notably, it shows that higher-quality data leads to lower nuclear norms and higher effective ranks, with effective rank being a more reliable measure for capturing quality differences, especially in reasoning tasks.'}, 'zh': {'title': 'æ•°æ®è´¨é‡ä¸è®­ç»ƒç¨³å®šæ€§çš„ç»Ÿä¸€è§†è§’', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åè®­ç»ƒé˜¶æ®µä¸­ï¼Œä¸åŒæ•°æ®å¯¹å¾®è°ƒåŠ¨æ€çš„å½±å“ã€‚æˆ‘ä»¬é€šè¿‡å¯¹ä½è´¨é‡å’Œé«˜è´¨é‡æŒ‡ä»¤åŠæ¨ç†æ•°æ®çš„å±‚çº§æ¢¯åº¦è¿›è¡Œè°±åˆ†æï¼Œå‘ç°å¸¸ç”¨çš„æ•°æ®è¯„ä¼°æŒ‡æ ‡å¯ä»¥é€šè¿‡æ¢¯åº¦çš„å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰è°±ç‰¹æ€§æ¥è§£é‡Šå’Œç»Ÿä¸€ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé«˜è´¨é‡æ•°æ®é€šå¸¸ä¸è¾ƒä½çš„æ ¸èŒƒæ•°å’Œè¾ƒé«˜çš„æœ‰æ•ˆç§©ç›¸å…³ï¼Œä¸”æœ‰æ•ˆç§©åœ¨æ•æ‰ç»†å¾®è´¨é‡å·®å¼‚æ–¹é¢è¡¨ç°å‡ºæ›´å¥½çš„é²æ£’æ€§å’Œåˆ†è¾¨ç‡ã€‚æˆ‘ä»¬çš„å®éªŒè¿˜è¡¨æ˜ï¼ŒåŒä¸€å®¶æ—çš„æ¨¡å‹åœ¨æ¢¯åº¦æ¨¡å¼ä¸Šç›¸ä¼¼ï¼Œè€Œä¸åŒæ¨¡å‹å®¶æ—ä¹‹é—´åˆ™å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10465', 'title': 'Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding', 'url': 'https://huggingface.co/papers/2504.10465', 'abstract': "Multimodal Large Language Models (MLLMs) achieve remarkable performance for fine-grained pixel-level understanding tasks. However, all the works rely heavily on extra components, such as vision encoder (CLIP), segmentation experts, leading to high system complexity and limiting model scaling. In this work, our goal is to explore a highly simplified MLLM without introducing extra components. Our work is motivated by the recent works on Single trAnsformer as a unified vIsion-Language Model (SAIL) design, where these works jointly learn vision tokens and text tokens in transformers. We present Pixel-SAIL, a single transformer for pixel-wise MLLM tasks. In particular, we present three technical improvements on the plain baseline. First, we design a learnable upsampling module to refine visual token features. Secondly, we propose a novel visual prompt injection strategy to enable the single transformer to understand visual prompt inputs and benefit from the early fusion of visual prompt embeddings and vision tokens. Thirdly, we introduce a vision expert distillation strategy to efficiently enhance the single transformer's fine-grained feature extraction capability. In addition, we have collected a comprehensive pixel understanding benchmark (PerBench), using a manual check. It includes three tasks: detailed object description, visual prompt-based question answering, and visual-text referring segmentation. Extensive experiments on four referring segmentation benchmarks, one visual prompt benchmark, and our PerBench show that our Pixel-SAIL achieves comparable or even better results with a much simpler pipeline. Code and model will be released at https://github.com/magic-research/Sa2VA.", 'score': 14, 'issue_id': 3260, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': '6c90d31f3f941694', 'authors': ['Tao Zhang', 'Xiangtai Li', 'Zilong Huang', 'Yanwei Li', 'Weixian Lei', 'Xueqing Deng', 'Shihao Chen', 'Shunping Ji', 'Jiashi Feng'], 'affiliations': ['Bytedance Seed', 'WHU'], 'pdf_title_img': 'assets/pdf/title_img/2504.10465.jpg', 'data': {'categories': ['#survey', '#optimization', '#benchmark', '#multimodal', '#training', '#architecture', '#games'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Pixel-SAIL - ĞµĞ´Ğ¸Ğ½ÑƒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ: Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‰ĞµĞ¹ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ğ¸Ğ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… PerBench Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Simplifying Multimodal Learning with Pixel-SAIL', 'desc': "This paper introduces Pixel-SAIL, a simplified Multimodal Large Language Model (MLLM) designed for pixel-level understanding tasks without relying on additional components like vision encoders. The authors propose three key innovations: a learnable upsampling module for refining visual features, a visual prompt injection strategy for better integration of visual and text inputs, and a vision expert distillation method to enhance feature extraction. By focusing on a single transformer architecture, Pixel-SAIL aims to reduce system complexity while maintaining high performance. The paper also presents a new benchmark, PerBench, to evaluate the model's effectiveness across various pixel understanding tasks."}, 'zh': {'title': 'ç®€åŒ–çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼šPixel-SAIL', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç»†ç²’åº¦åƒç´ çº§ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å¤§å¤šæ•°å·¥ä½œä¾èµ–äºé¢å¤–çš„ç»„ä»¶ï¼Œå¦‚è§†è§‰ç¼–ç å™¨å’Œåˆ†å‰²ä¸“å®¶ï¼Œå¯¼è‡´ç³»ç»Ÿå¤æ‚æ€§é«˜ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ‰©å±•æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€åŒ–çš„MLLMï¼Œåä¸ºPixel-SAILï¼Œæ—¨åœ¨ä¸å¼•å…¥é¢å¤–ç»„ä»¶çš„æƒ…å†µä¸‹è¿›è¡Œåƒç´ çº§ä»»åŠ¡ã€‚æˆ‘ä»¬é€šè¿‡è®¾è®¡å¯å­¦ä¹ çš„ä¸Šé‡‡æ ·æ¨¡å—ã€è§†è§‰æç¤ºæ³¨å…¥ç­–ç•¥å’Œè§†è§‰ä¸“å®¶è’¸é¦ç­–ç•¥ï¼Œæå‡äº†å•ä¸€å˜æ¢å™¨çš„ç‰¹å¾æå–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPixel-SAILåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸”å…·æœ‰æ›´ç®€å•çš„å¤„ç†æµç¨‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11442', 'title': 'TextArena', 'url': 'https://huggingface.co/papers/2504.11442', 'abstract': 'TextArena is an open-source collection of competitive text-based games for training and evaluation of agentic behavior in Large Language Models (LLMs). It spans 57+ unique environments (including single-player, two-player, and multi-player setups) and allows for easy evaluation of model capabilities via an online-play system (against humans and other submitted models) with real-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social skills such as negotiation, theory of mind, and deception, creating a gap that TextArena addresses. Designed with research, community and extensibility in mind, TextArena emphasizes ease of adding new games, adapting the framework, testing models, playing against the models, and training models. Detailed documentation of environments, games, leaderboard, and examples are available on https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.', 'score': 13, 'issue_id': 3259, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': '7c9ae6533757828a', 'authors': ['Leon Guertler', 'Bobby Cheng', 'Simon Yu', 'Bo Liu', 'Leshem Choshen', 'Cheston Tan'], 'affiliations': ['Centre for Frontier AI Research (CFAR), A*STAR Institute of High Performance Computing, A*STAR', 'MIT, MIT-IBM Watson AI Lab', 'National University of Singapore', 'Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2504.11442.jpg', 'data': {'categories': ['#agents', '#games', '#benchmark', '#open_source'], 'emoji': 'ğŸ®', 'ru': {'title': 'TextArena: ĞÑ€ĞµĞ½Ğ° Ğ´Ğ»Ñ Ğ¾Ñ‚Ñ‚Ğ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'TextArena - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ³Ñ€ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 57 ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ€ĞµĞ´, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¸Ğ³Ñ€Ñ‹ Ñ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ¼ TrueSkill Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. TextArena Ğ²Ğ¾ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ» Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ², Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ñ€Ğ°Ğ·ÑƒĞ¼Ğ° Ğ¸ Ğ¾Ğ±Ğ¼Ğ°Ğ½. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ñ†ĞµĞ»ĞµĞ¹, Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡Ğ°Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ³Ñ€ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Train LLMs in Competitive Text Games!', 'desc': 'TextArena is a platform designed for training and evaluating Large Language Models (LLMs) through competitive text-based games. It features over 57 unique environments that support various gameplay modes, allowing models to interact with both human players and other models. This framework addresses the lack of benchmarks for assessing social skills like negotiation and deception, which are crucial for agentic behavior. TextArena is built for research and community engagement, making it easy to add new games and adapt the system for testing and training purposes.'}, 'zh': {'title': 'TextArenaï¼šæå‡è¯­è¨€æ¨¡å‹çš„ç¤¾äº¤æŠ€èƒ½è®­ç»ƒå¹³å°', 'desc': 'TextArenaæ˜¯ä¸€ä¸ªå¼€æºçš„ç«äº‰æ€§æ–‡æœ¬æ¸¸æˆé›†åˆï¼Œæ—¨åœ¨è®­ç»ƒå’Œè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä»£ç†è¡Œä¸ºã€‚å®ƒåŒ…å«57ä¸ªä»¥ä¸Šç‹¬ç‰¹çš„ç¯å¢ƒï¼Œæ”¯æŒå•äººã€åŒäººå’Œå¤šäººè®¾ç½®ï¼Œå¹¶é€šè¿‡åœ¨çº¿æ¸¸æˆç³»ç»Ÿè½»æ¾è¯„ä¼°æ¨¡å‹èƒ½åŠ›ã€‚ä¼ ç»ŸåŸºå‡†æµ‹è¯•é€šå¸¸æ— æ³•è¯„ä¼°åŠ¨æ€ç¤¾äº¤æŠ€èƒ½ï¼Œå¦‚è°ˆåˆ¤ã€å¿ƒæ™ºç†è®ºå’Œæ¬ºéª—ï¼Œè€ŒTextArenaæ­£å¥½å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚è¯¥å¹³å°å¼ºè°ƒæ˜“äºæ·»åŠ æ–°æ¸¸æˆã€é€‚åº”æ¡†æ¶ã€æµ‹è¯•æ¨¡å‹å’Œè®­ç»ƒæ¨¡å‹ï¼Œé€‚åˆç ”ç©¶å’Œç¤¾åŒºä½¿ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11346', 'title': 'Seedream 3.0 Technical Report', 'url': 'https://huggingface.co/papers/2504.11346', 'abstract': 'We present Seedream 3.0, a high-performance Chinese-English bilingual image generation foundation model. We develop several technical improvements to address existing challenges in Seedream 2.0, including alignment with complicated prompts, fine-grained typography generation, suboptimal visual aesthetics and fidelity, and limited image resolutions. Specifically, the advancements of Seedream 3.0 stem from improvements across the entire pipeline, from data construction to model deployment. At the data stratum, we double the dataset using a defect-aware training paradigm and a dual-axis collaborative data-sampling framework. Furthermore, we adopt several effective techniques such as mixed-resolution training, cross-modality RoPE, representation alignment loss, and resolution-aware timestep sampling in the pre-training phase. During the post-training stage, we utilize diversified aesthetic captions in SFT, and a VLM-based reward model with scaling, thereby achieving outputs that well align with human preferences. Furthermore, Seedream 3.0 pioneers a novel acceleration paradigm. By employing consistent noise expectation and importance-aware timestep sampling, we achieve a 4 to 8 times speedup while maintaining image quality. Seedream 3.0 demonstrates significant improvements over Seedream 2.0: it enhances overall capabilities, in particular for text-rendering in complicated Chinese characters which is important to professional typography generation. In addition, it provides native high-resolution output (up to 2K), allowing it to generate images with high visual quality.', 'score': 11, 'issue_id': 3259, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': '3a4b797a3a9516d2', 'authors': ['Yu Gao', 'Lixue Gong', 'Qiushan Guo', 'Xiaoxia Hou', 'Zhichao Lai', 'Fanshi Li', 'Liang Li', 'Xiaochen Lian', 'Chao Liao', 'Liyang Liu', 'Wei Liu', 'Yichun Shi', 'Shiqi Sun', 'Yu Tian', 'Zhi Tian', 'Peng Wang', 'Rui Wang', 'Xuanda Wang', 'Xun Wang', 'Ye Wang', 'Guofeng Wu', 'Jie Wu', 'Xin Xia', 'Xuefeng Xiao', 'Zhonghua Zhai', 'Xinyu Zhang', 'Qi Zhang', 'Yuwei Zhang', 'Shijia Zhao', 'Jianchao Yang', 'Weilin Huang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.11346.jpg', 'data': {'categories': ['#alignment', '#data', '#optimization', '#dataset', '#multimodal', '#architecture', '#training'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Seedream 3.0 Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ°Ğ½ĞºÑƒ', 'desc': 'Seedream 3.0 - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ VLM. Seedream 3.0 Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ² 4-8 Ñ€Ğ°Ğ· Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Revolutionizing Bilingual Image Generation with Seedream 3.0', 'desc': 'Seedream 3.0 is an advanced bilingual image generation model designed for Chinese and English. It improves upon its predecessor by enhancing prompt alignment, typography generation, and overall image quality through a series of technical upgrades. Key innovations include a larger dataset, mixed-resolution training, and a novel acceleration method that speeds up processing while preserving image fidelity. The model excels in generating high-resolution images and accurately rendering complex Chinese characters, making it valuable for professional typography applications.'}, 'zh': {'title': 'Seedream 3.0ï¼šé«˜æ•ˆçš„ä¸­è‹±æ–‡å›¾åƒç”Ÿæˆæ–°çºªå…ƒ', 'desc': 'Seedream 3.0 æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„ä¸­è‹±æ–‡åŒè¯­å›¾åƒç”ŸæˆåŸºç¡€æ¨¡å‹ã€‚å®ƒé€šè¿‡å¤šé¡¹æŠ€æœ¯æ”¹è¿›ï¼Œè§£å†³äº† Seedream 2.0 ä¸­å­˜åœ¨çš„æŒ‘æˆ˜ï¼Œå¦‚å¤æ‚æç¤ºçš„å¯¹é½ã€ç»†è‡´çš„æ’ç‰ˆç”Ÿæˆå’Œå›¾åƒåˆ†è¾¨ç‡é™åˆ¶ã€‚è¯¥æ¨¡å‹åœ¨æ•°æ®æ„å»ºå’Œæ¨¡å‹éƒ¨ç½²çš„æ•´ä¸ªæµç¨‹ä¸­è¿›è¡Œäº†æ”¹è¿›ï¼Œé‡‡ç”¨äº†ç¼ºé™·æ„ŸçŸ¥è®­ç»ƒå’ŒåŒè½´åä½œæ•°æ®é‡‡æ ·ç­‰æ–¹æ³•ã€‚Seedream 3.0 è¿˜å¼•å…¥äº†æ–°çš„åŠ é€ŸèŒƒå¼ï¼Œå®ç°äº†å›¾åƒè´¨é‡ä¸ç”Ÿæˆé€Ÿåº¦çš„æ˜¾è‘—æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚æ±‰å­—çš„æ–‡æœ¬æ¸²æŸ“æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10462', 'title': 'The Scalability of Simplicity: Empirical Analysis of Vision-Language\n  Learning with a Single Transformer', 'url': 'https://huggingface.co/papers/2504.10462', 'abstract': "This paper introduces SAIL, a single transformer unified multimodal large language model (MLLM) that integrates raw pixel encoding and language decoding within a singular architecture. Unlike existing modular MLLMs, which rely on a pre-trained vision transformer (ViT), SAIL eliminates the need for a separate vision encoder, presenting a more minimalist architecture design. Instead of introducing novel architectural components, SAIL adapts mix-attention mechanisms and multimodal positional encodings to better align with the distinct characteristics of visual and textual modalities. We systematically compare SAIL's properties-including scalability, cross-modal information flow patterns, and visual representation capabilities-with those of modular MLLMs. By scaling both training data and model size, SAIL achieves performance comparable to modular MLLMs. Notably, the removal of pretrained ViT components enhances SAIL's scalability and results in significantly different cross-modal information flow patterns. Moreover, SAIL demonstrates strong visual representation capabilities, achieving results on par with ViT-22B in vision tasks such as semantic segmentation. Code and models are available at https://github.com/bytedance/SAIL.", 'score': 8, 'issue_id': 3260, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': '1f70a22447fd1fc5', 'authors': ['Weixian Lei', 'Jiacong Wang', 'Haochen Wang', 'Xiangtai Li', 'Jun Hao Liew', 'Jiashi Feng', 'Zilong Huang'], 'affiliations': ['Bytedance Seed'], 'pdf_title_img': 'assets/pdf/title_img/2504.10462.jpg', 'data': {'categories': ['#multimodal', '#agi', '#architecture', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'SAIL: ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'SAIL - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, SAIL Ğ½Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ vision transformer, Ğ° Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¸ĞºÑĞµĞ»Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. SAIL Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'SAIL: A Unified Approach to Multimodal Learning', 'desc': "This paper presents SAIL, a unified multimodal large language model that combines image and text processing in one architecture without needing a separate vision encoder. SAIL uses mix-attention mechanisms and multimodal positional encodings to effectively handle both visual and textual data. The study shows that SAIL can scale well with increased training data and model size, achieving performance similar to existing modular models. Additionally, SAIL's design leads to unique patterns in how information flows between modalities, while also excelling in visual tasks like semantic segmentation."}, 'zh': {'title': 'SAILï¼šç®€çº¦æ¶æ„ä¸‹çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹', 'desc': 'æœ¬æ–‡ä»‹ç»äº†SAILï¼Œè¿™æ˜¯ä¸€ç§å•ä¸€å˜æ¢å™¨ç»Ÿä¸€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå®ƒåœ¨ä¸€ä¸ªæ¶æ„ä¸­æ•´åˆäº†åŸå§‹åƒç´ ç¼–ç å’Œè¯­è¨€è§£ç ã€‚ä¸ç°æœ‰çš„æ¨¡å—åŒ–MLLMä¸åŒï¼ŒSAILä¸éœ€è¦å•ç‹¬çš„è§†è§‰ç¼–ç å™¨ï¼Œå‘ˆç°å‡ºæ›´ç®€çº¦çš„æ¶æ„è®¾è®¡ã€‚SAILé‡‡ç”¨æ··åˆæ³¨æ„åŠ›æœºåˆ¶å’Œå¤šæ¨¡æ€ä½ç½®ç¼–ç ï¼Œä»¥æ›´å¥½åœ°é€‚åº”è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„ç‹¬ç‰¹ç‰¹å¾ã€‚é€šè¿‡æ‰©å¤§è®­ç»ƒæ•°æ®å’Œæ¨¡å‹è§„æ¨¡ï¼ŒSAILåœ¨æ€§èƒ½ä¸Šä¸æ¨¡å—åŒ–MLLMç›¸å½“ï¼ŒåŒæ—¶åœ¨è§†è§‰è¡¨ç¤ºèƒ½åŠ›ä¸Šä¹Ÿè¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10559', 'title': 'Efficient Process Reward Model Training via Active Learning', 'url': 'https://huggingface.co/papers/2504.10559', 'abstract': "Process Reward Models (PRMs) provide step-level supervision to large language models (LLMs), but scaling up training data annotation remains challenging for both humans and LLMs. To address this limitation, we propose an active learning approach, ActPRM, which proactively selects the most uncertain samples for training, substantially reducing labeling costs. During training, we use the PRM to estimate uncertainty after the forward pass, retaining only highly uncertain data. A capable yet costly reasoning model then labels this data. Then we compute the loss with respect to the labels and update the PRM's weights. We compare ActPRM vs. vanilla fine-tuning, on a pool-based active learning setting, demonstrating that ActPRM reduces 50% annotation, but achieving the comparable or even better performance. Beyond annotation efficiency, we further advance the actively trained PRM by filtering over 1M+ math reasoning trajectories with ActPRM, retaining 60% of the data. A subsequent training on this selected dataset yields a new state-of-the-art (SOTA) PRM on ProcessBench (75.0%) and PRMBench (65.5%) compared with same sized models.", 'score': 6, 'issue_id': 3259, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': 'f718e5da41cde633', 'authors': ['Keyu Duan', 'Zichen Liu', 'Xin Mao', 'Tianyu Pang', 'Changyu Chen', 'Qiguang Chen', 'Michael Qizhe Shieh', 'Longxu Dou'], 'affiliations': ['National University of Singapore', 'Sea AI Lab', 'Singapore Management University'], 'pdf_title_img': 'assets/pdf/title_img/2504.10559.jpg', 'data': {'categories': ['#data', '#reasoning', '#optimization', '#benchmark', '#math', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ActPRM - Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² (PRM). ActPRM Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ PRM Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ActPRM ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° 50% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹.'}, 'en': {'title': 'Efficient Learning with Uncertainty: ActPRM for Enhanced Model Training', 'desc': 'This paper introduces ActPRM, an active learning method designed to enhance Process Reward Models (PRMs) for training large language models (LLMs). By focusing on the most uncertain samples, ActPRM significantly cuts down the costs associated with data labeling while maintaining or improving model performance. The approach involves using the PRM to estimate uncertainty and selectively retaining only the most ambiguous data for labeling by a more complex reasoning model. The results show that ActPRM not only reduces annotation requirements by 50% but also achieves state-of-the-art performance on benchmark tasks.'}, 'zh': {'title': 'ä¸»åŠ¨å­¦ä¹ æå‡æ¨¡å‹è®­ç»ƒæ•ˆç‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸»åŠ¨å­¦ä¹ æ–¹æ³•ActPRMï¼Œç”¨äºæé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è®­ç»ƒæ•ˆç‡ã€‚é€šè¿‡é€‰æ‹©æœ€ä¸ç¡®å®šçš„æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼ŒActPRMæ˜¾è‘—é™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰æ¥ä¼°è®¡ä¸ç¡®å®šæ€§ï¼Œä»…ä¿ç•™é«˜åº¦ä¸ç¡®å®šçš„æ•°æ®è¿›è¡Œæ ‡æ³¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒActPRMåœ¨å‡å°‘50%æ ‡æ³¨çš„åŒæ—¶ï¼Œæ€§èƒ½ä¸ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•ç›¸å½“ç”šè‡³æ›´å¥½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11427', 'title': 'NormalCrafter: Learning Temporally Consistent Normals from Video\n  Diffusion Priors', 'url': 'https://huggingface.co/papers/2504.11427', 'abstract': 'Surface normal estimation serves as a cornerstone for a spectrum of computer vision applications. While numerous efforts have been devoted to static image scenarios, ensuring temporal coherence in video-based normal estimation remains a formidable challenge. Instead of merely augmenting existing methods with temporal components, we present NormalCrafter to leverage the inherent temporal priors of video diffusion models. To secure high-fidelity normal estimation across sequences, we propose Semantic Feature Regularization (SFR), which aligns diffusion features with semantic cues, encouraging the model to concentrate on the intrinsic semantics of the scene. Moreover, we introduce a two-stage training protocol that leverages both latent and pixel space learning to preserve spatial accuracy while maintaining long temporal context. Extensive evaluations demonstrate the efficacy of our method, showcasing a superior performance in generating temporally consistent normal sequences with intricate details from diverse videos.', 'score': 3, 'issue_id': 3259, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': 'f1c298d14d78b468', 'authors': ['Yanrui Bin', 'Wenbo Hu', 'Haoyuan Wang', 'Xinya Chen', 'Bing Wang'], 'affiliations': ['ARC Lab, Tencent PCG', 'City University of Hong Kong', 'Huazhong University of Science and Technology', 'Spatial Intelligence Group, The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2504.11427.jpg', 'data': {'categories': ['#cv', '#long_context', '#diffusion', '#video', '#training'], 'emoji': 'ğŸ¥', 'ru': {'title': 'NormalCrafter: Ğ’Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ NormalCrafter - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² (SFR) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¸ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹.'}, 'en': {'title': 'NormalCrafter: Enhancing Video Normal Estimation with Semantic Insights', 'desc': "This paper introduces NormalCrafter, a novel approach for estimating surface normals in video sequences. It addresses the challenge of maintaining temporal coherence, which is often overlooked in traditional static image methods. The authors propose Semantic Feature Regularization (SFR) to enhance the model's focus on the scene's semantics, improving the quality of normal estimation. Additionally, a two-stage training protocol is implemented to balance spatial accuracy and long-term temporal context, resulting in high-fidelity normal sequences across various videos."}, 'zh': {'title': 'è§†é¢‘æ³•çº¿ä¼°è®¡çš„æ–°æ–¹æ³•ï¼šNormalCrafter', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•NormalCrafterï¼Œç”¨äºè§†é¢‘ä¸­çš„æ³•çº¿ä¼°è®¡ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ—¶é—´å…ˆéªŒï¼Œç¡®ä¿æ³•çº¿ä¼°è®¡åœ¨æ—¶é—´ä¸Šçš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†è¯­ä¹‰ç‰¹å¾æ­£åˆ™åŒ–ï¼ˆSFRï¼‰ï¼Œé€šè¿‡å¯¹é½æ‰©æ•£ç‰¹å¾å’Œè¯­ä¹‰çº¿ç´¢ï¼Œå¸®åŠ©æ¨¡å‹å…³æ³¨åœºæ™¯çš„å†…åœ¨è¯­ä¹‰ã€‚é€šè¿‡ä¸¤é˜¶æ®µçš„è®­ç»ƒåè®®ï¼Œæˆ‘ä»¬åœ¨ä¿æŒç©ºé—´ç²¾åº¦çš„åŒæ—¶ï¼Œå¢å¼ºäº†å¯¹é•¿æ—¶é—´ä¸Šä¸‹æ–‡çš„å­¦ä¹ ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨ç”Ÿæˆç»†èŠ‚ä¸°å¯Œä¸”æ—¶é—´ä¸€è‡´çš„æ³•çº¿åºåˆ—æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11343', 'title': 'A Minimalist Approach to LLM Reasoning: from Rejection Sampling to\n  Reinforce', 'url': 'https://huggingface.co/papers/2504.11343', 'abstract': "Reinforcement learning (RL) has become a prevailing approach for fine-tuning large language models (LLMs) on complex reasoning tasks. Among recent methods, GRPO stands out for its empirical success in training models such as DeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In this work, we revisit GRPO from a reinforce-like algorithm perspective and analyze its core components. Surprisingly, we find that a simple rejection sampling baseline, RAFT, which trains only on positively rewarded samples, yields competitive performance than GRPO and PPO. Our ablation studies reveal that GRPO's main advantage arises from discarding prompts with entirely incorrect responses, rather than from its reward normalization. Motivated by this insight, we propose Reinforce-Rej, a minimal extension of policy gradient that filters both entirely incorrect and entirely correct samples. Reinforce-Rej improves KL efficiency and stability, serving as a lightweight yet effective alternative to more complex RL algorithms. We advocate RAFT as a robust and interpretable baseline, and suggest that future advances should focus on more principled designs for incorporating negative samples, rather than relying on them indiscriminately. Our findings provide guidance for future work in reward-based LLM post-training.", 'score': 3, 'issue_id': 3260, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': '72714d765a5a497f', 'authors': ['Wei Xiong', 'Jiarui Yao', 'Yuhui Xu', 'Bo Pang', 'Lei Wang', 'Doyen Sahoo', 'Junnan Li', 'Nan Jiang', 'Tong Zhang', 'Caiming Xiong', 'Hanze Dong'], 'affiliations': ['Salesforce AI Research', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2504.11343.jpg', 'data': {'categories': ['#optimization', '#training', '#reasoning', '#rl', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² RAFT Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ğ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº GRPO. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Reinforce-Rej, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµÑ‚ ĞºĞ°Ğº Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ RAFT ĞºĞ°Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°Ñ….'}, 'en': {'title': 'Simplifying Reinforcement Learning for Better Language Model Training', 'desc': "This paper explores the effectiveness of the GRPO method in reinforcement learning for fine-tuning large language models on reasoning tasks. The authors discover that a simpler method, RAFT, which only uses positively rewarded samples, performs comparably to GRPO and PPO. They find that GRPO's strength lies in its ability to discard prompts with completely incorrect responses, rather than its reward normalization technique. To enhance performance, they introduce Reinforce-Rej, which filters out both incorrect and correct samples, improving efficiency and stability in training."}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ çš„æ–°è§†è§’ï¼šæ‹’ç»é‡‡æ ·çš„åŠ›é‡', 'desc': 'å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¾®è°ƒä¸­å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚æœ¬æ–‡é‡æ–°å®¡è§†äº†GRPOç®—æ³•ï¼Œå‘ç°ä¸€ä¸ªç®€å•çš„æ‹’ç»é‡‡æ ·åŸºçº¿RAFTåœ¨è®­ç»ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œç”šè‡³ä¸GRPOå’ŒPPOç›¸å½“ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒGRPOçš„ä¸»è¦ä¼˜åŠ¿åœ¨äºä¸¢å¼ƒå®Œå…¨é”™è¯¯çš„æç¤ºï¼Œè€Œä¸æ˜¯å…¶å¥–åŠ±å½’ä¸€åŒ–ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†Reinforce-Rejï¼Œè¿™æ˜¯ä¸€ç§è¿‡æ»¤å®Œå…¨é”™è¯¯å’Œå®Œå…¨æ­£ç¡®æ ·æœ¬çš„ç­–ç•¥æ¢¯åº¦æ‰©å±•ï¼Œèƒ½å¤Ÿæé«˜KLæ•ˆç‡å’Œç¨³å®šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11447', 'title': 'Diffusion Distillation With Direct Preference Optimization For Efficient\n  3D LiDAR Scene Completion', 'url': 'https://huggingface.co/papers/2504.11447', 'abstract': "The application of diffusion models in 3D LiDAR scene completion is limited due to diffusion's slow sampling speed. Score distillation accelerates diffusion sampling but with performance degradation, while post-training with direct policy optimization (DPO) boosts performance using preference data. This paper proposes Distillation-DPO, a novel diffusion distillation framework for LiDAR scene completion with preference aligment. First, the student model generates paired completion scenes with different initial noises. Second, using LiDAR scene evaluation metrics as preference, we construct winning and losing sample pairs. Such construction is reasonable, since most LiDAR scene metrics are informative but non-differentiable to be optimized directly. Third, Distillation-DPO optimizes the student model by exploiting the difference in score functions between the teacher and student models on the paired completion scenes. Such procedure is repeated until convergence. Extensive experiments demonstrate that, compared to state-of-the-art LiDAR scene completion diffusion models, Distillation-DPO achieves higher-quality scene completion while accelerating the completion speed by more than 5-fold. Our method is the first to explore adopting preference learning in distillation to the best of our knowledge and provide insights into preference-aligned distillation. Our code is public available on https://github.com/happyw1nd/DistillationDPO.", 'score': 2, 'issue_id': 3258, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': 'cda8d39111df28d8', 'authors': ['An Zhaol', 'Shengyuan Zhang', 'Ling Yang', 'Zejian Li', 'Jiale Wu', 'Haoran Xu', 'AnYang Wei', 'Perry Pengyun GU Lingyun Sun'], 'affiliations': ['Peking University', 'Zhejiang Green Zhixing Technology co., ltd', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.11447.jpg', 'data': {'categories': ['#open_source', '#3d', '#rlhf', '#training', '#diffusion', '#optimization'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ½Ğ¾Ğµ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ÑÑ†ĞµĞ½ LiDAR Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Distillation-DPO', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Distillation-DPO Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½ LiDAR Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ (DPO), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ†ĞµĞ½ LiDAR Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. Distillation-DPO Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ñƒ Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑÑ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ¾Ğ¼ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 5 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Accelerating 3D LiDAR Scene Completion with Distillation-DPO', 'desc': 'This paper introduces Distillation-DPO, a new framework that enhances 3D LiDAR scene completion using diffusion models. It combines score distillation with direct policy optimization (DPO) to improve performance while speeding up the sampling process. The method generates paired scene completions with varying initial noises and uses LiDAR evaluation metrics to create winning and losing pairs for optimization. The results show that Distillation-DPO significantly outperforms existing models in both quality and speed, marking a novel approach in preference-aligned distillation for LiDAR applications.'}, 'zh': {'title': 'æå‡LiDARåœºæ™¯è¡¥å…¨é€Ÿåº¦ä¸è´¨é‡çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ‰©æ•£è’¸é¦æ¡†æ¶ï¼Œç§°ä¸ºDistillation-DPOï¼Œç”¨äº3D LiDARåœºæ™¯è¡¥å…¨ã€‚è¯¥æ–¹æ³•é€šè¿‡åå¥½å¯¹é½æ¥ä¼˜åŒ–å­¦ç”Ÿæ¨¡å‹ï¼Œé¦–å…ˆç”Ÿæˆä¸åŒåˆå§‹å™ªå£°çš„é…å¯¹è¡¥å…¨åœºæ™¯ã€‚ç„¶åï¼Œåˆ©ç”¨LiDARåœºæ™¯è¯„ä¼°æŒ‡æ ‡æ„å»ºèƒœè´Ÿæ ·æœ¬å¯¹ï¼Œä»¥æ­¤æ¥ä¼˜åŒ–æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒDistillation-DPOåœ¨åœºæ™¯è¡¥å…¨è´¨é‡å’Œé€Ÿåº¦ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œè¡¥å…¨é€Ÿåº¦æé«˜äº†5å€ä»¥ä¸Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.10188', 'title': 'Efficient Generative Model Training via Embedded Representation Warmup', 'url': 'https://huggingface.co/papers/2504.10188', 'abstract': "Diffusion models excel at generating high-dimensional data but fall short in training efficiency and representation quality compared to self-supervised methods. We identify a key bottleneck: the underutilization of high-quality, semantically rich representations during training notably slows down convergence. Our systematic analysis reveals a critical representation processing region -- primarily in the early layers -- where semantic and structural pattern learning takes place before generation can occur. To address this, we propose Embedded Representation Warmup (ERW), a plug-and-play framework where in the first stage we get the ERW module serves as a warmup that initializes the early layers of the diffusion model with high-quality, pretrained representations. This warmup minimizes the burden of learning representations from scratch, thereby accelerating convergence and boosting performance. Our theoretical analysis demonstrates that ERW's efficacy depends on its precise integration into specific neural network layers -- termed the representation processing region -- where the model primarily processes and transforms feature representations for later generation. We further establish that ERW not only accelerates training convergence but also enhances representation quality: empirically, our method achieves a 40times acceleration in training speed compared to REPA, the current state-of-the-art methods. Code is available at https://github.com/LINs-lab/ERW.", 'score': 2, 'issue_id': 3261, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'}, 'hash': '280d2a5386c25fa2', 'authors': ['Deyuan Liu', 'Peng Sun', 'Xufeng Li', 'Tao Lin'], 'affiliations': ['Nanjing University', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.10188.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#architecture', '#training'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Embedded Representation Warmup (ERW). ERW Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸, Ğ³Ğ´Ğµ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ERW ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² 40 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Accelerating Diffusion Models with Embedded Representation Warmup', 'desc': 'This paper addresses the limitations of diffusion models in generating high-dimensional data efficiently. It identifies that the slow training process is due to the underutilization of high-quality representations in the early layers of the model. The authors propose a method called Embedded Representation Warmup (ERW), which initializes these layers with pretrained representations to enhance learning speed and quality. Their results show that ERW significantly accelerates training convergence and improves representation quality, achieving a 40 times faster training speed compared to existing methods.'}, 'zh': {'title': 'åŠ é€Ÿæ‰©æ•£æ¨¡å‹è®­ç»ƒçš„åµŒå…¥è¡¨ç¤ºé¢„çƒ­', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜ç»´æ•°æ®æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è®­ç»ƒæ•ˆç‡å’Œè¡¨ç¤ºè´¨é‡ä¸Šä¸å¦‚è‡ªç›‘ç£æ–¹æ³•ã€‚æˆ‘ä»¬å‘ç°ä¸€ä¸ªå…³é”®ç“¶é¢ˆï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœªå……åˆ†åˆ©ç”¨é«˜è´¨é‡ã€è¯­ä¹‰ä¸°å¯Œçš„è¡¨ç¤ºï¼Œæ˜¾è‘—å‡ç¼“äº†æ”¶æ•›é€Ÿåº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åµŒå…¥è¡¨ç¤ºé¢„çƒ­ï¼ˆERWï¼‰æ¡†æ¶ï¼Œé€šè¿‡åœ¨åˆå§‹é˜¶æ®µä½¿ç”¨é¢„è®­ç»ƒçš„é«˜è´¨é‡è¡¨ç¤ºæ¥åˆå§‹åŒ–æ‰©æ•£æ¨¡å‹çš„æ—©æœŸå±‚ï¼Œä»è€ŒåŠ é€Ÿæ”¶æ•›å¹¶æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç†è®ºåˆ†æè¡¨æ˜ï¼ŒERWçš„æœ‰æ•ˆæ€§ä¾èµ–äºå…¶åœ¨ç‰¹å®šç¥ç»ç½‘ç»œå±‚çš„ç²¾ç¡®é›†æˆï¼Œè¿™äº›å±‚ä¸»è¦å¤„ç†å’Œè½¬æ¢ç‰¹å¾è¡¨ç¤ºä»¥ä¾¿åç»­ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.06949', 'title': 'Adaptive Computation Pruning for the Forgetting Transformer', 'url': 'https://huggingface.co/papers/2504.06949', 'abstract': 'The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each timestep to rely primarily on the local context. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, a method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. This is achieved using a dynamically set pruning threshold that ensures that the pruned attention weights remain negligible. We apply ACP to language model pretraining with FoX and show it consistently reduces the number of FLOPs in softmax attention by around 70% across different model sizes and context lengths, resulting in a roughly 10% to 35% improvement in training throughput. Furthermore, longer context lengths yield greater computational savings. All these speed improvements are achieved without any performance degradation. We also perform several analyses to provide deeper insights into our method, such as examining the pruning patterns and analyzing the distribution of FLOP savings across different attention heads. Our code is available at https://github.com/zhixuan-lin/arctic-fox.', 'score': 2, 'issue_id': 3259, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 9', 'zh': '4æœˆ9æ—¥'}, 'hash': 'bda352daa194f6f8', 'authors': ['Zhixuan Lin', 'Johan Obando-Ceron', 'Xu Owen He', 'Aaron Courville'], 'affiliations': ['MakerMaker AI', 'Mila & Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2504.06949.jpg', 'data': {'categories': ['#architecture', '#inference', '#training', '#optimization'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹: Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ, Ğ½Ğ¾ Ğ½Ğµ Ñ…ÑƒĞ¶Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Adaptive Computation Pruning (ACP) - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Forgetting Transformer (FoX). ACP Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²ĞµÑĞ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ·Ğ°Ğ¿ÑÑ‚Ğ¾Ğ¹ Ğ½Ğ° 70% Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° 10-35%, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ´Ğ»Ğ¸Ğ½ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'Boosting Efficiency with Adaptive Computation Pruning in FoX', 'desc': "The paper introduces the Forgetting Transformer (FoX), which enhances softmax attention by integrating a forget gate, leading to improved performance over traditional RoPE-based Transformers. It observes that many attention heads in FoX forget information quickly, focusing more on local context. To address this, the authors propose Adaptive Computation Pruning (ACP), which dynamically reduces unnecessary computations based on the forget gate's influence. This method significantly decreases the number of floating-point operations (FLOPs) during language model pretraining, improving training speed by 10% to 35% without sacrificing model performance."}, 'zh': {'title': 'è‡ªé€‚åº”è®¡ç®—å‰ªææå‡FoXæ•ˆç‡', 'desc': 'æœ€è¿‘æå‡ºçš„é—å¿˜å˜æ¢å™¨ï¼ˆFoXï¼‰åœ¨è½¯æœ€å¤§æ³¨æ„åŠ›ä¸­å¼•å…¥äº†é—å¿˜é—¨ï¼Œä¸æ ‡å‡†çš„RoPEå˜æ¢å™¨ç›¸æ¯”ï¼Œè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚FoXä¸­çš„è®¸å¤šæ³¨æ„åŠ›å¤´å¿«é€Ÿé—å¿˜ï¼Œä½¿å¾—å®ƒä»¬åœ¨æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºä¸»è¦ä¾èµ–äºå±€éƒ¨ä¸Šä¸‹æ–‡ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”è®¡ç®—å‰ªæï¼ˆACPï¼‰æ–¹æ³•ï¼ŒåŠ¨æ€å‰ªé™¤ä¸è¾“å…¥è¾“å‡ºä¾èµ–å…³ç³»å¼ºçƒˆè¡°å‡çš„è®¡ç®—ã€‚é€šè¿‡åœ¨FoXçš„è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¸­åº”ç”¨ACPï¼Œæˆ‘ä»¬å®ç°äº†çº¦70%çš„è®¡ç®—é‡å‡å°‘ï¼ŒåŒæ—¶è®­ç»ƒååé‡æé«˜äº†10%åˆ°35%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11326', 'title': 'PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of\n  Complex Videos in the Wild', 'url': 'https://huggingface.co/papers/2504.11326', 'abstract': 'This report provides a comprehensive overview of the 4th Pixel-level Video Understanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025. It summarizes the challenge outcomes, participating methodologies, and future research directions. The challenge features two tracks: MOSE, which focuses on complex scene video object segmentation, and MeViS, which targets motion-guided, language-based video segmentation. Both tracks introduce new, more challenging datasets designed to better reflect real-world scenarios. Through detailed evaluation and analysis, the challenge offers valuable insights into the current state-of-the-art and emerging trends in complex video segmentation. More information can be found on the workshop website: https://pvuw.github.io/.', 'score': 1, 'issue_id': 3260, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': 'abbfe1ceb2f9688a', 'authors': ['Henghui Ding', 'Chang Liu', 'Nikhila Ravi', 'Shuting He', 'Yunchao Wei', 'Song Bai', 'Philip Torr', 'Kehuan Song', 'Xinglin Xie', 'Kexin Zhang', 'Licheng Jiao', 'Lingling Li', 'Shuyuan Yang', 'Xuqiang Cao', 'Linnan Zhao', 'Jiaxuan Zhao', 'Fang Liu', 'Mengjiao Wang', 'Junpei Zhang', 'Xu Liu', 'Yuting Yang', 'Mengru Ma', 'Hao Fang', 'Runmin Cong', 'Xiankai Lu', 'Zhiyang Che', 'Wei Zhan', 'Tianming Liang', 'Haichao Jiang', 'Wei-Shi Zheng', 'Jian-Fang Hu', 'Haobo Yuan', 'Xiangtai Li', 'Tao Zhang', 'Lu Qi', 'Ming-Hsuan Yang'], 'affiliations': ['the Institute of Big Data, Fudan University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.11326.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ 4-Ğ³Ğ¾ ĞºĞ¾Ğ½ĞºÑƒÑ€ÑĞ° PVUW Ğ¿Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹, Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… CVPR 2025. ĞšĞ¾Ğ½ĞºÑƒÑ€Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°Ğ» Ğ´Ğ²Ğ° Ñ‚Ñ€ĞµĞºĞ°: MOSE Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… Ğ¸ MeViS Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°. Ğ‘Ñ‹Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ, Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ´Ğ°ĞµÑ‚ Ñ†ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¸ Ğ¸ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸ÑÑ… Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Advancing Video Segmentation: Insights from the PVUW Challenge', 'desc': 'This paper reviews the 4th Pixel-level Video Understanding in the Wild (PVUW) Challenge, which took place at CVPR 2025. It highlights two main tracks: MOSE for complex scene video object segmentation and MeViS for motion-guided, language-based video segmentation. The challenge introduced new datasets that are more representative of real-world video scenarios, pushing the boundaries of current segmentation techniques. The findings provide insights into the latest advancements and future directions in the field of video segmentation.'}, 'zh': {'title': 'æ¨åŠ¨å¤æ‚è§†é¢‘åˆ†å‰²çš„å‰æ²¿æŒ‘æˆ˜', 'desc': 'æœ¬æŠ¥å‘Šå…¨é¢æ¦‚è¿°äº†2025å¹´CVPRä¼šè®®æœŸé—´ä¸¾è¡Œçš„ç¬¬å››å±Šåƒç´ çº§è§†é¢‘ç†è§£æŒ‘æˆ˜èµ›ï¼ˆPVUWï¼‰ã€‚æŒ‘æˆ˜èµ›åŒ…æ‹¬ä¸¤ä¸ªèµ›é“ï¼šMOSEä¸“æ³¨äºå¤æ‚åœºæ™¯çš„è§†é¢‘ç‰©ä½“åˆ†å‰²ï¼Œè€ŒMeViSåˆ™é’ˆå¯¹åŸºäºè¿åŠ¨å¼•å¯¼å’Œè¯­è¨€çš„è§†é¢‘åˆ†å‰²ã€‚ä¸¤ä¸ªèµ›é“éƒ½å¼•å…¥äº†æ–°çš„ã€æ›´å…·æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ï¼Œä»¥æ›´å¥½åœ°åæ˜ ç°å®ä¸–ç•Œçš„åœºæ™¯ã€‚é€šè¿‡è¯¦ç»†çš„è¯„ä¼°å’Œåˆ†æï¼Œè¯¥æŒ‘æˆ˜èµ›ä¸ºå¤æ‚è§†é¢‘åˆ†å‰²çš„æœ€æ–°æŠ€æœ¯çŠ¶æ€å’Œæ–°å…´è¶‹åŠ¿æä¾›äº†å®è´µçš„è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.11001', 'title': 'ReZero: Enhancing LLM search ability by trying one-more-time', 'url': 'https://huggingface.co/papers/2504.11001', 'abstract': 'Retrieval-Augmented Generation (RAG) improves Large Language Model (LLM) performance on knowledge-intensive tasks but depends heavily on initial search query quality. Current methods, often using Reinforcement Learning (RL), typically focus on query formulation or reasoning over results, without explicitly encouraging persistence after a failed search. We introduce ReZero (Retry-Zero), a novel RL framework that directly rewards the act of retrying a search query following an initial unsuccessful attempt. This incentivizes the LLM to explore alternative queries rather than prematurely halting. ReZero demonstrates significant improvement, achieving 46.88% accuracy compared to a 25% baseline. By rewarding persistence, ReZero enhances LLM robustness in complex information-seeking scenarios where initial queries may prove insufficient.', 'score': 0, 'issue_id': 3263, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 15', 'zh': '4æœˆ15æ—¥'}, 'hash': '9e93b5032c2a9d9c', 'authors': ['Alan Dao', 'Thinh Le'], 'affiliations': ['Menlo Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.11001.jpg', 'data': {'categories': ['#optimization', '#training', '#reasoning', '#rl', '#rag'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ°ÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¾ĞºÑƒĞ¿Ğ°ĞµÑ‚ÑÑ: ReZero Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ReZero Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ÑĞ»Ğµ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€ĞµĞ¶Ğ´ĞµĞ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ. ReZero Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ 25% Ğ´Ğ¾ 46.88% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼.'}, 'en': {'title': 'Persistence Pays Off: Enhancing LLMs with ReZero', 'desc': 'This paper presents ReZero, a new reinforcement learning framework designed to improve the performance of Retrieval-Augmented Generation (RAG) in Large Language Models (LLMs). Unlike traditional methods that focus on refining search queries or analyzing results, ReZero encourages LLMs to persist and retry after an unsuccessful search. By rewarding the act of retrying, it promotes exploration of alternative queries, leading to better outcomes. The results show a significant accuracy increase, demonstrating that persistence can enhance LLM effectiveness in challenging knowledge-intensive tasks.'}, 'zh': {'title': 'é‡è¯•æœç´¢ï¼Œæå‡æ¨¡å‹é²æ£’æ€§', 'desc': 'Retrieval-Augmented Generationï¼ˆRAGï¼‰é€šè¿‡å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œä½†å…¶æ•ˆæœä¾èµ–äºåˆå§‹æœç´¢æŸ¥è¯¢çš„è´¨é‡ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä¸»è¦å…³æ³¨æŸ¥è¯¢çš„åˆ¶å®šæˆ–ç»“æœçš„æ¨ç†ï¼Œè€Œæ²¡æœ‰æ˜ç¡®é¼“åŠ±åœ¨æœç´¢å¤±è´¥åç»§ç»­å°è¯•ã€‚æˆ‘ä»¬æå‡ºäº†ReZeroï¼ˆRetry-Zeroï¼‰ï¼Œä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç›´æ¥å¥–åŠ±åœ¨åˆæ¬¡æœç´¢å¤±è´¥åé‡è¯•æŸ¥è¯¢çš„è¡Œä¸ºã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—æé«˜äº†LLMçš„é²æ£’æ€§ï¼Œåœ¨å¤æ‚çš„ä¿¡æ¯æ£€ç´¢åœºæ™¯ä¸­è¡¨ç°å‡ºæ›´å¥½çš„å‡†ç¡®æ€§ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (1)', '#agi (1)', '#alignment (1)', '#architecture (5)', '#audio', '#benchmark (5)', '#cv (1)', '#data (3)', '#dataset (4)', '#diffusion (3)', '#ethics', '#games (2)', '#graphs', '#hallucinations', '#healthcare', '#inference (1)', '#interpretability (2)', '#leakage', '#long_context (2)', '#low_resource', '#machine_translation', '#math (2)', '#multilingual', '#multimodal (3)', '#open_source (3)', '#optimization (10)', '#plp', '#rag (1)', '#reasoning (6)', '#rl (3)', '#rlhf (1)', '#robotics', '#science', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic', '#training (12)', '#transfer_learning', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-04-16 07:11',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-04-16 07:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-04-16 07:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    