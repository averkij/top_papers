
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 14 papers. January 7.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">7 января</span> | <span id="title-articles-count">14 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-01-06.html">⬅️ <span id="prev-date">06.01</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-01-08.html">➡️ <span id="next-date">08.01</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-01.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '7 января', 'en': 'January 7', 'zh': '1月7日'};
        let feedDateNext = {'ru': '08.01', 'en': '01/08', 'zh': '1月8日'};
        let feedDatePrev = {'ru': '06.01', 'en': '01/06', 'zh': '1月6日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2501.02976', 'title': 'STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution', 'url': 'https://huggingface.co/papers/2501.02976', 'abstract': 'Image diffusion models have been adapted for real-world video super-resolution to tackle over-smoothing issues in GAN-based methods. However, these models struggle to maintain temporal consistency, as they are trained on static images, limiting their ability to capture temporal dynamics effectively. Integrating text-to-video (T2V) models into video super-resolution for improved temporal modeling is straightforward. However, two key challenges remain: artifacts introduced by complex degradations in real-world scenarios, and compromised fidelity due to the strong generative capacity of powerful T2V models (e.g., CogVideoX-5B). To enhance the spatio-temporal quality of restored videos, we introduce~\\name (Spatial-Temporal Augmentation with T2V models for Real-world video super-resolution), a novel approach that leverages T2V models for real-world video super-resolution, achieving realistic spatial details and robust temporal consistency. Specifically, we introduce a Local Information Enhancement Module (LIEM) before the global attention block to enrich local details and mitigate degradation artifacts. Moreover, we propose a Dynamic Frequency (DF) Loss to reinforce fidelity, guiding the model to focus on different frequency components across diffusion steps. Extensive experiments demonstrate~\\name~outperforms state-of-the-art methods on both synthetic and real-world datasets.', 'score': 31, 'issue_id': 1527, 'pub_date': '2025-01-06', 'pub_date_card': {'ru': '6 января', 'en': 'January 6', 'zh': '1月6日'}, 'hash': '13ac412646c508f5', 'authors': ['Rui Xie', 'Yinhong Liu', 'Penghao Zhou', 'Chen Zhao', 'Jun Zhou', 'Kai Zhang', 'Zhenyu Zhang', 'Jian Yang', 'Zhenheng Yang', 'Ying Tai'], 'affiliations': ['ByteDance', 'Nanjing University', 'Southwest University'], 'pdf_title_img': 'assets/pdf/title_img/2501.02976.jpg', 'data': {'categories': ['#cv', '#optimization', '#diffusion', '#multimodal', '#video'], 'emoji': '🎥', 'ru': {'title': 'Качественное суперразрешение видео с помощью T2V моделей', 'desc': 'Представлена новая методика STAR для суперразрешения видео в реальных условиях с использованием моделей text-to-video. Предложен модуль LIEM для улучшения локальных деталей и устранения артефактов деградации. Введена функция потерь Dynamic Frequency для усиления точности восстановления на разных частотах. Эксперименты показывают превосходство STAR над современными методами на синтетических и реальных датасетах.'}, 'en': {'title': 'Enhancing Video Quality with T2V Models for Real-World Super-Resolution', 'desc': 'This paper presents a new method called Spatial-Temporal Augmentation with T2V models for Real-world video super-resolution, which aims to improve video quality by addressing issues of over-smoothing and temporal consistency. Traditional image diffusion models struggle with video because they are designed for static images, leading to challenges in capturing motion dynamics. The proposed approach incorporates a Local Information Enhancement Module to enhance local details and reduce artifacts, along with a Dynamic Frequency Loss to maintain fidelity across different frequency components. Experimental results show that this method outperforms existing techniques in both synthetic and real-world scenarios, providing better spatial and temporal quality in restored videos.'}, 'zh': {'title': '提升视频超分辨率的时空一致性', 'desc': '本文提出了一种新方法，名为~\\name~，用于提高真实世界视频超分辨率的时空质量。该方法结合了文本到视频（T2V）模型，以解决传统生成对抗网络（GAN）方法中的过平滑问题。通过引入局部信息增强模块（LIEM）和动态频率损失（DF Loss），该方法能够有效改善视频的局部细节和时间一致性。实验结果表明，~\\name~在合成和真实世界数据集上均优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2501.03226', 'title': 'BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning', 'url': 'https://huggingface.co/papers/2501.03226', 'abstract': "Cutting-edge large language models (LLMs) demonstrate promising performance in solving complex math problems with a divide-and-conquer pipeline and the assistance of in-context learning (ICL) examples. However, their potential for improvement is limited by two critical problems within their ICL examples: granularity-mismatch and the ensuing negative-effect noise problem. Specifically, the LLMs are capable of the dividing process yet mostly failed by inaccurate reasoning within a few conquer steps, while the ICL examples retrieved in question-grained sometimes lack relevant steps for a specific challenging reasoning step. Further, this disconnect may hinder the correct reasoning due to its irrelevance. To this end, we focus on improving the reasoning quality within each step and present BoostStep. BoostStep aligns the granularity between the retrieving and reasoning on step grained, and provides highly related ICL examples for each reasoning step with a novel `first-try' strategy. BoostStep provides more relevant examples than the coarse question-grained strategy, enhancing the model reasoning quality within each step steadily. BoostStep is a general and robust reasoning-enhancing method that not only improves standalone reasoning performance but also integrates seamlessly with Monte Carlo Tree Search methods (MCTS) to refine both candidate generation and decision-making. Quantitatively, it improves GPT-4o and Qwen2.5-Math-72B by 3.6\\% and 2.0\\% respectively on various mathematical benchmarks, and 7.5\\% gain combined with MCTS.", 'score': 17, 'issue_id': 1532, 'pub_date': '2025-01-06', 'pub_date_card': {'ru': '6 января', 'en': 'January 6', 'zh': '1月6日'}, 'hash': '94a01c7d4516c725', 'authors': ['Beichen Zhang', 'Yuhong Liu', 'Xiaoyi Dong', 'Yuhang Zang', 'Pan Zhang', 'Haodong Duan', 'Yuhang Cao', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2501.03226.jpg', 'data': {'categories': ['#training', '#optimization', '#math', '#reasoning'], 'emoji': '🧮', 'ru': {'title': 'BoostStep: Повышение точности рассуждений ИИ в решении математических задач', 'desc': 'Статья представляет метод BoostStep для улучшения решения сложных математических задач большими языковыми моделями. BoostStep решает проблемы несоответствия детализации и негативного шума в примерах обучения в контексте. Метод выравнивает гранулярность между извлечением и рассуждением на уровне шагов, предоставляя релевантные примеры для каждого шага рассуждения. BoostStep повышает качество рассуждений модели и может интегрироваться с методами поиска по дереву Монте-Карло для улучшения генерации кандидатов и принятия решений.'}, 'en': {'title': 'Boosting Reasoning Quality in Large Language Models with BoostStep', 'desc': "This paper introduces BoostStep, a method designed to enhance the reasoning quality of large language models (LLMs) when solving complex math problems. It addresses two main issues: granularity-mismatch and negative-effect noise in in-context learning (ICL) examples, which can lead to inaccurate reasoning. By aligning the granularity of retrieved examples with the specific reasoning steps required, BoostStep provides more relevant ICL examples, improving the model's performance. The method not only boosts standalone reasoning but also integrates effectively with Monte Carlo Tree Search (MCTS) to enhance decision-making processes."}, 'zh': {'title': '提升推理质量的BoostStep方法', 'desc': '这篇论文探讨了大型语言模型（LLMs）在解决复杂数学问题时的表现，特别是通过分而治之的策略和上下文学习（ICL）示例的辅助。研究发现，ICL示例中的粒度不匹配和负面噪声问题限制了模型的改进潜力。为了解决这些问题，论文提出了BoostStep方法，它通过对每个推理步骤的粒度进行对齐，提供更相关的ICL示例，从而提高推理质量。BoostStep不仅提升了独立推理的性能，还能与蒙特卡洛树搜索（MCTS）方法无缝集成，进一步优化候选生成和决策过程。'}}}, {'id': 'https://huggingface.co/papers/2501.03218', 'title': 'Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction', 'url': 'https://huggingface.co/papers/2501.03218', 'abstract': 'Active Real-time interaction with video LLMs introduces a new paradigm for human-computer interaction, where the model not only understands user intent but also responds while continuously processing streaming video on the fly. Unlike offline video LLMs, which analyze the entire video before answering questions, active real-time interaction requires three capabilities: 1) Perception: real-time video monitoring and interaction capturing. 2) Decision: raising proactive interaction in proper situations, 3) Reaction: continuous interaction with users. However, inherent conflicts exist among the desired capabilities. The Decision and Reaction require a contrary Perception scale and grain, and the autoregressive decoding blocks the real-time Perception and Decision during the Reaction. To unify the conflicted capabilities within a harmonious system, we present Dispider, a system that disentangles Perception, Decision, and Reaction. Dispider features a lightweight proactive streaming video processing module that tracks the video stream and identifies optimal moments for interaction. Once the interaction is triggered, an asynchronous interaction module provides detailed responses, while the processing module continues to monitor the video in the meantime. Our disentangled and asynchronous design ensures timely, contextually accurate, and computationally efficient responses, making Dispider ideal for active real-time interaction for long-duration video streams. Experiments show that Dispider not only maintains strong performance in conventional video QA tasks, but also significantly surpasses previous online models in streaming scenario responses, thereby validating the effectiveness of our architecture. The code and model are released at https://github.com/Mark12Ding/Dispider.', 'score': 17, 'issue_id': 1532, 'pub_date': '2025-01-06', 'pub_date_card': {'ru': '6 января', 'en': 'January 6', 'zh': '1月6日'}, 'hash': '1e9974be2d206516', 'authors': ['Rui Qian', 'Shuangrui Ding', 'Xiaoyi Dong', 'Pan Zhang', 'Yuhang Zang', 'Yuhang Cao', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Shanghai AI Laboratory', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2501.03218.jpg', 'data': {'categories': ['#long_context', '#video', '#optimization', '#architecture', '#interpretability'], 'emoji': '🎥', 'ru': {'title': 'Dispider: Интеллектуальное взаимодействие с видео в реальном времени', 'desc': 'Статья представляет систему Dispider для активного взаимодействия с видео в реальном времени с использованием языковых моделей. Система разделяет процессы восприятия, принятия решений и реакции, что позволяет эффективно обрабатывать потоковое видео и взаимодействовать с пользователем. Dispider использует легковесный модуль обработки видео для отслеживания потока и определения оптимальных моментов для взаимодействия. Асинхронная архитектура обеспечивает своевременные и точные ответы при длительной обработке видеопотоков.'}, 'en': {'title': 'Dispider: Real-time Interaction Redefined for Video LLMs', 'desc': 'This paper introduces Dispider, a system designed for active real-time interaction with video using large language models (LLMs). Unlike traditional offline models, Dispider can process video streams continuously while engaging with users, requiring three key capabilities: Perception, Decision, and Reaction. The system addresses conflicts between these capabilities by disentangling them, allowing for efficient monitoring and interaction without lag. Experimental results demonstrate that Dispider outperforms previous models in streaming scenarios, providing timely and contextually relevant responses during long-duration video interactions.'}, 'zh': {'title': '主动实时交互的新范式', 'desc': '本论文介绍了一种名为Dispider的系统，旨在实现视频大语言模型的主动实时交互。该系统通过分离感知、决策和反应三个能力，解决了实时交互中的固有冲突。Dispider具备轻量级的流媒体处理模块，能够实时监控视频流并识别最佳交互时机。实验结果表明，Dispider在传统视频问答任务中表现优异，并在流媒体场景响应上显著超越了之前的在线模型。'}}}, {'id': 'https://huggingface.co/papers/2501.02157', 'title': 'Personalized Graph-Based Retrieval for Large Language Models', 'url': 'https://huggingface.co/papers/2501.02157', 'abstract': 'As large language models (LLMs) evolve, their ability to deliver personalized and context-aware responses offers transformative potential for improving user experiences. Existing personalization approaches, however, often rely solely on user history to augment the prompt, limiting their effectiveness in generating tailored outputs, especially in cold-start scenarios with sparse data. To address these limitations, we propose Personalized Graph-based Retrieval-Augmented Generation (PGraphRAG), a framework that leverages user-centric knowledge graphs to enrich personalization. By directly integrating structured user knowledge into the retrieval process and augmenting prompts with user-relevant context, PGraphRAG enhances contextual understanding and output quality. We also introduce the Personalized Graph-based Benchmark for Text Generation, designed to evaluate personalized text generation tasks in real-world settings where user history is sparse or unavailable. Experimental results show that PGraphRAG significantly outperforms state-of-the-art personalization methods across diverse tasks, demonstrating the unique advantages of graph-based retrieval for personalization.', 'score': 11, 'issue_id': 1527, 'pub_date': '2025-01-04', 'pub_date_card': {'ru': '4 января', 'en': 'January 4', 'zh': '1月4日'}, 'hash': '65e3736cfc1e3295', 'authors': ['Steven Au', 'Cameron J. Dimacali', 'Ojasmitha Pedirappagari', 'Namyong Park', 'Franck Dernoncourt', 'Yu Wang', 'Nikos Kanakaris', 'Hanieh Deilamsalehy', 'Ryan A. Rossi', 'Nesreen K. Ahmed'], 'affiliations': ['Adobe Research', 'Cisco AI Research', 'Meta AI', 'University of California Santa Cruz', 'University of Oregon', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2501.02157.jpg', 'data': {'categories': ['#rag', '#optimization', '#graphs', '#multimodal', '#benchmark', '#games'], 'emoji': '🕸️', 'ru': {'title': 'Графы знаний на службе персонализации языковых моделей', 'desc': 'Статья представляет новый подход к персонализации ответов больших языковых моделей (LLM) под названием PGraphRAG. В отличие от существующих методов, полагающихся на историю пользователя, PGraphRAG использует ориентированные на пользователя графы знаний для обогащения контекста. Этот метод улучшает понимание контекста и качество генерируемых ответов, особенно в сценариях с ограниченными данными о пользователе. Экспериментальные результаты показывают, что PGraphRAG превосходит современные методы персонализации в различных задачах.'}, 'en': {'title': 'Revolutionizing Personalization with Graph-based Retrieval', 'desc': "This paper introduces a new framework called Personalized Graph-based Retrieval-Augmented Generation (PGraphRAG) that enhances the personalization of large language models (LLMs). Unlike traditional methods that depend only on user history, PGraphRAG utilizes user-centric knowledge graphs to provide richer context for generating responses. By integrating structured user information into the retrieval process, it improves the model's understanding and the quality of its outputs, especially in situations where user data is limited. The authors also present a benchmark for evaluating personalized text generation, showing that PGraphRAG outperforms existing methods in various tasks."}, 'zh': {'title': '个性化图谱提升生成质量', 'desc': '随着大型语言模型的发展，它们在提供个性化和上下文感知的响应方面展现出巨大的潜力。现有的个性化方法通常仅依赖用户历史数据来增强提示，这在数据稀疏的冷启动场景中效果有限。为了解决这些问题，我们提出了个性化图谱检索增强生成（PGraphRAG）框架，利用以用户为中心的知识图谱来丰富个性化。实验结果表明，PGraphRAG在多种任务中显著优于现有的个性化方法，展示了基于图谱的检索在个性化中的独特优势。'}}}, {'id': 'https://huggingface.co/papers/2501.02497', 'title': 'Test-time Computing: from System-1 Thinking to System-2 Thinking', 'url': 'https://huggingface.co/papers/2501.02497', 'abstract': "The remarkable performance of the o1 model in complex reasoning demonstrates that test-time computing scaling can further unlock the model's potential, enabling powerful System-2 thinking. However, there is still a lack of comprehensive surveys for test-time computing scaling. We trace the concept of test-time computing back to System-1 models. In System-1 models, test-time computing addresses distribution shifts and improves robustness and generalization through parameter updating, input modification, representation editing, and output calibration. In System-2 models, it enhances the model's reasoning ability to solve complex problems through repeated sampling, self-correction, and tree search. We organize this survey according to the trend of System-1 to System-2 thinking, highlighting the key role of test-time computing in the transition from System-1 models to weak System-2 models, and then to strong System-2 models. We also point out a few possible future directions.", 'score': 10, 'issue_id': 1528, 'pub_date': '2025-01-05', 'pub_date_card': {'ru': '5 января', 'en': 'January 5', 'zh': '1月5日'}, 'hash': '7d9414c60fe7701d', 'authors': ['Yixin Ji', 'Juntao Li', 'Hai Ye', 'Kaixin Wu', 'Jia Xu', 'Linjian Mo', 'Min Zhang'], 'affiliations': ['Ant Group', 'Department of Computer Science, National University of Singapore', 'School of Computer Science and Technology, Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2501.02497.jpg', 'data': {'categories': ['#reasoning', '#math', '#survey', '#training'], 'emoji': '🧠', 'ru': {'title': 'Масштабирование вычислений: путь к мышлению System-2', 'desc': 'Эта статья рассматривает масштабирование вычислений во время тестирования для улучшения производительности моделей машинного обучения. Авторы прослеживают эволюцию этой концепции от моделей System-1 до моделей System-2. В работе описываются различные методы, такие как обновление параметров, модификация входных данных и древовидный поиск. Исследование подчеркивает ключевую роль вычислений во время тестирования в переходе от моделей System-1 к сильным моделям System-2.'}, 'en': {'title': 'Unlocking Model Potential: The Power of Test-Time Computing', 'desc': 'This paper explores the concept of test-time computing scaling and its impact on machine learning models, particularly in enhancing reasoning capabilities. It distinguishes between System-1 models, which focus on improving robustness and generalization through techniques like parameter updating and output calibration, and System-2 models, which utilize methods such as repeated sampling and self-correction for complex problem-solving. The authors trace the evolution from System-1 to System-2 thinking, emphasizing how test-time computing plays a crucial role in this transition. Additionally, the paper identifies potential future research directions in this area.'}, 'zh': {'title': '测试时计算：从系统-1到强系统-2的关键转变', 'desc': '这篇论文探讨了测试时计算扩展对机器学习模型的影响，特别是在复杂推理中的应用。作者指出，测试时计算可以通过参数更新、输入修改、表示编辑和输出校准来提高模型的鲁棒性和泛化能力。对于系统-2模型，测试时计算通过重复采样、自我修正和树搜索来增强模型的推理能力。论文还强调了测试时计算在从系统-1模型向弱系统-2模型再到强系统-2模型转变中的关键作用，并提出了一些未来的研究方向。'}}}, {'id': 'https://huggingface.co/papers/2501.02045', 'title': 'METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring', 'url': 'https://huggingface.co/papers/2501.02045', 'abstract': 'We pretrain METAGENE-1, a 7-billion-parameter autoregressive transformer model, which we refer to as a metagenomic foundation model, on a novel corpus of diverse metagenomic DNA and RNA sequences comprising over 1.5 trillion base pairs. This dataset is sourced from a large collection of human wastewater samples, processed and sequenced using deep metagenomic (next-generation) sequencing methods. Unlike genomic models that focus on individual genomes or curated sets of specific species, the aim of METAGENE-1 is to capture the full distribution of genomic information present within this wastewater, to aid in tasks relevant to pandemic monitoring and pathogen detection. We carry out byte-pair encoding (BPE) tokenization on our dataset, tailored for metagenomic sequences, and then pretrain our model. In this paper, we first detail the pretraining dataset, tokenization strategy, and model architecture, highlighting the considerations and design choices that enable the effective modeling of metagenomic data. We then show results of pretraining this model on our metagenomic dataset, providing details about our losses, system metrics, and training stability over the course of pretraining. Finally, we demonstrate the performance of METAGENE-1, which achieves state-of-the-art results on a set of genomic benchmarks and new evaluations focused on human-pathogen detection and genomic sequence embedding, showcasing its potential for public health applications in pandemic monitoring, biosurveillance, and early detection of emerging health threats.', 'score': 10, 'issue_id': 1528, 'pub_date': '2025-01-03', 'pub_date_card': {'ru': '3 января', 'en': 'January 3', 'zh': '1月3日'}, 'hash': '60a3568f555ed60f', 'authors': ['Ollie Liu', 'Sami Jaghouar', 'Johannes Hagemann', 'Shangshang Wang', 'Jason Wiemels', 'Jeff Kaufman', 'Willie Neiswanger'], 'affiliations': ['Nucleic Acid Observatory', 'Prime Intellect', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2501.02045.jpg', 'data': {'categories': ['#benchmark', '#data', '#training', '#architecture', '#science', '#dataset', '#healthcare'], 'emoji': '🧬', 'ru': {'title': 'METAGENE-1: Метагеномная модель для мониторинга здоровья населения', 'desc': 'METAGENE-1 - это автореграссивная трансформерная модель с 7 миллиардами параметров, обученная на разнообразных метагеномных последовательностях ДНК и РНК. Модель создана для анализа геномной информации из образцов сточных вод с целью мониторинга пандемий и обнаружения патогенов. Авторы описывают процесс предобучения, включая токенизацию и архитектуру модели, а также демонстрируют результаты на различных геномных задачах. METAGENE-1 показывает высокую эффективность в обнаружении патогенов человека и встраивании геномных последовательностей, что открывает перспективы для применения в общественном здравоохранении.'}, 'en': {'title': 'Unlocking Metagenomics: METAGENE-1 for Pandemic Preparedness', 'desc': 'The paper introduces METAGENE-1, a large autoregressive transformer model designed for metagenomic data analysis. It is pretrained on a vast dataset of metagenomic DNA and RNA sequences derived from human wastewater, totaling over 1.5 trillion base pairs. The model aims to enhance pandemic monitoring and pathogen detection by capturing the diverse genomic information present in wastewater samples. The authors detail their tokenization strategy and model architecture, demonstrating that METAGENE-1 achieves state-of-the-art performance in genomic benchmarks and applications related to public health.'}, 'zh': {'title': 'METAGENE-1：元基因组基础模型助力公共卫生监测', 'desc': '我们预训练了METAGENE-1，这是一个拥有70亿参数的自回归变换器模型，称为元基因组基础模型。该模型在一个包含超过1.5万亿碱基对的多样化元基因组DNA和RNA序列的新数据集上进行训练，这些数据来自大量人类废水样本。METAGENE-1的目标是捕捉废水中存在的基因组信息的完整分布，以帮助进行疫情监测和病原体检测。我们展示了该模型在元基因组数据集上的预训练结果，证明其在公共卫生应用中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2501.03006', 'title': 'TransPixar: Advancing Text-to-Video Generation with Transparency', 'url': 'https://huggingface.co/papers/2501.03006', 'abstract': 'Text-to-video generative models have made significant strides, enabling diverse applications in entertainment, advertising, and education. However, generating RGBA video, which includes alpha channels for transparency, remains a challenge due to limited datasets and the difficulty of adapting existing models. Alpha channels are crucial for visual effects (VFX), allowing transparent elements like smoke and reflections to blend seamlessly into scenes. We introduce TransPixar, a method to extend pretrained video models for RGBA generation while retaining the original RGB capabilities. TransPixar leverages a diffusion transformer (DiT) architecture, incorporating alpha-specific tokens and using LoRA-based fine-tuning to jointly generate RGB and alpha channels with high consistency. By optimizing attention mechanisms, TransPixar preserves the strengths of the original RGB model and achieves strong alignment between RGB and alpha channels despite limited training data. Our approach effectively generates diverse and consistent RGBA videos, advancing the possibilities for VFX and interactive content creation.', 'score': 8, 'issue_id': 1527, 'pub_date': '2025-01-06', 'pub_date_card': {'ru': '6 января', 'en': 'January 6', 'zh': '1月6日'}, 'hash': 'e85e5fa9a03d5d04', 'authors': ['Luozhou Wang', 'Yijun Li', 'Zhifei Chen', 'Jui-Hsien Wang', 'Zhifei Zhang', 'He Zhang', 'Zhe Lin', 'Yingcong Chen'], 'affiliations': ['Adobe Research', 'HKUST', 'HKUST(GZ)'], 'pdf_title_img': 'assets/pdf/title_img/2501.03006.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training', '#diffusion', '#video'], 'emoji': '🎬', 'ru': {'title': 'TransPixar: Прорыв в генерации RGBA-видео для визуальных эффектов', 'desc': 'TransPixar - это новый метод генерации RGBA-видео, расширяющий возможности предобученных видеомоделей. Он использует архитектуру диффузионного трансформера (DiT) и токены, специфичные для альфа-канала, для совместной генерации RGB и альфа-каналов с высокой согласованностью. Метод применяет тонкую настройку на основе LoRA и оптимизирует механизмы внимания для сохранения сильных сторон исходной RGB-модели. TransPixar эффективно генерирует разнообразные и согласованные RGBA-видео, открывая новые возможности для создания визуальных эффектов и интерактивного контента.'}, 'en': {'title': 'TransPixar: Bridging RGB and Alpha for Enhanced Video Generation', 'desc': 'This paper presents TransPixar, a novel method for generating RGBA videos, which include transparency information crucial for visual effects. The challenge lies in the limited datasets and the need to adapt existing models to handle alpha channels effectively. TransPixar utilizes a diffusion transformer architecture and incorporates alpha-specific tokens, allowing it to generate both RGB and alpha channels simultaneously. By optimizing attention mechanisms and employing LoRA-based fine-tuning, TransPixar achieves high consistency between RGB and alpha outputs, enhancing the quality of video generation for applications in VFX and interactive media.'}, 'zh': {'title': 'TransPixar：生成高质量RGBA视频的新方法', 'desc': '本文介绍了一种名为TransPixar的方法，旨在生成包含透明通道的RGBA视频。传统的视频生成模型在处理透明效果时面临挑战，TransPixar通过扩展预训练模型来解决这一问题。该方法利用扩散变换器架构，结合特定的透明通道标记，并通过LoRA微调实现RGB和透明通道的高一致性生成。最终，TransPixar在有限的数据集上优化了注意力机制，成功生成多样且一致的RGBA视频，推动了视觉特效和互动内容创作的可能性。'}}}, {'id': 'https://huggingface.co/papers/2501.01790', 'title': 'Ingredients: Blending Custom Photos with Video Diffusion Transformers', 'url': 'https://huggingface.co/papers/2501.01790', 'abstract': 'This paper presents a powerful framework to customize video creations by incorporating multiple specific identity (ID) photos, with video diffusion Transformers, referred to as Ingredients. Generally, our method consists of three primary modules: (i) a facial extractor that captures versatile and precise facial features for each human ID from both global and local perspectives; (ii) a multi-scale projector that maps face embeddings into the contextual space of image query in video diffusion transformers; (iii) an ID router that dynamically combines and allocates multiple ID embedding to the corresponding space-time regions. Leveraging a meticulously curated text-video dataset and a multi-stage training protocol, Ingredients demonstrates superior performance in turning custom photos into dynamic and personalized video content. Qualitative evaluations highlight the advantages of proposed method, positioning it as a significant advancement toward more effective generative video control tools in Transformer-based architecture, compared to existing methods. The data, code, and model weights are publicly available at: https://github.com/feizc/Ingredients.', 'score': 5, 'issue_id': 1528, 'pub_date': '2025-01-03', 'pub_date_card': {'ru': '3 января', 'en': 'January 3', 'zh': '1月3日'}, 'hash': 'dd1ccebdd2fcf276', 'authors': ['Zhengcong Fei', 'Debang Li', 'Di Qiu', 'Changqian Yu', 'Mingyuan Fan'], 'affiliations': ['Kunlun Inc. Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2501.01790.jpg', 'data': {'categories': ['#open_source', '#training', '#architecture', '#video', '#dataset', '#diffusion', '#multimodal'], 'emoji': '🎬', 'ru': {'title': 'Персонализированное видео из фотографий: новый уровень контроля в генеративных моделях', 'desc': 'Статья представляет новый метод под названием Ingredients для создания персонализированных видео с использованием нескольких фотографий конкретных людей. Метод состоит из трех основных модулей: экстрактора лицевых признаков, многомасштабного проектора и маршрутизатора идентификаторов. Ingredients использует тщательно подобранный набор данных текст-видео и многоэтапный протокол обучения для достижения превосходных результатов. Качественная оценка показывает преимущества предложенного метода по сравнению с существующими подходами в области генеративного контроля видео на основе архитектуры Transformer.'}, 'en': {'title': 'Transforming Photos into Personalized Videos with Ingredients', 'desc': 'This paper introduces a novel framework called Ingredients for creating personalized videos using multiple identity photos. It employs a facial extractor to accurately capture facial features, a multi-scale projector to integrate these features into video diffusion transformers, and an ID router to manage the allocation of identity embeddings across different time and space regions in the video. The framework is trained on a carefully selected text-video dataset, enhancing its ability to generate dynamic video content from custom images. The results show that Ingredients outperforms existing methods, marking a significant step forward in generative video control using Transformer architectures.'}, 'zh': {'title': '个性化视频创作的新突破', 'desc': '本文提出了一种强大的框架，通过结合多个特定身份照片，定制视频创作，称为Ingredients。该方法主要由三个模块组成：面部提取器、多个尺度投影器和身份路由器，分别用于提取面部特征、映射面部嵌入和动态分配身份嵌入。通过精心策划的文本-视频数据集和多阶段训练协议，Ingredients在将自定义照片转化为动态个性化视频内容方面表现出色。定性评估显示，该方法在基于Transformer的架构中，相较于现有方法，显著提升了生成视频控制工具的有效性。'}}}, {'id': 'https://huggingface.co/papers/2501.02576', 'title': 'DepthMaster: Taming Diffusion Models for Monocular Depth Estimation', 'url': 'https://huggingface.co/papers/2501.02576', 'abstract': "Monocular depth estimation within the diffusion-denoising paradigm demonstrates impressive generalization ability but suffers from low inference speed. Recent methods adopt a single-step deterministic paradigm to improve inference efficiency while maintaining comparable performance. However, they overlook the gap between generative and discriminative features, leading to suboptimal results. In this work, we propose DepthMaster, a single-step diffusion model designed to adapt generative features for the discriminative depth estimation task. First, to mitigate overfitting to texture details introduced by generative features, we propose a Feature Alignment module, which incorporates high-quality semantic features to enhance the denoising network's representation capability. Second, to address the lack of fine-grained details in the single-step deterministic framework, we propose a Fourier Enhancement module to adaptively balance low-frequency structure and high-frequency details. We adopt a two-stage training strategy to fully leverage the potential of the two modules. In the first stage, we focus on learning the global scene structure with the Feature Alignment module, while in the second stage, we exploit the Fourier Enhancement module to improve the visual quality. Through these efforts, our model achieves state-of-the-art performance in terms of generalization and detail preservation, outperforming other diffusion-based methods across various datasets. Our project page can be found at https://indu1ge.github.io/DepthMaster_page.", 'score': 3, 'issue_id': 1536, 'pub_date': '2025-01-05', 'pub_date_card': {'ru': '5 января', 'en': 'January 5', 'zh': '1月5日'}, 'hash': 'a8429b95ef4eb7b7', 'authors': ['Ziyang Song', 'Zerong Wang', 'Bo Li', 'Hao Zhang', 'Ruijie Zhu', 'Li Liu', 'Peng-Tao Jiang', 'Tianzhu Zhang'], 'affiliations': ['School of Information Science and Technology, University of Science and Technology of China (USTC), Hefei 230026, P.R.China', 'vivo Mobile Communication Co., Ltd., Hangzhou 310030, P.R.China'], 'pdf_title_img': 'assets/pdf/title_img/2501.02576.jpg', 'data': {'categories': ['#optimization', '#training', '#diffusion', '#cv', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'DepthMaster: Однопроходная диффузионная модель для точной оценки глубины с улучшенной генерализацией', 'desc': 'DepthMaster - это однопроходная диффузионная модель для монокулярной оценки глубины. Она использует модуль выравнивания признаков для улучшения представления семантических особенностей и модуль улучшения Фурье для балансировки низкочастотной структуры и высокочастотных деталей. Модель обучается в два этапа: сначала фокусируется на глобальной структуре сцены, затем улучшает визуальное качество. DepthMaster превосходит другие диффузионные методы по обобщающей способности и сохранению деталей на различных наборах данных.'}, 'en': {'title': 'DepthMaster: Bridging Generative and Discriminative Depth Estimation', 'desc': 'This paper introduces DepthMaster, a single-step diffusion model aimed at improving monocular depth estimation. It addresses the inefficiencies of previous methods by integrating a Feature Alignment module to enhance the representation of semantic features and reduce overfitting to textures. Additionally, a Fourier Enhancement module is proposed to balance low-frequency structures with high-frequency details, ensuring finer depth estimation. The two-stage training strategy allows the model to first learn global scene structures and then refine visual quality, resulting in state-of-the-art performance across various datasets.'}, 'zh': {'title': 'DepthMaster：提升深度估计的单步扩散模型', 'desc': '本文提出了一种名为DepthMaster的单步扩散模型，用于单目深度估计。该模型通过特征对齐模块和傅里叶增强模块，优化生成特征以适应判别性深度估计任务。特征对齐模块增强了去噪网络的表示能力，而傅里叶增强模块则平衡了低频结构和高频细节。通过两阶段训练策略，DepthMaster在泛化能力和细节保留方面达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2501.03059', 'title': 'Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation', 'url': 'https://huggingface.co/papers/2501.03059', 'abstract': "We consider the task of Image-to-Video (I2V) generation, which involves transforming static images into realistic video sequences based on a textual description. While recent advancements produce photorealistic outputs, they frequently struggle to create videos with accurate and consistent object motion, especially in multi-object scenarios. To address these limitations, we propose a two-stage compositional framework that decomposes I2V generation into: (i) An explicit intermediate representation generation stage, followed by (ii) A video generation stage that is conditioned on this representation. Our key innovation is the introduction of a mask-based motion trajectory as an intermediate representation, that captures both semantic object information and motion, enabling an expressive but compact representation of motion and semantics. To incorporate the learned representation in the second stage, we utilize object-level attention objectives. Specifically, we consider a spatial, per-object, masked-cross attention objective, integrating object-specific prompts into corresponding latent space regions and a masked spatio-temporal self-attention objective, ensuring frame-to-frame consistency for each object. We evaluate our method on challenging benchmarks with multi-object and high-motion scenarios and empirically demonstrate that the proposed method achieves state-of-the-art results in temporal coherence, motion realism, and text-prompt faithfulness. Additionally, we introduce \\benchmark, a new challenging benchmark for single-object and multi-object I2V generation, and demonstrate our method's superiority on this benchmark. Project page is available at https://guyyariv.github.io/TTM/.", 'score': 3, 'issue_id': 1532, 'pub_date': '2025-01-06', 'pub_date_card': {'ru': '6 января', 'en': 'January 6', 'zh': '1月6日'}, 'hash': '4f24667b663efb7d', 'authors': ['Guy Yariv', 'Yuval Kirstain', 'Amit Zohar', 'Shelly Sheynin', 'Yaniv Taigman', 'Yossi Adi', 'Sagie Benaim', 'Adam Polyak'], 'affiliations': ['FAIR, Meta', 'GenAI, Meta', 'The Hebrew University of Jerusalem'], 'pdf_title_img': 'assets/pdf/title_img/2501.03059.jpg', 'data': {'categories': ['#video', '#multimodal', '#benchmark'], 'emoji': '🎬', 'ru': {'title': 'Генерация реалистичных видео из статичных изображений с помощью масок траекторий движения', 'desc': 'Статья представляет новый подход к генерации видео из изображений (I2V) на основе текстового описания. Авторы предлагают двухэтапную композиционную модель, которая сначала генерирует промежуточное представление в виде маски траектории движения объектов. Затем это представление используется для генерации видео с применением объектно-ориентированных целевых функций внимания. Эксперименты показывают, что предложенный метод достигает лучших результатов по временной согласованности, реалистичности движения и соответствию текстовому описанию.'}, 'en': {'title': 'Transforming Images into Realistic Videos with Motion Precision', 'desc': 'This paper addresses the challenge of generating videos from static images using textual descriptions, known as Image-to-Video (I2V) generation. The authors propose a two-stage framework that first creates an intermediate representation to capture object semantics and motion, followed by a video generation stage that utilizes this representation. A key innovation is the use of a mask-based motion trajectory, which helps maintain accurate object motion and consistency across frames. The method is evaluated against challenging benchmarks and shows superior performance in terms of motion realism and coherence, while also introducing a new benchmark for I2V generation.'}, 'zh': {'title': '图像到视频生成的新突破', 'desc': '本文探讨了图像到视频（I2V）生成的任务，即根据文本描述将静态图像转换为逼真的视频序列。尽管近期的进展能够生成照片级真实感的输出，但在多物体场景中，视频的物体运动准确性和一致性仍然存在挑战。为了解决这些问题，我们提出了一种两阶段的组合框架，首先生成明确的中间表示，然后基于该表示生成视频。我们的创新在于引入了一种基于掩码的运动轨迹作为中间表示，能够捕捉语义物体信息和运动，从而实现运动和语义的紧凑而富有表现力的表示。'}}}, {'id': 'https://huggingface.co/papers/2501.02690', 'title': 'GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking', 'url': 'https://huggingface.co/papers/2501.02690', 'abstract': '4D video control is essential in video generation as it enables the use of sophisticated lens techniques, such as multi-camera shooting and dolly zoom, which are currently unsupported by existing methods. Training a video Diffusion Transformer (DiT) directly to control 4D content requires expensive multi-view videos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that optimizes a 4D representation and renders videos according to different 4D elements, such as camera pose and object motion editing, we bring pseudo 4D Gaussian fields to video generation. Specifically, we propose a novel framework that constructs a pseudo 4D Gaussian field with dense 3D point tracking and renders the Gaussian field for all video frames. Then we finetune a pretrained DiT to generate videos following the guidance of the rendered video, dubbed as GS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense 3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field construction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art sparse 3D point tracking method, in accuracy and accelerates the inference speed by two orders of magnitude. During the inference stage, GS-DiT can generate videos with the same dynamic content while adhering to different camera parameters, addressing a significant limitation of current video generation models. GS-DiT demonstrates strong generalization capabilities and extends the 4D controllability of Gaussian splatting to video generation beyond just camera poses. It supports advanced cinematic effects through the manipulation of the Gaussian field and camera intrinsics, making it a powerful tool for creative video production. Demos are available at https://wkbian.github.io/Projects/GS-DiT/.', 'score': 3, 'issue_id': 1530, 'pub_date': '2025-01-05', 'pub_date_card': {'ru': '5 января', 'en': 'January 5', 'zh': '1月5日'}, 'hash': 'b4c147a2637166a8', 'authors': ['Weikang Bian', 'Zhaoyang Huang', 'Xiaoyu Shi', 'Yijin Li', 'Fu-Yun Wang', 'Hongsheng Li'], 'affiliations': ['Avolution AI', 'Centre for Perceptual and Interactive Intelligence', 'Multimedia Laboratory, The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2501.02690.jpg', 'data': {'categories': ['#video', '#games', '#diffusion', '#3d'], 'emoji': '🎥', 'ru': {'title': 'Революция в генерации видео: 4D-контроль с помощью гауссовых полей', 'desc': 'Эта статья представляет инновационный подход к генерации видео с 4D-контролем, используя псевдо-4D гауссовы поля и модель Diffusion Transformer (DiT). Авторы предлагают метод Dense 3D Point Tracking (D3D-PT) для эффективного построения гауссовых полей, превосходящий существующие решения по точности и скорости. Разработанная система GS-DiT позволяет генерировать видео с одинаковым динамическим содержанием, но с разными параметрами камеры, что открывает новые возможности для создания кинематографических эффектов. Метод демонстрирует сильные обобщающие способности и расширяет возможности 4D-контроля в генерации видео.'}, 'en': {'title': 'Revolutionizing Video Generation with 4D Control', 'desc': 'This paper introduces a new method for generating videos that can be controlled in four dimensions (4D), which includes both camera movement and object motion. The authors propose a framework called GS-DiT that utilizes pseudo 4D Gaussian fields to enhance video generation, allowing for advanced cinematic effects. They also present a Dense 3D Point Tracking (D3D-PT) technique that improves the accuracy and speed of tracking 3D points compared to existing methods. Overall, GS-DiT enables the creation of dynamic videos with flexible camera parameters, significantly advancing the capabilities of video generation models.'}, 'zh': {'title': '伪4D高斯场：视频生成的新突破', 'desc': '本论文提出了一种新颖的框架，利用伪4D高斯场进行视频生成，以支持复杂的镜头技术。我们通过密集的3D点跟踪构建伪4D高斯场，并为所有视频帧渲染该高斯场。为了提升GS-DiT的训练效果，我们还提出了一种高效的密集3D点跟踪方法，显著提高了准确性和推理速度。GS-DiT能够在不同的相机参数下生成具有相同动态内容的视频，扩展了视频生成的4D可控性，成为创意视频制作的强大工具。'}}}, {'id': 'https://huggingface.co/papers/2501.01830', 'title': 'Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models', 'url': 'https://huggingface.co/papers/2501.01830', 'abstract': 'Automated red-teaming has become a crucial approach for uncovering vulnerabilities in large language models (LLMs). However, most existing methods focus on isolated safety flaws, limiting their ability to adapt to dynamic defenses and uncover complex vulnerabilities efficiently. To address this challenge, we propose Auto-RT, a reinforcement learning framework that automatically explores and optimizes complex attack strategies to effectively uncover security vulnerabilities through malicious queries. Specifically, we introduce two key mechanisms to reduce exploration complexity and improve strategy optimization: 1) Early-terminated Exploration, which accelerate exploration by focusing on high-potential attack strategies; and 2) Progressive Reward Tracking algorithm with intermediate downgrade models, which dynamically refine the search trajectory toward successful vulnerability exploitation. Extensive experiments across diverse LLMs demonstrate that, by significantly improving exploration efficiency and automatically optimizing attack strategies, Auto-RT detects a boarder range of vulnerabilities, achieving a faster detection speed and 16.63\\% higher success rates compared to existing methods.', 'score': 3, 'issue_id': 1529, 'pub_date': '2025-01-03', 'pub_date_card': {'ru': '3 января', 'en': 'January 3', 'zh': '1月3日'}, 'hash': '5b08b81c52ec8da8', 'authors': ['Yanjiang Liu', 'Shuhen Zhou', 'Yaojie Lu', 'Huijia Zhu', 'Weiqiang Wang', 'Hongyu Lin', 'Ben He', 'Xianpei Han', 'Le Sun'], 'affiliations': ['Ant Group', 'Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China', 'University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2501.01830.jpg', 'data': {'categories': ['#security', '#rl', '#rlhf'], 'emoji': '🛡️', 'ru': {'title': 'Auto-RT: Умная защита больших языковых моделей', 'desc': 'Авторы представляют Auto-RT - фреймворк на основе обучения с подкреплением для автоматизированного поиска уязвимостей в больших языковых моделях (LLM). Система использует механизмы раннего прекращения исследования и прогрессивного отслеживания наград для оптимизации стратегий атак. Auto-RT превосходит существующие методы, обнаруживая более широкий спектр уязвимостей с большей скоростью и на 16.63% более высоким уровнем успеха. Этот подход позволяет эффективно выявлять сложные уязвимости в LLM через вредоносные запросы.'}, 'en': {'title': 'Auto-RT: Revolutionizing Vulnerability Detection in LLMs', 'desc': 'This paper presents Auto-RT, a reinforcement learning framework designed to enhance automated red-teaming for large language models (LLMs). Unlike traditional methods that target isolated safety flaws, Auto-RT efficiently uncovers complex vulnerabilities by optimizing attack strategies through malicious queries. It introduces two innovative mechanisms: Early-terminated Exploration to prioritize promising attack strategies, and Progressive Reward Tracking to refine the search process dynamically. Experimental results show that Auto-RT significantly improves exploration efficiency and detection success rates, outperforming existing approaches.'}, 'zh': {'title': '自动化红队：高效发现语言模型漏洞的利器', 'desc': '自动化红队技术在发现大型语言模型（LLMs）中的漏洞方面变得至关重要。现有方法大多集中于孤立的安全缺陷，限制了其适应动态防御和高效发现复杂漏洞的能力。为了解决这个问题，我们提出了Auto-RT，一个强化学习框架，能够自动探索和优化复杂的攻击策略，通过恶意查询有效发现安全漏洞。我们的实验表明，Auto-RT显著提高了探索效率和攻击策略的自动优化，检测到更广泛的漏洞，检测速度更快，成功率提高了16.63%。'}}}, {'id': 'https://huggingface.co/papers/2501.02832', 'title': 'Samba-asr state-of-the-art speech recognition leveraging structured state-space models', 'url': 'https://huggingface.co/papers/2501.02832', 'abstract': 'We propose Samba ASR, the first state-of-the-art Automatic Speech Recognition (ASR) model leveraging the novel Mamba architecture as both encoder and decoder, built on the foundation of state-space models (SSMs). Unlike transformer-based ASR models, which rely on self-attention mechanisms to capture dependencies, Samba ASR effectively models both local and global temporal dependencies using efficient state-space dynamics, achieving remarkable performance gains. By addressing the limitations of transformers, such as quadratic scaling with input length and difficulty in handling long-range dependencies, Samba ASR achieves superior accuracy and efficiency.   Experimental results demonstrate that Samba ASR surpasses existing open-source transformer-based ASR models across various standard benchmarks, establishing it as the new state of the art in ASR. Extensive evaluations on benchmark datasets show significant improvements in Word Error Rate (WER), with competitive performance even in low-resource scenarios. Furthermore, the computational efficiency and parameter optimization of the Mamba architecture make Samba ASR a scalable and robust solution for diverse ASR tasks.   Our contributions include:   A new Samba ASR architecture demonstrating the superiority of SSMs over transformer-based models for speech sequence processing. A comprehensive evaluation on public benchmarks showcasing state-of-the-art performance. An analysis of computational efficiency, robustness to noise, and sequence generalization. This work highlights the viability of Mamba SSMs as a transformer-free alternative for efficient and accurate ASR. By leveraging state-space modeling advancements, Samba ASR sets a new benchmark for ASR performance and future research.', 'score': 2, 'issue_id': 1530, 'pub_date': '2025-01-06', 'pub_date_card': {'ru': '6 января', 'en': 'January 6', 'zh': '1月6日'}, 'hash': 'ed3c4a6192d0c5f9', 'authors': ['Syed Abdul Gaffar Shakhadri', 'Kruthika KR', 'Kartik Basavaraj Angadi'], 'affiliations': ['SandLogic Technologies Pvt Ltd'], 'pdf_title_img': 'assets/pdf/title_img/2501.02832.jpg', 'data': {'categories': ['#audio', '#architecture', '#benchmark', '#low_resource', '#open_source'], 'emoji': '🎙️', 'ru': {'title': 'Samba ASR: революция в распознавании речи с помощью моделей пространства состояний', 'desc': 'Представлена модель Samba ASR - первая современная система автоматического распознавания речи, использующая архитектуру Mamba в качестве энкодера и декодера на основе моделей пространства состояний (SSM). В отличие от трансформерных моделей, Samba ASR эффективно моделирует локальные и глобальные временные зависимости, достигая значительных улучшений производительности. Экспериментальные результаты показывают, что Samba ASR превосходит существующие модели с открытым исходным кодом на основе трансформеров по различным стандартным показателям. Модель демонстрирует значительное снижение показателя Word Error Rate (WER) и высокую эффективность даже при ограниченных ресурсах.'}, 'en': {'title': 'Samba ASR: Redefining Speech Recognition with State-Space Models', 'desc': 'Samba ASR is a groundbreaking Automatic Speech Recognition model that utilizes the innovative Mamba architecture, which functions as both the encoder and decoder. This model departs from traditional transformer-based approaches by employing state-space models (SSMs) to effectively capture both local and global temporal dependencies, leading to enhanced performance. By overcoming the challenges associated with transformers, such as their inefficiency with long input sequences, Samba ASR achieves superior accuracy and efficiency in recognizing speech. Extensive testing shows that Samba ASR not only outperforms existing transformer-based models but also excels in low-resource environments, making it a robust solution for various ASR applications.'}, 'zh': {'title': 'Samba ASR：超越变换器的语音识别新标杆', 'desc': '我们提出了Samba ASR，这是第一个利用新型Mamba架构作为编码器和解码器的最先进自动语音识别（ASR）模型。与基于变换器的ASR模型不同，Samba ASR通过高效的状态空间动态建模局部和全局时间依赖关系，从而实现显著的性能提升。该模型克服了变换器在处理长距离依赖和输入长度的平方扩展等方面的局限性，展现出更高的准确性和效率。实验结果表明，Samba ASR在多个标准基准测试中超越了现有的开源变换器ASR模型，确立了其在ASR领域的新标杆。'}}}, {'id': 'https://huggingface.co/papers/2501.02506', 'title': 'ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use', 'url': 'https://huggingface.co/papers/2501.02506', 'abstract': 'Effective evaluation of multi-hop tool use is critical for analyzing the understanding, reasoning, and function-calling capabilities of large language models (LLMs). However, progress has been hindered by a lack of reliable evaluation datasets. To address this, we present ToolHop, a dataset comprising 995 user queries and 3,912 associated tools, specifically designed for rigorous evaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful interdependencies, locally executable tools, detailed feedback, and verifiable answers through a novel query-driven data construction approach that includes tool creation, document refinement, and code generation. We evaluate 14 LLMs across five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and GPT), uncovering significant challenges in handling multi-hop tool-use scenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%, underscoring substantial room for improvement. Further analysis reveals variations in tool-use strategies for various families, offering actionable insights to guide the development of more effective approaches. Code and data can be found in https://huggingface.co/bytedance-research/ToolHop.', 'score': 2, 'issue_id': 1529, 'pub_date': '2025-01-05', 'pub_date_card': {'ru': '5 января', 'en': 'January 5', 'zh': '1月5日'}, 'hash': 'f785173226e5f9fc', 'authors': ['Junjie Ye', 'Zhengyin Du', 'Xuesong Yao', 'Weijian Lin', 'Yufei Xu', 'Zehui Chen', 'Zaiyuan Wang', 'Sining Zhu', 'Zhiheng Xi', 'Siyu Yuan', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang', 'Jiechao Chen'], 'affiliations': ['ByteDance', 'Institute of Modern Languages and Linguistics, Fudan University', 'School of Computer Science, Fudan University', 'School of Data Science, Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2501.02506.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#dataset', '#optimization'], 'emoji': '🛠️', 'ru': {'title': 'ToolHop: новый стандарт для оценки многоэтапного использования инструментов в LLM', 'desc': 'Статья представляет новый набор данных ToolHop для оценки многоэтапного использования инструментов большими языковыми моделями (LLM). ToolHop содержит 995 пользовательских запросов и 3912 связанных инструментов, обеспечивая разнообразие запросов, взаимозависимости и возможность локального выполнения. Авторы оценили 14 LLM из пяти семейств моделей, выявив значительные трудности в обработке сценариев многоэтапного использования инструментов. Лучшая модель, GPT-4o, достигла точности 49.04%, что указывает на большой потенциал для улучшения.'}, 'en': {'title': 'ToolHop: Advancing Multi-Hop Tool Use Evaluation for LLMs', 'desc': 'This paper introduces ToolHop, a new dataset designed to evaluate how well large language models (LLMs) can use multiple tools in a single task. It includes 995 user queries and 3,912 tools, focusing on diverse and interdependent queries that can be executed locally. The authors tested 14 different LLMs, revealing that even the best-performing model, GPT-4o, only achieved 49.04% accuracy, indicating significant challenges in multi-hop tool use. The findings highlight different strategies employed by various model families, providing insights for future improvements in LLM capabilities.'}, 'zh': {'title': 'ToolHop：多跳工具使用的有效评估数据集', 'desc': '本文介绍了ToolHop数据集，该数据集包含995个用户查询和3912个相关工具，旨在有效评估大型语言模型（LLMs）在多跳工具使用中的理解、推理和功能调用能力。通过新颖的查询驱动数据构建方法，ToolHop确保了查询的多样性、工具的局部可执行性和可验证的答案。我们对14个不同模型（如LLaMA3.1、Qwen2.5等）进行了评估，发现它们在处理多跳工具使用场景时面临显著挑战。尽管GPT-4o模型的准确率为49.04%，但仍有很大的改进空间，分析还揭示了不同模型家族在工具使用策略上的差异，为未来的研究提供了有价值的见解。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents', '#agi', '#alignment', '#architecture (5)', '#audio (1)', '#benchmark (5)', '#cv (2)', '#data (1)', '#dataset (4)', '#diffusion (5)', '#ethics', '#games (2)', '#graphs (1)', '#hallucinations', '#healthcare (1)', '#inference', '#interpretability (1)', '#leakage', '#long_context (1)', '#low_resource (1)', '#machine_translation', '#math (2)', '#multilingual', '#multimodal (4)', '#open_source (2)', '#optimization (7)', '#plp', '#rag (1)', '#reasoning (3)', '#rl (1)', '#rlhf (1)', '#robotics', '#science (1)', '#security (1)', '#small_models', '#story_generation', '#survey (1)', '#synthetic', '#training (6)', '#transfer_learning', '#video (6)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-01-07 12:18',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-01-07 12:18')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-01-07 12:18')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    