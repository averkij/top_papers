
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 25 papers. April 2.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">2 апреля</span> | <span id="title-articles-count">25 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-04-01.html">⬅️ <span id="prev-date">01.04</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-04-03.html">➡️ <span id="next-date">03.04</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-04.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'};
        let feedDateNext = {'ru': '03.04', 'en': '04/03', 'zh': '4月3日'};
        let feedDatePrev = {'ru': '01.04', 'en': '04/01', 'zh': '4月1日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.24379', 'title': 'Any2Caption:Interpreting Any Condition to Caption for Controllable Video\n  Generation', 'url': 'https://huggingface.co/papers/2503.24379', 'abstract': 'To address the bottleneck of accurate user intent interpretation within the current video generation community, we present Any2Caption, a novel framework for controllable video generation under any condition. The key idea is to decouple various condition interpretation steps from the video synthesis step. By leveraging modern multimodal large language models (MLLMs), Any2Caption interprets diverse inputs--text, images, videos, and specialized cues such as region, motion, and camera poses--into dense, structured captions that offer backbone video generators with better guidance. We also introduce Any2CapIns, a large-scale dataset with 337K instances and 407K conditions for any-condition-to-caption instruction tuning. Comprehensive evaluations demonstrate significant improvements of our system in controllability and video quality across various aspects of existing video generation models. Project Page: https://sqwu.top/Any2Cap/', 'score': 36, 'issue_id': 3018, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'dce65db5da1b8c34', 'authors': ['Shengqiong Wu', 'Weicai Ye', 'Jiahao Wang', 'Quande Liu', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Kun Gai', 'Shuicheng Yan', 'Hao Fei', 'Tat-Seng Chua'], 'affiliations': ['Kuaishou Technology', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.24379.jpg', 'data': {'categories': ['#multimodal', '#video', '#dataset'], 'emoji': '🎬', 'ru': {'title': 'Универсальный контроль генерации видео через интерпретацию любых входных данных', 'desc': 'Any2Caption - это новая система для контролируемой генерации видео с использованием различных входных данных. Ключевая идея заключается в разделении этапов интерпретации условий и синтеза видео. Система использует мультимодальные большие языковые модели для создания подробных структурированных описаний на основе разнообразных входных данных. Авторы также представили большой набор данных Any2CapIns для обучения модели генерации описаний по различным условиям.'}, 'en': {'title': 'Revolutionizing Video Generation with Any2Caption', 'desc': "Any2Caption is a new framework designed to improve how user intentions are understood in video generation. It separates the process of interpreting different conditions from the actual video creation, allowing for more precise control. By using advanced multimodal large language models, it can transform various inputs like text and images into detailed captions that guide video generators more effectively. The introduction of the Any2CapIns dataset, which contains a large number of examples, further enhances the system's ability to generate high-quality videos based on diverse conditions."}, 'zh': {'title': '可控视频生成的新突破：Any2Caption', 'desc': '为了克服当前视频生成领域中准确理解用户意图的瓶颈，我们提出了Any2Caption，这是一个新颖的可控视频生成框架。其核心思想是将各种条件解释步骤与视频合成步骤解耦。通过利用现代多模态大语言模型（MLLMs），Any2Caption能够将文本、图像、视频及特定提示（如区域、运动和相机姿态）转化为密集的结构化字幕，从而为视频生成器提供更好的指导。我们还引入了Any2CapIns，这是一个包含337K实例和407K条件的大规模数据集，用于任何条件到字幕的指令调优。'}}}, {'id': 'https://huggingface.co/papers/2503.24376', 'title': 'Exploring the Effect of Reinforcement Learning on Video Understanding:\n  Insights from SEED-Bench-R1', 'url': 'https://huggingface.co/papers/2503.24376', 'abstract': "Recent advancements in Chain of Thought (COT) generation have significantly improved the reasoning capabilities of Large Language Models (LLMs), with reinforcement learning (RL) emerging as an effective post-training approach. Multimodal Large Language Models (MLLMs) inherit this reasoning potential but remain underexplored in tasks requiring both perception and logical reasoning. To address this, we introduce SEED-Bench-R1, a benchmark designed to systematically evaluate post-training methods for MLLMs in video understanding. It includes intricate real-world videos and complex everyday planning tasks in the format of multiple-choice questions, requiring sophisticated perception and reasoning. SEED-Bench-R1 assesses generalization through a three-level hierarchy: in-distribution, cross-environment, and cross-environment-task scenarios, equipped with a large-scale training dataset with easily verifiable ground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL with supervised fine-tuning (SFT), demonstrating RL's data efficiency and superior performance on both in-distribution and out-of-distribution tasks, even outperforming SFT on general video understanding benchmarks like LongVideoBench. Our detailed analysis reveals that RL enhances visual perception but often produces less logically coherent reasoning chains. We identify key limitations such as inconsistent reasoning and overlooked visual cues, and suggest future improvements in base model reasoning, reward modeling, and RL robustness against noisy signals.", 'score': 19, 'issue_id': 3018, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'd22966d0969ded43', 'authors': ['Yi Chen', 'Yuying Ge', 'Rui Wang', 'Yixiao Ge', 'Lu Qiu', 'Ying Shan', 'Xihui Liu'], 'affiliations': ['ARC Lab, Tencent PCG', 'The Chinese University of Hong Kong', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.24376.jpg', 'data': {'categories': ['#reasoning', '#training', '#multimodal', '#rl', '#benchmark', '#optimization', '#video'], 'emoji': '🎥', 'ru': {'title': 'Обучение с подкреплением улучшает понимание видео мультимодальными ИИ-моделями', 'desc': 'Статья представляет новый бенчмарк SEED-Bench-R1 для оценки методов пост-обучения мультимодальных больших языковых моделей (MLLM) в задачах понимания видео. Исследователи сравнивают обучение с подкреплением (RL) и обычное обучение с учителем (SFT) на модели Qwen2-VL-Instruct-7B. Результаты показывают, что RL более эффективно использует данные и лучше работает как на распределении обучающей выборки, так и вне его. Однако анализ выявляет, что RL улучшает визуальное восприятие, но иногда производит менее логически связные цепочки рассуждений.'}, 'en': {'title': 'Unlocking Reasoning in Multimodal Models with SEED-Bench-R1', 'desc': "This paper discusses the advancements in Chain of Thought (COT) generation for Large Language Models (LLMs) and introduces a new benchmark called SEED-Bench-R1 for evaluating Multimodal Large Language Models (MLLMs) in video understanding tasks. The benchmark includes complex real-world videos and planning tasks presented as multiple-choice questions, assessing the models' perception and reasoning abilities. The study compares reinforcement learning (RL) with supervised fine-tuning (SFT) using the Qwen2-VL-Instruct-7B model, showing that RL is more data-efficient and performs better on various tasks. However, the analysis also highlights limitations in reasoning consistency and visual cue recognition, suggesting areas for future research to enhance model performance."}, 'zh': {'title': '强化学习提升多模态模型推理能力', 'desc': '最近，链式思维（COT）生成的进展显著提升了大型语言模型（LLMs）的推理能力，而强化学习（RL）成为一种有效的后训练方法。多模态大型语言模型（MLLMs）继承了这种推理潜力，但在需要感知和逻辑推理的任务中仍然未被充分探索。为了解决这个问题，我们引入了SEED-Bench-R1，这是一个旨在系统评估MLLMs在视频理解中后训练方法的基准，包含复杂的真实视频和多项选择题的日常规划任务。我们的研究表明，RL在数据效率和性能上优于监督微调（SFT），但在逻辑连贯性方面存在一定的局限性。'}}}, {'id': 'https://huggingface.co/papers/2503.23145', 'title': 'CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive\n  Program Synthesis', 'url': 'https://huggingface.co/papers/2503.23145', 'abstract': 'Inductive program synthesis, or programming by example, requires synthesizing functions from input-output examples that generalize to unseen inputs. While large language model agents have shown promise in programming tasks guided by natural language, their ability to perform inductive program synthesis is underexplored. Existing evaluation protocols rely on static sets of examples and held-out tests, offering no feedback when synthesized functions are incorrect and failing to reflect real-world scenarios such as reverse engineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge, a new evaluation framework where agents interact with a hidden target function by querying it with new inputs, synthesizing candidate functions, and iteratively refining their solutions using a differential testing oracle. This interactive setting encourages agents to perform function calls and self-correction based on feedback. We construct the first large-scale benchmark for general-purpose inductive program synthesis, featuring 1114 functions. Among 18 models evaluated, o3-mini performs best with a success rate of 52.7%, highlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on curated synthesis traces yields up to a 31% relative performance gain. CodeARC provides a more realistic and challenging testbed for evaluating LLM-based program synthesis and inductive reasoning.', 'score': 18, 'issue_id': 3021, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 марта', 'en': 'March 29', 'zh': '3月29日'}, 'hash': '945cc4b51522e668', 'authors': ['Anjiang Wei', 'Tarun Suresh', 'Jiannan Cao', 'Naveen Kannan', 'Yuheng Wu', 'Kai Yan', 'Thiago S. F. X. Teixeira', 'Ke Wang', 'Alex Aiken'], 'affiliations': ['Intel', 'MIT', 'Stanford University', 'University of Illinois Urbana-Champaign', 'Visa Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.23145.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#benchmark', '#plp', '#agents', '#training'], 'emoji': '🧠', 'ru': {'title': 'CodeARC: Новый рубеж в индуктивном синтезе программ', 'desc': 'Статья представляет CodeARC - новую систему оценки для индуктивного синтеза программ с использованием больших языковых моделей. В отличие от существующих статических методов, CodeARC позволяет агентам интерактивно взаимодействовать со скрытой целевой функцией, итеративно улучшая свои решения. Авторы создали масштабный бенчмарк из 1114 функций для оценки способностей моделей к индуктивному рассуждению и синтезу программ. Результаты показывают, что даже лучшие модели достигают лишь 52.7% успеха, что подчеркивает сложность задачи.'}, 'en': {'title': 'CodeARC: A New Frontier in Inductive Program Synthesis Evaluation', 'desc': 'This paper introduces CodeARC, a new evaluation framework for inductive program synthesis, which is the process of creating functions based on input-output examples. Unlike traditional methods that use static examples, CodeARC allows agents to interact with a hidden target function, enabling them to refine their solutions through feedback. The framework includes a large-scale benchmark with 1114 functions, demonstrating the challenges faced by models in this domain. The results show that fine-tuning models like LLaMA-3.1-8B-Instruct can significantly improve performance, emphasizing the need for dynamic evaluation in program synthesis tasks.'}, 'zh': {'title': 'CodeARC：提升程序合成的评估新标准', 'desc': '归纳程序合成，也称为示例编程，是从输入输出示例中合成函数的过程，要求能够推广到未见过的输入。虽然大型语言模型在自然语言指导的编程任务中表现出色，但它们在归纳程序合成方面的能力尚未得到充分探索。现有的评估协议依赖于静态示例集和保留测试，无法在合成函数错误时提供反馈，也未能反映现实世界中的场景。我们提出了CodeARC，一个新的评估框架，允许代理通过查询隐藏的目标函数与之互动，从而合成候选函数并根据反馈迭代改进解决方案。'}}}, {'id': 'https://huggingface.co/papers/2504.00050', 'title': 'JudgeLRM: Large Reasoning Models as a Judge', 'url': 'https://huggingface.co/papers/2504.00050', 'abstract': 'The rise of Large Language Models (LLMs) as evaluators offers a scalable alternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for judges approaches often fall short in domains requiring complex reasoning. In this work, we investigate whether LLM judges truly benefit from enhanced reasoning capabilities. Through a detailed analysis of reasoning requirements across evaluation tasks, we reveal a negative correlation between SFT performance gains and the proportion of reasoning-demanding samples - highlighting the limitations of SFT in such scenarios. To address this, we introduce JudgeLRM, a family of judgment-oriented LLMs trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM models consistently outperform both SFT-tuned and state-of-the-art reasoning models. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms DeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks requiring deep reasoning.', 'score': 15, 'issue_id': 3022, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '5060d3d364f635eb', 'authors': ['Nuo Chen', 'Zhiyuan Hu', 'Qingyun Zou', 'Jiaying Wu', 'Qian Wang', 'Bryan Hooi', 'Bingsheng He'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2504.00050.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#training', '#rl'], 'emoji': '⚖️', 'ru': {'title': 'Революция в обучении ИИ-судей: от простой настройки к глубоким рассуждениям', 'desc': 'Исследование показывает ограничения обычной тонкой настройки языковых моделей для задач оценки, требующих сложных рассуждений. Авторы предлагают новый подход JudgeLRM, использующий обучение с подкреплением для создания моделей-судей. JudgeLRM превосходит существующие методы, включая GPT-4, особенно в задачах, требующих глубоких рассуждений. Модель JudgeLRM-7B демонстрирует улучшение на 2.79% по F1-мере по сравнению с DeepSeek-R1.'}, 'en': {'title': 'Reinforcement Learning Boosts LLMs for Complex Judging Tasks', 'desc': 'This paper explores the effectiveness of Large Language Models (LLMs) as judges in evaluation tasks, particularly in scenarios that require complex reasoning. It highlights that traditional Supervised Fine-Tuning (SFT) methods do not perform well when faced with tasks that demand higher reasoning skills. The authors introduce JudgeLRM, a new approach that utilizes reinforcement learning (RL) to enhance the reasoning capabilities of LLMs by providing judge-specific rewards. The results show that JudgeLRM models significantly outperform both SFT-tuned models and other leading reasoning models, demonstrating superior performance in tasks that require deep reasoning.'}, 'zh': {'title': 'JudgeLRM：深度推理的评判者', 'desc': '本研究探讨了大型语言模型（LLMs）作为评估者的潜力，尤其是在复杂推理任务中的表现。我们发现，现有的监督微调（SFT）方法在处理需要深度推理的样本时效果不佳。为了解决这个问题，我们提出了JudgeLRM，这是一种基于强化学习（RL）训练的评判导向LLM，能够提供更有效的评估。实验结果表明，JudgeLRM模型在评判任务中表现优于传统的SFT模型和最先进的推理模型，尤其在需要深度推理的任务中表现突出。'}}}, {'id': 'https://huggingface.co/papers/2504.00906', 'title': 'Agent S2: A Compositional Generalist-Specialist Framework for Computer\n  Use Agents', 'url': 'https://huggingface.co/papers/2504.00906', 'abstract': 'Computer use agents automate digital tasks by directly interacting with graphical user interfaces (GUIs) on computers and mobile devices, offering significant potential to enhance human productivity by completing an open-ended space of user queries. However, current agents face significant challenges: imprecise grounding of GUI elements, difficulties with long-horizon task planning, and performance bottlenecks from relying on single generalist models for diverse cognitive tasks. To this end, we introduce Agent S2, a novel compositional framework that delegates cognitive responsibilities across various generalist and specialist models. We propose a novel Mixture-of-Grounding technique to achieve precise GUI localization and introduce Proactive Hierarchical Planning, dynamically refining action plans at multiple temporal scales in response to evolving observations. Evaluations demonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance on three prominent computer use benchmarks. Specifically, Agent S2 achieves 18.9% and 32.7% relative improvements over leading baseline agents such as Claude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation. Moreover, Agent S2 generalizes effectively to other operating systems and applications, surpassing previous best methods by 52.8% on WindowsAgentArena and by 16.52% on AndroidWorld relatively. Code available at https://github.com/simular-ai/Agent-S.', 'score': 11, 'issue_id': 3017, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'e51174b579a417e9', 'authors': ['Saaket Agashe', 'Kyle Wong', 'Vincent Tu', 'Jiachen Yang', 'Ang Li', 'Xin Eric Wang'], 'affiliations': ['Simular Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.00906.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Agent S2: Новый уровень автоматизации компьютерных задач с помощью ИИ', 'desc': 'Статья представляет Agent S2 - новую композиционную систему для автоматизации компьютерных задач через графический интерфейс. Система использует множество специализированных и обобщенных моделей, а также новые методы точной локализации элементов интерфейса и иерархического планирования действий. Agent S2 достигает значительных улучшений производительности по сравнению с существующими решениями на нескольких бенчмарках для разных операционных систем. Авторы утверждают, что их подход открывает новые возможности для повышения продуктивности человека при работе с компьютером.'}, 'en': {'title': 'Agent S2: Revolutionizing Task Automation with Smart Planning and Grounding', 'desc': 'This paper presents Agent S2, a new framework designed to improve the performance of computer use agents that automate tasks by interacting with graphical user interfaces (GUIs). The framework addresses key challenges such as accurately identifying GUI elements and planning complex tasks over time. It introduces a Mixture-of-Grounding technique for better GUI localization and Proactive Hierarchical Planning to adapt action plans based on real-time observations. Evaluations show that Agent S2 outperforms existing agents on multiple benchmarks, demonstrating significant improvements in task execution across different operating systems.'}, 'zh': {'title': 'Agent S2：智能代理的新纪元', 'desc': '本文介绍了一种名为Agent S2的新型智能代理框架，旨在通过将认知任务分配给不同的通用模型和专业模型来提高数字任务的自动化效率。我们提出了一种新的混合定位技术，以实现精确的图形用户界面（GUI）元素定位，并引入了主动层次规划，能够根据不断变化的观察动态调整行动计划。评估结果表明，Agent S2在三个主要的计算机使用基准测试中达到了新的最先进性能，显著超越了现有的领先代理。特别是在OSWorld评估中，Agent S2相较于其他代理实现了18.9%和32.7%的相对提升，展现了其在不同操作系统和应用中的良好泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2504.00810', 'title': 'Z1: Efficient Test-time Scaling with Code', 'url': 'https://huggingface.co/papers/2504.00810', 'abstract': 'Large Language Models (LLMs) can achieve enhanced complex problem-solving through test-time computing scaling, yet this often entails longer contexts and numerous reasoning token costs. In this paper, we propose an efficient test-time scaling method that trains LLMs on code-related reasoning trajectories, facilitating their reduction of excess thinking tokens while maintaining performance. First, we create Z1-Code-Reasoning-107K, a curated dataset of simple and complex coding problems paired with their short and long solution trajectories. Second, we present a novel Shifted Thinking Window to mitigate overthinking overhead by removing context-delimiting tags (e.g., <think>. . . </think>) and capping reasoning tokens. Trained with long and short trajectory data and equipped with Shifted Thinking Window, our model, Z1-7B, demonstrates the ability to adjust its reasoning level as the complexity of problems and exhibits efficient test-time scaling across different reasoning tasks that matches R1-Distill-Qwen-7B performance with about 30% of its average thinking tokens. Notably, fine-tuned with only code trajectories, Z1-7B demonstrates generalization to broader reasoning tasks (47.5% on GPQA Diamond). Our analysis of efficient reasoning elicitation also provides valuable insights for future research.', 'score': 11, 'issue_id': 3019, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'd982593a14ba7da9', 'authors': ['Zhaojian Yu', 'Yinghao Wu', 'Yilun Zhao', 'Arman Cohan', 'Xiao-Ping Zhang'], 'affiliations': ['Tsinghua University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2504.00810.jpg', 'data': {'categories': ['#reasoning', '#rl', '#dataset', '#training', '#long_context'], 'emoji': '🧠', 'ru': {'title': 'Эффективное масштабирование рассуждений в языковых моделях', 'desc': 'Статья представляет эффективный метод масштабирования языковых моделей во время тестирования, основанный на обучении траекториям рассуждений, связанным с кодом. Авторы создали датасет Z1-Code-Reasoning-107K, содержащий задачи по программированию с короткими и длинными решениями. Они также предложили технику Shifted Thinking Window для уменьшения избыточных токенов мышления. Модель Z1-7B, обученная на этих данных, демонстрирует способность адаптировать уровень рассуждений к сложности задач и обобщать на более широкие задачи рассуждения.'}, 'en': {'title': 'Efficient Reasoning in Large Language Models', 'desc': 'This paper introduces a new method for improving the efficiency of Large Language Models (LLMs) during problem-solving by reducing unnecessary reasoning tokens. The authors created a dataset called Z1-Code-Reasoning-107K, which includes various coding problems and their solution paths. They also developed a technique called the Shifted Thinking Window, which helps the model focus on relevant information and limits excessive reasoning. The resulting model, Z1-7B, shows strong performance on complex tasks while using significantly fewer reasoning tokens compared to other models.'}, 'zh': {'title': '高效推理，简化思考！', 'desc': '本文提出了一种高效的测试时间扩展方法，旨在通过训练大型语言模型（LLMs）在代码相关的推理轨迹上，减少多余的思考标记，同时保持性能。我们创建了一个名为Z1-Code-Reasoning-107K的数据集，包含简单和复杂的编码问题及其解决轨迹。我们还提出了一种新颖的移位思维窗口，通过去除上下文分隔标签和限制推理标记，来减轻过度思考的负担。经过训练的模型Z1-7B能够根据问题的复杂性调整推理水平，并在不同的推理任务中实现高效的测试时间扩展。'}}}, {'id': 'https://huggingface.co/papers/2504.01016', 'title': 'GeometryCrafter: Consistent Geometry Estimation for Open-world Videos\n  with Diffusion Priors', 'url': 'https://huggingface.co/papers/2504.01016', 'abstract': 'Despite remarkable advancements in video depth estimation, existing methods exhibit inherent limitations in achieving geometric fidelity through the affine-invariant predictions, limiting their applicability in reconstruction and other metrically grounded downstream tasks. We propose GeometryCrafter, a novel framework that recovers high-fidelity point map sequences with temporal coherence from open-world videos, enabling accurate 3D/4D reconstruction, camera parameter estimation, and other depth-based applications. At the core of our approach lies a point map Variational Autoencoder (VAE) that learns a latent space agnostic to video latent distributions for effective point map encoding and decoding. Leveraging the VAE, we train a video diffusion model to model the distribution of point map sequences conditioned on the input videos. Extensive evaluations on diverse datasets demonstrate that GeometryCrafter achieves state-of-the-art 3D accuracy, temporal consistency, and generalization capability.', 'score': 9, 'issue_id': 3017, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '9430b45c3324fb61', 'authors': ['Tian-Xing Xu', 'Xiangjun Gao', 'Wenbo Hu', 'Xiaoyu Li', 'Song-Hai Zhang', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'HKUST', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.01016.jpg', 'data': {'categories': ['#video', '#3d', '#architecture', '#diffusion', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'GeometryCrafter: Высокоточная оценка глубины видео для 3D реконструкции', 'desc': 'GeometryCrafter - это новая система для высокоточной оценки глубины видео. Она использует вариационный автоэнкодер для кодирования карт точек и диффузионную модель для моделирования последовательностей карт точек. Система позволяет выполнять точную 3D/4D реконструкцию и оценку параметров камеры. Эксперименты показали, что GeometryCrafter превосходит существующие методы по точности 3D, временной согласованности и способности к обобщению.'}, 'en': {'title': 'GeometryCrafter: Elevating Video Depth Estimation with High-Fidelity Point Maps', 'desc': 'This paper introduces GeometryCrafter, a new framework designed to improve video depth estimation by producing high-fidelity point map sequences that maintain temporal coherence. It addresses the limitations of existing methods in achieving accurate geometric representations, which are crucial for tasks like 3D reconstruction and camera parameter estimation. The framework utilizes a point map Variational Autoencoder (VAE) to effectively encode and decode point maps, independent of the video latent distributions. By training a video diffusion model on these point map sequences, GeometryCrafter demonstrates superior performance in 3D accuracy and generalization across various datasets.'}, 'zh': {'title': 'GeometryCrafter：高保真视频深度估计的新框架', 'desc': '尽管视频深度估计取得了显著进展，但现有方法在几何保真度方面存在固有局限，限制了其在重建和其他度量基础下游任务中的应用。我们提出了GeometryCrafter，这是一种新颖的框架，可以从开放世界视频中恢复具有时间一致性的高保真点图序列，从而实现准确的3D/4D重建和相机参数估计。我们的方法核心是一个点图变分自编码器（VAE），它学习一个与视频潜在分布无关的潜在空间，以有效地进行点图编码和解码。通过利用VAE，我们训练了一个视频扩散模型，以建模基于输入视频的点图序列的分布。'}}}, {'id': 'https://huggingface.co/papers/2503.24377', 'title': 'Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for\n  Large Language Models', 'url': 'https://huggingface.co/papers/2503.24377', 'abstract': 'Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to perform complex reasoning tasks, transitioning from fast and intuitive thinking (System 1) to slow and deep reasoning (System 2). While System 2 reasoning improves task accuracy, it often incurs substantial computational costs due to its slow thinking nature and inefficient or unnecessary reasoning behaviors. In contrast, System 1 reasoning is computationally efficient but leads to suboptimal performance. Consequently, it is critical to balance the trade-off between performance (benefits) and computational costs (budgets), giving rise to the concept of reasoning economy. In this survey, we provide a comprehensive analysis of reasoning economy in both the post-training and test-time inference stages of LLMs, encompassing i) the cause of reasoning inefficiency, ii) behavior analysis of different reasoning patterns, and iii) potential solutions to achieve reasoning economy. By offering actionable insights and highlighting open challenges, we aim to shed light on strategies for improving the reasoning economy of LLMs, thereby serving as a valuable resource for advancing research in this evolving area. We also provide a public repository to continually track developments in this fast-evolving field.', 'score': 9, 'issue_id': 3018, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'd18ba97a459aea36', 'authors': ['Rui Wang', 'Hongru Wang', 'Boyang Xue', 'Jianhui Pang', 'Shudong Liu', 'Yi Chen', 'Jiahao Qiu', 'Derek Fai Wong', 'Heng Ji', 'Kam-Fai Wong'], 'affiliations': ['Princeton University', 'The Chinese University of Hong Kong', 'The University of Hong Kong', 'University of Illinois Urbana-Champaign', 'University of Macau'], 'pdf_title_img': 'assets/pdf/title_img/2503.24377.jpg', 'data': {'categories': ['#inference', '#survey', '#reasoning', '#training'], 'emoji': '⚖️', 'ru': {'title': 'Экономия рассуждений в больших языковых моделях: балансируя производительность и затраты', 'desc': 'Статья рассматривает проблему баланса между производительностью и вычислительными затратами в больших языковых моделях (LLM) при выполнении задач рассуждения. Авторы анализируют причины неэффективности рассуждений, различные паттерны рассуждений и потенциальные решения для достижения экономии рассуждений. Исследование охватывает как этап после обучения, так и этап вывода во время тестирования LLM. Авторы стремятся предоставить ценные insights и выделить открытые проблемы для продвижения исследований в этой области.'}, 'en': {'title': 'Balancing Performance and Cost in Language Model Reasoning', 'desc': 'This paper discusses the advancements in Large Language Models (LLMs) that allow them to perform complex reasoning tasks more effectively. It highlights the difference between two types of reasoning: System 1, which is fast and efficient but less accurate, and System 2, which is slower and more accurate but computationally expensive. The authors introduce the concept of reasoning economy, which aims to balance the trade-off between performance and computational costs. They analyze the inefficiencies in reasoning, explore different reasoning patterns, and propose solutions to enhance the reasoning economy of LLMs, providing insights for future research.'}, 'zh': {'title': '推理经济：平衡性能与计算成本的关键', 'desc': '近年来，大型语言模型（LLMs）的进步显著提升了其执行复杂推理任务的能力，尤其是在快速直觉思维（系统1）与缓慢深度推理（系统2）之间的转变。虽然系统2推理提高了任务的准确性，但由于其思维缓慢和推理行为低效，往往会带来较高的计算成本。相对而言，系统1推理计算效率高，但可能导致性能不佳。因此，平衡性能与计算成本之间的权衡，形成了推理经济的概念，这是本研究的核心。'}}}, {'id': 'https://huggingface.co/papers/2504.00595', 'title': 'Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal\n  LLMs on Academic Resources', 'url': 'https://huggingface.co/papers/2504.00595', 'abstract': 'The reproduction of state-of-the-art multimodal LLM pre-training faces barriers at every stage of the pipeline, including high-quality data filtering, multimodal data mixture strategies, sequence packing techniques, and training frameworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter Multimodal Large Language Model pre-trained efficiently on 29M image-text pairs using only 442 A100-40G GPU hours. Our approach employs low-to-high dynamic image resolution and multimodal sequence packing to significantly enhance pre-training efficiency. The training dataset was carefully curated using both MLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based filtering methods, substantially improving data quality and training efficiency. The Open-Qwen2VL pre-training is conducted on academic level 8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36\\% of 1.4T multimodal pre-training tokens of Qwen2-VL. The final instruction-tuned Open-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on various multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista, indicating the remarkable training efficiency of Open-Qwen2VL. We open-source all aspects of our work, including compute-efficient and data-efficient training details, data filtering methods, sequence packing scripts, pre-training data in WebDataset format, FSDP-based training codebase, and both base and instruction-tuned model checkpoints. We redefine "fully open" for multimodal LLMs as the complete release of: 1) the training codebase, 2) detailed data filtering techniques, and 3) all pre-training and supervised fine-tuning data used to develop the model.', 'score': 7, 'issue_id': 3019, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '3e8c667bd93d754e', 'authors': ['Weizhi Wang', 'Yu Tian', 'Linjie Yang', 'Heng Wang', 'Xifeng Yan'], 'affiliations': ['Nvidia Research', 'Seed Vision Team, ByteDance', 'UC Santa Barbara'], 'pdf_title_img': 'assets/pdf/title_img/2504.00595.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#dataset', '#training', '#data', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Открытая и эффективная мультимодальная ИИ-модель', 'desc': 'Статья представляет Open-Qwen2VL - полностью открытую мультимодальную языковую модель с 2 миллиардами параметров. Модель была эффективно обучена на 29 миллионах пар изображение-текст, используя всего 442 часа GPU A100-40G. Авторы применили динамическое разрешение изображений и мультимодальную упаковку последовательностей для повышения эффективности предобучения. Open-Qwen2VL превосходит частично открытую модель Qwen2-VL-2B по различным мультимодальным бенчмаркам, демонстрируя высокую эффективность обучения.'}, 'en': {'title': 'Unlocking Efficiency in Multimodal LLMs with Open-Qwen2VL', 'desc': 'The paper presents Open-Qwen2VL, a multimodal large language model (LLM) that is fully open-source and pre-trained on 29 million image-text pairs. It addresses challenges in multimodal LLM pre-training by utilizing advanced data filtering techniques and efficient training strategies, achieving significant improvements in training efficiency. The model is trained using a dynamic image resolution approach and multimodal sequence packing, which enhances the overall performance while reducing resource consumption. Open-Qwen2VL outperforms existing models on various benchmarks, showcasing its effectiveness and the benefits of open-source collaboration in machine learning.'}, 'zh': {'title': '高效开源的多模态大语言模型', 'desc': '本文介绍了Open-Qwen2VL，这是一个完全开源的多模态大语言模型，具有20亿参数，使用2900万对图像-文本数据进行高效预训练。我们采用了动态图像分辨率和多模态序列打包技术，显著提高了预训练的效率。通过使用MLLM和CLIP的过滤技术，提升了数据质量和训练效率。最终，Open-Qwen2VL在多个多模态基准测试中超越了部分开源的最先进模型，展示了其卓越的训练效率。'}}}, {'id': 'https://huggingface.co/papers/2504.01019', 'title': 'MixerMDM: Learnable Composition of Human Motion Diffusion Models', 'url': 'https://huggingface.co/papers/2504.01019', 'abstract': 'Generating human motion guided by conditions such as textual descriptions is challenging due to the need for datasets with pairs of high-quality motion and their corresponding conditions. The difficulty increases when aiming for finer control in the generation. To that end, prior works have proposed to combine several motion diffusion models pre-trained on datasets with different types of conditions, thus allowing control with multiple conditions. However, the proposed merging strategies overlook that the optimal way to combine the generation processes might depend on the particularities of each pre-trained generative model and also the specific textual descriptions. In this context, we introduce MixerMDM, the first learnable model composition technique for combining pre-trained text-conditioned human motion diffusion models. Unlike previous approaches, MixerMDM provides a dynamic mixing strategy that is trained in an adversarial fashion to learn to combine the denoising process of each model depending on the set of conditions driving the generation. By using MixerMDM to combine single- and multi-person motion diffusion models, we achieve fine-grained control on the dynamics of every person individually, and also on the overall interaction. Furthermore, we propose a new evaluation technique that, for the first time in this task, measures the interaction and individual quality by computing the alignment between the mixed generated motions and their conditions as well as the capabilities of MixerMDM to adapt the mixing throughout the denoising process depending on the motions to mix.', 'score': 6, 'issue_id': 3022, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '745108d1df40d1b4', 'authors': ['Pablo Ruiz-Ponce', 'German Barquero', 'Cristina Palmero', 'Sergio Escalera', 'José García-Rodríguez'], 'affiliations': ['Kings College London, UK', 'Universidad de Alicante, Spain', 'Universitat de Barcelona and Computer Vision Center, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2504.01019.jpg', 'data': {'categories': ['#multimodal', '#cv', '#diffusion', '#benchmark', '#dataset'], 'emoji': '🕺', 'ru': {'title': 'Динамическое смешивание диффузионных моделей для улучшенной генерации движений человека', 'desc': 'Статья представляет MixerMDM - первую обучаемую технику композиции моделей для объединения предобученных диффузионных моделей генерации движений человека на основе текстовых описаний. В отличие от предыдущих подходов, MixerMDM обеспечивает динамическую стратегию смешивания, которая обучается состязательным образом для комбинирования процесса шумоподавления каждой модели в зависимости от заданных условий. Используя MixerMDM для объединения диффузионных моделей движения одного и нескольких человек, авторы достигают тонкого контроля над динамикой каждого человека индивидуально, а также над общим взаимодействием. Кроме того, предлагается новая техника оценки, которая измеряет качество взаимодействия и индивидуальных движений.'}, 'en': {'title': 'Dynamic Control of Human Motion Generation with MixerMDM', 'desc': 'This paper addresses the challenge of generating human motion based on textual descriptions by introducing MixerMDM, a novel learnable model composition technique. Unlike previous methods, MixerMDM dynamically combines pre-trained motion diffusion models in an adversarial manner, allowing for better control over the generated motions based on specific conditions. The approach enables fine-grained control over individual and overall interactions in multi-person scenarios. Additionally, the authors propose a new evaluation method to assess the quality of generated motions in relation to their conditions, highlighting the adaptability of MixerMDM during the denoising process.'}, 'zh': {'title': '动态混合，精细控制人类运动生成', 'desc': '本文提出了一种新的模型组合技术，称为MixerMDM，用于结合预训练的文本条件人类运动扩散模型。与以往的方法不同，MixerMDM采用动态混合策略，通过对抗训练学习如何根据生成条件组合去噪过程。该方法能够实现对单人和多人运动的精细控制，提升了每个人的动态表现及整体互动效果。此外，本文还提出了一种新的评估技术，首次量化了生成运动与条件之间的对齐程度，以及MixerMDM在去噪过程中适应混合的能力。'}}}, {'id': 'https://huggingface.co/papers/2504.00509', 'title': 'Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on\n  Elementary School-Level Reasoning Problems?', 'url': 'https://huggingface.co/papers/2504.00509', 'abstract': "The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved a miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs' remarkable reasoning ability indeed comes from true intelligence by human standards, or are they simply reciting solutions witnessed during training at an Internet level? To study this problem, we propose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's recitation behavior when asked simple reasoning problems but with conditions subtly shifted, and conduct empirical analysis on our benchmark. Surprisingly, we found existing cutting-edge LLMs unanimously exhibits extremely severe recitation behavior; by changing one phrase in the condition, top models such as OpenAI-o1 and DeepSeek-R1 can suffer 60% performance loss on elementary school-level arithmetic and reasoning problems. Such findings are a wake-up call to the LLM community that compels us to re-evaluate the true intelligence level of cutting-edge LLMs.", 'score': 6, 'issue_id': 3018, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'c9697e67f23cfa4e', 'authors': ['Kai Yan', 'Yufei Xu', 'Zhengyin Du', 'Xuesong Yao', 'Zheyu Wang', 'Xiaowen Guo', 'Jiecao Chen'], 'affiliations': ['ByteDance Seed', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2504.00509.jpg', 'data': {'categories': ['#multimodal', '#hallucinations', '#benchmark', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Языковые модели: умные рассуждения или простое воспроизведение?', 'desc': 'Авторы статьи предлагают новый мультимодальный бенчмарк RoR-Bench для выявления склонности языковых моделей (LLM) к простому воспроизведению заученных решений. Исследование показало, что даже передовые LLM демонстрируют серьезные проблемы с рассуждениями при небольших изменениях в условиях задач. Результаты указывают на значительное снижение производительности ведущих моделей (до 60%) на элементарных арифметических и логических задачах при изменении всего одной фразы. Авторы призывают сообщество переосмыслить реальный уровень интеллекта современных LLM.'}, 'en': {'title': 'Reassessing LLM Intelligence: Are They Truly Reasoning?', 'desc': 'This paper introduces RoR-Bench, a new benchmark designed to evaluate the reasoning capabilities of large language models (LLMs). The authors investigate whether LLMs demonstrate genuine intelligence or merely replicate learned responses from their training data. Their empirical analysis reveals that even advanced LLMs, like OpenAI-o1 and DeepSeek-R1, show significant performance drops—up to 60%—when faced with slight changes in problem phrasing. This raises important questions about the actual reasoning abilities of these models and suggests a need for a deeper understanding of their intelligence.'}, 'zh': {'title': '重新审视LLM的智能水平', 'desc': '近年来，LLM基准测试的难度从小学水平迅速上升到前沿问题，这让研究人员感到我们离超越人类智能只有一步之遥。然而，LLM的推理能力是否真的是人类标准下的真正智能，还是仅仅在训练中见过的解决方案的复述？为了解决这个问题，我们提出了RoR-Bench，一个新颖的多模态基准，用于检测LLM在简单推理问题中是否存在复述行为。我们的实证分析发现，现有的顶尖LLM在条件稍微改变时，表现出极其严重的复述行为，这促使我们重新评估这些模型的真实智能水平。'}}}, {'id': 'https://huggingface.co/papers/2504.01005', 'title': 'When To Solve, When To Verify: Compute-Optimal Problem Solving and\n  Generative Verification for LLM Reasoning', 'url': 'https://huggingface.co/papers/2504.01005', 'abstract': 'Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should you spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, we evaluate GenRM against SC under a fixed inference budget. Interestingly, we find that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8x the inference compute and requires significantly more compute to outperform it. Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. Our work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at https://github.com/nishadsinghi/sc-genrm-scaling.', 'score': 5, 'issue_id': 3017, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'ee8e4951bf6c7a18', 'authors': ['Nishad Singhi', 'Hritik Bansal', 'Arian Hosseini', 'Aditya Grover', 'Kai-Wei Chang', 'Marcus Rohrbach', 'Anna Rohrbach'], 'affiliations': ['Google DeepMind', 'Mila', 'TU Darmstadt & hessian.AI', 'University of California Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2504.01005.jpg', 'data': {'categories': ['#training', '#math', '#optimization', '#reasoning', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Баланс между генерацией решений и их верификацией в языковых моделях', 'desc': 'Статья исследует стратегии улучшения способностей больших языковых моделей (LLM) к рассуждениям, особенно в задачах математического характера. Авторы сравнивают метод Self-Consistency (SC), генерирующий множество решений и выбирающий наиболее частое, с подходом Generative Reward Models (GenRM), который оценивает решения путем генерации цепочек рассуждений. Исследование показывает, что SC более эффективен по вычислительным ресурсам для большинства практических задач. Авторы также выводят законы масштабирования для парадигмы GenRM, предоставляя практические рекомендации по оптимизации вычислений во время тестирования.'}, 'en': {'title': 'Balancing Solution Generation and Verification for Efficient Reasoning in LLMs', 'desc': 'This paper explores how to improve the reasoning abilities of large language models (LLMs) during problem-solving by adjusting the amount of computation used at test time. It compares two methods: Self-Consistency (SC), which generates multiple answers and picks the most common, and Generative Reward Models (GenRM), which scores answers based on a next-token prediction approach. The study finds that SC is generally more efficient in terms of compute resources compared to GenRM, especially under limited budgets. The authors provide insights on how to effectively balance the generation of solutions and their verification to optimize performance.'}, 'zh': {'title': '优化推理能力：解生成与验证的平衡', 'desc': '本文探讨了在大语言模型（LLMs）中，如何通过扩展测试时计算来提升推理能力，尤其是在数学问题解决任务中。传统的自一致性（Self-Consistency, SC）方法通过生成多个解并采用多数投票选择最常见的答案。最近的生成奖励模型（Generative Reward Models, GenRM）则将验证重构为下一个标记预测任务，从而在推理时引入新的扩展方式。研究表明，在固定的推理预算下，SC在大多数实际情况下比GenRM更具计算效率，提供了在测试时扩展中优化解生成与验证的实用指导。'}}}, {'id': 'https://huggingface.co/papers/2503.22952', 'title': 'OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming\n  Video Contexts', 'url': 'https://huggingface.co/papers/2503.22952', 'abstract': 'The rapid advancement of multi-modal language models (MLLMs) like GPT-4o has propelled the development of Omni language models, designed to process and proactively respond to continuous streams of multi-modal data. Despite their potential, evaluating their real-world interactive capabilities in streaming video contexts remains a formidable challenge. In this work, we introduce OmniMMI, a comprehensive multi-modal interaction benchmark tailored for OmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 videos and 2,290 questions, addressing two critical yet underexplored challenges in existing video benchmarks: streaming video understanding and proactive reasoning, across six distinct subtasks. Moreover, we propose a novel framework, Multi-modal Multiplexing Modeling (M4), designed to enable an inference-efficient streaming model that can see, listen while generating.', 'score': 5, 'issue_id': 3024, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 марта', 'en': 'March 29', 'zh': '3月29日'}, 'hash': '8b78ccf427a5cdc0', 'authors': ['Yuxuan Wang', 'Yueqian Wang', 'Bo Chen', 'Tong Wu', 'Dongyan Zhao', 'Zilong Zheng'], 'affiliations': ['Beijing Institute for General Artificial Intelligence', 'State Key Laboratory of General Artificial Intelligence', 'Wangxuan Institute of Computer Technology, Peking University', 'X-LANCE Lab, Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.22952.jpg', 'data': {'categories': ['#video', '#games', '#inference', '#multimodal', '#reasoning', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'OmniMMI: Новый стандарт для оценки мультимодальных моделей в потоковом видео', 'desc': 'Статья представляет OmniMMI - комплексный бенчмарк для оценки возможностей мультимодальных языковых моделей в контексте потокового видео. Бенчмарк включает более 1000 видео и 2000 вопросов, охватывающих шесть различных подзадач. Авторы также предлагают новую архитектуру Multi-modal Multiplexing Modeling (M4) для эффективной обработки потокового видео. Этот подход позволяет модели одновременно воспринимать визуальную и аудиоинформацию во время генерации текста.'}, 'en': {'title': 'Enhancing Interaction with OmniLLMs in Streaming Video', 'desc': 'This paper presents OmniMMI, a new benchmark for evaluating Omni language models (OmniLLMs) in the context of streaming video. It addresses the challenges of understanding and reasoning in real-time video interactions, which are often overlooked in current benchmarks. The benchmark includes a large dataset of over 1,121 videos and 2,290 questions, focusing on proactive reasoning across six subtasks. Additionally, the authors introduce a framework called Multi-modal Multiplexing Modeling (M4) that enhances the efficiency of streaming models by allowing them to process audio and visual data simultaneously while generating responses.'}, 'zh': {'title': '提升多模态语言模型的互动能力', 'desc': '本论文介绍了OmniMMI，这是一个专为Omni语言模型在流媒体视频环境中设计的多模态交互基准。该基准包含超过1121个视频和2290个问题，旨在解决现有视频基准中流媒体视频理解和主动推理的两个关键挑战。我们还提出了一种新框架，称为多模态复用建模（M4），旨在实现高效推理的流媒体模型，能够在生成内容的同时进行视觉和听觉处理。通过这些创新，我们希望提升多模态语言模型在实际应用中的互动能力。'}}}, {'id': 'https://huggingface.co/papers/2504.00557', 'title': 'Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features', 'url': 'https://huggingface.co/papers/2504.00557', 'abstract': 'Visual token reduction lowers inference costs caused by extensive image features in large vision-language models (LVLMs). Unlike relevant studies that prune tokens in self-attention-only LVLMs, our work uniquely addresses cross-attention-based models, which achieve superior performance. We identify that the key-value (KV) cache size for image tokens in cross-attention layers significantly exceeds that of text tokens in self-attention layers, posing a major compute bottleneck. To mitigate this issue, we exploit the sparse nature in cross-attention maps to selectively prune redundant visual features. Our Trimmed Llama effectively reduces KV cache demands without requiring additional training. By benefiting from 50%-reduced visual features, our model can reduce inference latency and memory usage while achieving benchmark parity.', 'score': 4, 'issue_id': 3018, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'c5628fbf1a189a06', 'authors': ['Jewon Lee', 'Ki-Ung Song', 'Seungmin Yang', 'Donguk Lim', 'Jaeyeon Kim', 'Wooksu Shin', 'Bo-Kyeong Kim', 'Yong Jae Lee', 'Tae-Ho Kim'], 'affiliations': ['Nota Inc.', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2504.00557.jpg', 'data': {'categories': ['#inference', '#optimization', '#benchmark', '#cv'], 'emoji': '✂️', 'ru': {'title': 'Эффективное сжатие визуальных данных в мультимодальных ИИ-моделях', 'desc': 'Статья представляет метод снижения вычислительных затрат в крупных визуально-языковых моделях путем сокращения визуальных токенов. Авторы фокусируются на моделях с кросс-вниманием, выявляя проблему большого размера кэша ключ-значение для визуальных токенов. Они предлагают алгоритм избирательного удаления избыточных визуальных признаков, основанный на разреженности карт кросс-внимания. Метод позволяет снизить задержку и использование памяти при сохранении производительности модели на уровне базовых показателей.'}, 'en': {'title': 'Trimmed Llama: Efficient Visual Token Reduction for Faster Inference', 'desc': 'This paper presents a method called Trimmed Llama, which reduces the number of visual tokens in large vision-language models (LVLMs) to lower inference costs. Unlike previous studies that focused on self-attention models, this work specifically targets cross-attention models, which are known for their better performance. The authors highlight that the key-value (KV) cache for image tokens in cross-attention layers is much larger than that for text tokens, creating a significant computational bottleneck. By selectively pruning redundant visual features based on the sparse nature of cross-attention maps, the model achieves a 50% reduction in visual features, leading to decreased latency and memory usage while maintaining performance benchmarks.'}, 'zh': {'title': '视觉特征修剪，提升推理效率', 'desc': '本论文提出了一种视觉标记减少的方法，以降低大型视觉语言模型（LVLMs）在推理时的计算成本。与以往只针对自注意力模型的研究不同，我们的工作专注于基于交叉注意力的模型，这些模型在性能上更为优越。我们发现交叉注意力层中图像标记的键值（KV）缓存大小远大于自注意力层中的文本标记，成为计算瓶颈。通过利用交叉注意力图的稀疏特性，我们选择性地修剪冗余的视觉特征，从而有效减少KV缓存需求，降低推理延迟和内存使用，同时保持基准性能。'}}}, {'id': 'https://huggingface.co/papers/2504.00294', 'title': 'Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies\n  Ahead', 'url': 'https://huggingface.co/papers/2504.00294', 'abstract': "Inference-time scaling can enhance the reasoning capabilities of large language models (LLMs) on complex problems that benefit from step-by-step problem solving. Although lengthening generated scratchpads has proven effective for mathematical tasks, the broader impact of this approach on other tasks remains less clear. In this work, we investigate the benefits and limitations of scaling methods across nine state-of-the-art models and eight challenging tasks, including math and STEM reasoning, calendar planning, NP-hard problems, navigation, and spatial reasoning. We compare conventional models (e.g., GPT-4o) with models fine-tuned for inference-time scaling (e.g., o1) through evaluation protocols that involve repeated model calls, either independently or sequentially with feedback. These evaluations approximate lower and upper performance bounds and potential for future performance improvements for each model, whether through enhanced training or multi-model inference systems. Our extensive empirical analysis reveals that the advantages of inference-time scaling vary across tasks and diminish as problem complexity increases. In addition, simply using more tokens does not necessarily translate to higher accuracy in these challenging regimes. Results from multiple independent runs with conventional models using perfect verifiers show that, for some tasks, these models can achieve performance close to the average performance of today's most advanced reasoning models. However, for other tasks, a significant performance gap remains, even in very high scaling regimes. Encouragingly, all models demonstrate significant gains when inference is further scaled with perfect verifiers or strong feedback, suggesting ample potential for future improvements.", 'score': 3, 'issue_id': 3018, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'b455a4adb4eae588', 'authors': ['Vidhisha Balachandran', 'Jingya Chen', 'Lingjiao Chen', 'Shivam Garg', 'Neel Joshi', 'Yash Lara', 'John Langford', 'Besmira Nushi', 'Vibhav Vineet', 'Yue Wu', 'Safoora Yousefi'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.00294.jpg', 'data': {'categories': ['#inference', '#reasoning', '#training', '#math', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Масштабирование LLM: потенциал и ограничения в сложных задачах', 'desc': 'Статья исследует влияние масштабирования во время вывода на способности крупных языковых моделей (LLM) решать сложные задачи. Авторы сравнивают обычные модели и модели, настроенные на масштабирование, на восьми различных типах задач. Результаты показывают, что преимущества масштабирования варьируются в зависимости от задачи и уменьшаются с ростом сложности проблемы. Исследование также выявляет потенциал для улучшения производительности моделей при использовании совершенных верификаторов или сильной обратной связи.'}, 'en': {'title': 'Scaling Inference for Smarter Problem Solving in LLMs', 'desc': 'This paper explores how inference-time scaling can improve the reasoning abilities of large language models (LLMs) when tackling complex problems. It examines the effectiveness of extending generated scratchpads for various tasks, including math reasoning and navigation, across nine advanced models. The study finds that while scaling can enhance performance, its benefits vary by task and may decrease with increased complexity. Additionally, the research indicates that simply increasing the number of tokens does not guarantee better accuracy, but using strong feedback mechanisms can lead to significant performance improvements.'}, 'zh': {'title': '推理时间扩展：提升模型推理能力的关键', 'desc': '本研究探讨了推理时间扩展对大型语言模型（LLMs）在复杂问题上的推理能力的影响。我们分析了九种最先进模型在八个具有挑战性的任务上的表现，包括数学推理和空间推理等。结果表明，推理时间扩展的优势因任务而异，且在问题复杂性增加时会减弱。尽管使用更多的标记并不总能提高准确性，但在有强反馈的情况下，所有模型都显示出显著的性能提升潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.23361', 'title': 'Discovering Knowledge Deficiencies of Language Models on Massive\n  Knowledge Base', 'url': 'https://huggingface.co/papers/2503.23361', 'abstract': "Large language models (LLMs) possess impressive linguistic capabilities but often fail to faithfully retain factual knowledge, leading to hallucinations and unreliable outputs. Understanding LLMs' knowledge deficiencies by exhaustively evaluating against full-scale knowledge bases is computationally prohibitive, especially for closed-weight models. We propose stochastic error ascent (SEA), a scalable and efficient framework for discovering knowledge deficiencies (errors) in closed-weight LLMs under a strict query budget. Rather than naively probing all knowledge candidates, SEA formulates error discovery as a stochastic optimization process: it iteratively retrieves new high-error candidates by leveraging the semantic similarity to previously observed failures. To further enhance search efficiency and coverage, SEA employs hierarchical retrieval across document and paragraph levels, and constructs a relation directed acyclic graph to model error propagation and identify systematic failure modes. Empirically, SEA uncovers 40.7x more knowledge errors than Automated Capability Discovery and 26.7% more than AutoBencher, while reducing the cost-per-error by 599x and 9x, respectively. Human evaluation confirms the high quality of generated questions, while ablation and convergence analyses validate the contribution of each component in SEA. Further analysis on the discovered errors reveals correlated failure patterns across LLM families and recurring deficits, highlighting the need for better data coverage and targeted fine-tuning in future LLM development.", 'score': 3, 'issue_id': 3017, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 марта', 'en': 'March 30', 'zh': '3月30日'}, 'hash': '9b957c49c958aea3', 'authors': ['Linxin Song', 'Xuwei Ding', 'Jieyu Zhang', 'Taiwei Shi', 'Ryotaro Shimizu', 'Rahul Gupta', 'Yang Liu', 'Jian Kang', 'Jieyu Zhao'], 'affiliations': ['AGI', 'Amazon', 'University of Rochester', 'University of Southern California', 'University of Washington', 'University of Wisconsin-Madison', 'ZOZO Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.23361.jpg', 'data': {'categories': ['#training', '#hallucinations', '#optimization', '#graphs', '#benchmark', '#data'], 'emoji': '🔍', 'ru': {'title': 'SEA: Эффективный поиск пробелов в знаниях языковых моделей', 'desc': 'Эта статья представляет новый метод под названием SEA (стохастический подъем ошибок) для эффективного обнаружения пробелов в знаниях крупных языковых моделей (LLM). SEA использует итеративный процесс оптимизации, чтобы находить новые кандидаты на ошибки, основываясь на семантическом сходстве с ранее обнаруженными ошибками. Метод применяет иерархический поиск и построение графа отношений для выявления систематических ошибок. Эмпирические результаты показывают, что SEA значительно превосходит существующие методы по эффективности обнаружения ошибок в LLM.'}, 'en': {'title': 'Uncovering Knowledge Deficiencies in LLMs Efficiently with SEA', 'desc': 'This paper introduces a new method called Stochastic Error Ascent (SEA) to identify knowledge deficiencies in large language models (LLMs) that often produce unreliable outputs. SEA efficiently discovers errors by using a stochastic optimization approach, focusing on high-error candidates based on their similarity to previously identified failures. The framework enhances its search capabilities through hierarchical retrieval and a directed acyclic graph to track error propagation. The results show that SEA significantly outperforms existing methods in uncovering knowledge errors while drastically reducing the cost of error discovery.'}, 'zh': {'title': '发现LLM知识缺陷的新方法', 'desc': '大型语言模型（LLMs）在语言能力上表现出色，但常常无法准确保留事实知识，导致幻觉和不可靠的输出。我们提出了一种名为随机错误上升（SEA）的框架，用于在严格的查询预算下发现闭合权重LLMs中的知识缺陷。SEA通过利用与先前观察到的失败的语义相似性，迭代检索新的高错误候选项，从而将错误发现过程形式化为随机优化过程。实验证明，SEA发现的知识错误数量显著高于现有方法，同时大幅降低了每个错误的成本。'}}}, {'id': 'https://huggingface.co/papers/2504.00927', 'title': 'Multi-Token Attention', 'url': 'https://huggingface.co/papers/2504.00927', 'abstract': 'Soft attention is a critical mechanism powering LLMs to locate relevant parts within a given context. However, individual attention weights are determined by the similarity of only a single query and key token vector. This "single token attention" bottlenecks the amount of information used in distinguishing a relevant part from the rest of the context. To address this issue, we propose a new attention method, Multi-Token Attention (MTA), which allows LLMs to condition their attention weights on multiple query and key vectors simultaneously. This is achieved by applying convolution operations over queries, keys and heads, allowing nearby queries and keys to affect each other\'s attention weights for more precise attention. As a result, our method can locate relevant context using richer, more nuanced information that can exceed a single vector\'s capacity. Through extensive evaluations, we demonstrate that MTA achieves enhanced performance on a range of popular benchmarks. Notably, it outperforms Transformer baseline models on standard language modeling tasks, and on tasks that require searching for information within long contexts, where our method\'s ability to leverage richer information proves particularly beneficial.', 'score': 2, 'issue_id': 3020, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '2af4c7adceecde31', 'authors': ['Olga Golovneva', 'Tianlu Wang', 'Jason Weston', 'Sainbayar Sukhbaatar'], 'affiliations': ['FAIR at Meta'], 'pdf_title_img': 'assets/pdf/title_img/2504.00927.jpg', 'data': {'categories': ['#long_context', '#architecture', '#benchmark', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Многотокенное внимание: новый шаг в повышении точности языковых моделей', 'desc': 'Статья представляет новый метод внимания для языковых моделей - Multi-Token Attention (MTA). В отличие от стандартного механизма soft attention, MTA позволяет учитывать информацию из нескольких токенов запроса и ключа одновременно. Это достигается с помощью операций свертки над запросами, ключами и головами внимания. MTA демонстрирует улучшенную производительность на ряде популярных бенчмарков, особенно в задачах, требующих поиска информации в длинных контекстах.'}, 'en': {'title': 'Unlocking Richer Context with Multi-Token Attention', 'desc': 'This paper introduces Multi-Token Attention (MTA), a novel attention mechanism designed to improve the performance of large language models (LLMs). Unlike traditional single token attention, which relies on a single query and key token vector, MTA allows for the simultaneous consideration of multiple query and key vectors. By utilizing convolution operations, MTA enhances the interaction between nearby queries and keys, leading to more accurate attention weights. The results show that MTA significantly outperforms standard Transformer models, especially in tasks involving long contexts and information retrieval.'}, 'zh': {'title': '多令牌注意力：提升上下文理解的关键', 'desc': '软注意力机制是大型语言模型（LLMs）中一个重要的组成部分，用于在给定上下文中定位相关部分。然而，传统的单个令牌注意力方法仅依赖于单个查询和键向量的相似性，这限制了信息的使用。为了解决这个问题，我们提出了一种新的注意力方法——多令牌注意力（MTA），它允许LLMs同时基于多个查询和键向量来调整注意力权重。通过对查询、键和头部应用卷积操作，我们的方法能够利用更丰富的信息，从而在长上下文中更准确地定位相关内容。'}}}, {'id': 'https://huggingface.co/papers/2504.00869', 'title': 'm1: Unleash the Potential of Test-Time Scaling for Medical Reasoning\n  with Large Language Models', 'url': 'https://huggingface.co/papers/2504.00869', 'abstract': "Test-time scaling has emerged as a powerful technique for enhancing the reasoning capabilities of large language models. However, its effectiveness in medical reasoning remains uncertain, as the medical domain fundamentally differs from mathematical tasks in terms of knowledge representation and decision-making processes. In this paper, we provide the first comprehensive investigation of test-time scaling for medical reasoning and present m1, a simple yet effective approach that increases a model's medical reasoning capability at inference. Our evaluation across diverse medical tasks demonstrates that test-time scaling consistently enhances medical reasoning, enabling lightweight fine-tuned models under 10B parameters to establish new state-of-the-art performance, while our 32B model rivals previous 70B-scale medical LLMs. However, we identify an optimal reasoning token budget of approximately 4K, beyond which performance may degrade due to overthinking. Budget forcing, which extends test-time computation through iterative prompts, helps models double-check answers but does not necessarily improve the overall medical QA performance and, in some cases, even introduces errors into previously correct responses. Our case-by-case analysis identifies insufficient medical knowledge as a key bottleneck that prevents further performance gains through test-time scaling. We find that increasing data scale, improving data quality, and expanding model capacity consistently enhance medical knowledge grounding, enabling continued performance improvements, particularly on challenging medical benchmarks where smaller models reach saturation. These findings underscore fundamental differences between medical and mathematical reasoning in LLMs, highlighting that enriched medical knowledge, other than increased reasoning depth alone, is essential for realizing the benefits of test-time scaling.", 'score': 2, 'issue_id': 3019, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'bd9586b08ce02a05', 'authors': ['Xiaoke Huang', 'Juncheng Wu', 'Hui Liu', 'Xianfeng Tang', 'Yuyin Zhou'], 'affiliations': ['Amazon Research', 'UC Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2504.00869.jpg', 'data': {'categories': ['#reasoning', '#healthcare', '#science', '#training', '#inference'], 'emoji': '🩺', 'ru': {'title': 'Медицинские знания важнее глубины рассуждений', 'desc': 'В статье исследуется применение техники масштабирования во время тестирования для улучшения медицинских рассуждений в больших языковых моделях. Авторы представляют метод m1, который повышает способность модели к медицинским рассуждениям на этапе вывода. Исследование показывает, что масштабирование во время тестирования улучшает результаты на медицинских задачах, но выявляет, что недостаток медицинских знаний ограничивает дальнейший прогресс. Для достижения лучших результатов необходимо увеличивать объем данных, улучшать их качество и расширять возможности модели.'}, 'en': {'title': 'Enhancing Medical Reasoning with Test-Time Scaling', 'desc': "This paper explores the use of test-time scaling to improve the reasoning abilities of large language models specifically in the medical field. The authors introduce a method called m1, which enhances medical reasoning during inference, showing that smaller models can achieve state-of-the-art results. They find that while increasing the reasoning token budget can help, it may also lead to performance degradation if overused. The study emphasizes that having rich medical knowledge is crucial for effective reasoning, rather than just increasing the model's complexity or depth of reasoning."}, 'zh': {'title': '医学推理的新突破：测试时缩放的力量', 'desc': '本文探讨了测试时缩放技术在医学推理中的应用，提出了一种名为m1的方法，能够有效提升模型在推理时的医学能力。研究表明，测试时缩放在多种医学任务中均能显著提高推理效果，尤其是对于参数少于10B的轻量级微调模型，能够达到新的最佳性能。我们发现，最佳的推理令牌预算约为4K，超出此范围可能导致性能下降。此外，增加数据规模、提高数据质量和扩展模型容量是提升医学知识基础的关键，尤其是在小模型性能饱和的情况下。'}}}, {'id': 'https://huggingface.co/papers/2503.23733', 'title': 'AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models\n  with Unsupervised Coefficient Optimization', 'url': 'https://huggingface.co/papers/2503.23733', 'abstract': 'Recently, model merging methods have demonstrated powerful strengths in combining abilities on various tasks from multiple Large Language Models (LLMs). While previous model merging methods mainly focus on merging homogeneous models with identical architecture, they meet challenges when dealing with Multimodal Large Language Models (MLLMs) with inherent heterogeneous property, including differences in model architecture and the asymmetry in the parameter space. In this work, we propose AdaMMS, a novel model merging method tailored for heterogeneous MLLMs. Our method tackles the challenges in three steps: mapping, merging and searching. Specifically, we first design mapping function between models to apply model merging on MLLMs with different architecture. Then we apply linear interpolation on model weights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in the hyper-parameter searching step, we propose an unsupervised hyper-parameter selection method for model merging. As the first model merging method capable of merging heterogeneous MLLMs without labeled data, extensive experiments on various model combinations demonstrated that AdaMMS outperforms previous model merging methods on various vision-language benchmarks.', 'score': 2, 'issue_id': 3017, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'ed45063868071c13', 'authors': ['Yiyang Du', 'Xiaochen Wang', 'Chi Chen', 'Jiabo Ye', 'Yiru Wang', 'Peng Li', 'Ming Yan', 'Ji Zhang', 'Fei Huang', 'Zhifang Sui', 'Maosong Sun', 'Yang Liu'], 'affiliations': ['Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China', 'Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China', 'Institute of Intelligent Computing, Alibaba Group', 'Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China', 'ModelTC Open Source Organization, Beijing, China', 'School of Software Microelectronics, Peking University, Beijing, China', 'Shanghai Artificial Intelligence Laboratory, Shanghai, China', 'State Key Laboratory of Multimedia Information Processing, Peking University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.23733.jpg', 'data': {'categories': ['#training', '#architecture', '#transfer_learning', '#optimization', '#multimodal', '#benchmark'], 'emoji': '🔀', 'ru': {'title': 'AdaMMS: Эффективное слияние разнородных мультимодальных языковых моделей', 'desc': 'Статья представляет AdaMMS - новый метод объединения мультимодальных языковых моделей (MLLM) с разнородной архитектурой. Метод включает три этапа: отображение параметров между моделями, их слияние с помощью линейной интерполяции и поиск оптимальных гиперпараметров. AdaMMS решает проблемы, связанные с различиями в архитектуре и асимметрией в пространстве параметров разнородных MLLM. Эксперименты показали превосходство AdaMMS над существующими методами объединения моделей на различных мультимодальных задачах.'}, 'en': {'title': 'Merging Diverse Models with AdaMMS', 'desc': 'This paper introduces AdaMMS, a new method for merging Multimodal Large Language Models (MLLMs) that have different architectures. Traditional merging techniques struggle with these heterogeneous models due to their varying structures and parameter spaces. AdaMMS addresses this by first mapping the models, then merging their weights through linear interpolation, and finally optimizing hyper-parameters using an unsupervised approach. The results show that AdaMMS significantly improves performance on vision-language tasks compared to earlier methods.'}, 'zh': {'title': '异质模型合并的新突破', 'desc': '最近，模型合并方法在结合多个大型语言模型（LLMs）在不同任务上的能力方面表现出强大的优势。以往的模型合并方法主要集中在合并具有相同架构的同质模型，但在处理具有固有异质性的多模态大型语言模型（MLLMs）时面临挑战。我们提出了AdaMMS，这是一种专为异质MLLMs设计的新型模型合并方法，采用映射、合并和搜索三个步骤来解决这些挑战。通过设计模型之间的映射函数、对模型权重进行线性插值以及提出无监督的超参数选择方法，AdaMMS在各种视觉-语言基准测试中超越了以往的模型合并方法。'}}}, {'id': 'https://huggingface.co/papers/2503.21860', 'title': 'ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via\n  Residual Learning', 'url': 'https://huggingface.co/papers/2503.21860', 'abstract': 'Human hands play a central role in interacting, motivating increasing research in dexterous robotic manipulation. Data-driven embodied AI algorithms demand precise, large-scale, human-like manipulation sequences, which are challenging to obtain with conventional reinforcement learning or real-world teleoperation. To address this, we introduce ManipTrans, a novel two-stage method for efficiently transferring human bimanual skills to dexterous robotic hands in simulation. ManipTrans first pre-trains a generalist trajectory imitator to mimic hand motion, then fine-tunes a specific residual module under interaction constraints, enabling efficient learning and accurate execution of complex bimanual tasks. Experiments show that ManipTrans surpasses state-of-the-art methods in success rate, fidelity, and efficiency. Leveraging ManipTrans, we transfer multiple hand-object datasets to robotic hands, creating DexManipNet, a large-scale dataset featuring previously unexplored tasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K episodes of robotic manipulation and is easily extensible, facilitating further policy training for dexterous hands and enabling real-world deployments.', 'score': 2, 'issue_id': 3017, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '0d9ea55946287027', 'authors': ['Kailin Li', 'Puhao Li', 'Tengyu Liu', 'Yuyang Li', 'Siyuan Huang'], 'affiliations': ['Department of Automation, Tsinghua University', 'Institute for Artificial Intelligence, Peking University', 'State Key Laboratory of General Artificial Intelligence, BIGAI'], 'pdf_title_img': 'assets/pdf/title_img/2503.21860.jpg', 'data': {'categories': ['#training', '#dataset', '#robotics', '#transfer_learning', '#optimization', '#agents'], 'emoji': '🤖', 'ru': {'title': 'ManipTrans: эффективная передача человеческих навыков манипуляции роботам', 'desc': 'ManipTrans - это новый метод передачи навыков бимануальной манипуляции от человека к роботизированным рукам в симуляции. Он включает предварительное обучение имитатора траектории движения рук и дообучение специфического остаточного модуля с учетом ограничений взаимодействия. Метод превосходит существующие подходы по точности и эффективности выполнения сложных задач манипуляции. На его основе создан крупномасштабный датасет DexManipNet с 3.3 тыс. эпизодов роботизированной манипуляции для обучения политик управления ловкими руками.'}, 'en': {'title': 'Efficiently Teaching Robots to Manipulate Like Humans', 'desc': 'This paper presents ManipTrans, a two-stage method designed to transfer human bimanual manipulation skills to robotic hands in a simulated environment. The first stage involves pre-training a trajectory imitator that learns to replicate human hand movements, while the second stage fine-tunes a residual module to enhance performance under specific interaction constraints. This approach allows for efficient learning and execution of complex tasks, outperforming existing methods in terms of success rate and efficiency. Additionally, the authors introduce DexManipNet, a comprehensive dataset that includes diverse manipulation tasks, paving the way for improved policy training and real-world applications of dexterous robotic hands.'}, 'zh': {'title': '高效转移人类双手技能的机器人手', 'desc': '本论文介绍了一种名为ManipTrans的新方法，用于将人类双手的技能高效地转移到灵巧的机器人手上。该方法分为两个阶段：首先训练一个通用的轨迹模仿器来模拟手部动作，然后在交互约束下微调特定的残差模块，从而实现复杂双手任务的高效学习和准确执行。实验结果表明，ManipTrans在成功率、保真度和效率上超越了现有的最先进方法。此外，利用ManipTrans，我们创建了一个名为DexManipNet的大规模数据集，包含了3.3K个机器人操作的实例，支持进一步的策略训练和实际应用。'}}}, {'id': 'https://huggingface.co/papers/2504.01017', 'title': 'Scaling Language-Free Visual Representation Learning', 'url': 'https://huggingface.co/papers/2504.01017', 'abstract': 'Visual Self-Supervised Learning (SSL) currently underperforms Contrastive Language-Image Pretraining (CLIP) in multimodal settings such as Visual Question Answering (VQA). This multimodal gap is often attributed to the semantics introduced by language supervision, even though visual SSL and CLIP models are often trained on different data. In this work, we ask the question: "Do visual self-supervised approaches lag behind CLIP due to the lack of language supervision, or differences in the training data?" We study this question by training both visual SSL and CLIP models on the same MetaCLIP data, and leveraging VQA as a diverse testbed for vision encoders. In this controlled setup, visual SSL models scale better than CLIP models in terms of data and model capacity, and visual SSL performance does not saturate even after scaling up to 7B parameters. Consequently, we observe visual SSL methods achieve CLIP-level performance on a wide range of VQA and classic vision benchmarks. These findings demonstrate that pure visual SSL can match language-supervised visual pretraining at scale, opening new opportunities for vision-centric representation learning.', 'score': 1, 'issue_id': 3023, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '9ab970f68b26c2ea', 'authors': ['David Fan', 'Shengbang Tong', 'Jiachen Zhu', 'Koustuv Sinha', 'Zhuang Liu', 'Xinlei Chen', 'Michael Rabbat', 'Nicolas Ballas', 'Yann LeCun', 'Amir Bar', 'Saining Xie'], 'affiliations': ['FAIR, Meta', 'New York University', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2504.01017.jpg', 'data': {'categories': ['#cv', '#training', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Визуальное самообучение не уступает языковому контролю при масштабировании', 'desc': 'Исследование сравнивает методы визуального самоконтролируемого обучения (SSL) и контрастивного обучения на языке и изображениях (CLIP) в задачах мультимодального анализа. Авторы обнаружили, что при обучении на одинаковых данных, модели SSL масштабируются лучше и достигают производительности CLIP на различных задачах визуального анализа вопросов и ответов (VQA). Результаты показывают, что чисто визуальное SSL может соответствовать обучению с языковым контролем при увеличении масштаба. Это открывает новые возможности для обучения визуальных представлений без использования языковой разметки.'}, 'en': {'title': 'Visual SSL: Bridging the Gap with Scale and Data', 'desc': 'This paper investigates the performance gap between Visual Self-Supervised Learning (SSL) and Contrastive Language-Image Pretraining (CLIP) in multimodal tasks like Visual Question Answering (VQA). The authors explore whether this gap is due to the absence of language supervision in visual SSL or differences in the training datasets used. By training both types of models on the same MetaCLIP data, they find that visual SSL models can outperform CLIP models when scaled up, achieving comparable performance on various benchmarks. This suggests that visual SSL can effectively compete with language-supervised methods, highlighting its potential for advancing vision-centric representation learning.'}, 'zh': {'title': '视觉自监督学习的潜力与CLIP相媲美', 'desc': '本研究探讨了视觉自监督学习（SSL）与对比语言-图像预训练（CLIP）在多模态任务中的表现差异。我们通过在相同的MetaCLIP数据上训练视觉SSL和CLIP模型，来分析语言监督的缺乏是否是导致视觉SSL落后的原因。结果显示，视觉SSL模型在数据和模型容量方面的扩展性优于CLIP模型，并且在参数达到70亿时性能仍未饱和。我们的发现表明，纯视觉SSL在大规模下可以达到与语言监督视觉预训练相当的性能，为视觉中心的表示学习开辟了新的机会。'}}}, {'id': 'https://huggingface.co/papers/2504.00698', 'title': 'Command A: An Enterprise-Ready Large Language Model', 'url': 'https://huggingface.co/papers/2504.00698', 'abstract': 'In this report we describe the development of Command A, a powerful large language model purpose-built to excel at real-world enterprise use cases. Command A is an agent-optimised and multilingual-capable model, with support for 23 languages of global business, and a novel hybrid architecture balancing efficiency with top of the range performance. It offers best-in-class Retrieval Augmented Generation (RAG) capabilities with grounding and tool use to automate sophisticated business processes. These abilities are achieved through a decentralised training approach, including self-refinement algorithms and model merging techniques. We also include results for Command R7B which shares capability and architectural similarities to Command A. Weights for both models have been released for research purposes. This technical report details our original training pipeline and presents an extensive evaluation of our models across a suite of enterprise-relevant tasks and public benchmarks, demonstrating excellent performance and efficiency.', 'score': 1, 'issue_id': 3020, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '8670e6d1cc4f6bee', 'authors': ['Team Cohere', 'Aakanksha', 'Arash Ahmadian', 'Marwan Ahmed', 'Jay Alammar', 'Yazeed Alnumay', 'Sophia Althammer', 'Arkady Arkhangorodsky', 'Viraat Aryabumi', 'Dennis Aumiller', 'Raphaël Avalos', 'Zahara Aviv', 'Sammie Bae', 'Saurabh Baji', 'Alexandre Barbet', 'Max Bartolo', 'Björn Bebensee', 'Neeral Beladia', 'Walter Beller-Morales', 'Alexandre Bérard', 'Andrew Berneshawi', 'Anna Bialas', 'Phil Blunsom', 'Matt Bobkin', 'Adi Bongale', 'Sam Braun', 'Maxime Brunet', 'Samuel Cahyawijaya', 'David Cairuz', 'Jon Ander Campos', 'Cassie Cao', 'Kris Cao', 'Roman Castagné', 'Julián Cendrero', 'Leila Chan Currie', 'Yash Chandak', 'Diane Chang', 'Giannis Chatziveroglou', 'Hongyu Chen', 'Claire Cheng', 'Alexis Chevalier', 'Justin T. Chiu', 'Eugene Cho', 'Eugene Choi', 'Eujeong Choi', 'Tim Chung', 'Volkan Cirik', 'Ana Cismaru', 'Pierre Clavier', 'Henry Conklin', 'Lucas Crawhall-Stein', 'Devon Crouse', 'Andres Felipe Cruz-Salinas', 'Ben Cyrus', "Daniel D'souza", 'Hugo Dalla-Torre', 'John Dang', 'William Darling', 'Omar Darwiche Domingues', 'Saurabh Dash', 'Antoine Debugne', 'Théo Dehaze', 'Shaan Desai', 'Joan Devassy', 'Rishit Dholakia', 'Kyle Duffy', 'Ali Edalati', 'Ace Eldeib', 'Abdullah Elkady', 'Sarah Elsharkawy', 'Irem Ergün', 'Beyza Ermis', 'Marzieh Fadaee', 'Boyu Fan', 'Lucas Fayoux', 'Yannis Flet-Berliac', 'Nick Frosst', 'Matthias Gallé', 'Wojciech Galuba', 'Utsav Garg', 'Matthieu Geist', 'Mohammad Gheshlaghi Azar', 'Seraphina Goldfarb-Tarrant', 'Tomas Goldsack', 'Aidan Gomez', 'Victor Machado Gonzaga', 'Nithya Govindarajan', 'Manoj Govindassamy', 'Nathan Grinsztajn', 'Nikolas Gritsch', 'Patrick Gu', 'Shangmin Guo', 'Kilian Haefeli', 'Rod Hajjar', 'Tim Hawes', 'Jingyi He', 'Sebastian Hofstätter', 'Sungjin Hong', 'Sara Hooker', 'Tom Hosking', 'Stephanie Howe', 'Eric Hu', 'Renjie Huang', 'Hemant Jain', 'Ritika Jain', 'Nick Jakobi', 'Madeline Jenkins', 'JJ Jordan', 'Dhruti Joshi', 'Jason Jung', 'Trushant Kalyanpur', 'Siddhartha Rao Kamalakara', 'Julia Kedrzycki', 'Gokce Keskin', 'Edward Kim', 'Joon Kim', 'Wei-Yin Ko', 'Tom Kocmi', 'Michael Kozakov', 'Wojciech Kryściński', 'Arnav Kumar Jain', 'Komal Kumar Teru', 'Sander Land', 'Michael Lasby', 'Olivia Lasche', 'Justin Lee', 'Patrick Lewis', 'Jeffrey Li', 'Jonathan Li', 'Hangyu Lin', 'Acyr Locatelli', 'Kevin Luong', 'Raymond Ma', 'Lukas Mach', 'Marina Machado', 'Joanne Magbitang', 'Brenda Malacara Lopez', 'Aryan Mann', 'Kelly Marchisio', 'Olivia Markham', 'Alexandre Matton', 'Alex McKinney', 'Dominic McLoughlin', 'Jozef Mokry', 'Adrien Morisot', 'Autumn Moulder', 'Harry Moynehan', 'Maximilian Mozes', 'Vivek Muppalla', 'Lidiya Murakhovska', 'Hemangani Nagarajan', 'Alekhya Nandula', 'Hisham Nasir', 'Shauna Nehra', 'Josh Netto-Rosen', 'Daniel Ohashi', 'James Owers-Bardsley', 'Jason Ozuzu', 'Dennis Padilla', 'Gloria Park', 'Sam Passaglia', 'Jeremy Pekmez', 'Laura Penstone', 'Aleksandra Piktus', 'Case Ploeg', 'Andrew Poulton', 'Youran Qi', 'Shubha Raghvendra', 'Miguel Ramos', 'Ekagra Ranjan', 'Pierre Richemond', 'Cécile Robert-Michon', 'Aurélien Rodriguez', 'Sudip Roy', 'Laura Ruis', 'Louise Rust', 'Anubhav Sachan', 'Alejandro Salamanca', 'Kailash Karthik Saravanakumar', 'Isha Satyakam', 'Alice Schoenauer Sebag', 'Priyanka Sen', 'Sholeh Sepehri', 'Preethi Seshadri', 'Ye Shen', 'Tom Sherborne', 'Sylvie Chang Shi', 'Sanal Shivaprasad', 'Vladyslav Shmyhlo', 'Anirudh Shrinivason', 'Inna Shteinbuk', 'Amir Shukayev', 'Mathieu Simard', 'Ella Snyder', 'Ava Spataru', 'Victoria Spooner', 'Trisha Starostina', 'Florian Strub', 'Yixuan Su', 'Jimin Sun', 'Dwarak Talupuru', 'Eugene Tarassov', 'Elena Tommasone', 'Jennifer Tracey', 'Billy Trend', 'Evren Tumer', 'Ahmet Üstün', 'Bharat Venkitesh', 'David Venuto', 'Pat Verga', 'Maxime Voisin', 'Alex Wang', 'Donglu Wang', 'Shijian Wang', 'Edmond Wen', 'Naomi White', 'Jesse Willman', 'Marysia Winkels', 'Chen Xia', 'Jessica Xie', 'Minjie Xu', 'Bowen Yang', 'Tan Yi-Chern', 'Ivan Zhang', 'Zhenyu Zhao', 'Zhoujie Zhao'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.00698.jpg', 'data': {'categories': ['#training', '#low_resource', '#agents', '#open_source', '#rag', '#multilingual', '#architecture', '#benchmark'], 'emoji': '🚀', 'ru': {'title': 'Command A: Мощная многоязычная ИИ-модель для бизнеса', 'desc': 'В статье описывается разработка Command A - мощной языковой модели, оптимизированной для корпоративного использования. Модель поддерживает 23 языка и обладает гибридной архитектурой, сочетающей эффективность и высокую производительность. Command A предлагает передовые возможности Retrieval Augmented Generation (RAG) для автоматизации сложных бизнес-процессов. Эти возможности достигаются с помощью децентрализованного обучения, включая алгоритмы самоулучшения и техники объединения моделей.'}, 'en': {'title': 'Empowering Enterprises with Command A: The Future of Language Models', 'desc': 'This paper presents Command A, a large language model designed specifically for enterprise applications. It features a hybrid architecture that optimizes both performance and efficiency, supporting 23 languages to cater to global business needs. The model excels in Retrieval Augmented Generation (RAG), enabling it to automate complex business processes through effective grounding and tool usage. The training process incorporates decentralized methods, including self-refinement and model merging, and the paper also discusses the similar Command R7B model, providing insights into their training and evaluation results.'}, 'zh': {'title': 'Command A：企业应用的强大语言模型', 'desc': '本文介绍了Command A的开发，这是一种专为企业实际应用而设计的大型语言模型。Command A具备多语言能力，支持23种全球商业语言，并采用新颖的混合架构，兼顾效率与高性能。它提供了最佳的检索增强生成（RAG）能力，能够通过工具使用和基础知识支持来自动化复杂的业务流程。我们还展示了与Command A相似的Command R7B模型的结果，并发布了这两个模型的权重以供研究使用。'}}}, {'id': 'https://huggingface.co/papers/2503.23434', 'title': 'Towards Trustworthy GUI Agents: A Survey', 'url': 'https://huggingface.co/papers/2503.23434', 'abstract': 'GUI agents, powered by large foundation models, can interact with digital interfaces, enabling various applications in web automation, mobile navigation, and software testing. However, their increasing autonomy has raised critical concerns about their security, privacy, and safety. This survey examines the trustworthiness of GUI agents in five critical dimensions: security vulnerabilities, reliability in dynamic environments, transparency and explainability, ethical considerations, and evaluation methodologies. We also identify major challenges such as vulnerability to adversarial attacks, cascading failure modes in sequential decision-making, and a lack of realistic evaluation benchmarks. These issues not only hinder real-world deployment but also call for comprehensive mitigation strategies beyond task success. As GUI agents become more widespread, establishing robust safety standards and responsible development practices is essential. This survey provides a foundation for advancing trustworthy GUI agents through systematic understanding and future research.', 'score': 1, 'issue_id': 3024, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 марта', 'en': 'March 30', 'zh': '3月30日'}, 'hash': 'e19e4d94bcea9cb0', 'authors': ['Yucheng Shi', 'Wenhao Yu', 'Wenlin Yao', 'Wenhu Chen', 'Ninghao Liu'], 'affiliations': ['Amazon', 'Tencent AI Seattle Lab', 'University of Georgia', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2503.23434.jpg', 'data': {'categories': ['#ethics', '#security', '#agents', '#survey', '#training', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Доверие к ИИ: обеспечение надежности графических агентов', 'desc': 'Это обзор исследует надежность графических агентов искусственного интеллекта, взаимодействующих с цифровыми интерфейсами. Рассматриваются пять ключевых аспектов: уязвимости безопасности, надежность в динамических средах, прозрачность и объяснимость, этические вопросы и методологии оценки. Выявлены основные проблемы, такие как уязвимость к состязательным атакам и каскадные сбои при последовательном принятии решений. Подчеркивается необходимость разработки надежных стандартов безопасности и ответственных практик разработки для широкого внедрения графических ИИ-агентов.'}, 'en': {'title': 'Ensuring Trust in Autonomous GUI Agents', 'desc': 'This paper surveys the trustworthiness of GUI agents that use large foundation models to interact with digital interfaces. It highlights five critical dimensions of trustworthiness: security vulnerabilities, reliability in changing environments, transparency, ethical considerations, and evaluation methods. The authors discuss significant challenges such as susceptibility to adversarial attacks and the need for realistic evaluation benchmarks. The paper emphasizes the importance of developing robust safety standards and responsible practices as GUI agents become more prevalent.'}, 'zh': {'title': '构建可信赖的GUI代理，保障安全与隐私', 'desc': '本论文探讨了基于大型基础模型的图形用户界面（GUI）代理的信任性问题。我们分析了五个关键维度，包括安全漏洞、动态环境中的可靠性、透明性与可解释性、伦理考量以及评估方法。研究还指出了主要挑战，如对抗性攻击的脆弱性、序列决策中的级联失败模式，以及缺乏现实的评估基准。这些问题不仅阻碍了GUI代理的实际应用，还需要超越任务成功的全面缓解策略。'}}}, {'id': 'https://huggingface.co/papers/2504.00072', 'title': 'Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs', 'url': 'https://huggingface.co/papers/2504.00072', 'abstract': "We address the task of video chaptering, i.e., partitioning a long video timeline into semantic units and generating corresponding chapter titles. While relatively underexplored, automatic chaptering has the potential to enable efficient navigation and content retrieval in long-form videos. In this paper, we achieve strong chaptering performance on hour-long videos by efficiently addressing the problem in the text domain with our 'Chapter-Llama' framework. Specifically, we leverage a pretrained large language model (LLM) with large context window, and feed as input (i) speech transcripts and (ii) captions describing video frames, along with their respective timestamps. Given the inefficiency of exhaustively captioning all frames, we propose a lightweight speech-guided frame selection strategy based on speech transcript content, and experimentally demonstrate remarkable advantages. We train the LLM to output timestamps for the chapter boundaries, as well as free-form chapter titles. This simple yet powerful approach scales to processing one-hour long videos in a single forward pass. Our results demonstrate substantial improvements (e.g., 45.3 vs 26.7 F1 score) over the state of the art on the recent VidChapters-7M benchmark. To promote further research, we release our code and models at our project page.", 'score': 0, 'issue_id': 3021, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '48f5266ddbfa7bca', 'authors': ['Lucas Ventura', 'Antoine Yang', 'Cordelia Schmid', 'Gül Varol'], 'affiliations': ['Google DeepMind', 'Inria, Ecole normale superieure, CNRS, PSL Research University', 'LIGM, Ecole des Ponts, IP Paris, Univ Gustave Eiffel, CNRS'], 'pdf_title_img': 'assets/pdf/title_img/2504.00072.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#open_source', '#video', '#multimodal'], 'emoji': '📽️', 'ru': {'title': 'Эффективное разделение видео на главы с помощью ИИ', 'desc': "Статья представляет новый подход к автоматическому разделению видео на главы с использованием большой языковой модели (LLM). Метод 'Chapter-Llama' обрабатывает транскрипты речи и описания кадров, выбранных на основе содержания речи. LLM обучена определять временные метки границ глав и генерировать их названия. Этот подход значительно превосходит существующие методы на бенчмарке VidChapters-7M, показывая F1-score 45.3 против 26.7."}, 'en': {'title': 'Efficient Video Chaptering with Chapter-Llama', 'desc': "This paper presents a novel approach to video chaptering, which involves dividing long videos into meaningful segments and generating titles for these segments. The authors introduce the 'Chapter-Llama' framework that utilizes a pretrained large language model (LLM) to process speech transcripts and video captions efficiently. By implementing a speech-guided frame selection strategy, they enhance the model's performance while reducing the need for extensive captioning. The results show significant improvements in chaptering accuracy, achieving a notable F1 score on the VidChapters-7M benchmark, and the authors provide their code and models for further research."}, 'zh': {'title': '高效视频章节划分的新方法', 'desc': "本文探讨了视频章节划分的任务，即将长视频时间线划分为语义单元并生成相应的章节标题。我们提出了'Chapter-Llama'框架，通过高效处理文本领域的问题，实现了对长达一小时视频的强大章节划分性能。该方法利用了预训练的大型语言模型（LLM），输入包括语音转录文本和描述视频帧的字幕，以及它们各自的时间戳。我们还提出了一种基于语音转录内容的轻量级帧选择策略，显著提高了章节划分的效率和准确性。"}}}, {'id': 'https://huggingface.co/papers/2503.24210', 'title': 'DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D\n  Gaussian Splatting', 'url': 'https://huggingface.co/papers/2503.24210', 'abstract': 'Reconstructing sharp 3D representations from blurry multi-view images are long-standing problem in computer vision. Recent works attempt to enhance high-quality novel view synthesis from the motion blur by leveraging event-based cameras, benefiting from high dynamic range and microsecond temporal resolution. However, they often reach sub-optimal visual quality in either restoring inaccurate color or losing fine-grained details. In this paper, we present DiET-GS, a diffusion prior and event stream-assisted motion deblurring 3DGS. Our framework effectively leverages both blur-free event streams and diffusion prior in a two-stage training strategy. Specifically, we introduce the novel framework to constraint 3DGS with event double integral, achieving both accurate color and well-defined details. Additionally, we propose a simple technique to leverage diffusion prior to further enhance the edge details. Qualitative and quantitative results on both synthetic and real-world data demonstrate that our DiET-GS is capable of producing significantly better quality of novel views compared to the existing baselines. Our project page is https://diet-gs.github.io', 'score': 0, 'issue_id': 3023, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'df1e0752d5790146', 'authors': ['Seungjun Lee', 'Gim Hee Lee'], 'affiliations': ['Department of Computer Science, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.24210.jpg', 'data': {'categories': ['#synthetic', '#3d', '#cv', '#diffusion'], 'emoji': '🔍', 'ru': {'title': 'Четкое 3D из размытого 2D: DiET-GS раскрывает детали', 'desc': 'Статья представляет DiET-GS - новый метод для реконструкции четких 3D-представлений из размытых многоракурсных изображений. Авторы используют событийные камеры и диффузионные модели для улучшения качества синтеза новых ракурсов. Ключевая идея заключается в ограничении 3DGS с помощью двойного интеграла событий, что позволяет достичь точной цветопередачи и хорошо определенных деталей. Результаты экспериментов показывают значительное превосходство DiET-GS над существующими методами.'}, 'en': {'title': 'Enhancing 3D Image Clarity with DiET-GS', 'desc': 'This paper addresses the challenge of creating clear 3D images from blurry multi-view pictures, a common issue in computer vision. The authors introduce DiET-GS, a new framework that combines event-based camera data with a diffusion prior to improve the quality of 3D image synthesis. By using a two-stage training approach, the framework effectively restores accurate colors and fine details in the images. The results show that DiET-GS outperforms existing methods in generating high-quality novel views from both synthetic and real-world datasets.'}, 'zh': {'title': '清晰三维重建的新方法', 'desc': '本论文提出了一种名为DiET-GS的框架，用于从模糊的多视图图像中重建清晰的三维表示。该方法结合了无模糊事件流和扩散先验，通过两阶段的训练策略来提高图像质量。我们引入了一种新的约束方法，利用事件双重积分来确保颜色准确和细节清晰。此外，我们还提出了一种简单的技术，利用扩散先验进一步增强边缘细节。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (5)', '#agi', '#alignment', '#architecture (6)', '#audio', '#benchmark (14)', '#cv (4)', '#data (2)', '#dataset (5)', '#diffusion (3)', '#ethics (1)', '#games (1)', '#graphs (1)', '#hallucinations (2)', '#healthcare (1)', '#inference (6)', '#interpretability', '#leakage', '#long_context (3)', '#low_resource (1)', '#machine_translation', '#math (2)', '#multilingual (1)', '#multimodal (9)', '#open_source (3)', '#optimization (10)', '#plp (1)', '#rag (1)', '#reasoning (10)', '#rl (3)', '#rlhf', '#robotics (1)', '#science (1)', '#security (1)', '#small_models', '#story_generation', '#survey (2)', '#synthetic (1)', '#training (15)', '#transfer_learning (2)', '#video (5)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-04-02 09:11',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-04-02 09:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-04-02 09:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    