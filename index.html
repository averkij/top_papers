
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 21 papers. March 12.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">12 марта</span> | <span id="title-articles-count">21 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-03-11.html">⬅️ <span id="prev-date">11.03</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-03-13.html">➡️ <span id="next-date">13.03</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-03.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'};
        let feedDateNext = {'ru': '13.03', 'en': '03/13', 'zh': '3月13日'};
        let feedDatePrev = {'ru': '11.03', 'en': '03/11', 'zh': '3月11日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.08638', 'title': 'YuE: Scaling Open Foundation Models for Long-Form Music Generation', 'url': 'https://huggingface.co/papers/2503.08638', 'abstract': "We tackle the task of long-form music generation--particularly the challenging lyrics-to-song problem--by introducing YuE, a family of open foundation models based on the LLaMA2 architecture. Specifically, YuE scales to trillions of tokens and generates up to five minutes of music while maintaining lyrical alignment, coherent musical structure, and engaging vocal melodies with appropriate accompaniment. It achieves this through (1) track-decoupled next-token prediction to overcome dense mixture signals, (2) structural progressive conditioning for long-context lyrical alignment, and (3) a multitask, multiphase pre-training recipe to converge and generalize. In addition, we redesign the in-context learning technique for music generation, enabling versatile style transfer (e.g., converting Japanese city pop into an English rap while preserving the original accompaniment) and bidirectional generation. Through extensive evaluation, we demonstrate that YuE matches or even surpasses some of the proprietary systems in musicality and vocal agility. In addition, fine-tuning YuE enables additional controls and enhanced support for tail languages. Furthermore, beyond generation, we show that YuE's learned representations can perform well on music understanding tasks, where the results of YuE match or exceed state-of-the-art methods on the MARBLE benchmark. Keywords: lyrics2song, song generation, long-form, foundation model, music generation", 'score': 31, 'issue_id': 2655, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': 'eb1539380bf435c9', 'authors': ['Ruibin Yuan', 'Hanfeng Lin', 'Shuyue Guo', 'Ge Zhang', 'Jiahao Pan', 'Yongyi Zang', 'Haohe Liu', 'Yiming Liang', 'Wenye Ma', 'Xingjian Du', 'Xinrun Du', 'Zhen Ye', 'Tianyu Zheng', 'Yinghao Ma', 'Minghao Liu', 'Zeyue Tian', 'Ziya Zhou', 'Liumeng Xue', 'Xingwei Qu', 'Yizhi Li', 'Shangda Wu', 'Tianhao Shen', 'Ziyang Ma', 'Jun Zhan', 'Chunhui Wang', 'Yatian Wang', 'Xiaowei Chi', 'Xinyue Zhang', 'Zhenzhu Yang', 'Xiangzhou Wang', 'Shansong Liu', 'Lingrui Mei', 'Peng Li', 'Junjie Wang', 'Jianwei Yu', 'Guojian Pang', 'Xu Li', 'Zihao Wang', 'Xiaohuan Zhou', 'Lijun Yu', 'Emmanouil Benetos', 'Yong Chen', 'Chenghua Lin', 'Xie Chen', 'Gus Xia', 'Zhaoxiang Zhang', 'Chao Zhang', 'Wenhu Chen', 'Xinyu Zhou', 'Xipeng Qiu', 'Roger Dannenberg', 'Jiaheng Liu', 'Jian Yang', 'Wenhao Huang', 'Wei Xue', 'Xu Tan', 'Yike Guo'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.08638.jpg', 'data': {'categories': ['#training', '#multimodal', '#long_context', '#benchmark', '#low_resource', '#open_source', '#story_generation', '#audio'], 'emoji': '🎵', 'ru': {'title': 'YuE: Открытая модель для генерации длинных музыкальных композиций на основе текста', 'desc': 'YuE - это семейство открытых моделей для генерации музыки на основе архитектуры LLaMA2. Модель способна генерировать до пяти минут музыки, сохраняя согласованность с текстом песни, музыкальную структуру и вокальные мелодии с аккомпанементом. YuE использует несколько инновационных техник, включая предсказание следующего токена с разделением треков и структурное прогрессивное кондиционирование. Модель также может выполнять перенос стиля и двунаправленную генерацию, а её представления эффективны для задач понимания музыки.'}, 'en': {'title': 'Transforming Lyrics into Melodies with YuE!', 'desc': 'This paper presents YuE, a new family of open foundation models designed for long-form music generation, specifically focusing on the lyrics-to-song challenge. YuE utilizes the LLaMA2 architecture and can generate up to five minutes of music while ensuring that the lyrics align with the musical structure and melodies. Key innovations include track-decoupled next-token prediction for better signal clarity, structural progressive conditioning for lyrical alignment, and a multitask pre-training approach to enhance generalization. The model also supports versatile style transfer and performs well on music understanding tasks, achieving results that rival existing proprietary systems.'}, 'zh': {'title': 'YuE：歌词转歌曲的音乐生成新突破', 'desc': '本文介绍了一种名为YuE的开放基础模型，专注于长篇音乐生成，特别是歌词转歌曲的问题。YuE基于LLaMA2架构，能够生成长达五分钟的音乐，同时保持歌词的对齐、连贯的音乐结构和引人入胜的旋律。通过采用解耦的下一个标记预测、结构性渐进条件和多任务预训练等技术，YuE在音乐生成中表现出色。经过评估，YuE在音乐性和声乐灵活性方面与一些专有系统相匹敌，甚至超越了它们。'}}}, {'id': 'https://huggingface.co/papers/2503.07920', 'title': 'Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural\n  Vision-Language Dataset for Southeast Asia', 'url': 'https://huggingface.co/papers/2503.07920', 'abstract': 'Southeast Asia (SEA) is a region of extraordinary linguistic and cultural diversity, yet it remains significantly underrepresented in vision-language (VL) research. This often results in artificial intelligence (AI) models that fail to capture SEA cultural nuances. To fill this gap, we present SEA-VL, an open-source initiative dedicated to developing high-quality, culturally relevant data for SEA languages. By involving contributors from SEA countries, SEA-VL aims to ensure better cultural relevance and diversity, fostering greater inclusivity of underrepresented languages in VL research. Beyond crowdsourcing, our initiative goes one step further in the exploration of the automatic collection of culturally relevant images through crawling and image generation. First, we find that image crawling achieves approximately ~85% cultural relevance while being more cost- and time-efficient than crowdsourcing. Second, despite the substantial progress in generative vision models, synthetic images remain unreliable in accurately reflecting SEA cultures. The generated images often fail to reflect the nuanced traditions and cultural contexts of the region. Collectively, we gather 1.28M SEA culturally-relevant images, more than 50 times larger than other existing datasets. Through SEA-VL, we aim to bridge the representation gap in SEA, fostering the development of more inclusive AI systems that authentically represent diverse cultures across SEA.', 'score': 27, 'issue_id': 2654, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '32c690b6ffb8b143', 'authors': ['Samuel Cahyawijaya', 'Holy Lovenia', 'Joel Ruben Antony Moniz', 'Tack Hwa Wong', 'Mohammad Rifqi Farhansyah', 'Thant Thiri Maung', 'Frederikus Hudi', 'David Anugraha', 'Muhammad Ravi Shulthan Habibi', 'Muhammad Reza Qorib', 'Amit Agarwal', 'Joseph Marvin Imperial', 'Hitesh Laxmichand Patel', 'Vicky Feliren', 'Bahrul Ilmi Nasution', 'Manuel Antonio Rufino', 'Genta Indra Winata', 'Rian Adam Rajagede', 'Carlos Rafael Catalan', 'Mohamed Fazli Imam', 'Priyaranjan Pattnayak', 'Salsabila Zahirah Pranida', 'Kevin Pratama', 'Yeshil Bangera', 'Adisai Na-Thalang', 'Patricia Nicole Monderin', 'Yueqi Song', 'Christian Simon', 'Lynnette Hui Xian Ng', "Richardy Lobo' Sapan", 'Taki Hasan Rafi', 'Bin Wang', 'Supryadi', 'Kanyakorn Veerakanjana', 'Piyalitt Ittichaiwong', 'Matthew Theodore Roque', 'Karissa Vincentio', 'Takdanai Kreangphet', 'Phakphum Artkaew', 'Kadek Hendrawan Palgunadi', 'Yanzhi Yu', 'Rochana Prih Hastuti', 'William Nixon', 'Mithil Bangera', 'Adrian Xuan Wei Lim', 'Aye Hninn Khine', 'Hanif Muhammad Zhafran', 'Teddy Ferdinan', 'Audra Aurora Izzani', 'Ayushman Singh', 'Evan', 'Jauza Akbar Krito', 'Michael Anugraha', 'Fenal Ashokbhai Ilasariya', 'Haochen Li', 'John Amadeo Daniswara', 'Filbert Aurelian Tjiaranata', 'Eryawan Presma Yulianrifat', 'Can Udomcharoenchaikit', 'Fadil Risdian Ansori', 'Mahardika Krisna Ihsani', 'Giang Nguyen', 'Anab Maulana Barik', 'Dan John Velasco', 'Rifo Ahmad Genadi', 'Saptarshi Saha', 'Chengwei Wei', 'Isaiah Flores', 'Kenneth Ko Han Chen', 'Anjela Gail Santos', 'Wan Shen Lim', 'Kaung Si Phyo', 'Tim Santos', 'Meisyarah Dwiastuti', 'Jiayun Luo', 'Jan Christian Blaise Cruz', 'Ming Shan Hee', 'Ikhlasul Akmal Hanif', 'M. Alif Al Hakim', "Muhammad Rizky Sya'ban", 'Kun Kerdthaisong', 'Lester James V. Miranda', 'Fajri Koto', 'Tirana Noor Fatyanosa', 'Alham Fikri Aji', 'Jostin Jerico Rosal', 'Jun Kevin', 'Robert Wijaya', 'Onno P. Kampman', 'Ruochen Zhang', 'Börje F. Karlsson', 'Peerat Limkonchotiwat'], 'affiliations': ['AI Singapore', 'Allen AI', 'Ateneo de Manila University', 'Auburn University', 'Bandung Institute of Technology', 'Beijing Academy of Artificial Intelligence (BAAI)', 'Binus University', 'Brawijaya University', 'Brown University', 'Capital One', 'Carnegie Mellon University', 'Chulalongkorn University', 'Cohere', 'Dataxet:Sonar', 'Faculty of Medicine Siriraj Hospital, Mahidol University', 'Graphcore', 'Hanyang University', 'Independent', 'Indian Statistical Institute, Kolkata', 'IndoNLP', 'Institut Teknologi Sepuluh Nopember', 'Institute for Infocomm Research, Singapore', 'King Mongkuts University of Technology Thonburi', 'MBZUAI', 'MOH Office for Healthcare Transformation', 'Macau University of Science and Technology', 'Meta', 'Mila - Quebec AI Institute', 'Monash University, Indonesia', 'Nara Institute of Science and Technology', 'National University Philippines', 'National University of Singapore', 'New York University', 'Oracle', 'Polytechnique Montreal', 'SCB 10X', 'SEACrowd', 'Samsung R&D Institute Philippines', 'Seoul National University of Science and Technology', 'Singapore Polytechnic', 'Singapore University of Technology and Design', 'Sony Group Corporation', 'Srinakharinwirot University', 'Thammasat University', 'The University of Manchester', 'Tianjin University', 'Ton Duc Thang University', 'Universitas Gadjah Mada', 'Universitas Islam Indonesia', 'Universitas Pelita Harapan', 'University of Bath', 'University of Illiinois, Urbana-Champaign', 'University of Indonesia', 'University of New Haven', 'University of Toronto', 'University of the Philippines', 'Vidyasirimedhi Institute of Science and Technology', 'Works Applications', 'Wrocław Tech'], 'pdf_title_img': 'assets/pdf/title_img/2503.07920.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#open_source', '#data', '#multimodal'], 'emoji': '🌏', 'ru': {'title': 'Преодоление культурного разрыва в ИИ для Юго-Восточной Азии', 'desc': 'Статья представляет SEA-VL - инициативу по созданию качественных данных для языков Юго-Восточной Азии в области vision-language моделей. Проект направлен на улучшение культурной релевантности и разнообразия в исследованиях ИИ. Авторы изучают методы автоматического сбора культурно значимых изображений через краулинг и генерацию изображений. В результате собрано 1,28 млн культурно релевантных изображений, что в 50 раз больше существующих датасетов.'}, 'en': {'title': 'Bridging the Cultural Gap in AI with SEA-VL', 'desc': "The paper introduces SEA-VL, an initiative aimed at enhancing representation of Southeast Asian cultures in vision-language (VL) research. It highlights the creation of a large dataset containing 1.28 million culturally relevant images, which is significantly larger than existing datasets. The initiative combines crowdsourcing with automated image crawling, achieving high cultural relevance efficiently. However, it also notes challenges with generative models, as synthetic images often fail to accurately depict the region's diverse cultural nuances."}, 'zh': {'title': '填补东南亚文化在AI研究中的空白', 'desc': '东南亚地区语言和文化多样性极为丰富，但在视觉语言研究中却严重缺乏代表性。为了解决这一问题，我们提出了SEA-VL，这是一个开放源代码的项目，旨在为东南亚语言开发高质量、文化相关的数据。通过吸引来自东南亚国家的贡献者，SEA-VL确保了更好的文化相关性和多样性，促进了在视觉语言研究中对被低估语言的包容性。我们收集了128万张与东南亚文化相关的图像，远超现有数据集的规模，旨在缩小东南亚的代表性差距，推动更具包容性的人工智能系统的发展。'}}}, {'id': 'https://huggingface.co/papers/2503.08120', 'title': 'UniF^2ace: Fine-grained Face Understanding and Generation\n  with Unified Multimodal Models', 'url': 'https://huggingface.co/papers/2503.08120', 'abstract': "Unified multimodal models (UMMs) have emerged as a powerful paradigm in foundational computer vision research, demonstrating significant potential in both image understanding and generation. However, existing research in the face domain primarily focuses on coarse facial attribute understanding, with limited capacity to handle fine-grained facial attributes and without addressing generation capabilities. To overcome these limitations, we propose UniF^2ace, the first UMM tailored specifically for fine-grained face understanding and generation. In general, we train UniF^2ace on a self-constructed, specialized dataset utilizing two mutually beneficial diffusion techniques and a two-level mixture-of-experts architecture. Specifically, we first build a large-scale facial dataset, UniF^2ace-130K, which contains 130K image-text pairs with one million question-answering pairs that span a wide range of facial attributes. Second, we establish a theoretical connection between discrete diffusion score matching and masked generative models, optimizing both evidence lower bounds simultaneously, which significantly improves the model's ability to synthesize facial details. Finally, we introduce both token-level and sequence-level mixture-of-experts, enabling efficient fine-grained representation learning for both understanding and generation tasks. Extensive experiments on UniF^2ace-130K demonstrate that UniF^2ace outperforms existing UMMs and generative models, achieving superior performance across both understanding and generation tasks.", 'score': 19, 'issue_id': 2654, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '7c6e7c685b283d61', 'authors': ['Junzhe Li', 'Xuerui Qiu', 'Linrui Xu', 'Liya Guo', 'Delin Qu', 'Tingting Long', 'Chun Fan', 'Ming Li'], 'affiliations': ['Central South University', 'Computer Center, Peking University', 'Fudan University', 'Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)', 'Institute of Automation, Chinese Academy of Sciences', 'School of Computer Science, Peking University', 'Yau Mathematical Sciences Center and Department of Mathematical Sciences, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08120.jpg', 'data': {'categories': ['#synthetic', '#cv', '#dataset', '#multimodal', '#architecture', '#diffusion'], 'emoji': '🧑', 'ru': {'title': 'UniF^2ace: Новый уровень в понимании и генерации лиц', 'desc': 'UniF^2ace - это новая унифицированная мультимодальная модель (UMM), специально разработанная для детального понимания и генерации лиц. Модель обучена на большом специализированном наборе данных, содержащем 130 тысяч пар изображений и текстов с миллионом пар вопросов-ответов о различных атрибутах лица. В UniF^2ace применяются две взаимодополняющие техники диффузии и двухуровневая архитектура смеси экспертов для эффективного обучения представлений. Эксперименты показывают, что UniF^2ace превосходит существующие UMM и генеративные модели в задачах понимания и генерации лиц.'}, 'en': {'title': 'UniF^2ace: Mastering Fine-Grained Facial Understanding and Generation', 'desc': 'This paper introduces UniF^2ace, a unified multimodal model designed for fine-grained understanding and generation of facial attributes. Unlike previous models that only focus on coarse attributes, UniF^2ace leverages a large-scale dataset of 130K image-text pairs and one million question-answering pairs to enhance its learning capabilities. The model employs innovative diffusion techniques and a mixture-of-experts architecture to optimize its performance in synthesizing detailed facial features. Experimental results show that UniF^2ace significantly outperforms existing models in both understanding and generating facial attributes.'}, 'zh': {'title': '细粒度面部理解与生成的统一模型', 'desc': '统一多模态模型（UMMs）在计算机视觉研究中展现出强大的潜力，尤其是在图像理解和生成方面。然而，现有的面部领域研究主要集中在粗略的面部属性理解上，缺乏处理细粒度面部属性的能力，并且未能解决生成能力的问题。为了解决这些限制，我们提出了UniF^2ace，这是第一个专门针对细粒度面部理解和生成的UMM。通过构建一个包含13万张图像-文本对的大规模数据集，并采用两种互补的扩散技术和双层专家混合架构，UniF^2ace在理解和生成任务上均表现出色。'}}}, {'id': 'https://huggingface.co/papers/2503.07703', 'title': 'Seedream 2.0: A Native Chinese-English Bilingual Image Generation\n  Foundation Model', 'url': 'https://huggingface.co/papers/2503.07703', 'abstract': 'Rapid advancement of diffusion models has catalyzed remarkable progress in the field of image generation. However, prevalent models such as Flux, SD3.5 and Midjourney, still grapple with issues like model bias, limited text rendering capabilities, and insufficient understanding of Chinese cultural nuances. To address these limitations, we present Seedream 2.0, a native Chinese-English bilingual image generation foundation model that excels across diverse dimensions, which adeptly manages text prompt in both Chinese and English, supporting bilingual image generation and text rendering. We develop a powerful data system that facilitates knowledge integration, and a caption system that balances the accuracy and richness for image description. Particularly, Seedream is integrated with a self-developed bilingual large language model as a text encoder, allowing it to learn native knowledge directly from massive data. This enable it to generate high-fidelity images with accurate cultural nuances and aesthetic expressions described in either Chinese or English. Beside, Glyph-Aligned ByT5 is applied for flexible character-level text rendering, while a Scaled ROPE generalizes well to untrained resolutions. Multi-phase post-training optimizations, including SFT and RLHF iterations, further improve the overall capability. Through extensive experimentation, we demonstrate that Seedream 2.0 achieves state-of-the-art performance across multiple aspects, including prompt-following, aesthetics, text rendering, and structural correctness. Furthermore, Seedream 2.0 has been optimized through multiple RLHF iterations to closely align its output with human preferences, as revealed by its outstanding ELO score. In addition, it can be readily adapted to an instruction-based image editing model, such as SeedEdit, with strong editing capability that balances instruction-following and image consistency.', 'score': 18, 'issue_id': 2654, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '00cd4369f3c531f9', 'authors': ['Lixue Gong', 'Xiaoxia Hou', 'Fanshi Li', 'Liang Li', 'Xiaochen Lian', 'Fei Liu', 'Liyang Liu', 'Wei Liu', 'Wei Lu', 'Yichun Shi', 'Shiqi Sun', 'Yu Tian', 'Zhi Tian', 'Peng Wang', 'Xun Wang', 'Ye Wang', 'Guofeng Wu', 'Jie Wu', 'Xin Xia', 'Xuefeng Xiao', 'Linjie Yang', 'Zhonghua Zhai', 'Xinyu Zhang', 'Qi Zhang', 'Yuwei Zhang', 'Shijia Zhao', 'Jianchao Yang', 'Weilin Huang'], 'affiliations': ['Seed Vision Team, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2503.07703.jpg', 'data': {'categories': ['#cv', '#training', '#dataset', '#optimization', '#rlhf', '#alignment', '#multimodal', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Seedream 2.0: Революция в двуязычной генерации изображений', 'desc': 'Seedream 2.0 - это двуязычная модель генерации изображений, созданная для преодоления ограничений существующих моделей. Она использует мощную систему данных и двуязычную языковую модель для точного понимания культурных нюансов. Модель применяет Glyph-Aligned ByT5 для гибкого рендеринга текста и Scaled ROPE для генерализации на нетренированные разрешения. Многоэтапная оптимизация, включая SFT и RLHF, улучшает общие возможности модели.'}, 'en': {'title': 'Seedream 2.0: Bridging Cultures in Image Generation', 'desc': 'This paper introduces Seedream 2.0, a bilingual image generation model designed to overcome limitations found in existing models like Flux and Midjourney. It effectively handles text prompts in both Chinese and English, allowing for culturally nuanced image generation and accurate text rendering. The model incorporates a bilingual large language model for enhanced understanding and a robust data system for knowledge integration. Extensive optimizations, including reinforcement learning from human feedback, ensure that Seedream 2.0 excels in aesthetics, prompt adherence, and overall image quality.'}, 'zh': {'title': '双语图像生成的未来：Seedream 2.0', 'desc': '本论文介绍了Seedream 2.0，这是一个中英双语的图像生成基础模型，旨在解决现有模型在文本渲染和文化理解方面的不足。该模型能够处理中文和英文的文本提示，支持双语图像生成，并通过强大的数据系统和描述系统提升图像描述的准确性和丰富性。Seedream 2.0结合了自研的双语大语言模型，能够从海量数据中直接学习本土知识，生成高保真度的图像，准确表达文化细节和美学特征。此外，通过多阶段的后训练优化，Seedream 2.0在多个方面实现了最先进的性能，包括遵循提示、审美、文本渲染和结构正确性。'}}}, {'id': 'https://huggingface.co/papers/2503.08625', 'title': 'SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by\n  Imitating Human Annotator Trajectories', 'url': 'https://huggingface.co/papers/2503.08625', 'abstract': "While MLLMs have demonstrated adequate image understanding capabilities, they still struggle with pixel-level comprehension, limiting their practical applications. Current evaluation tasks like VQA and visual grounding remain too coarse to assess fine-grained pixel comprehension accurately. Though segmentation is foundational for pixel-level understanding, existing methods often require MLLMs to generate implicit tokens, decoded through external pixel decoders. This approach disrupts the MLLM's text output space, potentially compromising language capabilities and reducing flexibility and extensibility, while failing to reflect the model's intrinsic pixel-level understanding.   Thus, we introduce the Human-Like Mask Annotation Task (HLMAT), a new paradigm where MLLMs mimic human annotators using interactive segmentation tools. Modeling segmentation as a multi-step Markov Decision Process, HLMAT enables MLLMs to iteratively generate text-based click points, achieving high-quality masks without architectural changes or implicit tokens. Through this setup, we develop SegAgent, a model fine-tuned on human-like annotation trajectories, which achieves performance comparable to state-of-the-art (SOTA) methods and supports additional tasks like mask refinement and annotation filtering.   HLMAT provides a protocol for assessing fine-grained pixel understanding in MLLMs and introduces a vision-centric, multi-step decision-making task that facilitates exploration of MLLMs' visual reasoning abilities. Our adaptations of policy improvement method StaR and PRM-guided tree search further enhance model robustness in complex segmentation tasks, laying a foundation for future advancements in fine-grained visual perception and multi-step decision-making for MLLMs.", 'score': 16, 'issue_id': 2655, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '081b91105002410a', 'authors': ['Muzhi Zhu', 'Yuzhuo Tian', 'Hao Chen', 'Chunluan Zhou', 'Qingpei Guo', 'Yang Liu', 'Ming Yang', 'Chunhua Shen'], 'affiliations': ['Ant Group', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.08625.jpg', 'data': {'categories': ['#agents', '#cv', '#rl', '#reasoning', '#optimization', '#games'], 'emoji': '🖼️', 'ru': {'title': 'Человекоподобная сегментация изображений мультимодальными языковыми моделями', 'desc': 'Статья представляет новый подход к оценке понимания изображений на уровне пикселей для мультимодальных языковых моделей (MLLM). Авторы вводят задачу аннотации масок, подобную человеческой (HLMAT), где MLLM имитируют действия человека-аннотатора. Разработанная модель SegAgent достигает результатов на уровне современных методов сегментации без изменения архитектуры. HLMAT открывает новые возможности для исследования визуального мышления MLLM и улучшения их восприятия изображений.'}, 'en': {'title': 'Enhancing Pixel-Level Understanding in MLLMs with Human-Like Annotation', 'desc': "This paper addresses the limitations of Multimodal Language Models (MLLMs) in understanding images at the pixel level, which restricts their practical use. It critiques existing evaluation methods for being too broad and introduces the Human-Like Mask Annotation Task (HLMAT) to improve pixel comprehension. HLMAT allows MLLMs to simulate human-like annotation through interactive segmentation, modeled as a multi-step Markov Decision Process. The proposed SegAgent model, trained on this new task, achieves competitive performance while maintaining the MLLM's language capabilities and flexibility."}, 'zh': {'title': '提升像素理解的创新标注任务', 'desc': '本论文探讨了多模态大语言模型（MLLMs）在像素级理解方面的不足，尤其是在细粒度评估任务中的局限性。我们提出了一种新的任务——人类样本标注任务（HLMAT），通过模拟人类标注者的方式，使用交互式分割工具来提高模型的像素理解能力。HLMAT将分割建模为多步骤的马尔可夫决策过程，使得MLLMs能够迭代生成基于文本的点击点，从而无需改变模型架构或使用隐式标记。通过这一方法，我们开发了SegAgent模型，其性能与最先进的方法相当，并支持掩膜细化和标注过滤等额外任务。'}}}, {'id': 'https://huggingface.co/papers/2503.07536', 'title': 'LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through\n  Two-Stage Rule-Based RL', 'url': 'https://huggingface.co/papers/2503.07536', 'abstract': 'Enhancing reasoning in Large Multimodal Models (LMMs) faces unique challenges from the complex interplay between visual perception and logical reasoning, particularly in compact 3B-parameter architectures where architectural constraints limit reasoning capacity and modality alignment.   While rule-based reinforcement learning (RL) excels in text-only domains, its multimodal extension confronts two critical barriers: (1) data limitations due to ambiguous answers and scarce complex reasoning examples, and (2) degraded foundational reasoning induced by multimodal pretraining.   To address these challenges, we propose \\method, a two-stage framework adapting rule-based RL for multimodal reasoning through Foundational Reasoning Enhancement (FRE) followed by Multimodal Generalization Training (MGT). The FRE stage first strengthens reasoning abilities using text-only data with rule-based RL, then the MGT stage generalizes these reasoning capabilities to multimodal domains.   Experiments on Qwen2.5-VL-Instruct-3B demonstrate that \\method achieves 4.83\\% and 4.5\\% average improvements over baselines in multimodal and text-only benchmarks, respectively, with a 3.63\\% gain in complex Football Game tasks. These results validate that text-based reasoning enhancement enables effective multimodal generalization, offering a data-efficient paradigm that bypasses costly high-quality multimodal training data.', 'score': 14, 'issue_id': 2654, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '59c304598f64f1e6', 'authors': ['Yingzhe Peng', 'Gongrui Zhang', 'Miaosen Zhang', 'Zhiyuan You', 'Jie Liu', 'Qipeng Zhu', 'Kai Yang', 'Xingzhong Xu', 'Xin Geng', 'Xu Yang'], 'affiliations': ['Ant Group', 'Fudan University', 'Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.07536.jpg', 'data': {'categories': ['#reasoning', '#training', '#rl', '#benchmark', '#small_models', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Усиление рассуждений в мультимодальных моделях через обучение на текстах', 'desc': 'Статья представляет метод улучшения рассуждений в больших мультимодальных моделях (LMM) с архитектурой 3B-параметров. Авторы предлагают двухэтапный подход: сначала усиление базовых способностей рассуждения с помощью обучения с подкреплением на текстовых данных, затем обобщение этих навыков на мультимодальные задачи. Эксперименты на модели Qwen2.5-VL-Instruct-3B показывают значительное улучшение производительности как в мультимодальных, так и в текстовых тестах. Метод позволяет эффективно обучать мультимодальные модели без необходимости в больших объемах качественных мультимодальных данных.'}, 'en': {'title': 'Boosting Reasoning in Multimodal Models Efficiently', 'desc': 'This paper addresses the challenges of improving reasoning in Large Multimodal Models (LMMs), particularly in smaller architectures with limited parameters. It identifies two main issues: the lack of sufficient data for complex reasoning in multimodal contexts and the negative impact of multimodal pretraining on foundational reasoning. The authors propose a two-stage framework that first enhances reasoning using rule-based reinforcement learning on text-only data, followed by training to generalize these skills to multimodal scenarios. Experimental results show significant improvements in reasoning performance across both multimodal and text-only tasks, demonstrating the effectiveness of their approach in reducing the need for extensive multimodal training data.'}, 'zh': {'title': '增强多模态推理能力的高效方法', 'desc': '本论文探讨了在大型多模态模型（LMMs）中增强推理能力的挑战，特别是在参数量为3B的紧凑架构中，视觉感知与逻辑推理之间的复杂互动。我们提出了一种名为\textit{method}的两阶段框架，通过基础推理增强（FRE）和多模态泛化训练（MGT）来适应多模态推理。FRE阶段利用基于规则的强化学习（RL）增强文本数据的推理能力，MGT阶段则将这些推理能力推广到多模态领域。实验结果表明，\textit{method}在多模态和文本基准测试中分别比基线提高了4.83%和4.5%，在复杂的足球比赛任务中提高了3.63%。'}}}, {'id': 'https://huggingface.co/papers/2503.05978', 'title': 'MagicInfinite: Generating Infinite Talking Videos with Your Words and\n  Voice', 'url': 'https://huggingface.co/papers/2503.05978', 'abstract': "We present MagicInfinite, a novel diffusion Transformer (DiT) framework that overcomes traditional portrait animation limitations, delivering high-fidelity results across diverse character types-realistic humans, full-body figures, and stylized anime characters. It supports varied facial poses, including back-facing views, and animates single or multiple characters with input masks for precise speaker designation in multi-character scenes. Our approach tackles key challenges with three innovations: (1) 3D full-attention mechanisms with a sliding window denoising strategy, enabling infinite video generation with temporal coherence and visual quality across diverse character styles; (2) a two-stage curriculum learning scheme, integrating audio for lip sync, text for expressive dynamics, and reference images for <PRE_TAG>identity preservation</POST_TAG>, enabling flexible multi-modal control over long sequences; and (3) region-specific masks with adaptive loss functions to balance global textual control and local audio guidance, supporting speaker-specific animations. Efficiency is enhanced via our innovative unified step and cfg distillation techniques, achieving a 20x inference speed boost over the basemodel: generating a 10 second 540x540p video in 10 seconds or 720x720p in 30 seconds on 8 H100 GPUs, without quality loss. Evaluations on our new benchmark demonstrate MagicInfinite's superiority in audio-lip synchronization, identity preservation, and motion naturalness across diverse scenarios. It is publicly available at https://www.hedra.com/, with examples at https://magicinfinite.github.io/.", 'score': 14, 'issue_id': 2658, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '2111564f7e67ca23', 'authors': ['Hongwei Yi', 'Tian Ye', 'Shitong Shao', 'Xuancheng Yang', 'Jiantong Zhao', 'Hanzhong Guo', 'Terrance Wang', 'Qingyu Yin', 'Zeke Xie', 'Lei Zhu', 'Wei Li', 'Michael Lingelbach', 'Daquan Zhou'], 'affiliations': ['HKU', 'HKUST(GZ)', 'Hedra Inc.', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.05978.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#inference', '#diffusion', '#video', '#open_source'], 'emoji': '🎭', 'ru': {'title': 'MagicInfinite: революция в анимации портретов с помощью ИИ', 'desc': 'MagicInfinite - это новая система на основе диффузионного трансформера для анимации портретов. Она способна создавать высококачественные анимации для разных типов персонажей, включая реалистичных людей и аниме-персонажей. Система использует 3D механизм полного внимания, двухэтапное обучение с учителем и регион-специфичные маски для улучшения качества и контроля анимации. MagicInfinite демонстрирует превосходные результаты в синхронизации губ с аудио, сохранении идентичности и естественности движений.'}, 'en': {'title': 'Revolutionizing Portrait Animation with MagicInfinite', 'desc': 'MagicInfinite is a cutting-edge diffusion Transformer framework designed to enhance portrait animation by producing high-quality results for various character types, including realistic humans and stylized anime. It introduces innovative techniques such as 3D full-attention mechanisms and a sliding window denoising strategy, allowing for infinite video generation while maintaining visual coherence. The framework employs a two-stage curriculum learning approach that integrates audio and text inputs for improved lip synchronization and expressive dynamics, along with region-specific masks for precise control over animations. With a significant boost in efficiency, MagicInfinite can generate high-resolution videos rapidly, outperforming existing models in key areas like audio-lip synchronization and identity preservation.'}, 'zh': {'title': '魔法无限：突破肖像动画的界限', 'desc': 'MagicInfinite是一种新型的扩散变换器框架，旨在克服传统肖像动画的局限性，能够在多种角色类型中提供高保真度的结果，包括真实人类、全身人物和风格化的动漫角色。该框架支持多种面部姿势的动画，包括背面视图，并能够通过输入掩码精确指定多角色场景中的发言者。我们的方法通过三项创新来解决关键挑战：3D全注意力机制与滑动窗口去噪策略，支持无限视频生成并保持时间一致性和视觉质量；以及区域特定掩码与自适应损失函数的结合，平衡全局文本控制和局部音频指导。我们的评估表明，MagicInfinite在音频与唇同步、身份保留和动作自然性方面优于其他方法。'}}}, {'id': 'https://huggingface.co/papers/2503.07604', 'title': 'Implicit Reasoning in Transformers is Reasoning through Shortcuts', 'url': 'https://huggingface.co/papers/2503.07604', 'abstract': "Test-time compute is emerging as a new paradigm for enhancing language models' complex multi-step reasoning capabilities, as demonstrated by the success of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit reasoning in test-time compute, implicit reasoning is more inference-efficient, requiring fewer generated tokens. However, why does the advanced reasoning capability fail to emerge in the implicit reasoning style? In this work, we train GPT-2 from scratch on a curated multi-step mathematical reasoning dataset and conduct analytical experiments to investigate how language models perform implicit reasoning in multi-step tasks. Our findings reveal: 1) Language models can perform step-by-step reasoning and achieve high accuracy in both in-domain and out-of-domain tests via implicit reasoning. However, this capability only emerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning abilities emerging from training on un<PRE_TAG>fixed-pattern data</POST_TAG> tend to overfit a specific pattern and fail to generalize further. Notably, this limitation is also observed in state-of-the-art large language models. These findings suggest that language models acquire implicit reasoning through shortcut learning, enabling strong performance on tasks with similar patterns while lacking generalization.", 'score': 13, 'issue_id': 2654, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '313594788f663498', 'authors': ['Tianhe Lin', 'Jian Xie', 'Siyu Yuan', 'Deqing Yang'], 'affiliations': ['School of Computer Science, Fudan University', 'School of Data Science, Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07604.jpg', 'data': {'categories': ['#reasoning', '#training', '#dataset', '#math', '#data'], 'emoji': '🧠', 'ru': {'title': 'Неявные рассуждения в языковых моделях: сила шаблонов и проблемы обобщения', 'desc': 'Исследование показывает, что языковые модели способны выполнять пошаговые рассуждения и достигать высокой точности при неявном рассуждении, но только при обучении на данных с фиксированным шаблоном. При обучении на данных с нефиксированным шаблоном модели склонны переобучаться конкретному паттерну и не могут обобщать. Это ограничение наблюдается даже у современных больших языковых моделей. Результаты указывают на то, что языковые модели приобретают навыки неявного рассуждения через обучение коротким путям, что позволяет им хорошо справляться с задачами схожего паттерна, но ограничивает обобщение.'}, 'en': {'title': 'Unlocking Implicit Reasoning in Language Models', 'desc': 'This paper explores how language models, specifically GPT-2, can perform implicit reasoning in multi-step mathematical tasks. It shows that while these models can achieve high accuracy through implicit reasoning, this ability is contingent on being trained with fixed-pattern data. When trained on variable patterns, the models tend to overfit and struggle to generalize their reasoning skills. The research highlights that language models often rely on shortcut learning, which allows them to excel in specific tasks but limits their broader reasoning capabilities.'}, 'zh': {'title': '隐式推理的捷径与局限性', 'desc': '本文探讨了在测试时计算中，隐式推理与显式推理的效率差异。研究发现，语言模型在固定模式数据上训练时，能够通过隐式推理实现逐步推理并在多步任务中取得高准确率。相反，在非固定模式数据上训练的隐式推理能力容易过拟合特定模式，导致泛化能力不足。总的来说，语言模型通过捷径学习获得隐式推理能力，但在面对不同模式时表现不佳。'}}}, {'id': 'https://huggingface.co/papers/2503.08605', 'title': 'Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled\n  Sampling', 'url': 'https://huggingface.co/papers/2503.08605', 'abstract': 'While recent advancements in text-to-video diffusion models enable high-quality short video generation from a single prompt, generating real-world long videos in a single pass remains challenging due to limited data and high computational costs. To address this, several works propose tuning-free approaches, i.e., extending existing models for long video generation, specifically using multiple prompts to allow for dynamic and controlled content changes. However, these methods primarily focus on ensuring smooth transitions between adjacent frames, often leading to content drift and a gradual loss of semantic coherence over longer sequences. To tackle such an issue, we propose Synchronized Coupled Sampling (SynCoS), a novel inference framework that synchronizes denoising paths across the entire video, ensuring long-range consistency across both adjacent and distant frames. Our approach combines two complementary sampling strategies: reverse and optimization-based sampling, which ensure seamless local transitions and enforce global coherence, respectively. However, directly alternating between these samplings misaligns denoising trajectories, disrupting prompt guidance and introducing unintended content changes as they operate independently. To resolve this, SynCoS synchronizes them through a grounded timestep and a fixed baseline noise, ensuring fully coupled sampling with aligned denoising paths. Extensive experiments show that SynCoS significantly improves multi-event long video generation, achieving smoother transitions and superior long-range coherence, outperforming previous approaches both quantitatively and qualitatively.', 'score': 12, 'issue_id': 2653, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '6f7bf7b6c171af43', 'authors': ['Subin Kim', 'Seoung Wug Oh', 'Jui-Hsien Wang', 'Joon-Young Lee', 'Jinwoo Shin'], 'affiliations': ['Adobe Research', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.08605.jpg', 'data': {'categories': ['#inference', '#diffusion', '#video'], 'emoji': '🎬', 'ru': {'title': 'SynCoS: Синхронизированная генерация длинных видео с сохранением согласованности', 'desc': 'Статья представляет новый метод генерации длинных видео с использованием текстовых подсказок, названный Synchronized Coupled Sampling (SynCoS). Этот подход объединяет обратную выборку и выборку на основе оптимизации для обеспечения плавных переходов между кадрами и глобальной согласованности контента. SynCoS синхронизирует траектории шумоподавления через фиксированный временной шаг и базовый шум, что позволяет избежать нежелательных изменений содержания. Эксперименты показывают, что SynCoS значительно улучшает генерацию длинных видео с несколькими событиями, превосходя предыдущие подходы как количественно, так и качественно.'}, 'en': {'title': 'Achieving Long-Range Coherence in Video Generation with SynCoS', 'desc': 'This paper introduces Synchronized Coupled Sampling (SynCoS), a new framework for generating long videos from text prompts while maintaining semantic coherence. Traditional methods struggle with content drift and frame transitions, especially over extended sequences. SynCoS addresses these issues by synchronizing denoising paths across the video, combining reverse and optimization-based sampling strategies for better local and global coherence. Experimental results demonstrate that SynCoS enhances the quality of multi-event long video generation, outperforming existing techniques in both smoothness and consistency.'}, 'zh': {'title': '同步耦合采样：提升长视频生成的一致性', 'desc': '本文提出了一种新的推理框架，称为同步耦合采样（SynCoS），旨在解决长视频生成中的一致性问题。通过同步去噪路径，SynCoS确保了相邻帧和远程帧之间的长范围一致性。该方法结合了反向采样和基于优化的采样策略，以实现局部平滑过渡和全局一致性。实验结果表明，SynCoS在多事件长视频生成方面显著优于之前的方法，提供了更平滑的过渡和更好的长范围语义一致性。'}}}, {'id': 'https://huggingface.co/papers/2503.08619', 'title': 'LightGen: Efficient Image Generation through Knowledge Distillation and\n  Direct Preference Optimization', 'url': 'https://huggingface.co/papers/2503.08619', 'abstract': 'Recent advances in text-to-image generation have primarily relied on extensive datasets and parameter-heavy architectures. These requirements severely limit accessibility for researchers and practitioners who lack substantial computational resources. In this paper, we introduce \\model, an efficient training paradigm for image generation models that uses knowledge distillation (KD) and Direct Preference Optimization (DPO). Drawing inspiration from the success of data KD techniques widely adopted in Multi-Modal Large Language Models (MLLMs), LightGen distills knowledge from state-of-the-art (SOTA) text-to-image models into a compact Masked Autoregressive (MAR) architecture with only 0.7B parameters. Using a compact <PRE_TAG>synthetic dataset</POST_TAG> of just 2M high-quality images generated from varied captions, we demonstrate that data diversity significantly outweighs data volume in determining model performance. This strategy dramatically reduces computational demands and reduces pre-training time from potentially thousands of GPU-days to merely 88 GPU-days. Furthermore, to address the inherent shortcomings of synthetic data, particularly poor high-frequency details and spatial inaccuracies, we integrate the DPO technique that refines image fidelity and positional accuracy. Comprehensive experiments confirm that LightGen achieves image generation quality comparable to SOTA models while significantly reducing computational resources and expanding accessibility for resource-constrained environments. Code is available at https://github.com/XianfengWu01/LightGen', 'score': 8, 'issue_id': 2655, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '1645a4236e6d91b6', 'authors': ['Xianfeng Wu', 'Yajing Bai', 'Haoze Zheng', 'Harold Haodong Chen', 'Yexin Liu', 'Zihao Wang', 'Xuran Ma', 'Wen-Jie Shu', 'Xianzu Wu', 'Harry Yang', 'Ser-Nam Lim'], 'affiliations': ['Everlyn AI', 'The Hong Kong University of Science and Technology', 'University of Central Florida'], 'pdf_title_img': 'assets/pdf/title_img/2503.08619.jpg', 'data': {'categories': ['#training', '#dataset', '#synthetic', '#architecture', '#optimization'], 'emoji': '🖼️', 'ru': {'title': 'Эффективная генерация изображений с минимальными ресурсами', 'desc': 'Эта статья представляет LightGen - эффективный метод обучения моделей генерации изображений, использующий дистилляцию знаний и Direct Preference Optimization. Модель использует компактную архитектуру с маскированной авторегрессией всего в 0.7 миллиардов параметров, обученную на синтетическом наборе данных из 2 миллионов изображений. Этот подход значительно сокращает вычислительные требования и время предобучения до 88 GPU-дней. Эксперименты показывают, что LightGen достигает качества генерации изображений, сравнимого с передовыми моделями, при существенно меньших вычислительных ресурсах.'}, 'en': {'title': 'Efficient Image Generation with LightGen: Less is More!', 'desc': 'This paper presents LightGen, a new approach to text-to-image generation that focuses on efficiency and accessibility. It utilizes knowledge distillation (KD) and Direct Preference Optimization (DPO) to create a compact model with only 0.7 billion parameters, making it less demanding on computational resources. By training on a small but diverse synthetic dataset of 2 million images, the authors show that the variety of data is more important than sheer volume for model performance. The results indicate that LightGen can produce high-quality images comparable to state-of-the-art models while significantly reducing training time and resource requirements.'}, 'zh': {'title': '高效图像生成，资源友好！', 'desc': '本文介绍了一种名为LightGen的高效图像生成模型训练范式，利用知识蒸馏（KD）和直接偏好优化（DPO）。该方法从最先进的文本到图像模型中提取知识，构建了一个仅有0.7亿参数的紧凑型自回归架构。通过使用仅200万张高质量合成图像的数据集，研究表明数据的多样性对模型性能的影响远大于数据的数量。LightGen显著降低了计算需求，将预训练时间从数千个GPU天缩短至仅88个GPU天，同时保持了与最先进模型相当的图像生成质量。'}}}, {'id': 'https://huggingface.co/papers/2503.07860', 'title': 'Video Action Differencing', 'url': 'https://huggingface.co/papers/2503.07860', 'abstract': 'How do two individuals differ when performing the same action? In this work, we introduce Video Action Differencing (VidDiff), the novel task of identifying subtle differences between videos of the same action, which has many applications, such as coaching and skill learning. To enable development on this new task, we first create VidDiffBench, a benchmark dataset containing 549 video pairs, with human annotations of 4,469 fine-grained action differences and 2,075 localization timestamps indicating where these differences occur. Our experiments demonstrate that VidDiffBench poses a significant challenge for state-of-the-art large multimodal models (LMMs), such as GPT-4o and Qwen2-VL. By analyzing failure cases of LMMs on VidDiffBench, we highlight two key challenges for this task: localizing relevant sub-actions over two videos and fine-grained frame comparison. To overcome these, we propose the VidDiff method, an agentic workflow that breaks the task into three stages: action difference proposal, keyframe localization, and frame differencing, each stage utilizing specialized foundation models. To encourage future research in this new task, we release the benchmark at https://huggingface.co/datasets/jmhb/VidDiffBench and code at http://jmhb0.github.io/viddiff.', 'score': 8, 'issue_id': 2653, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '76b8b9c677de83cd', 'authors': ['James Burgess', 'Xiaohan Wang', 'Yuhui Zhang', 'Anita Rau', 'Alejandro Lozano', 'Lisa Dunlap', 'Trevor Darrell', 'Serena Yeung-Levy'], 'affiliations': ['Stanford', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.07860.jpg', 'data': {'categories': ['#benchmark', '#agents', '#multimodal', '#dataset', '#video'], 'emoji': '🎥', 'ru': {'title': 'Новый фронтир в анализе видео: выявление тонких различий в действиях', 'desc': 'Статья представляет новую задачу в области компьютерного зрения - Video Action Differencing (VidDiff), которая направлена на выявление тонких различий между видео с одинаковыми действиями. Авторы создали датасет VidDiffBench, содержащий 549 пар видео с аннотациями различий в действиях и временными метками. Эксперименты показали, что современные мультимодальные языковые модели (LLM) испытывают трудности с этой задачей. Для решения проблемы предложен метод VidDiff, использующий трехэтапный подход с применением специализированных фундаментальных моделей.'}, 'en': {'title': 'Unveiling Subtle Differences in Action Videos with VidDiff', 'desc': 'This paper introduces Video Action Differencing (VidDiff), a new task focused on identifying subtle differences in videos depicting the same action. To support this task, the authors created VidDiffBench, a benchmark dataset with 549 video pairs and detailed annotations of action differences and localization timestamps. The study reveals that current large multimodal models struggle with this task, particularly in localizing sub-actions and performing fine-grained frame comparisons. To address these challenges, the authors propose a structured approach called VidDiff, which divides the task into three stages, each leveraging specialized foundation models for improved performance.'}, 'zh': {'title': '视频动作差异化：识别细微差别的新挑战', 'desc': '本文介绍了一种新任务，称为视频动作差异化（VidDiff），旨在识别同一动作视频之间的细微差别。为此，我们创建了VidDiffBench，一个包含549对视频的基准数据集，标注了4469个细粒度动作差异和2075个时间戳。我们的实验表明，VidDiffBench对现有的大型多模态模型（LMMs）提出了重大挑战，尤其是在局部化相关子动作和细粒度帧比较方面。为了解决这些问题，我们提出了VidDiff方法，将任务分为三个阶段：动作差异提议、关键帧定位和帧差异化，每个阶段都利用了专门的基础模型。'}}}, {'id': 'https://huggingface.co/papers/2503.07891', 'title': 'Gemini Embedding: Generalizable Embeddings from Gemini', 'url': 'https://huggingface.co/papers/2503.07891', 'abstract': "In this report, we introduce Gemini Embedding, a state-of-the-art embedding model leveraging the power of Gemini, Google's most capable large language model. Capitalizing on Gemini's inherent multilingual and code understanding capabilities, Gemini Embedding produces highly generalizable embeddings for text spanning numerous languages and textual modalities. The representations generated by Gemini Embedding can be precomputed and applied to a variety of downstream tasks including classification, similarity, clustering, ranking, and retrieval. Evaluated on the Massive Multilingual Text Embedding Benchmark (MMTEB), which includes over one hundred tasks across 250+ languages, Gemini Embedding substantially outperforms prior state-of-the-art models, demonstrating considerable improvements in embedding quality. Achieving state-of-the-art performance across MMTEB's multilingual, English, and code benchmarks, our unified model demonstrates strong capabilities across a broad selection of tasks and surpasses specialized domain-specific models.", 'score': 6, 'issue_id': 2655, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '149f8fc31808d626', 'authors': ['Jinhyuk Lee', 'Feiyang Chen', 'Sahil Dua', 'Daniel Cer', 'Madhuri Shanbhogue', 'Iftekhar Naim', 'Gustavo Hernández Ábrego', 'Zhe Li', 'Kaifeng Chen', 'Henrique Schechter Vera', 'Xiaoqi Ren', 'Shanfeng Zhang', 'Daniel Salz', 'Michael Boratko', 'Jay Han', 'Blair Chen', 'Shuo Huang', 'Vikram Rao', 'Paul Suganthan', 'Feng Han', 'Andreas Doumanoglou', 'Nithi Gupta', 'Fedor Moiseev', 'Cathy Yip', 'Aashi Jain', 'Simon Baumgartner', 'Shahrokh Shahi', 'Frank Palma Gomez', 'Sandeep Mariserla', 'Min Choi', 'Parashar Shah', 'Sonam Goenka', 'Ke Chen', 'Ye Xia', 'Koert Chen', 'Sai Meher Karthik Duddu', 'Yichang Chen', 'Trevor Walker', 'Wenlei Zhou', 'Rakesh Ghiya', 'Zach Gleicher', 'Karan Gill', 'Zhe Dong', 'Mojtaba Seyedhosseini', 'Yunhsuan Sung', 'Raphael Hoffmann', 'Tom Duerig'], 'affiliations': ['Google'], 'pdf_title_img': 'assets/pdf/title_img/2503.07891.jpg', 'data': {'categories': ['#multimodal', '#multilingual', '#dataset', '#transfer_learning', '#benchmark', '#open_source'], 'emoji': '🌐', 'ru': {'title': 'Gemini Embedding: универсальные эмбеддинги для текста и кода на множестве языков', 'desc': 'Статья представляет Gemini Embedding - передовую модель встраивания, основанную на мощной языковой модели Gemini от Google. Модель использует многоязычные возможности и понимание кода Gemini для создания универсальных эмбеддингов текста на различных языках. Gemini Embedding превосходит предыдущие модели по результатам оценки на бенчмарке MMTEB, включающем более 100 задач на 250+ языках. Модель демонстрирует высокую эффективность в многоязычных задачах, задачах на английском языке и задачах, связанных с кодом, превосходя специализированные модели.'}, 'en': {'title': 'Unifying Multilingual Understanding with Gemini Embedding', 'desc': "Gemini Embedding is a cutting-edge embedding model that utilizes Google's advanced Gemini large language model. It excels in generating versatile embeddings for text in multiple languages and formats, thanks to its multilingual and code comprehension abilities. These embeddings can be precomputed and effectively used in various machine learning tasks such as classification, similarity, clustering, ranking, and retrieval. In evaluations on the Massive Multilingual Text Embedding Benchmark (MMTEB), Gemini Embedding significantly outperformed previous models, showcasing its superior embedding quality across a wide range of languages and tasks."}, 'zh': {'title': 'Gemini Embedding：多语言嵌入的新标杆', 'desc': '本报告介绍了Gemini Embedding，这是一种先进的嵌入模型，利用了谷歌最强大的大型语言模型Gemini的能力。Gemini Embedding能够生成高度通用的文本嵌入，适用于多种语言和文本形式，充分利用了Gemini的多语言和代码理解能力。生成的表示可以预先计算，并应用于分类、相似性、聚类、排序和检索等多种下游任务。在大规模多语言文本嵌入基准测试（MMTEB）中，Gemini Embedding显著超越了之前的最先进模型，展示了嵌入质量的显著提升。'}}}, {'id': 'https://huggingface.co/papers/2503.07572', 'title': 'Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning', 'url': 'https://huggingface.co/papers/2503.07572', 'abstract': "Training models to effectively use test-time compute is crucial for improving the reasoning performance of LLMs. Current methods mostly do so via fine-tuning on search traces or running RL with 0/1 outcome reward, but do these approaches efficiently utilize test-time compute? Would these approaches continue to scale as the budget improves? In this paper, we try to answer these questions. We formalize the problem of optimizing test-time compute as a meta-reinforcement learning (RL) problem, which provides a principled perspective on spending test-time compute. This perspective enables us to view the long output stream from the LLM as consisting of several episodes run at test time and leads us to use a notion of cumulative regret over output tokens as a way to measure the efficacy of test-time compute. Akin to how RL algorithms can best tradeoff exploration and exploitation over training, minimizing cumulative regret would also provide the best balance between exploration and exploitation in the token stream. While we show that state-of-the-art models do not minimize regret, one can do so by maximizing a dense reward bonus in conjunction with the outcome 0/1 reward RL. This bonus is the ''progress'' made by each subsequent block in the output stream, quantified by the change in the likelihood of eventual success. Using these insights, we develop Meta Reinforcement Fine-Tuning, or MRT, a new class of fine-tuning methods for optimizing test-time compute. MRT leads to a 2-3x relative gain in performance and roughly a 1.5x gain in token efficiency for math reasoning compared to outcome-reward RL.", 'score': 6, 'issue_id': 2653, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '49ca445bc3322f0d', 'authors': ['Yuxiao Qu', 'Matthew Y. R. Yang', 'Amrith Setlur', 'Lewis Tunstall', 'Edward Emanuel Beeching', 'Ruslan Salakhutdinov', 'Aviral Kumar'], 'affiliations': ['Carnegie Mellon University', 'Hugging Face'], 'pdf_title_img': 'assets/pdf/title_img/2503.07572.jpg', 'data': {'categories': ['#optimization', '#training', '#rl', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Оптимизация вычислений LLM через мета-обучение с подкреплением', 'desc': "Статья представляет новый подход к оптимизации вычислений во время тестирования для улучшения рассуждений в больших языковых моделях (LLM). Авторы формализуют эту задачу как проблему мета-обучения с подкреплением, вводя понятие кумулятивного сожаления для оценки эффективности вычислений. Они разрабатывают метод Meta Reinforcement Fine-Tuning (MRT), который максимизирует плотное вознаграждение за 'прогресс' в дополнение к бинарному результату. MRT показывает значительное улучшение производительности и эффективности использования токенов в задачах математических рассуждений."}, 'en': {'title': 'Optimizing Test-Time Compute for Enhanced LLM Reasoning', 'desc': "This paper addresses the challenge of optimizing test-time compute for large language models (LLMs) to enhance their reasoning capabilities. It introduces a meta-reinforcement learning framework that treats the output generation process as a series of episodes, allowing for a structured approach to manage compute resources effectively. The authors propose minimizing cumulative regret over output tokens as a metric for evaluating the efficiency of test-time compute, which balances exploration and exploitation in the model's output. They present a new fine-tuning method called Meta Reinforcement Fine-Tuning (MRT), which significantly improves performance and token efficiency in mathematical reasoning tasks compared to traditional outcome-reward reinforcement learning methods."}, 'zh': {'title': '优化测试时计算，提升推理性能！', 'desc': '本文探讨了如何有效利用测试时计算资源来提升大型语言模型（LLM）的推理性能。我们将优化测试时计算的问题形式化为元强化学习（RL）问题，从而提供了一个系统化的视角来支配测试时计算。通过将LLM的输出流视为多个测试时的回合，并使用累积遗憾的概念来衡量测试时计算的有效性，我们提出了一种新的微调方法，称为元强化微调（MRT）。实验结果表明，MRT在数学推理任务中相较于传统的结果奖励RL方法，性能提升了2-3倍，令令牌效率提高了约1.5倍。'}}}, {'id': 'https://huggingface.co/papers/2503.08686', 'title': 'OmniMamba: Efficient and Unified Multimodal Understanding and Generation\n  via State Space Models', 'url': 'https://huggingface.co/papers/2503.08686', 'abstract': "Recent advancements in unified multimodal understanding and visual generation (or multimodal generation) models have been hindered by their quadratic computational complexity and dependence on large-scale training data. We present OmniMamba, the first linear-architecture-based multimodal generation model that generates both text and images through a unified next-token prediction paradigm. The model fully leverages Mamba-2's high computational and memory efficiency, extending its capabilities from text generation to multimodal generation. To address the data inefficiency of existing unified models, we propose two key innovations: (1) decoupled vocabularies to guide modality-specific generation, and (2) task-specific LoRA for parameter-efficient adaptation. Furthermore, we introduce a decoupled two-stage training strategy to mitigate data imbalance between two tasks. Equipped with these techniques, OmniMamba achieves competitive performance with JanusFlow while surpassing Show-o across benchmarks, despite being trained on merely 2M image-text pairs, which is 1,000 times fewer than Show-o. Notably, OmniMamba stands out with outstanding inference efficiency, achieving up to a 119.2 times speedup and 63% GPU memory reduction for long-sequence generation compared to Transformer-based counterparts. Code and models are released at https://github.com/hustvl/OmniMamba", 'score': 5, 'issue_id': 2655, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '7bb3cd796d69dc2c', 'authors': ['Jialv Zou', 'Bencheng Liao', 'Qian Zhang', 'Wenyu Liu', 'Xinggang Wang'], 'affiliations': ['Horizon Robotics', 'Institute of Artificial Intelligence, Huazhong University of Science & Technology', 'School of EIC, Huazhong University of Science & Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.08686.jpg', 'data': {'categories': ['#training', '#inference', '#multimodal', '#architecture', '#open_source', '#optimization'], 'emoji': '🦋', 'ru': {'title': 'OmniMamba: Эффективная мультимодальная генерация с линейной архитектурой', 'desc': 'OmniMamba - это первая мультимодальная модель генерации, основанная на линейной архитектуре, которая генерирует как текст, так и изображения через единую парадигму предсказания следующего токена. Модель использует высокую вычислительную эффективность Mamba-2, расширяя ее возможности от генерации текста до мультимодальной генерации. Для решения проблемы неэффективности данных существующих унифицированных моделей, авторы предлагают разделенные словари и задаче-специфичную LoRA для адаптации с малым количеством параметров. OmniMamba достигает конкурентоспособной производительности, будучи обученной всего на 2 миллионах пар изображение-текст, что в 1000 раз меньше, чем у аналогов.'}, 'en': {'title': 'OmniMamba: Efficient Multimodal Generation with Linear Architecture', 'desc': 'OmniMamba is a groundbreaking multimodal generation model that efficiently creates both text and images using a linear architecture. It introduces a unified next-token prediction approach, which significantly reduces computational complexity and memory usage compared to traditional models. The model employs innovative techniques like decoupled vocabularies for specific modality guidance and task-specific LoRA for efficient parameter adaptation. With a unique two-stage training strategy, OmniMamba achieves impressive performance on benchmarks while requiring far less training data, demonstrating remarkable speed and memory efficiency during inference.'}, 'zh': {'title': 'OmniMamba：高效的多模态生成新选择', 'desc': 'OmniMamba是一种新型的多模态生成模型，采用线性架构，能够同时生成文本和图像。该模型通过统一的下一个标记预测范式，充分利用了Mamba-2的高效计算和内存性能。为了提高数据利用效率，OmniMamba引入了解耦词汇和任务特定的LoRA技术，并采用了两阶段的训练策略来解决任务间的数据不平衡问题。最终，OmniMamba在生成效率上表现出色，相比于传统的Transformer模型，推理速度提高了119.2倍，GPU内存减少了63%。'}}}, {'id': 'https://huggingface.co/papers/2503.08588', 'title': 'BiasEdit: Debiasing Stereotyped Language Models via Model Editing', 'url': 'https://huggingface.co/papers/2503.08588', 'abstract': "Previous studies have established that language models manifest stereotyped biases. Existing debiasing strategies, such as retraining a model with counterfactual data, representation projection, and prompting often fail to efficiently eliminate bias or directly alter the models' biased internal representations. To address these issues, we propose BiasEdit, an efficient model editing method to remove stereotypical bias from language models through lightweight networks that act as editors to generate parameter updates. BiasEdit employs a debiasing loss guiding editor networks to conduct local edits on partial parameters of a language model for debiasing while preserving the language modeling abilities during editing through a retention loss. Experiments on StereoSet and Crows-Pairs demonstrate the effectiveness, efficiency, and robustness of BiasEdit in eliminating bias compared to tangental debiasing baselines and little to no impact on the language models' general capabilities. In addition, we conduct bias tracing to probe bias in various modules and explore bias editing impacts on different components of language models.", 'score': 5, 'issue_id': 2658, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '183584887772b6e7', 'authors': ['Xin Xu', 'Wei Xu', 'Ningyu Zhang', 'Julian McAuley'], 'affiliations': ['Georgia Institute of Technology', 'University of California, San Diego', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08588.jpg', 'data': {'categories': ['#hallucinations', '#ethics', '#architecture', '#data', '#training'], 'emoji': '⚖️', 'ru': {'title': 'Эффективное устранение предубеждений в языковых моделях без потери их возможностей', 'desc': 'Статья представляет метод BiasEdit для устранения стереотипных предубеждений в языковых моделях. В отличие от существующих методов дебиасинга, BiasEdit использует легковесные сети-редакторы для генерации обновлений параметров модели. Метод применяет функцию потерь для дебиасинга, направляющую редакторы на локальное редактирование частичных параметров, одновременно сохраняя языковые способности модели. Эксперименты показывают эффективность BiasEdit в устранении предубеждений без существенного влияния на общие возможности языковых моделей.'}, 'en': {'title': 'BiasEdit: Efficiently Editing Bias in Language Models', 'desc': "This paper introduces BiasEdit, a novel method designed to reduce stereotypical biases in language models. Unlike traditional debiasing techniques that often fail to effectively alter biased representations, BiasEdit utilizes lightweight networks to make targeted updates to the model's parameters. It incorporates a debiasing loss to guide these edits while ensuring that the model's language processing abilities remain intact through a retention loss. The results show that BiasEdit is not only effective in reducing bias but also maintains the overall performance of the language models across various tasks."}, 'zh': {'title': '高效去偏见，提升语言模型公正性', 'desc': '本研究提出了一种名为BiasEdit的模型编辑方法，旨在通过轻量级网络去除语言模型中的刻板偏见。与传统的去偏见策略相比，BiasEdit能够高效地进行局部参数编辑，同时保持语言模型的能力。该方法使用去偏见损失指导编辑网络进行参数更新，并通过保留损失确保编辑过程中的语言建模能力不受影响。实验结果表明，BiasEdit在消除偏见方面表现出色，且对语言模型的整体能力影响较小。'}}}, {'id': 'https://huggingface.co/papers/2503.08689', 'title': 'QuoTA: Query-oriented Token Assignment via CoT Query Decouple for Long\n  Video Comprehension', 'url': 'https://huggingface.co/papers/2503.08689', 'abstract': 'Recent advances in long video understanding typically mitigate visual redundancy through visual token pruning based on attention distribution. However, while existing methods employ post-hoc low-response token pruning in decoder layers, they overlook the input-level semantic correlation between visual tokens and instructions (query). In this paper, we propose QuoTA, an ante-hoc training-free modular that extends existing large video-language models (LVLMs) for visual token assignment based on query-oriented frame-level importance assessment. The query-oriented token selection is crucial as it aligns visual processing with task-specific requirements, optimizing token budget utilization while preserving semantically relevant content. Specifically, (i) QuoTA strategically allocates frame-level importance scores based on query relevance, enabling one-time visual token assignment before cross-modal interactions in decoder layers, (ii) we decouple the query through Chain-of-Thoughts reasoning to facilitate more precise LVLM-based frame importance scoring, and (iii) QuoTA offers a plug-and-play functionality that extends to existing LVLMs. Extensive experimental results demonstrate that implementing QuoTA with LLaVA-Video-7B yields an average performance improvement of 3.2% across six benchmarks (including Video-MME and MLVU) while operating within an identical visual token budget as the baseline. Codes are open-sourced at https://github.com/MAC-AutoML/QuoTA.', 'score': 4, 'issue_id': 2655, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '7dbb5f0edfd69aad', 'authors': ['Yongdong Luo', 'Wang Chen', 'Xiawu Zheng', 'Weizhong Huang', 'Shukang Yin', 'Haojia Lin', 'Chaoyou Fu', 'Jinfa Huang', 'Jiayi Ji', 'Jiebo Luo', 'Rongrong Ji'], 'affiliations': ['Nanjing University', 'University of Rochester', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08689.jpg', 'data': {'categories': ['#alignment', '#multimodal', '#long_context', '#architecture', '#video', '#benchmark', '#open_source', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'Умный отбор кадров для эффективного понимания видео ИИ', 'desc': 'Статья представляет QuoTA - новый модуль для улучшения понимания длинных видео большими видео-языковыми моделями (LVLM). QuoTA выполняет отбор визуальных токенов на основе оценки важности кадров с учетом запроса пользователя, что позволяет более эффективно использовать ограниченный бюджет токенов. Метод включает декомпозицию запроса через рассуждения по цепочке мыслей для более точной оценки важности кадров. Эксперименты показывают, что применение QuoTA с LLaVA-Video-7B дает среднее улучшение производительности на 3.2% по шести бенчмаркам при том же бюджете визуальных токенов.'}, 'en': {'title': 'Optimizing Visual Token Selection for Better Video Understanding', 'desc': 'This paper introduces QuoTA, a new method for improving long video understanding by optimizing how visual tokens are selected based on their relevance to specific queries. Unlike previous approaches that prune tokens after processing, QuoTA assesses the importance of visual tokens before they are used, ensuring that only the most relevant information is retained. By using Chain-of-Thoughts reasoning, QuoTA enhances the accuracy of frame importance scoring, allowing for better alignment between visual data and task requirements. The results show that QuoTA can significantly boost performance in existing large video-language models while maintaining the same token budget.'}, 'zh': {'title': '优化视觉标记分配，提升视频理解性能', 'desc': '本文提出了一种名为QuoTA的模块，旨在改进长视频理解中的视觉标记分配。与现有方法不同，QuoTA在输入层面上考虑视觉标记与查询之间的语义关联，从而优化标记的使用效率。通过基于查询相关性的帧级重要性评估，QuoTA能够在解码器层之前进行一次性视觉标记分配。实验结果表明，QuoTA在多个基准测试中显著提高了性能，同时保持了与基线相同的视觉标记预算。'}}}, {'id': 'https://huggingface.co/papers/2503.08685', 'title': '"Principal Components" Enable A New Language of Images', 'url': 'https://huggingface.co/papers/2503.08685', 'abstract': 'We introduce a novel visual tokenization framework that embeds a provable PCA-like structure into the latent token space. While existing visual tokenizers primarily optimize for reconstruction fidelity, they often neglect the structural properties of the latent space -- a critical factor for both interpretability and downstream tasks. Our method generates a 1D causal token sequence for images, where each successive token contributes non-overlapping information with mathematically guaranteed decreasing explained variance, analogous to principal component analysis. This structural constraint ensures the tokenizer extracts the most salient visual features first, with each subsequent token adding diminishing yet complementary information. Additionally, we identified and resolved a semantic-spectrum coupling effect that causes the unwanted entanglement of high-level semantic content and low-level spectral details in the tokens by leveraging a diffusion decoder. Experiments demonstrate that our approach achieves state-of-the-art reconstruction performance and enables better interpretability to align with the human vision system. Moreover, auto-regressive models trained on our token sequences achieve performance comparable to current state-of-the-art methods while requiring fewer tokens for training and inference.', 'score': 2, 'issue_id': 2654, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': 'a013cfdc2d1e9d7c', 'authors': ['Xin Wen', 'Bingchen Zhao', 'Ismail Elezi', 'Jiankang Deng', 'Xiaojuan Qi'], 'affiliations': ['Imperial College London', 'Noahs Ark Lab', 'University of Edinburgh', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.08685.jpg', 'data': {'categories': ['#training', '#cv', '#interpretability', '#diffusion', '#architecture', '#optimization'], 'emoji': '🧩', 'ru': {'title': 'Структурированная визуальная токенизация: ПК-анализ для изображений', 'desc': 'Авторы представляют новый метод визуальной токенизации, который встраивает доказуемую PCA-подобную структуру в пространство латентных токенов. Этот подход генерирует одномерную причинную последовательность токенов для изображений, где каждый последующий токен вносит неперекрывающуюся информацию с математически гарантированной убывающей объясненной дисперсией. Метод также решает проблему связи семантического содержания и спектральных деталей в токенах с помощью диффузионного декодера. Эксперименты показывают, что этот подход достигает современного уровня производительности реконструкции и обеспечивает лучшую интерпретируемость.'}, 'en': {'title': 'Enhancing Visual Tokenization with Structured Latent Spaces', 'desc': "This paper presents a new visual tokenization framework that incorporates a PCA-like structure into the latent token space. Unlike traditional visual tokenizers that focus mainly on how well they can recreate images, this method emphasizes the importance of the latent space's structure for better interpretability and performance in other tasks. The framework produces a sequence of tokens that each provide unique information, ensuring that the most important visual features are captured first, similar to how principal component analysis works. Additionally, the authors address a problem where high-level semantic information and low-level details become mixed in the tokens, leading to improved clarity and efficiency in training auto-regressive models."}, 'zh': {'title': '创新视觉标记化，提升可解释性与性能', 'desc': '我们提出了一种新颖的视觉标记化框架，将可证明的主成分分析（PCA）结构嵌入潜在标记空间。现有的视觉标记器主要优化重建精度，但往往忽视潜在空间的结构特性，这对可解释性和下游任务至关重要。我们的方法为图像生成一维因果标记序列，每个后续标记提供不重叠的信息，并且具有数学上保证的递减解释方差，类似于主成分分析。实验表明，我们的方法在重建性能上达到了最先进的水平，并提高了与人类视觉系统的对齐可解释性。'}}}, {'id': 'https://huggingface.co/papers/2503.07639', 'title': 'Mixture of Experts Made Intrinsically Interpretable', 'url': 'https://huggingface.co/papers/2503.07639', 'abstract': 'Neurons in large language models often exhibit polysemanticity, simultaneously encoding multiple unrelated concepts and obscuring interpretability. Instead of relying on post-hoc methods, we present MoE-X, a Mixture-of-Experts (MoE) language model designed to be intrinsically interpretable. Our approach is motivated by the observation that, in language models, wider networks with <PRE_TAG>sparse activations</POST_TAG> are more likely to capture interpretable factors. However, directly training such large sparse networks is computationally prohibitive. MoE architectures offer a scalable alternative by activating only a subset of experts for any given input, inherently aligning with interpretability objectives. In MoE-X, we establish this connection by rewriting the MoE layer as an equivalent sparse, large MLP. This approach enables efficient scaling of the hidden size while maintaining sparsity. To further enhance interpretability, we enforce sparse activation within each expert and redesign the routing mechanism to prioritize experts with the highest activation sparsity. These designs ensure that only the most salient features are routed and processed by the experts. We evaluate MoE-X on chess and natural language tasks, showing that it achieves performance comparable to dense models while significantly improving interpretability. MoE-X achieves a perplexity better than GPT-2, with interpretability surpassing even sparse autoencoder (SAE)-based approaches.', 'score': 2, 'issue_id': 2653, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '7e9a13248a2692b5', 'authors': ['Xingyi Yang', 'Constantin Venhoff', 'Ashkan Khakzar', 'Christian Schroeder de Witt', 'Puneet K. Dokania', 'Adel Bibi', 'Philip Torr'], 'affiliations': ['National University of Singapore', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.07639.jpg', 'data': {'categories': ['#training', '#games', '#architecture', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'MoE-X: Интерпретируемые языковые модели через разреженные экспертные системы', 'desc': 'Статья представляет MoE-X - новую архитектуру языковой модели, основанную на принципе Mixture-of-Experts. Эта модель разработана для повышения интерпретируемости нейронных сетей путем использования разреженных активаций и селективного задействования экспертов. MoE-X переписывает слой MoE как эквивалентную разреженную большую многослойную перцептронную сеть, что позволяет эффективно масштабировать скрытый размер при сохранении разреженности. Результаты экспериментов показывают, что MoE-X достигает производительности, сравнимой с плотными моделями, при значительно улучшенной интерпретируемости.'}, 'en': {'title': 'MoE-X: Enhancing Interpretability in Language Models with Sparse Activations', 'desc': 'This paper introduces MoE-X, a Mixture-of-Experts language model that aims to improve interpretability in large language models by utilizing sparse activations. The authors argue that wider networks with sparse activations can better capture distinct concepts, making them more interpretable. MoE-X efficiently scales by activating only a subset of experts for each input, which aligns with the goal of enhancing interpretability. The model is evaluated on chess and natural language tasks, demonstrating performance on par with dense models while offering superior interpretability compared to existing methods.'}, 'zh': {'title': 'MoE-X：可解释的混合专家语言模型', 'desc': '在大型语言模型中，神经元常常表现出多义性，同时编码多个无关的概念，导致可解释性差。我们提出了MoE-X，这是一种混合专家（MoE）语言模型，旨在内在上具有可解释性。我们的研究表明，具有稀疏激活的宽网络更有可能捕捉可解释的因素。通过激活仅一部分专家，MoE架构提供了一种可扩展的替代方案，从而在保持稀疏性的同时实现高效的隐藏层规模。'}}}, {'id': 'https://huggingface.co/papers/2502.18858', 'title': 'Evaluating Intelligence via Trial and Error', 'url': 'https://huggingface.co/papers/2502.18858', 'abstract': "Intelligence is a crucial trait for species to find solutions within a limited number of trial-and-error attempts. Building on this idea, we introduce Survival Game as a framework to evaluate intelligence based on the number of failed attempts in a trial-and-error process. Fewer failures indicate higher intelligence. When the expectation and variance of failure counts are both finite, it signals the ability to consistently find solutions to new challenges, which we define as the Autonomous Level of intelligence. Using Survival Game, we comprehensively evaluate existing AI systems. Our results show that while AI systems achieve the Autonomous Level in simple tasks, they are still far from it in more complex tasks, such as vision, search, recommendation, and language. While scaling current AI technologies might help, this would come at an astronomical cost. Projections suggest that achieving the Autonomous Level for general tasks would require 10^{26} parameters. To put this into perspective, loading such a massive model requires so many H100 GPUs that their total value is 10^{7} times that of Apple Inc.'s market value. Even with Moore's Law, supporting such a parameter scale would take 70 years. This staggering cost highlights the complexity of human tasks and the inadequacies of current AI technologies. To further investigate this phenomenon, we conduct a theoretical analysis of Survival Game and its experimental results. Our findings suggest that human tasks possess a criticality property. As a result, Autonomous Level requires a deep understanding of the task's underlying mechanisms. Current AI systems, however, do not fully grasp these mechanisms and instead rely on superficial mimicry, making it difficult for them to reach an autonomous level. We believe Survival Game can not only guide the future development of AI but also offer profound insights into human intelligence.", 'score': 2, 'issue_id': 2656, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': '54797794006daa5e', 'authors': ['Jingtao Zhan', 'Jiahao Zhao', 'Jiayu Li', 'Yiqun Liu', 'Bo Zhang', 'Qingyao Ai', 'Jiaxin Mao', 'Hongning Wang', 'Min Zhang', 'Shaoping Ma'], 'affiliations': ['Renmin University of China', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.18858.jpg', 'data': {'categories': ['#reasoning', '#training', '#agents', '#agi', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Автономный интеллект: путь через Survival Game', 'desc': 'Статья представляет концепцию Survival Game для оценки интеллекта на основе количества неудачных попыток в процессе проб и ошибок. Авторы вводят понятие Автономного Уровня интеллекта и оценивают существующие системы искусственного интеллекта по этому критерию. Результаты показывают, что для достижения Автономного Уровня в сложных задачах потребуется огромное количество параметров модели, что экономически нецелесообразно. Исследование выявляет, что человеческие задачи обладают свойством критичности, требующим глубокого понимания механизмов, которого не хватает современным системам ИИ.'}, 'en': {'title': 'Measuring Intelligence: The Survival Game Framework', 'desc': 'This paper introduces the Survival Game framework to measure intelligence based on the number of failed attempts in problem-solving. It defines the Autonomous Level of intelligence as the ability to find solutions with fewer failures, indicating a deeper understanding of tasks. The study evaluates existing AI systems, revealing that while they perform well on simple tasks, they struggle with complex ones like vision and language. The findings suggest that achieving a high Autonomous Level in AI would require an impractical scale of parameters, highlighting the limitations of current technologies and the complexity of human-like intelligence.'}, 'zh': {'title': '生存游戏：评估智能的新框架', 'desc': '本论文提出了一种名为生存游戏的框架，用于评估智能水平，特别是在试错过程中失败次数的多少。失败次数越少，表示智能水平越高。研究表明，尽管现有的人工智能系统在简单任务中达到了自主水平，但在复杂任务（如视觉、搜索、推荐和语言处理）中仍然远未达到。我们认为，生存游戏不仅可以指导未来的人工智能发展，还能深入理解人类智能的本质。'}}}, {'id': 'https://huggingface.co/papers/2503.07699', 'title': 'RayFlow: Instance-Aware Diffusion Acceleration via Adaptive Flow\n  Trajectories', 'url': 'https://huggingface.co/papers/2503.07699', 'abstract': "Diffusion models have achieved remarkable success across various domains. However, their slow generation speed remains a critical challenge. Existing acceleration methods, while aiming to reduce steps, often compromise sample quality, controllability, or introduce training complexities. Therefore, we propose RayFlow, a novel diffusion framework that addresses these limitations. Unlike previous methods, RayFlow guides each sample along a unique path towards an instance-specific target distribution. This method minimizes sampling steps while preserving generation diversity and stability. Furthermore, we introduce Time Sampler, an importance sampling technique to enhance training efficiency by focusing on crucial timesteps. Extensive experiments demonstrate RayFlow's superiority in generating high-quality images with improved speed, control, and training efficiency compared to existing acceleration techniques.", 'score': 1, 'issue_id': 2656, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '49ebba2cb63d91f8', 'authors': ['Huiyang Shao', 'Xin Xia', 'Yuhong Yang', 'Yuxi Ren', 'Xing Wang', 'Xuefeng Xiao'], 'affiliations': ['ByteDance Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2503.07699.jpg', 'data': {'categories': ['#cv', '#diffusion', '#optimization', '#training'], 'emoji': '🚀', 'ru': {'title': 'RayFlow: быстрая и качественная генерация изображений с помощью диффузионных моделей', 'desc': 'RayFlow - это новая система диффузионных моделей, которая решает проблему медленной генерации изображений. Она направляет каждый сэмпл по уникальному пути к индивидуальному целевому распределению, минимизируя количество шагов без ущерба для качества и разнообразия. RayFlow также включает метод Time Sampler для повышения эффективности обучения путем фокусировки на ключевых временных шагах. Эксперименты показывают превосходство RayFlow над существующими методами ускорения в скорости, контроле и эффективности обучения.'}, 'en': {'title': 'RayFlow: Accelerating Diffusion Models Without Compromise', 'desc': 'This paper introduces RayFlow, a new diffusion model framework designed to improve the speed of image generation without sacrificing quality. Unlike traditional methods that often reduce the number of steps at the cost of sample diversity or stability, RayFlow customizes the sampling path for each instance, ensuring a more efficient and effective generation process. Additionally, the authors present Time Sampler, an importance sampling technique that optimizes training by prioritizing key timesteps. Experimental results show that RayFlow outperforms existing methods in generating high-quality images more quickly and with better control.'}, 'zh': {'title': 'RayFlow：加速扩散模型的新方法', 'desc': '扩散模型在多个领域取得了显著成功，但其生成速度较慢仍然是一个关键挑战。现有的加速方法虽然旨在减少步骤，但往往会影响样本质量、可控性或增加训练复杂性。为此，我们提出了RayFlow，这是一种新颖的扩散框架，能够解决这些限制。RayFlow通过引导每个样本沿着独特路径朝向特定目标分布，最小化采样步骤，同时保持生成的多样性和稳定性。'}}}, {'id': 'https://huggingface.co/papers/2503.08102', 'title': 'AI-native Memory 2.0: Second Me', 'url': 'https://huggingface.co/papers/2503.08102', 'abstract': 'Human interaction with the external world fundamentally involves the exchange of personal memory, whether with other individuals, websites, applications, or, in the future, AI agents. A significant portion of this interaction is redundant, requiring users to repeatedly provide the same information across different contexts. Existing solutions, such as browser-stored credentials, autofill mechanisms, and unified authentication systems, have aimed to mitigate this redundancy by serving as intermediaries that store and retrieve commonly used user data. The advent of large language models (LLMs) presents an opportunity to redefine memory management through an AI-native paradigm: SECOND ME. SECOND ME acts as an intelligent, persistent memory offload system that retains, organizes, and dynamically utilizes user-specific knowledge. By serving as an intermediary in user interactions, it can autonomously generate context-aware responses, prefill required information, and facilitate seamless communication with external systems, significantly reducing cognitive load and interaction friction. Unlike traditional memory storage solutions, SECOND ME extends beyond static data retention by leveraging LLM-based memory parameterization. This enables structured organization, contextual reasoning, and adaptive knowledge retrieval, facilitating a more systematic and intelligent approach to memory management. As AI-driven personal agents like SECOND ME become increasingly integrated into digital ecosystems, SECOND ME further represents a critical step toward augmenting human-world interaction with persistent, contextually aware, and self-optimizing memory systems. We have open-sourced the fully localizable deployment system at GitHub: https://github.com/Mindverse/Second-Me.', 'score': 0, 'issue_id': 2658, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '45b3f3aee8d173f9', 'authors': ['Jiale Wei', 'Xiang Ying', 'Tao Gao', 'Felix Tao', 'Jingbo Shang'], 'affiliations': ['Mindverse.ai'], 'pdf_title_img': 'assets/pdf/title_img/2503.08102.jpg', 'data': {'categories': ['#multimodal', '#agi', '#agents', '#optimization', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'SECOND ME: ИИ-ассистент для расширения вашей памяти', 'desc': 'SECOND ME - это система, использующая большие языковые модели для управления личной памятью пользователя. Она хранит, организует и динамически применяет пользовательские знания, выступая посредником во взаимодействиях. SECOND ME может автономно генерировать контекстно-зависимые ответы, заполнять формы и облегчать коммуникацию с внешними системами. В отличие от традиционных решений, система использует параметризацию памяти на основе нейросетей для более интеллектуального подхода к управлению информацией.'}, 'en': {'title': 'Revolutionizing Memory Management with AI: Meet SECOND ME!', 'desc': 'The paper introduces SECOND ME, an AI-driven memory management system designed to enhance user interactions with digital environments. It addresses the issue of redundant information sharing by autonomously storing and organizing user-specific knowledge, allowing for context-aware responses and prefilled information. Unlike traditional memory solutions, SECOND ME utilizes large language models to enable dynamic knowledge retrieval and contextual reasoning. This innovative approach aims to reduce cognitive load and improve the efficiency of human-AI interactions in various applications.'}, 'zh': {'title': '智能记忆管理，提升人机互动体验', 'desc': '这篇论文介绍了一个名为SECOND ME的智能记忆管理系统，旨在减少用户在与外部世界互动时的冗余信息输入。通过利用大型语言模型（LLMs），SECOND ME能够智能地存储、组织和动态使用用户特定的知识。与传统的记忆存储解决方案不同，SECOND ME不仅仅是静态数据的保留，而是通过上下文推理和自适应知识检索来优化用户体验。随着AI驱动的个人代理的普及，SECOND ME代表了增强人类与世界互动的重要一步。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (4)', '#agi (2)', '#alignment (2)', '#architecture (7)', '#audio (1)', '#benchmark (7)', '#cv (5)', '#data (3)', '#dataset (7)', '#diffusion (6)', '#ethics (1)', '#games (2)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (3)', '#interpretability (2)', '#leakage', '#long_context (2)', '#low_resource (1)', '#machine_translation', '#math (1)', '#multilingual (1)', '#multimodal (11)', '#open_source (7)', '#optimization (9)', '#plp', '#rag', '#reasoning (5)', '#rl (3)', '#rlhf (1)', '#robotics', '#science', '#security', '#small_models (1)', '#story_generation (1)', '#survey', '#synthetic (3)', '#training (12)', '#transfer_learning (1)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-03-12 07:10',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-12 07:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-12 07:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    