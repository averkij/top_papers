
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 31 papers. February 20.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">20 февраля</span> | <span id="title-articles-count">31 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-02-19.html">⬅️ <span id="prev-date">19.02</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-02-21.html">➡️ <span id="next-date">21.02</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-02.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '20 февраля', 'en': 'February 20', 'zh': '2月20日'};
        let feedDateNext = {'ru': '21.02', 'en': '02/21', 'zh': '2月21日'};
        let feedDatePrev = {'ru': '19.02', 'en': '02/19', 'zh': '2月19日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2502.12900', 'title': 'Soundwave: Less is More for Speech-Text Alignment in LLMs', 'url': 'https://huggingface.co/papers/2502.12900', 'abstract': 'Existing end-to-end speech large language models (LLMs) usually rely on large-scale annotated data for training, while data-efficient training has not been discussed in depth. We focus on two fundamental problems between speech and text: the representation space gap and sequence length inconsistency. We propose Soundwave, which utilizes an efficient training strategy and a novel architecture to address these issues. Results show that Soundwave outperforms the advanced Qwen2-Audio in speech translation and AIR-Bench speech tasks, using only one-fiftieth of the training data. Further analysis shows that Soundwave still retains its intelligence during conversation. The project is available at https://github.com/FreedomIntelligence/Soundwave.', 'score': 63, 'issue_id': 2289, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '95780ecdf251cffd', 'authors': ['Yuhao Zhang', 'Zhiheng Liu', 'Fan Bu', 'Ruiyu Zhang', 'Benyou Wang', 'Haizhou Li'], 'affiliations': ['The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2502.12900.jpg', 'data': {'categories': ['#training', '#audio', '#transfer_learning', '#open_source', '#optimization', '#data', '#architecture'], 'emoji': '🎙️', 'ru': {'title': 'Эффективное обучение речевых моделей с минимумом данных', 'desc': 'Исследователи представили Soundwave - новую модель обработки речи, которая решает проблемы разрыва пространства представлений и несоответствия длины последовательностей между речью и текстом. Модель использует эффективную стратегию обучения и инновационную архитектуру. Soundwave превосходит передовую модель Qwen2-Audio в задачах перевода речи и тестах AIR-Bench, используя всего 1/50 часть обучающих данных. При этом модель сохраняет свой интеллект во время диалога.'}, 'en': {'title': 'Soundwave: Efficient Speech Translation with Minimal Data', 'desc': 'This paper introduces Soundwave, a new model designed for speech translation that requires significantly less training data compared to existing large language models. It addresses two key challenges: the differences in how speech and text are represented and the varying lengths of sequences in speech data. By employing an efficient training strategy and a unique architecture, Soundwave achieves superior performance on speech tasks while using only 2% of the data needed by its competitors. The findings indicate that Soundwave maintains high conversational intelligence, making it a promising approach for data-efficient speech processing.'}, 'zh': {'title': '高效训练，语音翻译新突破！', 'desc': '现有的端到端语音大型语言模型通常依赖于大规模标注数据进行训练，而数据高效训练尚未深入探讨。我们关注语音和文本之间的两个基本问题：表示空间差距和序列长度不一致。我们提出了Soundwave，它利用高效的训练策略和新颖的架构来解决这些问题。结果表明，Soundwave在语音翻译和AIR-Bench语音任务中表现优于先进的Qwen2-Audio，仅使用了五十分之一的训练数据。'}}}, {'id': 'https://huggingface.co/papers/2502.13063', 'title': 'Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the Limits of Embedding Space Capacity', 'url': 'https://huggingface.co/papers/2502.13063', 'abstract': 'A range of recent works addresses the problem of compression of sequence of tokens into a shorter sequence of real-valued vectors to be used as inputs instead of token embeddings or key-value cache. These approaches allow to reduce the amount of compute in existing language models. Despite relying on powerful models as encoders, the maximum attainable lossless compression ratio is typically not higher than x10. This fact is highly intriguing because, in theory, the maximum information capacity of large real-valued vectors is far beyond the presented rates even for 16-bit precision and a modest vector size. In this work, we explore the limits of compression by replacing the encoder with a per-sample optimization procedure. We show that vectors with compression ratios up to x1500 exist, which highlights two orders of magnitude gap between existing and practically attainable solutions. Furthermore, we empirically show that the compression limits are determined not by the length of the input but by the amount of uncertainty to be reduced, namely, the cross-entropy loss on this sequence without any conditioning. The obtained limits highlight the substantial gap between the theoretical capacity of input embeddings and their practical utilization, suggesting significant room for optimization in model design.', 'score': 51, 'issue_id': 2293, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': 'bd5537cf011da83d', 'authors': ['Yuri Kuratov', 'Mikhail Arkhipov', 'Aydar Bulatov', 'Mikhail Burtsev'], 'affiliations': ['AIRI, Moscow, Russia', 'Independent Researcher, Amsterdam, Netherlands', 'London Institute for Mathematical Sciences, London, UK', 'Neural Networks and Deep Learning Lab, MIPT, Dolgoprudny, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2502.13063.jpg', 'data': {'categories': ['#inference', '#optimization', '#training'], 'emoji': '🗜️', 'ru': {'title': 'Раскрывая потенциал сверхсжатия токенов в языковых моделях', 'desc': 'Статья исследует возможности сжатия последовательностей токенов в более короткие последовательности векторов для использования в языковых моделях. Авторы показывают, что существующие методы достигают коэффициента сжатия не выше 10, хотя теоретически возможно гораздо большее сжатие. Используя оптимизацию для каждого образца, исследователи демонстрируют возможность сжатия до 1500 раз. Результаты указывают на значительный разрыв между теоретической ёмкостью входных эмбеддингов и их практическим использованием в современных нейронных сетях.'}, 'en': {'title': 'Unlocking Compression: From x10 to x1500 in Token Sequences', 'desc': 'This paper investigates the compression of token sequences into shorter real-valued vectors for use in language models. It reveals that while current methods achieve a maximum lossless compression ratio of about x10, a new optimization approach can reach ratios as high as x1500. The study emphasizes that the limits of compression are influenced more by the uncertainty in the data rather than the input length itself. This finding indicates a significant disparity between the theoretical potential of embeddings and their actual performance, suggesting opportunities for further optimization in model architecture.'}, 'zh': {'title': '压缩潜力：从10倍到1500倍的飞跃', 'desc': '这篇论文探讨了将序列的标记压缩为更短的实值向量序列的问题，以便用作输入，而不是使用标记嵌入或键值缓存。尽管现有的编码器模型非常强大，但无损压缩的最大比率通常不超过10倍。研究表明，通过每个样本的优化程序替代编码器，可以实现高达1500倍的压缩比，显示出现有解决方案与可实际达到的解决方案之间的巨大差距。此外，压缩的限制主要由需要减少的不确定性决定，而不是输入的长度，这为模型设计的优化提供了重要的空间。'}}}, {'id': 'https://huggingface.co/papers/2502.11564', 'title': 'Continuous Diffusion Model for Language Modeling', 'url': 'https://huggingface.co/papers/2502.11564', 'abstract': 'Diffusion models have emerged as a promising alternative to autoregressive models in modeling discrete categorical data. Yet diffusion models that directly work on discrete data space do not fully exploit the power of iterative refinement, as the signals are lost during the transition between discrete states. Existing continuous diffusion models for discrete data have limited performance compared to discrete approaches, and the unclear link between them restricts the development of diffusion models for discrete data. In this work, we propose a continuous diffusion model for language modeling that incorporates the geometry of the underlying categorical distribution. We establish a connection between the discrete diffusion and continuous flow on the statistical manifold, and building on the analogy, we introduce a simple design for the diffusion process that generalizes previous discrete diffusion models. We further propose a simulation-free training framework based on radial symmetry and a simple technique to address the high dimensionality of the manifold. Comprehensive experiments on language modeling benchmarks and other modalities show that our method outperforms existing discrete diffusion models and approaches the performance of autoregressive models. Codes available at https://github.com/harryjo97/RDLM{https://github.com/harryjo97/RDLM}.', 'score': 42, 'issue_id': 2287, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': 'f5889d3a88d24b7c', 'authors': ['Jaehyeong Jo', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'Korea Advanced Institute of Science and Technology (KAIST)'], 'pdf_title_img': 'assets/pdf/title_img/2502.11564.jpg', 'data': {'categories': ['#training', '#benchmark', '#optimization', '#dataset', '#diffusion', '#architecture'], 'emoji': '🌊', 'ru': {'title': 'Непрерывная диффузия на статистическом многообразии для языкового моделирования', 'desc': 'Статья представляет новый подход к моделированию естественного языка с использованием непрерывных диффузионных моделей. Авторы устанавливают связь между дискретной диффузией и непрерывным потоком на статистическом многообразии. Предложенный метод обобщает предыдущие дискретные диффузионные модели и вводит безсимуляционную схему обучения. Эксперименты показывают, что данный подход превосходит существующие дискретные диффузионные модели и приближается к производительности авторегрессионных моделей в задачах языкового моделирования.'}, 'en': {'title': 'Revolutionizing Language Modeling with Continuous Diffusion', 'desc': 'This paper presents a new continuous diffusion model designed for language modeling, which effectively handles discrete categorical data. The authors highlight the limitations of existing discrete diffusion models and propose a method that leverages the geometry of categorical distributions to enhance performance. By establishing a connection between discrete diffusion and continuous flow on a statistical manifold, they introduce a novel design that improves iterative refinement. Their experiments demonstrate that this approach not only surpasses traditional discrete models but also approaches the performance of autoregressive models.'}, 'zh': {'title': '连续扩散模型：提升离散数据建模的性能', 'desc': '扩散模型作为一种新兴的替代自回归模型的方法，在建模离散分类数据方面展现了良好的前景。现有的连续扩散模型在处理离散数据时性能有限，且二者之间的联系不明确，限制了扩散模型的发展。本文提出了一种新的连续扩散模型，结合了基础分类分布的几何特性，并建立了离散扩散与连续流动之间的联系。通过全面的实验，我们的方法在语言建模基准测试中超越了现有的离散扩散模型，接近自回归模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2502.11079', 'title': 'Phantom: Subject-consistent video generation via cross-modal alignment', 'url': 'https://huggingface.co/papers/2502.11079', 'abstract': 'The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-consistent video through textual instructions. We believe that the essence of subject-to-video lies in balancing the dual-modal prompts of text and image, thereby deeply and simultaneously aligning both text and visual content. To this end, we propose Phantom, a unified video generation framework for both single and multi-subject references. Building on existing text-to-video and image-to-video architectures, we redesign the joint text-image injection model and drive it to learn cross-modal alignment via text-image-video triplet data. In particular, we emphasize subject consistency in human generation, covering existing ID-preserving video generation while offering enhanced advantages. The project homepage is here https://phantom-video.github.io/Phantom/.', 'score': 41, 'issue_id': 2286, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 февраля', 'en': 'February 16', 'zh': '2月16日'}, 'hash': 'e443a635e58b164f', 'authors': ['Lijie Liu', 'Tianxiang Ma', 'Bingchuan Li', 'Zhuowei Chen', 'Jiawei Liu', 'Qian He', 'Xinglong Wu'], 'affiliations': ['Intelligent Creation Team, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2502.11079.jpg', 'data': {'categories': ['#architecture', '#video', '#multimodal'], 'emoji': '🎥', 'ru': {'title': 'Phantom: баланс текста и изображения для создания согласованного видео', 'desc': 'Эта статья представляет Phantom - унифицированную систему генерации видео на основе текстовых и визуальных подсказок. Авторы предлагают новый подход к созданию видео с сохранением характеристик субъектов из опорных изображений. Phantom переосмысливает модель совместной обработки текста и изображений, обучаясь на триплетах текст-изображение-видео. Особое внимание уделяется сохранению идентичности людей при генерации видео.'}, 'en': {'title': 'Phantom: Consistent Video Generation from Text and Images', 'desc': 'This paper introduces a new approach called Subject-to-Video, which focuses on generating videos that maintain consistency with specific subjects extracted from reference images. The authors present Phantom, a unified framework that integrates both text and image inputs to create videos, ensuring that the generated content aligns well with the provided prompts. By utilizing a triplet data structure of text, image, and video, Phantom enhances the learning of cross-modal relationships, improving the quality of video generation. The framework particularly excels in generating videos of humans while preserving their identity across frames, marking a significant advancement in video generation technology.'}, 'zh': {'title': '实现主题一致性的视频生成', 'desc': '这篇论文介绍了一种新的视频生成模型，称为Phantom，旨在实现主题一致性的视频生成。该模型通过提取参考图像中的主题元素，并结合文本指令生成视频。Phantom框架能够处理单一和多个主题的参考，强调文本和图像的双模态提示的平衡。研究者们通过文本-图像-视频三元组数据来学习跨模态对齐，从而提升人类生成视频的主题一致性。'}}}, {'id': 'https://huggingface.co/papers/2502.13131', 'title': 'Rethinking Diverse Human Preference Learning through Principal Component Analysis', 'url': 'https://huggingface.co/papers/2502.13131', 'abstract': 'Understanding human preferences is crucial for improving foundation models and building personalized AI systems. However, preferences are inherently diverse and complex, making it difficult for traditional reward models to capture their full range. While fine-grained preference data can help, collecting it is expensive and hard to scale. In this paper, we introduce Decomposed Reward Models (DRMs), a novel approach that extracts diverse human preferences from binary comparisons without requiring fine-grained annotations. Our key insight is to represent human preferences as vectors and analyze them using Principal Component Analysis (PCA). By constructing a dataset of embedding differences between preferred and rejected responses, DRMs identify orthogonal basis vectors that capture distinct aspects of preference. These decomposed rewards can be flexibly combined to align with different user needs, offering an interpretable and scalable alternative to traditional reward models. We demonstrate that DRMs effectively extract meaningful preference dimensions (e.g., helpfulness, safety, humor) and adapt to new users without additional training. Our results highlight DRMs as a powerful framework for personalized and interpretable LLM alignment.', 'score': 32, 'issue_id': 2286, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': 'b3376cde29e0b44f', 'authors': ['Feng Luo', 'Rui Yang', 'Hao Sun', 'Chunyuan Deng', 'Jiarui Yao', 'Jingyan Shen', 'Huan Zhang', 'Hanjie Chen'], 'affiliations': ['Columbia University', 'Rice University', 'University of Cambridge', 'University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2502.13131.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#training', '#dataset', '#interpretability'], 'emoji': '🧩', 'ru': {'title': 'Разложение предпочтений для персонализированного обучения языковых моделей', 'desc': 'Статья представляет новый подход к извлечению разнообразных человеческих предпочтений из бинарных сравнений без необходимости в детальных аннотациях - Decomposed Reward Models (DRMs). Метод использует векторное представление предпочтений и анализ методом главных компонент (PCA) для выявления ортогональных базисных векторов, отражающих различные аспекты предпочтений. DRMs позволяют гибко комбинировать разложенные награды для адаптации к потребностям разных пользователей, предлагая интерпретируемую и масштабируемую альтернативу традиционным моделям вознаграждения. Эксперименты показывают, что DRMs эффективно извлекают значимые измерения предпочтений и адаптируются к новым пользователям без дополнительного обучения.'}, 'en': {'title': 'Decomposed Reward Models: Unlocking Diverse Human Preferences for Personalized AI', 'desc': 'This paper presents Decomposed Reward Models (DRMs), a new method for understanding human preferences in AI systems. Instead of relying on detailed preference data, DRMs use binary comparisons to extract diverse preferences, making the process more scalable and cost-effective. By representing preferences as vectors and applying Principal Component Analysis (PCA), the model identifies key dimensions of preference that can be combined to meet different user needs. The results show that DRMs can effectively adapt to new users and provide interpretable insights into preferences like helpfulness and safety.'}, 'zh': {'title': '分解奖励模型：个性化AI的新方法', 'desc': '理解人类偏好对于改善基础模型和构建个性化AI系统至关重要。传统的奖励模型难以捕捉偏好的多样性和复杂性。我们提出了一种新方法，称为分解奖励模型（DRMs），它通过二元比较提取人类偏好，而无需细粒度的注释。DRMs利用主成分分析（PCA）分析偏好向量，能够灵活组合以满足不同用户需求，提供了一种可解释且可扩展的替代方案。'}}}, {'id': 'https://huggingface.co/papers/2502.13130', 'title': 'Magma: A Foundation Model for Multimodal AI Agents', 'url': 'https://huggingface.co/papers/2502.13130', 'abstract': 'We present Magma, a foundation model that serves multimodal AI agentic tasks in both the digital and physical worlds. Magma is a significant extension of vision-language (VL) models in that it not only retains the VL understanding ability (verbal intelligence) of the latter, but is also equipped with the ability to plan and act in the visual-spatial world (spatial-temporal intelligence) and complete agentic tasks ranging from UI navigation to robot manipulation. To endow the agentic capabilities, Magma is pretrained on large amounts of heterogeneous datasets spanning from images, videos to robotics data, where the actionable visual objects (e.g., clickable buttons in GUI) in images are labeled by Set-of-Mark (SoM) for action grounding, and the object movements (e.g., the trace of human hands or robotic arms) in videos are labeled by Trace-of-Mark (ToM) for action planning. Extensive experiments show that SoM and ToM reach great synergy and facilitate the acquisition of spatial-temporal intelligence for our Magma model, which is fundamental to a wide range of tasks as shown in Fig.1. In particular, Magma creates new state-of-the-art results on UI navigation and robotic manipulation tasks, outperforming previous models that are specifically tailored to these tasks. On image and video-related multimodal tasks, Magma also compares favorably to popular large multimodal models that are trained on much larger datasets. We make our model and code public for reproducibility at https://microsoft.github.io/Magma.', 'score': 29, 'issue_id': 2288, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '1851b242ac65c88f', 'authors': ['Jianwei Yang', 'Reuben Tan', 'Qianhui Wu', 'Ruijie Zheng', 'Baolin Peng', 'Yongyuan Liang', 'Yu Gu', 'Mu Cai', 'Seonghyeon Ye', 'Joel Jang', 'Yuquan Deng', 'Lars Liden', 'Jianfeng Gao'], 'affiliations': ['KAIST', 'Microsoft Research', 'University of Maryland', 'University of Washington', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2502.13130.jpg', 'data': {'categories': ['#cv', '#robotics', '#agi', '#multimodal', '#agents', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'Magma: мультимодальный ИИ-агент для цифрового и физического мира', 'desc': 'Статья представляет Magma - новую мультимодальную модель искусственного интеллекта, способную выполнять агентные задачи как в цифровом, так и в физическом мире. Модель расширяет возможности существующих vision-language моделей, добавляя способность планировать и действовать в визуально-пространственном мире. Magma обучена на больших гетерогенных наборах данных, включая изображения, видео и робототехнические данные, с использованием специальных методов разметки Set-of-Mark и Trace-of-Mark. Эксперименты показывают, что Magma достигает новых передовых результатов в задачах навигации по пользовательскому интерфейсу и манипуляции роботами.'}, 'en': {'title': 'Magma: Bridging Digital and Physical Worlds with Multimodal Intelligence', 'desc': 'Magma is a foundation model designed for multimodal AI tasks that operate in both digital and physical environments. It enhances traditional vision-language models by integrating spatial-temporal intelligence, allowing it to plan and execute actions in real-world scenarios. The model is pretrained on diverse datasets, utilizing Set-of-Mark (SoM) for action grounding and Trace-of-Mark (ToM) for action planning, which together improve its ability to understand and interact with visual-spatial elements. Magma achieves state-of-the-art performance in UI navigation and robotic manipulation, surpassing specialized models and competing effectively with larger multimodal models.'}, 'zh': {'title': 'Magma：多模态智能的未来', 'desc': 'Magma是一个基础模型，能够处理数字和物理世界中的多模态人工智能任务。它不仅保留了视觉-语言模型的理解能力，还具备在视觉空间中规划和行动的能力。Magma通过大量异构数据集进行预训练，这些数据集包括图像、视频和机器人数据，使用Set-of-Mark和Trace-of-Mark进行动作标定。实验表明，Magma在用户界面导航和机器人操作任务上创造了新的最先进结果，超越了专门为这些任务设计的模型。'}}}, {'id': 'https://huggingface.co/papers/2502.13145', 'title': 'Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation', 'url': 'https://huggingface.co/papers/2502.13145', 'abstract': "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable performance but face deployment challenges due to their quadratic computational complexity, growing Key-Value cache requirements, and reliance on separate vision encoders. We propose mmMamba, a framework for developing linear-complexity native multimodal state space models through progressive distillation from existing MLLMs using moderate academic computational resources. Our approach enables the direct conversion of trained decoder-only MLLMs to linear-complexity architectures without requiring pre-trained RNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba from trained Transformer and a three-stage distillation recipe, which can effectively transfer the knowledge from Transformer to Mamba while preserving multimodal capabilities. Our method also supports flexible hybrid architectures that combine Transformer and Mamba layers for customizable efficiency-performance trade-offs. Distilled from the Transformer-based decoder-only HoVLE, mmMamba-linear achieves competitive performance against existing linear and quadratic-complexity VLMs, while mmMamba-hybrid further improves performance significantly, approaching HoVLE's capabilities. At 103K tokens, mmMamba-linear demonstrates 20.6times speedup and 75.8% GPU memory reduction compared to HoVLE, while mmMamba-hybrid achieves 13.5times speedup and 60.2% memory savings. Code and models are released at https://github.com/hustvl/mmMamba", 'score': 27, 'issue_id': 2286, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '6810113d41bfd26d', 'authors': ['Bencheng Liao', 'Hongyuan Tao', 'Qian Zhang', 'Tianheng Cheng', 'Yingyue Li', 'Haoran Yin', 'Wenyu Liu', 'Xinggang Wang'], 'affiliations': ['Horizon Robotics', 'Institute of Artificial Intelligence, Huazhong University of Science & Technology', 'School of EIC, Huazhong University of Science & Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.13145.jpg', 'data': {'categories': ['#open_source', '#optimization', '#transfer_learning', '#architecture', '#training', '#multimodal'], 'emoji': '🚀', 'ru': {'title': 'mmMamba: эффективные мультимодальные модели с линейной сложностью', 'desc': 'Статья представляет mmMamba - фреймворк для разработки мультимодальных моделей с линейной сложностью на основе state space models. Авторы предлагают метод прогрессивной дистилляции знаний от существующих мультимодальных языковых моделей (MLLM) к линейным архитектурам без использования предобученных RNN-моделей или энкодеров изображений. Предложенный подход включает стратегию инициализации Mamba из обученного Transformer и трехэтапный рецепт дистилляции. Результаты показывают, что mmMamba достигает конкурентоспособной производительности по сравнению с существующими моделями, обеспечивая значительное ускорение и экономию памяти.'}, 'en': {'title': 'Efficient Multimodal Models with mmMamba', 'desc': 'The paper introduces mmMamba, a new framework designed to create efficient multimodal large language models (MLLMs) with linear computational complexity. It utilizes a progressive distillation process to convert existing decoder-only MLLMs into more efficient architectures without needing separate vision encoders or pre-trained RNNs. The proposed seeding strategy and three-stage distillation recipe allow for effective knowledge transfer from Transformer models while maintaining multimodal capabilities. The results show that mmMamba-linear and mmMamba-hybrid significantly outperform traditional models in terms of speed and memory usage, making them suitable for practical deployment.'}, 'zh': {'title': 'mmMamba：高效的多模态模型架构', 'desc': '最近的多模态大型语言模型（MLLMs）在性能上取得了显著进展，但由于其计算复杂度呈平方增长、对键值缓存的需求增加以及依赖于独立的视觉编码器，面临部署挑战。我们提出了mmMamba框架，通过从现有的MLLMs进行渐进蒸馏，开发线性复杂度的本地多模态状态空间模型，使用适度的学术计算资源。该方法允许将训练好的仅解码器MLLMs直接转换为线性复杂度架构，而无需预训练的基于RNN的LLM或视觉编码器。我们的蒸馏策略有效地将知识从Transformer转移到Mamba，同时保留多模态能力，并支持灵活的混合架构，以实现可定制的效率与性能权衡。'}}}, {'id': 'https://huggingface.co/papers/2502.13143', 'title': 'SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation', 'url': 'https://huggingface.co/papers/2502.13143', 'abstract': "Spatial intelligence is a critical component of embodied AI, promoting robots to understand and interact with their environments. While recent advances have enhanced the ability of VLMs to perceive object locations and positional relationships, they still lack the capability to precisely understand object orientations-a key requirement for tasks involving fine-grained manipulations. Addressing this limitation not only requires geometric reasoning but also an expressive and intuitive way to represent orientation. In this context, we propose that natural language offers a more flexible representation space than canonical frames, making it particularly suitable for instruction-following robotic systems. In this paper, we introduce the concept of semantic orientation, which defines object orientations using natural language in a reference-frame-free manner (e.g., the ''plug-in'' direction of a USB or the ''handle'' direction of a knife). To support this, we construct OrienText300K, a large-scale dataset of 3D models annotated with semantic orientations that link geometric understanding to functional semantics. By integrating semantic orientation into a VLM system, we enable robots to generate manipulation actions with both positional and orientational constraints. Extensive experiments in simulation and real world demonstrate that our approach significantly enhances robotic manipulation capabilities, e.g., 48.7% accuracy on Open6DOR and 74.9% accuracy on SIMPLER.", 'score': 26, 'issue_id': 2286, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '7aab95a55286d8b2', 'authors': ['Zekun Qi', 'Wenyao Zhang', 'Yufei Ding', 'Runpei Dong', 'Xinqiang Yu', 'Jingwen Li', 'Lingyun Xu', 'Baoyu Li', 'Xialin He', 'Guofan Fan', 'Jiazhao Zhang', 'Jiawei He', 'Jiayuan Gu', 'Xin Jin', 'Kaisheng Ma', 'Zhizheng Zhang', 'He Wang', 'Li Yi'], 'affiliations': ['Eastern Institute of Technology', 'Galbot', 'Peking University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Shanghai Qi Zhi Institute', 'ShanghaiTech University', 'Tsinghua University', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2502.13143.jpg', 'data': {'categories': ['#3d', '#games', '#dataset', '#reasoning', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Семантическая ориентация: новый подход к пространственному интеллекту роботов', 'desc': 'Статья представляет концепцию семантической ориентации для улучшения пространственного интеллекта роботов. Авторы создали датасет OrienText300K с 3D моделями, аннотированными семантическими ориентациями на естественном языке. Интеграция семантической ориентации в систему с визуально-языковой моделью (VLM) позволяет роботам генерировать действия с учетом позиционных и ориентационных ограничений. Эксперименты показали значительное улучшение возможностей роботов в манипуляциях с объектами.'}, 'en': {'title': 'Empowering Robots with Semantic Orientation for Enhanced Manipulation', 'desc': 'This paper addresses the challenge of teaching robots to understand object orientations, which is essential for precise manipulation tasks. It introduces the concept of semantic orientation, allowing robots to interpret orientations using natural language instead of traditional geometric frames. The authors present OrienText300K, a dataset of 3D models annotated with these semantic orientations, linking geometric understanding to functional semantics. By incorporating this approach into vision-language models (VLMs), the paper demonstrates improved accuracy in robotic manipulation tasks, showcasing the effectiveness of using natural language for orientation representation.'}, 'zh': {'title': '用自然语言提升机器人的空间智能', 'desc': '空间智能是具身人工智能的重要组成部分，帮助机器人理解和与环境互动。尽管最近的进展提高了视觉语言模型（VLMs）对物体位置和关系的感知能力，但它们仍然缺乏精确理解物体方向的能力，这对于细致操作任务至关重要。为了解决这个问题，我们提出了语义方向的概念，使用自然语言以无参考框架的方式定义物体方向。通过构建OrienText300K数据集，我们将几何理解与功能语义联系起来，从而提升机器人在操作中的能力。'}}}, {'id': 'https://huggingface.co/papers/2502.12464', 'title': 'SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models', 'url': 'https://huggingface.co/papers/2502.12464', 'abstract': 'Deploying large language models (LLMs) in real-world applications requires robust safety guard models to detect and block harmful user prompts. While large safety guard models achieve strong performance, their computational cost is substantial. To mitigate this, smaller distilled models are used, but they often underperform on "hard" examples where the larger model provides accurate predictions. We observe that many inputs can be reliably handled by the smaller model, while only a small fraction require the larger model\'s capacity. Motivated by this, we propose SafeRoute, a binary router that distinguishes hard examples from easy ones. Our method selectively applies the larger safety guard model to the data that the router considers hard, improving efficiency while maintaining accuracy compared to solely using the larger safety guard model. Experimental results on multiple benchmark datasets demonstrate that our adaptive model selection significantly enhances the trade-off between computational cost and safety performance, outperforming relevant baselines.', 'score': 25, 'issue_id': 2288, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '5edcf5af6c8edecb', 'authors': ['Seanie Lee', 'Dong Bok Lee', 'Dominik Wagner', 'Minki Kang', 'Haebin Seong', 'Tobias Bocklet', 'Juho Lee', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST', 'Technische Hochschule Nürnberg Georg Simon Ohm'], 'pdf_title_img': 'assets/pdf/title_img/2502.12464.jpg', 'data': {'categories': ['#security', '#benchmark', '#training', '#inference', '#optimization'], 'emoji': '🛡️', 'ru': {'title': 'Умная маршрутизация для эффективной защиты языковых моделей', 'desc': "Статья предлагает метод SafeRoute для повышения эффективности моделей безопасности в крупных языковых моделях (LLM). SafeRoute использует бинарный маршрутизатор для разделения входных данных на 'простые' и 'сложные'. Сложные примеры обрабатываются большой моделью безопасности, а простые - меньшей дистиллированной моделью. Этот подход позволяет сохранить точность крупной модели при значительном снижении вычислительных затрат. Эксперименты на нескольких наборах данных показывают преимущество SafeRoute перед базовыми методами."}, 'en': {'title': 'Smart Routing for Safer AI: Balancing Efficiency and Accuracy', 'desc': "This paper introduces SafeRoute, a binary router designed to improve the efficiency of safety guard models used with large language models (LLMs). The router identifies which inputs are 'hard' and require the computational power of a larger safety model, while 'easy' inputs can be handled by smaller, more efficient models. By selectively applying the larger model only to challenging cases, SafeRoute reduces overall computational costs without sacrificing safety performance. Experimental results show that this adaptive approach significantly enhances the balance between resource usage and accuracy compared to using only the larger model."}, 'zh': {'title': '智能路由，提升安全与效率！', 'desc': '在实际应用中部署大型语言模型（LLMs）需要强大的安全防护模型来检测和阻止有害的用户提示。虽然大型安全防护模型的性能很强，但其计算成本也很高。为了解决这个问题，研究者们使用了较小的蒸馏模型，但在处理“困难”示例时，它们的表现往往不如大型模型。我们提出了SafeRoute，一个二元路由器，可以区分困难示例和简单示例，从而提高效率，同时保持准确性。'}}}, {'id': 'https://huggingface.co/papers/2502.09245', 'title': "You Do Not Fully Utilize Transformer's Representation Capacity", 'url': 'https://huggingface.co/papers/2502.09245', 'abstract': "In contrast to RNNs, which compress previous tokens into a single hidden state, Transformers can attend to all previous tokens directly. However, standard Transformers only use representations from the immediately preceding layer. In this paper, we show that this design choice causes representation collapse and leads to suboptimal performance. To address this issue, we introduce Layer-Integrated Memory (LIMe), a simple yet powerful approach that preserves the model's overall memory footprint while expanding its representational capacity by allowing access to hidden states from earlier layers. Through extensive experiments across various architectures and different lookup mechanisms, we demonstrate consistent performance improvements on a wide range of tasks. Moreover, our analysis of the learned representation dynamics and our exploration of depthwise circuits reveal how LIMe integrates information across layers, pointing to promising directions for future research.", 'score': 24, 'issue_id': 2291, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': '8526a2bbb83d3754', 'authors': ['Gleb Gerasimov', 'Yaroslav Aksenov', 'Nikita Balagansky', 'Viacheslav Sinii', 'Daniil Gavrilov'], 'affiliations': ['HSE University', 'Moscow Institute of Physics and Technology', 'T-Tech'], 'pdf_title_img': 'assets/pdf/title_img/2502.09245.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'LIMe: расширение памяти трансформеров для лучшей производительности', 'desc': 'Статья представляет новый подход к архитектуре трансформеров под названием Layer-Integrated Memory (LIMe). В отличие от стандартных трансформеров, которые используют только представления из предыдущего слоя, LIMe позволяет обращаться к скрытым состояниям из более ранних слоев. Это решение помогает избежать проблемы схлопывания представлений и улучшает производительность модели. Эксперименты показали, что LIMe последовательно улучшает результаты на широком спектре задач без увеличения общего объема памяти модели.'}, 'en': {'title': 'Unlocking Transformer Potential with Layer-Integrated Memory', 'desc': "This paper discusses the limitations of standard Transformers, which only utilize information from the most recent layer, leading to representation collapse and reduced performance. The authors propose a new method called Layer-Integrated Memory (LIMe) that allows access to hidden states from earlier layers, enhancing the model's representational capacity without increasing memory usage. Through various experiments, they show that LIMe consistently improves performance across different tasks and architectures. Additionally, the paper explores how LIMe integrates information across layers, suggesting new avenues for future research in model design."}, 'zh': {'title': '层集成记忆：提升变换器性能的新方法', 'desc': '与递归神经网络（RNN）不同，变换器（Transformers）可以直接关注所有之前的标记。然而，标准的变换器仅使用来自前一层的表示，这种设计选择导致了表示崩溃，从而影响了性能。为了解决这个问题，我们提出了一种名为层集成记忆（LIMe）的方法，它在保持模型整体内存占用的同时，扩展了表示能力，允许访问早期层的隐藏状态。通过在各种架构和不同查找机制上的广泛实验，我们展示了在多种任务上的一致性能提升。'}}}, {'id': 'https://huggingface.co/papers/2502.11433', 'title': 'FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading', 'url': 'https://huggingface.co/papers/2502.11433', 'abstract': 'Large language models (LLMs) fine-tuned on multimodal financial data have demonstrated impressive reasoning capabilities in various financial tasks. However, they often struggle with multi-step, goal-oriented scenarios in interactive financial markets, such as trading, where complex agentic approaches are required to improve decision-making. To address this, we propose FLAG-Trader, a unified architecture integrating linguistic processing (via LLMs) with gradient-driven reinforcement learning (RL) policy optimization, in which a partially fine-tuned LLM acts as the policy network, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning. Through policy gradient optimization driven by trading rewards, our framework not only enhances LLM performance in trading but also improves results on other financial-domain tasks. We present extensive empirical evidence to validate these enhancements.', 'score': 24, 'issue_id': 2286, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': '0731051fe5131889', 'authors': ['Guojun Xiong', 'Zhiyang Deng', 'Keyi Wang', 'Yupeng Cao', 'Haohang Li', 'Yangyang Yu', 'Xueqing Peng', 'Mingquan Lin', 'Kaleb E Smith', 'Xiao-Yang Liu', 'Jimin Huang', 'Sophia Ananiadou', 'Qianqian Xie'], 'affiliations': ['Columbia University', 'Harvard University', 'NVIDIA', 'Rensselaer Polytechnic Institute', 'Stevens Institute of Technology', 'TheFinAI', 'University of Manchester', 'University of Minnesota'], 'pdf_title_img': 'assets/pdf/title_img/2502.11433.jpg', 'data': {'categories': ['#optimization', '#rlhf', '#architecture', '#training', '#reasoning', '#rl', '#multimodal'], 'emoji': '📈', 'ru': {'title': 'Усиление торговых стратегий с помощью языковых моделей и обучения с подкреплением', 'desc': 'В статье представлена архитектура FLAG-Trader, объединяющая языковую обработку с помощью больших языковых моделей (LLM) и обучение с подкреплением для оптимизации торговых стратегий. LLM используется в качестве политики, адаптируясь к финансовой области через эффективную дообучение. Метод улучшает принятие решений в интерактивных финансовых сценариях, таких как торговля. Эмпирические результаты подтверждают эффективность подхода не только в торговле, но и в других финансовых задачах.'}, 'en': {'title': 'Empowering Financial Trading with FLAG-Trader: Merging Language and Reinforcement Learning', 'desc': 'This paper introduces FLAG-Trader, a new architecture that combines large language models (LLMs) with reinforcement learning (RL) to improve decision-making in financial trading. The approach uses a partially fine-tuned LLM as a policy network, allowing it to utilize its pre-trained knowledge while adapting specifically to financial tasks. By applying policy gradient optimization based on trading rewards, FLAG-Trader enhances the performance of LLMs in trading scenarios and other financial applications. The authors provide extensive empirical evidence demonstrating the effectiveness of their proposed method.'}, 'zh': {'title': 'FLAG-Trader：提升金融决策的智能交易架构', 'desc': '本文提出了一种名为FLAG-Trader的统一架构，旨在提升大型语言模型（LLMs）在金融市场中的决策能力。该架构结合了语言处理和基于梯度的强化学习（RL）策略优化，使得部分微调的LLM可以作为策略网络，利用预训练知识并适应金融领域。通过交易奖励驱动的策略梯度优化，FLAG-Trader不仅提高了LLM在交易中的表现，还改善了其他金融领域任务的结果。我们提供了大量实证证据来验证这些改进。'}}}, {'id': 'https://huggingface.co/papers/2502.12513', 'title': 'RealSyn: An Effective and Scalable Multimodal Interleaved Document Transformation Paradigm', 'url': 'https://huggingface.co/papers/2502.12513', 'abstract': 'After pre-training on extensive image-text pairs, Contrastive Language-Image Pre-training (CLIP) demonstrates promising performance on a wide variety of benchmarks. However, a substantial volume of non-paired data, such as multimodal interleaved documents, remains underutilized for vision-language representation learning. To fully leverage these unpaired documents, we initially establish a Real-World Data Extraction pipeline to extract high-quality images and texts. Then we design a hierarchical retrieval method to efficiently associate each image with multiple semantically relevant realistic texts. To further enhance fine-grained visual information, we propose an image semantic augmented generation module for synthetic text production. Furthermore, we employ a semantic balance sampling strategy to improve dataset diversity, enabling better learning of long-tail concepts. Based on these innovations, we construct RealSyn, a dataset combining realistic and synthetic texts, available in three scales: 15M, 30M, and 100M. Extensive experiments demonstrate that RealSyn effectively advances vision-language representation learning and exhibits strong scalability. Models pre-trained on RealSyn achieve state-of-the-art performance on multiple downstream tasks. To facilitate future research, the RealSyn dataset and pre-trained model weights are released at https://github.com/deepglint/RealSyn.', 'score': 14, 'issue_id': 2286, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '1ed14365f683281c', 'authors': ['Tiancheng Gu', 'Kaicheng Yang', 'Chaoyi Zhang', 'Yin Xie', 'Xiang An', 'Ziyong Feng', 'Dongnan Liu', 'Weidong Cai', 'Jiankang Deng'], 'affiliations': ['DeepGlint', 'Imperial College London', 'The University of Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2502.12513.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#data', '#synthetic', '#dataset', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'RealSyn: Улучшение мультимодального обучения с помощью реальных и синтетических данных', 'desc': 'Статья представляет новый подход к обучению мультимодальных моделей, использующих изображения и текст. Авторы разработали метод RealSyn, который извлекает высококачественные данные из непарных документов и создает синтетические тексты для улучшения визуальной информации. Они также применили стратегию семантически сбалансированной выборки для повышения разнообразия данных. Эксперименты показали, что модели, предобученные на датасете RealSyn, достигают наилучших результатов в различных задачах компьютерного зрения и обработки естественного языка.'}, 'en': {'title': 'Unlocking Vision-Language Learning with RealSyn Dataset', 'desc': 'This paper introduces RealSyn, a new dataset designed to improve vision-language representation learning by utilizing both realistic and synthetic texts. The authors develop a data extraction pipeline to gather high-quality images and texts from unpaired multimodal documents. They also implement a hierarchical retrieval method to link images with relevant texts and propose an image semantic augmented generation module to create synthetic text. The resulting dataset, RealSyn, shows significant improvements in model performance on various tasks, demonstrating its effectiveness and scalability in the field of machine learning.'}, 'zh': {'title': '利用未配对数据提升视觉-语言学习的创新方法', 'desc': '这篇论文介绍了一种新的数据集RealSyn，用于视觉-语言表示学习。研究者们通过提取高质量的图像和文本，利用未配对的数据来提升模型的性能。为了更好地关联图像和文本，设计了一种层次检索方法，并提出了图像语义增强生成模块来生成合成文本。实验结果表明，基于RealSyn预训练的模型在多个下游任务上达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2502.11271', 'title': 'OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning', 'url': 'https://huggingface.co/papers/2502.11271', 'abstract': "Solving complex reasoning tasks may involve visual understanding, domain knowledge retrieval, numerical calculation, and multi-step reasoning. Existing methods augment large language models (LLMs) with external tools but are restricted to specialized domains, limited tool types, or require additional training data. In this paper, we introduce OctoTools, a training-free, user-friendly, and easily extensible open-source agentic framework designed to tackle complex reasoning across diverse domains. OctoTools introduces standardized tool cards to encapsulate tool functionality, a planner for both high-level and low-level planning, and an executor to carry out tool usage. We validate OctoTools' generality across 16 diverse tasks (including MathVista, MMLU-Pro, MedQA, and GAIA-Text), achieving substantial average accuracy gains of 9.3% over GPT-4o. Furthermore, OctoTools outperforms AutoGen, GPT-Functions and LangChain by up to 10.6% when given the same set of tools. Through comprehensive analysis and ablations, OctoTools demonstrates advantages in task planning, effective tool usage, and multi-step problem solving.", 'score': 10, 'issue_id': 2291, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 февраля', 'en': 'February 16', 'zh': '2月16日'}, 'hash': '1477460cf6641c67', 'authors': ['Pan Lu', 'Bowen Chen', 'Sheng Liu', 'Rahul Thapa', 'Joseph Boen', 'James Zou'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2502.11271.jpg', 'data': {'categories': ['#reasoning', '#agents', '#multimodal', '#open_source', '#training'], 'emoji': '🐙', 'ru': {'title': 'OctoTools: универсальный агент для сложных рассуждений с помощью ИИ', 'desc': 'Статья представляет OctoTools - универсальный фреймворк для решения сложных задач рассуждения в различных областях. OctoTools использует стандартизированные карточки инструментов, планировщик для высокоуровневого и низкоуровневого планирования, и исполнитель для применения инструментов. Фреймворк показывает значительное повышение точности на 9.3% по сравнению с GPT-4 на 16 разнообразных задачах. OctoTools также превосходит другие подходы, такие как AutoGen и LangChain, демонстрируя преимущества в планировании задач и эффективном использовании инструментов.'}, 'en': {'title': 'OctoTools: Empowering LLMs for Complex Reasoning Tasks', 'desc': 'This paper presents OctoTools, an innovative framework designed to enhance large language models (LLMs) in solving complex reasoning tasks without the need for additional training. OctoTools features standardized tool cards that define the capabilities of various tools, a planner for organizing tasks, and an executor to implement the planned actions. The framework has been tested across 16 different tasks, showing an impressive average accuracy improvement of 9.3% compared to GPT-4o. Additionally, OctoTools outperforms other existing methods like AutoGen and LangChain by up to 10.6%, highlighting its effectiveness in multi-step reasoning and tool utilization.'}, 'zh': {'title': 'OctoTools：跨领域复杂推理的新解决方案', 'desc': '本论文介绍了一种名为OctoTools的开源框架，旨在解决复杂推理任务。OctoTools不需要额外的训练数据，用户友好且易于扩展，能够在多个领域中应用。它通过标准化的工具卡片来封装工具功能，并提供高层次和低层次的规划器以及执行器来执行工具使用。实验结果表明，OctoTools在16个不同任务上相较于GPT-4o平均提高了9.3%的准确率，并在相同工具集下超越了其他方法。'}}}, {'id': 'https://huggingface.co/papers/2502.12859', 'title': 'PAFT: Prompt-Agnostic Fine-Tuning', 'url': 'https://huggingface.co/papers/2502.12859', 'abstract': 'While Large Language Models (LLMs) adapt well to downstream tasks after fine-tuning, this adaptability often compromises prompt robustness, as even minor prompt variations can significantly degrade performance. To address this, we propose Prompt-Agnostic Fine-Tuning(PAFT), a simple yet effective approach that dynamically adjusts prompts during fine-tuning. This encourages the model to learn underlying task principles rather than overfitting to specific prompt formulations. PAFT operates in two stages: First, a diverse set of meaningful, synthetic candidate prompts is constructed. Second, during fine-tuning, prompts are randomly sampled from this set to create dynamic training inputs. Extensive experiments across diverse datasets and LLMs demonstrate that models trained with PAFT exhibit strong robustness and generalization across a wide range of prompts, including unseen ones. This enhanced robustness improves both model performance and inference speed while maintaining training efficiency. Ablation studies further confirm the effectiveness of PAFT.', 'score': 10, 'issue_id': 2290, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': 'f5398572cb7bbb47', 'authors': ['Chenxing Wei', 'Yao Shu', 'Mingwen Ou', 'Ying Tiffany He', 'Fei Richard Yu'], 'affiliations': ['College of Computer Science and Software Engineering, Shenzhen University, China', 'Guangdong Lab of AI and Digital Economy (SZ), China', 'School of Information Technology, Carleton University, Canada', 'Tsinghua Shenzhen International Graduate School, Tsinghua University, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.12859.jpg', 'data': {'categories': ['#training', '#dataset', '#optimization', '#synthetic', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Устойчивость к промптам: новый подход к тонкой настройке языковых моделей', 'desc': 'Эта статья представляет новый метод тонкой настройки больших языковых моделей (LLM), называемый Prompt-Agnostic Fine-Tuning (PAFT). PAFT решает проблему снижения производительности модели при изменении промптов, используя динамическую корректировку промптов во время обучения. Метод включает создание разнообразного набора синтетических промптов и случайный выбор из них во время тонкой настройки. Эксперименты показывают, что PAFT повышает устойчивость модели к различным промптам, улучшая производительность и скорость вывода.'}, 'en': {'title': 'Enhancing Robustness in LLMs with Dynamic Prompting', 'desc': 'This paper introduces Prompt-Agnostic Fine-Tuning (PAFT), a method designed to improve the robustness of Large Language Models (LLMs) when adapting to various tasks. PAFT works by dynamically adjusting prompts during the fine-tuning process, which helps the model focus on the core principles of the tasks instead of memorizing specific prompt formats. The approach involves creating a diverse set of synthetic prompts and randomly sampling from them during training, leading to better generalization and performance on unseen prompts. Experimental results show that PAFT enhances both the robustness and inference speed of LLMs while keeping training efficient.'}, 'zh': {'title': '提升模型鲁棒性与泛化能力的提示无关微调', 'desc': '大型语言模型（LLMs）在微调后能够很好地适应下游任务，但这种适应性往往会影响提示的鲁棒性，因为即使是微小的提示变化也会显著降低性能。为了解决这个问题，我们提出了一种简单而有效的方法——提示无关微调（PAFT），该方法在微调过程中动态调整提示。PAFT的操作分为两个阶段：首先，构建一组多样化且有意义的合成候选提示；其次，在微调过程中，从这组提示中随机抽取，以创建动态训练输入。通过在多种数据集和LLMs上的广泛实验，证明了使用PAFT训练的模型在各种提示（包括未见过的提示）上表现出强大的鲁棒性和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2502.12170', 'title': 'MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections', 'url': 'https://huggingface.co/papers/2502.12170', 'abstract': 'We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input stream (the query, key, value or residual) of a Transformer block. MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer. Extensive experiments show that MUDDFormer significantly outperforms Transformers across various model architectures and scales in language modeling, achieving the performance of Transformers trained with 1.8X-2.4X compute. Notably, MUDDPythia-2.8B matches Pythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B in five-shot settings, while adding only 0.23% parameters and 0.4% computation. Code in JAX and PyTorch and pre-trained models are available at https://github.com/Caiyun-AI/MUDDFormer .', 'score': 10, 'issue_id': 2287, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': 'f06bcb1b611b7c39', 'authors': ['Da Xiao', 'Qingye Meng', 'Shengping Li', 'Xingyuan Yuan'], 'affiliations': ['Beijing University of Posts and Telecommunications, Beijing, China', 'ColorfulClouds Technology Co., Ltd., Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.12170.jpg', 'data': {'categories': ['#optimization', '#training', '#open_source', '#architecture'], 'emoji': '🔀', 'ru': {'title': 'Динамические связи для эффективных трансформеров', 'desc': 'Исследователи предлагают новый метод MUDD (MUltiway Dynamic Dense) для улучшения связей между слоями в архитектуре Transformer. MUDD генерирует веса связей динамически в зависимости от скрытых состояний на каждой позиции последовательности и для каждого входного потока блока Transformer. Эксперименты показывают, что MUDDFormer значительно превосходит стандартные Transformer-модели в задачах языкового моделирования, достигая производительности моделей, обученных с использованием в 1.8-2.4 раза больше вычислительных ресурсов. Примечательно, что MUDDPythia-2.8B сопоставима по эффективности с Pythia-6.9B и даже конкурирует с Pythia-12B в некоторых задачах, добавляя всего 0.23% параметров и 0.4% вычислений.'}, 'en': {'title': 'Dynamic Connections for Enhanced Transformer Performance', 'desc': 'The paper introduces MUDD connections, which improve the flow of information between layers in Transformer models. Unlike traditional residual connections that use fixed weights, MUDD connections adaptively generate weights based on the hidden states of the input at each position. This dynamic approach allows for better integration of information from different input streams, enhancing the overall performance of the model. The proposed MUDDFormer architecture shows significant improvements in language modeling tasks, achieving results comparable to larger models while maintaining a smaller parameter count.'}, 'zh': {'title': '动态连接，提升Transformer性能！', 'desc': '我们提出了一种名为多向动态稠密连接（MUDD）的简单有效方法，旨在解决残差连接的局限性，并增强Transformer中跨层信息流动。与现有的静态共享连接权重的稠密连接方法不同，MUDD根据每个序列位置的隐藏状态动态生成连接权重，并针对Transformer块的每个解耦输入流（查询、键、值或残差）。MUDD连接可以无缝集成到任何Transformer架构中，形成MUDDFormer。大量实验表明，MUDDFormer在语言建模中显著超越了各种模型架构和规模的Transformer，表现出与训练时计算量为1.8X-2.4X的Transformer相当的性能。'}}}, {'id': 'https://huggingface.co/papers/2502.12215', 'title': 'Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?', 'url': 'https://huggingface.co/papers/2502.12215', 'abstract': "The advent of test-time scaling in large language models (LLMs), exemplified by OpenAI's o1 series, has advanced reasoning capabilities by scaling computational resource allocation during inference. While successors like QwQ, Deepseek-R1 (R1) and LIMO replicate these advancements, whether these models truly possess test-time scaling capabilities remains underexplored. This study found that longer CoTs of these o1-like models do not consistently enhance accuracy; in fact, correct solutions are often shorter than incorrect ones for the same questions. Further investigation shows this phenomenon is closely related to models' self-revision capabilities - longer CoTs contain more self-revisions, which often lead to performance degradation. We then compare sequential and parallel scaling strategies on QwQ, R1 and LIMO, finding that parallel scaling achieves better coverage and scalability. Based on these insights, we propose Shortest Majority Vote, a method that combines parallel scaling strategies with CoT length characteristics, significantly improving models' test-time scalability compared to conventional majority voting approaches.", 'score': 9, 'issue_id': 2288, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': 'a02df3e32ba854de', 'authors': ['Zhiyuan Zeng', 'Qinyuan Cheng', 'Zhangyue Yin', 'Yunhua Zhou', 'Xipeng Qiu'], 'affiliations': ['School of Computer Science, Fudan University, Shanghai, China', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2502.12215.jpg', 'data': {'categories': ['#inference', '#reasoning', '#training', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Короче - лучше: новый взгляд на масштабирование языковых моделей', 'desc': 'Это исследование посвящено масштабированию во время вывода в больших языковых моделях (LLM). Авторы обнаружили, что более длинные цепочки рассуждений не всегда повышают точность, и часто правильные решения короче неправильных. Исследование показывает, что это связано со способностью моделей к самокоррекции, которая может ухудшать производительность. На основе этих наблюдений предложен метод Shortest Majority Vote, сочетающий параллельное масштабирование со свойствами длины цепочек рассуждений.'}, 'en': {'title': 'Enhancing LLM Performance with Shorter, Smarter Reasoning', 'desc': 'This paper investigates the concept of test-time scaling in large language models (LLMs), particularly focusing on the o1 series by OpenAI and its successors. It reveals that longer chains of thought (CoTs) do not always lead to better accuracy, as shorter CoTs can yield correct answers more frequently. The study highlights that the presence of self-revisions in longer CoTs can negatively impact performance. To enhance test-time scalability, the authors propose a new method called Shortest Majority Vote, which integrates parallel scaling strategies with CoT length characteristics, outperforming traditional majority voting methods.'}, 'zh': {'title': '提升模型推理能力的新方法', 'desc': '本文探讨了大型语言模型（LLMs）在推理时的测试时间缩放能力，特别是OpenAI的o1系列。研究发现，虽然一些后续模型如QwQ和Deepseek-R1模仿了这些进展，但它们的测试时间缩放能力仍未得到充分验证。更长的链式思维（CoT）并不总是提高准确性，反而正确答案往往比错误答案更短。我们提出了一种新的方法——最短多数投票，结合并行缩放策略和CoT长度特征，显著提升了模型的测试时间可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2502.13092', 'title': 'Text2World: Benchmarking Large Language Models for Symbolic World Model Generation', 'url': 'https://huggingface.co/papers/2502.13092', 'abstract': 'Recently, there has been growing interest in leveraging large language models (LLMs) to generate symbolic world models from textual descriptions. Although LLMs have been extensively explored in the context of world modeling, prior studies encountered several challenges, including evaluation randomness, dependence on indirect metrics, and a limited domain scope. To address these limitations, we introduce a novel benchmark, Text2World, based on planning domain definition language (PDDL), featuring hundreds of diverse domains and employing multi-criteria, execution-based metrics for a more robust evaluation. We benchmark current LLMs using Text2World and find that reasoning models trained with large-scale reinforcement learning outperform others. However, even the best-performing model still demonstrates limited capabilities in world modeling. Building on these insights, we examine several promising strategies to enhance the world modeling capabilities of LLMs, including test-time scaling, agent training, and more. We hope that Text2World can serve as a crucial resource, laying the groundwork for future research in leveraging LLMs as world models. The project page is available at https://text-to-world.github.io/.', 'score': 8, 'issue_id': 2296, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': 'f5365c5f8afa57df', 'authors': ['Mengkang Hu', 'Tianxing Chen', 'Yude Zou', 'Yuheng Lei', 'Qiguang Chen', 'Ming Li', 'Hongyuan Zhang', 'Wenqi Shao', 'Ping Luo'], 'affiliations': ['Harbin Institute of Technology', 'Shanghai AI Laboratory', 'Shenzhen University', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.13092.jpg', 'data': {'categories': ['#rl', '#games', '#reasoning', '#benchmark'], 'emoji': '🌍', 'ru': {'title': 'Text2World: новый бенчмарк для оценки LLM в моделировании мира', 'desc': 'Статья представляет новый бенчмарк Text2World для оценки способностей больших языковых моделей (LLM) генерировать символические модели мира из текстовых описаний. Бенчмарк основан на языке определения доменов планирования (PDDL) и включает сотни разнообразных доменов. Авторы оценивают современные LLM с помощью Text2World и обнаруживают, что модели рассуждений, обученные с помощью масштабного обучения с подкреплением, превосходят другие. Исследование также рассматривает стратегии улучшения возможностей моделирования мира для LLM.'}, 'en': {'title': 'Enhancing World Modeling with Text2World Benchmark', 'desc': 'This paper discusses the use of large language models (LLMs) to create symbolic representations of worlds from text descriptions. It identifies challenges in previous research, such as evaluation randomness and limited domain coverage. To overcome these issues, the authors introduce a new benchmark called Text2World, which uses planning domain definition language (PDDL) and offers a variety of evaluation metrics. The study finds that while LLMs trained with reinforcement learning show promise, they still struggle with world modeling, prompting the exploration of strategies to improve their capabilities.'}, 'zh': {'title': '利用LLMs提升世界建模能力的探索', 'desc': '最近，利用大型语言模型（LLMs）从文本描述生成符号世界模型的兴趣日益增长。尽管在世界建模方面对LLMs进行了广泛研究，但之前的研究面临评估随机性、依赖间接指标和领域范围有限等挑战。为了解决这些问题，我们引入了一个新的基准测试Text2World，基于规划领域定义语言（PDDL），涵盖数百个多样化的领域，并采用多标准、基于执行的指标进行更稳健的评估。我们使用Text2World对当前的LLMs进行基准测试，发现经过大规模强化学习训练的推理模型表现优于其他模型，但即使是表现最好的模型在世界建模能力上仍然有限。'}}}, {'id': 'https://huggingface.co/papers/2502.12996', 'title': 'Eager Updates For Overlapped Communication and Computation in DiLoCo', 'url': 'https://huggingface.co/papers/2502.12996', 'abstract': 'Distributed optimization methods such as DiLoCo have been shown to be effective in training very large models across multiple distributed workers, such as datacenters. These methods split updates into two parts: an inner optimization phase, where the workers independently execute multiple optimization steps on their own local data, and an outer optimization step, where the inner updates are synchronized. While such approaches require orders of magnitude less communication than standard data-parallel training, in settings where the workers are datacenters, even the limited communication requirements of these approaches can still cause significant slow downs due to the blocking necessary at each outer optimization step. In this paper, we investigate techniques to mitigate this issue by overlapping communication with computation in a manner that allows the outer optimization step to fully overlap with the inner optimization phase. We show that a particular variant, dubbed eager updates, provides competitive performance with standard DiLoCo in settings with low bandwidth between workers.', 'score': 7, 'issue_id': 2295, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '61c2807bd3fe3a67', 'authors': ['Satyen Kale', 'Arthur Douillard', 'Yanislav Donchev'], 'affiliations': ['Apple', 'Google DeepMind', 'Google Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.12996.jpg', 'data': {'categories': ['#training', '#optimization', '#inference'], 'emoji': '🚀', 'ru': {'title': 'Ускорение распределенного обучения с помощью перекрытия коммуникации и вычислений', 'desc': "В статье рассматриваются методы распределенной оптимизации для обучения очень больших моделей на нескольких распределенных узлах, таких как дата-центры. Описывается подход DiLoCo, который разделяет обновления на внутреннюю фазу оптимизации на локальных данных и внешний шаг синхронизации. Авторы предлагают технику под названием 'eager updates' для перекрытия коммуникации и вычислений, что позволяет полностью совместить внешний шаг оптимизации с внутренней фазой. Показано, что этот метод обеспечивает конкурентоспособную производительность по сравнению со стандартным DiLoCo в условиях низкой пропускной способности между узлами."}, 'en': {'title': 'Enhancing Distributed Training with Eager Updates', 'desc': "This paper explores distributed optimization methods, specifically DiLoCo, which are used to train large machine learning models across multiple workers in datacenters. The process involves an inner optimization phase where workers perform local updates and an outer phase for synchronizing these updates. However, the communication required during the outer phase can slow down training, even with reduced communication needs. The authors propose a solution that overlaps communication with computation, introducing 'eager updates' to enhance performance in low bandwidth scenarios while maintaining competitive results with traditional DiLoCo methods."}, 'zh': {'title': '重叠通信与计算，提升分布式优化效率', 'desc': '本文探讨了一种分布式优化方法，称为DiLoCo，适用于在多个分布式工作者（如数据中心）上训练大型模型。这种方法将更新分为两个部分：内部优化阶段和外部优化阶段，工作者在本地数据上独立执行多个优化步骤。尽管这种方法比标准的数据并行训练需要更少的通信，但在数据中心环境中，外部优化步骤的阻塞仍可能导致显著的延迟。我们提出了一种技术，通过重叠通信与计算，使外部优化步骤与内部优化阶段完全重叠，从而提高性能。'}}}, {'id': 'https://huggingface.co/papers/2502.09838', 'title': 'HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation', 'url': 'https://huggingface.co/papers/2502.09838', 'abstract': 'We present HealthGPT, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained large language models (LLMs). This is achieved through a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is complemented by a tailored hierarchical visual perception approach and a three-stage learning strategy. To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called VL-Health. Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at https://github.com/DCDmllm/HealthGPT.', 'score': 7, 'issue_id': 2287, 'pub_date': '2025-02-14', 'pub_date_card': {'ru': '14 февраля', 'en': 'February 14', 'zh': '2月14日'}, 'hash': 'ce920c7d40d11dad', 'authors': ['Tianwei Lin', 'Wenqiao Zhang', 'Sijing Li', 'Yuqian Yuan', 'Binhe Yu', 'Haoyuan Li', 'Wanggui He', 'Hao Jiang', 'Mengze Li', 'Xiaohui Song', 'Siliang Tang', 'Jun Xiao', 'Hui Lin', 'Yueting Zhuang', 'Beng Chin Ooi'], 'affiliations': ['Alibaba', 'National University of Singapore', 'The Hong Kong University of Science and Technology', 'University of Electronic Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.09838.jpg', 'data': {'categories': ['#data', '#training', '#cv', '#dataset', '#healthcare'], 'emoji': '🏥', 'ru': {'title': 'HealthGPT: Единая модель для понимания и генерации медицинских изображений', 'desc': 'HealthGPT - это мощная мультимодальная модель для медицинской визуальной обработки и генерации. Она использует новую технику адаптации H-LoRA и иерархический подход к визуальному восприятию. Модель обучена на специально созданном наборе данных VL-Health. HealthGPT демонстрирует исключительную производительность в медицинских визуальных задачах.'}, 'en': {'title': 'Revolutionizing Medical AI with HealthGPT', 'desc': 'HealthGPT is a Medical Large Vision-Language Model that combines understanding and generating medical images and text in one system. It uses a unique method called heterogeneous low-rank adaptation (H-LoRA) to enhance pre-trained large language models with diverse medical knowledge. The model is trained on a specialized dataset named VL-Health, which focuses on medical comprehension and generation tasks. Results show that HealthGPT performs exceptionally well in various medical visual tasks, demonstrating its effectiveness and scalability.'}, 'zh': {'title': 'HealthGPT：医疗视觉语言模型的创新整合', 'desc': '我们提出了HealthGPT，这是一种强大的医疗大型视觉语言模型（Med-LVLM），它将医疗视觉理解和生成能力整合在一个统一的自回归框架中。我们的自举理念是逐步将异构的理解和生成知识适应于预训练的大型语言模型（LLMs）。这通过一种新颖的异构低秩适应（H-LoRA）技术实现，并辅以定制的分层视觉感知方法和三阶段学习策略。实验结果表明，HealthGPT在医疗视觉统一任务中表现出色，具有良好的可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2502.12018', 'title': 'Atom of Thoughts for Markov LLM Test-Time Scaling', 'url': 'https://huggingface.co/papers/2502.12018', 'abstract': 'Large Language Models (LLMs) achieve superior performance through training-time scaling, and test-time scaling further enhances their capabilities by conducting effective reasoning during inference. However, as the scale of reasoning increases, existing test-time scaling methods suffer from accumulated historical information, which not only wastes computational resources but also interferes with effective reasoning. To address this issue, we observe that complex reasoning progress is often achieved by solving a sequence of independent subquestions, each being self-contained and verifiable. These subquestions are essentially atomic questions, relying primarily on their current state rather than accumulated history, similar to the memoryless transitions in a Markov process. Based on this observation, we propose Atom of Thoughts (AoT), where each state transition in the reasoning process consists of decomposing the current question into a dependency-based directed acyclic graph and contracting its subquestions, forming a new atomic question state. This iterative decomposition-contraction process continues until reaching directly solvable atomic questions, naturally realizing Markov transitions between question states. Furthermore, these atomic questions can be seamlessly integrated into existing test-time scaling methods, enabling AoT to serve as a plug-in enhancement for improving reasoning capabilities. Experiments across six benchmarks demonstrate the effectiveness of AoT both as a standalone framework and a plug-in enhancement. Notably, on HotpotQA, when applied to gpt-4o-mini, AoT achieves an 80.6% F1 score, surpassing o3-mini by 3.4% and DeepSeek-R1 by 10.6%. The code will be available at https://github.com/qixucen/atom.', 'score': 6, 'issue_id': 2295, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': 'acefee7f3111548b', 'authors': ['Fengwei Teng', 'Zhaoyang Yu', 'Quan Shi', 'Jiayi Zhang', 'Chenglin Wu', 'Yuyu Luo'], 'affiliations': ['DeepWisdom', 'Renmin University of China', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.12018.jpg', 'data': {'categories': ['#training', '#benchmark', '#reasoning', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Атомарное мышление: новый подход к рассуждениям языковых моделей', 'desc': "Статья представляет новый метод под названием 'Atom of Thoughts' (AoT) для улучшения рассуждений больших языковых моделей (LLM) во время вывода. AoT разбивает сложные вопросы на атомарные подвопросы, организованные в направленный ациклический граф, что позволяет эффективно использовать вычислительные ресурсы и избегать накопления избыточной информации. Метод может быть интегрирован в существующие подходы к масштабированию во время вывода, значительно улучшая их производительность. Эксперименты на шести бенчмарках показали эффективность AoT как самостоятельного фреймворка и как дополнения к другим методам."}, 'en': {'title': 'Enhancing Reasoning in LLMs with Atomic Question Decomposition', 'desc': 'This paper introduces Atom of Thoughts (AoT), a method designed to improve reasoning in Large Language Models (LLMs) during inference. AoT addresses the problem of accumulated historical information in existing test-time scaling methods, which can hinder effective reasoning and waste computational resources. By breaking down complex questions into independent subquestions, AoT allows for a more efficient reasoning process that resembles memoryless transitions in a Markov process. The proposed method not only enhances reasoning capabilities but also integrates well with existing frameworks, showing significant performance improvements in benchmark tests.'}, 'zh': {'title': '思维原子的力量：提升推理能力的创新方法', 'desc': '大型语言模型（LLMs）通过扩展训练规模和测试规模来提高性能。在推理过程中，现有的测试时间扩展方法由于历史信息的累积，导致计算资源浪费和推理效果干扰。为了解决这个问题，我们提出了“思维原子”（Atom of Thoughts，AoT），通过将当前问题分解为依赖关系的有向无环图，并收缩其子问题，形成新的原子问题状态。这种迭代的分解-收缩过程实现了问题状态之间的马尔可夫转移，并可以与现有的测试时间扩展方法无缝集成。'}}}, {'id': 'https://huggingface.co/papers/2502.12574', 'title': 'HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading', 'url': 'https://huggingface.co/papers/2502.12574', 'abstract': 'Transformer-based large language models (LLMs) demonstrate impressive performance in long context generation. Extending the context length has disproportionately shifted the memory footprint of LLMs during inference to the key-value cache (KV cache). In this paper, we propose HEADINFER, which offloads the KV cache to CPU RAM while avoiding the need to fully store the KV cache for any transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise offloading strategy, maintaining only selective attention heads KV cache on the GPU while computing attention output dynamically. Through roofline analysis, we demonstrate that HEADINFER maintains computational efficiency while significantly reducing memory footprint. We evaluate HEADINFER on the Llama-3-8B model with a 1-million-token sequence, reducing the GPU memory footprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage from 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline inference. Notably, HEADINFER enables 4-million-token inference with an 8B model on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without approximation methods.', 'score': 6, 'issue_id': 2286, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '6e5de7c584198857', 'authors': ['Cheng Luo', 'Zefan Cai', 'Hanshi Sun', 'Jinqi Xiao', 'Bo Yuan', 'Wen Xiao', 'Junjie Hu', 'Jiawei Zhao', 'Beidi Chen', 'Anima Anandkumar'], 'affiliations': ['California Institute of Technology', 'Carnegie Mellon University', 'Microsoft', 'Rutgers University', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2502.12574.jpg', 'data': {'categories': ['#optimization', '#architecture', '#inference', '#training', '#long_context'], 'emoji': '🧠', 'ru': {'title': 'HEADINFER: эффективное использование памяти для LLM с длинным контекстом', 'desc': 'HEADINFER - это новый метод для оптимизации памяти при работе с большими языковыми моделями (LLM). Он позволяет выгружать кэш ключей-значений (KV cache) в оперативную память CPU, значительно снижая нагрузку на GPU. Используя стратегию выборочного хранения данных для отдельных голов внимания, HEADINFER сохраняет вычислительную эффективность при существенном уменьшении объема используемой памяти. Метод был успешно протестирован на модели Llama-3-8B с последовательностью в 1 миллион токенов, сократив использование памяти GPU на 92% по сравнению с базовым методом.'}, 'en': {'title': 'HEADINFER: Efficient Memory Management for Long Contexts in LLMs', 'desc': 'This paper introduces HEADINFER, a novel approach to manage the memory usage of large language models (LLMs) during long context generation. By offloading the key-value (KV) cache to CPU RAM, HEADINFER reduces the reliance on GPU memory, which is crucial for efficient inference. The method employs a fine-grained, head-wise offloading strategy, allowing selective storage of attention heads on the GPU while dynamically computing attention outputs. Evaluation on the Llama-3-8B model shows a remarkable reduction in GPU memory usage, enabling efficient processing of extremely long sequences without compromising performance.'}, 'zh': {'title': 'HEADINFER：优化大语言模型的内存使用', 'desc': '本文提出了一种名为HEADINFER的方法，旨在优化大语言模型（LLM）在长上下文生成中的内存使用。通过将关键值缓存（KV缓存）转移到CPU RAM，HEADINFER避免了在GPU上完全存储KV缓存的需求。该方法采用细粒度的头部级别卸载策略，仅在GPU上保留选择性的注意力头KV缓存，同时动态计算注意力输出。实验结果表明，HEADINFER在显著减少内存占用的同时，保持了计算效率，使得在单个消费级GPU上实现了对长达400万标记的推理。'}}}, {'id': 'https://huggingface.co/papers/2502.12501', 'title': 'Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for LLM-as-a-Judge', 'url': 'https://huggingface.co/papers/2502.12501', 'abstract': "LLM-as-a-Judge, which generates chain-of-thought (CoT) judgments, has become a widely adopted auto-evaluation method. However, its reliability is compromised by the CoT reasoning's inability to capture comprehensive and deeper details, often leading to incomplete outcomes. Existing methods mainly rely on majority voting or criteria expansion, which is insufficient to address the limitation in CoT. We propose Crowd-based Comparative Evaluation, which introduces additional crowd responses to compare with the candidate responses, thereby exposing deeper and more comprehensive details within the candidate responses. This process effectively guides LLM-as-a-Judge to provide a more detailed CoT judgment. Extensive experiments demonstrate that our approach enhances evaluation reliability, achieving an average accuracy gain of 6.7% across five benchmarks. Moreover, our method produces higher-quality CoTs that facilitate judge distillation and exhibit superior performance in rejection sampling for supervised fine-tuning (SFT), referred to as crowd rejection sampling, thereby enabling more efficient SFT. Our analysis confirms that CoTs generated by ours are more comprehensive and of higher quality, and evaluation accuracy improves as inference scales.", 'score': 5, 'issue_id': 2286, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '1c26233f72f504ee', 'authors': ['Qiyuan Zhang', 'Yufei Wang', 'Yuxin Jiang', 'Liangyou Li', 'Chuhan Wu', 'Yasheng Wang', 'Xin Jiang', 'Lifeng Shang', 'Ruiming Tang', 'Fuyuan Lyu', 'Chen Ma'], 'affiliations': ['City University of Hong Kong', 'Huawei Noahs Ark Lab', 'McGill University & MILA', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.12501.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#inference', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Улучшение оценки ИИ через сравнение с мнением толпы', 'desc': "Статья представляет новый метод оценки моделей машинного обучения под названием 'Crowd-based Comparative Evaluation'. Этот подход улучшает традиционный метод LLM-as-a-Judge, добавляя сравнение с ответами толпы для более глубокого анализа. Эксперименты показали повышение точности оценки в среднем на 6.7% на пяти бенчмарках. Метод также демонстрирует улучшенное качество рассуждений (Chain-of-Thought) и эффективность в отборе данных для обучения с учителем."}, 'en': {'title': 'Enhancing LLM Evaluations with Crowd Insights', 'desc': 'This paper addresses the limitations of the LLM-as-a-Judge method, which generates chain-of-thought (CoT) judgments for auto-evaluation. The authors identify that CoT reasoning often lacks depth and comprehensiveness, leading to incomplete evaluations. To overcome this, they propose a new method called Crowd-based Comparative Evaluation, which incorporates additional crowd responses to enhance the evaluation process. Their experiments show that this approach improves the reliability of evaluations, increases accuracy by 6.7%, and produces higher-quality CoTs that benefit supervised fine-tuning.'}, 'zh': {'title': '提升LLM评估可靠性的创新方法', 'desc': '本文提出了一种新的评估方法，称为基于人群的比较评估（Crowd-based Comparative Evaluation），旨在提高大型语言模型（LLM）作为评判者的可靠性。传统的链式推理（CoT）方法常常无法捕捉到全面和深入的细节，导致评估结果不完整。我们的方法通过引入额外的人群反馈，与候选响应进行比较，从而揭示候选响应中的更深层次和更全面的细节。实验结果表明，该方法在五个基准测试中平均提高了6.7%的评估准确性，并生成了更高质量的CoT，促进了监督微调的效率。'}}}, {'id': 'https://huggingface.co/papers/2502.12929', 'title': 'Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options', 'url': 'https://huggingface.co/papers/2502.12929', 'abstract': 'We present a novel reasoning approach called Flow-of-Options (FoO), designed to address intrinsic biases in Large Language Models (LLMs). FoO enables LLMs to systematically explore a diverse range of possibilities in their reasoning, as demonstrated by an FoO-based agentic system for autonomously solving Machine Learning tasks (AutoML). Our framework outperforms state-of-the-art baselines, achieving improvements of 38.2% - 69.2% on standard data science tasks, and 37.4% - 47.9% on therapeutic chemistry tasks. With an overall operation cost under $1 per task, our framework is well-suited for cost-sensitive applications. Beyond classification and regression, we illustrate the broader applicability of our FoO-based agentic system to tasks such as reinforcement learning and image generation. Our framework presents significant advancements compared to current state-of-the-art agentic systems for AutoML, due to the benefits of FoO in enforcing diversity in LLM solutions through compressed, explainable representations that also support long-term memory when combined with case-based reasoning.', 'score': 4, 'issue_id': 2296, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '7cde53c7aa78c7ed', 'authors': ['Lakshmi Nair', 'Ian Trase', 'Mark Kim'], 'affiliations': ['Flagship Pioneering, Cambridge, MA 02142, United States'], 'pdf_title_img': 'assets/pdf/title_img/2502.12929.jpg', 'data': {'categories': ['#interpretability', '#training', '#rl', '#reasoning', '#cv', '#agents'], 'emoji': '🧠', 'ru': {'title': 'Революция в автоматическом машинном обучении: FoO преодолевает предубеждения ИИ', 'desc': 'Представлен новый подход к рассуждениям под названием Flow-of-Options (FoO), предназначенный для устранения внутренних предубеждений в больших языковых моделях (LLM). FoO позволяет LLM систематически исследовать разнообразные возможности в своих рассуждениях, что демонстрируется агентной системой на основе FoO для автономного решения задач машинного обучения (AutoML). Этот фреймворк превосходит современные базовые модели, достигая улучшений на 38.2% - 69.2% в стандартных задачах анализа данных и на 37.4% - 47.9% в задачах терапевтической химии. Система применима к широкому спектру задач, включая обучение с подкреплением и генерацию изображений.'}, 'en': {'title': 'Unlocking Diverse Reasoning in Large Language Models with Flow-of-Options', 'desc': 'The paper introduces a new reasoning method called Flow-of-Options (FoO) that helps Large Language Models (LLMs) overcome their inherent biases. FoO allows these models to explore a wide variety of reasoning paths, which is particularly useful for automating machine learning tasks (AutoML). The proposed framework shows significant performance improvements over existing methods, achieving better results in both data science and therapeutic chemistry tasks while maintaining low operational costs. Additionally, FoO enhances the versatility of LLMs, enabling them to tackle not just classification and regression, but also reinforcement learning and image generation tasks.'}, 'zh': {'title': '选项流：提升大型语言模型推理的多样性与性能', 'desc': '我们提出了一种新的推理方法，称为选项流（Flow-of-Options，FoO），旨在解决大型语言模型（LLMs）中的内在偏见。FoO使得LLMs能够系统地探索多种推理可能性，特别是在自动化机器学习（AutoML）任务中表现出色。我们的框架在标准数据科学任务上提高了38.2%到69.2%的性能，在治疗化学任务上提高了37.4%到47.9%。此外，FoO还适用于强化学习和图像生成等更广泛的任务，展现了其在多样性和可解释性方面的优势。'}}}, {'id': 'https://huggingface.co/papers/2502.10708', 'title': 'Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey', 'url': 'https://huggingface.co/papers/2502.10708', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable success in various tasks such as natural language understanding, text summarization, and machine translation. However, their general-purpose nature often limits their effectiveness in domain-specific applications that require specialized knowledge, such as healthcare, chemistry, or legal analysis. To address this, researchers have explored diverse methods to enhance LLMs by integrating domain-specific knowledge. In this survey, we provide a comprehensive overview of these methods, which we categorize into four key approaches: dynamic knowledge injection, static knowledge embedding, modular adapters, and prompt optimization. Each approach offers unique mechanisms to equip LLMs with domain expertise, balancing trade-offs between flexibility, scalability, and efficiency. We discuss how these methods enable LLMs to tackle specialized tasks, compare their advantages and disadvantages, evaluate domain-specific LLMs against general LLMs, and highlight the challenges and opportunities in this emerging field. For those interested in delving deeper into this area, we also summarize the commonly used datasets and benchmarks. To keep researchers updated on the latest studies, we maintain an open-source at: https://github.com/abilliyb/Knowledge_Injection_Survey_Papers, dedicated to documenting research in the field of specialized LLM.', 'score': 3, 'issue_id': 2291, 'pub_date': '2025-02-15', 'pub_date_card': {'ru': '15 февраля', 'en': 'February 15', 'zh': '2月15日'}, 'hash': '01a93665c8e86d8d', 'authors': ['Zirui Song', 'Bin Yan', 'Yuhan Liu', 'Miao Fang', 'Mingzhe Li', 'Rui Yan', 'Xiuying Chen'], 'affiliations': ['ByteDance', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)', 'Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2502.10708.jpg', 'data': {'categories': ['#survey', '#dataset', '#healthcare', '#multilingual', '#benchmark', '#machine_translation', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Усиление LLM доменными знаниями: ключ к специализированным задачам', 'desc': 'Статья представляет обзор методов улучшения больших языковых моделей (LLM) для специализированных задач путем внедрения доменных знаний. Авторы выделяют четыре основных подхода: динамическое внедрение знаний, статическое встраивание знаний, модульные адаптеры и оптимизация промптов. В работе сравниваются преимущества и недостатки каждого метода, а также оценивается эффективность доменно-специфичных LLM по сравнению с общими моделями. Исследователи также обсуждают вызовы и возможности в этой развивающейся области и предоставляют информацию о наборах данных и бенчмарках для оценки моделей.'}, 'en': {'title': 'Enhancing LLMs with Domain-Specific Knowledge', 'desc': 'This paper surveys methods to enhance Large Language Models (LLMs) for specialized tasks in fields like healthcare and law. It categorizes these methods into four approaches: dynamic knowledge injection, static knowledge embedding, modular adapters, and prompt optimization. Each method aims to integrate domain-specific knowledge into LLMs, improving their performance while considering trade-offs in flexibility and efficiency. The paper also evaluates the effectiveness of these specialized LLMs compared to general-purpose models and discusses the challenges and opportunities in this area.'}, 'zh': {'title': '增强大型语言模型的领域知识', 'desc': '大型语言模型（LLMs）在自然语言理解、文本摘要和机器翻译等任务中表现出色，但在需要专业知识的领域应用中效果有限。为了解决这个问题，研究人员探索了多种方法来增强LLMs，主要包括动态知识注入、静态知识嵌入、模块适配器和提示优化。每种方法都有其独特的机制，旨在为LLMs提供领域专业知识，同时在灵活性、可扩展性和效率之间进行权衡。本文综述了这些方法的优缺点，并讨论了领域特定LLMs与通用LLMs的比较，以及该领域面临的挑战和机遇。'}}}, {'id': 'https://huggingface.co/papers/2502.12524', 'title': 'YOLOv12: Attention-Centric Real-Time Object Detectors', 'url': 'https://huggingface.co/papers/2502.12524', 'abstract': 'Enhancing the network architecture of the YOLO framework has been crucial for a long time, but has focused on CNN-based improvements despite the proven superiority of attention mechanisms in modeling capabilities. This is because attention-based models cannot match the speed of CNN-based models. This paper proposes an attention-centric YOLO framework, namely YOLOv12, that matches the speed of previous CNN-based ones while harnessing the performance benefits of attention mechanisms. YOLOv12 surpasses all popular real-time object detectors in accuracy with competitive speed. For example, YOLOv12-N achieves 40.6% mAP with an inference latency of 1.64 ms on a T4 GPU, outperforming advanced YOLOv10-N / YOLOv11-N by 2.1%/1.2% mAP with a comparable speed. This advantage extends to other model scales. YOLOv12 also surpasses end-to-end real-time detectors that improve DETR, such as RT-DETR / RT-DETRv2: YOLOv12-S beats RT-DETR-R18 / RT-DETRv2-R18 while running 42% faster, using only 36% of the computation and 45% of the parameters. More comparisons are shown in Figure 1.', 'score': 2, 'issue_id': 2302, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '6f915c875f0fb15d', 'authors': ['Yunjie Tian', 'Qixiang Ye', 'David Doermann'], 'affiliations': ['University at Buffalo', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2502.12524.jpg', 'data': {'categories': ['#cv', '#optimization', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'YOLOv12: Внимание со скоростью CNN', 'desc': 'YOLOv12 - это новая архитектура для обнаружения объектов, основанная на механизмах внимания. Она сочетает высокую точность моделей с вниманием и скорость CNN-моделей. YOLOv12 превосходит существующие детекторы реального времени по точности при сопоставимой скорости. Например, YOLOv12-N достигает 40.6% mAP с задержкой вывода 1.64 мс на GPU T4, превосходя YOLOv10-N и YOLOv11-N.'}, 'en': {'title': 'YOLOv12: Merging Speed and Accuracy with Attention Mechanisms', 'desc': 'This paper introduces YOLOv12, an enhanced version of the YOLO framework that integrates attention mechanisms to improve object detection accuracy while maintaining high speed. Unlike previous models that relied solely on CNN improvements, YOLOv12 achieves a mean Average Precision (mAP) of 40.6% with an inference latency of just 1.64 ms on a T4 GPU. It outperforms earlier YOLO versions and other real-time detectors like RT-DETR, demonstrating superior performance with reduced computational requirements. The results indicate that attention-based models can now compete effectively with traditional CNNs in real-time applications.'}, 'zh': {'title': 'YOLOv12：速度与准确性的完美结合', 'desc': '本论文提出了一种以注意力机制为中心的YOLO框架，称为YOLOv12。与传统的基于卷积神经网络（CNN）的模型相比，YOLOv12在保持相似速度的同时，利用了注意力机制的性能优势。实验结果表明，YOLOv12在准确性上超越了所有流行的实时目标检测器，并且在推理延迟方面表现出色。该模型在不同规模下均展现出优越的性能，尤其是在计算和参数使用上更为高效。'}}}, {'id': 'https://huggingface.co/papers/2502.08869', 'title': 'Harnessing Vision Models for Time Series Analysis: A Survey', 'url': 'https://huggingface.co/papers/2502.08869', 'abstract': 'Time series analysis has witnessed the inspiring development from traditional autoregressive models, deep learning models, to recent Transformers and Large Language Models (LLMs). Efforts in leveraging vision models for time series analysis have also been made along the way but are less visible to the community due to the predominant research on sequence modeling in this domain. However, the discrepancy between continuous time series and the discrete token space of LLMs, and the challenges in explicitly modeling the correlations of variates in multivariate time series have shifted some research attentions to the equally successful Large Vision Models (LVMs) and Vision Language Models (VLMs). To fill the blank in the existing literature, this survey discusses the advantages of vision models over LLMs in time series analysis. It provides a comprehensive and in-depth overview of the existing methods, with dual views of detailed taxonomy that answer the key research questions including how to encode time series as images and how to model the imaged time series for various tasks. Additionally, we address the challenges in the pre- and post-processing steps involved in this framework and outline future directions to further advance time series analysis with vision models.', 'score': 2, 'issue_id': 2299, 'pub_date': '2025-02-13', 'pub_date_card': {'ru': '13 февраля', 'en': 'February 13', 'zh': '2月13日'}, 'hash': '8bfec40a634b6df7', 'authors': ['Jingchao Ni', 'Ziming Zhao', 'ChengAo Shen', 'Hanghang Tong', 'Dongjin Song', 'Wei Cheng', 'Dongsheng Luo', 'Haifeng Chen'], 'affiliations': ['Florida International University', 'NEC Laboratories America', 'University of Connecticut', 'University of Houston', 'University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2502.08869.jpg', 'data': {'categories': ['#multimodal', '#cv', '#survey', '#data'], 'emoji': '📊', 'ru': {'title': 'Визуальные модели открывают новые горизонты в анализе временных рядов', 'desc': 'Данная статья представляет собой обзор методов анализа временных рядов с использованием моделей компьютерного зрения. Авторы рассматривают преимущества визуальных моделей по сравнению с языковыми моделями для анализа временных рядов. В работе представлена подробная таксономия методов кодирования временных рядов в изображения и их последующего моделирования. Также обсуждаются проблемы предобработки и постобработки данных в этом подходе и намечаются перспективные направления исследований.'}, 'en': {'title': 'Harnessing Vision Models for Enhanced Time Series Analysis', 'desc': 'This paper surveys the use of vision models, such as Large Vision Models (LVMs) and Vision Language Models (VLMs), in time series analysis, highlighting their advantages over traditional Large Language Models (LLMs). It discusses the challenges of converting continuous time series data into a format suitable for these models, particularly focusing on encoding time series as images. The paper also explores how to effectively model these imaged time series for various analytical tasks. Furthermore, it identifies key challenges in the pre- and post-processing stages and suggests future research directions to enhance the application of vision models in this field.'}, 'zh': {'title': '视觉模型助力时间序列分析的未来', 'desc': '时间序列分析经历了从传统自回归模型到深度学习模型，再到最近的变换器和大型语言模型（LLMs）的发展。尽管在时间序列分析中，视觉模型的应用也在增加，但由于对序列建模的研究占主导地位，这些努力并不显著。本文综述了视觉模型在时间序列分析中的优势，探讨了如何将时间序列编码为图像以及如何对图像化的时间序列进行建模。我们还讨论了该框架中预处理和后处理步骤的挑战，并提出了未来的研究方向。'}}}, {'id': 'https://huggingface.co/papers/2502.12669', 'title': 'Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research', 'url': 'https://huggingface.co/papers/2502.12669', 'abstract': 'The rapid advancement of perovskite solar cells (PSCs) has led to an exponential growth in research publications, creating an urgent need for efficient knowledge management and reasoning systems in this domain. We present a comprehensive knowledge-enhanced system for PSCs that integrates three key components. First, we develop Perovskite-KG, a domain-specific knowledge graph constructed from 1,517 research papers, containing 23,789 entities and 22,272 relationships. Second, we create two complementary datasets: Perovskite-Chat, comprising 55,101 high-quality question-answer pairs generated through a novel multi-agent framework, and Perovskite-Reasoning, containing 2,217 carefully curated materials science problems. Third, we introduce two specialized large language models: Perovskite-Chat-LLM for domain-specific knowledge assistance and Perovskite-Reasoning-LLM for scientific reasoning tasks. Experimental results demonstrate that our system significantly outperforms existing models in both domain-specific knowledge retrieval and scientific reasoning tasks, providing researchers with effective tools for literature review, experimental design, and complex problem-solving in PSC research.', 'score': 2, 'issue_id': 2291, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': 'cb1e20aec1d7e12c', 'authors': ['Xiang Liu', 'Penglei Sun', 'Shuyan Chen', 'Longhan Zhang', 'Peijie Dong', 'Huajie You', 'Yongqi Zhang', 'Chang Yan', 'Xiaowen Chu', 'Tong-yi Zhang'], 'affiliations': ['Guangzhou Municipal Key Laboratory of Materials Informatics', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.12669.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#data', '#science', '#architecture', '#multimodal', '#training', '#graphs'], 'emoji': '☀️', 'ru': {'title': 'Революция в исследованиях перовскитных солнечных элементов с помощью ИИ', 'desc': 'Статья представляет комплексную систему для исследований перовскитных солнечных элементов (PSC), включающую три ключевых компонента. Первый - это Perovskite-KG, предметно-ориентированный граф знаний, построенный на основе 1517 научных статей. Второй компонент - два набора данных: Perovskite-Chat с 55101 парой вопрос-ответ и Perovskite-Reasoning с 2217 задачами по материаловедению. Третий компонент - две специализированные языковые модели: Perovskite-Chat-LLM для помощи в предметной области и Perovskite-Reasoning-LLM для задач научного рассуждения.'}, 'en': {'title': 'Empowering PSC Research with Knowledge and Reasoning Systems', 'desc': 'This paper presents a knowledge-enhanced system specifically designed for perovskite solar cells (PSCs) research. It includes a knowledge graph built from over 1,500 research papers, which organizes entities and relationships relevant to PSCs. Additionally, the authors created two datasets: one for question-answer pairs and another for materials science problems, both aimed at improving knowledge retrieval and reasoning. The system also features specialized large language models that outperform existing tools, aiding researchers in literature review and experimental design.'}, 'zh': {'title': '钙钛矿太阳能电池的知识管理与推理系统', 'desc': '这篇论文介绍了一种针对钙钛矿太阳能电池的知识增强系统。该系统包括三个主要部分：首先，构建了一个包含1517篇研究论文的领域特定知识图谱，涵盖23789个实体和22272个关系。其次，创建了两个互补的数据集，分别用于问答和材料科学问题。最后，提出了两个专门的大型语言模型，显著提高了领域知识检索和科学推理的效果。'}}}, {'id': 'https://huggingface.co/papers/2502.13142', 'title': 'Pre-training Auto-regressive Robotic Models with 4D Representations', 'url': 'https://huggingface.co/papers/2502.13142', 'abstract': 'Foundation models pre-trained on massive unlabeled datasets have revolutionized natural language and computer vision, exhibiting remarkable generalization capabilities, thus highlighting the importance of pre-training. Yet, efforts in robotics have struggled to achieve similar success, limited by either the need for costly robotic annotations or the lack of representations that effectively model the physical world. In this paper, we introduce ARM4R, an Auto-regressive Robotic Model that leverages low-level 4D Representations learned from human video data to yield a better pre-trained robotic model. Specifically, we focus on utilizing 3D point tracking representations from videos derived by lifting 2D representations into 3D space via monocular depth estimation across time. These 4D representations maintain a shared geometric structure between the points and robot state representations up to a linear transformation, enabling efficient transfer learning from human video data to low-level robotic control. Our experiments show that ARM4R can transfer efficiently from human video data to robotics and consistently improves performance on tasks across various robot environments and configurations.', 'score': 2, 'issue_id': 2290, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': '3c2f3b3e998c6c95', 'authors': ['Dantong Niu', 'Yuvan Sharma', 'Haoru Xue', 'Giscard Biamby', 'Junyi Zhang', 'Ziteng Ji', 'Trevor Darrell', 'Roei Herzig'], 'affiliations': ['BAIR, UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2502.13142.jpg', 'data': {'categories': ['#training', '#dataset', '#transfer_learning', '#3d', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Обучение роботов на видео с людьми: революция в предобучении для робототехники', 'desc': 'Статья представляет ARM4R - авторегрессионную модель для робототехники, использующую 4D-представления, полученные из видеоданных с людьми. Модель применяет 3D-отслеживание точек из видео, поднимая 2D-представления в 3D-пространство с помощью монокулярной оценки глубины. Это позволяет эффективно переносить знания с видеоданных людей на низкоуровневое управление роботами. Эксперименты показывают, что ARM4R улучшает производительность в различных роботизированных средах и конфигурациях.'}, 'en': {'title': 'Revolutionizing Robotics with Human Video Insights', 'desc': 'This paper presents ARM4R, an Auto-regressive Robotic Model designed to enhance robotic learning by utilizing low-level 4D representations derived from human video data. By converting 2D video representations into 3D point tracking through monocular depth estimation, ARM4R captures the geometric relationships necessary for effective robotic control. The model enables efficient transfer learning, allowing robots to learn from human actions without the need for extensive annotations. Experimental results demonstrate that ARM4R significantly improves performance across diverse robotic tasks and environments.'}, 'zh': {'title': '利用视频数据提升机器人控制能力', 'desc': '本论文介绍了一种名为ARM4R的自回归机器人模型，它利用从人类视频数据中学习的低级4D表示来改进机器人的预训练模型。我们专注于利用从视频中提取的3D点跟踪表示，这些表示通过单目深度估计将2D表示提升到3D空间。4D表示在点和机器人状态表示之间保持共享的几何结构，从而实现从人类视频数据到低级机器人控制的高效迁移学习。实验结果表明，ARM4R能够有效地从人类视频数据迁移到机器人，并在各种机器人环境和配置中持续提高任务性能。'}}}, {'id': 'https://huggingface.co/papers/2502.10852', 'title': 'Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages', 'url': 'https://huggingface.co/papers/2502.10852', 'abstract': 'While multilingual language models like XLM-R have advanced multilingualism in NLP, they still perform poorly in extremely low-resource languages. This situation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen support far fewer languages than XLM-R, making text generation models non-existent for many languages in the world. To tackle this challenge, we propose a novel framework for adapting multilingual encoders to text generation in extremely low-resource languages. By reusing the weights between the encoder and the decoder, our framework allows the model to leverage the learned semantic space of the encoder, enabling efficient learning and effective generalization in low-resource languages. Applying this framework to four Chinese minority languages, we present XLM-SWCM, and demonstrate its superior performance on various downstream tasks even when compared with much larger models.', 'score': 2, 'issue_id': 2287, 'pub_date': '2025-02-15', 'pub_date_card': {'ru': '15 февраля', 'en': 'February 15', 'zh': '2月15日'}, 'hash': '11a782268b627ec1', 'authors': ['Zeli Su', 'Ziyin Zhang', 'Guixian Xu', 'Jianing Liu', 'XU Han', 'Ting Zhang', 'Yushuang Dong'], 'affiliations': ['Key Laboratory of Ethnic Language Intelligent Analysis and Security Governance of MOE', 'Minzu University of China', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2502.10852.jpg', 'data': {'categories': ['#multilingual', '#low_resource'], 'emoji': '🌍', 'ru': {'title': 'Преодоление языкового барьера: эффективная генерация текста для малоресурсных языков', 'desc': 'Статья представляет новый подход к адаптации многоязычных энкодеров для генерации текста на языках с крайне ограниченными ресурсами. Авторы предлагают метод повторного использования весов между энкодером и декодером, что позволяет модели эффективно использовать семантическое пространство энкодера. Применяя этот подход к четырем китайским языкам меньшинств, исследователи разработали модель XLM-SWCM. Эксперименты показали превосходные результаты XLM-SWCM на различных задачах даже в сравнении с гораздо более крупными моделями.'}, 'en': {'title': 'Empowering Low-Resource Languages with Efficient Multilingual Models', 'desc': 'This paper addresses the challenges faced by multilingual language models in generating text for extremely low-resource languages. It introduces a new framework that adapts multilingual encoders for text generation by reusing weights between the encoder and decoder. This approach allows the model to utilize the semantic knowledge learned by the encoder, leading to better performance in low-resource settings. The authors demonstrate the effectiveness of their framework, named XLM-SWCM, on four Chinese minority languages, showing that it outperforms larger models in various tasks.'}, 'zh': {'title': '为极低资源语言赋能的多语言文本生成框架', 'desc': '多语言模型如XLM-R在自然语言处理中的多语言能力有所提升，但在极低资源语言上表现仍然较差。现代大型语言模型如LLaMA和Qwen支持的语言数量远少于XLM-R，导致许多语言缺乏文本生成模型。为了解决这个问题，我们提出了一种新框架，将多语言编码器适应于极低资源语言的文本生成。通过重用编码器和解码器之间的权重，我们的框架能够利用编码器学习到的语义空间，从而在低资源语言中实现高效学习和有效泛化。'}}}, {'id': 'https://huggingface.co/papers/2502.12130', 'title': 'Scaling Autonomous Agents via Automatic Reward Modeling And Planning', 'url': 'https://huggingface.co/papers/2502.12130', 'abstract': "Large language models (LLMs) have demonstrated remarkable capabilities across a range of text-generation tasks. However, LLMs still struggle with problems requiring multi-step decision-making and environmental feedback, such as online shopping, scientific reasoning, and mathematical problem-solving. Unlike pure text data, collecting large-scale decision-making data is challenging. Moreover, many powerful LLMs are only accessible through APIs, which hinders their fine-tuning for agent tasks due to cost and complexity. To address LLM agents' limitations, we propose a framework that can automatically learn a reward model from the environment without human annotations. This model can be used to evaluate the action trajectories of LLM agents and provide heuristics for task planning. Specifically, our approach involves employing one LLM-based agent to navigate an environment randomly, generating diverse action trajectories. Subsequently, a separate LLM is leveraged to assign a task intent and synthesize a negative response alongside the correct response for each trajectory. These triplets (task intent, positive response, and negative response) are then utilized as training data to optimize a reward model capable of scoring action trajectories. The effectiveness and generalizability of our framework are demonstrated through evaluations conducted on different agent benchmarks. In conclusion, our proposed framework represents a significant advancement in enhancing LLM agents' decision-making capabilities. By automating the learning of reward models, we overcome the challenges of data scarcity and API limitations, potentially revolutionizing the application of LLMs in complex and interactive environments. This research paves the way for more sophisticated AI agents capable of tackling a wide range of real-world problems requiring multi-step decision-making.", 'score': 1, 'issue_id': 2307, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': 'dc6a66994fdf7f15', 'authors': ['Zhenfang Chen', 'Delin Chen', 'Rui Sun', 'Wenjun Liu', 'Chuang Gan'], 'affiliations': ['MIT-IBM Watson AI Lab', 'UMass Amherst', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2502.12130.jpg', 'data': {'categories': ['#agents', '#benchmark', '#reasoning', '#optimization', '#rlhf'], 'emoji': '🤖', 'ru': {'title': 'Автоматическое обучение LLM-агентов принятию решений', 'desc': 'Статья представляет новый подход к улучшению способностей языковых моделей (LLM) в задачах, требующих многоэтапного принятия решений. Авторы предлагают фреймворк для автоматического обучения модели вознаграждения без аннотаций человека. Этот метод использует один LLM-агент для случайной навигации по среде и генерации разнообразных траекторий действий, а другой LLM для создания обучающих данных. Эффективность подхода демонстрируется на различных агентных бенчмарках.'}, 'en': {'title': 'Empowering LLMs for Complex Decision-Making Tasks', 'desc': 'This paper presents a new framework to improve large language models (LLMs) in decision-making tasks that require multiple steps and feedback from the environment. The authors address the challenge of collecting large-scale decision-making data by proposing a method to automatically learn a reward model without needing human annotations. By using one LLM to generate diverse action trajectories and another to evaluate these actions, the framework creates training data that helps optimize the reward model. This advancement allows LLM agents to better navigate complex tasks, making them more effective in real-world applications that involve intricate decision-making processes.'}, 'zh': {'title': '自动学习奖励模型，提升LLM代理决策能力', 'desc': '大型语言模型（LLMs）在文本生成任务中表现出色，但在需要多步决策和环境反馈的问题上仍然存在困难。我们提出了一种框架，可以自动从环境中学习奖励模型，而无需人工标注，从而评估LLM代理的行动轨迹。该方法通过一个LLM代理随机导航环境，生成多样的行动轨迹，并利用另一个LLM为每个轨迹分配任务意图和合成正负响应。我们的框架在不同的代理基准测试中展示了有效性和通用性，显著提升了LLM代理的决策能力。'}}}, {'id': 'https://huggingface.co/papers/2502.10990', 'title': 'FinMTEB: Finance Massive Text Embedding Benchmark', 'url': 'https://huggingface.co/papers/2502.10990', 'abstract': 'Embedding models play a crucial role in representing and retrieving information across various NLP applications. Recent advances in large language models (LLMs) have further enhanced the performance of embedding models. While these models are often benchmarked on general-purpose datasets, real-world applications demand domain-specific evaluation. In this work, we introduce the Finance Massive Text Embedding Benchmark (FinMTEB), a specialized counterpart to MTEB designed for the financial domain. FinMTEB comprises 64 financial domain-specific embedding datasets across 7 tasks that cover diverse textual types in both Chinese and English, such as financial news articles, corporate annual reports, ESG reports, regulatory filings, and earnings call transcripts. We also develop a finance-adapted model, FinPersona-E5, using a persona-based data synthetic method to cover diverse financial embedding tasks for training. Through extensive evaluation of 15 embedding models, including FinPersona-E5, we show three key findings: (1) performance on general-purpose benchmarks shows limited correlation with financial domain tasks; (2) domain-adapted models consistently outperform their general-purpose counterparts; and (3) surprisingly, a simple Bag-of-Words (BoW) approach outperforms sophisticated dense embeddings in financial Semantic Textual Similarity (STS) tasks, underscoring current limitations in dense embedding techniques. Our work establishes a robust evaluation framework for financial NLP applications and provides crucial insights for developing domain-specific embedding models.', 'score': 1, 'issue_id': 2293, 'pub_date': '2025-02-16', 'pub_date_card': {'ru': '16 февраля', 'en': 'February 16', 'zh': '2月16日'}, 'hash': '4cba4906d5d024ae', 'authors': ['Yixuan Tang', 'Yi Yang'], 'affiliations': ['The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.10990.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#multilingual', '#transfer_learning', '#science', '#benchmark'], 'emoji': '💹', 'ru': {'title': 'FinMTEB: Новый стандарт оценки эмбеддингов в финансовой сфере', 'desc': 'Статья представляет FinMTEB - специализированный бенчмарк для оценки моделей эмбеддингов в финансовой сфере. Он включает 64 набора данных по 7 задачам на английском и китайском языках. Авторы разработали модель FinPersona-E5, адаптированную для финансовой области. Результаты показывают, что модели, адаптированные для конкретной области, превосходят модели общего назначения, а простой подход Bag-of-Words превосходит сложные плотные эмбеддинги в задачах семантического сходства текстов.'}, 'en': {'title': 'FinMTEB: Elevating Financial NLP with Domain-Specific Embeddings', 'desc': 'This paper introduces the Finance Massive Text Embedding Benchmark (FinMTEB), which is designed specifically for evaluating embedding models in the financial domain. It includes 64 datasets across 7 tasks, focusing on various types of financial texts in both Chinese and English. The authors also present FinPersona-E5, a finance-adapted model that utilizes a persona-based data synthesis method for training on financial tasks. The study reveals that general-purpose benchmarks do not correlate well with financial tasks, domain-adapted models perform better, and a simple Bag-of-Words approach can outperform complex dense embeddings in certain financial tasks.'}, 'zh': {'title': '金融领域的嵌入模型评估新标准', 'desc': '嵌入模型在自然语言处理（NLP）应用中扮演着重要角色，尤其是在信息表示和检索方面。本文介绍了金融大规模文本嵌入基准（FinMTEB），这是一个专为金融领域设计的评估工具，包含64个金融特定的嵌入数据集，涵盖7个任务。我们还开发了一个适应金融领域的模型FinPersona-E5，利用基于角色的数据合成方法来训练多样化的金融嵌入任务。研究结果表明，通用基准的表现与金融领域任务的相关性有限，领域适应模型的表现优于通用模型，而简单的词袋模型在金融语义文本相似性任务中表现出乎意料地优于复杂的密集嵌入。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (4)', '#agi (1)', '#alignment (1)', '#architecture (10)', '#audio (1)', '#benchmark (9)', '#cv (5)', '#data (5)', '#dataset (10)', '#diffusion (1)', '#ethics', '#games (2)', '#graphs (1)', '#hallucinations', '#healthcare (2)', '#inference (8)', '#interpretability (2)', '#leakage', '#long_context (1)', '#low_resource (1)', '#machine_translation (1)', '#math', '#multilingual (3)', '#multimodal (8)', '#open_source (7)', '#optimization (15)', '#plp', '#rag', '#reasoning (10)', '#rl (3)', '#rlhf (3)', '#robotics (3)', '#science (2)', '#security (1)', '#small_models', '#story_generation', '#survey (2)', '#synthetic (3)', '#training (20)', '#transfer_learning (4)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-02-20 02:12',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-02-20 02:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-02-20 02:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    