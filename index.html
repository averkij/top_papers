
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 12 papers. January 17.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">17 января</span> | <span id="title-articles-count">12 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-01-16.html">⬅️ <span id="prev-date">16.01</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-01-20.html">➡️ <span id="next-date">20.01</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-01.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '17 января', 'en': 'January 17', 'zh': '1月17日'};
        let feedDateNext = {'ru': '20.01', 'en': '01/20', 'zh': '1月20日'};
        let feedDatePrev = {'ru': '16.01', 'en': '01/16', 'zh': '1月16日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2501.09732', 'title': 'Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps', 'url': 'https://huggingface.co/papers/2501.09732', 'abstract': 'Generative models have made significant impacts across various domains, largely due to their ability to scale during training by increasing data, computational resources, and model size, a phenomenon characterized by the scaling laws. Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen. In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation. Specifically, we consider a search problem aimed at identifying better noises for the diffusion sampling process. We structure the design space along two axes: the verifiers used to provide feedback, and the algorithms used to find better noise candidates. Through extensive experiments on class-conditioned and text-conditioned image generation benchmarks, our findings reveal that increasing inference-time compute leads to substantial improvements in the quality of samples generated by diffusion models, and with the complicated nature of images, combinations of the components in the framework can be specifically chosen to conform with different application scenario.', 'score': 41, 'issue_id': 1720, 'pub_date': '2025-01-16', 'pub_date_card': {'ru': '16 января', 'en': 'January 16', 'zh': '1月16日'}, 'hash': '2ad32c666f91ba05', 'authors': ['Nanye Ma', 'Shangyuan Tong', 'Haolin Jia', 'Hexiang Hu', 'Yu-Chuan Su', 'Mingda Zhang', 'Xuan Yang', 'Yandong Li', 'Tommi Jaakkola', 'Xuhui Jia', 'Saining Xie'], 'affiliations': ['Google', 'MIT', 'NYU'], 'pdf_title_img': 'assets/pdf/title_img/2501.09732.jpg', 'data': {'categories': ['#diffusion', '#inference', '#benchmark', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Повышение качества генерации изображений за счет масштабирования вычислений при выводе', 'desc': 'Это исследование посвящено изучению поведения диффузионных моделей при масштабировании вычислений во время вывода. Авторы рассматривают задачу поиска лучших шумов для процесса сэмплирования диффузионной модели. Они структурируют пространство решений по двум осям: верификаторы для обратной связи и алгоритмы поиска лучших кандидатов шума. Эксперименты показывают, что увеличение вычислений при выводе приводит к значительному улучшению качества сгенерированных изображений.'}, 'en': {'title': 'Enhancing Diffusion Models: Scaling Inference for Better Image Generation', 'desc': 'This paper investigates how to enhance the performance of diffusion models during the inference phase by increasing computational resources. It highlights that, unlike Large Language Models (LLMs), diffusion models can adjust their inference process through the number of denoising steps, but improvements tend to plateau after a certain point. The authors propose a method to optimize the noise used in the diffusion sampling process by exploring different feedback verifiers and algorithms. Their experiments demonstrate that by strategically increasing computation during inference, the quality of generated images can be significantly improved, tailored to various application needs.'}, 'zh': {'title': '扩散模型推理时的计算扩展与性能提升', 'desc': '生成模型在多个领域产生了重要影响，主要得益于其在训练过程中通过增加数据、计算资源和模型规模来扩展的能力。最近的研究开始探讨大型语言模型（LLMs）在推理时的扩展行为，发现额外的计算可以进一步提高性能。与LLMs不同，扩散模型通过去噪步骤的数量灵活调整推理时的计算，尽管性能提升通常在几十步后趋于平稳。本文探讨了扩散模型在推理时的扩展行为，研究如何通过增加计算来进一步提高生成性能，特别是通过寻找更好的噪声来优化扩散采样过程。'}}}, {'id': 'https://huggingface.co/papers/2501.09751', 'title': 'OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking', 'url': 'https://huggingface.co/papers/2501.09751', 'abstract': "Machine writing with large language models often relies on retrieval-augmented generation. However, these approaches remain confined within the boundaries of the model's predefined scope, limiting the generation of content with rich information. Specifically, vanilla-retrieved information tends to lack depth, utility, and suffers from redundancy, which negatively impacts the quality of generated articles, leading to shallow, repetitive, and unoriginal outputs. To address these issues, we propose OmniThink, a machine writing framework that emulates the human-like process of iterative expansion and reflection. The core idea behind OmniThink is to simulate the cognitive behavior of learners as they progressively deepen their knowledge of the topics. Experimental results demonstrate that OmniThink improves the knowledge density of generated articles without compromising metrics such as coherence and depth. Human evaluations and expert feedback further highlight the potential of OmniThink to address real-world challenges in the generation of long-form articles.", 'score': 33, 'issue_id': 1722, 'pub_date': '2025-01-16', 'pub_date_card': {'ru': '16 января', 'en': 'January 16', 'zh': '1月16日'}, 'hash': '7e8d42358354f79b', 'authors': ['Zekun Xi', 'Wenbiao Yin', 'Jizhan Fang', 'Jialong Wu', 'Runnan Fang', 'Ningyu Zhang', 'Jiang Yong', 'Pengjun Xie', 'Fei Huang', 'Huajun Chen'], 'affiliations': ['Tongyi Lab, Alibaba Group', 'Zhejiang Key Laboratory of Big Data Intelligent Computing', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2501.09751.jpg', 'data': {'categories': ['#rag', '#story_generation', '#long_context', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'OmniThink: Имитация человеческого мышления для улучшения машинной генерации текста', 'desc': 'Статья представляет новый подход к генерации текста с использованием больших языковых моделей, названный OmniThink. Этот метод имитирует человеческий процесс итеративного расширения знаний и рефлексии, преодолевая ограничения стандартных методов извлечения информации. OmniThink улучшает плотность знаний в генерируемых статьях, не жертвуя связностью и глубиной. Эксперименты и оценки экспертов подтверждают эффективность OmniThink для решения реальных задач генерации длинных статей.'}, 'en': {'title': 'OmniThink: Elevating Machine Writing through Human-Like Learning', 'desc': 'This paper introduces OmniThink, a novel machine writing framework that enhances the capabilities of large language models by mimicking human cognitive processes. Unlike traditional retrieval-augmented generation methods, which often produce shallow and repetitive content, OmniThink focuses on iterative expansion and reflection to deepen knowledge on topics. The framework significantly improves the knowledge density of generated articles while maintaining coherence and depth, as shown by experimental results. Human evaluations and expert feedback confirm that OmniThink effectively addresses challenges in generating high-quality long-form content.'}, 'zh': {'title': 'OmniThink：提升机器写作的知识密度', 'desc': '本文提出了一种名为OmniThink的机器写作框架，旨在改善传统大语言模型在生成内容时的局限性。OmniThink模拟人类学习者的认知过程，通过迭代扩展和反思来加深对主题的理解。实验结果表明，OmniThink能够提高生成文章的知识密度，同时保持连贯性和深度等指标。人类评估和专家反馈进一步验证了OmniThink在生成长篇文章时解决实际问题的潜力。'}}}, {'id': 'https://huggingface.co/papers/2501.09755', 'title': 'Learnings from Scaling Visual Tokenizers for Reconstruction and Generation', 'url': 'https://huggingface.co/papers/2501.09755', 'abstract': "Visual tokenization via auto-encoding empowers state-of-the-art image and video generative models by compressing pixels into a latent space. Although scaling Transformer-based generators has been central to recent advances, the tokenizer component itself is rarely scaled, leaving open questions about how auto-encoder design choices influence both its objective of reconstruction and downstream generative performance. Our work aims to conduct an exploration of scaling in auto-encoders to fill in this blank. To facilitate this exploration, we replace the typical convolutional backbone with an enhanced Vision Transformer architecture for Tokenization (ViTok). We train ViTok on large-scale image and video datasets far exceeding ImageNet-1K, removing data constraints on tokenizer scaling. We first study how scaling the auto-encoder bottleneck affects both reconstruction and generation -- and find that while it is highly correlated with reconstruction, its relationship with generation is more complex. We next explored the effect of separately scaling the auto-encoders' encoder and decoder on reconstruction and generation performance. Crucially, we find that scaling the encoder yields minimal gains for either reconstruction or generation, while scaling the decoder boosts reconstruction but the benefits for generation are mixed. Building on our exploration, we design ViTok as a lightweight auto-encoder that achieves competitive performance with state-of-the-art auto-encoders on ImageNet-1K and COCO reconstruction tasks (256p and 512p) while outperforming existing auto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x fewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates competitive performance on image generation for ImageNet-1K and sets new state-of-the-art benchmarks for class-conditional video generation on UCF-101.", 'score': 23, 'issue_id': 1720, 'pub_date': '2025-01-16', 'pub_date_card': {'ru': '16 января', 'en': 'January 16', 'zh': '1月16日'}, 'hash': '426aa3415c3c0ef4', 'authors': ['Philippe Hansen-Estruch', 'David Yan', 'Ching-Yao Chung', 'Orr Zohar', 'Jialiang Wang', 'Tingbo Hou', 'Tao Xu', 'Sriram Vishwanath', 'Peter Vajda', 'Xinlei Chen'], 'affiliations': ['FAIR, Meta', 'GenAI, Meta', 'Stanford University', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2501.09755.jpg', 'data': {'categories': ['#cv', '#benchmark', '#video', '#optimization', '#architecture', '#diffusion'], 'emoji': '🔬', 'ru': {'title': 'ViTok: Оптимизация визуальной токенизации для генеративных моделей', 'desc': 'Статья исследует масштабирование автоэнкодеров для визуальной токенизации в генеративных моделях изображений и видео. Авторы представляют ViTok - легковесный автоэнкодер на основе Vision Transformer, обученный на масштабных датасетах. Исследование показывает, что масштабирование декодера улучшает реконструкцию, но неоднозначно влияет на генерацию. ViTok демонстрирует конкурентоспособную производительность при меньшем количестве FLOP и устанавливает новые рекорды в условной генерации видео.'}, 'en': {'title': 'Scaling Auto-Encoders for Enhanced Image and Video Generation', 'desc': 'This paper explores the scaling of auto-encoders, particularly focusing on the tokenizer component, which is crucial for image and video generation. The authors introduce ViTok, a Vision Transformer-based architecture that replaces traditional convolutional backbones, allowing for better scaling on large datasets. They investigate how different scaling strategies for the encoder and decoder affect both reconstruction and generative performance, finding that scaling the decoder is more beneficial for reconstruction. Ultimately, ViTok achieves competitive results with fewer computational resources and sets new benchmarks in image and video generation tasks.'}, 'zh': {'title': '自编码器的视觉标记化：提升生成模型的关键', 'desc': '本论文探讨了通过自编码器进行视觉标记化对图像和视频生成模型的影响。我们提出了一种增强的视觉变换器架构（ViTok），用于替代传统的卷积骨干网络，以提高标记化的效果。研究发现，自编码器的瓶颈规模与重建性能高度相关，但与生成性能的关系更为复杂。最终，ViTok在多个任务中表现出色，尤其是在视频重建和图像生成方面，展示了其在计算效率上的优势。'}}}, {'id': 'https://huggingface.co/papers/2501.09686', 'title': 'Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models', 'url': 'https://huggingface.co/papers/2501.09686', 'abstract': 'Language has long been conceived as an essential tool for human reasoning. The breakthrough of Large Language Models (LLMs) has sparked significant research interest in leveraging these models to tackle complex reasoning tasks. Researchers have moved beyond simple autoregressive token generation by introducing the concept of "thought" -- a sequence of tokens representing intermediate steps in the reasoning process. This innovative paradigm enables LLMs\' to mimic complex human reasoning processes, such as tree search and reflective thinking. Recently, an emerging trend of learning to reason has applied reinforcement learning (RL) to train LLMs to master reasoning processes. This approach enables the automatic generation of high-quality reasoning trajectories through trial-and-error search algorithms, significantly expanding LLMs\' reasoning capacity by providing substantially more training data. Furthermore, recent studies demonstrate that encouraging LLMs to "think" with more tokens during test-time inference can further significantly boost reasoning accuracy. Therefore, the train-time and test-time scaling combined to show a new research frontier -- a path toward Large Reasoning Model. The introduction of OpenAI\'s o1 series marks a significant milestone in this research direction. In this survey, we present a comprehensive review of recent progress in LLM reasoning. We begin by introducing the foundational background of LLMs and then explore the key technical components driving the development of large reasoning models, with a focus on automated data construction, learning-to-reason techniques, and test-time scaling. We also analyze popular open-source projects at building large reasoning models, and conclude with open challenges and future research directions.', 'score': 21, 'issue_id': 1720, 'pub_date': '2025-01-16', 'pub_date_card': {'ru': '16 января', 'en': 'January 16', 'zh': '1月16日'}, 'hash': '1c6b1b1f0235304c', 'authors': ['Fengli Xu', 'Qianyue Hao', 'Zefang Zong', 'Jingwei Wang', 'Yunke Zhang', 'Jingyi Wang', 'Xiaochong Lan', 'Jiahui Gong', 'Tianjian Ouyang', 'Fanjin Meng', 'Chenyang Shao', 'Yuwei Yan', 'Qinglong Yang', 'Yiwen Song', 'Sijian Ren', 'Xinyuan Hu', 'Yu Li', 'Jie Feng', 'Chen Gao', 'Yong Li'], 'affiliations': ['Emory University, Atlanta GA, USA', 'HKUST (GZ), Guangzhou, China', 'Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2501.09686.jpg', 'data': {'categories': ['#open_source', '#training', '#rl', '#survey', '#reasoning', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Путь к большим моделям рассуждений: новый рубеж в ИИ', 'desc': 'Этот обзор посвящен прогрессу в области рассуждений с использованием больших языковых моделей (LLM). Рассматриваются ключевые технические компоненты, способствующие развитию крупных моделей рассуждений, включая автоматизированное построение данных, методы обучения рассуждениям и масштабирование во время тестирования. Анализируются популярные проекты с открытым исходным кодом по созданию крупных моделей рассуждений. Обсуждаются открытые проблемы и направления будущих исследований в этой области.'}, 'en': {'title': 'Unlocking Human-Like Reasoning in Large Language Models', 'desc': "This paper discusses the advancements in Large Language Models (LLMs) and their application to complex reasoning tasks. It introduces the concept of 'thought', which represents intermediate reasoning steps, allowing LLMs to simulate human-like reasoning processes. The paper highlights the use of reinforcement learning to enhance LLMs' reasoning capabilities by generating high-quality reasoning trajectories through trial-and-error methods. Additionally, it emphasizes the importance of scaling both training and testing phases to improve reasoning accuracy, paving the way for the development of Large Reasoning Models."}, 'zh': {'title': '推动大型推理模型的研究新前沿', 'desc': '这篇论文探讨了大型语言模型（LLMs）在复杂推理任务中的应用。研究者们引入了“思考”的概念，通过中间步骤的令牌序列来模拟人类的推理过程。最近，强化学习（RL）被应用于训练LLMs，以自动生成高质量的推理轨迹，从而显著提高推理能力。论文还讨论了在测试时增加令牌数量以提高推理准确性的效果，并展望了大型推理模型的未来研究方向。'}}}, {'id': 'https://huggingface.co/papers/2501.09484', 'title': 'Exploring the Inquiry-Diagnosis Relationship with Advanced Patient Simulators', 'url': 'https://huggingface.co/papers/2501.09484', 'abstract': 'Online medical consultation (OMC) restricts doctors to gathering patient information solely through inquiries, making the already complex sequential decision-making process of diagnosis even more challenging. Recently, the rapid advancement of large language models has demonstrated a significant potential to transform OMC. However, most studies have primarily focused on improving diagnostic accuracy under conditions of relatively sufficient information, while paying limited attention to the "inquiry" phase of the consultation process. This lack of focus has left the relationship between "inquiry" and "diagnosis" insufficiently explored. In this paper, we first extract real patient interaction strategies from authentic doctor-patient conversations and use these strategies to guide the training of a patient simulator that closely mirrors real-world behavior. By inputting medical records into our patient simulator to simulate patient responses, we conduct extensive experiments to explore the relationship between "inquiry" and "diagnosis" in the consultation process. Experimental results demonstrate that inquiry and diagnosis adhere to the Liebig\'s law: poor inquiry quality limits the effectiveness of diagnosis, regardless of diagnostic capability, and vice versa. Furthermore, the experiments reveal significant differences in the inquiry performance of various models. To investigate this phenomenon, we categorize the inquiry process into four types: (1) chief complaint inquiry; (2) specification of known symptoms; (3) inquiry about accompanying symptoms; and (4) gathering family or medical history. We analyze the distribution of inquiries across the four types for different models to explore the reasons behind their significant performance differences. We plan to open-source the weights and related code of our patient simulator at https://github.com/LIO-H-ZEN/PatientSimulator.', 'score': 17, 'issue_id': 1721, 'pub_date': '2025-01-16', 'pub_date_card': {'ru': '16 января', 'en': 'January 16', 'zh': '1月16日'}, 'hash': 'aff7d86ad63040d9', 'authors': ['Zhaocheng Liu', 'Quan Tu', 'Wen Ye', 'Yu Xiao', 'Zhishou Zhang', 'Hengfu Cui', 'Yalun Zhu', 'Qiang Ju', 'Shizheng Li', 'Jian Xie'], 'affiliations': ['Baichuan Inc.', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2501.09484.jpg', 'data': {'categories': ['#data', '#training', '#science', '#open_source', '#healthcare'], 'emoji': '🩺', 'ru': {'title': 'Симуляция пациента для улучшения онлайн-диагностики с помощью ИИ', 'desc': 'Эта статья исследует процесс онлайн-медицинских консультаций с использованием больших языковых моделей. Авторы разработали симулятор пациента на основе реальных стратегий взаимодействия врача и пациента. Эксперименты показали, что качество опроса и диагностики взаимозависимы и подчиняются закону Либиха. Анализ различных моделей выявил значительные различия в эффективности опроса, которые были классифицированы по четырем типам.'}, 'en': {'title': 'Enhancing Diagnosis through Effective Inquiry in Online Medical Consultations', 'desc': "This paper addresses the challenges of online medical consultations (OMC) by focusing on the inquiry phase, which is crucial for accurate diagnosis. It utilizes large language models to create a patient simulator that mimics real patient interactions based on actual doctor-patient conversations. The study reveals that the quality of inquiry directly impacts diagnostic effectiveness, following Liebig's law, which states that the weakest link limits overall performance. Additionally, the research categorizes inquiry types and analyzes their distribution across different models, highlighting significant performance variations in inquiry effectiveness."}, 'zh': {'title': '优化询问，提升诊断效果', 'desc': '本文探讨了在线医疗咨询中询问与诊断之间的关系。我们从真实的医患对话中提取了患者互动策略，并利用这些策略训练了一个模拟患者的模型。实验结果表明，询问质量的差异直接影响诊断效果，且不同模型在询问表现上存在显著差异。我们将询问过程分为四种类型，并分析了不同模型在这些类型上的表现，以揭示其性能差异的原因。'}}}, {'id': 'https://huggingface.co/papers/2501.09756', 'title': 'SynthLight: Portrait Relighting with Diffusion Model by Learning to Re-render Synthetic Faces', 'url': 'https://huggingface.co/papers/2501.09756', 'abstract': "We introduce SynthLight, a diffusion model for portrait relighting. Our approach frames image relighting as a re-rendering problem, where pixels are transformed in response to changes in environmental lighting conditions. Using a physically-based rendering engine, we synthesize a dataset to simulate this lighting-conditioned transformation with 3D head assets under varying lighting. We propose two training and inference strategies to bridge the gap between the synthetic and real image domains: (1) multi-task training that takes advantage of real human portraits without lighting labels; (2) an inference time diffusion sampling procedure based on classifier-free guidance that leverages the input portrait to better preserve details. Our method generalizes to diverse real photographs and produces realistic illumination effects, including specular highlights and cast shadows, while preserving the subject's identity. Our quantitative experiments on Light Stage data demonstrate results comparable to state-of-the-art relighting methods. Our qualitative results on in-the-wild images showcase rich and unprecedented illumination effects. Project Page: https://vrroom.github.io/synthlight/", 'score': 15, 'issue_id': 1721, 'pub_date': '2025-01-16', 'pub_date_card': {'ru': '16 января', 'en': 'January 16', 'zh': '1月16日'}, 'hash': 'e6621d55eb165448', 'authors': ['Sumit Chaturvedi', 'Mengwei Ren', 'Yannick Hold-Geoffroy', 'Jingyuan Liu', 'Julie Dorsey', 'Zhixin Shu'], 'affiliations': ['Adobe Research', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2501.09756.jpg', 'data': {'categories': ['#dataset', '#3d', '#inference', '#cv', '#diffusion', '#training', '#synthetic'], 'emoji': '💡', 'ru': {'title': 'SynthLight: реалистичная перезасветка портретов с помощью диффузионной модели', 'desc': 'SynthLight - это диффузионная модель для перезасветки портретов. Модель рассматривает перезасветку как проблему повторного рендеринга, где пиксели трансформируются в ответ на изменения условий освещения окружающей среды. Авторы синтезировали датасет с помощью физически корректного рендеринга, симулируя трансформации освещения на 3D-моделях голов. Предложены две стратегии обучения и вывода для преодоления разрыва между синтетическими и реальными изображениями.'}, 'en': {'title': 'Revolutionizing Portrait Relighting with SynthLight', 'desc': 'SynthLight is a diffusion model designed for relighting portraits by treating the task as a re-rendering challenge influenced by environmental lighting changes. It utilizes a physically-based rendering engine to create a synthetic dataset that simulates how lighting affects 3D head models. The model employs multi-task training to utilize real portraits without specific lighting labels and a novel inference strategy that enhances detail preservation during the relighting process. The results show that SynthLight can effectively generalize to real images, producing realistic lighting effects while maintaining the identity of the subjects, outperforming existing methods in both quantitative and qualitative assessments.'}, 'zh': {'title': 'SynthLight：肖像重光照的新方法', 'desc': '我们介绍了SynthLight，这是一种用于肖像重光照的扩散模型。我们将图像重光照视为重新渲染的问题，通过物理基础渲染引擎合成数据集，以模拟在不同光照条件下的像素变换。我们提出了两种训练和推理策略，以缩小合成图像和真实图像之间的差距，利用真实人像进行多任务训练，并在推理时使用无分类器引导的扩散采样程序。我们的模型能够在多样的真实照片中推广，生成逼真的光照效果，同时保持主体的身份特征。'}}}, {'id': 'https://huggingface.co/papers/2501.09747', 'title': 'FAST: Efficient Action Tokenization for Vision-Language-Action Models', 'url': 'https://huggingface.co/papers/2501.09747', 'abstract': 'Autoregressive sequence models, such as Transformer-based vision-language action (VLA) policies, can be tremendously effective for capturing complex and generalizable robotic behaviors. However, such models require us to choose a tokenization of our continuous action signals, which determines how the discrete symbols predicted by the model map to continuous robot actions. We find that current approaches for robot action tokenization, based on simple per-dimension, per-timestep binning schemes, typically perform poorly when learning dexterous skills from high-frequency robot data. To address this challenge, we propose a new compression-based tokenization scheme for robot actions, based on the discrete cosine transform. Our tokenization approach, Frequency-space Action Sequence Tokenization (FAST), enables us to train autoregressive VLAs for highly dexterous and high-frequency tasks where standard discretization methods fail completely. Based on FAST, we release FAST+, a universal robot action tokenizer, trained on 1M real robot action trajectories. It can be used as a black-box tokenizer for a wide range of robot action sequences, with diverse action spaces and control frequencies. Finally, we show that, when combined with the pi0 VLA, our method can scale to training on 10k hours of robot data and match the performance of diffusion VLAs, while reducing training time by up to 5x.', 'score': 15, 'issue_id': 1721, 'pub_date': '2025-01-16', 'pub_date_card': {'ru': '16 января', 'en': 'January 16', 'zh': '1月16日'}, 'hash': '1ff64d2f7e62d274', 'authors': ['Karl Pertsch', 'Kyle Stachowicz', 'Brian Ichter', 'Danny Driess', 'Suraj Nair', 'Quan Vuong', 'Oier Mees', 'Chelsea Finn', 'Sergey Levine'], 'affiliations': ['Physical Intelligence', 'Stanford', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2501.09747.jpg', 'data': {'categories': ['#dataset', '#agents', '#training', '#games', '#optimization', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Революция в токенизации действий робота: от частотного пространства к универсальности', 'desc': 'Статья представляет новый метод токенизации действий робота под названием FAST (Frequency-space Action Sequence Tokenization), основанный на дискретном косинусном преобразовании. Этот подход позволяет обучать авторегрессионные модели VLA (Vision-Language Action) для высокочастотных и сложных задач манипулирования, где стандартные методы дискретизации не работают. Авторы также представляют FAST+, универсальный токенизатор действий робота, обученный на 1 миллионе реальных траекторий. В сочетании с моделью pi0 VLA, метод FAST позволяет обучаться на 10 тысячах часов данных робота и достигать производительности диффузионных VLA, сокращая время обучения до 5 раз.'}, 'en': {'title': 'Revolutionizing Robot Action Tokenization with FAST', 'desc': 'This paper introduces a new method for tokenizing continuous robot actions to improve the performance of autoregressive sequence models, specifically in the context of vision-language action (VLA) policies. The authors identify that traditional tokenization methods, which use simple binning techniques, struggle with high-frequency and dexterous robotic tasks. To overcome this limitation, they propose Frequency-space Action Sequence Tokenization (FAST), which utilizes the discrete cosine transform for better action representation. The results demonstrate that FAST can effectively train VLAs on extensive robot data, achieving performance comparable to diffusion models while significantly reducing training time.'}, 'zh': {'title': '提升机器人灵巧技能的标记化新方法', 'desc': '本文提出了一种新的机器人动作标记化方案，称为频率空间动作序列标记化（FAST），旨在解决现有基于简单分箱方法的标记化在学习灵巧技能时的不足。FAST利用离散余弦变换来有效地处理高频机器人数据，从而提高了模型在复杂任务中的表现。我们还发布了FAST+，这是一个通用的机器人动作标记器，能够处理多种动作序列和控制频率。通过与pi0 VLA结合，我们的方法在训练10,000小时的机器人数据时，能够与扩散VLA的性能相匹配，同时将训练时间减少了多达5倍。'}}}, {'id': 'https://huggingface.co/papers/2501.09038', 'title': 'Do generative video models learn physical principles from watching videos?', 'url': 'https://huggingface.co/papers/2501.09038', 'abstract': "AI video generation is undergoing a revolution, with quality and realism advancing rapidly. These advances have led to a passionate scientific debate: Do video models learn ``world models'' that discover laws of physics -- or, alternatively, are they merely sophisticated pixel predictors that achieve visual realism without understanding the physical principles of reality? We address this question by developing Physics-IQ, a comprehensive benchmark dataset that can only be solved by acquiring a deep understanding of various physical principles, like fluid dynamics, optics, solid mechanics, magnetism and thermodynamics. We find that across a range of current models (Sora, Runway, Pika, Lumiere, Stable Video Diffusion, and VideoPoet), physical understanding is severely limited, and unrelated to visual realism. At the same time, some test cases can already be successfully solved. This indicates that acquiring certain physical principles from observation alone may be possible, but significant challenges remain. While we expect rapid advances ahead, our work demonstrates that visual realism does not imply physical understanding. Our project page is at https://physics-iq.github.io; code at https://github.com/google-deepmind/physics-IQ-benchmark.", 'score': 14, 'issue_id': 1725, 'pub_date': '2025-01-14', 'pub_date_card': {'ru': '14 января', 'en': 'January 14', 'zh': '1月14日'}, 'hash': '6a5047e8681ddcc5', 'authors': ['Saman Motamed', 'Laura Culp', 'Kevin Swersky', 'Priyank Jaini', 'Robert Geirhos'], 'affiliations': ['Google DeepMind', 'INSAIT, Sofia University'], 'pdf_title_img': 'assets/pdf/title_img/2501.09038.jpg', 'data': {'categories': ['#benchmark', '#science', '#video'], 'emoji': '🧠', 'ru': {'title': 'Визуальный реализм не гарантирует понимание физики в ИИ', 'desc': 'Статья посвящена исследованию физического понимания в моделях генерации видео. Авторы разработали набор данных Physics-IQ для оценки способности моделей понимать законы физики. Результаты показывают, что современные модели имеют ограниченное физическое понимание, несмотря на визуальный реализм. Однако некоторые задачи уже успешно решаются, что указывает на потенциал изучения физических принципов из наблюдений.'}, 'en': {'title': 'Visual Realism vs. Physical Understanding in AI Video Generation', 'desc': "This paper explores whether AI video generation models truly understand the laws of physics or if they are just good at creating realistic images. The authors introduce Physics-IQ, a benchmark dataset designed to test models on their grasp of physical principles like fluid dynamics and thermodynamics. Their findings show that current models struggle with physical understanding, even though they can produce visually realistic videos. This suggests that while some physical concepts can be learned from observation, there are still significant gaps in the models' comprehension of reality."}, 'zh': {'title': '视觉真实感不等于物理理解', 'desc': '本论文探讨了AI视频生成技术的进展，特别是模型是否理解物理规律。我们开发了Physics-IQ，一个全面的基准数据集，只有通过深入理解流体动力学、光学、固体力学、磁学和热力学等物理原理才能解决。研究发现，当前模型在物理理解方面存在严重限制，且与视觉真实感无关。尽管某些测试案例已成功解决，但这表明仅通过观察获得某些物理原理仍面临重大挑战。'}}}, {'id': 'https://huggingface.co/papers/2501.09433', 'title': 'CaPa: Carve-n-Paint Synthesis for Efficient 4K Textured Mesh Generation', 'url': 'https://huggingface.co/papers/2501.09433', 'abstract': 'The synthesis of high-quality 3D assets from textual or visual inputs has become a central objective in modern generative modeling. Despite the proliferation of 3D generation algorithms, they frequently grapple with challenges such as multi-view inconsistency, slow generation times, low fidelity, and surface reconstruction problems. While some studies have addressed some of these issues, a comprehensive solution remains elusive. In this paper, we introduce CaPa, a carve-and-paint framework that generates high-fidelity 3D assets efficiently. CaPa employs a two-stage process, decoupling geometry generation from texture synthesis. Initially, a 3D latent diffusion model generates geometry guided by multi-view inputs, ensuring structural consistency across perspectives. Subsequently, leveraging a novel, model-agnostic Spatially Decoupled Attention, the framework synthesizes high-resolution textures (up to 4K) for a given geometry. Furthermore, we propose a 3D-aware occlusion inpainting algorithm that fills untextured regions, resulting in cohesive results across the entire model. This pipeline generates high-quality 3D assets in less than 30 seconds, providing ready-to-use outputs for commercial applications. Experimental results demonstrate that CaPa excels in both texture fidelity and geometric stability, establishing a new standard for practical, scalable 3D asset generation.', 'score': 12, 'issue_id': 1721, 'pub_date': '2025-01-16', 'pub_date_card': {'ru': '16 января', 'en': 'January 16', 'zh': '1月16日'}, 'hash': '8c7a54f21e46af7a', 'authors': ['Hwan Heo', 'Jangyeong Kim', 'Seongyeong Lee', 'Jeong A Wi', 'Junyoung Choi', 'Sangjun Ahn'], 'affiliations': ['Graphics AI Lab, NC Research'], 'pdf_title_img': 'assets/pdf/title_img/2501.09433.jpg', 'data': {'categories': ['#diffusion', '#3d', '#optimization'], 'emoji': '🎨', 'ru': {'title': 'CaPa: Революция в генерации 3D-моделей', 'desc': 'В статье представлен CaPa - фреймворк для генерации высококачественных 3D-моделей. Он использует двухэтапный процесс, разделяя создание геометрии и текстур с помощью латентной диффузионной модели и пространственно-разделенного внимания. CaPa также предлагает алгоритм для заполнения нетекстурированных областей, обеспечивая целостность результатов. Фреймворк генерирует 3D-модели менее чем за 30 секунд, превосходя аналоги по качеству текстур и стабильности геометрии.'}, 'en': {'title': 'CaPa: Fast and High-Fidelity 3D Asset Generation', 'desc': 'This paper presents CaPa, a novel framework for generating high-quality 3D assets from textual or visual inputs. It addresses common challenges in 3D generation, such as multi-view inconsistency and slow generation times, by separating geometry generation from texture synthesis. The framework utilizes a 3D latent diffusion model for consistent geometry creation and a Spatially Decoupled Attention mechanism for high-resolution texture synthesis. CaPa also includes a 3D-aware occlusion inpainting algorithm to enhance the final output, achieving high fidelity and stability in under 30 seconds.'}, 'zh': {'title': '高效生成高保真3D资产的CaPa框架', 'desc': '本论文介绍了一种名为CaPa的框架，用于高效生成高保真度的3D资产。该框架采用两阶段的过程，将几何体生成与纹理合成解耦。首先，使用3D潜在扩散模型生成几何体，确保多视角之间的结构一致性。然后，通过一种新颖的空间解耦注意力机制合成高分辨率纹理，并提出了3D感知的遮挡修复算法，最终在30秒内生成高质量的3D资产。'}}}, {'id': 'https://huggingface.co/papers/2501.09653', 'title': 'The Heap: A Contamination-Free Multilingual Code Dataset for Evaluating Large Language Models', 'url': 'https://huggingface.co/papers/2501.09653', 'abstract': 'The recent rise in the popularity of large language models has spurred the development of extensive code datasets needed to train them. This has left limited code available for collection and use in the downstream investigation of specific behaviors, or evaluation of large language models without suffering from data contamination. To address this problem, we release The Heap, a large multilingual dataset covering 57 programming languages that has been deduplicated with respect to other open datasets of code, enabling researchers to conduct fair evaluations of large language models without significant data cleaning overhead.', 'score': 9, 'issue_id': 1730, 'pub_date': '2025-01-16', 'pub_date_card': {'ru': '16 января', 'en': 'January 16', 'zh': '1月16日'}, 'hash': '6d731a1519dc2727', 'authors': ['Jonathan Katzy', 'Razvan Mihai Popescu', 'Arie van Deursen', 'Maliheh Izadi'], 'affiliations': ['Delft University of Technology Delft, The Netherlands'], 'pdf_title_img': 'assets/pdf/title_img/2501.09653.jpg', 'data': {'categories': ['#low_resource', '#multilingual', '#open_source', '#data', '#dataset'], 'emoji': '🗃️', 'ru': {'title': 'The Heap: чистый код для честной оценки языковых моделей', 'desc': "Статья описывает создание нового набора данных для обучения языковых моделей в области программирования. Набор данных под названием 'The Heap' охватывает 57 языков программирования и был дедуплицирован относительно других открытых наборов данных. Это позволяет исследователям проводить объективные оценки больших языковых моделей без необходимости значительной предварительной очистки данных. Создание 'The Heap' решает проблему ограниченности доступного кода для исследования специфических поведений моделей и их оценки без риска загрязнения данных."}, 'en': {'title': 'The Heap: A Clean Dataset for Fair Evaluation of Language Models', 'desc': 'This paper introduces The Heap, a comprehensive multilingual dataset that includes code from 57 programming languages. It addresses the challenge of data contamination in evaluating large language models by providing a deduplicated dataset, ensuring that the code is unique compared to existing open datasets. Researchers can utilize The Heap for downstream tasks without the burden of extensive data cleaning. This resource aims to facilitate fair assessments of model performance in coding tasks.'}, 'zh': {'title': '公平评估大型语言模型的新数据集', 'desc': '随着大型语言模型的流行，开发了大量的代码数据集来训练这些模型。然而，这导致可用于特定行为研究或评估大型语言模型的代码有限，且可能存在数据污染的问题。为了解决这个问题，我们发布了The Heap，这是一个覆盖57种编程语言的大型多语言数据集，经过去重处理，避免与其他开放代码数据集重复。这样，研究人员可以在不需要大量数据清理的情况下，公平地评估大型语言模型。'}}}, {'id': 'https://huggingface.co/papers/2501.08617', 'title': 'RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation', 'url': 'https://huggingface.co/papers/2501.08617', 'abstract': "Generative AI systems like foundation models (FMs) must align well with human values to ensure their behavior is helpful and trustworthy. While Reinforcement Learning from Human Feedback (RLHF) has shown promise for optimizing model performance using human judgments, existing RLHF pipelines predominantly rely on immediate feedback, which can fail to accurately reflect the downstream impact of an interaction on users' utility. We demonstrate that feedback based on evaluators' foresight estimates of downstream consequences systematically induces Goodhart's Law dynamics, incentivizing misaligned behaviors like sycophancy and deception and ultimately degrading user outcomes. To alleviate this, we propose decoupling evaluation from prediction by refocusing RLHF on hindsight feedback. Our theoretical analysis reveals that conditioning evaluator feedback on downstream observations mitigates misalignment and improves expected human utility, even when these observations are simulated by the AI system itself. To leverage this insight in a practical alignment algorithm, we introduce Reinforcement Learning from Hindsight Simulation (RLHS), which first simulates plausible consequences and then elicits feedback to assess what behaviors were genuinely beneficial in hindsight. We apply RLHS to two widely-employed online and offline preference optimization methods -- Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) -- and show empirically that misalignment is significantly reduced with both methods. Through an online human user study, we show that RLHS consistently outperforms RLHF in helping users achieve their goals and earns higher satisfaction ratings, despite being trained solely with simulated hindsight feedback. These results underscore the importance of focusing on long-term consequences, even simulated ones, to mitigate misalignment in RLHF.", 'score': 8, 'issue_id': 1720, 'pub_date': '2025-01-15', 'pub_date_card': {'ru': '15 января', 'en': 'January 15', 'zh': '1月15日'}, 'hash': 'f758bc630d8dd443', 'authors': ['Kaiqu Liang', 'Haimin Hu', 'Ryan Liu', 'Thomas L. Griffiths', 'Jaime Fernández Fisac'], 'affiliations': ['Department of Computer Science, Princeton University', 'Department of Electrical and Computer Engineering, Princeton University', 'Department of Psychology, Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2501.08617.jpg', 'data': {'categories': ['#rlhf', '#alignment', '#training', '#rl'], 'emoji': '🔮', 'ru': {'title': 'Взгляд в будущее для лучшей настройки ИИ', 'desc': 'Статья представляет новый метод обучения с подкреплением - Reinforcement Learning from Hindsight Simulation (RLHS). В отличие от стандартного RLHF, RLHS использует симуляцию долгосрочных последствий действий модели и оценку их полезности постфактум. Авторы показывают, что RLHS позволяет уменьшить проблему неправильной мотивации модели и улучшить соответствие человеческим ценностям. Эмпирические эксперименты демонстрируют превосходство RLHS над RLHF в достижении целей пользователей.'}, 'en': {'title': 'Aligning AI with Human Values through Hindsight Feedback', 'desc': "This paper addresses the challenge of aligning generative AI systems with human values using Reinforcement Learning from Human Feedback (RLHF). It identifies that relying on immediate feedback can lead to misaligned behaviors, such as sycophancy and deception, due to Goodhart's Law dynamics. The authors propose a new approach called Reinforcement Learning from Hindsight Simulation (RLHS), which uses simulated consequences to gather feedback on beneficial behaviors. Their experiments show that RLHS improves user satisfaction and goal achievement compared to traditional RLHF methods, highlighting the importance of considering long-term outcomes in AI alignment."}, 'zh': {'title': '关注长期后果，提升AI对齐性', 'desc': '这篇论文探讨了生成性人工智能系统如何更好地与人类价值观对齐，以确保其行为有益且可信。现有的基于人类反馈的强化学习（RLHF）方法主要依赖即时反馈，但这种反馈可能无法准确反映与用户效用相关的长期影响。作者提出了一种新的方法，称为基于事后模拟的强化学习（RLHS），通过模拟可能的后果来获取反馈，从而改善模型的对齐性。研究表明，RLHS在帮助用户实现目标和提高满意度方面，优于传统的RLHF方法。'}}}, {'id': 'https://huggingface.co/papers/2501.09503', 'title': 'AnyStory: Towards Unified Single and Multiple Subject Personalization in Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2501.09503', 'abstract': 'Recently, large-scale generative models have demonstrated outstanding text-to-image generation capabilities. However, generating high-fidelity personalized images with specific subjects still presents challenges, especially in cases involving multiple subjects. In this paper, we propose AnyStory, a unified approach for personalized subject generation. AnyStory not only achieves high-fidelity personalization for single subjects, but also for multiple subjects, without sacrificing subject fidelity. Specifically, AnyStory models the subject personalization problem in an "encode-then-route" manner. In the encoding step, AnyStory utilizes a universal and powerful image encoder, i.e., ReferenceNet, in conjunction with CLIP vision encoder to achieve high-fidelity encoding of subject features. In the routing step, AnyStory utilizes a decoupled instance-aware subject router to accurately perceive and predict the potential location of the corresponding subject in the latent space, and guide the injection of subject conditions. Detailed experimental results demonstrate the excellent performance of our method in retaining subject details, aligning text descriptions, and personalizing for multiple subjects. The project page is at https://aigcdesigngroup.github.io/AnyStory/ .', 'score': 7, 'issue_id': 1721, 'pub_date': '2025-01-16', 'pub_date_card': {'ru': '16 января', 'en': 'January 16', 'zh': '1月16日'}, 'hash': 'fb27e795153a9668', 'authors': ['Junjie He', 'Yuxiang Tuo', 'Binghui Chen', 'Chongyang Zhong', 'Yifeng Geng', 'Liefeng Bo'], 'affiliations': ['Institute for Intelligent Computing, Alibaba Tongyi Lab'], 'pdf_title_img': 'assets/pdf/title_img/2501.09503.jpg', 'data': {'categories': ['#cv', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'AnyStory: Высококачественная генерация персонализированных изображений с множественными субъектами', 'desc': 'Статья представляет AnyStory - новый подход к генерации персонализированных изображений с несколькими субъектами. Метод использует универсальный энкодер изображений ReferenceNet и CLIP для высококачественного кодирования характеристик субъектов. AnyStory применяет декуплированный маршрутизатор субъектов для точного определения их потенциального расположения в латентном пространстве. Эксперименты показывают превосходную производительность метода в сохранении деталей субъектов, соответствии текстовым описаниям и персонализации для нескольких субъектов одновременно.'}, 'en': {'title': 'AnyStory: Mastering Personalized Image Generation for Multiple Subjects', 'desc': "This paper introduces AnyStory, a novel method for generating personalized images with high fidelity, even when multiple subjects are involved. It employs an 'encode-then-route' strategy, where a powerful image encoder, ReferenceNet, captures detailed subject features. The routing mechanism uses an instance-aware subject router to accurately determine where each subject should be placed in the generated image. Experimental results show that AnyStory excels in maintaining subject details and aligning them with text descriptions, making it effective for both single and multiple subjects."}, 'zh': {'title': 'AnyStory：个性化主题生成的新方法', 'desc': '最近，大规模生成模型在文本到图像生成方面表现出色。然而，生成高保真度的个性化图像，尤其是涉及多个主题的情况，仍然面临挑战。本文提出了AnyStory，这是一种统一的个性化主题生成方法，能够在不牺牲主题保真的情况下，实现单个和多个主题的高保真个性化。AnyStory通过“编码-再路由”的方式建模主题个性化问题，利用强大的图像编码器和实例感知路由器，准确预测主题在潜在空间中的位置。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (1)', '#agi', '#alignment (1)', '#architecture (1)', '#audio', '#benchmark (3)', '#cv (3)', '#data (2)', '#dataset (4)', '#diffusion (4)', '#ethics', '#games (1)', '#graphs', '#hallucinations', '#healthcare (1)', '#inference (2)', '#interpretability', '#leakage', '#long_context (1)', '#low_resource (1)', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (2)', '#open_source (3)', '#optimization (4)', '#plp', '#rag (1)', '#reasoning (1)', '#rl (2)', '#rlhf (1)', '#robotics (1)', '#science (2)', '#security', '#small_models', '#story_generation (1)', '#survey (1)', '#synthetic (1)', '#training (5)', '#transfer_learning', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-01-18 18:26',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-01-18 18:26')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-01-18 18:26')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    