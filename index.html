
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 21 papers. June 11.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">11 июня</span> | <span id="title-articles-count">21 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-06-10.html">⬅️ <span id="prev-date">10.06</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-06-12.html">➡️ <span id="next-date">12.06</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-06.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'};
        let feedDateNext = {'ru': '12.06', 'en': '06/12', 'zh': '6月12日'};
        let feedDatePrev = {'ru': '10.06', 'en': '06/10', 'zh': '6月10日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.06751', 'title': 'Geopolitical biases in LLMs: what are the "good" and the "bad" countries\n  according to contemporary language models', 'url': 'https://huggingface.co/papers/2506.06751', 'abstract': "LLMs exhibit significant geopolitical biases in their interpretation of historical events, and simple debiasing methods have limited effectiveness; a novel dataset for further research is provided.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper evaluates geopolitical biases in LLMs with respect to various countries though an analysis of their interpretation of historical events with conflicting national perspectives (USA, UK, USSR, and China). We introduce a novel dataset with neutral event descriptions and contrasting viewpoints from different countries. Our findings show significant geopolitical biases, with models favoring specific national narratives. Additionally, simple debiasing prompts had a limited effect in reducing these biases. Experiments with manipulated participant labels reveal models' sensitivity to attribution, sometimes amplifying biases or recognizing inconsistencies, especially with swapped labels. This work highlights national narrative biases in LLMs, challenges the effectiveness of simple debiasing methods, and offers a framework and dataset for future geopolitical bias research.", 'score': 52, 'issue_id': 4234, 'pub_date': '2025-06-07', 'pub_date_card': {'ru': '7 июня', 'en': 'June 7', 'zh': '6月7日'}, 'hash': '87a1fbaf018382d4', 'authors': ['Mikhail Salnikov', 'Dmitrii Korzh', 'Ivan Lazichny', 'Elvir Karimov', 'Artyom Iudin', 'Ivan Oseledets', 'Oleg Y. Rogov', 'Alexander Panchenko', 'Natalia Loukachevitch', 'Elena Tutubalina'], 'affiliations': ['AIRI', 'Kazan Federal University', 'Lomonosov MSU', 'MIPT', 'MTUCI', 'Sber AI', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2506.06751.jpg', 'data': {'categories': ['#ethics', '#alignment', '#data', '#dataset'], 'emoji': '🌍', 'ru': {'title': 'Геополитические предубеждения в LLM: вызов объективности искусственного интеллекта', 'desc': 'Исследование оценивает геополитические предубеждения в больших языковых моделях (LLM) при интерпретации исторических событий с конфликтующими национальными перспективами. Авторы представляют новый набор данных с нейтральными описаниями событий и противоречивыми точками зрения разных стран. Результаты показывают значительные геополитические предубеждения, причем модели отдают предпочтение определенным национальным нарративам. Простые методы дебиасинга оказались малоэффективными в снижении этих предубеждений.'}, 'en': {'title': 'Uncovering Geopolitical Biases in Language Models', 'desc': 'This paper investigates the presence of geopolitical biases in large language models (LLMs) by analyzing how they interpret historical events from different national perspectives, specifically focusing on the USA, UK, USSR, and China. It introduces a new dataset that contains neutral descriptions of events alongside varying viewpoints from these countries to facilitate further research. The results indicate that LLMs tend to favor certain national narratives, demonstrating significant biases in their outputs. Additionally, the study finds that basic debiasing techniques are not very effective in mitigating these biases, suggesting a need for more robust methods in future research.'}, 'zh': {'title': '揭示大型语言模型的地缘政治偏见', 'desc': '本论文评估了大型语言模型（LLMs）在解释历史事件时的地缘政治偏见，特别是针对美国、英国、苏联和中国等国家的不同视角。我们引入了一个新数据集，包含中立的事件描述和来自不同国家的对立观点。研究结果显示，模型倾向于支持特定国家的叙事，且简单的去偏见方法效果有限。通过操控参与者标签的实验，我们发现模型对归因非常敏感，有时会放大偏见或识别不一致，尤其是在标签交换的情况下。'}}}, {'id': 'https://huggingface.co/papers/2506.09040', 'title': 'Autoregressive Semantic Visual Reconstruction Helps VLMs Understand\n  Better', 'url': 'https://huggingface.co/papers/2506.09040', 'abstract': 'Autoregressive Semantic Visual Reconstruction (ASVR) improves multimodal understanding by focusing on semantic reconstruction rather than raw visual appearance, enhancing performance across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Typical large vision-language models (LVLMs) apply autoregressive supervision solely to textual sequences, without fully incorporating the visual modality into the learning process. This results in three key limitations: (1) an inability to utilize images without accompanying captions, (2) the risk that captions omit critical visual details, and (3) the challenge that certain vision-centric content cannot be adequately conveyed through text. As a result, current LVLMs often prioritize vision-to-language alignment while potentially overlooking fine-grained visual information. While some prior works have explored autoregressive image generation, effectively leveraging autoregressive visual supervision to enhance image understanding remains an open challenge. In this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR), which enables joint learning of visual and textual modalities within a unified autoregressive framework. We show that autoregressively reconstructing the raw visual appearance of images does not enhance and may even impair multimodal understanding. In contrast, autoregressively reconstructing the semantic representation of images consistently improves comprehension. Notably, we find that even when models are given continuous image features as input, they can effectively reconstruct discrete semantic tokens, resulting in stable and consistent improvements across a wide range of multimodal understanding benchmarks. Our approach delivers significant performance gains across varying data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is available at https://github.com/AlenjandroWang/ASVR.', 'score': 26, 'issue_id': 4237, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '09d042607d92f156', 'authors': ['Dianyi Wang', 'Wei Song', 'Yikun Wang', 'Siyuan Wang', 'Kaicheng Yu', 'Zhongyu Wei', 'Jiaqi Wang'], 'affiliations': ['AutoLab, Westlake University', 'Fudan University', 'Shanghai AI Lab', 'Shanghai Innovation Institute', 'University of Southern California', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.09040.jpg', 'data': {'categories': ['#optimization', '#interpretability', '#benchmark', '#games', '#cv', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Семантическая реконструкция изображений для улучшения мультимодального понимания', 'desc': 'Авторегрессивная семантическая визуальная реконструкция (ASVR) улучшает мультимодальное понимание, фокусируясь на семантической реконструкции, а не на восстановлении исходного визуального образа. Этот подход позволяет эффективно использовать изображения без сопроводительных подписей и учитывать важные визуальные детали, которые могут быть упущены в текстовых описаниях. ASVR объединяет обучение визуальной и текстовой модальностей в единой авторегрессивной структуре, что приводит к значительному улучшению производительности на различных мультимодальных тестах. Метод показывает стабильные улучшения при различных масштабах данных и типах базовых языковых моделей.'}, 'en': {'title': 'Revolutionizing Multimodal Understanding with Semantic Focus', 'desc': 'The paper introduces Autoregressive Semantic Visual Reconstruction (ASVR), a method that enhances multimodal understanding by focusing on reconstructing the semantic content of images rather than their raw visual appearance. This approach addresses limitations in existing large vision-language models (LVLMs) that primarily rely on textual sequences, which can lead to missing critical visual details. By enabling joint learning of visual and textual modalities, ASVR allows models to effectively reconstruct discrete semantic tokens from continuous image features, leading to improved comprehension. The results show significant performance gains across various benchmarks, demonstrating the effectiveness of semantic reconstruction in multimodal tasks.'}, 'zh': {'title': '自回归语义重建，提升多模态理解！', 'desc': '自回归语义视觉重建（ASVR）通过关注语义重建而非原始视觉外观，提升了多模态理解的能力。传统的大型视觉语言模型（LVLM）仅对文本序列应用自回归监督，未能充分整合视觉模态，导致无法利用没有配套说明的图像。ASVR 通过在统一的自回归框架内实现视觉和文本模态的联合学习，克服了这一限制。我们的研究表明，重建图像的语义表示能够显著提高多模态理解的效果。'}}}, {'id': 'https://huggingface.co/papers/2506.08672', 'title': 'RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic\n  Sampling', 'url': 'https://huggingface.co/papers/2506.08672', 'abstract': 'RuleReasoner enhances rule-based reasoning in small models through dynamic domain sampling, achieving superior performance and efficiency compared to large models.  \t\t\t\t\tAI-generated summary \t\t\t\t Rule-based reasoning has been acknowledged as one of the fundamental problems in reasoning, while deviations in rule formats, types, and complexity in real-world applications pose severe challenges. Recent studies have shown that large reasoning models (LRMs) have remarkable reasoning capabilities, and their performance is substantially enhanced by reinforcement learning (RL). However, it remains an open question whether small reasoning models (SRMs) can learn rule-based reasoning effectively with robust generalization across diverse tasks and domains. To address this, we introduce Reinforced Rule-based Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct rule-based reasoning via a wide collection of curated tasks and a novel domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples each training batch by updating the sampling weights of different domains based on historical rewards. This facilitates domain augmentation and flexible online learning schedules for RL, obviating the need for pre-hoc human-engineered mix-training recipes used in existing methods. Empirical evaluations on in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that RuleReasoner outperforms frontier LRMs by a significant margin (Delta4.1% average points on eight ID tasks and Delta10.4% average points on three OOD tasks over OpenAI-o1). Notably, our approach also exhibits higher computational efficiency compared to prior dynamic sampling methods for RL.', 'score': 23, 'issue_id': 4239, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': 'f9f7805577b3b091', 'authors': ['Yang Liu', 'Jiaqi Li', 'Zilong Zheng'], 'affiliations': ['NLCo Lab, Beijing Institute for General Artificial Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2506.08672.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#rl', '#optimization', '#small_models', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эффективное рассуждение на основе правил для малых моделей', 'desc': 'RuleReasoner - это новый метод для улучшения рассуждений на основе правил в небольших моделях машинного обучения. Он использует динамическую выборку доменов и подкрепляющее обучение для повышения производительности. RuleReasoner превосходит крупные модели рассуждений по точности на задачах в распределении и вне распределения. Метод также демонстрирует более высокую вычислительную эффективность по сравнению с существующими подходами.'}, 'en': {'title': 'Boosting Small Models with Dynamic Domain Sampling', 'desc': 'RuleReasoner is a method that improves rule-based reasoning in small models by using dynamic domain sampling. It addresses the challenges posed by varying rule formats and complexities in real-world applications. By updating sampling weights based on historical rewards, RuleReasoner enhances the learning process and generalization across different tasks. Empirical results show that it significantly outperforms large reasoning models while being more computationally efficient.'}, 'zh': {'title': '小模型的规则推理新突破', 'desc': 'RuleReasoner是一种增强小型模型规则推理的方法，通过动态领域采样实现了优越的性能和效率。该方法利用强化学习（RL）来优化规则推理，解决了现实应用中规则格式和复杂性带来的挑战。通过更新不同领域的采样权重，RuleReasoner能够灵活地进行在线学习，避免了传统方法中需要人工设计的混合训练方案。实验证明，RuleReasoner在多个任务上显著超越了大型推理模型，且计算效率更高。'}}}, {'id': 'https://huggingface.co/papers/2506.07927', 'title': 'Solving Inequality Proofs with Large Language Models', 'url': 'https://huggingface.co/papers/2506.07927', 'abstract': 'The investigation into inequality proving using large language models uncovers significant challenges in constructing rigorous proofs, revealing gaps between finding answers and generating valid step-wise solutions.  \t\t\t\t\tAI-generated summary \t\t\t\t Inequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategic theorem application. This makes it a distinct, demanding frontier for large language models (LLMs), offering insights beyond general mathematical problem-solving. Progress in this area is hampered by existing datasets that are often scarce, synthetic, or rigidly formal. We address this by proposing an informal yet verifiable task formulation, recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated dataset of Olympiad-level inequalities, including a test set and training corpus enriched with step-wise solutions and theorem annotations. We also develop a novel LLM-as-judge evaluation framework, combining a final-answer judge with four step-wise judges designed to detect common reasoning flaws. A systematic evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even top models like o1 achieve less than 10% overall accuracy under step-wise scrutiny; this is a drop of up to 65.5% from their accuracy considering only final answer equivalence. This discrepancy exposes fragile deductive chains and a critical gap for current LLMs between merely finding an answer and constructing a rigorous proof. Scaling model size and increasing test-time computation yield limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement. Code and data are available at https://ineqmath.github.io/.', 'score': 14, 'issue_id': 4235, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '0171dcd88ad3a14f', 'authors': ['Jiayi Sheng', 'Luna Lyu', 'Jikai Jin', 'Tony Xia', 'Alex Gu', 'James Zou', 'Pan Lu'], 'affiliations': ['Massachusetts Institute of Technology', 'Stanford University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2506.07927.jpg', 'data': {'categories': ['#survey', '#data', '#benchmark', '#dataset', '#reasoning', '#math'], 'emoji': '📊', 'ru': {'title': 'LLM могут найти ответ, но не могут доказать', 'desc': 'Исследование способности больших языковых моделей (LLM) доказывать неравенства выявило значительные трудности в построении строгих доказательств. Эксперты создали набор данных IneqMath с олимпиадными задачами и разработали новую систему оценки с использованием LLM в качестве судьи. Результаты показали, что даже лучшие модели достигают менее 10% точности при пошаговой проверке, что значительно ниже точности при учете только конечного ответа. Это исследование выявило существенный разрыв между способностью LLM находить ответ и строить корректное доказательство.'}, 'en': {'title': 'Bridging the Gap: From Answers to Rigorous Proofs in Inequality Proving', 'desc': 'This paper explores the challenges faced by large language models (LLMs) in the domain of inequality proving, which is essential for advanced reasoning in mathematics and science. It identifies a significant gap between generating answers and producing valid, step-by-step proofs, highlighting the limitations of current datasets that are often inadequate. The authors propose a new task formulation that breaks down inequality proving into two checkable subtasks: bound estimation and relation prediction, and introduce the IneqMath dataset for training and evaluation. Their evaluation of 29 leading LLMs reveals that even the best models struggle with rigorous proof construction, achieving less than 10% accuracy when assessed on step-wise reasoning, indicating a need for improved methodologies in theorem-guided reasoning and self-refinement.'}, 'zh': {'title': '揭示不等式证明中的推理挑战', 'desc': '这篇论文探讨了使用大型语言模型进行不等式证明的挑战，揭示了找到答案与生成有效逐步解决方案之间的差距。不等式证明在科学和数学领域至关重要，考验着高级推理能力，如发现紧界和战略性定理应用。为了应对现有数据集稀缺和形式化的问题，作者提出了一种非正式但可验证的任务形式，将不等式证明重构为两个可自动检查的子任务：界限估计和关系预测。此外，研究还发布了IneqMath数据集，并开发了一种新的评估框架，以检测常见的推理缺陷。'}}}, {'id': 'https://huggingface.co/papers/2506.08009', 'title': 'Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion', 'url': 'https://huggingface.co/papers/2506.08009', 'abstract': "Self Forcing, a novel training method for autoregressive video diffusion models, reduces exposure bias and improves generation quality through holistic video-level supervision and efficient caching mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Self Forcing, a novel training paradigm for autoregressive video diffusion models. It addresses the longstanding issue of exposure bias, where models trained on ground-truth context must generate sequences conditioned on their own imperfect outputs during inference. Unlike prior methods that denoise future frames based on ground-truth context frames, Self Forcing conditions each frame's generation on previously self-generated outputs by performing autoregressive rollout with key-value (KV) caching during training. This strategy enables supervision through a holistic loss at the video level that directly evaluates the quality of the entire generated sequence, rather than relying solely on traditional frame-wise objectives. To ensure training efficiency, we employ a few-step diffusion model along with a stochastic gradient truncation strategy, effectively balancing computational cost and performance. We further introduce a rolling KV cache mechanism that enables efficient autoregressive video extrapolation. Extensive experiments demonstrate that our approach achieves real-time streaming video generation with sub-second latency on a single GPU, while matching or even surpassing the generation quality of significantly slower and non-causal diffusion models. Project website: http://self-forcing.github.io/", 'score': 13, 'issue_id': 4235, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'e63130d065bba1fd', 'authors': ['Xun Huang', 'Zhengqi Li', 'Guande He', 'Mingyuan Zhou', 'Eli Shechtman'], 'affiliations': ['Adobe Research', 'The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2506.08009.jpg', 'data': {'categories': ['#optimization', '#video', '#diffusion', '#training'], 'emoji': '🎬', 'ru': {'title': 'Self Forcing: реалистичная генерация видео в реальном времени', 'desc': 'Представлен новый метод обучения авторегрессионных видео-диффузионных моделей под названием Self Forcing. Он решает проблему смещения экспозиции, обучая модель на собственных сгенерированных выходных данных вместо истинных. Self Forcing использует кэширование ключ-значение и целостную функцию потерь на уровне всего видео. Метод позволяет генерировать видео в реальном времени с низкой задержкой на одном GPU, сохраняя высокое качество.'}, 'en': {'title': 'Self Forcing: Enhancing Video Generation with Autoregressive Training', 'desc': 'The paper presents Self Forcing, a new training method for autoregressive video diffusion models that aims to reduce exposure bias and enhance video generation quality. It allows models to generate video frames based on their own previously generated outputs instead of relying solely on ground-truth frames, which helps in better training. By using a holistic video-level supervision approach, the method evaluates the quality of the entire video sequence rather than just individual frames. Additionally, it incorporates efficient caching mechanisms and a few-step diffusion model to optimize performance and ensure real-time video generation on a single GPU.'}, 'zh': {'title': '自我强制：提升视频生成质量的新方法', 'desc': 'Self Forcing是一种新颖的自回归视频扩散模型训练方法，旨在减少曝光偏差并提高生成质量。该方法通过整体视频级监督和高效缓存机制，解决了模型在推理时必须依赖自身不完美输出生成序列的问题。与以往方法不同，Self Forcing在训练过程中使用自生成输出进行条件生成，从而实现视频级的整体损失评估。实验表明，该方法能够在单个GPU上实现实时视频生成，且生成质量优于传统的非因果扩散模型。'}}}, {'id': 'https://huggingface.co/papers/2506.08002', 'title': 'Aligning Text, Images, and 3D Structure Token-by-Token', 'url': 'https://huggingface.co/papers/2506.08002', 'abstract': "A unified language, image, and 3D scene model framework is proposed, achieving optimal training and performance across various 3D tasks and datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Creating machines capable of understanding the world in 3D is essential in assisting designers that build and edit 3D environments and robots navigating and interacting within a three-dimensional space. Inspired by advances in language and image modeling, we investigate the potential of autoregressive models for a new modality: structured 3D scenes. To this end, we propose a unified LLM framework that aligns language, images, and 3D scenes and provide a detailed ''cookbook'' outlining critical design choices for achieving optimal training and performance addressing key questions related to data representation, modality-specific objectives, and more. We evaluate performance across four core 3D tasks -- rendering, recognition, instruction-following, and question-answering -- and four 3D datasets, synthetic and real-world. We extend our approach to reconstruct complex 3D object shapes by enriching our 3D modality with quantized shape encodings, and show our model's effectiveness on real-world 3D object recognition tasks. Project webpage: https://glab-caltech.github.io/kyvo/", 'score': 13, 'issue_id': 4232, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'ab2da61a8a27783d', 'authors': ['Aadarsh Sahoo', 'Vansh Tibrewal', 'Georgia Gkioxari'], 'affiliations': ['California Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.08002.jpg', 'data': {'categories': ['#optimization', '#training', '#synthetic', '#3d', '#multimodal'], 'emoji': '🌐', 'ru': {'title': 'Единая мультимодальная модель для понимания 3D-мира', 'desc': 'Предложена унифицированная модель для языка, изображений и 3D-сцен, достигающая оптимальной производительности в различных 3D-задачах. Модель основана на авторегрессионных методах и использует выравнивание модальностей. Авторы предоставляют детальное руководство по ключевым аспектам обучения, включая представление данных и специфические для модальностей целевые функции. Модель оценивается на четырех основных 3D-задачах и четырех наборах данных, демонстрируя эффективность в реконструкции сложных 3D-форм объектов.'}, 'en': {'title': 'Unifying Language, Image, and 3D Understanding for Enhanced Machine Interaction', 'desc': 'This paper presents a unified framework that integrates language, images, and 3D scenes to enhance machine understanding of three-dimensional environments. By leveraging autoregressive models, the authors explore how to effectively represent and process structured 3D scenes alongside traditional modalities. The framework includes a comprehensive guide for optimal training and performance, addressing critical aspects like data representation and modality-specific objectives. The model is evaluated on various 3D tasks, demonstrating its capability in rendering, recognition, instruction-following, and question-answering across both synthetic and real-world datasets.'}, 'zh': {'title': '统一三维场景模型，提升AI理解能力', 'desc': '本文提出了一种统一的语言、图像和三维场景模型框架，旨在实现各种三维任务和数据集的最佳训练和性能。该框架利用自回归模型，探索了结构化三维场景的新模式，帮助设计师构建和编辑三维环境。我们提供了一份详细的“食谱”，阐述了实现最佳训练和性能的关键设计选择，并评估了在四个核心三维任务和数据集上的表现。通过丰富三维模态的量化形状编码，我们的模型在复杂三维物体识别任务中展现了有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.04614', 'title': 'Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error\n  Diagnosis in GUI Automation', 'url': 'https://huggingface.co/papers/2506.04614', 'abstract': "A pre-operative critic mechanism with Suggestion-aware Gradient Relative Policy Optimization enhances the reliability of multimodal reasoning tasks in GUI automation.  \t\t\t\t\tAI-generated summary \t\t\t\t In recent years, Multimodal Large Language Models (MLLMs) have been extensively utilized for multimodal reasoning tasks, including Graphical User Interface (GUI) automation. Unlike general offline multimodal tasks, GUI automation is executed in online interactive environments, necessitating step-by-step decision-making based on real-time status of the environment. This task has a lower tolerance for decision-making errors at each step, as any mistakes may cumulatively disrupt the process and potentially lead to irreversible outcomes like deletions or payments. To address these issues, we introduce a pre-operative critic mechanism that provides effective feedback prior to the actual execution, by reasoning about the potential outcome and correctness of actions. Specifically, we propose a Suggestion-aware Gradient Relative Policy Optimization (S-GRPO) strategy to construct our pre-operative critic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance the reliability of the model's feedback. Furthermore, we develop a reasoning-bootstrapping based data collection pipeline to create a GUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic data. Static experiments on the GUI-Critic-Test across both mobile and web domains reveal that our GUI-Critic-R1 offers significant advantages in critic accuracy compared to current MLLMs. Dynamic evaluation on GUI automation benchmark further highlights the effectiveness and superiority of our model, as evidenced by improved success rates and operational efficiency.", 'score': 13, 'issue_id': 4236, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '9eb1f2457722a2cd', 'authors': ['Yuyang Wanyan', 'Xi Zhang', 'Haiyang Xu', 'Haowei Liu', 'Junyang Wang', 'Jiabo Ye', 'Yutong Kou', 'Ming Yan', 'Fei Huang', 'Xiaoshan Yang', 'Weiming Dong', 'Changsheng Xu'], 'affiliations': ['Alibaba Group', 'Beijing Jiaotong University', 'MAIS, Institute of Automation, Chinese Academy of Sciences, China', 'School of Artificial Intelligence, University of Chinese Academy of Sciences, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.04614.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#multimodal', '#optimization', '#rl', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Повышение надежности автоматизации GUI с помощью предоперационной критики', 'desc': 'Статья представляет новый механизм предоперационной критики для повышения надежности задач мультимодального рассуждения в автоматизации графического интерфейса пользователя (GUI). Авторы предлагают стратегию оптимизации политики с учетом градиента и предложений (S-GRPO) для создания модели GUI-Critic-R1. Исследование включает разработку процесса сбора данных на основе бутстрэппинга рассуждений для создания наборов данных GUI-Critic-Train и GUI-Critic-Test. Эксперименты показывают, что GUI-Critic-R1 превосходит существующие мультимодальные большие языковые модели (MLLM) в точности критики и эффективности автоматизации GUI.'}, 'en': {'title': 'Enhancing GUI Automation Reliability with Pre-operative Critique', 'desc': 'This paper presents a new method to improve the reliability of GUI automation using a pre-operative critic mechanism. The mechanism provides feedback before actions are executed, helping to prevent errors that could lead to serious issues like unwanted deletions. The authors introduce a Suggestion-aware Gradient Relative Policy Optimization (S-GRPO) strategy to enhance the feedback process, making it more effective. Experiments show that their model, GUI-Critic-R1, significantly outperforms existing models in both accuracy and operational efficiency during GUI automation tasks.'}, 'zh': {'title': '提升GUI自动化可靠性的预操作评估机制', 'desc': '本文提出了一种预操作评估机制，旨在提高图形用户界面（GUI）自动化中的多模态推理任务的可靠性。我们引入了一种名为建议感知梯度相对策略优化（S-GRPO）的策略，以构建预操作评估模型GUI-Critic-R1，并通过引入新颖的建议奖励来增强模型反馈的可靠性。该机制在实际执行之前提供有效反馈，帮助推理潜在结果和行动的正确性，从而减少决策错误的风险。通过在移动和网页领域的静态实验，我们的模型在评估准确性上显著优于现有的多模态大语言模型。'}}}, {'id': 'https://huggingface.co/papers/2506.07177', 'title': 'Frame Guidance: Training-Free Guidance for Frame-Level Control in Video\n  Diffusion Models', 'url': 'https://huggingface.co/papers/2506.07177', 'abstract': 'Frame Guidance offers a training-free method for controlling video generation using frame-level signals, reducing memory usage and enhancing globally coherent video output.  \t\t\t\t\tAI-generated summary \t\t\t\t Advancements in diffusion models have significantly improved video quality, directing attention to fine-grained controllability. However, many existing methods depend on fine-tuning large-scale video models for specific tasks, which becomes increasingly impractical as model sizes continue to grow. In this work, we present Frame Guidance, a training-free guidance for controllable video generation based on frame-level signals, such as keyframes, style reference images, sketches, or depth maps. For practical training-free guidance, we propose a simple latent processing method that dramatically reduces memory usage, and apply a novel latent optimization strategy designed for globally coherent video generation. Frame Guidance enables effective control across diverse tasks, including keyframe guidance, stylization, and looping, without any training, compatible with any video models. Experimental results show that Frame Guidance can produce high-quality controlled videos for a wide range of tasks and input signals.', 'score': 12, 'issue_id': 4233, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 июня', 'en': 'June 8', 'zh': '6月8日'}, 'hash': '7d83fcff01c3595a', 'authors': ['Sangwon Jang', 'Taekyung Ki', 'Jaehyeong Jo', 'Jaehong Yoon', 'Soo Ye Kim', 'Zhe Lin', 'Sung Ju Hwang'], 'affiliations': ['Adobe Research', 'DeepAuto.ai', 'KAIST', 'UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2506.07177.jpg', 'data': {'categories': ['#optimization', '#video', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Управление генерацией видео без переобучения модели', 'desc': 'Статья представляет метод Frame Guidance для управления генерацией видео без дополнительного обучения моделей. Этот подход использует покадровые сигналы, такие как ключевые кадры, эталонные изображения стиля или карты глубины. Frame Guidance значительно снижает использование памяти благодаря обработке латентного пространства. Метод применяет новую стратегию оптимизации латентного пространства для создания глобально согласованных видео.'}, 'en': {'title': 'Effortless Control in Video Generation with Frame Guidance', 'desc': 'Frame Guidance introduces a novel approach to video generation that does not require any training, allowing for effective control using frame-level signals. This method significantly reduces memory usage while ensuring that the generated videos maintain global coherence. By utilizing a simple latent processing technique and a unique latent optimization strategy, Frame Guidance can adapt to various tasks such as keyframe guidance and stylization. The results demonstrate that this approach can produce high-quality videos across different input types without the need for fine-tuning large models.'}, 'zh': {'title': '无训练的视频生成控制新方法', 'desc': '本论文提出了一种名为Frame Guidance的方法，用于控制视频生成，且无需训练。该方法利用帧级信号，如关键帧和风格参考图像，来实现对视频生成的精细控制。Frame Guidance显著降低了内存使用，并增强了视频输出的全局一致性。实验结果表明，该方法能够在多种任务中生成高质量的可控视频，且与任何视频模型兼容。'}}}, {'id': 'https://huggingface.co/papers/2506.08279', 'title': 'Seeing Voices: Generating A-Roll Video from Audio with Mirage', 'url': 'https://huggingface.co/papers/2506.08279', 'abstract': "Mirage generates realistic video from audio inputs, integrating with speech synthesis to create compelling multimodal content through a unified, self-attention-based training approach.  \t\t\t\t\tAI-generated summary \t\t\t\t From professional filmmaking to user-generated content, creators and consumers have long recognized that the power of video depends on the harmonious integration of what we hear (the video's audio track) with what we see (the video's image sequence). Current approaches to video generation either ignore sound to focus on general-purpose but silent image sequence generation or address both visual and audio elements but focus on restricted application domains such as re-dubbing. We introduce Mirage, an audio-to-video foundation model that excels at generating realistic, expressive output imagery from scratch given an audio input. When integrated with existing methods for speech synthesis (text-to-speech, or TTS), Mirage results in compelling multimodal video. When trained on audio-video footage of people talking (A-roll) and conditioned on audio containing speech, Mirage generates video of people delivering a believable interpretation of the performance implicit in input audio. Our central technical contribution is a unified method for training self-attention-based audio-to-video generation models, either from scratch or given existing weights. This methodology allows Mirage to retain generality as an approach to audio-to-video generation while producing outputs of superior subjective quality to methods that incorporate audio-specific architectures or loss components specific to people, speech, or details of how images or audio are captured. We encourage readers to watch and listen to the results of Mirage for themselves (see paper and comments for links).", 'score': 7, 'issue_id': 4242, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '5309366c2181217d', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#optimization', '#architecture', '#video', '#multimodal', '#audio', '#games'], 'emoji': '🎬', 'ru': {'title': 'Звук оживает: Mirage превращает аудио в реалистичное видео', 'desc': 'Mirage - это модель машинного обучения для генерации видео на основе аудио. Она использует единый подход на основе механизма самовнимания для создания реалистичных и выразительных видеопоследовательностей. Mirage может генерировать видео с говорящими людьми, интерпретируя входной аудиосигнал. В сочетании с синтезом речи эта модель позволяет создавать убедительный мультимодальный видеоконтент.'}, 'en': {'title': 'Transforming Audio into Realistic Video with Mirage', 'desc': 'Mirage is a novel model that generates realistic videos from audio inputs by combining audio and visual elements through a self-attention-based training method. Unlike previous models that either ignore sound or are limited to specific applications, Mirage creates expressive video content from scratch based on audio cues. It excels in generating videos of people speaking, accurately reflecting the performance implied in the audio. This unified approach enhances the quality of the generated videos, making them more compelling and realistic compared to traditional methods.'}, 'zh': {'title': 'Mirage：音频驱动的逼真视频生成', 'desc': 'Mirage是一种音频到视频的基础模型，能够根据音频输入生成逼真的视频图像。它结合了语音合成技术，创造出引人入胜的多模态内容。与传统方法不同，Mirage不仅关注视觉元素，还能有效处理音频信息，从而生成高质量的视频。该模型采用自注意力机制进行统一训练，确保了生成结果的优越性和通用性。'}}}, {'id': 'https://huggingface.co/papers/2506.05167', 'title': 'ECoRAG: Evidentiality-guided Compression for Long Context RAG', 'url': 'https://huggingface.co/papers/2506.05167', 'abstract': 'ECoRAG framework enhances LLM performance in ODQA by compressing retrieved documents based on evidentiality, reducing latency and token usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have shown remarkable performance in Open-Domain Question Answering (ODQA) by leveraging external documents through Retrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer context, context compression is necessary. However, prior compression methods do not focus on filtering out non-evidential information, which limit the performance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or ECoRAG framework. ECoRAG improves LLM performance by compressing retrieved documents based on evidentiality, ensuring whether answer generation is supported by the correct evidence. As an additional step, ECoRAG reflects whether the compressed content provides sufficient evidence, and if not, retrieves more until sufficient. Experiments show that ECoRAG improves LLM performance on ODQA tasks, outperforming existing compression methods. Furthermore, ECoRAG is highly cost-efficient, as it not only reduces latency but also minimizes token usage by retaining only the necessary information to generate the correct answer. Code is available at https://github.com/ldilab/ECoRAG.', 'score': 7, 'issue_id': 4231, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'd979315df3a92206', 'authors': ['Yeonseok Jeong', 'Jinsu Kim', 'Dohyeon Lee', 'Seung-won Hwang'], 'affiliations': ['IPAI, Seoul National University', 'Korea University', 'Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05167.jpg', 'data': {'categories': ['#rag', '#alignment', '#long_context'], 'emoji': '🔍', 'ru': {'title': 'ECoRAG: Умное сжатие для точных ответов', 'desc': 'ECoRAG - это новый фреймворк для улучшения производительности больших языковых моделей (LLM) в задачах открытого вопросно-ответного поиска (ODQA). Он использует сжатие полученных документов на основе доказательности, что позволяет снизить задержку и использование токенов. ECoRAG превосходит существующие методы сжатия и повышает эффективность LLM в задачах ODQA. Этот подход не только улучшает производительность, но и является экономически эффективным, сохраняя только необходимую информацию для генерации правильного ответа.'}, 'en': {'title': 'ECoRAG: Elevating LLMs with Evidence-Based Compression', 'desc': 'The ECoRAG framework enhances the performance of Large Language Models (LLMs) in Open-Domain Question Answering (ODQA) by focusing on evidentiality during document retrieval and compression. By filtering out non-evidential information, ECoRAG ensures that the generated answers are supported by relevant evidence, improving the overall accuracy of responses. Additionally, the framework optimizes resource usage by reducing latency and minimizing token consumption, making it more efficient than previous methods. Experiments demonstrate that ECoRAG significantly outperforms existing compression techniques in ODQA tasks.'}, 'zh': {'title': 'ECoRAG：提升问答性能的证据性压缩框架', 'desc': 'ECoRAG框架通过基于证据性压缩检索到的文档，提升了大型语言模型（LLM）在开放领域问答（ODQA）中的表现。该方法解决了以往压缩技术未能有效过滤非证据性信息的问题，从而提高了生成答案的准确性。ECoRAG确保生成的答案有足够的证据支持，并在必要时进行额外的文档检索。实验结果表明，ECoRAG在ODQA任务中优于现有的压缩方法，同时降低了延迟和令牌使用，具有很高的成本效益。'}}}, {'id': 'https://huggingface.co/papers/2506.07932', 'title': 'Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural\n  Compressor', 'url': 'https://huggingface.co/papers/2506.07932', 'abstract': 'A novel framework called Squeeze3D uses pre-trained models to compress 3D data efficiently, achieving high compression ratios while maintaining visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Squeeze3D, a novel framework that leverages implicit prior knowledge learnt by existing pre-trained 3D generative models to compress 3D data at extremely high compression ratios. Our approach bridges the latent spaces between a pre-trained encoder and a pre-trained generation model through trainable mapping networks. Any 3D model represented as a mesh, point cloud, or a radiance field is first encoded by the pre-trained encoder and then transformed (i.e. compressed) into a highly compact latent code. This latent code can effectively be used as an extremely compressed representation of the mesh or point cloud. A mapping network transforms the compressed latent code into the latent space of a powerful generative model, which is then conditioned to recreate the original 3D model (i.e. decompression). Squeeze3D is trained entirely on generated synthetic data and does not require any 3D datasets. The Squeeze3D architecture can be flexibly used with existing pre-trained 3D encoders and existing generative models. It can flexibly support different formats, including meshes, point clouds, and radiance fields. Our experiments demonstrate that Squeeze3D achieves compression ratios of up to 2187x for textured meshes, 55x for point clouds, and 619x for radiance fields while maintaining visual quality comparable to many existing methods. Squeeze3D only incurs a small compression and decompression latency since it does not involve training object-specific networks to compress an object.', 'score': 6, 'issue_id': 4233, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'a13860cb07518cb2', 'authors': ['Rishit Dagli', 'Yushi Guan', 'Sankeerth Durvasula', 'Mohammadreza Mofayezi', 'Nandita Vijaykumar'], 'affiliations': ['University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2506.07932.jpg', 'data': {'categories': ['#synthetic', '#architecture', '#3d'], 'emoji': '🗜️', 'ru': {'title': 'Эффективное сжатие 3D-данных с помощью нейросетей', 'desc': 'Squeeze3D - это новая система для сжатия 3D-данных, использующая предобученные модели. Она позволяет достичь высоких степеней сжатия при сохранении визуального качества для сеток, облаков точек и полей излучения. Система работает путем преобразования входных данных в компактное латентное представление с помощью энкодера, а затем восстановления 3D-модели генеративной моделью. Squeeze3D не требует специальных 3D-датасетов для обучения и обеспечивает быстрое сжатие и распаковку данных.'}, 'en': {'title': 'Efficient 3D Data Compression with Squeeze3D', 'desc': 'Squeeze3D is a new framework designed to compress 3D data efficiently using pre-trained models. It connects the latent spaces of a pre-trained encoder and a generative model through trainable mapping networks, allowing for high compression ratios while preserving visual quality. The framework can handle various 3D representations, such as meshes and point clouds, and is trained solely on synthetic data, eliminating the need for specific 3D datasets. Experiments show that Squeeze3D achieves impressive compression rates, making it a versatile tool for 3D data compression.'}, 'zh': {'title': 'Squeeze3D：高效压缩3D数据的新方法', 'desc': 'Squeeze3D是一个新颖的框架，利用预训练模型高效压缩3D数据，达到高压缩比并保持视觉质量。该方法通过可训练的映射网络连接预训练编码器和生成模型之间的潜在空间。3D模型首先被编码为紧凑的潜在代码，然后通过映射网络转换为生成模型的潜在空间，以重建原始3D模型。实验表明，Squeeze3D在不同格式的3D数据上实现了显著的压缩效果，同时延迟较小。'}}}, {'id': 'https://huggingface.co/papers/2506.07045', 'title': 'Interpretable and Reliable Detection of AI-Generated Images via Grounded\n  Reasoning in MLLMs', 'url': 'https://huggingface.co/papers/2506.07045', 'abstract': 'A dataset with annotations aids in fine-tuning MLLMs for accurate detection and localization of AI-generated images with meaningful explanations.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of image generation technologies intensifies the demand for interpretable and robust detection methods. Although existing approaches often attain high accuracy, they typically operate as black boxes without providing human-understandable justifications. Multi-modal Large Language Models (MLLMs), while not originally intended for forgery detection, exhibit strong analytical and reasoning capabilities. When properly fine-tuned, they can effectively identify AI-generated images and offer meaningful explanations. However, existing MLLMs still struggle with hallucination and often fail to align their visual interpretations with actual image content and human reasoning. To bridge this gap, we construct a dataset of AI-generated images annotated with bounding boxes and descriptive captions that highlight synthesis artifacts, establishing a foundation for human-aligned visual-textual grounded reasoning. We then finetune MLLMs through a multi-stage optimization strategy that progressively balances the objectives of accurate detection, visual localization, and coherent textual explanation. The resulting model achieves superior performance in both detecting AI-generated images and localizing visual flaws, significantly outperforming baseline methods.', 'score': 6, 'issue_id': 4241, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 июня', 'en': 'June 8', 'zh': '6月8日'}, 'hash': 'ced31e12be23b119', 'authors': ['Yikun Ji', 'Hong Yan', 'Jun Lan', 'Huijia Zhu', 'Weiqiang Wang', 'Qi Fan', 'Liqing Zhang', 'Jianfu Zhang'], 'affiliations': ['Ant Group', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07045.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#data', '#training', '#reasoning', '#optimization', '#hallucinations', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Интерпретируемое обнаружение ИИ-изображений с помощью мультимодальных языковых моделей', 'desc': 'Статья представляет новый подход к обнаружению и локализации изображений, сгенерированных искусственным интеллектом, с использованием мультимодальных языковых моделей (MLLM). Авторы создали датасет с аннотациями, содержащий ограничивающие рамки и описательные подписи, выделяющие артефакты синтеза. Используя многоэтапную стратегию оптимизации, они дообучили MLLM для точного обнаружения, визуальной локализации и создания связных текстовых объяснений. Полученная модель превосходит базовые методы в обнаружении сгенерированных ИИ изображений и локализации визуальных дефектов.'}, 'en': {'title': 'Enhancing AI Image Detection with Human-Aligned Reasoning', 'desc': "This paper presents a method to improve the detection and localization of AI-generated images using Multi-modal Large Language Models (MLLMs). It highlights the importance of a well-annotated dataset that includes bounding boxes and descriptive captions to enhance the model's understanding of visual artifacts. The authors propose a multi-stage optimization strategy to fine-tune MLLMs, aiming to balance accurate detection with coherent explanations. The results show that the fine-tuned model significantly outperforms existing methods in both identifying AI-generated images and providing meaningful insights into their flaws."}, 'zh': {'title': '微调MLLMs以检测AI生成图像的创新方法', 'desc': '本论文提出了一种利用带注释的数据集来微调多模态大型语言模型（MLLMs），以准确检测和定位AI生成的图像。随着图像生成技术的快速发展，对可解释和稳健的检测方法的需求日益增加。虽然现有方法通常具有高准确性，但它们往往作为黑箱操作，无法提供人类可理解的解释。通过构建一个带有边界框和描述性标题的AI生成图像数据集，本文为人类对齐的视觉-文本推理奠定了基础，并通过多阶段优化策略微调MLLMs，显著提高了检测和定位的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.08887', 'title': 'DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for\n  Parameter-Efficient Video-Text Retrieval', 'url': 'https://huggingface.co/papers/2506.08887', 'abstract': 'The paper proposes DiscoVLA to improve video-text retrieval using CLIP by addressing vision, language, and alignment discrepancies, achieving superior performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The parameter-efficient adaptation of the image-text pretraining model CLIP for video-text retrieval is a prominent area of research. While CLIP is focused on image-level vision-language matching, video-text retrieval demands comprehensive understanding at the video level. Three key discrepancies emerge in the transfer from image-level to video-level: vision, language, and alignment. However, existing methods mainly focus on vision while neglecting language and alignment. In this paper, we propose Discrepancy Reduction in Vision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all three discrepancies. Specifically, we introduce Image-Video Features Fusion to integrate image-level and video-level features, effectively tackling both vision and language discrepancies. Additionally, we generate pseudo image captions to learn fine-grained image-level alignment. To mitigate alignment discrepancies, we propose Image-to-Video Alignment Distillation, which leverages image-level alignment knowledge to enhance video-level alignment. Extensive experiments demonstrate the superiority of our DiscoVLA. In particular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous methods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is available at https://github.com/LunarShen/DsicoVLA.', 'score': 4, 'issue_id': 4232, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '068c2f58bc5049d2', 'authors': ['Leqi Shen', 'Guoqiang Gong', 'Tianxiang Hao', 'Tao He', 'Yifeng Zhang', 'Pengzhang Liu', 'Sicheng Zhao', 'Jungong Han', 'Guiguang Ding'], 'affiliations': ['BNRist', 'Department of Automation, Tsinghua University', 'GRG Banking Equipment Co., Ltd.', 'Hangzhou Zhuoxi Institute of Brain and Intelligence', 'JD.com', 'School of Software', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.08887.jpg', 'data': {'categories': ['#transfer_learning', '#alignment', '#video', '#multimodal'], 'emoji': '🎥', 'ru': {'title': 'Преодоление разрыва между изображениями и видео в мультимодальном поиске', 'desc': 'Статья представляет DiscoVLA - метод для улучшения поиска видео по текстовым запросам с использованием модели CLIP. Авторы решают проблемы несоответствия в зрении, языке и выравнивании при переходе от изображений к видео. DiscoVLA объединяет признаки изображений и видео, генерирует псевдо-подписи к изображениям и применяет дистилляцию знаний для улучшения выравнивания видео и текста. Эксперименты показывают превосходство DiscoVLA над существующими методами в задаче поиска видео по текстовым запросам.'}, 'en': {'title': 'Bridging Gaps in Video-Text Retrieval with DiscoVLA', 'desc': 'The paper introduces DiscoVLA, a method designed to enhance video-text retrieval by addressing three main discrepancies: vision, language, and alignment. Unlike existing approaches that primarily focus on visual aspects, DiscoVLA integrates both image and video features to improve understanding at the video level. It also generates pseudo image captions to refine image-level alignment and employs Image-to-Video Alignment Distillation to strengthen video-level alignment using knowledge from image-level data. Experimental results show that DiscoVLA significantly outperforms previous methods, achieving a notable improvement in retrieval accuracy.'}, 'zh': {'title': '提升视频-文本检索的DiscoVLA方法', 'desc': '本文提出了一种名为DiscoVLA的方法，旨在通过解决视觉、语言和对齐的差异来改善视频-文本检索。传统的CLIP模型主要关注图像级别的视觉-语言匹配，而视频-文本检索需要在视频级别上进行全面理解。我们的方法通过图像-视频特征融合来整合图像和视频的特征，有效应对视觉和语言的差异。同时，我们生成伪图像标题以学习细粒度的图像级别对齐，从而减轻对齐差异。实验结果表明，DiscoVLA在视频-文本检索任务中表现优越，尤其在MSRVTT数据集上取得了50.5%的R@1成绩。'}}}, {'id': 'https://huggingface.co/papers/2506.08500', 'title': 'DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in\n  Search-Augmented LLMs', 'url': 'https://huggingface.co/papers/2506.08500', 'abstract': 'CONFLICTS, a benchmark for evaluating how LLMs handle knowledge conflicts in RAG, reveals significant challenges in conflict resolution but shows improvement with explicit prompting.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval Augmented Generation (RAG) is a commonly used approach for enhancing large language models (LLMs) with relevant and up-to-date information. However, the retrieved sources can often contain conflicting information and it remains unclear how models should address such discrepancies. In this work, we first propose a novel taxonomy of knowledge conflict types in RAG, along with the desired model behavior for each type. We then introduce CONFLICTS, a high-quality benchmark with expert annotations of conflict types in a realistic RAG setting. CONFLICTS is the first benchmark that enables tracking progress on how models address a wide range of knowledge conflicts. We conduct extensive experiments on this benchmark, showing that LLMs often struggle to appropriately resolve conflicts between sources. While prompting LLMs to explicitly reason about the potential conflict in the retrieved documents significantly improves the quality and appropriateness of their responses, substantial room for improvement in future research remains.', 'score': 4, 'issue_id': 4241, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '3dec58d73ae743df', 'authors': ['Arie Cattan', 'Alon Jacovi', 'Ori Ram', 'Jonathan Herzig', 'Roee Aharoni', 'Sasha Goldshtein', 'Eran Ofek', 'Idan Szpektor', 'Avi Caciularu'], 'affiliations': ['Bar-Ilan University', 'Google Research'], 'pdf_title_img': 'assets/pdf/title_img/2506.08500.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#rag', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Разрешение конфликтов знаний: новый рубеж для LLM', 'desc': 'Статья представляет CONFLICTS - новый бенчмарк для оценки способности больших языковых моделей (LLM) разрешать конфликты знаний в контексте Retrieval Augmented Generation (RAG). Авторы предлагают таксономию типов конфликтов знаний и желаемое поведение модели для каждого типа. Эксперименты показывают, что LLM часто испытывают трудности с разрешением конфликтов между источниками. Явное указание моделям рассуждать о потенциальных конфликтах значительно улучшает качество и уместность их ответов.'}, 'en': {'title': 'Navigating Knowledge Conflicts in LLMs with CONFLICTS Benchmark', 'desc': 'This paper introduces CONFLICTS, a benchmark designed to evaluate how large language models (LLMs) manage knowledge conflicts in Retrieval Augmented Generation (RAG) systems. It presents a new taxonomy categorizing different types of knowledge conflicts and outlines the expected behavior of models when faced with these conflicts. The authors demonstrate that while LLMs often struggle with resolving discrepancies in retrieved information, providing explicit prompts can enhance their performance. The study highlights the need for further advancements in conflict resolution strategies within LLMs.'}, 'zh': {'title': '解决知识冲突，提升模型表现！', 'desc': '本论文提出了一个新的基准测试CONFLICTS，用于评估大型语言模型（LLMs）在检索增强生成（RAG）中处理知识冲突的能力。我们首先定义了知识冲突的类型，并描述了模型在每种类型下的期望行为。研究表明，LLMs在解决信息来源之间的冲突时面临显著挑战，但通过明确提示可以显著提高其响应的质量。尽管如此，未来的研究仍有很大的改进空间。'}}}, {'id': 'https://huggingface.co/papers/2506.08300', 'title': "Institutional Books 1.0: A 242B token dataset from Harvard Library's\n  collections, refined for accuracy and usability", 'url': 'https://huggingface.co/papers/2506.08300', 'abstract': "Institutional Books 1.0 provides a large dataset of public domain books from Harvard Library for training and inference of large language models, enhancing data accessibility and sustainability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) use data to learn about the world in order to produce meaningful correlations and predictions. As such, the nature, scale, quality, and diversity of the datasets used to train these models, or to support their work at inference time, have a direct impact on their quality. The rapid development and adoption of LLMs of varying quality has brought into focus the scarcity of publicly available, high-quality training data and revealed an urgent need to ground the stewardship of these datasets in sustainable practices with clear provenance chains. To that end, this technical report introduces Institutional Books 1.0, a large collection of public domain books originally digitized through Harvard Library's participation in the Google Books project, beginning in 2006. Working with Harvard Library, we extracted, analyzed, and processed these volumes into an extensively-documented dataset of historic texts. This analysis covers the entirety of Harvard Library's collection scanned as part of that project, originally spanning 1,075,899 volumes written in over 250 different languages for a total of approximately 250 billion tokens. As part of this initial release, the OCR-extracted text (original and post-processed) as well as the metadata (bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens, identified as being in the public domain have been made available. This report describes this project's goals and methods as well as the results of the analyses we performed, all in service of making this historical collection more accessible and easier for humans and machines alike to filter, read and use.", 'score': 4, 'issue_id': 4239, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '8e42e765c2efbe2a', 'authors': ['Matteo Cargnelutti', 'Catherine Brobston', 'John Hess', 'Jack Cushman', 'Kristi Mukk', 'Aristana Scourtas', 'Kyle Courtney', 'Greg Leppert', 'Amanda Watson', 'Martha Whitehead', 'Jonathan Zittrain'], 'affiliations': ['Harvard Law School Library', 'Harvard Law School, Harvard School of Engineering and Applied Sciences, Harvard Kennedy School', 'Harvard Library', 'Institutional Data Initiative, Harvard Law School Library', 'Library Innovation Lab, Harvard Law School Library'], 'pdf_title_img': 'assets/pdf/title_img/2506.08300.jpg', 'data': {'categories': ['#open_source', '#dataset', '#synthetic', '#data'], 'emoji': '📚', 'ru': {'title': 'Большие данные для больших моделей: историческая библиотека в цифровом формате', 'desc': 'Датасет Institutional Books 1.0 предоставляет обширную коллекцию книг из публичного достояния Гарвардской библиотеки для обучения и использования крупных языковых моделей. Он включает 983,004 тома на более чем 250 языках, содержащих около 242 миллиардов токенов. Датасет содержит OCR-извлеченный текст и метаданные книг, находящихся в открытом доступе. Проект направлен на повышение доступности исторической коллекции и облегчение ее использования как людьми, так и машинами.'}, 'en': {'title': 'Unlocking Knowledge: A Sustainable Dataset for Language Models', 'desc': 'Institutional Books 1.0 is a comprehensive dataset of public domain books sourced from Harvard Library, aimed at improving the training and inference processes of large language models (LLMs). The dataset includes over 983,000 volumes and approximately 242 billion tokens, providing a rich resource for enhancing the quality and diversity of training data. By ensuring clear provenance and sustainable practices in data stewardship, this initiative addresses the critical need for high-quality, publicly available datasets in the rapidly evolving field of machine learning. The project not only facilitates better model performance but also promotes accessibility to historical texts for both human and machine use.'}, 'zh': {'title': '提升语言模型的数据可用性与可持续性', 'desc': 'Institutional Books 1.0 是一个大型数据集，包含来自哈佛图书馆的公共领域书籍，旨在为大型语言模型的训练和推理提供数据支持。这些书籍的多样性和质量直接影响到语言模型的表现，因此需要高质量的训练数据。该项目从哈佛图书馆提取和处理了超过983,000本书籍的文本和元数据，确保数据的可访问性和可持续性。通过这个数据集，研究人员和开发者可以更方便地使用历史文本，推动机器学习的发展。'}}}, {'id': 'https://huggingface.co/papers/2506.07976', 'title': 'Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction', 'url': 'https://huggingface.co/papers/2506.07976', 'abstract': 'Test-Time Interaction (TTI) improves web agent performance by scaling interaction, enabling adaptive behavior and balancing exploration and exploitation without adding per-step compute.  \t\t\t\t\tAI-generated summary \t\t\t\t The current paradigm of test-time scaling relies on generating long reasoning traces ("thinking" more) before producing a response. In agent problems that require interaction, this can be done by generating thinking traces before acting in the world. However, this process does not allow agents to acquire new information from the environment or adapt their behavior over time. In this work, we propose to scale test-time interaction, an untapped dimension of test-time scaling that increases the agent\'s interaction horizon to enable running rich behaviors such as exploration, backtracking, and dynamic re-planning within a single rollout. To demonstrate the promise of this scaling dimension, we study the domain of web agents. We first show that even prompting-based interaction scaling without any training can improve task success on web benchmarks non-trivially. Building on this, we introduce TTI (Test-Time Interaction), a curriculum-based online reinforcement learning (RL) approach that trains agents by adaptively adjusting their rollout lengths. Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data web agents on WebVoyager and WebArena benchmarks. We further show that TTI enables agents to balance exploration and exploitation adaptively. Our results establish interaction scaling as a powerful, complementary axis to scaling per-step compute, offering new avenues for training adaptive agents.', 'score': 4, 'issue_id': 4239, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'b35082cb7ca5bb65', 'authors': ['Junhong Shen', 'Hao Bai', 'Lunjun Zhang', 'Yifei Zhou', 'Amrith Setlur', 'Shengbang Tong', 'Diego Caples', 'Nan Jiang', 'Tong Zhang', 'Ameet Talwalkar', 'Aviral Kumar'], 'affiliations': ['Carnegie Mellon University', 'New York University', 'Scribe', 'The AGI Company', 'University of California, Berkeley', 'University of Illinois Urbana-Champaign', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2506.07976.jpg', 'data': {'categories': ['#reasoning', '#rl', '#optimization', '#agents', '#open_source', '#training'], 'emoji': '🕸️', 'ru': {'title': 'Адаптивные веб-агенты: новые горизонты с масштабированием взаимодействия', 'desc': 'Статья представляет новый подход к улучшению производительности веб-агентов с использованием масштабирования взаимодействия во время тестирования (Test-Time Interaction, TTI). TTI позволяет агентам адаптивно корректировать свое поведение, балансируя между исследованием и эксплуатацией без увеличения вычислительных затрат на каждом шаге. Авторы демонстрируют эффективность TTI на бенчмарках WebVoyager и WebArena, используя языковую модель Gemma 3 12B. Результаты показывают, что масштабирование взаимодействия является мощным дополнением к масштабированию вычислений на каждом шаге для обучения адаптивных агентов.'}, 'en': {'title': 'Empowering Web Agents with Test-Time Interaction', 'desc': 'This paper introduces Test-Time Interaction (TTI), a novel approach that enhances the performance of web agents by allowing them to interact more effectively with their environment. Unlike traditional methods that focus on generating long reasoning traces before acting, TTI enables agents to adapt their behavior in real-time by increasing their interaction horizon. This method supports complex behaviors such as exploration, backtracking, and dynamic re-planning within a single decision-making process. The authors demonstrate that TTI significantly improves task success rates on web benchmarks, showcasing its potential as a complementary strategy to existing scaling techniques in reinforcement learning.'}, 'zh': {'title': '测试时交互：提升代理智能的新维度', 'desc': '本文提出了一种新的方法，称为测试时交互（TTI），旨在提高网络代理的性能。TTI通过扩展代理的交互范围，使其能够在单次执行中进行探索、回溯和动态重新规划。与传统的依赖长推理轨迹的方式不同，TTI允许代理在与环境互动时获取新信息并适应其行为。我们的实验表明，TTI在WebVoyager和WebArena基准测试中表现出色，展示了交互扩展作为一种强大的补充方法。'}}}, {'id': 'https://huggingface.co/papers/2506.05928', 'title': 'MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient\n  Fine-Tuning of Large Language Models', 'url': 'https://huggingface.co/papers/2506.05928', 'abstract': 'A heterogeneous Mixture-of-Adapters (MoA) approach enhances parameter-efficient fine-tuning in LLMs by integrating diverse adapter experts, outperforming homogeneous MoE-LoRA methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies integrate Low-Rank Adaptation (LoRA) and Mixture-of-Experts (MoE) to further enhance the performance of parameter-efficient fine-tuning (PEFT) methods in Large Language Model (LLM) applications. Existing methods employ homogeneous MoE-LoRA architectures composed of LoRA experts with either similar or identical structures and capacities. However, these approaches often suffer from representation collapse and expert load imbalance, which negatively impact the potential of LLMs. To address these challenges, we propose a heterogeneous Mixture-of-Adapters (MoA) approach. This method dynamically integrates PEFT adapter experts with diverse structures, leveraging their complementary representational capabilities to foster expert specialization, thereby enhancing the effective transfer of pre-trained knowledge to downstream tasks. MoA supports two variants: (i) Soft MoA achieves fine-grained integration by performing a weighted fusion of all expert outputs; (ii) Sparse MoA activates adapter experts sparsely based on their contribution, achieving this with negligible performance degradation. Experimental results demonstrate that heterogeneous MoA outperforms homogeneous MoE-LoRA methods in both performance and parameter efficiency. Our project is available at https://github.com/DCDmllm/MoA.', 'score': 4, 'issue_id': 4232, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': 'f78e6f70ebb69ac2', 'authors': ['Jie Cao', 'Tianwei Lin', 'Hongyang He', 'Rolan Yan', 'Wenqiao Zhang', 'Juncheng Li', 'Dongping Zhang', 'Siliang Tang', 'Yueting Zhuang'], 'affiliations': ['Tencent', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05928.jpg', 'data': {'categories': ['#transfer_learning', '#training', '#optimization', '#small_models'], 'emoji': '🧠', 'ru': {'title': 'Гетерогенная смесь адаптеров: новый шаг в эффективной настройке языковых моделей', 'desc': 'Статья представляет новый подход к эффективной настройке больших языковых моделей - гетерогенную Mixture-of-Adapters (MoA). MoA интегрирует различные адаптеры-эксперты, что позволяет преодолеть проблемы коллапса представлений и дисбаланса нагрузки экспертов, характерные для гомогенных методов MoE-LoRA. Предложены два варианта MoA: Soft MoA с взвешенным слиянием выходов всех экспертов и Sparse MoA с активацией только наиболее значимых экспертов. Эксперименты показывают, что MoA превосходит гомогенные методы MoE-LoRA как по производительности, так и по эффективности использования параметров.'}, 'en': {'title': 'Unlocking LLM Potential with Diverse Adapter Experts', 'desc': 'This paper introduces a new approach called heterogeneous Mixture-of-Adapters (MoA) for fine-tuning Large Language Models (LLMs) more efficiently. Unlike traditional homogeneous MoE-LoRA methods that use similar adapter experts, MoA combines diverse adapter structures to improve performance and prevent issues like representation collapse. The method includes two variants: Soft MoA, which fuses outputs from all experts, and Sparse MoA, which selectively activates experts based on their effectiveness. Experimental results show that MoA significantly outperforms existing methods in both performance and parameter efficiency.'}, 'zh': {'title': '异构适配器混合：提升大语言模型微调效率的创新方法', 'desc': '本文提出了一种异构的适配器混合（MoA）方法，以增强大语言模型（LLM）中参数高效微调的效果。与传统的同质MoE-LoRA方法不同，MoA集成了具有不同结构的适配器专家，从而克服了表示崩溃和专家负载不平衡的问题。该方法通过动态整合适配器专家的互补表示能力，促进了专家的专业化，提升了预训练知识向下游任务的有效转移。实验结果表明，异构MoA在性能和参数效率上均优于同质MoE-LoRA方法。'}}}, {'id': 'https://huggingface.co/papers/2506.05700', 'title': 'RKEFino1: A Regulation Knowledge-Enhanced Large Language Model', 'url': 'https://huggingface.co/papers/2506.05700', 'abstract': 'RKEFino1, a knowledge-enhanced financial reasoning model, addresses accuracy and compliance challenges in Digital Regulatory Reporting through fine-tuning with domain knowledge from XBRL, CDM, and MOF, and introduces a novel Numerical NER task.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) hold great promise for financial applications but introduce critical accuracy and compliance challenges in Digital Regulatory Reporting (DRR). To address these issues, we propose RKEFino1, a regulation knowledge-enhanced financial reasoning model built upon Fino1, fine-tuned with domain knowledge from XBRL, CDM, and MOF. We formulate two QA tasks-knowledge-based and mathematical reasoning-and introduce a novel Numerical NER task covering financial entities in both sentences and tables. Experimental results demonstrate the effectiveness and generalization capacity of RKEFino1 in compliance-critical financial tasks. We have released our model on Hugging Face.', 'score': 2, 'issue_id': 4231, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': 'a23a28bb68811316', 'authors': ['Yan Wang', 'Yueru He', 'Ruoyu Xiang', 'Jeff Zhao'], 'affiliations': ['Columbia University', 'New York University', 'The University of Texas at Austin', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05700.jpg', 'data': {'categories': ['#data', '#multimodal', '#reasoning', '#training', '#healthcare', '#open_source'], 'emoji': '📊', 'ru': {'title': 'Умная финансовая модель для точной регуляторной отчетности', 'desc': 'RKEFino1 - это модель финансового рассуждения, улучшенная знаниями о регулировании, построенная на основе Fino1. Она решает проблемы точности и соответствия нормам в цифровой регуляторной отчетности путем дообучения на доменных знаниях из XBRL, CDM и MOF. Модель сформулирована для решения двух задач вопросно-ответной системы: на основе знаний и математических рассуждений. RKEFino1 также вводит новую задачу числового распознавания именованных сущностей (NER) для финансовых объектов в предложениях и таблицах.'}, 'en': {'title': 'Enhancing Financial Compliance with RKEFino1', 'desc': 'RKEFino1 is a financial reasoning model designed to improve accuracy and compliance in Digital Regulatory Reporting (DRR). It enhances the Fino1 model by incorporating domain knowledge from XBRL, CDM, and MOF, which are essential for understanding financial regulations. The model introduces a new task called Numerical Named Entity Recognition (NER) to identify financial entities in both text and tabular formats. Experimental results show that RKEFino1 effectively addresses compliance challenges and generalizes well to various financial tasks.'}, 'zh': {'title': '知识增强的金融推理，提升合规性与准确性', 'desc': 'RKEFino1是一种增强知识的金融推理模型，旨在解决数字监管报告中的准确性和合规性挑战。该模型基于Fino1，并通过XBRL、CDM和MOF等领域知识进行微调。我们提出了两个问答任务——基于知识的问答和数学推理，并引入了一种新的数值命名实体识别任务，涵盖了句子和表格中的金融实体。实验结果表明，RKEFino1在合规性关键的金融任务中表现出色，具有良好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2506.07047', 'title': 'Mathesis: Towards Formal Theorem Proving from Natural Languages', 'url': 'https://huggingface.co/papers/2506.07047', 'abstract': "Recent advances in large language models show strong promise for formal reasoning. However, most LLM-based theorem provers have long been constrained by the need for expert-written formal statements as inputs, limiting their applicability to real-world problems expressed in natural language. We tackle this gap with Mathesis, the first end-to-end theorem proving pipeline processing informal problem statements. It contributes Mathesis-Autoformalizer, the first autoformalizer using reinforcement learning to enhance the formalization ability of natural language problems, aided by our novel LeanScorer framework for nuanced formalization quality assessment. It also proposes a Mathesis-Prover, which generates formal proofs from the formalized statements. To evaluate the real-world applicability of end-to-end formal theorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex problems from China's national college entrance exam. Our approach is carefully designed, with a thorough study of each component. Experiments demonstrate Mathesis's effectiveness, with the autoformalizer outperforming the best baseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other model combinations, achieving 64% accuracy on MiniF2F with pass@32 and a state-of-the-art 18% on Gaokao-Formal.", 'score': 1, 'issue_id': 4237, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 июня', 'en': 'June 8', 'zh': '6月8日'}, 'hash': 'd3bc82dde4f2b8bc', 'authors': ['Yu Xuejun', 'Jianyuan Zhong', 'Zijin Feng', 'Pengyi Zhai', 'Roozbeh Yousefzadeh', 'Wei Chong Ng', 'Haoxiong Liu', 'Ziyi Shou', 'Jing Xiong', 'Yudong Zhou', 'Claudia Beth Ong', 'Austen Jeremy Sugiarto', 'Yaoxi Zhang', 'Wai Ming Tai', 'Huan Cao', 'Dongcai Lu', 'Jiacheng Sun', 'Qiang Xu', 'Shen Xin', 'Zhenguo Li'], 'affiliations': ['Huawei Celia Team', 'Huawei Noahs Ark Lab', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.07047.jpg', 'data': {'categories': ['#rl', '#reasoning', '#training', '#benchmark', '#dataset', '#math'], 'emoji': '🧠', 'ru': {'title': 'Автоматизация формального доказательства теорем от естественного языка до математической логики', 'desc': 'Исследователи представили Mathesis - первый сквозной конвейер для доказательства теорем, обрабатывающий неформальные формулировки задач. Ключевым компонентом является Mathesis-Autoformalizer, использующий обучение с подкреплением для улучшения формализации естественно-языковых задач. Система также включает Mathesis-Prover для генерации формальных доказательств. Для оценки применимости подхода авторы создали набор данных Gaokao-Formal из 488 сложных задач китайского вступительного экзамена в вузы.'}, 'en': {'title': 'Bridging Natural Language and Formal Reasoning with Mathesis', 'desc': "This paper presents Mathesis, a novel end-to-end theorem proving system that processes informal problem statements, addressing the limitations of existing LLM-based theorem provers. It introduces Mathesis-Autoformalizer, which utilizes reinforcement learning to automatically convert natural language problems into formal statements, supported by the LeanScorer framework for assessing formalization quality. Additionally, the Mathesis-Prover generates formal proofs from these formalized statements. The system's effectiveness is validated through experiments on the Gaokao-Formal benchmark, demonstrating significant improvements in accuracy and pass rates compared to existing methods."}, 'zh': {'title': 'Mathesis：非正式问题的定理证明新突破', 'desc': '本论文介绍了Mathesis，这是第一个能够处理非正式问题陈述的端到端定理证明管道。它包括Mathesis-Autoformalizer，这是一个使用强化学习的自动形式化工具，能够提高自然语言问题的形式化能力，并通过LeanScorer框架评估形式化质量。论文还提出了Mathesis-Prover，能够从形式化的陈述中生成正式证明。通过在中国高考的488个复杂问题上进行评估，实验结果表明Mathesis在通过率和准确性上均优于现有模型。'}}}, {'id': 'https://huggingface.co/papers/2506.04688', 'title': 'MMRefine: Unveiling the Obstacles to Robust Refinement in Multimodal\n  Large Language Models', 'url': 'https://huggingface.co/papers/2506.04688', 'abstract': "MMRefine evaluates the error refinement capabilities of Multimodal Large Language Models through a benchmark that categorizes errors and identifies performance bottlenecks.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces MMRefine, a MultiModal Refinement benchmark designed to evaluate the error refinement capabilities of Multimodal Large Language Models (MLLMs). As the emphasis shifts toward enhancing reasoning during inference, MMRefine provides a framework that evaluates MLLMs' abilities to detect and correct errors across six distinct scenarios beyond just comparing final accuracy before and after refinement. Furthermore, the benchmark analyzes the refinement performance by categorizing errors into six error types. Experiments with various open and closed MLLMs reveal bottlenecks and factors impeding refinement performance, highlighting areas for improvement in effective reasoning enhancement. Our code and dataset are publicly available at https://github.com/naver-ai/MMRefine.", 'score': 1, 'issue_id': 4241, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'd7ca0acf8a8fe586', 'authors': ['Gio Paik', 'Geewook Kim', 'Jinbae Im'], 'affiliations': ['KAIST AI', 'NAVER Cloud AI', 'Theta One, Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2506.04688.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#multimodal', '#open_source'], 'emoji': '🔍', 'ru': {'title': 'MMRefine: Новый подход к оценке мультимодальных языковых моделей', 'desc': 'Статья представляет MMRefine - бенчмарк для оценки способностей мультимодальных языковых моделей (MLLM) обнаруживать и исправлять ошибки. Бенчмарк анализирует шесть различных сценариев и типов ошибок, выходя за рамки простого сравнения точности. Эксперименты с различными MLLM выявляют узкие места и факторы, препятствующие эффективному уточнению результатов. Исследование направлено на улучшение рассуждений моделей во время вывода.'}, 'en': {'title': 'Refining Reasoning: Enhancing MLLMs with MMRefine', 'desc': "MMRefine is a benchmark designed to assess how well Multimodal Large Language Models (MLLMs) can refine their outputs by correcting errors. It categorizes errors into six types and evaluates the models' performance in detecting and addressing these errors during inference. The focus is not just on the final accuracy but also on the models' reasoning capabilities before and after refinement. The findings from experiments highlight specific bottlenecks that hinder effective error correction, providing insights for future improvements in MLLMs."}, 'zh': {'title': '提升多模态模型的错误修正能力', 'desc': 'MMRefine是一个用于评估多模态大型语言模型（MLLMs）错误修正能力的基准测试。它不仅关注最终准确率的比较，还通过六种不同场景来检测和纠正错误。该基准将错误分为六种类型，以分析修正性能的瓶颈和影响因素。实验结果揭示了在推理增强方面的改进空间，促进了对MLLMs的进一步研究。'}}}, {'id': 'https://huggingface.co/papers/2506.04020', 'title': 'QQSUM: A Novel Task and Model of Quantitative Query-Focused\n  Summarization for Review-based Product Question Answering', 'url': 'https://huggingface.co/papers/2506.04020', 'abstract': 'QQSUM-RAG extends Retrieval-Augmented Generation to provide diverse, representative Key Point summaries with quantified opinions for product question answering, outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Review-based Product Question Answering (PQA) allows e-commerce platforms to automatically address customer queries by leveraging insights from user reviews. However, existing PQA systems generate answers with only a single perspective, failing to capture the diversity of customer opinions. In this paper we introduce a novel task Quantitative Query-Focused Summarization (QQSUM), which aims to summarize diverse customer opinions into representative Key Points (KPs) and quantify their prevalence to effectively answer user queries. While Retrieval-Augmented Generation (RAG) shows promise for PQA, its generated answers still fall short of capturing the full diversity of viewpoints. To tackle this challenge, our model QQSUM-RAG, which extends RAG, employs few-shot learning to jointly train a KP-oriented retriever and a KP summary generator, enabling KP-based summaries that capture diverse and representative opinions. Experimental results demonstrate that QQSUM-RAG achieves superior performance compared to state-of-the-art RAG baselines in both textual quality and quantification accuracy of opinions. Our source code is available at: https://github.com/antangrocket1312/QQSUMM', 'score': 1, 'issue_id': 4240, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '8e25a42b034fd30c', 'authors': ['An Quang Tang', 'Xiuzhen Zhang', 'Minh Ngoc Dinh', 'Zhuang Li'], 'affiliations': ['RMIT University, Australia'], 'pdf_title_img': 'assets/pdf/title_img/2506.04020.jpg', 'data': {'categories': ['#rag', '#optimization', '#multimodal', '#open_source', '#games'], 'emoji': '🛍️', 'ru': {'title': 'Умные ответы на вопросы о товарах с учетом разнообразия мнений покупателей', 'desc': 'Статья представляет новый метод QQSUM-RAG для ответов на вопросы о продуктах на основе отзывов. Этот подход расширяет технологию Retrieval-Augmented Generation, чтобы генерировать разнообразные ключевые моменты из отзывов и количественно оценивать их распространенность. QQSUM-RAG использует few-shot learning для совместного обучения ретривера и генератора суммаризаций. Эксперименты показывают, что QQSUM-RAG превосходит существующие методы по качеству текста и точности количественной оценки мнений.'}, 'en': {'title': 'Diverse Insights for Better Product Answers', 'desc': 'The paper introduces QQSUM-RAG, an advanced model that enhances Retrieval-Augmented Generation (RAG) for product question answering (PQA) by generating diverse and representative Key Point summaries. It addresses the limitation of existing PQA systems that typically provide answers from a single viewpoint, thereby missing the variety of customer opinions. QQSUM focuses on Quantitative Query-Focused Summarization, which not only summarizes diverse opinions but also quantifies their prevalence to improve response accuracy. Experimental results show that QQSUM-RAG outperforms current RAG methods in both the quality of generated text and the accuracy of opinion quantification.'}, 'zh': {'title': '多样化客户意见的智能摘要生成', 'desc': '本文提出了一种新的任务，称为定量查询聚焦摘要（QQSUM），旨在将多样化的客户意见总结为代表性的关键点（KPs），并量化其普遍性，以有效回答用户查询。QQSUM-RAG模型扩展了检索增强生成（RAG），通过少量学习联合训练KP导向的检索器和KP摘要生成器，从而生成能够捕捉多样化和代表性意见的摘要。实验结果表明，QQSUM-RAG在文本质量和意见量化准确性方面优于现有的RAG基线方法。该研究为电子商务平台的产品问答系统提供了更全面的客户意见视角。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (1)', '#agi', '#alignment (4)', '#architecture (2)', '#audio (1)', '#benchmark (7)', '#cv (1)', '#data (5)', '#dataset (6)', '#diffusion (2)', '#ethics (1)', '#games (3)', '#graphs', '#hallucinations (1)', '#healthcare (1)', '#inference', '#interpretability (2)', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math (2)', '#multilingual', '#multimodal (9)', '#open_source (5)', '#optimization (11)', '#plp', '#rag (3)', '#reasoning (9)', '#rl (4)', '#rlhf', '#robotics', '#science', '#security', '#small_models (2)', '#story_generation', '#survey (1)', '#synthetic (3)', '#training (8)', '#transfer_learning (2)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-06-11 23:12',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-11 23:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-11 23:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    