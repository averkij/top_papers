
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 9 papers. June 25.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">25 июня</span> | <span id="title-articles-count">9 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-06-24.html">⬅️ <span id="prev-date">24.06</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-06-26.html">➡️ <span id="next-date">26.06</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-06.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '25 июня', 'en': 'June 25', 'zh': '6月25日'};
        let feedDateNext = {'ru': '26.06', 'en': '06/26', 'zh': '6月26日'};
        let feedDatePrev = {'ru': '24.06', 'en': '06/24', 'zh': '6月24日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.16141', 'title': 'GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal\n  Reasoning', 'url': 'https://huggingface.co/papers/2506.16141', 'abstract': "GRPO-CARE, a reinforcement learning framework optimizing for consistency and correctness, outperforms standard GRPO on a new video understanding benchmark, SEED-Bench-R1, improving both performance and logical coherence in multimodal large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent reinforcement learning approaches, such as outcome-supervised GRPO, have advanced Chain-of-Thought reasoning in large language models (LLMs), yet their adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack of rigorous evaluation for MLLM post-training methods, we introduce SEED-Bench-R1, a benchmark with complex real-world videos requiring balanced perception and reasoning. It offers a large training set and evaluates generalization across three escalating challenges: in-distribution, cross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1, we find that standard GRPO, while improving answer accuracy, often reduces logical coherence between reasoning steps and answers, with only a 57.9% consistency rate. This stems from reward signals focusing solely on final answers, encouraging shortcuts, and strict KL penalties limiting exploration.To address this, we propose GRPO-CARE, a consistency-aware RL framework optimizing both answer correctness and reasoning coherence without explicit supervision. GRPO-CARE introduces a two-tiered reward: (1) a base reward for answer correctness, and (2) an adaptive consistency bonus, computed by comparing the model's reasoning-to-answer likelihood (via a slowly-evolving reference model) against group peers.This dual mechanism amplifies rewards for reasoning paths that are both correct and logically consistent. Replacing KL penalties with this adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1, achieving a 6.7% performance gain on the hardest evaluation level and a 24.5% improvement in consistency. It also shows strong transferability, improving model performance across diverse video understanding benchmarks. Our work contributes a systematically designed benchmark and a generalizable post-training framework, advancing the development of more interpretable and robust MLLMs.", 'score': 14, 'issue_id': 4471, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 июня', 'en': 'June 19', 'zh': '6月19日'}, 'hash': '0d8fc795754c4210', 'authors': ['Yi Chen', 'Yuying Ge', 'Rui Wang', 'Yixiao Ge', 'Junhao Cheng', 'Ying Shan', 'Xihui Liu'], 'affiliations': ['ARC Lab, Tencent PCG', 'The Chinese University of Hong Kong', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.16141.jpg', 'data': {'categories': ['#transfer_learning', '#rl', '#interpretability', '#multimodal', '#benchmark', '#training', '#optimization', '#video', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'GRPO-CARE: Согласованное обучение с подкреплением для мультимодальных ИИ', 'desc': 'GRPO-CARE - это новый фреймворк обучения с подкреплением для мультимодальных больших языковых моделей. Он оптимизирует как точность ответов, так и логическую согласованность рассуждений без явного контроля. GRPO-CARE вводит двухуровневую систему вознаграждений: базовую за правильность ответа и адаптивный бонус за согласованность. Фреймворк превосходит стандартный GRPO на новом бенчмарке SEED-Bench-R1 для понимания видео, улучшая производительность и логическую связность.'}, 'en': {'title': 'Enhancing Consistency and Correctness in Multimodal Learning', 'desc': 'The paper introduces GRPO-CARE, a new reinforcement learning framework that enhances the performance and logical coherence of multimodal large language models (MLLMs) in video understanding tasks. It addresses the limitations of standard GRPO, which often sacrifices reasoning consistency for accuracy, by implementing a two-tiered reward system that promotes both correct answers and coherent reasoning. The authors present SEED-Bench-R1, a benchmark designed to rigorously evaluate MLLMs on complex video tasks, revealing that GRPO-CARE significantly outperforms GRPO with a notable increase in consistency and performance. This work not only proposes a novel framework but also contributes a valuable benchmark for future research in MLLM development.'}, 'zh': {'title': '提升一致性与正确性的强化学习框架', 'desc': 'GRPO-CARE是一种强化学习框架，旨在优化一致性和正确性，超越了标准的GRPO方法。它在新的视频理解基准SEED-Bench-R1上表现出色，提升了多模态大语言模型的性能和逻辑连贯性。通过引入双重奖励机制，GRPO-CARE不仅关注答案的正确性，还鼓励推理过程的逻辑一致性。该框架在复杂的真实世界视频任务中展现了强大的迁移能力，推动了更具可解释性和鲁棒性的多模态大语言模型的发展。'}}}, {'id': 'https://huggingface.co/papers/2506.19851', 'title': 'AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion\n  Models', 'url': 'https://huggingface.co/papers/2506.19851', 'abstract': 'AnimaX creates multi-skeleton 3D animations by blending video diffusion model priors with skeleton-based control, using joint video-pose diffusion and shared positional encodings.  \t\t\t\t\tAI-generated summary \t\t\t\t We present AnimaX, a feed-forward 3D animation framework that bridges the motion priors of video diffusion models with the controllable structure of skeleton-based animation. Traditional motion synthesis methods are either restricted to fixed skeletal topologies or require costly optimization in high-dimensional deformation spaces. In contrast, AnimaX effectively transfers video-based motion knowledge to the 3D domain, supporting diverse articulated meshes with arbitrary skeletons. Our method represents 3D motion as multi-view, multi-frame 2D pose maps, and enables joint video-pose diffusion conditioned on template renderings and a textual motion prompt. We introduce shared positional encodings and modality-aware embeddings to ensure spatial-temporal alignment between video and pose sequences, effectively transferring video priors to motion generation task. The resulting multi-view pose sequences are triangulated into 3D joint positions and converted into mesh animation via inverse kinematics. Trained on a newly curated dataset of 160,000 rigged sequences, AnimaX achieves state-of-the-art results on VBench in generalization, motion fidelity, and efficiency, offering a scalable solution for category-agnostic 3D animation. Project page: https://anima-x.github.io/{https://anima-x.github.io/}.', 'score': 12, 'issue_id': 4470, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 июня', 'en': 'June 24', 'zh': '6月24日'}, 'hash': 'ce9a811b1a9e7d9d', 'authors': ['Zehuan Huang', 'Haoran Feng', 'Yangtian Sun', 'Yuanchen Guo', 'Yanpei Cao', 'Lu Sheng'], 'affiliations': ['Beihang University, China', 'The University of Hong Kong, China', 'Tsinghua University, China', 'VAST, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.19851.jpg', 'data': {'categories': ['#optimization', '#3d', '#transfer_learning', '#diffusion', '#dataset'], 'emoji': '🦾', 'ru': {'title': 'AnimaX: универсальная 3D-анимация с помощью диффузионных моделей видео', 'desc': 'AnimaX - это фреймворк для создания 3D-анимации, объединяющий преимущества диффузионных моделей видео с контролируемой структурой скелетной анимации. Метод представляет 3D-движение в виде многоракурсных 2D-карт поз и позволяет выполнять совместную диффузию видео и поз на основе шаблонных рендеров и текстового описания движения. AnimaX использует общие позиционные кодировки и модально-специфичные эмбеддинги для обеспечения пространственно-временного выравнивания между видео и последовательностями поз. Результаты демонстрируют передовую производительность в обобщении, точности движений и эффективности для анимации различных 3D-моделей.'}, 'en': {'title': 'Bridging Video Motion and 3D Animation with AnimaX', 'desc': 'AnimaX is a novel framework for creating 3D animations that combines the strengths of video diffusion models with skeleton-based control. It allows for the generation of animations without being limited to fixed skeletal structures, overcoming the challenges of high-dimensional optimization. By using multi-view, multi-frame 2D pose maps and shared positional encodings, AnimaX ensures that video motion knowledge is effectively transferred to 3D motion generation. This method has been trained on a large dataset and demonstrates superior performance in terms of generalization, motion fidelity, and efficiency for diverse animated characters.'}, 'zh': {'title': 'AnimaX：无类别 3D 动画的创新解决方案', 'desc': 'AnimaX 是一个前馈式的 3D 动画框架，它结合了视频扩散模型的运动先验和基于骨骼的可控结构。与传统的运动合成方法不同，AnimaX 支持任意骨骼的多样化关节网格，并有效地将视频中的运动知识转移到 3D 领域。该方法通过多视角、多帧的 2D 姿态图表示 3D 运动，并利用共享位置编码和模态感知嵌入确保视频和姿态序列之间的时空对齐。经过在一个包含 160,000 个绑定序列的新数据集上训练，AnimaX 在 VBench 上实现了最先进的结果，提供了一种可扩展的无类别 3D 动画解决方案。'}}}, {'id': 'https://huggingface.co/papers/2506.19290', 'title': 'Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in\n  LLMs', 'url': 'https://huggingface.co/papers/2506.19290', 'abstract': "An automated data-curation pipeline for software engineering improves large language model performance on SWE tasks, achieving state-of-the-art results with and without test-time scaling techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Software engineering (SWE) has recently emerged as a crucial testbed for next-generation LLM agents, demanding inherent capabilities in two critical dimensions: sustained iterative problem-solving (e.g., >50 interaction rounds) and long-context dependency resolution (e.g., >32k tokens). However, the data curation process in SWE remains notoriously time-consuming, as it heavily relies on manual annotation for code file filtering and the setup of dedicated runtime environments to execute and validate unit tests. Consequently, most existing datasets are limited to only a few thousand GitHub-sourced instances. To this end, we propose an incremental, automated data-curation pipeline that systematically scales both the volume and diversity of SWE datasets. Our dataset comprises 10,169 real-world Python task instances from 2,531 distinct GitHub repositories, each accompanied by a task specified in natural language and a dedicated runtime-environment image for automated unit-test validation. We have carefully curated over 8,000 successfully runtime-validated training trajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE model on these trajectories, we uncover a striking data scaling phenomenon: the trained model's performance for software engineering capabilities in LLMs continues to improve as the data size increases, showing no signs of saturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on the SWE-bench Verified benchmark without using verifiers or multiple rollouts, establishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based LLMs built on the OpenHands agent framework. Furthermore, with the incorporation of test-time scaling techniques, the performance further improves to 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter models. We release the Skywork-SWE-32B model checkpoint to accelerate future research.", 'score': 7, 'issue_id': 4471, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 июня', 'en': 'June 24', 'zh': '6月24日'}, 'hash': '7c26a879c3db6096', 'authors': ['Liang Zeng', 'Yongcong Li', 'Yuzhen Xiao', 'Changshi Li', 'Chris Yuhao Liu', 'Rui Yan', 'Tianwen Wei', 'Jujie He', 'Xuchen Song', 'Yang Liu', 'Yahui Zhou'], 'affiliations': ['Skywork AI, Kunlun Inc'], 'pdf_title_img': 'assets/pdf/title_img/2506.19290.jpg', 'data': {'categories': ['#open_source', '#dataset', '#data', '#long_context', '#benchmark', '#training'], 'emoji': '🚀', 'ru': {'title': 'Автоматизация курирования данных поднимает LLM на новый уровень в программной инженерии', 'desc': 'Исследователи разработали автоматизированный конвейер для курирования данных в области программной инженерии, что позволило значительно улучшить производительность больших языковых моделей (LLM) на задачах разработки программного обеспечения. Новый подход позволил создать набор данных из более чем 10 000 реальных задач Python с автоматизированной валидацией через модульные тесты. Обучение модели Skywork-SWE на этих данных показало непрерывное улучшение производительности с увеличением объема данных. Модель достигла наилучших результатов среди LLM на базе Qwen2.5-Coder-32B, как с применением техник масштабирования во время тестирования, так и без них.'}, 'en': {'title': 'Automating Data Curation to Boost LLMs in Software Engineering', 'desc': "This paper presents an automated data-curation pipeline designed to enhance the performance of large language models (LLMs) in software engineering (SWE) tasks. The pipeline addresses the challenges of manual data annotation and environment setup by providing a diverse dataset of over 10,000 real-world Python task instances from GitHub. The authors demonstrate that increasing the dataset size leads to improved model performance, with their Skywork-SWE model achieving state-of-the-art accuracy on the SWE-bench Verified benchmark. Additionally, the incorporation of test-time scaling techniques further boosts the model's performance, establishing new benchmarks for LLMs in SWE."}, 'zh': {'title': '自动化数据整理，提升软件工程模型表现', 'desc': '本文提出了一种自动化数据整理管道，旨在提升大型语言模型（LLM）在软件工程（SWE）任务上的表现。该管道通过系统性地扩展数据集的规模和多样性，解决了传统数据整理过程中的时间消耗问题。我们构建了一个包含10,169个真实Python任务实例的数据集，并成功验证了超过8,000个训练轨迹。实验结果表明，随着数据量的增加，模型在软件工程能力上的表现持续提升，最终在SWE-bench Verified基准测试中达到了38.0%的准确率，创造了新的最先进水平。'}}}, {'id': 'https://huggingface.co/papers/2506.19767', 'title': 'SRFT: A Single-Stage Method with Supervised and Reinforcement\n  Fine-Tuning for Reasoning', 'url': 'https://huggingface.co/papers/2506.19767', 'abstract': 'Supervised Reinforcement Fine-Tuning (SRFT) integrates Supervised Fine-Tuning and Reinforcement Learning through entropy-aware weighting to achieve high accuracy in language model optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have achieved remarkable progress in reasoning tasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) remains a fundamental challenge. Through comprehensive analysis of token distributions, learning dynamics, and integration mechanisms from entropy-based perspectives, we reveal key differences between these paradigms: SFT induces coarse-grained global changes to LLM policy distributions, while RL performs fine-grained selective optimizations, with entropy serving as a critical indicator of training effectiveness. Building on these observations, we propose Supervised Reinforcement Fine-Tuning (SRFT), a single-stage method that unifies both fine-tuning paradigms through entropy-aware weighting mechanisms. Our approach simultaneously applies SFT and RL to directly optimize the LLM using demonstrations and self-exploration rollouts rather than through two-stage sequential methods. Extensive experiments show that SRFT achieves 59.1% average accuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning benchmarks and 10.9% on three out-of-distribution benchmarks.', 'score': 5, 'issue_id': 4471, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 июня', 'en': 'June 24', 'zh': '6月24日'}, 'hash': 'dfdf02be41523939', 'authors': ['Yuqian Fu', 'Tinghong Chen', 'Jiajun Chai', 'Xihuai Wang', 'Songjun Tu', 'Guojun Yin', 'Wei Lin', 'Qichao Zhang', 'Yuanheng Zhu', 'Dongbin Zhao'], 'affiliations': ['Institute of Automation, Chinese Academy of Sciences', 'Meituan', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.19767.jpg', 'data': {'categories': ['#rl', '#rlhf', '#training', '#optimization', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Объединение supervised и reinforcement learning для улучшения языковых моделей', 'desc': 'Статья представляет новый метод обучения языковых моделей - Supervised Reinforcement Fine-Tuning (SRFT). Этот подход объединяет Supervised Fine-Tuning и Reinforcement Learning, используя механизм взвешивания на основе энтропии. SRFT применяет оба метода одновременно, оптимизируя языковую модель напрямую с помощью демонстраций и самоисследования. Эксперименты показывают, что SRFT превосходит другие методы на задачах математических рассуждений и вне распределения обучающих данных.'}, 'en': {'title': 'Unifying Fine-Tuning for Superior Language Model Performance', 'desc': 'Supervised Reinforcement Fine-Tuning (SRFT) combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to enhance the performance of language models. It uses entropy-aware weighting to balance the strengths of both methods, allowing for better optimization of model policies. The paper highlights how SFT makes broad changes to model behavior, while RL focuses on specific improvements, with entropy being a key measure of success. SRFT has been shown to significantly improve accuracy on various reasoning tasks compared to traditional methods.'}, 'zh': {'title': '监督强化微调：优化语言模型的新方法', 'desc': '监督强化微调（SRFT）结合了监督微调和强化学习，通过熵感知加权实现语言模型优化的高准确性。研究表明，监督微调会对语言模型的策略分布产生粗粒度的全局变化，而强化学习则进行细粒度的选择性优化，熵是训练效果的重要指标。SRFT方法通过熵感知加权机制，将这两种微调范式统一为单阶段方法，直接优化语言模型。实验结果显示，SRFT在五个数学推理基准上平均准确率达到59.1%，比零强化学习方法提高了9.0%。'}}}, {'id': 'https://huggingface.co/papers/2506.19794', 'title': 'Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic\n  Empirical Study', 'url': 'https://huggingface.co/papers/2506.19794', 'abstract': "Enhancements to open-source large language models' data analysis capabilities through strategic planning, interaction design, and data quality improvements were identified and applied.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate models across three dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities.", 'score': 3, 'issue_id': 4472, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 июня', 'en': 'June 24', 'zh': '6月24日'}, 'hash': '590e2169fc8a24c8', 'authors': ['Yuqi Zhu', 'Yi Zhong', 'Jintian Zhang', 'Ziheng Zhang', 'Shuofei Qiao', 'Yujie Luo', 'Lun Du', 'Da Zheng', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['Ant Group', 'Zhejiang University', 'Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph'], 'pdf_title_img': 'assets/pdf/title_img/2506.19794.jpg', 'data': {'categories': ['#synthetic', '#multimodal', '#benchmark', '#reasoning', '#open_source', '#dataset', '#data'], 'emoji': '📊', 'ru': {'title': 'Стратегическое планирование - ключ к улучшению анализа данных у открытых LLM', 'desc': 'Исследование посвящено улучшению возможностей анализа данных у открытых языковых моделей (LLM). Авторы оценивали модели по трем параметрам: понимание данных, генерация кода и стратегическое планирование. Было выявлено, что качество стратегического планирования является ключевым фактором производительности модели. Разработанная методология синтеза данных позволила значительно улучшить аналитические способности открытых LLM.'}, 'en': {'title': 'Boosting Open-Source LLMs for Better Data Analysis', 'desc': 'This paper explores how to improve the data analysis abilities of open-source Large Language Models (LLMs). It identifies that strategic planning is crucial for enhancing model performance in reasoning tasks. The study also highlights the importance of interaction design and task complexity, as well as the role of data quality over diversity in achieving better results. By applying these findings, the authors propose a new data synthesis method that significantly boosts the analytical reasoning skills of LLMs.'}, 'zh': {'title': '提升开源LLMs的数据分析能力', 'desc': '本研究探讨了如何提升开源大型语言模型（LLMs）在数据分析任务中的能力。我们通过策划多样化的真实场景数据集，评估模型在数据理解、代码生成和战略规划等三个维度的表现。研究发现，战略规划的质量是模型性能的主要决定因素，交互设计和任务复杂性也显著影响推理能力。此外，数据质量对实现最佳性能的影响大于数据的多样性。基于这些发现，我们开发了一种数据合成方法，显著提升了开源LLMs的分析推理能力。'}}}, {'id': 'https://huggingface.co/papers/2506.19838', 'title': 'SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution', 'url': 'https://huggingface.co/papers/2506.19838', 'abstract': 'Researchers propose design principles for cascaded video super-resolution models to improve high-resolution video generation by introduces degradation strategies, timestep sampling, noise augmentation, and interleaving temporal units with sparse local attention.  \t\t\t\t\tAI-generated summary \t\t\t\t Latent diffusion models have emerged as a leading paradigm for efficient video generation. However, as user expectations shift toward higher-resolution outputs, relying solely on latent computation becomes inadequate. A promising approach involves decoupling the process into two stages: semantic content generation and detail synthesis. The former employs a computationally intensive base model at lower resolutions, while the latter leverages a lightweight cascaded video super-resolution (VSR) model to achieve high-resolution output. In this work, we focus on studying key design principles for latter cascaded VSR models, which are underexplored currently. First, we propose two degradation strategies to generate training pairs that better mimic the output characteristics of the base model, ensuring alignment between the VSR model and its upstream generator. Second, we provide critical insights into VSR model behavior through systematic analysis of (1) timestep sampling strategies, (2) noise augmentation effects on low-resolution (LR) inputs. These findings directly inform our architectural and training innovations. Finally, we introduce interleaving temporal unit and sparse local attention to achieve efficient training and inference, drastically reducing computational overhead. Extensive experiments demonstrate the superiority of our framework over existing methods, with ablation studies confirming the efficacy of each design choice. Our work establishes a simple yet effective baseline for cascaded video super-resolution generation, offering practical insights to guide future advancements in efficient cascaded synthesis systems.', 'score': 2, 'issue_id': 4470, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 июня', 'en': 'June 24', 'zh': '6月24日'}, 'hash': 'a00bfa00a7f0f869', 'authors': ['Liangbin Xie', 'Yu Li', 'Shian Du', 'Menghan Xia', 'Xintao Wang', 'Fanghua Yu', 'Ziyan Chen', 'Pengfei Wan', 'Jiantao Zhou', 'Chao Dong'], 'affiliations': ['Kuaishou Technology', 'Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences', 'Shenzhen University of Advanced Technology', 'State Key Laboratory of Internet of Things for Smart City, University of Macau', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.19838.jpg', 'data': {'categories': ['#optimization', '#training', '#video', '#diffusion', '#architecture'], 'emoji': '🎥', 'ru': {'title': 'Эффективное сверхразрешение видео с помощью каскадных моделей', 'desc': 'Исследователи предлагают принципы проектирования каскадных моделей сверхразрешения видео для улучшения генерации видео высокого разрешения. Они вводят стратегии деградации, выборку временных шагов, аугментацию шума и чередование временных блоков с разреженным локальным вниманием. Эти методы позволяют эффективно обучать и использовать модели, значительно снижая вычислительные затраты. Эксперименты показывают превосходство предложенного подхода над существующими методами.'}, 'en': {'title': 'Enhancing Video Quality with Smart Cascaded Super-Resolution', 'desc': "This paper presents new design principles for improving cascaded video super-resolution (VSR) models, which are essential for generating high-resolution videos. The authors introduce innovative degradation strategies and timestep sampling methods to enhance the training process, ensuring that the VSR model aligns well with the base model's output. They also explore the impact of noise augmentation on low-resolution inputs and propose techniques like interleaving temporal units and sparse local attention to optimize training efficiency. The results show that their framework outperforms existing methods, providing a solid foundation for future developments in video super-resolution."}, 'zh': {'title': '提升视频超分辨率生成的设计原则', 'desc': '本研究提出了级联视频超分辨率模型的设计原则，以提高高分辨率视频生成的效果。我们引入了退化策略、时间步采样、噪声增强和稀疏局部注意力等方法，来优化模型的训练和推理过程。通过系统分析，我们揭示了时间步采样策略和噪声增强对低分辨率输入的影响，从而指导模型架构和训练创新。实验结果表明，我们的方法在现有技术中具有显著优势，为未来高效级联合成系统的发展提供了实用的见解。'}}}, {'id': 'https://huggingface.co/papers/2506.19807', 'title': 'KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality', 'url': 'https://huggingface.co/papers/2506.19807', 'abstract': 'KnowRL, a knowledge-enhanced reinforcement learning approach, reduces hallucinations in slow-thinking large language models by incorporating factuality rewards based on knowledge verification during training.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs), particularly slow-thinking models, often exhibit severe hallucination, outputting incorrect content due to an inability to accurately recognize knowledge boundaries during reasoning. While Reinforcement Learning (RL) can enhance complex reasoning abilities, its outcome-oriented reward mechanism often lacks factual supervision over the thinking process, further exacerbating the hallucination problem. To address the high hallucination in slow-thinking models, we propose Knowledge-enhanced RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. This targeted factual input during RL training enables the model to learn and internalize fact-based reasoning strategies. By directly rewarding adherence to facts within the reasoning steps, KnowRL fosters a more reliable thinking process. Experimental results on three hallucination evaluation datasets and two reasoning evaluation datasets demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking models while maintaining their original strong reasoning capabilities. Our code is available at https://github.com/zjunlp/KnowRL.', 'score': 2, 'issue_id': 4472, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 июня', 'en': 'June 24', 'zh': '6月24日'}, 'hash': '2f1016b014bf6ee5', 'authors': ['Baochang Ren', 'Shuofei Qiao', 'Wenhao Yu', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['Tencent AI Seattle Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.19807.jpg', 'data': {'categories': ['#hallucinations', '#rl', '#rlhf', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Фактическое мышление для ИИ: KnowRL против галлюцинаций', 'desc': 'KnowRL - это подход к обучению с подкреплением, улучшенный знаниями, который снижает галлюцинации в медленно мыслящих больших языковых моделях. Он включает вознаграждения за фактическую точность, основанные на проверке знаний во время обучения. KnowRL направляет модели на выполнение медленного мышления, основанного на фактах, интегрируя вознаграждение за фактическую точность в процесс обучения с подкреплением. Экспериментальные результаты показывают, что KnowRL эффективно снижает галлюцинации в медленно мыслящих моделях, сохраняя при этом их исходные сильные способности к рассуждению.'}, 'en': {'title': 'Enhancing Truthfulness in AI with KnowRL', 'desc': "KnowRL is a novel approach that enhances reinforcement learning by incorporating factuality rewards to reduce hallucinations in slow-thinking large language models. These models often generate incorrect information due to their inability to recognize the limits of their knowledge. By integrating knowledge verification into the training process, KnowRL encourages models to engage in fact-based reasoning. Experimental results show that this method effectively decreases hallucinations while preserving the models' reasoning abilities."}, 'zh': {'title': '知识增强的强化学习，减少幻觉现象', 'desc': 'KnowRL是一种增强知识的强化学习方法，通过在训练过程中引入基于知识验证的事实奖励，减少了慢思维大型语言模型的幻觉现象。慢思维模型常常因为无法准确识别知识边界而输出错误内容，导致严重的幻觉问题。KnowRL通过将事实奖励整合到强化学习训练中，指导模型进行基于事实的慢思维，从而帮助它们识别知识边界。实验结果表明，KnowRL在减轻幻觉的同时，保持了模型的强大推理能力。'}}}, {'id': 'https://huggingface.co/papers/2506.18843', 'title': 'USAD: Universal Speech and Audio Representation via Distillation', 'url': 'https://huggingface.co/papers/2506.18843', 'abstract': 'USAD integrates diverse audio types using efficient layer-to-layer distillation from domain-specific models, achieving competitive performance across various benchmarks with a single encoder.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-supervised learning (SSL) has revolutionized audio representations, yet models often remain domain-specific, focusing on either speech or non-speech tasks. In this work, we present Universal Speech and Audio Distillation (USAD), a unified approach to audio representation learning that integrates diverse audio types - speech, sound, and music - into a single model. USAD employs efficient layer-to-layer distillation from domain-specific SSL models to train a student on a comprehensive audio dataset. USAD offers competitive performance across various benchmarks and datasets, including frame and instance-level speech processing tasks, audio tagging, and sound classification, achieving near state-of-the-art results with a single encoder on SUPERB and HEAR benchmarks.', 'score': 2, 'issue_id': 4472, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 июня', 'en': 'June 23', 'zh': '6月23日'}, 'hash': 'c7855e42be638a70', 'authors': ['Heng-Jui Chang', 'Saurabhchand Bhati', 'James Glass', 'Alexander H. Liu'], 'affiliations': ['MIT CSAIL Cambridge, MA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2506.18843.jpg', 'data': {'categories': ['#benchmark', '#audio'], 'emoji': '🎧', 'ru': {'title': 'Единая модель для всех типов аудио', 'desc': 'USAD - это новый подход к обучению универсальных аудиопредставлений, объединяющий речь, звуки и музыку в единой модели. Метод использует эффективную дистилляцию от слоя к слою из специализированных моделей самообучения для обучения студента на разнообразном аудиодатасете. USAD демонстрирует конкурентоспособные результаты в различных бенчмарках, включая задачи обработки речи на уровне кадров и экземпляров, тегирование аудио и классификацию звуков. Модель достигает результатов близких к state-of-the-art с использованием единого энкодера на бенчмарках SUPERB и HEAR.'}, 'en': {'title': 'One Model, Many Sounds: USAD Unifies Audio Learning', 'desc': 'The paper introduces Universal Speech and Audio Distillation (USAD), a novel method for learning audio representations that combines different audio types such as speech, sound, and music into one model. It utilizes layer-to-layer distillation from specialized self-supervised learning (SSL) models, allowing a student model to learn from a wide-ranging audio dataset. This approach enables USAD to perform well on various tasks, including speech processing, audio tagging, and sound classification. The results demonstrate that USAD achieves competitive performance on multiple benchmarks with just a single encoder, showcasing its efficiency and versatility in audio representation learning.'}, 'zh': {'title': '统一音频表示，提升多任务性能', 'desc': 'USAD是一种统一的音频表示学习方法，能够整合多种音频类型，包括语音、声音和音乐。它通过从特定领域的自监督学习模型中进行高效的层间蒸馏，训练一个学生模型，以便在一个全面的音频数据集上进行学习。USAD在多个基准测试中表现出色，包括语音处理、音频标记和声音分类任务。该方法使用单一编码器，达到了接近最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.17612', 'title': 'JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo\n  Retouching Agent', 'url': 'https://huggingface.co/papers/2506.17612', 'abstract': 'JarvisArt, an MLLM-driven agent, achieves superior photo retouching by understanding user intent and coordinating multiple retouching tools in Lightroom, outperforming GPT-4o on a novel benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Photo retouching has become integral to contemporary visual storytelling, enabling users to capture aesthetics and express creativity. While professional tools such as Adobe Lightroom offer powerful capabilities, they demand substantial expertise and manual effort. In contrast, existing AI-based solutions provide automation but often suffer from limited adjustability and poor generalization, failing to meet diverse and personalized editing needs. To bridge this gap, we introduce JarvisArt, a multi-modal large language model (MLLM)-driven agent that understands user intent, mimics the reasoning process of professional artists, and intelligently coordinates over 200 retouching tools within Lightroom. JarvisArt undergoes a two-stage training process: an initial Chain-of-Thought supervised fine-tuning to establish basic reasoning and tool-use skills, followed by Group Relative Policy Optimization for Retouching (GRPO-R) to further enhance its decision-making and tool proficiency. We also propose the Agent-to-Lightroom Protocol to facilitate seamless integration with Lightroom. To evaluate performance, we develop MMArt-Bench, a novel benchmark constructed from real-world user edits. JarvisArt demonstrates user-friendly interaction, superior generalization, and fine-grained control over both global and local adjustments, paving a new avenue for intelligent photo retouching. Notably, it outperforms GPT-4o with a 60% improvement in average pixel-level metrics on MMArt-Bench for content fidelity, while maintaining comparable instruction-following capabilities. Project Page: https://jarvisart.vercel.app/.', 'score': 1, 'issue_id': 4472, 'pub_date': '2025-06-21', 'pub_date_card': {'ru': '21 июня', 'en': 'June 21', 'zh': '6月21日'}, 'hash': '2504aa6b996e0739', 'authors': ['Yunlong Lin', 'Zixu Lin', 'Kunjie Lin', 'Jinbin Bai', 'Panwang Pan', 'Chenxin Li', 'Haoyu Chen', 'Zhongdao Wang', 'Xinghao Ding', 'Wenbo Li', 'Shuicheng Yan'], 'affiliations': ['Bytedance', 'Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, Xiamen, Fujian, China', 'National University of Singapore', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology (Guangzhou)', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.17612.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#games', '#reasoning', '#optimization', '#agents', '#cv', '#training'], 'emoji': '🖼️', 'ru': {'title': 'JarvisArt: ИИ-помощник для профессиональной ретуши фото', 'desc': 'JarvisArt - это агент на основе мультимодальной большой языковой модели (МLLM), который улучшает ретушь фотографий, понимая намерения пользователя и координируя множество инструментов в Lightroom. Система проходит двухэтапное обучение: сначала fine-tuning с использованием цепочки рассуждений, затем оптимизацию с помощью Group Relative Policy Optimization for Retouching (GRPO-R). JarvisArt превосходит GPT-4o на новом бенчмарке MMArt-Bench, демонстрируя улучшение на 60% в метриках на уровне пикселей при сохранении точности выполнения инструкций.'}, 'en': {'title': 'Revolutionizing Photo Retouching with Intelligent AI', 'desc': 'JarvisArt is an advanced multi-modal large language model (MLLM) agent designed for photo retouching, which excels in understanding user intent and effectively coordinating over 200 tools in Adobe Lightroom. Unlike traditional AI solutions that lack flexibility, JarvisArt mimics the reasoning of professional artists, allowing for personalized and nuanced editing. It employs a two-stage training process that enhances its decision-making abilities and tool proficiency, ensuring high-quality results. The performance of JarvisArt is validated through a new benchmark, MMArt-Bench, where it significantly outperforms existing models like GPT-4o in pixel-level metrics for content fidelity.'}, 'zh': {'title': '智能修饰，超越传统！', 'desc': 'JarvisArt 是一个基于多模态大语言模型（MLLM）的智能代理，专注于照片修饰。它能够理解用户意图，并协调 Lightroom 中的200多种修饰工具，提供更优质的修饰效果。通过两阶段的训练过程，JarvisArt 提升了决策能力和工具使用熟练度，能够满足用户个性化的编辑需求。与现有的 AI 解决方案相比，JarvisArt 在内容保真度上表现出色，平均像素级指标提高了60%。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (1)', '#agi', '#alignment', '#architecture (1)', '#audio (1)', '#benchmark (5)', '#cv (1)', '#data (2)', '#dataset (3)', '#diffusion (2)', '#ethics', '#games (1)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference', '#interpretability (1)', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (3)', '#open_source (2)', '#optimization (5)', '#plp', '#rag', '#reasoning (5)', '#rl (3)', '#rlhf (2)', '#robotics', '#science', '#security', '#small_models', '#story_generation', '#survey', '#synthetic (1)', '#training (6)', '#transfer_learning (2)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-06-25 04:23',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-25 04:23')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-25 04:23')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    