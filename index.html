
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 32 papers. February 9.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ</span> | <span id="title-articles-count">32 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2026-02-06.html">â¬…ï¸ <span id="prev-date">06.02</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2026-02-10.html">â¡ï¸ <span id="next-date">10.02</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2026-02.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 9', 'zh': '2æœˆ9æ—¥'};
        let feedDateNext = {'ru': '10.02', 'en': '02/10', 'zh': '2æœˆ10æ—¥'};
        let feedDatePrev = {'ru': '06.02', 'en': '02/06', 'zh': '2æœˆ6æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2602.06570', 'title': 'Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making', 'url': 'https://huggingface.co/papers/2602.06570', 'abstract': 'Baichuan-M3 is a medical-enhanced large language model designed for clinical decision support with capabilities in proactive information gathering, long-horizon reasoning, and hallucination suppression.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.', 'score': 53, 'issue_id': 963, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 6', 'zh': '2æœˆ6æ—¥'}, 'hash': 'cb1f6a45692326d3', 'authors': ['Baichuan-M3 Team', ':', 'Chengfeng Dou', 'Fan Yang', 'Fei Li', 'Jiyuan Jia', 'Qiang Ju', 'Shuai Wang', 'Tianpeng Li', 'Xiangrong Zeng', 'Yijie Zhou', 'Hongda Zhang', 'Jinyang Tai', 'Linzhuang Sun', 'Peidong Guo', 'Yichuan Mo', 'Xiaochuan Wang', 'Hengfu Cui', 'Zhishou Zhang'], 'affiliations': ['Baichuan Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2602.06570.jpg', 'data': {'categories': ['#open_source', '#science', '#hallucinations', '#reasoning'], 'emoji': 'âš•ï¸', 'ru': {'title': 'ĞÑ‚ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğº Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ²Ñ€Ğ°Ñ‡ĞµĞ±Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞµ', 'desc': 'Baichuan-M3 â€” ÑÑ‚Ğ¾ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ°Ñ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¾Ñ‚ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğº Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞµ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğµ, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ñ€Ğ°Ñ‡Ğ°, Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞ±Ğ¾Ñ€ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ² ÑĞ²ÑĞ·Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾Ğ· Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ„Ğ°ĞºÑ‚-Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… HealthBench, HealthBench-Hallu Ğ¸ ScanBench, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ°Ñ†Ğ¸ÑÑ… Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ½Ğ° Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğµ Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Revolutionizing Clinical Decision Support with Baichuan-M3', 'desc': 'Baichuan-M3 is a large language model specifically designed for medical applications, focusing on enhancing clinical decision-making. It moves beyond traditional question-answering by actively supporting healthcare professionals with proactive information gathering and long-term reasoning. The model effectively integrates various pieces of evidence to form coherent diagnoses while minimizing inaccuracies through adaptive hallucination suppression. Evaluations show that Baichuan-M3 outperforms existing models like GPT-5.2 in clinical tasks, making it a significant advancement in medical AI.'}, 'zh': {'title': 'ä¸»åŠ¨å†³ç­–æ”¯æŒï¼Œè¶…è¶Šä¼ ç»Ÿé—®ç­”', 'desc': 'Baichuan-M3æ˜¯ä¸€ç§åŒ»å­¦å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨ä¸ºä¸´åºŠå†³ç­–æä¾›æ”¯æŒã€‚å®ƒçš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ä¸»åŠ¨ä¿¡æ¯è·å–ã€é•¿æ—¶é—´æ¨ç†å’Œå¹»è§‰æŠ‘åˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§£å†³ç°æœ‰ç³»ç»Ÿåœ¨å¼€æ”¾å¼å’¨è¯¢ä¸­çš„å±€é™æ€§ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸“é—¨çš„è®­ç»ƒæµç¨‹æ¨¡æ‹ŸåŒ»ç”Ÿçš„ç³»ç»Ÿå·¥ä½œæµç¨‹ï¼Œèƒ½å¤Ÿå°†åˆ†æ•£çš„è¯æ®æ•´åˆä¸ºè¿è´¯çš„è¯Šæ–­ã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒBaichuan-M3åœ¨HealthBenchç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—è¶…è¶Šäº†GPT-5.2ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.05843', 'title': 'OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions', 'url': 'https://huggingface.co/papers/2602.05843', 'abstract': "OdysseyArena presents a new framework for evaluating large language models on long-horizon, inductive agent tasks that emphasize autonomous discovery of environmental transition laws.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena", 'score': 49, 'issue_id': 963, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 5', 'zh': '2æœˆ5æ—¥'}, 'hash': '22d2e90196a24045', 'authors': ['Fangzhi Xu', 'Hang Yan', 'Qiushi Sun', 'Jinyang Wu', 'Zixian Huang', 'Muye Huang', 'Jingyang Gong', 'Zichen Ding', 'Kanzhi Cheng', 'Yian Wang', 'Xinyu Che', 'Zeyi Sun', 'Jian Zhang', 'Zhangyue Yin', 'Haoran Luo', 'Xuanjing Huang', 'Ben Kao', 'Jun Liu', 'Qika Lin'], 'affiliations': ['Fudan University', 'Nanjing University', 'Nanyang Technological University', 'National University of Singapore', 'Shanghai AI Laboratory', 'The University of Hong Kong', 'Tsinghua University', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2602.05843.jpg', 'data': {'categories': ['#agents', '#benchmark', '#reasoning', '#long_context', '#open_source', '#dataset'], 'emoji': 'ğŸ§­', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ñ‹ Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ OdysseyArena â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ¾Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ¸Ğ· Ğ¾Ğ¿Ñ‹Ñ‚Ğ°. Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ñ‹ Ğ½Ğ° Ğ´ĞµĞ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğµ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ ÑĞ²Ğ½Ğ¾ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¾Ğ´Ğ½Ğ°ĞºĞ¾ Ğ¿Ñ€ĞµĞ½ĞµĞ±Ñ€ĞµĞ³Ğ°ÑÑ‚ Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ° Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ğº Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ´Ğ²Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°: OdysseyArena-Lite Ñ 120 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ OdysseyArena-Challenge Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ… (Ğ±Ğ¾Ğ»ĞµĞµ 200 ÑˆĞ°Ğ³Ğ¾Ğ²). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 15+ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰Ğ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ….'}, 'en': {'title': 'Empowering Agents to Learn and Adapt Autonomously', 'desc': 'OdysseyArena introduces a new framework for assessing large language models (LLMs) in tasks that require long-term planning and the ability to learn from their environment. Unlike traditional evaluations that rely on fixed rules, this framework emphasizes the importance of inductive reasoning, allowing agents to discover underlying transition laws through experience. The framework includes OdysseyArena-Lite for standardized benchmarking with 120 tasks, and OdysseyArena-Challenge to evaluate agent performance over extended interactions. Experiments show that even advanced LLMs struggle with these inductive tasks, highlighting a significant challenge in developing autonomous agents capable of effective long-term decision-making.'}, 'zh': {'title': 'è‡ªä¸»å‘ç°çš„æœªæ¥ï¼šOdysseyArenaæ¡†æ¶', 'desc': 'OdysseyArenaæ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿æ—¶é—´èŒƒå›´å†…çš„è‡ªä¸»å‘ç°ç¯å¢ƒè½¬å˜è§„å¾‹çš„èƒ½åŠ›ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¸»è¦åŸºäºæ¼”ç»æ¨ç†ï¼Œå¿½è§†äº†ä»£ç†ä»ç»éªŒä¸­è‡ªä¸»å‘ç°æ½œåœ¨è½¬å˜è§„å¾‹çš„å¿…è¦æ€§ã€‚æˆ‘ä»¬æå‡ºäº†OdysseyArenaï¼Œé‡æ–°èšç„¦äºé•¿æ—¶é—´èŒƒå›´å†…çš„ä¸»åŠ¨å’Œå½’çº³äº¤äº’ï¼Œå»ºç«‹äº†å››ä¸ªåŸºæœ¬åŸç†ï¼Œå°†æŠ½è±¡çš„è½¬å˜åŠ¨æ€è½¬åŒ–ä¸ºå…·ä½“çš„äº¤äº’ç¯å¢ƒã€‚é€šè¿‡æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•OdysseyArena-Liteï¼Œæˆ‘ä»¬æä¾›äº†120ä¸ªä»»åŠ¡æ¥è¡¡é‡ä»£ç†çš„å½’çº³æ•ˆç‡å’Œé•¿æ—¶é—´å‘ç°èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.03392', 'title': 'On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models', 'url': 'https://huggingface.co/papers/2602.03392', 'abstract': 'The paper establishes a theoretical framework for analyzing entropy dynamics in reinforcement fine-tuning of large language models, deriving expressions for entropy change and proposing entropy control methods based on discriminant analysis.  \t\t\t\t\tAI-generated summary \t\t\t\t Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning (RFT), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update. This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.', 'score': 39, 'issue_id': 962, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 3', 'zh': '2æœˆ3æ—¥'}, 'hash': 'ddcd17135a2a2ca9', 'authors': ['Shumin Wang', 'Yuexiang Xie', 'Wenhao Zhang', 'Yuchang Sun', 'Yanxi Chen', 'Yaliang Li', 'Yanyong Zhang'], 'affiliations': ['Tongyi Lab, Alibaba Group', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2602.03392.jpg', 'data': {'categories': ['#rlhf', '#math', '#training', '#rl'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ¢ĞµĞ¾Ñ€Ğ¸Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ² ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ÑÑ‚ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğµ Ğ¸ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ÑÑÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ñ„Ğ°Ğ¹Ğ½Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğµ LLM.'}, 'en': {'title': 'Optimizing Exploration-Exploitation in Language Models through Entropy Dynamics', 'desc': 'This paper presents a theoretical framework for understanding how entropy changes during the reinforcement fine-tuning (RFT) of large language models (LLMs). It introduces a discriminant expression to quantify entropy change with each logit update, leading to a first-order expression that can be applied to Group Relative Policy Optimization (GRPO). The authors propose new entropy control methods based on their analysis, which help balance exploration and exploitation in model training. Empirical results validate their findings and demonstrate the effectiveness of the proposed entropy-discriminator clipping techniques.'}, 'zh': {'title': 'ä¼˜åŒ–æ¢ç´¢ä¸åˆ©ç”¨çš„ç†µåŠ¨æ€åˆ†æ', 'desc': 'æœ¬æ–‡å»ºç«‹äº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œç”¨äºåˆ†æå¤§è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å¾®è°ƒè¿‡ç¨‹ä¸­çš„ç†µåŠ¨æ€ã€‚ç†µæ˜¯è¡¡é‡æ¨¡å‹è¾“å‡ºå¤šæ ·æ€§çš„é‡è¦æŒ‡æ ‡ï¼Œèƒ½å¤Ÿæä¾›æ¨¡å‹æ¢ç´¢èƒ½åŠ›çš„æ·±åˆ»è§è§£ã€‚æˆ‘ä»¬æ¨å¯¼äº†ç†µå˜åŒ–çš„è¡¨è¾¾å¼ï¼Œå¹¶æå‡ºäº†åŸºäºåˆ¤åˆ«åˆ†æçš„ç†µæ§åˆ¶æ–¹æ³•ï¼Œä»¥ä¼˜åŒ–æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡ã€‚é€šè¿‡å®è¯ç ”ç©¶ï¼Œæˆ‘ä»¬éªŒè¯äº†è¿™äº›æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºç°æœ‰ç ”ç©¶ä¸­çš„ç†µç›¸å…³æ–¹æ³•æä¾›äº†ç»Ÿä¸€çš„è§£é‡Šè§†è§’ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.06717', 'title': "F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare", 'url': 'https://huggingface.co/papers/2602.06717', 'abstract': 'RLVR methods using group sampling suffer from bias toward likely trajectories and missed rare-correct ones; a difficulty-aware advantage scaling technique improves performance on benchmarks without increasing computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 rightarrow 70.3 (GRPO), 69.3 rightarrow 72.5 (DAPO), and 73.2 rightarrow 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.', 'score': 27, 'issue_id': 964, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 6', 'zh': '2æœˆ6æ—¥'}, 'hash': 'a897454d6b9b2de1', 'authors': ['Daniil Plyusov', 'Alexey Gorbatovski', 'Boris Shaposhnikov', 'Viacheslav Sinii', 'Alexey Malakhov', 'Daniil Gavrilov'], 'affiliations': ['Saint Petersburg Electrotechnical University LETI', 'T-Tech'], 'pdf_title_img': 'assets/pdf/title_img/2602.06717.jpg', 'data': {'categories': [], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¡Ğ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: ĞºĞ°Ğº Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ñ€ĞµĞ´ĞºĞ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ (RLVR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ… Ğ³Ñ€ÑƒĞ¿Ğ¿ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ Ñ€ĞµĞ´ĞºĞ¸Ğµ, Ğ½Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ…. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ², Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½Ğ½Ñ‹Ğµ Focal loss, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ğ²ĞµÑ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ»ĞµĞ³ĞºĞ¾ Ñ€ĞµÑˆĞ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚.'}, 'en': {'title': 'Enhancing RLVR Performance with Difficulty-Aware Scaling', 'desc': 'This paper addresses the limitations of Reinforcement Learning with Verifiable Rewards (RLVR) that arise from group sampling methods. These methods tend to favor common trajectories, leading to a bias that overlooks rare but correct solutions. The authors introduce a difficulty-aware advantage scaling technique that adjusts the learning process to focus on less frequent but valuable outcomes. Their approach enhances performance on various benchmarks without increasing computational demands, demonstrating significant improvements in success rates across different RLVR algorithms.'}, 'zh': {'title': 'ä¼˜åŒ–å¼ºåŒ–å­¦ä¹ ï¼Œå…‹æœåå·®æŒ‘æˆ˜', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åŸºäºç¾¤ä½“é‡‡æ ·çš„å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æ–¹æ³•çš„åå·®é—®é¢˜ï¼Œå°¤å…¶æ˜¯å¯¹å¸¸è§è½¨è¿¹çš„åå‘å’Œå¯¹ç¨€æœ‰æ­£ç¡®è½¨è¿¹çš„é—æ¼ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¾ƒå¤§çš„ç¾¤ä½“è§„æ¨¡ä¼šå¯¼è‡´å­¦ä¹ åå‘äºå·²ç»å¯èƒ½çš„è½¨è¿¹ï¼Œè€Œè¾ƒå°çš„ç¾¤ä½“åˆ™å¯èƒ½é”™è¿‡ç¨€æœ‰çš„æ­£ç¡®è½¨è¿¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå›°éš¾æ„ŸçŸ¥çš„ä¼˜åŠ¿ç¼©æ”¾æŠ€æœ¯ï¼Œå¯ä»¥æœ‰æ•ˆæ”¹å–„ç®—æ³•æ€§èƒ½ï¼Œè€Œä¸å¢åŠ è®¡ç®—æˆæœ¬ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒæˆ–æ”¹å–„äº†å…¶ä»–æŒ‡æ ‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01734', 'title': 'MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration', 'url': 'https://huggingface.co/papers/2602.01734', 'abstract': 'Training instability in large language models is linked to weight matrix stable rank decline and Jacobian alignment, which MSign addresses through matrix sign operations to prevent gradient explosions.  \t\t\t\t\tAI-generated summary \t\t\t\t Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via Î¼P, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.', 'score': 23, 'issue_id': 963, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': '3da9fd3fb00930a1', 'authors': ['Lianhai Ren', 'Yucheng Ding', 'Xiao Liu', 'Qianxiao Li', 'Peng Cheng', 'Yeyun Gong'], 'affiliations': ['Department of Mathematics, National University of Singapore', 'Microsoft SIGMA Team, Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2602.01734.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization'], 'emoji': 'âš¡', 'ru': {'title': 'MSign: ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ½Ğ³Ğ° Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ²Ğ·Ñ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾ÑÑ‚Ğ° Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´ÑˆĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑÑƒ: ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ½Ğ³Ğ° Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ğ²ĞµÑĞ¾Ğ² Ğ¸ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚Ğ°ÑÑ‰ĞµĞµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞºĞ¾Ğ±Ğ¸Ğ°Ğ½Ğ¾Ğ² ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ñ… ÑĞ»Ğ¾Ñ‘Ğ². ĞĞ½Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğµ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ¾ÑÑ‚ Ğ½Ğ¾Ñ€Ğ¼Ñ‹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ MSign, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ½Ğ°ĞºĞ° Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ½Ğ³Ğ°.'}, 'en': {'title': 'Stabilizing Training in Large Language Models with MSign', 'desc': 'This paper addresses the problem of training instability in large language models, which often leads to gradient explosions during pretraining. The authors identify two critical factors that contribute to this instability: a rapid decline in the stable rank of weight matrices and increased alignment of Jacobians between layers. They introduce a new optimizer called MSign, which uses matrix sign operations to maintain stable rank and prevent these issues. Experimental results show that MSign successfully mitigates training failures with minimal additional computational cost.'}, 'zh': {'title': 'è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒä¸ç¨³å®šæ€§çš„æ–°æ–¹æ³•', 'desc': 'åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒä¸­ï¼Œè®­ç»ƒä¸ç¨³å®šæ€§æ˜¯ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ï¼Œå¸¸å¸¸è¡¨ç°ä¸ºçªå‘çš„æ¢¯åº¦çˆ†ç‚¸ï¼Œæµªè´¹å¤§é‡è®¡ç®—èµ„æºã€‚æˆ‘ä»¬ç ”ç©¶äº†ä¸€ä¸ª5Må‚æ•°çš„NanoGPTæ¨¡å‹çš„è®­ç»ƒå¤±è´¥ï¼Œå‘ç°äº†å¯¼è‡´å´©æºƒçš„ä¸¤ä¸ªå…³é”®ç°è±¡ï¼šæƒé‡çŸ©é˜µç¨³å®šç§©çš„å¿«é€Ÿä¸‹é™å’Œç›¸é‚»å±‚é›…å¯æ¯”çŸ©é˜µä¹‹é—´çš„å¯¹é½å¢åŠ ã€‚æˆ‘ä»¬ç†è®ºè¯æ˜è¿™ä¸¤ä¸ªæ¡ä»¶å…±åŒå¯¼è‡´äº†éšç€ç½‘ç»œæ·±åº¦çš„å¢åŠ ï¼Œæ¢¯åº¦èŒƒæ•°çš„æŒ‡æ•°å¢é•¿ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ä¸ç¨³å®šæœºåˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†MSignï¼Œä¸€ç§æ–°çš„ä¼˜åŒ–å™¨ï¼Œé€šè¿‡å‘¨æœŸæ€§åœ°åº”ç”¨çŸ©é˜µç¬¦å·æ“ä½œæ¥æ¢å¤ç¨³å®šç§©ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.06949', 'title': 'DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos', 'url': 'https://huggingface.co/papers/2602.06949', 'abstract': 'DreamDojo is a foundation world model trained on 44k hours of egocentric human videos that enables efficient simulation of dexterous robotic tasks through continuous latent actions and real-time distillation.  \t\t\t\t\tAI-generated summary \t\t\t\t Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.', 'score': 18, 'issue_id': 962, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 6', 'zh': '2æœˆ6æ—¥'}, 'hash': '4b7549558bf19284', 'authors': ['Shenyuan Gao', 'William Liang', 'Kaiyuan Zheng', 'Ayaan Malik', 'Seonghyeon Ye', 'Sihyun Yu', 'Wei-Cheng Tseng', 'Yuzhu Dong', 'Kaichun Mo', 'Chen-Hsuan Lin', 'Qianli Ma', 'Seungjun Nah', 'Loic Magne', 'Jiannan Xiang', 'Yuqi Xie', 'Ruijie Zheng', 'Dantong Niu', 'You Liang Tan', 'K. R. Zentner', 'George Kurian', 'Suneel Indupuru', 'Pooya Jannaty', 'Jinwei Gu', 'Jun Zhang', 'Jitendra Malik', 'Pieter Abbeel', 'Ming-Yu Liu', 'Yuke Zhu', 'Joel Jang', 'Linxi "Jim" Fan'], 'affiliations': ['HKUST', 'KAIST', 'NVIDIA', 'Stanford', 'UC Berkeley', 'UCSD', 'UT Austin', 'UW'], 'pdf_title_img': 'assets/pdf/title_img/2602.06949.jpg', 'data': {'categories': ['#inference', '#open_source', '#synthetic', '#transfer_learning', '#video', '#training', '#robotics', '#dataset'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ Ğ»Ğ¾Ğ²ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ¾Ğ±Ğ¾ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°', 'desc': 'DreamDojo â€” ÑÑ‚Ğ¾ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ°, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° 44 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… Ñ‡Ğ°ÑĞ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ½ĞµĞ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ€ĞµÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞŸĞ¾ÑĞ»Ğµ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ (10.81 FPS) Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ Ñ‚ĞµĞ»ĞµĞ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… out-of-distribution Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Revolutionizing Robotics with DreamDojo: A New Era of Simulation and Control', 'desc': 'DreamDojo is a foundational world model that leverages 44,000 hours of egocentric human videos to simulate complex robotic tasks effectively. It addresses the challenges of limited data and scarce action labels by introducing continuous latent actions, which serve as unified proxy actions for better knowledge transfer. After fine-tuning on specific robot data, DreamDojo exhibits strong physics understanding and precise control capabilities. This model not only accelerates real-time performance but also enhances context consistency, making it suitable for various applications like teleoperation and model-based planning.'}, 'zh': {'title': 'DreamDojoï¼šçµå·§æœºå™¨äººä»»åŠ¡çš„æœªæ¥æ¨¡æ‹Ÿå™¨', 'desc': 'DreamDojoæ˜¯ä¸€ä¸ªåŸºç¡€ä¸–ç•Œæ¨¡å‹ï¼Œåˆ©ç”¨44000å°æ—¶çš„è‡ªæˆ‘ä¸­å¿ƒäººç±»è§†é¢‘è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿé«˜æ•ˆæ¨¡æ‹Ÿçµå·§æœºå™¨äººä»»åŠ¡ã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥è¿ç»­æ½œåœ¨åŠ¨ä½œä½œä¸ºç»Ÿä¸€çš„ä»£ç†åŠ¨ä½œï¼Œè§£å†³äº†åŠ¨ä½œæ ‡ç­¾ç¨€ç¼ºçš„é—®é¢˜ï¼Œä»è€Œå¢å¼ºäº†ä»æœªæ ‡è®°è§†é¢‘ä¸­è½¬ç§»äº¤äº’çŸ¥è¯†çš„èƒ½åŠ›ã€‚ç»è¿‡å°è§„æ¨¡ç›®æ ‡æœºå™¨äººæ•°æ®çš„åè®­ç»ƒï¼ŒDreamDojoå±•ç°å‡ºå¯¹ç‰©ç†çš„å¼ºç†è§£å’Œç²¾ç¡®çš„åŠ¨ä½œæ§åˆ¶èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºåŸºäºç”Ÿæˆä¸–ç•Œæ¨¡å‹çš„é‡è¦åº”ç”¨å¥ å®šäº†åŸºç¡€ï¼ŒåŒ…æ‹¬å®æ—¶é¥æ“ä½œã€ç­–ç•¥è¯„ä¼°å’ŒåŸºäºæ¨¡å‹çš„è§„åˆ’ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.05940', 'title': 'Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training', 'url': 'https://huggingface.co/papers/2602.05940', 'abstract': 'TRIT framework improves multilingual reasoning by jointly training translation and reasoning components, enhancing question understanding and response generation across languages.  \t\t\t\t\tAI-generated summary \t\t\t\t Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200.', 'score': 15, 'issue_id': 962, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 5', 'zh': '2æœˆ5æ—¥'}, 'hash': '370ca1f731ec277b', 'authors': ['Junxiao Liu', 'Zhijun Wang', 'Yixiao Li', 'Zhejian Lai', 'Liqian Huang', 'Xin Huang', 'Xue Han', 'Junlan Feng', 'Shujian Huang'], 'affiliations': ['China Mobile Communications Company Limited Research Institute', 'National Key Laboratory for Novel Software Technology, Nanjing University', 'University of TÃ¼bingen'], 'pdf_title_img': 'assets/pdf/title_img/2602.05940.jpg', 'data': {'categories': ['#machine_translation', '#transfer_learning', '#multilingual', '#training', '#reasoning'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº TRIT, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ğ¸Ñ‚Ğ°ÑÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ´Ğ°Ğ¶Ğµ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰ĞµĞ³Ğ¾ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ â€” Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 7 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMATH Ñ Ğ·Ğ°Ğ¼ĞµÑ‚Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸ĞµĞ¼ ĞºÑ€Ğ¾ÑÑ-Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ².'}, 'en': {'title': 'Enhancing Multilingual Reasoning with Integrated Translation Training', 'desc': 'The TRIT framework enhances multilingual reasoning by combining translation and reasoning training. This approach addresses the challenges of understanding and responding to questions in different languages, which often leads to inaccuracies. By integrating translation into the reasoning process, TRIT improves both the accuracy of answers and the consistency of language used. The framework shows significant performance improvements on multilingual tasks, demonstrating its effectiveness without needing extra data or feedback.'}, 'zh': {'title': 'TRITæ¡†æ¶ï¼šæå‡å¤šè¯­è¨€æ¨ç†çš„æ–°æ–¹æ³•', 'desc': 'TRITæ¡†æ¶é€šè¿‡è”åˆè®­ç»ƒç¿»è¯‘å’Œæ¨ç†ç»„ä»¶ï¼Œæå‡äº†å¤šè¯­è¨€æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•è§£å†³äº†å¤šè¯­è¨€é—®é¢˜ç†è§£å’Œæ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œé¿å…äº†åœ¨éè‹±è¯­é—®é¢˜ä¸Šæ¨ç†æ—¶çš„å‡†ç¡®æ€§ä¸‹é™ã€‚TRITæ¡†æ¶æ— éœ€å¤–éƒ¨åé¦ˆæˆ–é¢å¤–çš„å¤šè¯­è¨€æ•°æ®ï¼Œèƒ½å¤Ÿè‡ªæˆ‘æå‡å¤šè¯­è¨€é—®é¢˜ç†è§£å’Œå›ç­”ç”Ÿæˆçš„æ•ˆæœã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTRITåœ¨MMATHæ•°æ®é›†ä¸Šå¹³å‡æé«˜äº†7ä¸ªç™¾åˆ†ç‚¹ï¼Œæ˜¾è‘—æ”¹å–„äº†ç­”æ¡ˆçš„æ­£ç¡®æ€§å’Œè¯­è¨€ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.06291', 'title': 'Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math', 'url': 'https://huggingface.co/papers/2602.06291', 'abstract': 'Consequence-Based Utility evaluates mathematical solutions by testing their effectiveness as exemplars for related problems, outperforming reward models and LLM judges in ranking quality and correct-wrong separation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose Consequence-Based Utility, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models, generative reward models, and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap, maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.', 'score': 14, 'issue_id': 963, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 6', 'zh': '2æœˆ6æ—¥'}, 'hash': '35fbd37e3ab0ae57', 'authors': ['Guijin Son', 'Donghun Yang', 'Hitesh Laxmichand Patel', 'Hyunwoo Ko', 'Amit Agarwal', 'Sunghee Ahn', 'Kyong-Ha Lee', 'Youngjae Yu'], 'affiliations': ['OnelineAI', 'Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2602.06291.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#science', '#math', '#dataset'], 'emoji': 'ğŸ§®', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ… Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Consequence-Based Utility Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… reward Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ»Ğ¸ ÑÑƒĞ´ĞµĞ¹ÑÑ‚Ğ²Ğ° Ñ‡ĞµÑ€ĞµĞ· LLM, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚, Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ´Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ (in-context exemplar) Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ñ… Ğ¸ Ğ½ĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ, Ñ‡ĞµĞ¼ Ğ½ĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Evaluating Math Solutions with Consequence-Based Utility', 'desc': 'This paper introduces Consequence-Based Utility, a new method for evaluating mathematical solutions by assessing their effectiveness as examples for similar problems. Unlike traditional reward models and LLM judges, this approach does not rely on pre-defined rewards but instead tests how well a solution can help solve related questions. The authors demonstrate that their method significantly improves the ranking quality of solutions, achieving higher accuracy and area under the curve (AUC) scores compared to existing models. This innovation addresses the challenge of verifying complex mathematical solutions while reducing the burden on expert evaluators.'}, 'zh': {'title': 'åŸºäºåæœçš„æ•ˆç”¨ï¼šæå‡æ•°å­¦è§£ç­”è¯„ä¼°çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ–¹æ³•ï¼Œç§°ä¸ºåŸºäºåæœçš„æ•ˆç”¨ï¼ˆConsequence-Based Utilityï¼‰ï¼Œç”¨äºè¯„ä¼°æ•°å­¦é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•é€šè¿‡æµ‹è¯•å€™é€‰è§£åœ¨è§£å†³ç›¸å…³å¯éªŒè¯é—®é¢˜ä¸­çš„è¡¨ç°ï¼Œæ¥åˆ¤æ–­å…¶ä½œä¸ºç¤ºä¾‹çš„ä»·å€¼ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨æ’åè´¨é‡ä¸Šä¼˜äºä¼ ç»Ÿçš„å¥–åŠ±æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯„ä¼°è€…ã€‚å…·ä½“è€Œè¨€ï¼ŒåŸºäºåæœçš„æ•ˆç”¨åœ¨å¤šä¸ªç ”ç©¶çº§æ•°å­¦é—®é¢˜ä¸Šæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨æ­£ç¡®ä¸é”™è¯¯çš„åŒºåˆ†ä¸Šè¡¨ç°æ›´ä¸ºå‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.06391', 'title': 'POINTS-GUI-G: GUI-Grounding Journey', 'url': 'https://huggingface.co/papers/2602.06391', 'abstract': "GUI agents for automated digital tasks rely on vision-language models with enhanced grounding capabilities, achieved through refined data engineering, improved training strategies, and reinforcement learning with verifiable rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of vision-language models has catalyzed the emergence of GUI agents, which hold immense potential for automating complex tasks, from online shopping to flight booking, thereby alleviating the burden of repetitive digital workflows. As a foundational capability, GUI grounding is typically established as a prerequisite for end-to-end task execution. It enables models to precisely locate interface elements, such as text and icons, to perform accurate operations like clicking and typing. Unlike prior works that fine-tune models already possessing strong spatial awareness (e.g., Qwen3-VL), we aim to master the full technical pipeline by starting from a base model with minimal grounding ability, such as POINTS-1.5. We introduce POINTS-GUI-G-8B, which achieves state-of-the-art performance with scores of 59.9 on ScreenSpot-Pro, 66.0 on OSWorld-G, 95.7 on ScreenSpot-v2, and 49.9 on UI-Vision. Our model's success is driven by three key factors: (1) Refined Data Engineering, involving the unification of diverse open-source datasets format alongside sophisticated strategies for augmentation, filtering, and difficulty grading; (2) Improved Training Strategies, including continuous fine-tuning of the vision encoder to enhance perceptual accuracy and maintaining resolution consistency between training and inference; and (3) Reinforcement Learning (RL) with Verifiable Rewards. While RL is traditionally used to bolster reasoning, we demonstrate that it significantly improves precision in the perception-intensive GUI grounding task. Furthermore, GUI grounding provides a natural advantage for RL, as rewards are easily verifiable and highly accurate.", 'score': 13, 'issue_id': 962, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 6', 'zh': '2æœˆ6æ—¥'}, 'hash': '1ce7e27df005ead5', 'authors': ['Zhongyin Zhao', 'Yuan Liu', 'Yikun Liu', 'Haicheng Wang', 'Le Tian', 'Xiao Zhou', 'Yangxiu You', 'Zilin Yu', 'Yang Yu', 'Jie Zhou'], 'affiliations': ['WeChat AI'], 'pdf_title_img': 'assets/pdf/title_img/2602.06391.jpg', 'data': {'categories': ['#multimodal', '#rl', '#benchmark', '#agents', '#training', '#cv', '#data'], 'emoji': 'ğŸ–±ï¸', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² GUI Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ POINTS-GUI-G-8B Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ScreenSpot-Pro (59.9) Ğ¸ OSWorld-G (66.0), Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¿Ğ¾ĞºÑƒĞ¿ĞºĞ¸ Ğ¸ Ğ±Ñ€Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¸Ğ»ĞµÑ‚Ğ¾Ğ², Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Empowering GUI Agents with Enhanced Grounding for Automation', 'desc': 'This paper discusses the development of GUI agents that automate digital tasks using advanced vision-language models. The authors focus on enhancing grounding capabilities, which allow models to accurately identify and interact with graphical user interface elements. They introduce a new model, POINTS-GUI-G-8B, which outperforms existing models by utilizing refined data engineering, improved training strategies, and reinforcement learning with verifiable rewards. The results show significant improvements in task execution accuracy, demonstrating the effectiveness of their approach in automating complex workflows.'}, 'zh': {'title': 'æå‡GUIä»£ç†çš„è‡ªåŠ¨åŒ–èƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†ï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹æ¥è‡ªåŠ¨åŒ–æ•°å­—ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºçš„æ¨¡å‹POINTS-GUI-G-8Båœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾ç¤ºäº†å…¶åœ¨ç•Œé¢å…ƒç´ å®šä½å’Œæ“ä½œæ‰§è¡Œæ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡ç²¾ç»†çš„æ•°æ®å·¥ç¨‹ã€æ”¹è¿›çš„è®­ç»ƒç­–ç•¥å’Œå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼Œæˆ‘ä»¬æ˜¾è‘—æå‡äº†æ¨¡å‹çš„ç²¾ç¡®åº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒGUIå®šä½ä»»åŠ¡çš„æˆåŠŸä¾èµ–äºè¿™äº›å…³é”®å› ç´ çš„ç»“åˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.06075', 'title': 'MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments', 'url': 'https://huggingface.co/papers/2602.06075', 'abstract': 'A comprehensive memory-focused benchmark for mobile GUI agents reveals significant memory capability gaps and provides systematic evaluation methods and design insights.  \t\t\t\t\tAI-generated summary \t\t\t\t Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5.2-11.8% memory-related tasks and no cross-session learning evaluation. We introduce MemGUI-Bench, a comprehensive memory-centric benchmark with pass@k and staged LLM-as-judge evaluation. Our contributions include: (1) a systematic memory taxonomy analyzing 11 agents across 5 architectures; (2) 128 tasks across 26 applications where 89.8% challenge memory through cross-temporal and cross-spatial retention; (3) MemGUI-Eval, an automated pipeline with Progressive Scrutiny and 7 hierarchical metrics; and (4) RQ-driven assessment of 11 state-of-the-art agents. Our experiments reveal significant memory deficits across all evaluated systems, identify 5 distinct failure modes, and synthesize 5 actionable design implications. All resources including code, benchmark, and evaluation results will be \\textit{fully open-sourced and continuously maintained} at https://lgy0404.github.io/MemGUI-Bench/.', 'score': 12, 'issue_id': 962, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 3', 'zh': '2æœˆ3æ—¥'}, 'hash': '85791424d234c269', 'authors': ['Guangyi Liu', 'Pengxiang Zhao', 'Yaozhen Liang', 'Qinyi Luo', 'Shunye Tang', 'Yuxiang Chai', 'Weifeng Lin', 'Han Xiao', 'WenHao Wang', 'Siheng Chen', 'Zhengxi Lu', 'Gao Wu', 'Hao Wang', 'Liang Liu', 'Yong Liu'], 'affiliations': ['Alibaba Group', 'ByteDance', 'Huawei', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2602.06075.jpg', 'data': {'categories': [], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞ°Ğ¼ÑÑ‚ÑŒ ĞºĞ°Ğº ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° MemGUI-Bench â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğº Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ°Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ³Ğ´Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 5.2-11.8% Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑÑ‚ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ¼ 11 Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° 5 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 128 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LLM-as-judge Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² 5 Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ²ÑĞµÑ… Ğ¾Ñ†ĞµĞ½Ñ‘Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Memory Evaluation for Mobile GUI Agents', 'desc': 'This paper addresses the shortcomings of current benchmarks for mobile GUI agents, particularly in evaluating their memory capabilities. It introduces MemGUI-Bench, a new benchmark that focuses on memory tasks and includes a systematic analysis of various agents. The benchmark features 128 tasks designed to challenge memory retention across different contexts and provides a structured evaluation method using advanced metrics. The findings highlight significant memory deficits in existing agents and propose actionable design insights to improve their performance.'}, 'zh': {'title': 'æå‡ç§»åŠ¨GUIä»£ç†çš„è®°å¿†èƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„ä»¥è®°å¿†ä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•MemGUI-Benchï¼Œç”¨äºè¯„ä¼°ç§»åŠ¨GUIä»£ç†çš„è®°å¿†èƒ½åŠ›ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°è®°å¿†èƒ½åŠ›æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œä»…æœ‰5.2%åˆ°11.8%çš„ä»»åŠ¡ä¸è®°å¿†ç›¸å…³ã€‚æˆ‘ä»¬è®¾è®¡äº†128ä¸ªä»»åŠ¡ï¼Œæ¶µç›–26ä¸ªåº”ç”¨ç¨‹åºï¼Œå…¶ä¸­89.8%çš„ä»»åŠ¡æŒ‘æˆ˜è·¨æ—¶é—´å’Œè·¨ç©ºé—´çš„è®°å¿†ä¿æŒèƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºæ‰€æœ‰è¯„ä¼°ç³»ç»Ÿåœ¨è®°å¿†èƒ½åŠ›ä¸Šå­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼Œå¹¶æå‡ºäº†äº”ç§å¯è¡Œçš„è®¾è®¡å»ºè®®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.05281', 'title': 'Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities', 'url': 'https://huggingface.co/papers/2602.05281', 'abstract': 'A novel reinforcement learning approach called ARM addresses entropy collapse in LLM reasoning by equilibrating confidence levels across correct responses through dynamic reward shaping.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation, our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks. Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse. Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32, highlighting its superior capability in generating diverse correct reasoning paths.', 'score': 11, 'issue_id': 962, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 5', 'zh': '2æœˆ5æ—¥'}, 'hash': '2267e7fff36f239f', 'authors': ['Pengyi Li', 'Elizaveta Goncharova', 'Andrey Kuznetsov', 'Ivan Oseledets'], 'affiliations': ['FusionBrain Lab, Russia Mathematics, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2602.05281.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#rl'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ£Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ARM (Advantage Re-weighting Mechanism) Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GRPO, Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ğ½Ğ¸Ğ·ĞºĞ¾ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ñ‚Ğ°Ğº ĞºĞ°Ğº Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿ÑƒÑ‚Ğ¸ Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ»Ğ°Ğ±Ğ»ÑÑ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿ĞµÑ€ĞµÑ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ¼Ğ°Ğ»Ğ¾Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Qwen2.5 Ğ¸ DeepSeek Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Balancing Confidence for Diverse Reasoning in LLMs', 'desc': 'The paper introduces a new reinforcement learning method called Advantage Re-weighting Mechanism (ARM) to tackle the problem of entropy collapse in Large Language Models (LLMs). Traditional methods like Group Relative Policy Optimization (GRPO) often lead to low diversity in outputs due to over-reliance on high-likelihood responses. ARM addresses this by dynamically adjusting the reward structure to balance confidence levels across all valid responses, promoting exploration of alternative reasoning paths. Experimental results show that ARM improves generative diversity and response entropy while maintaining accuracy, outperforming GRPO in various benchmarks.'}, 'zh': {'title': 'å¹³è¡¡ç½®ä¿¡åº¦ï¼Œæå‡æ¨ç†å¤šæ ·æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç§°ä¸ºä¼˜åŠ¿é‡åŠ æƒæœºåˆ¶ï¼ˆARMï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä¸­çš„ç†µå´©æºƒé—®é¢˜ã€‚é€šè¿‡åŠ¨æ€å¥–åŠ±å¡‘å½¢ï¼ŒARMèƒ½å¤Ÿå¹³è¡¡æ‰€æœ‰æ­£ç¡®å“åº”çš„ç½®ä¿¡æ°´å¹³ï¼Œé¿å…äº†ä¼ ç»Ÿç­–ç•¥ä¼˜åŒ–æ–¹æ³•å¯¼è‡´çš„ä½ç†µç­–ç•¥å’Œæ¨¡å¼å´©æºƒã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç»“åˆæç¤ºå›°æƒ‘åº¦å’Œç­”æ¡ˆç½®ä¿¡åº¦æ¥é‡æ–°å¡‘é€ å¥–åŠ±ä¿¡å·ï¼Œä»è€ŒæŠ‘åˆ¶è¿‡äºè‡ªä¿¡çš„æ¨ç†è·¯å¾„çš„æ¢¯åº¦æ›´æ–°ï¼Œå¹¶å°†æ¦‚ç‡è´¨é‡é‡æ–°åˆ†é…ç»™æœªå……åˆ†æ¢ç´¢çš„æ­£ç¡®è§£å†³æ–¹æ¡ˆã€‚å®éªŒè¯æ˜ï¼ŒARMæ˜¾è‘—æé«˜äº†ç”Ÿæˆçš„å¤šæ ·æ€§å’Œå“åº”ç†µï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰åŠ›çš„å‡†ç¡®æ€§ï¼ŒæˆåŠŸå®ç°äº†æ¨ç†ä»»åŠ¡ä¸­æ¢ç´¢ä¸åˆ©ç”¨çš„ä¼˜è¶Šå¹³è¡¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.05027', 'title': 'AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders', 'url': 'https://huggingface.co/papers/2602.05027', 'abstract': "Sparse Autoencoders trained on Whisper and HuBERT models demonstrate stable feature extraction and effective disentanglement of acoustic and semantic information, showing practical applications in audio processing and correlation with human neural activity.  \t\t\t\t\tAI-generated summary \t\t\t\t Sparse Autoencoders (SAEs) are powerful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT, provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, including environmental noises and paralinguistic sounds (e.g. laughter, whispering) and disentangle them effectively, requiring removal of only 19-27% of features to erase a concept. Feature steering reduces Whisper's false speech detections by 70% with negligible WER increase, demonstrating real-world applicability. Finally, we find SAE features correlated with human EEG activity during speech perception, indicating alignment with human neural processing. The code and checkpoints are available at https://github.com/audiosae/audiosae_demo.", 'score': 9, 'issue_id': 965, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 4', 'zh': '2æœˆ4æ—¥'}, 'hash': 'ad429b923319d2b1', 'authors': ['Georgii Aparin', 'Tasnima Sadekova', 'Alexey Rukhovich', 'Assel Yermekova', 'Laida Kushnareva', 'Vadim Popov', 'Kristian Kuznetsov', 'Irina Piontkovskaya'], 'affiliations': ['Huawei Noahs Ark Lab'], 'pdf_title_img': 'assets/pdf/title_img/2602.05027.jpg', 'data': {'categories': ['#open_source', '#interpretability'], 'emoji': 'ğŸ‘‚', 'ru': {'title': 'Ğ Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¸ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ·Ğ²ÑƒĞºĞ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¸ (SAE) Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Whisper Ğ¸ HuBERT. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ 50% Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, Ğ¸ SAE ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‚ Ğ°ĞºÑƒÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Ğ”ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑÑ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° 70%. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ° ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ SAE Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ·Ğ³Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼.'}, 'en': {'title': 'Unlocking Audio Insights with Sparse Autoencoders', 'desc': 'This paper explores the use of Sparse Autoencoders (SAEs) for extracting features from audio data, specifically using the Whisper and HuBERT models. The authors demonstrate that SAEs can effectively disentangle acoustic and semantic information, maintaining stability and interpretability across different training conditions. They show that a significant portion of features remains consistent, allowing for the identification of specific audio events while minimizing the loss of important information. Additionally, the study reveals a strong correlation between SAE features and human neural activity, suggesting that these models align well with how humans process speech.'}, 'zh': {'title': 'ç¨€ç–è‡ªç¼–ç å™¨ï¼šéŸ³é¢‘å¤„ç†çš„æ–°å·¥å…·', 'desc': 'ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„å·¥å…·ï¼Œç”¨äºè§£é‡Šç¥ç»è¡¨ç¤ºï¼Œä½†åœ¨éŸ³é¢‘é¢†åŸŸçš„åº”ç”¨ä»ç„¶è¾ƒå°‘ã€‚æˆ‘ä»¬åœ¨Whisperå’ŒHuBERTçš„æ‰€æœ‰ç¼–ç å™¨å±‚ä¸Šè®­ç»ƒSAEï¼Œå¹¶å¯¹å…¶ç¨³å®šæ€§å’Œå¯è§£é‡Šæ€§è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¶…è¿‡50%çš„ç‰¹å¾åœ¨éšæœºç§å­ä¹‹é—´ä¿æŒä¸€è‡´ï¼Œé‡å»ºè´¨é‡å¾—ä»¥ä¿æŒã€‚SAEç‰¹å¾æœ‰æ•ˆåœ°åˆ†ç¦»äº†å£°å­¦å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶ä¸äººç±»åœ¨è¯­éŸ³æ„ŸçŸ¥è¿‡ç¨‹ä¸­çš„è„‘ç”µæ´»åŠ¨ç›¸å…³è”ï¼Œå±•ç¤ºäº†å…¶åœ¨éŸ³é¢‘å¤„ç†ä¸­çš„å®é™…åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.06079', 'title': 'Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers', 'url': 'https://huggingface.co/papers/2602.06079', 'abstract': 'Canzona presents a unified asynchronous framework that addresses the conflict between matrix-based optimizers and distributed tensor fragmentation in LLM training, improving efficiency and reducing latency.  \t\t\t\t\tAI-generated summary \t\t\t\t The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, a Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an alpha-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving a 1.57x speedup in end-to-end iteration time and reducing optimizer step latency by 5.8x compared to the baseline.', 'score': 7, 'issue_id': 965, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 4', 'zh': '2æœˆ4æ—¥'}, 'hash': 'a71d4a1e946c02a3', 'authors': ['Liangyu Wang', 'Siqi Zhang', 'Junjie Wang', 'Yiming Dong', 'Bo Zheng', 'Zihan Qiu', 'Shengkun Tang', 'Di Wang', 'Rui Men', 'Dayiheng Liu'], 'affiliations': ['Alibaba Group', 'KAUST', 'MBZUAI', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2602.06079.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training'], 'emoji': 'âš¡', 'ru': {'title': 'ĞÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ: Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ÑÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ñ‹', 'desc': 'Canzona Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğ¹ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Shampoo Ğ¸ Muon, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ñ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğ¼ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ°Ğ¼ Ñ‚Ğ¸Ğ¿Ğ° Megatron, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ñ‹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¸ Ğ¼Ğ¸ĞºÑ€Ğ¾-Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ¾Ñ‚Ğ´ĞµĞ»ÑÑ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¾Ñ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² 1.57 Ñ€Ğ°Ğ·Ğ° Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ ÑˆĞ°Ğ³Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ² 5.8 Ñ€Ğ°Ğ· Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen3 Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ´Ğ¾ 32B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Canzona: Bridging the Gap in LLM Training Efficiency', 'desc': 'Canzona introduces a new framework that improves the training of Large Language Models (LLMs) by resolving issues between matrix-based optimizers and distributed tensor fragmentation. It offers a unified, asynchronous approach that allows for efficient updates without the need for holistic parameter updates, which are often problematic in distributed systems. The framework includes a novel alpha-Balanced Static Partitioning strategy for Data Parallelism and an Asynchronous Compute pipeline for Tensor Parallelism, which together enhance load balancing and reduce latency. Evaluations show that Canzona significantly speeds up training times and optimizes performance on large models across multiple GPUs.'}, 'zh': {'title': 'Canzonaï¼šæå‡å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®­ç»ƒæ•ˆç‡çš„å¼‚æ­¥æ¡†æ¶', 'desc': 'Canzonaæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å¼‚æ­¥æ¡†æ¶ï¼Œè§£å†³äº†çŸ©é˜µä¼˜åŒ–å™¨ä¸åˆ†å¸ƒå¼å¼ é‡ç¢ç‰‡åŒ–ä¹‹é—´çš„å†²çªï¼Œä»è€Œæé«˜äº†æ•ˆç‡å¹¶å‡å°‘äº†å»¶è¿Ÿã€‚å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ‰©å±•æ¨åŠ¨äº†å¯¹çŸ©é˜µä¼˜åŒ–å™¨çš„å…³æ³¨ï¼Œä½†å®ƒä»¬å¯¹æ•´ä½“æ›´æ–°çš„éœ€æ±‚ä¸åˆ†å¸ƒå¼æ¡†æ¶ä¸­çš„å¼ é‡ç¢ç‰‡åŒ–ç›¸çŸ›ç›¾ã€‚ç°æœ‰çš„è§£å†³æ–¹æ¡ˆæ•ˆç‡ä¸é«˜ï¼Œå¯¼è‡´è®¡ç®—å†—ä½™æˆ–æ— æ³•æœ‰æ•ˆæ²Ÿé€šã€‚Canzonaé€šè¿‡è§£è€¦é€»è¾‘ä¼˜åŒ–å™¨åˆ†é…ä¸ç‰©ç†å‚æ•°åˆ†å¸ƒï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å¹¶è¡Œå’Œå¼ é‡å¹¶è¡Œç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†è®­ç»ƒæ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.06960', 'title': 'InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2602.06960', 'abstract': 'InftyThink+ uses reinforcement learning to optimize iterative reasoning processes, improving accuracy and efficiency in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization. InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning, enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.', 'score': 5, 'issue_id': 962, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 6', 'zh': '2æœˆ6æ—¥'}, 'hash': '0d9d6ab163765d9f', 'authors': ['Yuchen Yan', 'Liang Jiang', 'Jin Jiang', 'Shuaicheng Li', 'Zujie Wen', 'Zhiqiang Zhang', 'Jun Zhou', 'Jian Shao', 'Yueting Zhuang', 'Yongliang Shen'], 'affiliations': ['Ant Group', 'Peking University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2602.06960.jpg', 'data': {'categories': ['#small_models', '#optimization', '#rl', '#training', '#reasoning', '#long_context'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€ĞµĞ·ÑĞ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° InftyThink+, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğµ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞµÑ€ĞµĞ´Ğ¸Ğ½Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€ĞµĞ·ÑĞ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑÑ‚Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼ Ğ¾ Ñ‚Ğ¾Ğ¼, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸ ĞºĞ°Ğº Ñ€ĞµĞ·ÑĞ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 21% Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ….'}, 'en': {'title': 'Optimizing Reasoning with Reinforcement Learning', 'desc': 'InftyThink+ is a novel framework that enhances large language models by using reinforcement learning to optimize iterative reasoning processes. It addresses the limitations of traditional reasoning methods, such as high computational costs and context length restrictions, by implementing strategic summarization of intermediate thoughts. The framework employs a two-stage training approach, starting with supervised learning and transitioning to reinforcement learning, allowing the model to effectively decide when to summarize and how to continue reasoning. Experimental results show that InftyThink+ significantly boosts accuracy and efficiency, outperforming existing methods in both reasoning performance and inference speed.'}, 'zh': {'title': 'ä¼˜åŒ–æ¨ç†ï¼Œæå‡æ•ˆç‡â€”â€”InftyThink+', 'desc': 'InftyThink+ æ˜¯ä¸€ä¸ªä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–è¿­ä»£æ¨ç†è¿‡ç¨‹çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚è¯¥æ–¹æ³•é€šè¿‡å®šæœŸæ€»ç»“ä¸­é—´æ€è·¯æ¥ç¼“è§£æ¨ç†è¿‡ç¨‹ä¸­çš„é—®é¢˜ï¼Œå¦‚äºŒæ¬¡æˆæœ¬å’Œä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•ä¸åŒï¼ŒInftyThink+ é‡‡ç”¨ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ ï¼Œä¼˜åŒ–æ•´ä¸ªè¿­ä»£æ¨ç†è½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒInftyThink+ åœ¨å‡†ç¡®æ€§å’Œæ¨ç†æ•ˆç‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.06663', 'title': 'PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks', 'url': 'https://huggingface.co/papers/2602.06663', 'abstract': "PlanViz benchmark evaluates unified multimodal models' capabilities in computer-use planning tasks through route planning, work diagramming, and web&UI displaying sub-tasks with a task-adaptive scoring system.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal models (UMMs) have shown impressive capabilities in generating natural images and supporting multimodal reasoning. However, their potential in supporting computer-use planning tasks, which are closely related to our lives, remain underexplored. Image generation and editing in computer-use tasks require capabilities like spatial reasoning and procedural understanding, and it is still unknown whether UMMs have these capabilities to finish these tasks or not. Therefore, we propose PlanViz, a new benchmark designed to evaluate image generation and editing for computer-use tasks. To achieve the goal of our evaluation, we focus on sub-tasks which frequently involve in daily life and require planning steps. Specifically, three new sub-tasks are designed: route planning, work diagramming, and web&UI displaying. We address challenges in data quality ensuring by curating human-annotated questions and reference images, and a quality control process. For challenges of comprehensive and exact evaluation, a task-adaptive score, PlanScore, is proposed. The score helps understanding the correctness, visual quality and efficiency of generated images. Through experiments, we highlight key limitations and opportunities for future research on this topic.", 'score': 5, 'issue_id': 962, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 6', 'zh': '2æœˆ6æ—¥'}, 'hash': '83fea21ac7f4026f', 'authors': ['Junxian Li', 'Kai Liu', 'Leyang Chen', 'Weida Wang', 'Zhixin Wang', 'Jiaqi Xu', 'Fan Li', 'Renjing Pei', 'Linghe Kong', 'Yulun Zhang'], 'affiliations': ['Alibaba Group', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2602.06663.jpg', 'data': {'categories': ['#survey', '#multimodal', '#benchmark', '#cv', '#dataset'], 'emoji': 'ğŸ“‹', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº PlanViz Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ñ Ğ²ĞµĞ±-Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ñ‹ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑƒĞ¼ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€. Ğ”Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ PlanScore, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ° Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Evaluating Multimodal Models for Everyday Computer Planning Tasks', 'desc': 'The paper introduces PlanViz, a benchmark designed to assess the capabilities of unified multimodal models (UMMs) in performing computer-use planning tasks. It focuses on three specific sub-tasks: route planning, work diagramming, and web & UI displaying, which are relevant to everyday life. The benchmark employs a task-adaptive scoring system called PlanScore to evaluate the correctness, visual quality, and efficiency of the generated images. This research aims to uncover the strengths and weaknesses of UMMs in spatial reasoning and procedural understanding within the context of computer-use tasks.'}, 'zh': {'title': 'è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹åœ¨è®¡ç®—æœºä½¿ç”¨è§„åˆ’ä¸­çš„èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†PlanVizåŸºå‡†ï¼Œç”¨äºè¯„ä¼°ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹åœ¨è®¡ç®—æœºä½¿ç”¨è§„åˆ’ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†é€šè¿‡è·¯çº¿è§„åˆ’ã€å·¥ä½œå›¾ç¤ºå’Œç½‘é¡µä¸ç”¨æˆ·ç•Œé¢å±•ç¤ºç­‰å­ä»»åŠ¡ï¼Œé‡‡ç”¨ä»»åŠ¡è‡ªé€‚åº”è¯„åˆ†ç³»ç»Ÿè¿›è¡Œè¯„ä¼°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹åœ¨ç”Ÿæˆè‡ªç„¶å›¾åƒå’Œæ”¯æŒå¤šæ¨¡æ€æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è®¡ç®—æœºä½¿ç”¨è§„åˆ’ä»»åŠ¡ä¸­çš„æ½œåŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬æ­ç¤ºäº†å½“å‰æ¨¡å‹çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœºä¼šã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.05367', 'title': 'RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs', 'url': 'https://huggingface.co/papers/2602.05367', 'abstract': 'Residual binarization framework RaBiT addresses feature co-adaptation in quantized LLMs through hierarchical path derivation and robust initialization, achieving superior accuracy-efficiency trade-offs.  \t\t\t\t\tAI-generated summary \t\t\t\t Efficient deployment of large language models (LLMs) requires extreme quantization, forcing a critical trade-off between low-bit efficiency and performance. Residual binarization enables hardware-friendly, matmul-free inference by stacking binary (pm1) layers, but is plagued by pathological feature co-adaptation. We identify a key failure mode, which we term inter-path adaptation: during quantization-aware training (QAT), parallel residual binary paths learn redundant features, degrading the error-compensation structure and limiting the expressive capacity of the model. While prior work relies on heuristic workarounds (e.g., path freezing) that constrain the solution space, we propose RaBiT, a novel quantization framework that resolves co-adaptation by algorithmically enforcing a residual hierarchy. Its core mechanism sequentially derives each binary path from a single shared full-precision weight, which ensures that every path corrects the error of the preceding one. This process is stabilized by a robust initialization that prioritizes functional preservation over mere weight approximation. RaBiT redefines the 2-bit accuracy-efficiency frontier: it achieves state-of-the-art performance, rivals even hardware-intensive Vector Quantization (VQ) methods, and delivers a 4.49times inference speed-up over full-precision models on an RTX 4090.', 'score': 5, 'issue_id': 963, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 5', 'zh': '2æœˆ5æ—¥'}, 'hash': '042ce2bd26c0169f', 'authors': ['Youngcheon You', 'Banseok Lee', 'Minseop Choi', 'Seonyoung Kim', 'Hyochan Chong', 'Changdong Kim', 'Youngmin Kim', 'Dongkyu Kim'], 'affiliations': ['Samsung Research, Seoul, Korea'], 'pdf_title_img': 'assets/pdf/title_img/2602.05367.jpg', 'data': {'categories': ['#inference', '#architecture', '#training', '#optimization'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° LLM', 'desc': 'RaBiT â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²ĞµÑĞ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼ ÑĞ»Ğ¾ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½ÑƒÑ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ°Ğ´ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ²ĞµÑĞ¾Ğ². RaBiT Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ, ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ 4,49-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ½Ğ° RTX 4090.'}, 'en': {'title': 'RaBiT: Revolutionizing Quantization for Efficient LLMs', 'desc': "The paper introduces the Residual Binarization framework (RaBiT) to tackle the issue of feature co-adaptation in quantized large language models (LLMs). It highlights a problem called inter-path adaptation, where multiple binary paths learn similar features, which reduces the model's performance. RaBiT addresses this by creating a hierarchical structure for binary paths that ensures each path builds on the previous one, improving error correction. This method not only enhances accuracy but also significantly speeds up inference, outperforming traditional quantization techniques."}, 'zh': {'title': 'æ®‹å·®äºŒå€¼åŒ–ï¼šæå‡é‡åŒ–æ¨¡å‹æ€§èƒ½çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºRaBiTçš„æ®‹å·®äºŒå€¼åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é‡åŒ–å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„ç‰¹å¾å…±é€‚åº”é—®é¢˜ã€‚é€šè¿‡åˆ†å±‚è·¯å¾„æ¨å¯¼å’Œç¨³å¥åˆå§‹åŒ–ï¼ŒRaBiTèƒ½å¤Ÿåœ¨ä½æ¯”ç‰¹æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ä¹‹é—´å®ç°ä¼˜è¶Šçš„æƒè¡¡ã€‚è¯¥æ¡†æ¶é€šè¿‡ç®—æ³•æ€§åœ°å¼ºåˆ¶æ‰§è¡Œæ®‹å·®å±‚æ¬¡ç»“æ„ï¼Œç¡®ä¿æ¯ä¸ªäºŒè¿›åˆ¶è·¯å¾„éƒ½èƒ½çº æ­£å‰ä¸€ä¸ªè·¯å¾„çš„é”™è¯¯ï¼Œä»è€Œæé«˜æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚æœ€ç»ˆï¼ŒRaBiTåœ¨2æ¯”ç‰¹ç²¾åº¦æ•ˆç‡çš„å‰æ²¿ä¸Šé‡æ–°å®šä¹‰äº†æ€§èƒ½æ ‡å‡†ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†é€Ÿåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.05711', 'title': 'OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale', 'url': 'https://huggingface.co/papers/2602.05711', 'abstract': 'OmniMoE presents a system-algorithm co-designed framework that achieves fine-grained expert specialization in Mixture-of-Experts architectures through vector-level atomic experts and optimized routing and scheduling mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Mixture-of-Experts (MoE) architectures are evolving towards finer granularity to improve parameter efficiency. However, existing MoE designs face an inherent trade-off between the granularity of expert specialization and hardware execution efficiency. We propose OmniMoE, a system-algorithm co-designed framework that pushes expert granularity to its logical extreme. OmniMoE introduces vector-level Atomic Experts, enabling scalable routing and execution within a single MoE layer, while retaining a shared dense MLP branch for general-purpose processing. Although this atomic design maximizes capacity, it poses severe challenges for routing complexity and memory access. To address these, OmniMoE adopts a system-algorithm co-design: (i) a Cartesian Product Router that decomposes the massive index space to reduce routing complexity from O(N) to O(sqrt(N)); and (ii) Expert-Centric Scheduling that inverts the execution order to turn scattered, memory-bound lookups into efficient dense matrix operations. Validated on seven benchmarks, OmniMoE (with 1.7B active parameters) achieves 50.9% zero-shot accuracy across seven benchmarks, outperforming coarse-grained (e.g., DeepSeekMoE) and fine-grained (e.g., PEER) baselines. Crucially, OmniMoE reduces inference latency from 73ms to 6.7ms (a 10.9-fold speedup) compared to PEER, demonstrating that massive-scale fine-grained MoE can be fast and accurate. Our code is open-sourced at https://github.com/flash-algo/omni-moe.', 'score': 4, 'issue_id': 962, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 5', 'zh': '2æœˆ5æ—¥'}, 'hash': '688adfd08d5f60d2', 'authors': ['Jingze Shi', 'Zhangyang Peng', 'Yizhang Zhu', 'Yifan Wu', 'Guang Liu', 'Yuyu Luo'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2602.05711.jpg', 'data': {'categories': ['#inference', '#training', '#architecture'], 'emoji': 'âš¡', 'ru': {'title': 'ĞÑ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹: Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸', 'desc': 'OmniMoE Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Mixture-of-Experts Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ². Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹: Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´ĞµĞºĞ°Ñ€Ñ‚Ğ¾Ğ²Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ O(N) Ğ´Ğ¾ O(âˆšN), Ğ° Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ½Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ², Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ñ€Ğ°ÑÑĞµÑĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ. ĞĞ° ÑĞµĞ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… OmniMoE Ñ 1,7B Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 50,9% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² zero-shot Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ ĞºĞ°Ğº Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ñ Ğ¼ĞµĞ»ĞºĞ¾Ğ¹ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ 10,9-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° (Ñ 73ms Ğ´Ğ¾ 6,7ms) Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ PEER, Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ fine-grained MoE Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹.'}, 'en': {'title': 'Unlocking Speed and Precision in Mixture-of-Experts with OmniMoE', 'desc': 'OmniMoE is a new framework designed to enhance Mixture-of-Experts (MoE) architectures by achieving fine-grained expert specialization. It introduces vector-level Atomic Experts, which allow for more efficient routing and execution within a single MoE layer while maintaining a shared dense MLP for general tasks. The framework addresses challenges in routing complexity and memory access through innovative techniques like the Cartesian Product Router and Expert-Centric Scheduling. As a result, OmniMoE significantly improves accuracy and reduces inference latency, demonstrating that fine-grained MoE can be both fast and effective.'}, 'zh': {'title': 'OmniMoEï¼šç»†ç²’åº¦ä¸“å®¶çš„é«˜æ•ˆå®ç°', 'desc': 'OmniMoEæ˜¯ä¸€ç§ç³»ç»Ÿä¸ç®—æ³•å…±åŒè®¾è®¡çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å‘é‡çº§åŸå­ä¸“å®¶å®ç°æ··åˆä¸“å®¶æ¶æ„ä¸­çš„ç»†ç²’åº¦ä¸“å®¶ä¸“ä¸šåŒ–ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¼˜åŒ–è·¯ç”±å’Œè°ƒåº¦æœºåˆ¶ï¼Œå…‹æœäº†ä¸“å®¶ä¸“ä¸šåŒ–ç²’åº¦ä¸ç¡¬ä»¶æ‰§è¡Œæ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚OmniMoEå¼•å…¥äº†åŸå­ä¸“å®¶ï¼Œä½¿å¾—åœ¨å•ä¸ªMoEå±‚å†…å®ç°å¯æ‰©å±•çš„è·¯ç”±å’Œæ‰§è¡Œï¼ŒåŒæ—¶ä¿ç•™äº†ç”¨äºé€šç”¨å¤„ç†çš„å…±äº«å¯†é›†MLPåˆ†æ”¯ã€‚ç»è¿‡ä¸ƒä¸ªåŸºå‡†æµ‹è¯•éªŒè¯ï¼ŒOmniMoEåœ¨æ¨ç†å»¶è¿Ÿå’Œå‡†ç¡®æ€§ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾ç¤ºå‡ºå¤§è§„æ¨¡ç»†ç²’åº¦MoEçš„å¿«é€Ÿä¸å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.06854', 'title': 'SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks', 'url': 'https://huggingface.co/papers/2602.06854', 'abstract': 'A novel framework called SEMA is introduced that effectively trains multi-turn attackers for large language models without relying on existing strategies or external data, achieving state-of-the-art attack success rates while being compact, reproducible, and transferable across different models and datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-turn jailbreaks capture the real threat model for safety-aligned chatbots, where single-turn attacks are merely a special case. Yet existing approaches break under exploration complexity and intent drift. We propose SEMA, a simple yet effective framework that trains a multi-turn attacker without relying on any existing strategies or external data. SEMA comprises two stages. Prefilling self-tuning enables usable rollouts by fine-tuning on non-refusal, well-structured, multi-turn adversarial prompts that are self-generated with a minimal prefix, thereby stabilizing subsequent learning. Reinforcement learning with intent-drift-aware reward trains the attacker to elicit valid multi-turn adversarial prompts while maintaining the same harmful objective. We anchor harmful intent in multi-turn jailbreaks via an intent-drift-aware reward that combines intent alignment, compliance risk, and level of detail. Our open-loop attack regime avoids dependence on victim feedback, unifies single- and multi-turn settings, and reduces exploration complexity. Across multiple datasets, victim models, and jailbreak judges, our method achieves state-of-the-art (SOTA) attack success rates (ASR), outperforming all single-turn baselines, manually scripted and template-driven multi-turn baselines, as well as our SFT (Supervised Fine-Tuning) and DPO (Direct Preference Optimization) variants. For instance, SEMA performs an average 80.1% ASR@1 across three closed-source and open-source victim models on AdvBench, 33.9% over SOTA. The approach is compact, reproducible, and transfers across targets, providing a stronger and more realistic stress test for large language model (LLM) safety and enabling automatic redteaming to expose and localize failure modes. Our code is available at: https://github.com/fmmarkmq/SEMA.', 'score': 3, 'issue_id': 963, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 6', 'zh': '2æœˆ6æ—¥'}, 'hash': '5545fb68273ad4d6', 'authors': ['Mingqian Feng', 'Xiaodong Liu', 'Weiwei Yang', 'Jialin Song', 'Xuekai Zhu', 'Chenliang Xu', 'Jianfeng Gao'], 'affiliations': ['Microsoft Research', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2602.06854.jpg', 'data': {'categories': ['#training', '#rl', '#alignment', '#security', '#open_source'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ğµ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ½Ğ° LLM Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº SEMA Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ñ ÑĞ°Ğ¼Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ñ… Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ´Ñ€ĞµĞ¹Ñ„Ğ° Ğ¸Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾Ğ¹ Ñ†ĞµĞ»Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ° Ğ² Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ°Ñ‚Ğ°Ğº (80.1% Ğ½Ğ° AdvBench), Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ğµ Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚ĞµĞ½, Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸-Ğ¶ĞµÑ€Ñ‚Ğ²Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'SEMA: Revolutionizing Multi-Turn Attacks for Safer Language Models', 'desc': 'The paper introduces SEMA, a new framework designed to train multi-turn attackers for large language models (LLMs) without using existing strategies or external data. It addresses the limitations of previous methods that struggle with exploration complexity and intent drift by employing a two-stage process: prefilling self-tuning and reinforcement learning with intent-drift-aware rewards. SEMA achieves state-of-the-art attack success rates by effectively generating adversarial prompts that maintain harmful intent across multiple turns. This approach is compact, reproducible, and transferable, making it a robust tool for testing the safety of LLMs.'}, 'zh': {'title': 'SEMAï¼šå¤šè½®æ”»å‡»çš„æ–°æ¡†æ¶ï¼Œæå‡è¯­è¨€æ¨¡å‹å®‰å…¨æ€§', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSEMAçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨æœ‰æ•ˆè®­ç»ƒå¤šè½®æ”»å‡»è€…ï¼Œä»¥åº”å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§é—®é¢˜ã€‚SEMAä¸ä¾èµ–äºç°æœ‰ç­–ç•¥æˆ–å¤–éƒ¨æ•°æ®ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒæ¨¡å‹å’Œæ•°æ®é›†ä¸Šå®ç°æœ€å…ˆè¿›çš„æ”»å‡»æˆåŠŸç‡ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šè‡ªæˆ‘è°ƒä¼˜çš„é¢„å¡«å……å’ŒåŸºäºæ„å›¾æ¼‚ç§»çš„å¼ºåŒ–å­¦ä¹ ï¼Œç¡®ä¿ç”Ÿæˆæœ‰æ•ˆçš„å¤šè½®å¯¹æŠ—æç¤ºã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼ŒSEMAåœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡å‹ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæä¾›äº†æ›´å¼ºå¤§å’Œç°å®çš„å‹åŠ›æµ‹è¯•ï¼Œä¿ƒè¿›äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.04837', 'title': 'Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing', 'url': 'https://huggingface.co/papers/2602.04837', 'abstract': 'Group-Evolving Agents enable open-ended self-improvement by treating groups of agents as evolutionary units, allowing efficient experience sharing and reuse to enhance coding performance and robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.', 'score': 3, 'issue_id': 965, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 4', 'zh': '2æœˆ4æ—¥'}, 'hash': '5f9249cbfedb151c', 'authors': ['Zhaotian Weng', 'Antonis Antoniades', 'Deepak Nathani', 'Zhen Zhang', 'Xiao Pu', 'Xin Eric Wang'], 'affiliations': ['University of California, Santa Barbara'], 'pdf_title_img': 'assets/pdf/title_img/2602.04837.jpg', 'data': {'categories': ['#benchmark', '#plp', '#training', '#agents'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ°Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Group-Evolving Agents (GEA) Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ°Ğº ĞµĞ´Ğ¸Ğ½ÑƒÑ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ñƒ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ğ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸, GEA Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¾Ğ±Ğ¼ĞµĞ½ Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğ¼ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµÑ‚Ğ²ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (71.0% Ğ½Ğ° SWE-bench Verified Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² 56.7% Ñƒ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ²). GEA Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒÑ Ñ€Ğ°Ğ½Ğ½ÑÑ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºÑƒÑ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ.'}, 'en': {'title': 'Empowering Agents Through Group Evolution', 'desc': 'Group-Evolving Agents (GEA) represent a novel approach in machine learning that allows groups of agents to evolve together, enhancing their ability to share and reuse experiences. This method enables agents to autonomously adapt their structures, leading to improved performance without heavy reliance on human input. GEA outperforms traditional self-evolving methods by effectively utilizing exploratory diversity, resulting in better coding capabilities and robustness. The framework demonstrates significant advancements in coding benchmarks, showcasing its ability to achieve long-term progress and adaptability across various models.'}, 'zh': {'title': 'ç¾¤ä½“è¿›åŒ–ä»£ç†ï¼šå¼€å¯è‡ªæˆ‘æ”¹è¿›çš„æ–°çºªå…ƒ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¼€æ”¾å¼è‡ªæˆ‘æ”¹è¿›ä»£ç†æ¨¡å‹ï¼Œç§°ä¸ºç¾¤ä½“è¿›åŒ–ä»£ç†ï¼ˆGEAï¼‰ã€‚GEAå°†ä»£ç†ç»„è§†ä¸ºåŸºæœ¬çš„è¿›åŒ–å•å…ƒï¼Œå…è®¸åœ¨è¿›åŒ–è¿‡ç¨‹ä¸­å®ç°ç»éªŒçš„æ˜¾å¼å…±äº«å’Œé‡ç”¨ï¼Œä»è€Œæé«˜ç¼–ç æ€§èƒ½å’Œé²æ£’æ€§ã€‚ä¸ç°æœ‰çš„æ ‘çŠ¶è¿›åŒ–æ–¹æ³•ä¸åŒï¼ŒGEAå…‹æœäº†ç”±äºå­¤ç«‹è¿›åŒ–åˆ†æ”¯å¯¼è‡´çš„æ¢ç´¢å¤šæ ·æ€§åˆ©ç”¨æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGEAåœ¨å¤šä¸ªç¼–ç åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†æœ€å…ˆè¿›çš„è‡ªæˆ‘è¿›åŒ–æ–¹æ³•ï¼Œå¹¶åœ¨ä¸åŒç¼–ç æ¨¡å‹ä¸­è¡¨ç°å‡ºä¸€è‡´çš„å¯è¿ç§»æ€§å’Œæ›´å¼ºçš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.06554', 'title': 'SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees', 'url': 'https://huggingface.co/papers/2602.06554', 'abstract': "SeeUPO is a critic-free reinforcement learning method that ensures convergence guarantees in multi-turn agent interactions by modeling sequential decision-making as multi-agent bandit problems and using backward induction for policy updates.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies.   In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties in single/multi-turn scenarios. We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPO's original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios.   To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), a critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems. Through turn-by-turn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backward induction.   Experiments on AppWorld and BFCL v4 demonstrate SeeUPO's substantial improvements over existing backbone algorithms: relative gains of 43.3%-54.6% on Qwen3-14B and 24.1%-41.9% on Qwen2.5-14B (averaged across benchmarks), along with superior training stability.", 'score': 2, 'issue_id': 963, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 6', 'zh': '2æœˆ6æ—¥'}, 'hash': 'b47e2950662223f8', 'authors': ['Tianyi Hu', 'Qingxu Fu', 'Yanxi Chen', 'Zhaoyang Liu', 'Bolin Ding'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2602.06554.jpg', 'data': {'categories': ['#agents', '#training', '#rl', '#alignment', '#optimization', '#reasoning'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ“Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ· ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ Ğ¸Ğ½Ğ´ÑƒĞºÑ†Ğ¸Ñ', 'desc': 'SeeUPO â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ· ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ…Ğ¾Ğ´Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ (PPO, REINFORCE) Ñ‚ĞµÑ€ÑÑÑ‚ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ğ¸ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ bepaÑ€ametrĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ ĞºĞ°Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ğ½Ğ´Ğ¸Ñ‚Ğ° Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ Ğ¸Ğ½Ğ´ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ² Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ Ñ…Ğ¾Ğ´Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… AppWorld Ğ¸ BFCL v4.'}, 'en': {'title': 'SeeUPO: Ensuring Convergence in Multi-Turn Reinforcement Learning', 'desc': 'The paper introduces SeeUPO, a novel reinforcement learning method designed for multi-turn interactions among agents. It addresses the lack of convergence guarantees in existing RL algorithms by modeling these interactions as multi-agent bandit problems. SeeUPO employs backward induction for policy updates, ensuring that the learning process leads to optimal solutions while maintaining stability. Experimental results show that SeeUPO significantly outperforms traditional RL methods, achieving higher performance and better training consistency.'}, 'zh': {'title': 'SeeUPOï¼šå¤šè½®äº¤äº’ä¸­çš„æ— è¯„è®ºå¼ºåŒ–å­¦ä¹ æ–°æ–¹æ³•', 'desc': 'SeeUPOæ˜¯ä¸€ç§æ— è¯„è®ºçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨ç¡®ä¿å¤šè½®ä»£ç†äº¤äº’ä¸­çš„æ”¶æ•›æ€§ã€‚å®ƒå°†é¡ºåºå†³ç­–å»ºæ¨¡ä¸ºå¤šæ™ºèƒ½ä½“èµŒåšé—®é¢˜ï¼Œå¹¶é€šè¿‡åå‘æ¨å¯¼è¿›è¡Œç­–ç•¥æ›´æ–°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨å¤šè½®åœºæ™¯ä¸­æ— æ³•åŒæ—¶å®ç°æ— è¯„è®ºå’Œæ”¶æ•›ä¿è¯ã€‚SeeUPOé€šè¿‡é€è½®çš„åå‘æ‰§è¡Œé¡ºåºç­–ç•¥æ›´æ–°ï¼Œç¡®ä¿äº†å•è°ƒæ”¹è¿›å’Œå…¨å±€æœ€ä¼˜è§£çš„æ”¶æ•›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.06471', 'title': 'Revisiting the Shape Convention of Transformer Language Models', 'url': 'https://huggingface.co/papers/2602.06471', 'abstract': 'Replacing conventional feed-forward networks with hourglass-shaped MLPs in Transformers improves model efficiency and performance by enabling better parameter utilization and competitive scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Dense Transformer language models have largely adhered to one consistent architectural shape: each layer consists of an attention module followed by a feed-forward network (FFN) with a narrow-wide-narrow MLP, allocating most parameters to the MLP at expansion ratios between 2 and 4. Motivated by recent results that residual wide-narrow-wide (hourglass) MLPs offer superior function approximation capabilities, we revisit the long-standing MLP shape convention in Transformer, challenging the necessity of the narrow-wide-narrow design. To study this, we develop a Transformer variant that replaces the conventional FFN with a deeper hourglass-shaped FFN, comprising a stack of hourglass sub-MLPs connected by residual pathways. We posit that a deeper but lighter hourglass FFN can serve as a competitive alternative to the conventional FFN, and that parameters saved by using a lighter hourglass FFN can be more effectively utilized, such as by enlarging model hidden dimensions under fixed budgets. We confirm these through empirical validations across model scales: hourglass FFNs outperform conventional FFNs up to 400M and achieve comparable performance at larger scales to 1B parameters; hourglass FFN variants with reduced FFN and increased attention parameters show consistent improvements over conventional configurations at matched budgets. Together, these findings shed new light on recent work and prompt a rethinking of the narrow-wide-narrow MLP convention and the balance between attention and FFN towards efficient and expressive modern language models.', 'score': 2, 'issue_id': 962, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 6', 'zh': '2æœˆ6æ—¥'}, 'hash': '766c634e6c31c129', 'authors': ['Feng-Ting Liao', 'Meng-Hsi Chen', 'Guan-Ting Yi', 'Da-shan Shiu'], 'affiliations': ['MediaTek Research', 'National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2602.06471.jpg', 'data': {'categories': ['#optimization'], 'emoji': 'â³', 'ru': {'title': 'ĞŸĞµÑĞ¾Ñ‡Ğ½Ñ‹Ğµ Ñ‡Ğ°ÑÑ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¸: Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑĞ²ÑĞ·Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ (FFN) Ğ² Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ… Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ ÑĞµÑ‚Ğ¸ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğµ Ğ¿ĞµÑĞ¾Ñ‡Ğ½Ñ‹Ñ… Ñ‡Ğ°ÑĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾-ÑƒĞ·ĞºĞ¾-Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ°Ñ (hourglass) Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ĞœĞ›ĞŸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑƒĞ·ĞºĞ¾-ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾-ÑƒĞ·ĞºĞ¾Ğ¹ ÑÑ…ĞµĞ¼Ğ¾Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑĞ¾Ñ‡Ğ½Ñ‹Ñ… Ñ‡Ğ°ÑĞ¾Ğ² Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ¾ 400 Ğ¼Ğ»Ğ½ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹, ÑÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° FFN, Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¿ĞµÑ€ĞµÑ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ² ÑĞ»Ğ¾Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¸ Ğ½Ğ°Ñ…Ğ¾Ğ´ĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑĞ²ÑĞ·Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ»Ğ¾ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Rethinking MLP Shapes for Efficient Transformers', 'desc': 'This paper explores the use of hourglass-shaped multi-layer perceptrons (MLPs) in Transformers, replacing the traditional narrow-wide-narrow feed-forward networks (FFNs). The authors argue that hourglass MLPs provide better parameter efficiency and performance by allowing for deeper architectures with effective residual connections. Empirical results show that hourglass FFNs outperform conventional designs in smaller models and maintain competitive performance in larger models. This research encourages a reevaluation of the standard MLP structure in Transformers, promoting a more efficient balance between attention mechanisms and feed-forward networks.'}, 'zh': {'title': 'ç”¨å°æ—¶glass MLPæå‡Transformeræ•ˆç‡ä¸æ€§èƒ½', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„Transformeræ¶æ„ï¼Œä½¿ç”¨å°æ—¶glasså½¢çŠ¶çš„å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰æ›¿ä»£ä¼ ç»Ÿçš„å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ï¼Œä»¥æé«˜æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°æ—¶glass MLPåœ¨å‚æ•°åˆ©ç”¨ç‡å’ŒåŠŸèƒ½é€¼è¿‘èƒ½åŠ›ä¸Šä¼˜äºä¼ ç»Ÿçš„çª„-å®½-çª„è®¾è®¡ã€‚é€šè¿‡å®éªŒè¯æ˜ï¼Œå°æ—¶glass FFNåœ¨æ¨¡å‹è§„æ¨¡è¾¾åˆ°4äº¿å‚æ•°æ—¶è¡¨ç°ä¼˜äºä¼ ç»ŸFFNï¼Œå¹¶åœ¨æ›´å¤§è§„æ¨¡ï¼ˆå¦‚10äº¿å‚æ•°ï¼‰æ—¶æ€§èƒ½ç›¸å½“ã€‚è¯¥ç ”ç©¶ä¿ƒä½¿æˆ‘ä»¬é‡æ–°æ€è€ƒTransformerä¸­FFNä¸æ³¨æ„åŠ›æœºåˆ¶ä¹‹é—´çš„å¹³è¡¡ï¼Œæ¨åŠ¨ç°ä»£è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆä¸è¡¨è¾¾èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.05847', 'title': 'OmniVideo-R1: Reinforcing Audio-visual Reasoning with Query Intention and Modality Attention', 'url': 'https://huggingface.co/papers/2602.05847', 'abstract': 'OmniVideo-R1 enhances audio-visual understanding through reinforced frameworks that integrate self-supervised and contrastive learning for multimodal reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t While humans perceive the world through diverse modalities that operate synergistically to support a holistic understanding of their surroundings, existing omnivideo models still face substantial challenges on audio-visual understanding tasks. In this paper, we propose OmniVideo-R1, a novel reinforced framework that improves mixed-modality reasoning. OmniVideo-R1 empowers models to "think with omnimodal cues" by two key strategies: (1) query-intensive grounding based on self-supervised learning paradigms; and (2) modality-attentive fusion built upon contrastive learning paradigms. Extensive experiments on multiple benchmarks demonstrate that OmniVideo-R1 consistently outperforms strong baselines, highlighting its effectiveness and robust generalization capabilities.', 'score': 2, 'issue_id': 968, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 5', 'zh': '2æœˆ5æ—¥'}, 'hash': '112491ce815401a9', 'authors': ['Zhangquan Chen', 'Jiale Tao', 'Ruihuang Li', 'Yihao Hu', 'Ruitao Chen', 'Zhantao Yang', 'Xinlei Yu', 'Haodong Jing', 'Manyuan Zhang', 'Shuai Shao', 'Biao Wang', 'Qinglin Lu', 'Ruqi Huang'], 'affiliations': ['CUHK', 'HNU', 'NUS', 'THU', 'Tencent HY', 'XJTU'], 'pdf_title_img': 'assets/pdf/title_img/2602.05847.jpg', 'data': {'categories': ['#training', '#video', '#rl', '#multimodal', '#audio'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ²ÑƒĞºĞ° Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'OmniVideo-R1 â€” ÑÑ‚Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ğµ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ¸Ğ· Ğ²ÑĞµÑ… Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ.'}, 'en': {'title': 'Empowering Multimodal Reasoning with OmniVideo-R1', 'desc': 'OmniVideo-R1 is a new framework designed to enhance how machines understand audio and visual information together. It uses self-supervised learning to help models focus on important cues in the data, and contrastive learning to effectively combine different types of information. This approach allows the model to reason better across multiple modalities, similar to how humans process sensory information. Experiments show that OmniVideo-R1 performs better than existing models on various tasks, proving its strength and adaptability.'}, 'zh': {'title': 'OmniVideo-R1ï¼šæå‡éŸ³è§†é¢‘ç†è§£çš„æ–°æ¡†æ¶', 'desc': 'OmniVideo-R1 æ˜¯ä¸€ç§å¢å¼ºéŸ³è§†é¢‘ç†è§£çš„æ¡†æ¶ï¼Œç»“åˆäº†è‡ªç›‘ç£å­¦ä¹ å’Œå¯¹æ¯”å­¦ä¹ çš„æ–¹æ³•ã€‚è¯¥æ¨¡å‹é€šè¿‡æŸ¥è¯¢å¯†é›†çš„è‡ªç›‘ç£å­¦ä¹ å’ŒåŸºäºå¯¹æ¯”å­¦ä¹ çš„æ¨¡æ€æ³¨æ„åŠ›èåˆï¼Œæå‡äº†å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniVideo-R1 åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„å¼ºåŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºå…¶æœ‰æ•ˆæ€§å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤ç ”ç©¶ä¸ºéŸ³è§†é¢‘ç†è§£ä»»åŠ¡æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.03548', 'title': 'SEAD: Self-Evolving Agent for Multi-Turn Service Dialogue', 'url': 'https://huggingface.co/papers/2602.03548', 'abstract': 'SEAD framework enables service dialogue agents to learn effective strategies through self-evolving user modeling components, achieving superior task completion and dialogue efficiency compared to existing foundation and commercial models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models have demonstrated remarkable capabilities in open-domain dialogues. However, current methods exhibit suboptimal performance in service dialogues, as they rely on noisy, low-quality human conversation data. This limitation arises from data scarcity and the difficulty of simulating authentic, goal-oriented user behaviors. To address these issues, we propose SEAD (Self-Evolving Agent for Service Dialogue), a framework that enables agents to learn effective strategies without large-scale human annotations. SEAD decouples user modeling into two components: a Profile Controller that generates diverse user states to manage training curriculum, and a User Role-play Model that focuses on realistic role-playing. This design ensures the environment provides adaptive training scenarios rather than acting as an unfair adversary. Experiments demonstrate that SEAD significantly outperforms Open-source Foundation Models and Closed-source Commercial Models, improving task completion rate by 17.6% and dialogue efficiency by 11.1%. Code is available at: https://github.com/Da1yuqin/SEAD.', 'score': 2, 'issue_id': 968, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 3', 'zh': '2æœˆ3æ—¥'}, 'hash': '2156450b8d007196', 'authors': ['Yuqin Dai', 'Ning Gao', 'Wei Zhang', 'Jie Wang', 'Zichen Luo', 'Jinpeng Wang', 'Yujie Wang', 'Ruiyuan Wu', 'Chaozheng Wang'], 'affiliations': ['Meituan'], 'pdf_title_img': 'assets/pdf/title_img/2602.03548.jpg', 'data': {'categories': ['#multimodal', '#training', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'SEAD â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞµÑ€Ğ²Ğ¸ÑĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ ĞµĞ³Ğ¾ Ğ½Ğ° Ğ´Ğ²Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Profile Controller Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¸ User Role-play Model Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ñ‚Ğ°Ğ³Ğ¾Ğ½Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ€ĞµĞ´Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ: Ğ½Ğ° 17,6% Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ½Ğ° 11,1% ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Empowering Dialogue Agents with Self-Evolving User Models', 'desc': 'The SEAD framework enhances service dialogue agents by allowing them to develop effective strategies through self-evolving user modeling components. It addresses the limitations of existing models that rely on low-quality human conversation data, which often leads to poor performance in service dialogues. By decoupling user modeling into a Profile Controller and a User Role-play Model, SEAD creates adaptive training scenarios that simulate realistic user behaviors. Experimental results show that SEAD outperforms both open-source and commercial models, achieving a 17.6% increase in task completion and an 11.1% boost in dialogue efficiency.'}, 'zh': {'title': 'SEADï¼šæœåŠ¡å¯¹è¯çš„è‡ªæˆ‘æ¼”åŒ–ä»£ç†', 'desc': 'SEADæ¡†æ¶ä½¿æœåŠ¡å¯¹è¯ä»£ç†èƒ½å¤Ÿé€šè¿‡è‡ªæˆ‘æ¼”åŒ–çš„ç”¨æˆ·å»ºæ¨¡ç»„ä»¶å­¦ä¹ æœ‰æ•ˆç­–ç•¥ï¼Œä»è€Œåœ¨ä»»åŠ¡å®Œæˆç‡å’Œå¯¹è¯æ•ˆç‡ä¸Šè¶…è¶Šç°æœ‰çš„åŸºç¡€å’Œå•†ä¸šæ¨¡å‹ã€‚è¯¥æ¡†æ¶å°†ç”¨æˆ·å»ºæ¨¡åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šé…ç½®æ§åˆ¶å™¨ç”Ÿæˆå¤šæ ·åŒ–çš„ç”¨æˆ·çŠ¶æ€ä»¥ç®¡ç†è®­ç»ƒè¯¾ç¨‹ï¼Œè§’è‰²æ‰®æ¼”æ¨¡å‹åˆ™ä¸“æ³¨äºçœŸå®çš„è§’è‰²æ‰®æ¼”ã€‚é€šè¿‡è¿™ç§è®¾è®¡ï¼ŒSEADèƒ½å¤Ÿæä¾›é€‚åº”æ€§è®­ç»ƒåœºæ™¯ï¼Œè€Œä¸æ˜¯å……å½“ä¸å…¬å¹³çš„å¯¹æ‰‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSEADåœ¨ä»»åŠ¡å®Œæˆç‡ä¸Šæé«˜äº†17.6%ï¼Œå¯¹è¯æ•ˆç‡æé«˜äº†11.1%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.03075', 'title': 'ReMiT: RL-Guided Mid-Training for Iterative LLM Evolution', 'url': 'https://huggingface.co/papers/2602.03075', 'abstract': 'ReMiT introduces a bidirectional training approach where reinforcement learning-guided mid-training token reweighting improves large language model pre-training and post-training performance through an iterative feedback loop.  \t\t\t\t\tAI-generated summary \t\t\t\t Standard training pipelines for large language models (LLMs) are typically unidirectional, progressing from pre-training to post-training. However, the potential for a bidirectional process--where insights from post-training retroactively improve the pre-trained foundation--remains unexplored. We aim to establish a self-reinforcing flywheel: a cycle in which reinforcement learning (RL)-tuned model strengthens the base model, which in turn enhances subsequent post-training performance, requiring no specially trained teacher or reference model. To realize this, we analyze training dynamics and identify the mid-training (annealing) phase as a critical turning point for model capabilities. This phase typically occurs at the end of pre-training, utilizing high-quality corpora under a rapidly decaying learning rate. Building upon this insight, we introduce ReMiT (Reinforcement Learning-Guided Mid-Training). Specifically, ReMiT leverages the reasoning priors of RL-tuned models to dynamically reweight tokens during the mid-training phase, prioritizing those pivotal for reasoning. Empirically, ReMiT achieves an average improvement of 3\\% on 10 pre-training benchmarks, spanning math, code, and general reasoning, and sustains these gains by over 2\\% throughout the post-training pipeline. These results validate an iterative feedback loop, enabling continuous and self-reinforcing evolution of LLMs.', 'score': 2, 'issue_id': 968, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 3', 'zh': '2æœˆ3æ—¥'}, 'hash': 'd165243c4a9cbfbc', 'authors': ['Junjie Huang', 'Jiarui Qin', 'Di Yin', 'Weiwen Liu', 'Yong Yu', 'Xing Sun', 'Weinan Zhang'], 'affiliations': ['Shanghai Jiao Tong University', 'Tencent Youtu Lab'], 'pdf_title_img': 'assets/pdf/title_img/2602.03075.jpg', 'data': {'categories': ['#optimization', '#training', '#rl', '#reasoning'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² Ğ´Ğ²Ğ° Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ: ĞºĞ°Ğº Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'ReMiT Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ³Ğ´Ğµ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¿ĞµÑ€Ğµweighting Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ„Ğ°Ğ·Ğµ mid-training. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ„Ğ°Ğ·Ğ° Ğ¾Ñ‚Ğ¶Ğ¸Ğ³Ğ° Ğ² ĞºĞ¾Ğ½Ñ†Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸Ğ· RL-Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿ĞµÑ€Ğµweighting Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ²Ñ‹Ğ´ĞµĞ»ÑÑ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 3% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ±Ñ‹Ğ»ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 2% Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ÑĞ°Ğ¼Ğ¾ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ÑÑ Ñ†Ğ¸ĞºĞ» ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ LLM.'}, 'en': {'title': 'ReMiT: Enhancing LLMs with Bidirectional Training and Reinforcement Learning', 'desc': 'ReMiT presents a novel bidirectional training method that enhances large language models (LLMs) by integrating reinforcement learning (RL) during the mid-training phase. This approach allows insights gained from post-training to retroactively improve the pre-trained model, creating a self-reinforcing feedback loop. By dynamically reweighting tokens based on their importance for reasoning, ReMiT optimizes the training process without needing a separate teacher model. The results show significant performance improvements across various benchmarks, demonstrating the effectiveness of this iterative training strategy.'}, 'zh': {'title': 'åŒå‘è®­ç»ƒï¼ŒæŒç»­æå‡è¯­è¨€æ¨¡å‹æ€§èƒ½', 'desc': 'ReMiTæå‡ºäº†ä¸€ç§åŒå‘è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æŒ‡å¯¼çš„ä¸­æœŸè®­ç»ƒä»¤ç‰Œé‡åŠ æƒï¼Œæå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒå’Œåè®­ç»ƒæ€§èƒ½ã€‚ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒé€šå¸¸æ˜¯å•å‘çš„ï¼Œä»é¢„è®­ç»ƒåˆ°åè®­ç»ƒï¼Œè€ŒReMiTæ¢ç´¢äº†åŒå‘è¿‡ç¨‹çš„æ½œåŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†æè®­ç»ƒåŠ¨æ€ï¼Œè¯†åˆ«ä¸­æœŸè®­ç»ƒé˜¶æ®µä¸ºæ¨¡å‹èƒ½åŠ›çš„å…³é”®è½¬æŠ˜ç‚¹ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ è°ƒæ•´æ¨¡å‹åœ¨æ­¤é˜¶æ®µåŠ¨æ€é‡åŠ æƒä»¤ç‰Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReMiTåœ¨10ä¸ªé¢„è®­ç»ƒåŸºå‡†ä¸Šå¹³å‡æé«˜äº†3%ï¼Œå¹¶åœ¨åè®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒäº†è¶…è¿‡2%çš„å¢ç›Šï¼ŒéªŒè¯äº†è¿­ä»£åé¦ˆå¾ªç¯çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.06724', 'title': 'Table-as-Search: Formulate Long-Horizon Agentic Information Seeking as Table Completion', 'url': 'https://huggingface.co/papers/2602.06724', 'abstract': "Table-as-Search framework reformulates information seeking tasks as table completion problems, improving long-horizon search robustness through structured state management.  \t\t\t\t\tAI-generated summary \t\t\t\t Current Information Seeking (InfoSeeking) agents struggle to maintain focus and coherence during long-horizon exploration, as tracking search states, including planning procedure and massive search results, within one plain-text context is inherently fragile. To address this, we introduce Table-as-Search (TaS), a structured planning framework that reformulates the InfoSeeking task as a Table Completion task. TaS maps each query into a structured table schema maintained in an external database, where rows represent search candidates and columns denote constraints or required information. This table precisely manages the search states: filled cells strictly record the history and search results, while empty cells serve as an explicit search plan. Crucially, TaS unifies three distinct InfoSeeking tasks: Deep Search, Wide Search, and the challenging DeepWide Search. Extensive experiments demonstrate that TaS significantly outperforms numerous state-of-the-art baselines across three kinds of benchmarks, including multi-agent framework and commercial systems. Furthermore, our analysis validates the TaS's superior robustness in long-horizon InfoSeeking, alongside its efficiency, scalability and flexibility. Code and datasets are publicly released at https://github.com/AIDC-AI/Marco-Search-Agent.", 'score': 1, 'issue_id': 966, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 6', 'zh': '2æœˆ6æ—¥'}, 'hash': '058e980c028aa3ed', 'authors': ['Tian Lan', 'Felix Henry', 'Bin Zhu', 'Qianghuai Jia', 'Junyang Ren', 'Qihang Pu', 'Haijun Li', 'Longyue Wang', 'Zhao Xu', 'Weihua Luo'], 'affiliations': ['alibabainc.com'], 'pdf_title_img': 'assets/pdf/title_img/2602.06724.jpg', 'data': {'categories': ['#benchmark', '#agents', '#dataset'], 'emoji': 'ğŸ—‚ï¸', 'ru': {'title': 'Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° ĞºĞ°Ğº Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ‚Ğ¾Ñ€: ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¸Ñ… Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Table-as-Search (TaS) Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹, Ñ…Ñ€Ğ°Ğ½ÑÑ‰ÑƒÑÑÑ Ğ² Ğ±Ğ°Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ³Ğ´Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ° ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ñ‹ Ğ¾Ğ±Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ°ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµĞ¼ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Ğ—Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ğµ ÑÑ‡ĞµĞ¹ĞºĞ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ° Ğ¿ÑƒÑÑ‚Ñ‹Ğµ ÑÑ‡ĞµĞ¹ĞºĞ¸ ÑĞ»ÑƒĞ¶Ğ°Ñ‚ ÑĞ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ñ…. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ñ‚Ñ€Ñ‘Ñ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Revolutionizing Information Seeking with Table-as-Search', 'desc': 'The Table-as-Search (TaS) framework transforms information seeking tasks into table completion problems, enhancing the ability to manage long-term searches. By organizing search states in a structured table format, TaS allows for better tracking of queries and results, which improves coherence during exploration. Each table cell captures either search history or planned queries, providing a clear overview of the search process. Experimental results show that TaS outperforms existing methods in various benchmarks, demonstrating its robustness, efficiency, and scalability in handling complex information seeking tasks.'}, 'zh': {'title': 'è¡¨æ ¼æœç´¢ï¼šæå‡ä¿¡æ¯æ£€ç´¢çš„é²æ£’æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTable-as-Searchï¼ˆTaSï¼‰çš„æ¡†æ¶ï¼Œå°†ä¿¡æ¯æ£€ç´¢ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºè¡¨æ ¼è¡¥å…¨é—®é¢˜ï¼Œä»è€Œæé«˜äº†é•¿æ—¶é—´æœç´¢çš„é²æ£’æ€§ã€‚TaSé€šè¿‡å°†æ¯ä¸ªæŸ¥è¯¢æ˜ å°„åˆ°ä¸€ä¸ªå¤–éƒ¨æ•°æ®åº“ä¸­çš„ç»“æ„åŒ–è¡¨æ ¼æ¨¡å¼ï¼Œæ¥æœ‰æ•ˆç®¡ç†æœç´¢çŠ¶æ€ã€‚è¡¨æ ¼ä¸­çš„è¡Œè¡¨ç¤ºæœç´¢å€™é€‰é¡¹ï¼Œåˆ—åˆ™è¡¨ç¤ºçº¦æŸæˆ–æ‰€éœ€ä¿¡æ¯ï¼Œå¡«å……çš„å•å…ƒæ ¼è®°å½•å†å²å’Œæœç´¢ç»“æœï¼Œè€Œç©ºå•å…ƒæ ¼åˆ™ä½œä¸ºæ˜ç¡®çš„æœç´¢è®¡åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTaSåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç°äº†å…¶åœ¨é•¿æ—¶é—´ä¿¡æ¯æ£€ç´¢ä¸­çš„ä¼˜è¶Šæ€§ã€æ•ˆç‡ã€å¯æ‰©å±•æ€§å’Œçµæ´»æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.06139', 'title': 'EgoAVU: Egocentric Audio-Visual Understanding', 'url': 'https://huggingface.co/papers/2602.06139', 'abstract': 'Multi-modal large language models struggle to jointly understand audio and visual signals in egocentric videos, but a new scalable data engine and dataset significantly improve their performance through targeted fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding egocentric videos plays a vital role for embodied intelligence. Recent multi-modal large language models (MLLMs) can accept both visual and audio inputs. However, due to the challenge of obtaining text labels with coherent joint-modality information, whether MLLMs can jointly understand both modalities in egocentric videos remains under-explored. To address this problem, we introduce EgoAVU, a scalable data engine to automatically generate egocentric audio-visual narrations, questions, and answers. EgoAVU enriches human narrations with multimodal context and generates audio-visual narrations through cross-modal correlation modeling. Token-based video filtering and modular, graph-based curation ensure both data diversity and quality. Leveraging EgoAVU, we construct EgoAVU-Instruct, a large-scale training dataset of 3M samples, and EgoAVU-Bench, a manually verified evaluation split covering diverse tasks. EgoAVU-Bench clearly reveals the limitations of existing MLLMs: they bias heavily toward visual signals, often neglecting audio cues or failing to correspond audio with the visual source. Finetuning MLLMs on EgoAVU-Instruct effectively addresses this issue, enabling up to 113% performance improvement on EgoAVU-Bench. Such benefits also transfer to other benchmarks such as EgoTempo and EgoIllusion, achieving up to 28% relative performance gain. Code will be released to the community.', 'score': 1, 'issue_id': 962, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 5', 'zh': '2æœˆ5æ—¥'}, 'hash': '5b2f07457e4c1967', 'authors': ['Ashish Seth', 'Xinhao Mei', 'Changsheng Zhao', 'Varun Nagaraja', 'Ernie Chang', 'Gregory P. Meyer', 'Gael Le Lan', 'Yunyang Xiong', 'Vikas Chandra', 'Yangyang Shi', 'Dinesh Manocha', 'Zhipeng Cai'], 'affiliations': ['Meta', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2602.06139.jpg', 'data': {'categories': ['#audio', '#multimodal', '#benchmark', '#video', '#training', '#robotics', '#dataset'], 'emoji': 'ğŸ‘ï¸\u200dğŸ—¨ï¸', 'ru': {'title': 'Ğ“Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ‡Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ·Ğ²ÑƒĞºĞ° Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ…', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ EgoAVU â€” Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 3 Ğ¼Ğ»Ğ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾ 113%.'}, 'en': {'title': 'Enhancing Multi-Modal Understanding in Egocentric Videos', 'desc': 'This paper addresses the challenges faced by multi-modal large language models (MLLMs) in understanding both audio and visual signals in egocentric videos. The authors introduce EgoAVU, a scalable data engine that generates rich audio-visual narrations and questions, enhancing the training data for MLLMs. By fine-tuning these models on the newly created EgoAVU-Instruct dataset, significant performance improvements are achieved, particularly in tasks that require joint modality understanding. The study highlights the limitations of existing MLLMs and demonstrates how targeted training can enhance their ability to process and integrate audio and visual information effectively.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€ç†è§£ï¼Œçªç ´è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘çš„ç“¶é¢ˆ', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç†è§£è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ä¸­çš„éŸ³é¢‘å’Œè§†è§‰ä¿¡å·æ–¹é¢çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†EgoAVUï¼Œä¸€ä¸ªå¯æ‰©å±•çš„æ•°æ®å¼•æ“ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆè‡ªæˆ‘ä¸­å¿ƒçš„éŸ³é¢‘-è§†è§‰å™è¿°ã€é—®é¢˜å’Œç­”æ¡ˆã€‚é€šè¿‡äº¤å‰æ¨¡æ€å…³è”å»ºæ¨¡ï¼ŒEgoAVUä¸°å¯Œäº†äººç±»å™è¿°çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ï¼Œå¹¶ç¡®ä¿æ•°æ®çš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚å¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé’ˆå¯¹æ€§çš„å¾®è°ƒï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜å…¶åœ¨è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.06129', 'title': 'Urban Spatio-Temporal Foundation Models for Climate-Resilient Housing: Scaling Diffusion Transformers for Disaster Risk Prediction', 'url': 'https://huggingface.co/papers/2602.06129', 'abstract': 'A diffusion-transformer framework integrates spatio-temporal urban data to predict building-level climate risks while incorporating transportation network structures for emergency response applications.  \t\t\t\t\tAI-generated summary \t\t\t\t Climate hazards increasingly disrupt urban transportation and emergency-response operations by damaging housing stock, degrading infrastructure, and reducing network accessibility. This paper presents Skjold-DiT, a diffusion-transformer framework that integrates heterogeneous spatio-temporal urban data to forecast building-level climate-risk indicators while explicitly incorporating transportation-network structure and accessibility signals relevant to intelligent vehicles (e.g., emergency reachability and evacuation-route constraints). Concretely, Skjold-DiT enables hazard-conditioned routing constraints by producing calibrated, uncertainty-aware accessibility layers (reachability, travel-time inflation, and route redundancy) that can be consumed by intelligent-vehicle routing and emergency dispatch systems. Skjold-DiT combines: (1) Fjell-Prompt, a prompt-based conditioning interface designed to support cross-city transfer; (2) Norrland-Fusion, a cross-modal attention mechanism unifying hazard maps/imagery, building attributes, demographics, and transportation infrastructure into a shared latent representation; and (3) Valkyrie-Forecast, a counterfactual simulator for generating probabilistic risk trajectories under intervention prompts. We introduce the Baltic-Caspian Urban Resilience (BCUR) dataset with 847,392 building-level observations across six cities, including multi-hazard annotations (e.g., flood and heat indicators) and transportation accessibility features. Experiments evaluate prediction quality, cross-city generalization, calibration, and downstream transportation-relevant outcomes, including reachability and hazard-conditioned travel times under counterfactual interventions.', 'score': 1, 'issue_id': 965, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 5', 'zh': '2æœˆ5æ—¥'}, 'hash': 'f2f3cb493d6c1620', 'authors': ['Olaf Yunus Laitinen Imanov', 'Derya Umut Kulali', 'Taner Yilmaz'], 'affiliations': ['Department of Applied Mathematics and Computer Science (DTU Compute), Technical University of Denmark, Kongens Lyngby, Denmark', 'Department of Computer Engineering, Afyon Kocatepe University, Afyonkarahisar, TÃ¼rkiye', 'Department of Engineering, Eskisehir Technical University, Eskisehir, TÃ¼rkiye'], 'pdf_title_img': 'assets/pdf/title_img/2602.06129.jpg', 'data': {'categories': ['#graphs', '#open_source', '#benchmark', '#diffusion', '#dataset', '#science', '#transfer_learning', '#architecture', '#multimodal'], 'emoji': 'ğŸ¢', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ»Ğ¸Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ¸ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¾Ğ² ÑĞ²Ğ°ĞºÑƒĞ°Ñ†Ğ¸Ğ¸ Ğ² Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ°Ñ… Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ¼', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Skjold-DiT â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ĞºĞ»Ğ¸Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸, Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ ÑĞºÑÑ‚Ñ€ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ°Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ°Ğ¼Ğ¸, ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ² Ğ¾Ğ±Ñ‰ĞµĞµ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ€Ğ¸ÑĞºĞ¾Ğ². Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ BCUR Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ 847 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑˆĞµÑÑ‚Ğ¸ Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ°Ñ… Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°Ğ¼Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Predicting Climate Risks with Smart Urban Data Integration', 'desc': 'This paper introduces Skjold-DiT, a diffusion-transformer framework designed to predict climate risks at the building level by utilizing diverse urban data. It integrates transportation network structures to enhance emergency response capabilities, focusing on factors like accessibility and evacuation routes. The framework employs advanced techniques such as prompt-based conditioning and cross-modal attention to create a unified representation of various urban features. Additionally, it includes a counterfactual simulator to assess risk trajectories, supported by a comprehensive dataset of building observations across multiple cities.'}, 'zh': {'title': 'æ™ºèƒ½åº”æ€¥å“åº”çš„æ°”å€™é£é™©é¢„æµ‹æ–°æ¡†æ¶', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSkjold-DiTçš„æ‰©æ•£-å˜æ¢å™¨æ¡†æ¶ï¼Œæ—¨åœ¨æ•´åˆåŸå¸‚çš„æ—¶ç©ºæ•°æ®ï¼Œä»¥é¢„æµ‹å»ºç­‘çº§åˆ«çš„æ°”å€™é£é™©ã€‚è¯¥æ¡†æ¶ç‰¹åˆ«è€ƒè™‘äº†äº¤é€šç½‘ç»œç»“æ„ï¼Œä»¥ä¾¿åœ¨ç´§æ€¥å“åº”åº”ç”¨ä¸­æé«˜æ•ˆç‡ã€‚Skjold-DiTèƒ½å¤Ÿç”Ÿæˆç»è¿‡æ ¡å‡†çš„å¯è¾¾æ€§å±‚ï¼Œå¸®åŠ©æ™ºèƒ½è½¦è¾†è¿›è¡Œæ›´å¥½çš„è·¯å¾„è§„åˆ’å’Œåº”æ€¥è°ƒåº¦ã€‚ç ”ç©¶è¿˜å¼•å…¥äº†Baltic-CaspianåŸå¸‚éŸ§æ€§æ•°æ®é›†ï¼ŒåŒ…å«847,392ä¸ªå»ºç­‘è§‚å¯Ÿæ•°æ®ï¼Œæ”¯æŒå¤šç§æ°”å€™é£é™©çš„è¯„ä¼°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.04454', 'title': 'Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search', 'url': 'https://huggingface.co/papers/2602.04454', 'abstract': 'Seg-ReSearch introduces a novel segmentation approach that combines interleaved reasoning with external search to overcome limitations of frozen MLLM knowledge, using hierarchical reward design for training and demonstrating superior performance on video object segmentation benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Segmentation based on language has been a popular topic in computer vision. While recent advances in multimodal large language models (MLLMs) have endowed segmentation systems with reasoning capabilities, these efforts remain confined by the frozen internal knowledge of MLLMs, which limits their potential for real-world scenarios that involve up-to-date information or domain-specific concepts. In this work, we propose Seg-ReSearch, a novel segmentation paradigm that overcomes the knowledge bottleneck of existing approaches. By enabling interleaved reasoning and external search, Seg-ReSearch empowers segmentation systems to handle dynamic, open-world queries that extend beyond the frozen knowledge of MLLMs. To effectively train this capability, we introduce a hierarchical reward design that harmonizes initial guidance with progressive incentives, mitigating the dilemma between sparse outcome signals and rigid step-wise supervision. For evaluation, we construct OK-VOS, a challenging benchmark that explicitly requires outside knowledge for video object segmentation. Experiments on OK-VOS and two existing reasoning segmentation benchmarks demonstrate that our Seg-ReSearch improves state-of-the-art approaches by a substantial margin. Code and data will be released at https://github.com/iSEE-Laboratory/Seg-ReSearch.', 'score': 1, 'issue_id': 964, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 4', 'zh': '2æœˆ4æ—¥'}, 'hash': 'f339569c416d4d74', 'authors': ['Tianming Liang', 'Qirui Du', 'Jian-Fang Hu', 'Haichao Jiang', 'Zicheng Lin', 'Wei-Shi Zheng'], 'affiliations': ['ISEE Lab, Sun Yat-sen University'], 'pdf_title_img': 'assets/pdf/title_img/2602.04454.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#dataset', '#training', '#multimodal', '#rag', '#cv', '#benchmark', '#video'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ’Ñ‹Ñ…Ğ¾Ğ´ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹: ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Seg-ReSearch Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼ ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¾Ñ€Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº OK-VOS Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° OK-VOS Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Empowering Video Segmentation with Dynamic Reasoning and External Knowledge', 'desc': "Seg-ReSearch presents a new method for video object segmentation that integrates interleaved reasoning with external search capabilities. This approach addresses the limitations of traditional multimodal large language models (MLLMs) that rely on outdated, frozen knowledge. By implementing a hierarchical reward design during training, Seg-ReSearch effectively balances initial guidance with ongoing incentives, enhancing the model's adaptability to dynamic queries. The method shows significant improvements on video segmentation benchmarks, demonstrating its effectiveness in real-world applications that require current and specific information."}, 'zh': {'title': 'Seg-ReSearchï¼šçªç ´çŸ¥è¯†ç“¶é¢ˆçš„åˆ†å‰²æ–°æ–¹æ³•', 'desc': 'Seg-ReSearchæå‡ºäº†ä¸€ç§æ–°é¢–çš„åˆ†å‰²æ–¹æ³•ï¼Œç»“åˆäº†äº¤é”™æ¨ç†å’Œå¤–éƒ¨æœç´¢ï¼Œä»¥å…‹æœå†»ç»“çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çŸ¥è¯†çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å±‚æ¬¡å¥–åŠ±è®¾è®¡è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿå¤„ç†åŠ¨æ€çš„å¼€æ”¾ä¸–ç•ŒæŸ¥è¯¢ï¼Œè¶…è¶Šäº†MLLMçš„å›ºå®šçŸ¥è¯†ã€‚æˆ‘ä»¬æ„å»ºäº†OK-VOSåŸºå‡†ï¼Œæ˜ç¡®è¦æ±‚å¤–éƒ¨çŸ¥è¯†ç”¨äºè§†é¢‘ç‰©ä½“åˆ†å‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSeg-ReSearchåœ¨å¤šä¸ªåŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.02581', 'title': 'QuantLRM: Quantization of Large Reasoning Models via Fine-Tuning Signals', 'url': 'https://huggingface.co/papers/2602.02581', 'abstract': 'QuantLRM uses weight update magnitude signals from fine-tuning to improve quantization of Large Reasoning Models, achieving better performance than traditional methods through channel importance estimation.  \t\t\t\t\tAI-generated summary \t\t\t\t Weight-only quantization is important for compressing Large Language Models (LLMs). Inspired by the spirit of classical magnitude pruning, we study whether the magnitude of weight updates during reasoning-incentivized fine-tuning can provide valuable signals for quantizing Large Reasoning Models (LRMs). We hypothesize that the smallest and largest weight updates during fine-tuning are more important than those of intermediate magnitude, a phenomenon we term "protecting both ends". Upon hypothesis validation, we introduce QuantLRM, which stands for weight quantization of LRMs via fine-tuning signals. We fit simple restricted quadratic functions on weight updates to protect both ends. By multiplying the average quadratic values with the count of zero weight updates of channels, we compute channel importance that is more effective than using activation or second-order information. We run QuantLRM to quantize various fine-tuned models (including supervised, direct preference optimization, and reinforcement learning fine-tuning) over four reasoning benchmarks (AIME-120, FOLIO, temporal sequences, and GPQA-Diamond) and empirically find that QuantLRM delivers a consistent improvement for LRMs quantization, with an average improvement of 6.55% on a reinforcement learning fine-tuned model. Also supporting non-fine-tuned LRMs, QuantLRM gathers effective signals via pseudo-fine-tuning, which greatly enhances its applicability.', 'score': 1, 'issue_id': 965, 'pub_date': '2026-01-31', 'pub_date_card': {'ru': '31 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 31', 'zh': '1æœˆ31æ—¥'}, 'hash': 'a21d4b9f7f84c154', 'authors': ['Nan Zhang', 'Eugene Kwek', 'Yusen Zhang', 'Muyu Pan', 'Suhang Wang', 'Prasenjit Mitra', 'Rui Zhang'], 'affiliations': ['IBM Research', 'Pennsylvania State University'], 'pdf_title_img': 'assets/pdf/title_img/2602.02581.jpg', 'data': {'categories': ['#optimization', '#inference', '#benchmark', '#training', '#reasoning'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ğ±Ğ¾Ğ¸Ñ… ĞºĞ¾Ğ½Ñ†Ğ¾Ğ²: ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ fine-tuning', 'desc': 'QuantLRM â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ½Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ²ĞµÑĞ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ fine-tuning Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸ Ğ½Ğ°Ğ¸Ğ¼ĞµĞ½ĞµĞµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ğ°Ğ¶Ğ½Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ²ĞµÑĞ¾Ğ² Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğ½Ğ°Ğ»Ğ° Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ñ… Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾, Ñ‡ĞµĞ¼ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ°. QuantLRM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 6.55% Ğ¿Ñ€Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ reinforcement learning fine-tuning Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ñ Ğ½ĞµĞ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· pseudo-fine-tuning.'}, 'en': {'title': 'Enhancing LRM Quantization with Fine-Tuning Signals', 'desc': 'QuantLRM is a novel approach that enhances the quantization of Large Reasoning Models (LRMs) by utilizing weight update signals from fine-tuning. It operates on the principle that the magnitude of weight updates during fine-tuning can indicate the importance of different channels, particularly focusing on the extremes of these updates. By applying a quadratic function to these weight updates, QuantLRM effectively estimates channel importance, leading to better performance than traditional quantization methods. The method has been validated across various reasoning benchmarks, showing an average performance improvement of 6.55% for reinforcement learning fine-tuned models.'}, 'zh': {'title': 'é‡åŒ–å¤§å‹æ¨ç†æ¨¡å‹çš„æ–°æ–¹æ³•', 'desc': 'QuantLRMæ˜¯ä¸€ç§é€šè¿‡å¾®è°ƒä¿¡å·æ¥æ”¹è¿›å¤§å‹æ¨ç†æ¨¡å‹é‡åŒ–çš„æ–¹æ³•ã€‚å®ƒåˆ©ç”¨æƒé‡æ›´æ–°çš„å¹…åº¦ä¿¡å·ï¼Œä¼°è®¡é€šé“çš„é‡è¦æ€§ï¼Œä»è€Œå®ç°æ¯”ä¼ ç»Ÿæ–¹æ³•æ›´å¥½çš„æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†â€œä¿æŠ¤ä¸¤ç«¯â€çš„å‡è®¾ï¼Œè®¤ä¸ºå¾®è°ƒè¿‡ç¨‹ä¸­æœ€å°å’Œæœ€å¤§æƒé‡æ›´æ–°æ¯”ä¸­é—´å€¼æ›´é‡è¦ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼ŒQuantLRMåœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šå®ç°äº†é‡åŒ–çš„ä¸€è‡´æ€§æå‡ï¼Œå¹³å‡æé«˜äº†6.55%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.23039', 'title': 'Avoiding Premature Collapse: Adaptive Annealing for Entropy-Regularized Structural Inference', 'url': 'https://huggingface.co/papers/2601.23039', 'abstract': 'Researchers identify and address premature mode collapse in optimal transport-based structural prediction models through an adaptive stability control algorithm that prevents gradient explosions during large-scale training.  \t\t\t\t\tAI-generated summary \t\t\t\t Differentiable matching layers and residual connection paradigms, often implemented via entropy-regularized Optimal Transport (OT), serve as critical mechanisms in structural prediction and architectural scaling. However, recovering discrete permutations or maintaining identity mappings via annealing Îµto 0 is notoriously unstable. In this work, we identify a fundamental mechanism for this failure: Premature Mode Collapse. By analyzing the non-normal dynamics of the Sinkhorn fixed-point map, we reveal a theoretical thermodynamic speed limit: standard exponential cooling outpaces the contraction rate of the inference operator, which degrades as O(1/Îµ). To address this, we propose Efficient Piecewise Hybrid Adaptive Stability Control (EPH-ASC), an adaptive scheduling algorithm that monitors the stability of the inference process. We demonstrate that EPH-ASC is essential for stabilizing Manifold-Constrained Hyper-Connections (mHC) during large-scale training on the FineWeb-Edu dataset, effectively preventing late-stage gradient explosions by enforcing a linear stability law.', 'score': 1, 'issue_id': 963, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 30', 'zh': '1æœˆ30æ—¥'}, 'hash': '0710cf59390fcf1d', 'authors': ['Yizhi Liu'], 'affiliations': ['Department of Computer Science, Stony Brook University'], 'pdf_title_img': 'assets/pdf/title_img/2601.23039.jpg', 'data': {'categories': ['#optimization'], 'emoji': 'âš¡', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾Ğ² Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğµ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿Ñ€ĞµĞ¶Ğ´ĞµĞ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° Ğ¼Ğ¾Ğ´ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° Ğ¡Ğ¸Ğ½Ñ…Ğ¾Ñ€Ğ½Ğ° Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ: ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ñ…Ğ»Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ, Ñ‡ĞµĞ¼ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ EPH-ASC, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ²Ğ·Ñ€Ñ‹Ğ²Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° FineWeb-Edu, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Stabilizing Structural Prediction with Adaptive Control', 'desc': 'This paper tackles the issue of premature mode collapse in optimal transport-based structural prediction models, which can lead to instability during training. The authors introduce an adaptive stability control algorithm called Efficient Piecewise Hybrid Adaptive Stability Control (EPH-ASC) to prevent gradient explosions. By analyzing the dynamics of the Sinkhorn fixed-point map, they uncover a speed limit that contributes to the instability. The proposed method is shown to effectively stabilize the training process, particularly in large-scale applications like the FineWeb-Edu dataset.'}, 'zh': {'title': 'é˜²æ­¢æ—©æœŸæ¨¡å¼å´©æºƒçš„è‡ªé€‚åº”ç¨³å®šæ€§æ§åˆ¶', 'desc': 'æœ¬ç ”ç©¶é’ˆå¯¹åŸºäºæœ€ä¼˜ä¼ è¾“çš„ç»“æ„é¢„æµ‹æ¨¡å‹ä¸­çš„æ—©æœŸæ¨¡å¼å´©æºƒé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è‡ªé€‚åº”ç¨³å®šæ€§æ§åˆ¶ç®—æ³•ã€‚è¯¥ç®—æ³•èƒ½å¤Ÿåœ¨å¤§è§„æ¨¡è®­ç»ƒè¿‡ç¨‹ä¸­é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œç¡®ä¿æ¨¡å‹çš„ç¨³å®šæ€§ã€‚æˆ‘ä»¬åˆ†æäº†Sinkhornå›ºå®šç‚¹æ˜ å°„çš„éæ­£å¸¸åŠ¨æ€ï¼Œæ­ç¤ºäº†ä¸€ä¸ªç†è®ºçƒ­åŠ›å­¦é€Ÿåº¦æé™ï¼ŒæŒ‡å‡ºæ ‡å‡†çš„æŒ‡æ•°å†·å´é€Ÿåº¦è¶…è¿‡äº†æ¨ç†ç®—å­çš„æ”¶ç¼©é€Ÿç‡ã€‚é€šè¿‡å¼•å…¥é«˜æ•ˆåˆ†æ®µæ··åˆè‡ªé€‚åº”ç¨³å®šæ€§æ§åˆ¶ï¼ˆEPH-ASCï¼‰ï¼Œæˆ‘ä»¬æˆåŠŸåœ°åœ¨FineWeb-Eduæ•°æ®é›†ä¸Šç¨³å®šäº†æµå½¢çº¦æŸè¶…è¿æ¥ï¼ˆmHCï¼‰ï¼Œæœ‰æ•ˆé¿å…äº†åæœŸçš„æ¢¯åº¦çˆ†ç‚¸ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.06883', 'title': 'Vision Transformer Finetuning Benefits from Non-Smooth Components', 'url': 'https://huggingface.co/papers/2602.06883', 'abstract': 'Vision transformer components exhibit varying plasticity levels that correlate with finetuning performance, challenging the assumption that smoothness is always beneficial.  \t\t\t\t\tAI-generated summary \t\t\t\t The smoothness of the transformer architecture has been extensively studied in the context of generalization, training stability, and adversarial robustness. However, its role in transfer learning remains poorly understood. In this paper, we analyze the ability of vision transformer components to adapt their outputs to changes in inputs, or, in other words, their plasticity. Defined as an average rate of change, it captures the sensitivity to input perturbation; in particular, a high plasticity implies low smoothness. We demonstrate through theoretical analysis and comprehensive experiments that this perspective provides principled guidance in choosing the components to prioritize during adaptation. A key takeaway for practitioners is that the high plasticity of the attention modules and feedforward layers consistently leads to better finetuning performance. Our findings depart from the prevailing assumption that smoothness is desirable, offering a novel perspective on the functional properties of transformers. The code is available at https://github.com/ambroiseodt/vit-plasticity.', 'score': 0, 'issue_id': 964, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 6', 'zh': '2æœˆ6æ—¥'}, 'hash': 'd40bc4a7cb7a82da', 'authors': ['Ambroise Odonnat', 'Laetitia Chapel', 'Romain Tavenard', 'Ievgen Redko'], 'affiliations': ["Noah's Ark"], 'pdf_title_img': 'assets/pdf/title_img/2602.06883.jpg', 'data': {'categories': ['#open_source', '#transfer_learning', '#interpretability'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞŸĞ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ Ğ³Ğ»Ğ°Ğ´ĞºĞ¾ÑÑ‚Ğ¸: ĞºĞ»ÑÑ‡ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ğ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Vision Transformer â€” Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸĞ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ÑÑ ĞºĞ°Ğº ÑÑ€ĞµĞ´Ğ½ÑÑ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸ÑĞ¼ Ğ²Ñ…Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ Ğ¿Ğ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ»Ğ¾Ñ‘Ğ² Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ°Ñ‚ Ğ¾Ğ±Ñ‰ĞµĞ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾ Ğ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ³Ğ»Ğ°Ğ´ĞºĞ¾ÑÑ‚Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¸Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'High Plasticity, Better Performance: Rethinking Transformer Components', 'desc': 'This paper investigates how different parts of vision transformers respond to changes in input, a property known as plasticity. It challenges the common belief that smoother components always lead to better performance in machine learning tasks. The authors find that components with high plasticity, like attention modules and feedforward layers, actually improve finetuning results. This research provides new insights into how to select transformer components for better adaptation in transfer learning scenarios.'}, 'zh': {'title': 'é«˜å¯å¡‘æ€§åŠ©åŠ›è§†è§‰å˜æ¢å™¨å¾®è°ƒæ€§èƒ½', 'desc': 'æœ¬æ–‡æ¢è®¨äº†è§†è§‰å˜æ¢å™¨ï¼ˆVision Transformerï¼‰ç»„ä»¶çš„å¯å¡‘æ€§ä¸å¾®è°ƒæ€§èƒ½ä¹‹é—´çš„å…³ç³»ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç»„ä»¶çš„é«˜å¯å¡‘æ€§é€šå¸¸æ„å‘³ç€ä½å¹³æ»‘æ€§ï¼Œè¿™ä¸ä¼ ç»Ÿè§‚ç‚¹ç›¸æ‚–ã€‚é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒï¼Œæˆ‘ä»¬å‘ç°å…³æ³¨æ¨¡å—å’Œå‰é¦ˆå±‚çš„é«˜å¯å¡‘æ€§èƒ½å¤Ÿæ˜¾è‘—æå‡å¾®è°ƒæ•ˆæœã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºé€‰æ‹©é€‚åˆçš„ç»„ä»¶æä¾›äº†æ–°çš„æŒ‡å¯¼æ€è·¯ï¼ŒæŒ‘æˆ˜äº†å¹³æ»‘æ€§æ€»æ˜¯æœ‰ç›Šçš„å‡è®¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01064', 'title': 'Exploring Knowledge Purification in Multi-Teacher Knowledge Distillation for LLMs', 'url': 'https://huggingface.co/papers/2602.01064', 'abstract': 'Knowledge purification techniques consolidate rationales from multiple teacher LLMs to reduce conflicts and improve efficiency in distillation processes.  \t\t\t\t\tAI-generated summary \t\t\t\t Knowledge distillation has emerged as a pivotal technique for transferring knowledge from stronger large language models (LLMs) to smaller, more efficient models. However, traditional distillation approaches face challenges related to knowledge conflicts and high resource demands, particularly when leveraging multiple teacher models. In this paper, we introduce the concept of Knowledge Purification, which consolidates the rationales from multiple teacher LLMs into a single rationale, thereby mitigating conflicts and enhancing efficiency. To investigate the effectiveness of knowledge purification, we further propose five purification methods from various perspectives. Our experiments demonstrate that these methods not only improve the performance of the distilled model but also effectively alleviate knowledge conflicts. Moreover, router-based methods exhibit robust generalization capabilities, underscoring the potential of innovative purification techniques in optimizing multi-teacher distillation and facilitating the practical deployment of powerful yet lightweight models.', 'score': 0, 'issue_id': 962, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 1', 'zh': '2æœˆ1æ—¥'}, 'hash': 'cb1b1f412f81bede', 'authors': ['Ruihan Jin', 'Pengpeng Shao', 'Zhengqi Wen', 'Jinyang Wu', 'Mingkuan Feng', 'Shuo Yang', 'Chu Yuan Zhang', 'Jianhua Tao'], 'affiliations': ['Beijing National Research Center for Information Science and Technology', 'Department of Automation, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2602.01064.jpg', 'data': {'categories': ['#transfer_learning', '#small_models', '#training'], 'emoji': 'ğŸ§¹', 'ru': {'title': 'ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ñ… ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… LLM Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ¿ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ñ… Ğº Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ².'}, 'en': {'title': 'Streamlining Knowledge Transfer with Purification Techniques', 'desc': 'This paper presents Knowledge Purification, a novel approach to enhance knowledge distillation from multiple teacher large language models (LLMs). By consolidating rationales from these teachers, the method reduces conflicts that typically arise during the distillation process. The authors propose five distinct purification methods to evaluate their effectiveness in improving the performance of smaller models. Results show that these techniques not only boost model efficiency but also enhance generalization, making them valuable for deploying lightweight models in practical applications.'}, 'zh': {'title': 'çŸ¥è¯†å‡€åŒ–ï¼šæå‡è’¸é¦æ•ˆç‡çš„åˆ›æ–°æ–¹æ³•', 'desc': 'çŸ¥è¯†å‡€åŒ–æŠ€æœ¯é€šè¿‡æ•´åˆå¤šä¸ªæ•™å¸ˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†ï¼Œå‡å°‘äº†çŸ¥è¯†å†²çªå¹¶æé«˜äº†è’¸é¦è¿‡ç¨‹çš„æ•ˆç‡ã€‚ä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦æ–¹æ³•åœ¨ä½¿ç”¨å¤šä¸ªæ•™å¸ˆæ¨¡å‹æ—¶é¢ä¸´çŸ¥è¯†å†²çªå’Œèµ„æºéœ€æ±‚é«˜çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†çŸ¥è¯†å‡€åŒ–çš„æ¦‚å¿µï¼Œå°†å¤šä¸ªæ•™å¸ˆLLMsçš„æ¨ç†æ•´åˆä¸ºå•ä¸€æ¨ç†ï¼Œä»è€Œç¼“è§£å†²çªå¹¶æå‡æ•ˆç‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™äº›å‡€åŒ–æ–¹æ³•ä¸ä»…æé«˜äº†è’¸é¦æ¨¡å‹çš„æ€§èƒ½ï¼Œè¿˜æœ‰æ•ˆå‡è½»äº†çŸ¥è¯†å†²çªã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (6)', '#agi', '#alignment (2)', '#architecture (5)', '#audio (2)', '#benchmark (10)', '#cv (3)', '#data (1)', '#dataset (8)', '#diffusion (1)', '#ethics', '#games', '#graphs (1)', '#hallucinations (1)', '#healthcare', '#inference (4)', '#interpretability (2)', '#leakage', '#long_context (2)', '#low_resource', '#machine_translation (1)', '#math (2)', '#multilingual (1)', '#multimodal (7)', '#open_source (8)', '#optimization (10)', '#plp (1)', '#rag (1)', '#reasoning (10)', '#rl (8)', '#rlhf (1)', '#robotics (2)', '#science (3)', '#security (1)', '#small_models (2)', '#story_generation', '#survey (1)', '#synthetic (1)', '#training (20)', '#transfer_learning (5)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2026-02-09 11:41',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2026-02-09 11:41')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2026-02-09 11:41')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    