
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 21 papers. February 28.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">28 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ</span> | <span id="title-articles-count">21 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-02-27.html">â¬…ï¸ <span id="prev-date">27.02</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-03-03.html">â¡ï¸ <span id="next-date">03.03</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-02.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '28 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 28', 'zh': '2æœˆ28æ—¥'};
        let feedDateNext = {'ru': '03.03', 'en': '03/03', 'zh': '3æœˆ3æ—¥'};
        let feedDatePrev = {'ru': '27.02', 'en': '02/27', 'zh': '2æœˆ27æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2502.19613', 'title': 'Self-rewarding correction for mathematical reasoning', 'url': 'https://huggingface.co/papers/2502.19613', 'abstract': "We study self-rewarding reasoning large language models (LLMs), which can simultaneously generate step-by-step reasoning and evaluate the correctness of their outputs during the inference time-without external feedback. This integrated approach allows a single model to independently guide its reasoning process, offering computational advantages for model deployment. We particularly focus on the representative task of self-correction, where models autonomously detect errors in their responses, revise outputs, and decide when to terminate iterative refinement loops. To enable this, we propose a two-staged algorithmic framework for constructing self-rewarding reasoning models using only self-generated data. In the first stage, we employ sequential rejection sampling to synthesize long chain-of-thought trajectories that incorporate both self-rewarding and self-correction mechanisms. Fine-tuning models on these curated data allows them to learn the patterns of self-rewarding and self-correction. In the second stage, we further enhance the models' ability to assess response accuracy and refine outputs through reinforcement learning with rule-based signals. Experiments with Llama-3 and Qwen-2.5 demonstrate that our approach surpasses intrinsic self-correction capabilities and achieves performance comparable to systems that rely on external reward models.", 'score': 50, 'issue_id': 2455, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 26', 'zh': '2æœˆ26æ—¥'}, 'hash': 'e2535efc8aadcc9d', 'authors': ['Wei Xiong', 'Hanning Zhang', 'Chenlu Ye', 'Lichang Chen', 'Nan Jiang', 'Tong Zhang'], 'affiliations': ['University of Illinois Urbana-Champaign', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2502.19613.jpg', 'data': {'categories': ['#reasoning', '#training', '#inference', '#optimization', '#rl'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸ĞµÑÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ÑĞ°Ğ¼Ğ¾Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ°Ğ¼Ğ¾Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ° Ñ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ ÑĞ°Ğ¼Ğ¾Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸. Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ¿ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Empowering LLMs with Self-Rewarding Reasoning and Self-Correction', 'desc': "This paper explores self-rewarding reasoning in large language models (LLMs), enabling them to generate and evaluate their own reasoning without needing outside feedback. The focus is on self-correction, where models can identify and fix their mistakes independently. The authors introduce a two-stage framework that first uses sequential rejection sampling to create data for training the models on self-rewarding and self-correction. The second stage enhances the models' accuracy assessment and output refinement through reinforcement learning, showing that their method outperforms traditional self-correction techniques."}, 'zh': {'title': 'è‡ªæˆ‘å¥–åŠ±æ¨ç†ï¼šæ¨¡å‹çš„ç‹¬ç«‹æ€è€ƒä¸ä¿®æ­£', 'desc': 'æˆ‘ä»¬ç ”ç©¶äº†è‡ªæˆ‘å¥–åŠ±æ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŒæ—¶ç”Ÿæˆé€æ­¥æ¨ç†å¹¶è¯„ä¼°è¾“å‡ºçš„æ­£ç¡®æ€§ï¼Œè€Œæ— éœ€å¤–éƒ¨åé¦ˆã€‚è¿™ç§é›†æˆæ–¹æ³•ä½¿å¾—å•ä¸€æ¨¡å‹èƒ½å¤Ÿç‹¬ç«‹å¼•å¯¼å…¶æ¨ç†è¿‡ç¨‹ï¼Œä¸ºæ¨¡å‹éƒ¨ç½²æä¾›äº†è®¡ç®—ä¼˜åŠ¿ã€‚æˆ‘ä»¬ç‰¹åˆ«å…³æ³¨è‡ªæˆ‘ä¿®æ­£çš„ä»»åŠ¡ï¼Œæ¨¡å‹èƒ½å¤Ÿè‡ªä¸»æ£€æµ‹å“åº”ä¸­çš„é”™è¯¯ï¼Œä¿®æ­£è¾“å‡ºï¼Œå¹¶å†³å®šä½•æ—¶ç»ˆæ­¢è¿­ä»£ä¼˜åŒ–å¾ªç¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„ç®—æ³•æ¡†æ¶ï¼Œåˆ©ç”¨è‡ªç”Ÿæˆçš„æ•°æ®æ„å»ºè‡ªæˆ‘å¥–åŠ±æ¨ç†æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.19634', 'title': 'MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2502.19634', 'abstract': 'Reasoning is a critical frontier for advancing medical image analysis, where transparency and trustworthiness play a central role in both clinician trust and regulatory approval. Although Medical Visual Language Models (VLMs) show promise for radiological tasks, most existing VLMs merely produce final answers without revealing the underlying reasoning. To address this gap, we introduce MedVLM-R1, a medical VLM that explicitly generates natural language reasoning to enhance transparency and trustworthiness. Instead of relying on supervised fine-tuning (SFT), which often suffers from overfitting to training distributions and fails to foster genuine reasoning, MedVLM-R1 employs a reinforcement learning framework that incentivizes the model to discover human-interpretable reasoning paths without using any reasoning references. Despite limited training data (600 visual question answering samples) and model parameters (2B), MedVLM-R1 boosts accuracy from 55.11% to 78.22% across MRI, CT, and X-ray benchmarks, outperforming larger models trained on over a million samples. It also demonstrates robust domain generalization under out-of-distribution tasks. By unifying medical image analysis with explicit reasoning, MedVLM-R1 marks a pivotal step toward trustworthy and interpretable AI in clinical practice.', 'score': 43, 'issue_id': 2463, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 26', 'zh': '2æœˆ26æ—¥'}, 'hash': 'dffe0cc2daf1f23e', 'authors': ['Jiazhen Pan', 'Che Liu', 'Junde Wu', 'Fenglin Liu', 'Jiayuan Zhu', 'Hongwei Bran Li', 'Chen Chen', 'Cheng Ouyang', 'Daniel Rueckert'], 'affiliations': ['Chair for AI in Healthcare and Medicine, Technical University of Munich (TUM) and TUM University Hospital, Germany', 'Data Science Institute, Imperial College London, UK', 'Department of Computing, Imperial College London, UK', 'Department of Engineering Science, University of Oxford, UK', 'Massachusetts General Hospital, Harvard Medical School, USA', 'School of Computer Science, University of Sheffield, UK'], 'pdf_title_img': 'assets/pdf/title_img/2502.19634.jpg', 'data': {'categories': ['#reasoning', '#interpretability', '#rl', '#training', '#healthcare'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸: Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‡Ñ‘Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‰Ğ¸ĞºĞ°', 'desc': 'MedVLM-R1 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¸Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ² Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, MedVLM-R1 Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞœĞ Ğ¢, ĞšĞ¢ Ğ¸ Ñ€ĞµĞ½Ñ‚Ğ³ĞµĞ½Ğ¾Ğ²ÑĞºĞ¸Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜ Ğ´Ğ»Ñ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ¸.'}, 'en': {'title': 'Enhancing Trust in Medical AI with Transparent Reasoning', 'desc': 'This paper presents MedVLM-R1, a Medical Visual Language Model designed to improve reasoning transparency in medical image analysis. Unlike traditional models that only provide final answers, MedVLM-R1 generates natural language explanations for its decisions, enhancing trust among clinicians. It utilizes a reinforcement learning approach to develop reasoning paths without relying on extensive training data or references, which helps avoid overfitting. The model shows significant performance improvements on various imaging benchmarks, indicating its potential for reliable and interpretable AI in healthcare.'}, 'zh': {'title': 'åŒ»å­¦å½±åƒåˆ†æä¸­çš„é€æ˜æ¨ç†æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹MedVLM-R1ï¼Œæ—¨åœ¨æé«˜åŒ»å­¦å›¾åƒåˆ†æä¸­çš„é€æ˜åº¦å’Œå¯ä¿¡åº¦ã€‚ä¸ä¼ ç»Ÿæ¨¡å‹ä¸åŒï¼ŒMedVLM-R1èƒ½å¤Ÿç”Ÿæˆè‡ªç„¶è¯­è¨€æ¨ç†ï¼Œå¸®åŠ©åŒ»ç”Ÿç†è§£æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè€Œä¸æ˜¯ç›‘ç£å¾®è°ƒï¼Œé¿å…äº†è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå¹¶èƒ½å‘ç°å¯è¢«äººç±»ç†è§£çš„æ¨ç†è·¯å¾„ã€‚å°½ç®¡è®­ç»ƒæ•°æ®æœ‰é™ï¼ŒMedVLM-R1åœ¨å¤šä¸ªåŒ»å­¦å½±åƒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå‡†ç¡®ç‡æ˜¾è‘—æé«˜ï¼Œæ ‡å¿—ç€å¯ä¿¡èµ–çš„ä¸´åºŠäººå·¥æ™ºèƒ½çš„é‡è¦è¿›å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.20395', 'title': 'R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts', 'url': 'https://huggingface.co/papers/2502.20395', 'abstract': 'In large multimodal models (LMMs), the perception of non-language modalities (e.g., visual representations) is usually not on par with the large language models (LLMs)\' powerful reasoning capabilities, deterring LMMs\' performance on challenging downstream tasks. This weakness has been recently mitigated by replacing the vision encoder with a mixture-of-experts (MoE), which provides rich, multi-granularity, and diverse representations required by diverse downstream tasks. The performance of multimodal MoE largely depends on its router, which reweights and mixes the representations of different experts for each input. However, we find that the end-to-end trained router does not always produce the optimal routing weights for every test sample. To bridge the gap, we propose a novel and efficient method "Re-Routing in Test-Time(R2-T2) that locally optimizes the vector of routing weights in test-time by moving it toward those vectors of the correctly predicted samples in a neighborhood of the test sample. We propose three R2-T2 strategies with different optimization objectives and neighbor-search spaces. R2-T2 consistently and greatly improves state-of-the-art LMMs\' performance on challenging benchmarks of diverse tasks, without training any base-model parameters.', 'score': 33, 'issue_id': 2456, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 27', 'zh': '2æœˆ27æ—¥'}, 'hash': 'e8862deee761c4d0', 'authors': ['Zhongyang Li', 'Ziyue Li', 'Tianyi Zhou'], 'affiliations': ['Johns Hopkins University', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2502.20395.jpg', 'data': {'categories': ['#multimodal', '#training', '#architecture', '#optimization', '#benchmark'], 'emoji': 'ğŸ”€', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ»ĞµÑ‚Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Re-Routing in Test-Time (R2-T2) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (MoE). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñƒ end-to-end, Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²ĞµÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². R2-T2 Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµĞºÑ‚Ğ¾Ñ€ Ğ²ĞµÑĞ¾Ğ² Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°Ñ ĞµĞ³Ğ¾ Ğº Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ñ‚Ñ€Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ R2-T2 Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ»ÑĞ¼Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑĞ¾ÑĞµĞ´ĞµĞ¹.'}, 'en': {'title': 'Optimizing Multimodal Performance with Test-Time Re-Routing', 'desc': 'This paper addresses the performance gap in large multimodal models (LMMs) when processing non-language data compared to large language models (LLMs). The authors introduce a mixture-of-experts (MoE) approach to enhance the vision encoder, allowing for richer and more diverse representations. They identify that the router, which determines how to mix these expert representations, often fails to optimize routing weights effectively during testing. To solve this, they propose a method called Re-Routing in Test-Time (R2-T2), which fine-tunes routing weights based on nearby correctly predicted samples, significantly boosting the performance of LMMs on various challenging tasks without retraining the base model.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•', 'desc': 'åœ¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ä¸­ï¼Œè§†è§‰è¡¨ç¤ºçš„æ„ŸçŸ¥èƒ½åŠ›é€šå¸¸ä¸å¦‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™å½±å“äº†LMMsåœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æœ€è¿‘ï¼Œé€šè¿‡ç”¨ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ›¿æ¢è§†è§‰ç¼–ç å™¨ï¼Œç¼“è§£äº†è¿™ä¸€å¼±ç‚¹ï¼Œæä¾›äº†ä¸°å¯Œä¸”å¤šæ ·çš„è¡¨ç¤ºã€‚æˆ‘ä»¬å‘ç°ï¼Œç«¯åˆ°ç«¯è®­ç»ƒçš„è·¯ç”±å™¨å¹¶ä¸æ€»èƒ½ä¸ºæ¯ä¸ªæµ‹è¯•æ ·æœ¬ç”Ÿæˆæœ€ä½³çš„è·¯ç”±æƒé‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–ä¸”é«˜æ•ˆçš„æ–¹æ³•â€œæµ‹è¯•æ—¶é‡æ–°è·¯ç”±ï¼ˆR2-T2ï¼‰â€ï¼Œé€šè¿‡åœ¨æµ‹è¯•æ—¶ä¼˜åŒ–è·¯ç”±æƒé‡å‘é‡ï¼Œæ˜¾è‘—æå‡äº†LMMsåœ¨å¤šæ ·åŒ–ä»»åŠ¡åŸºå‡†ä¸Šçš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.20082', 'title': 'LongRoPE2: Near-Lossless LLM Context Window Scaling', 'url': 'https://huggingface.co/papers/2502.20082', 'abstract': 'LongRoPE2 is a novel approach that extends the effective context window of pre-trained large language models (LLMs) to the target length, while preserving the performance on the original shorter context window. This is achieved by three contributions: (1) a hypothesis that insufficient training in higher RoPE dimensions contributes to the persistent out-of-distribution (OOD) issues observed in existing methods; (2) an effective RoPE rescaling algorithm that adopts evolutionary search guided by "needle-driven" perplexity to address the insufficient training problem; (3) a mixed context window training approach that fine-tunes model weights to adopt rescaled RoPE for long-context sequences while preserving the short-context performance with the original RoPE. Extensive experiments on LLaMA3-8B and Phi3-mini-3.8B across various benchmarks validate the hypothesis and demonstrate the effectiveness of LongRoPE2. Remarkably, LongRoPE2 extends LLaMA3-8B to achieve a 128K effective context length while retaining over 98.5% of short-context performance, using only 10B tokens -- 80x fewer than Meta\'s approach, which fails to reach the target effective context length. Code will be available at https://github.com/microsoft/LongRoPE.', 'score': 21, 'issue_id': 2456, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 27', 'zh': '2æœˆ27æ—¥'}, 'hash': 'ee15387b2b27d4c6', 'authors': ['Ning Shang', 'Li Lyna Zhang', 'Siyuan Wang', 'Gaokai Zhang', 'Gilsinia Lopez', 'Fan Yang', 'Weizhu Chen', 'Mao Yang'], 'affiliations': ['Microsoft', 'Shanghai Jiao Tong University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.20082.jpg', 'data': {'categories': ['#training', '#architecture', '#benchmark', '#long_context'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'LongRoPE2 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ¾ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ğµ Ğ¾ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ² Ğ²Ñ‹ÑÑˆĞ¸Ñ… Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑÑ… RoPE Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ RoPE. LongRoPE2 Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ LongRoPE2 Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ LLaMA3-8B Ğ´Ğ¾ 128 Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'Extending Context Length Without Compromise', 'desc': 'LongRoPE2 is a new method that enhances the context length of large language models (LLMs) while maintaining their performance on shorter contexts. It introduces a hypothesis that inadequate training in higher dimensions of RoPE leads to out-of-distribution issues in existing models. The method employs a RoPE rescaling algorithm that uses evolutionary search to improve training effectiveness. Additionally, it utilizes a mixed context window training strategy to fine-tune model weights, allowing for long-context sequences without sacrificing short-context performance.'}, 'zh': {'title': 'æ‰©å±•ä¸Šä¸‹æ–‡çª—å£ï¼Œä¿æŒæ€§èƒ½çš„åˆ›æ–°æ–¹æ³•', 'desc': 'LongRoPE2æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æ‰©å±•é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æœ‰æ•ˆä¸Šä¸‹æ–‡çª—å£ï¼ŒåŒæ—¶ä¿æŒåœ¨åŸå§‹è¾ƒçŸ­ä¸Šä¸‹æ–‡çª—å£ä¸Šçš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸‰ä¸ªè´¡çŒ®å®ç°ï¼šé¦–å…ˆï¼Œæå‡ºäº†ä¸€ä¸ªå‡è®¾ï¼Œè®¤ä¸ºåœ¨æ›´é«˜RoPEç»´åº¦ä¸Šçš„è®­ç»ƒä¸è¶³å¯¼è‡´äº†ç°æœ‰æ–¹æ³•ä¸­æŒç»­å­˜åœ¨çš„åˆ†å¸ƒå¤–ï¼ˆOODï¼‰é—®é¢˜ï¼›å…¶æ¬¡ï¼Œæå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„RoPEé‡ç¼©æ”¾ç®—æ³•ï¼Œé€šè¿‡â€œé’ˆé©±åŠ¨â€çš„å›°æƒ‘åº¦æŒ‡å¯¼çš„è¿›åŒ–æœç´¢æ¥è§£å†³è®­ç»ƒä¸è¶³çš„é—®é¢˜ï¼›æœ€åï¼Œé‡‡ç”¨æ··åˆä¸Šä¸‹æ–‡çª—å£è®­ç»ƒæ–¹æ³•ï¼Œå¾®è°ƒæ¨¡å‹æƒé‡ä»¥é€‚åº”é•¿ä¸Šä¸‹æ–‡åºåˆ—çš„é‡ç¼©æ”¾RoPEï¼ŒåŒæ—¶ä¿æŒçŸ­ä¸Šä¸‹æ–‡çš„åŸå§‹RoPEæ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.20238', 'title': "FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through Reflective Puzzle Solving", 'url': 'https://huggingface.co/papers/2502.20238', 'abstract': 'Many challenging reasoning tasks require not just rapid, intuitive responses, but a more deliberate, multi-step approach. Recent progress in large language models (LLMs) highlights an important shift from the "System 1" way of quick reactions to the "System 2" style of reflection-and-correction problem solving. However, current benchmarks heavily rely on the final-answer accuracy, leaving much of a model\'s intermediate reasoning steps unexamined. This fails to assess the model\'s ability to reflect and rectify mistakes within the reasoning process. To bridge this gap, we introduce FINEREASON, a logic-puzzle benchmark for fine-grained evaluation of LLMs\' reasoning capabilities. Each puzzle can be decomposed into atomic steps, making it ideal for rigorous validation of intermediate correctness. Building on this, we introduce two tasks: state checking, and state transition, for a comprehensive evaluation of how models assess the current situation and plan the next move. To support broader research, we also provide a puzzle training set aimed at enhancing performance on general mathematical tasks. We show that models trained on our state checking and transition data demonstrate gains in math reasoning by up to 5.1% on GSM8K.', 'score': 19, 'issue_id': 2458, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 27', 'zh': '2æœˆ27æ—¥'}, 'hash': 'e121721fdef71315', 'authors': ['Guizhen Chen', 'Weiwen Xu', 'Hao Zhang', 'Hou Pong Chan', 'Chaoqun Liu', 'Lidong Bing', 'Deli Zhao', 'Anh Tuan Luu', 'Yu Rong'], 'affiliations': ['DAMO Academy, Alibaba Group, Singapore', 'Nanyang Technological University, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2502.20238.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#dataset', '#math'], 'emoji': 'ğŸ§©', 'ru': {'title': 'FINEREASON: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº FINEREASON Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², FINEREASON Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Reasoning in Language Models with FINEREASON', 'desc': "This paper introduces FINEREASON, a new benchmark designed to evaluate the reasoning capabilities of large language models (LLMs) through logic puzzles. Unlike traditional benchmarks that focus solely on final-answer accuracy, FINEREASON emphasizes the importance of intermediate reasoning steps, allowing for a more detailed assessment of a model's reflective and corrective abilities. The benchmark includes two specific tasks: state checking and state transition, which help evaluate how models understand their current context and plan their next actions. The authors demonstrate that training on this benchmark can improve LLM performance in mathematical reasoning tasks by up to 5.1%."}, 'zh': {'title': 'æå‡æ¨ç†èƒ½åŠ›çš„ç»†è‡´è¯„ä¼°', 'desc': 'è®¸å¤šå¤æ‚çš„æ¨ç†ä»»åŠ¡éœ€è¦ä¸ä»…å¿«é€Ÿçš„ç›´è§‰ååº”ï¼Œè¿˜éœ€è¦æ›´æ·±æ€ç†Ÿè™‘çš„å¤šæ­¥éª¤æ–¹æ³•ã€‚æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›å±•æ˜¾ç¤ºï¼Œä»å¿«é€Ÿååº”çš„â€œç³»ç»Ÿ1â€è½¬å‘åæ€å’Œçº æ­£é—®é¢˜è§£å†³çš„â€œç³»ç»Ÿ2â€é£æ ¼æ˜¯ä¸€ä¸ªé‡è¦çš„å˜åŒ–ã€‚å½“å‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦ä¾èµ–æœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼Œå¿½è§†äº†æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„ä¸­é—´æ­¥éª¤ï¼Œè¿™æ— æ³•è¯„ä¼°æ¨¡å‹åæ€å’Œçº æ­£é”™è¯¯çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FINEREASONï¼Œè¿™æ˜¯ä¸€ä¸ªé€»è¾‘éš¾é¢˜åŸºå‡†ï¼Œç”¨äºç»†è‡´è¯„ä¼°LLMsçš„æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.16645', 'title': 'CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale', 'url': 'https://huggingface.co/papers/2502.16645', 'abstract': "Large Language Models (LLMs) have exhibited exceptional performance in software engineering yet face challenges in adapting to continually evolving code knowledge, particularly regarding the frequent updates of third-party library APIs. This limitation, stemming from static pre-training datasets, often results in non-executable code or implementations with suboptimal safety and efficiency. To this end, this paper introduces CODESYNC, a data engine for identifying outdated code patterns and collecting real-time code knowledge updates from Python third-party libraries. Building upon CODESYNC, we develop CODESYNCBENCH, a comprehensive benchmark for assessing LLMs' ability to stay synchronized with code evolution, which covers real-world updates for 220 APIs from six Python libraries. Our benchmark offers 3,300 test cases across three evaluation tasks and an update-aware instruction tuning dataset consisting of 2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs reveal that they struggle with dynamic code evolution, even with the support of advanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe that our benchmark can offer a strong foundation for the development of more effective methods for real-time code knowledge updating in the future. The experimental code and dataset are publicly available at: https://github.com/Lucky-voyage/Code-Sync.", 'score': 15, 'issue_id': 2456, 'pub_date': '2025-02-23', 'pub_date_card': {'ru': '23 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 23', 'zh': '2æœˆ23æ—¥'}, 'hash': '616cf3f6f2ab1d17', 'authors': ['Chenlong Wang', 'Zhaoyang Chu', 'Zhengxiang Cheng', 'Xuyi Yang', 'Kaiyue Qiu', 'Yao Wan', 'Zhou Zhao', 'Xuanhua Shi', 'Dongping Chen'], 'affiliations': ['Huazhong University of Science and Technology', 'Wuhuan University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.16645.jpg', 'data': {'categories': ['#dataset', '#open_source', '#data', '#optimization', '#benchmark'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸ĞµĞ¹ ĞºĞ¾Ğ´Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CODESYNC - Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²ÑˆĞ¸Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² ĞºĞ¾Ğ´Ğ° Ğ¸ ÑĞ±Ğ¾Ñ€Ğ° Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ ĞºĞ¾Ğ´Ğµ Ğ¸Ğ· ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ñ… Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞº Python Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ CODESYNC Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ CODESYNCBENCH - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (LLM) ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸ĞµĞ¹ ĞºĞ¾Ğ´Ğ°, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ 220 API Ğ¸Ğ· ÑˆĞµÑÑ‚Ğ¸ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞº Python. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 14 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸ĞµĞ¹ ĞºĞ¾Ğ´Ğ° Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑ‚Ğ°Ñ‚ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ ĞºĞ¾Ğ´Ğµ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸.'}, 'en': {'title': 'CODESYNC: Keeping Code Knowledge Fresh for LLMs', 'desc': "This paper addresses the limitations of Large Language Models (LLMs) in adapting to changes in third-party library APIs, which can lead to outdated or inefficient code. It introduces CODESYNC, a data engine designed to identify outdated code patterns and gather real-time updates from Python libraries. Additionally, the authors present CODESYNCBENCH, a benchmark for evaluating LLMs' performance in keeping up with code evolution, featuring 3,300 test cases across various tasks. The findings indicate that even advanced LLMs struggle with dynamic code changes, highlighting the need for improved methods for real-time code knowledge updating."}, 'zh': {'title': 'å®æ—¶ä»£ç çŸ¥è¯†æ›´æ–°çš„åŸºå‡†æµ‹è¯•', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è½¯ä»¶å·¥ç¨‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é€‚åº”ä¸æ–­å˜åŒ–çš„ä»£ç çŸ¥è¯†æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ç¬¬ä¸‰æ–¹åº“APIçš„é¢‘ç¹æ›´æ–°ã€‚ç”±äºé™æ€çš„é¢„è®­ç»ƒæ•°æ®é›†ï¼Œè¿™ç§é™åˆ¶å¸¸å¸¸å¯¼è‡´ç”Ÿæˆçš„ä»£ç æ— æ³•æ‰§è¡Œæˆ–å®ç°çš„å®‰å…¨æ€§å’Œæ•ˆç‡ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†CODESYNCï¼Œä¸€ä¸ªç”¨äºè¯†åˆ«è¿‡æ—¶ä»£ç æ¨¡å¼å¹¶æ”¶é›†æ¥è‡ªPythonç¬¬ä¸‰æ–¹åº“çš„å®æ—¶ä»£ç çŸ¥è¯†æ›´æ–°çš„æ•°æ®å¼•æ“ã€‚åŸºäºCODESYNCï¼Œæˆ‘ä»¬å¼€å‘äº†CODESYNCBENCHï¼Œä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LLMsåœ¨ä»£ç æ¼”å˜ä¸­çš„åŒæ­¥èƒ½åŠ›ï¼Œæ¶µç›–äº†æ¥è‡ªå…­ä¸ªPythonåº“çš„220ä¸ªAPIçš„çœŸå®æ›´æ–°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.20321', 'title': 'UniTok: A Unified Tokenizer for Visual Generation and Understanding', 'url': 'https://huggingface.co/papers/2502.20321', 'abstract': 'The representation disparity between visual generation and understanding imposes a critical gap in integrating these capabilities into a single framework. To bridge this gap, we introduce UniTok, a discrete visual tokenizer that encodes fine-grained details for generation while also capturing high-level semantics for understanding. Despite recent studies have shown that these objectives could induce loss conflicts in training, we reveal that the underlying bottleneck stems from limited representational capacity of discrete tokens. We address this by introducing multi-codebook quantization, which divides vector quantization with several independent sub-codebooks to expand the latent feature space, while avoiding training instability caused by overlarge codebooks. Our method significantly raises the upper limit of unified discrete tokenizers to match or even surpass domain-specific continuous tokenizers. For instance, UniTok achieves a remarkable rFID of 0.38 (versus 0.87 for SD-VAE) and a zero-shot accuracy of 78.6% (versus 76.2% for CLIP) on ImageNet. Our code is available at https://github.com/FoundationVision/UniTok.', 'score': 13, 'issue_id': 2457, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 27', 'zh': '2æœˆ27æ—¥'}, 'hash': '1e65564c1594ef39', 'authors': ['Chuofan Ma', 'Yi Jiang', 'Junfeng Wu', 'Jihan Yang', 'Xin Yu', 'Zehuan Yuan', 'Bingyue Peng', 'Xiaojuan Qi'], 'affiliations': ['ByteDance Inc.', 'Huazhong University of Science and Technology', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.20321.jpg', 'data': {'categories': ['#training', '#architecture', '#multimodal', '#cv', '#optimization'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'UniTok: Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'UniTok - ÑÑ‚Ğ¾ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ĞºĞ¾Ğ´Ğ¾Ğ²ÑƒÑ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. UniTok Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ rFID 0.38 Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ zero-shot ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ 78.6% Ğ½Ğ° ImageNet. Ğ­Ñ‚Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Bridging Visual Generation and Understanding with UniTok', 'desc': 'This paper presents UniTok, a novel discrete visual tokenizer designed to improve the integration of visual generation and understanding. It addresses the challenges of representation disparity by using multi-codebook quantization, which enhances the capacity of discrete tokens without causing training instability. The method allows for better encoding of both fine-grained details and high-level semantics, leading to improved performance metrics. UniTok outperforms existing models, achieving higher accuracy and fidelity on tasks like image classification compared to traditional continuous tokenizers.'}, 'zh': {'title': 'UniTokï¼šç»Ÿä¸€è§†è§‰ç”Ÿæˆä¸ç†è§£çš„çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºUniTokçš„ç¦»æ•£è§†è§‰æ ‡è®°å™¨ï¼Œæ—¨åœ¨è§£å†³è§†è§‰ç”Ÿæˆä¸ç†è§£ä¹‹é—´çš„è¡¨ç¤ºå·®å¼‚ã€‚UniTokèƒ½å¤Ÿç¼–ç ç»†ç²’åº¦çš„ç»†èŠ‚ä»¥è¿›è¡Œç”Ÿæˆï¼ŒåŒæ—¶æ•æ‰é«˜å±‚æ¬¡çš„è¯­ä¹‰ä»¥ä¾¿äºç†è§£ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥å¤šä»£ç æœ¬é‡åŒ–æ¥æ‰©å±•æ½œåœ¨ç‰¹å¾ç©ºé—´ï¼Œä»è€Œå…‹æœç¦»æ•£æ ‡è®°çš„è¡¨ç¤ºèƒ½åŠ›é™åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniTokåœ¨ImageNetä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†é¢†åŸŸç‰¹å®šçš„è¿ç»­æ ‡è®°å™¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.19587', 'title': 'NeoBERT: A Next-Generation BERT', 'url': 'https://huggingface.co/papers/2502.19587', 'abstract': 'Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.', 'score': 10, 'issue_id': 2461, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 26', 'zh': '2æœˆ26æ—¥'}, 'hash': '7233167cf96a00e2', 'authors': ['Lola Le Breton', 'Quentin Fournier', 'Mariam El Mezouar', 'Sarath Chandar'], 'affiliations': ['Canada CIFAR AI Chair', 'Chandar Research Lab', 'Mila Quebec AI Institute', 'Polytechnique MontrÃ©al', 'Royal Military College of Canada'], 'pdf_title_img': 'assets/pdf/title_img/2502.19587.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#architecture', '#long_context', '#open_source', '#training'], 'emoji': 'ğŸš€', 'ru': {'title': 'NeoBERT: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'NeoBERT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑÑ… Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ² 250 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², NeoBERT Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MTEB, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ³Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ¼ĞµĞ½Ğ¾Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° MTEB.'}, 'en': {'title': 'NeoBERT: Redefining Bidirectional Models for Superior NLP Performance', 'desc': 'This paper introduces NeoBERT, a new encoder model that enhances the capabilities of bidirectional models in natural language processing. Unlike previous models like BERT and RoBERTa, NeoBERT incorporates advanced architectural innovations and optimized pre-training techniques to improve performance. It is designed to be easily integrated into existing systems, featuring a compact size of 250 million parameters while supporting a longer context of 4,096 tokens. NeoBERT achieves state-of-the-art results on the MTEB benchmark, demonstrating its effectiveness compared to other leading models under the same fine-tuning conditions.'}, 'zh': {'title': 'NeoBERTï¼šåŒå‘ç¼–ç å™¨çš„æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†NeoBERTï¼Œè¿™æ˜¯ä¸€ç§æ–°ä¸€ä»£çš„åŒå‘ç¼–ç å™¨ï¼Œæ—¨åœ¨æå‡è‡ªç„¶è¯­è¨€å¤„ç†çš„èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„BERTå’ŒRoBERTaç›¸æ¯”ï¼ŒNeoBERTç»“åˆäº†æœ€æ–°çš„æ¶æ„ã€ç°ä»£æ•°æ®å’Œä¼˜åŒ–çš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚å®ƒå…·æœ‰250Må‚æ•°ï¼Œèƒ½å¤Ÿåœ¨MTEBåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šå¤šä¸ªç°æœ‰æ¨¡å‹ï¼Œä¸”æ˜“äºæ›¿æ¢ç°æœ‰åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜æä¾›äº†æ‰€æœ‰ä»£ç ã€æ•°æ®å’Œè®­ç»ƒè„šæœ¬ï¼Œä»¥ä¿ƒè¿›ç ”ç©¶å’Œå®é™…åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.20172', 'title': 'Multimodal Representation Alignment for Image Generation: Text-Image Interleaved Control Is Easier Than You Think', 'url': 'https://huggingface.co/papers/2502.20172', 'abstract': 'The field of advanced text-to-image generation is witnessing the emergence of unified frameworks that integrate powerful text encoders, such as CLIP and T5, with Diffusion Transformer backbones. Although there have been efforts to control output images with additional conditions, like canny and depth map, a comprehensive framework for arbitrary text-image interleaved control is still lacking. This gap is especially evident when attempting to merge concepts or visual elements from multiple images in the generation process. To mitigate the gap, we conducted preliminary experiments showing that large multimodal models (LMMs) offer an effective shared representation space, where image and text can be well-aligned to serve as a condition for external diffusion models. Based on this discovery, we propose Dream Engine, an efficient and unified framework designed for arbitrary text-image interleaved control in image generation models. Building on powerful text-to-image models like SD3.5, we replace the original text-only encoders by incorporating versatile multimodal information encoders such as QwenVL. Our approach utilizes a two-stage training paradigm, consisting of joint text-image alignment and multimodal interleaved instruction tuning. Our experiments demonstrate that this training method is effective, achieving a 0.69 overall score on the GenEval benchmark, and matching the performance of state-of-the-art text-to-image models like SD3.5 and FLUX.', 'score': 9, 'issue_id': 2461, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 27', 'zh': '2æœˆ27æ—¥'}, 'hash': 'de63394c60041b93', 'authors': ['Liang Chen', 'Shuai Bai', 'Wenhao Chai', 'Weichu Xie', 'Haozhe Zhao', 'Leon Vinci', 'Junyang Lin', 'Baobao Chang'], 'affiliations': ['Alibaba Group', 'Bainance Labs', 'Beijing Institute of Technology', 'Peking University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2502.20172.jpg', 'data': {'categories': ['#benchmark', '#diffusion', '#multimodal', '#cv', '#training'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Dream Engine - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº QwenVL, Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Dream Engine Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ GenEval, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ.'}, 'en': {'title': 'Dream Engine: Unifying Text and Image Generation for Enhanced Control', 'desc': 'This paper introduces Dream Engine, a new framework for generating images from text that allows for flexible control over the output. It combines advanced text encoders with Diffusion Transformer models to improve the alignment between text and images. The authors highlight the importance of using large multimodal models to create a shared representation space for better integration of multiple concepts. Their two-stage training method shows promising results, achieving competitive performance on established benchmarks.'}, 'zh': {'title': 'Dream Engineï¼šå®ç°ä»»æ„æ–‡æœ¬ä¸å›¾åƒçš„äº¤é”™æ§åˆ¶', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸï¼Œæå‡ºäº†ä¸€ç§åä¸ºDream Engineçš„ç»Ÿä¸€æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¼ºå¤§çš„æ–‡æœ¬ç¼–ç å™¨å’Œæ‰©æ•£å˜æ¢å™¨ï¼Œæ—¨åœ¨å®ç°ä»»æ„æ–‡æœ¬ä¸å›¾åƒçš„äº¤é”™æ§åˆ¶ã€‚é€šè¿‡åˆæ­¥å®éªŒï¼Œæˆ‘ä»¬å‘ç°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹èƒ½å¤Ÿæä¾›æœ‰æ•ˆçš„å…±äº«è¡¨ç¤ºç©ºé—´ï¼Œä½¿å›¾åƒå’Œæ–‡æœ¬èƒ½å¤Ÿè‰¯å¥½å¯¹é½ã€‚æˆ‘ä»¬çš„è®­ç»ƒæ–¹æ³•ç»è¿‡éªŒè¯ï¼Œè¾¾åˆ°äº†GenEvalåŸºå‡†çš„0.69åˆ†ï¼Œè¡¨ç°ä¸æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ç›¸å½“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.16944', 'title': 'Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance', 'url': 'https://huggingface.co/papers/2502.16944', 'abstract': 'Proximal Policy Optimization (PPO)-based Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human preferences. It requires joint training of an actor and critic with a pretrained, fixed reward model for guidance. This approach increases computational complexity and instability due to actor-critic interdependence. Additionally, PPO lacks access to true environment rewards in LLM tasks, limiting its adaptability. Under such conditions, pretraining a value model or a reward model becomes equivalent, as both provide fixed supervisory signals without new ground-truth feedback. To address these issues, we propose Decoupled Value Policy Optimization (DVPO), a lean framework that replaces traditional reward modeling with a pretrained global value model (GVM). The GVM is conditioned on policy trajectories and predicts token-level return-to-go estimates. By decoupling value model from policy training (via frozen GVM-driven RL objectives), DVPO eliminates actor-critic interdependence, reducing GPU memory usage by 40\\% and training time by 35\\% compared to conventional RLHF. Experiments across benchmarks show DVPO outperforms efficient RLHF methods (e.g., DPO) while matching state-of-the-art PPO in performance.', 'score': 9, 'issue_id': 2459, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': '5eba083b71966ca8', 'authors': ['Chenghua Huang', 'Lu Wang', 'Fangkai Yang', 'Pu Zhao', 'Zhixu Li', 'Qingwei Lin', 'Dongmei Zhang', 'Saravan Rajmohan', 'Qi Zhang'], 'affiliations': ['Microsoft', 'School of Computer Science, Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2502.16944.jpg', 'data': {'categories': ['#alignment', '#rl', '#training', '#rlhf', '#optimization'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'DVPO: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ°ĞºÑ‚Ğ¾Ñ€Ğ° Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Decoupled Value Policy Optimization (DVPO). DVPO Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ (GVM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ return-to-go Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ°ĞºÑ‚Ğ¾Ñ€Ğ° Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ RLHF. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DVPO Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ RLHF Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ PPO Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Decoupling for Efficiency: DVPO Revolutionizes RLHF', 'desc': 'This paper introduces Decoupled Value Policy Optimization (DVPO), a new framework for improving Reinforcement Learning from Human Feedback (RLHF) in large language models. DVPO uses a pretrained global value model (GVM) to provide fixed supervisory signals, which helps to decouple the value model from policy training. This decoupling reduces the computational complexity and instability associated with traditional actor-critic methods, leading to significant reductions in GPU memory usage and training time. Experimental results demonstrate that DVPO not only outperforms existing efficient RLHF methods but also achieves performance comparable to state-of-the-art Proximal Policy Optimization (PPO).'}, 'zh': {'title': 'è§£è€¦å€¼ç­–ç•¥ä¼˜åŒ–ï¼šæå‡å¼ºåŒ–å­¦ä¹ æ•ˆç‡çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç§°ä¸ºè§£è€¦å€¼ç­–ç•¥ä¼˜åŒ–ï¼ˆDVPOï¼‰ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿçš„åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸­çš„ä¸€äº›é—®é¢˜ã€‚DVPOä½¿ç”¨é¢„è®­ç»ƒçš„å…¨å±€ä»·å€¼æ¨¡å‹ï¼ˆGVMï¼‰ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„å¥–åŠ±æ¨¡å‹ï¼Œä»è€Œå‡å°‘äº†æ¼”å‘˜å’Œè¯„è®ºå®¶ä¹‹é—´çš„ç›¸äº’ä¾èµ–ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒDVPOæ˜¾è‘—é™ä½äº†GPUå†…å­˜ä½¿ç”¨é‡å’Œè®­ç»ƒæ—¶é—´ï¼ŒåŒæ—¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„é«˜æ•ˆå¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæä¾›äº†æ›´ç¨³å®šå’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.20126', 'title': 'FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality Samples with Less Compute', 'url': 'https://huggingface.co/papers/2502.20126', 'abstract': 'Despite their remarkable performance, modern Diffusion Transformers are hindered by substantial resource requirements during inference, stemming from the fixed and large amount of compute needed for each denoising step. In this work, we revisit the conventional static paradigm that allocates a fixed compute budget per denoising iteration and propose a dynamic strategy instead. Our simple and sample-efficient framework enables pre-trained DiT models to be converted into flexible ones -- dubbed FlexiDiT -- allowing them to process inputs at varying compute budgets. We demonstrate how a single flexible model can generate images without any drop in quality, while reducing the required FLOPs by more than 40\\% compared to their static counterparts, for both class-conditioned and text-conditioned image generation. Our method is general and agnostic to input and conditioning modalities. We show how our approach can be readily extended for video generation, where FlexiDiT models generate samples with up to 75\\% less compute without compromising performance.', 'score': 9, 'issue_id': 2457, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 27', 'zh': '2æœˆ27æ—¥'}, 'hash': '405098fcd7a76209', 'authors': ['Sotiris Anagnostidis', 'Gregor Bachmann', 'Yeongmin Kim', 'Jonas Kohler', 'Markos Georgopoulos', 'Artsiom Sanakoyeu', 'Yuming Du', 'Albert Pumarola', 'Ali Thabet', 'Edgar SchÃ¶nfeld'], 'affiliations': ['ETH Zurich', 'KAIST', 'Meta GenAI'], 'pdf_title_img': 'assets/pdf/title_img/2502.20126.jpg', 'data': {'categories': ['#diffusion', '#cv', '#inference', '#video', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'FlexiDiT: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² (DiT). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ FlexiDiT, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ DiT Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ·Ğ°Ğ¿ÑÑ‚Ğ¾Ğ¹ (FLOP) Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 40% Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ“Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ ĞµĞ³Ğ¾ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ³Ğ´Ğµ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 75%.'}, 'en': {'title': 'Flexibility in Diffusion: Reducing Compute with FlexiDiT', 'desc': 'This paper addresses the high resource demands of Diffusion Transformers during image generation. The authors introduce a dynamic strategy that allows for flexible compute allocation during the denoising process, leading to the development of FlexiDiT models. These models can adapt to varying compute budgets while maintaining image quality, achieving over 40% reduction in FLOPs compared to traditional static models. Additionally, the approach is versatile and can be applied to video generation, achieving up to 75% less compute usage without sacrificing performance.'}, 'zh': {'title': 'çµæ´»è®¡ç®—ï¼Œæå‡ç”Ÿæˆæ•ˆç‡', 'desc': 'ç°ä»£æ‰©æ•£å˜æ¢å™¨åœ¨æ¨ç†æ—¶éœ€è¦å¤§é‡èµ„æºï¼Œä¸»è¦æ˜¯å› ä¸ºæ¯ä¸ªå»å™ªæ­¥éª¤éƒ½éœ€è¦å›ºå®šä¸”å¤§é‡çš„è®¡ç®—ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŠ¨æ€ç­–ç•¥ï¼Œå–ä»£ä¼ ç»Ÿçš„é™æ€è®¡ç®—é¢„ç®—åˆ†é…æ–¹æ³•ï¼Œä½¿å¾—é¢„è®­ç»ƒçš„æ‰©æ•£å˜æ¢å™¨æ¨¡å‹ï¼ˆDiTï¼‰èƒ½å¤Ÿçµæ´»å¤„ç†ä¸åŒçš„è®¡ç®—é¢„ç®—ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç§°ä¸ºFlexiDiTï¼Œèƒ½å¤Ÿåœ¨ä¸é™ä½ç”Ÿæˆå›¾åƒè´¨é‡çš„æƒ…å†µä¸‹ï¼Œå‡å°‘è¶…è¿‡40%çš„è®¡ç®—é‡ã€‚è¯¥æ–¹æ³•é€šç”¨ä¸”ä¸ä¾èµ–äºè¾“å…¥å’Œæ¡ä»¶æ¨¡å¼ï¼Œç”šè‡³å¯ä»¥æ‰©å±•åˆ°è§†é¢‘ç”Ÿæˆï¼Œæ˜¾è‘—é™ä½è®¡ç®—éœ€æ±‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.16750', 'title': 'Guardians of the Agentic System: Preventing Many Shots Jailbreak with Agentic System', 'url': 'https://huggingface.co/papers/2502.16750', 'abstract': 'The autonomous AI agents using large language models can create undeniable values in all span of the society but they face security threats from adversaries that warrants immediate protective solutions because trust and safety issues arise. Considering the many-shot jailbreaking and deceptive alignment as some of the main advanced attacks, that cannot be mitigated by the static guardrails used during the supervised training, points out a crucial research priority for real world robustness. The combination of static guardrails in dynamic multi-agent system fails to defend against those attacks. We intend to enhance security for LLM-based agents through the development of new evaluation frameworks which identify and counter threats for safe operational deployment. Our work uses three examination methods to detect rogue agents through a Reverse Turing Test and analyze deceptive alignment through multi-agent simulations and develops an anti-jailbreaking system by testing it with GEMINI 1.5 pro and llama-3.3-70B, deepseek r1 models using tool-mediated adversarial scenarios. The detection capabilities are strong such as 94\\% accuracy for GEMINI 1.5 pro yet the system suffers persistent vulnerabilities when under long attacks as prompt length increases attack success rates (ASR) and diversity metrics become ineffective in prediction while revealing multiple complex system faults. The findings demonstrate the necessity of adopting flexible security systems based on active monitoring that can be performed by the agents themselves together with adaptable interventions by system admin as the current models can create vulnerabilities that can lead to the unreliable and vulnerable system. So, in our work, we try to address such situations and propose a comprehensive framework to counteract the security issues.', 'score': 7, 'issue_id': 2469, 'pub_date': '2025-02-23', 'pub_date_card': {'ru': '23 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 23', 'zh': '2æœˆ23æ—¥'}, 'hash': 'e6f9253bc3e3a120', 'authors': ['Saikat Barua', 'Mostafizur Rahman', 'Md Jafor Sadek', 'Rafiul Islam', 'Shehnaz Khaled', 'Ahmedul Kabir'], 'affiliations': ['North South University, Dhaka', 'University of Dhaka, Dhaka'], 'pdf_title_img': 'assets/pdf/title_img/2502.16750.jpg', 'data': {'categories': ['#security', '#agents', '#inference', '#benchmark'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ£ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğµ Ğ¾Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑƒĞ³Ñ€Ğ¾Ğ·', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ğ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ ÑƒĞ³Ñ€Ğ¾Ğ·Ğ°Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ¢ÑŒÑÑ€Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¾Ğ±Ğ¼Ğ°Ğ½Ñ‡Ğ¸Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğ°, Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒÑ ĞµĞµ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… GEMINI 1.5 pro, llama-3.3-70B Ğ¸ deepseek r1. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ³Ğ¸Ğ±ĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ¾Ğ¼ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾Ğ¼ Ğ°Ğ´Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Security for Autonomous AI Agents Against Advanced Threats', 'desc': 'This paper discusses the security challenges faced by autonomous AI agents that use large language models (LLMs). It highlights the inadequacy of static guardrails against advanced attacks like many-shot jailbreaking and deceptive alignment, which threaten the trust and safety of these systems. The authors propose new evaluation frameworks to enhance the security of LLM-based agents, employing methods such as Reverse Turing Tests and multi-agent simulations to detect and counteract threats. Their findings indicate a need for flexible security systems that can adapt to ongoing attacks, ensuring reliable and safe operational deployment of AI agents.'}, 'zh': {'title': 'å¢å¼ºè‡ªä¸»AIä»£ç†çš„å®‰å…¨æ€§', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªä¸»AIä»£ç†åœ¨ç¤¾ä¼šå„ä¸ªé¢†åŸŸçš„åº”ç”¨ä»·å€¼ï¼Œä½†é¢ä¸´æ¥è‡ªå¯¹æ‰‹çš„å®‰å…¨å¨èƒã€‚ç ”ç©¶æŒ‡å‡ºï¼Œé™æ€é˜²æŠ¤æªæ–½æ— æ³•æœ‰æ•ˆåº”å¯¹å¤šæ¬¡è¶Šç‹±å’Œæ¬ºéª—æ€§å¯¹é½ç­‰é«˜çº§æ”»å‡»ï¼Œå› æ­¤éœ€è¦æ–°çš„è¯„ä¼°æ¡†æ¶æ¥å¢å¼ºå®‰å…¨æ€§ã€‚æˆ‘ä»¬é€šè¿‡åå›¾çµæµ‹è¯•å’Œå¤šä»£ç†æ¨¡æ‹Ÿåˆ†ææ¬ºéª—æ€§å¯¹é½ï¼Œå¹¶å¼€å‘åè¶Šç‹±ç³»ç»Ÿï¼Œæµ‹è¯•ç»“æœæ˜¾ç¤ºåœ¨é•¿æ—¶é—´æ”»å‡»ä¸‹ç³»ç»Ÿå­˜åœ¨æŒç»­çš„è„†å¼±æ€§ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†é‡‡ç”¨çµæ´»çš„å®‰å…¨ç³»ç»Ÿçš„é‡è¦æ€§ï¼Œä»¥ä¾¿ä»£ç†èƒ½å¤Ÿä¸»åŠ¨ç›‘æ§å¹¶è¿›è¡Œé€‚åº”æ€§å¹²é¢„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.20307', 'title': 'Mobius: Text to Seamless Looping Video Generation via Latent Shift', 'url': 'https://huggingface.co/papers/2502.20307', 'abstract': "We present Mobius, a novel method to generate seamlessly looping videos from text descriptions directly without any user annotations, thereby creating new visual materials for the multi-media presentation. Our method repurposes the pre-trained video latent diffusion model for generating looping videos from text prompts without any training. During inference, we first construct a latent cycle by connecting the starting and ending noise of the videos. Given that the temporal consistency can be maintained by the context of the video diffusion model, we perform multi-frame latent denoising by gradually shifting the first-frame latent to the end in each step. As a result, the denoising context varies in each step while maintaining consistency throughout the inference process. Moreover, the latent cycle in our method can be of any length. This extends our latent-shifting approach to generate seamless looping videos beyond the scope of the video diffusion model's context. Unlike previous cinemagraphs, the proposed method does not require an image as appearance, which will restrict the motions of the generated results. Instead, our method can produce more dynamic motion and better visual quality. We conduct multiple experiments and comparisons to verify the effectiveness of the proposed method, demonstrating its efficacy in different scenarios. All the code will be made available.", 'score': 7, 'issue_id': 2458, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 27', 'zh': '2æœˆ27æ—¥'}, 'hash': '4763c15bca31d3b1', 'authors': ['Xiuli Bi', 'Jianfei Yuan', 'Bo Liu', 'Yong Zhang', 'Xiaodong Cun', 'Chi-Man Pun', 'Bin Xiao'], 'affiliations': ['Chongqing University of Post and Telecommunications, China', 'GVC Lab, Great Bay University, China', 'Meituan, China', 'University of Macau, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.20307.jpg', 'data': {'categories': ['#diffusion', '#multimodal', '#video', '#open_source'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ‘ĞµÑÑˆĞ¾Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ñ†Ğ¸ĞºĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸', 'desc': 'ĞœĞµÑ‚Ğ¾Ğ´ Mobius Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ñ†Ğ¸ĞºĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ°. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑĞ¼ĞµÑ‰Ğ°Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ° Ğº ĞºĞ¾Ğ½Ñ†Ñƒ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ñ†Ğ¸ĞºĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸Ğ½ĞµĞ¼Ğ°Ğ³Ñ€Ğ°Ñ„Ğ°Ğ¼Ğ¸, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Seamless Video Creation from Text: Introducing Mobius!', 'desc': 'Mobius is a new technique that creates seamless looping videos directly from text descriptions without needing user input. It utilizes a pre-trained video latent diffusion model to generate these videos, ensuring that the start and end of the video connect smoothly. The method involves a latent cycle that allows for flexible video lengths while maintaining temporal consistency through multi-frame latent denoising. This approach enables the generation of dynamic and high-quality visuals, surpassing previous methods that relied on static images.'}, 'zh': {'title': 'Mobiusï¼šä»æ–‡æœ¬ç”Ÿæˆæ— ç¼å¾ªç¯è§†é¢‘çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMobiusçš„æ–°æ–¹æ³•ï¼Œå¯ä»¥ç›´æ¥ä»æ–‡æœ¬æè¿°ç”Ÿæˆæ— ç¼å¾ªç¯è§†é¢‘ï¼Œè€Œæ— éœ€ç”¨æˆ·æ³¨é‡Šã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡è¿æ¥è§†é¢‘çš„èµ·å§‹å’Œç»“æŸå™ªå£°æ„å»ºæ½œåœ¨å¾ªç¯ï¼Œä»è€Œç”Ÿæˆå¾ªç¯è§†é¢‘ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡é€æ­¥å°†ç¬¬ä¸€å¸§çš„æ½œåœ¨è¡¨ç¤ºè½¬ç§»åˆ°æœ€åä¸€å¸§ï¼Œè¿›è¡Œå¤šå¸§æ½œåœ¨å»å™ªï¼Œç¡®ä¿æ—¶é—´ä¸€è‡´æ€§ã€‚ä¸ä»¥å¾€çš„åŠ¨æ€å›¾ç‰‡ä¸åŒï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦å›¾åƒä½œä¸ºå¤–è§‚ï¼Œå› æ­¤èƒ½å¤Ÿç”Ÿæˆæ›´åŠ¨æ€çš„è¿åŠ¨å’Œæ›´å¥½çš„è§†è§‰è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.20127', 'title': 'SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning', 'url': 'https://huggingface.co/papers/2502.20127', 'abstract': 'Mainstream issue-resolving frameworks predominantly rely on commercial models, leading to high costs and privacy concerns. Existing training approaches for issue resolving struggle with poor generalization and fail to fully leverage open-source development resources. We propose Subtask-oriented Reinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue resolving capability of LLMs. We decomposes issue resolving into structured subtasks: file localization, function localization, line localization, and code edit generation. SoRFT consists of two training stages: (1) rejection-sampled supervised fine-tuning, Chain of Thought (CoT) data is filtered using ground-truth before fine-tuning the LLM, and (2) rule-based reinforcement learning, which leverages PPO with ground-truth based rewards. We evaluate the SoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving state-of-the-art (SOTA) performance among open-source models (e.g., resolve 21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental results demonstrate that SoRFT significantly enhances issue-resolving performance, improves model generalization, and provides a cost-efficient alternative to commercial models.', 'score': 7, 'issue_id': 2456, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 27', 'zh': '2æœˆ27æ—¥'}, 'hash': 'ad848cf98c7468a7', 'authors': ['Zexiong Ma', 'Chao Peng', 'Pengfei Gao', 'Xiangxin Meng', 'Yanzhen Zou', 'Bing Xie'], 'affiliations': ['ByteDance', 'School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2502.20127.jpg', 'data': {'categories': ['#open_source', '#training', '#rlhf', '#rl', '#optimization'], 'emoji': 'ğŸ”§', 'ru': {'title': 'SoRFT: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¯Ğœ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ´Ğµ - Subtask-oriented Reinforced Fine-Tuning (SoRFT). ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ„Ğ°Ğ¹Ğ»Ğ°, Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¸ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ ĞºĞ¾Ğ´Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. SoRFT Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ñ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ». Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SoRFT Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² ĞºĞ¾Ğ´Ğµ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡Ğ½ÑƒÑ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼.'}, 'en': {'title': 'Enhancing Issue Resolution with SoRFT: A Cost-Effective Approach', 'desc': 'This paper introduces Subtask-oriented Reinforced Fine-Tuning (SoRFT), a new method designed to improve the issue-resolving capabilities of large language models (LLMs). It breaks down the issue resolution process into specific subtasks, such as file and function localization, and code editing. The training process involves two stages: first, a supervised fine-tuning phase that uses filtered data, and second, a reinforcement learning phase that applies Proximal Policy Optimization (PPO) with rewards based on ground-truth data. The results show that models trained with SoRFT outperform existing open-source models, achieving state-of-the-art results while being more cost-effective and maintaining better privacy.'}, 'zh': {'title': 'æå‡é—®é¢˜è§£å†³èƒ½åŠ›çš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ³•ï¼Œç§°ä¸ºå­ä»»åŠ¡å¯¼å‘å¼ºåŒ–å¾®è°ƒï¼ˆSoRFTï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é—®é¢˜è§£å†³æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å°†é—®é¢˜è§£å†³åˆ†è§£ä¸ºç»“æ„åŒ–çš„å­ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡ä»¶å®šä½ã€åŠŸèƒ½å®šä½ã€è¡Œå®šä½å’Œä»£ç ç¼–è¾‘ç”Ÿæˆã€‚SoRFTåŒ…å«ä¸¤ä¸ªè®­ç»ƒé˜¶æ®µï¼šé¦–å…ˆæ˜¯åŸºäºæ‹’ç»é‡‡æ ·çš„ç›‘ç£å¾®è°ƒï¼Œå…¶æ¬¡æ˜¯åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼Œåˆ©ç”¨åŸºäºçœŸå®æ•°æ®çš„å¥–åŠ±è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSoRFTæ˜¾è‘—æå‡äº†é—®é¢˜è§£å†³æ€§èƒ½ï¼Œæ”¹å–„äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸ºå•†ä¸šæ¨¡å‹æä¾›äº†ä¸€ç§æˆæœ¬æ•ˆç›Šé«˜çš„æ›¿ä»£æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.19459', 'title': 'Building Interactable Replicas of Complex Articulated Objects via Gaussian Splatting', 'url': 'https://huggingface.co/papers/2502.19459', 'abstract': 'Building articulated objects is a key challenge in computer vision. Existing methods often fail to effectively integrate information across different object states, limiting the accuracy of part-mesh reconstruction and part dynamics modeling, particularly for complex multi-part articulated objects. We introduce ArtGS, a novel approach that leverages 3D Gaussians as a flexible and efficient representation to address these issues. Our method incorporates canonical Gaussians with coarse-to-fine initialization and updates for aligning articulated part information across different object states, and employs a skinning-inspired part dynamics modeling module to improve both part-mesh reconstruction and articulation learning. Extensive experiments on both synthetic and real-world datasets, including a new benchmark for complex multi-part objects, demonstrate that ArtGS achieves state-of-the-art performance in joint parameter estimation and part mesh reconstruction. Our approach significantly improves reconstruction quality and efficiency, especially for multi-part articulated objects. Additionally, we provide comprehensive analyses of our design choices, validating the effectiveness of each component to highlight potential areas for future improvement.', 'score': 6, 'issue_id': 2462, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 26', 'zh': '2æœˆ26æ—¥'}, 'hash': '4220bf3b04cbef88', 'authors': ['Yu Liu', 'Baoxiong Jia', 'Ruijie Lu', 'Junfeng Ni', 'Song-Chun Zhu', 'Siyuan Huang'], 'affiliations': ['Peking University', 'State Key Laboratory of General Artificial Intelligence, BIGAI', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.19459.jpg', 'data': {'categories': ['#3d', '#cv', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ArtGS: Ğ“Ğ¸Ğ±ĞºĞ°Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ ÑĞ¾Ñ‡Ğ»ĞµĞ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 3D Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ²', 'desc': 'ArtGS - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ ÑĞ¾Ñ‡Ğ»ĞµĞ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 3D Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°. ArtGS Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ°Ğ½Ğ¾Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ñ‹ Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ñ‚ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ³Ğ¾ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ñ‡Ğ°ÑÑ‚ÑÑ… Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑÑ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞºĞ¸Ğ½Ğ½Ğ¸Ğ½Ğ³Ğ¾Ğ¼, Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ‚Ğ¸ĞºÑƒĞ»ÑÑ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ArtGS Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑĞµÑ‚ĞºĞ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'ArtGS: Revolutionizing Articulated Object Reconstruction with 3D Gaussians', 'desc': 'This paper presents ArtGS, a new method for building articulated objects in computer vision. It uses 3D Gaussians to effectively represent and align information across different states of multi-part objects, enhancing part-mesh reconstruction and dynamics modeling. The approach includes a skinning-inspired module that improves the learning of object articulation. Experiments show that ArtGS outperforms existing methods, providing better quality and efficiency in reconstructing complex articulated structures.'}, 'zh': {'title': 'ArtGSï¼šæå‡å…³èŠ‚ç‰©ä½“é‡å»ºçš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºArtGSçš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è®¡ç®—æœºè§†è§‰ä¸­æ„å»ºå…³èŠ‚ç‰©ä½“çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨3Dé«˜æ–¯ä½œä¸ºçµæ´»é«˜æ•ˆçš„è¡¨ç¤ºï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•´åˆä¸åŒç‰©ä½“çŠ¶æ€çš„ä¿¡æ¯ã€‚ArtGSé€šè¿‡ç²—åˆ°ç»†çš„åˆå§‹åŒ–å’Œæ›´æ–°ï¼Œä¼˜åŒ–å…³èŠ‚éƒ¨åˆ†ä¿¡æ¯çš„å¯¹é½ï¼Œå¹¶é‡‡ç”¨ç±»ä¼¼è’™çš®çš„éƒ¨åˆ†åŠ¨æ€å»ºæ¨¡æ¨¡å—ï¼Œæå‡äº†éƒ¨åˆ†ç½‘æ ¼é‡å»ºå’Œå…³èŠ‚å­¦ä¹ çš„æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒArtGSåœ¨å¤šéƒ¨åˆ†å…³èŠ‚ç‰©ä½“çš„å‚æ•°ä¼°è®¡å’Œç½‘æ ¼é‡å»ºæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†é‡å»ºè´¨é‡å’Œæ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.19735', 'title': 'R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning', 'url': 'https://huggingface.co/papers/2502.19735', 'abstract': 'Despite recent breakthroughs in reasoning-enhanced large language models (LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine translation (MT), where human translators naturally employ structured, multi-layered reasoning chain-of-thoughts (CoTs), is yet underexplored. Existing methods either design a fixed CoT tailored for a specific MT sub-task (e.g., literature translation), or rely on synthesizing CoTs unaligned with humans and supervised fine-tuning (SFT) prone to catastrophic forgetting, limiting their adaptability to diverse translation scenarios. This paper introduces R1-Translator (R1-T1), a novel framework to achieve inference-time reasoning for general MT via reinforcement learning (RL) with human-aligned CoTs comprising six common patterns. Our approach pioneers three innovations: (1) extending reasoning-based translation beyond MT sub-tasks to six languages and diverse tasks (e.g., legal/medical domain adaptation, idiom resolution); (2) formalizing six expert-curated CoT templates that mirror hybrid human strategies like context-aware paraphrasing and back translation; and (3) enabling self-evolving CoT discovery and anti-forgetting adaptation through RL with KL-constrained rewards. Experimental results indicate a steady translation performance improvement in 21 languages and 80 translation directions on Flores-101 test set, especially on the 15 languages unseen from training, with its general multilingual abilities preserved compared with plain SFT.', 'score': 6, 'issue_id': 2457, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 27', 'zh': '2æœˆ27æ—¥'}, 'hash': '45b53278bddf4ab9', 'authors': ['Minggui He', 'Yilun Liu', 'Shimin Tao', 'Yuanchang Luo', 'Hongyong Zeng', 'Chang Su', 'Li Zhang', 'Hongxia Ma', 'Daimeng Wei', 'Weibin Meng', 'Hao Yang', 'Boxing Chen', 'Osamu Yoshie'], 'affiliations': ['Huawei Canada, Canada', 'Huawei, China', 'Waseda University, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2502.19735.jpg', 'data': {'categories': ['#reasoning', '#training', '#machine_translation', '#rl', '#multilingual'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°Ğ·ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ˜Ğ˜ Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ñ‡Ğ¸Ğº', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ R1-Translator (R1-T1) - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑˆĞµÑÑ‚ÑŒ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ² Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑÑ… Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ñ‡Ğ¸ĞºĞ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ±ĞµĞ· ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ½Ğ° 21 ÑĞ·Ñ‹ĞºĞµ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° 15 ÑĞ·Ñ‹ĞºĞ°Ñ…, Ğ½Ğµ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°Ğ²ÑˆĞ¸Ñ…ÑÑ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Revolutionizing Machine Translation with Reasoning-Enhanced Frameworks', 'desc': 'This paper presents R1-Translator (R1-T1), a new framework that enhances machine translation (MT) by incorporating reasoning during translation, similar to how human translators think. It addresses limitations of existing methods that either use fixed reasoning patterns or rely on supervised fine-tuning, which can lead to forgetting important information. The framework utilizes reinforcement learning (RL) to align translation with human reasoning through six expert-curated chain-of-thought (CoT) templates. Experimental results show that R1-T1 improves translation quality across multiple languages and tasks, particularly for languages not seen during training, while maintaining strong multilingual capabilities.'}, 'zh': {'title': 'æ¨ç†é©±åŠ¨çš„é€šç”¨æœºå™¨ç¿»è¯‘æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åœ¨æœºå™¨ç¿»è¯‘ä¸­å¼•å…¥æ¨ç†å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œå°¤å…¶æ˜¯å¦‚ä½•åœ¨æ¨ç†æ—¶è¿›è¡Œæœ‰æ•ˆçš„ç¿»è¯‘ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶R1-Translatorï¼ˆR1-T1ï¼‰ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç»“åˆäººç±»å¯¹æ¨ç†é“¾çš„ç†è§£ï¼Œæ¥å®ç°é€šç”¨çš„æœºå™¨ç¿»è¯‘ã€‚è¯¥æ–¹æ³•åˆ›æ–°æ€§åœ°æ‰©å±•äº†æ¨ç†ç¿»è¯‘çš„åº”ç”¨èŒƒå›´ï¼Œå¹¶åˆ¶å®šäº†å…­ç§ä¸“å®¶ç­–åˆ’çš„æ¨ç†æ¨¡æ¿ï¼Œä»¥é€‚åº”ä¸åŒçš„ç¿»è¯‘ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨21ç§è¯­è¨€å’Œ80ä¸ªç¿»è¯‘æ–¹å‘ä¸Šå‡è¡¨ç°å‡ºç¨³å®šçš„ç¿»è¯‘æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒä¸­æœªè§è¿‡çš„15ç§è¯­è¨€ä¸Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.20388', 'title': 'Beyond Next-Token: Next-X Prediction for Autoregressive Visual Generation', 'url': 'https://huggingface.co/papers/2502.20388', 'abstract': "Autoregressive (AR) modeling, known for its next-token prediction paradigm, underpins state-of-the-art language and visual generative models. Traditionally, a ``token'' is treated as the smallest prediction unit, often a discrete symbol in language or a quantized patch in vision. However, the optimal token definition for 2D image structures remains an open question. Moreover, AR models suffer from exposure bias, where teacher forcing during training leads to error accumulation at inference. In this paper, we propose xAR, a generalized AR framework that extends the notion of a token to an entity X, which can represent an individual patch token, a cell (a ktimes k grouping of neighboring patches), a subsample (a non-local grouping of distant patches), a scale (coarse-to-fine resolution), or even a whole image. Additionally, we reformulate discrete token classification as continuous entity regression, leveraging flow-matching methods at each AR step. This approach conditions training on noisy entities instead of ground truth tokens, leading to Noisy Context Learning, which effectively alleviates exposure bias. As a result, xAR offers two key advantages: (1) it enables flexible prediction units that capture different contextual granularity and spatial structures, and (2) it mitigates exposure bias by avoiding reliance on teacher forcing. On ImageNet-256 generation benchmark, our base model, xAR-B (172M), outperforms DiT-XL/SiT-XL (675M) while achieving 20times faster inference. Meanwhile, xAR-H sets a new state-of-the-art with an FID of 1.24, running 2.2times faster than the previous best-performing model without relying on vision foundation modules (\\eg, DINOv2) or advanced guidance interval sampling.", 'score': 5, 'issue_id': 2471, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 27', 'zh': '2æœˆ27æ—¥'}, 'hash': '4f9c4a95a0bb1365', 'authors': ['Sucheng Ren', 'Qihang Yu', 'Ju He', 'Xiaohui Shen', 'Alan Yuille', 'Liang-Chieh Chen'], 'affiliations': ['ByteDance', 'Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2502.20388.jpg', 'data': {'categories': ['#training', '#benchmark', '#optimization', '#cv', '#architecture'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'xAR: Ğ“Ğ¸Ğ±ĞºĞ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ xAR - Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. xAR Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ´Ğ¾ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚Ğ¸ X, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ ÑĞºÑĞ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ·Ğ° ÑÑ‡ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ÑÑ…. xAR Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ImageNet-256, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ.'}, 'en': {'title': 'xAR: Redefining Tokens for Enhanced Generative Modeling', 'desc': "This paper introduces xAR, a novel autoregressive modeling framework that redefines the concept of a 'token' in generative tasks. Instead of using traditional discrete symbols, xAR allows for flexible prediction units, such as patches, groups of patches, or even entire images, enhancing the model's ability to capture complex spatial structures. The authors address the issue of exposure bias in AR models by employing Noisy Context Learning, which trains the model on noisy entities rather than relying solely on ground truth tokens. As a result, xAR not only improves performance on benchmarks like ImageNet-256 but also achieves faster inference times compared to existing models."}, 'zh': {'title': 'xARï¼šçµæ´»çš„è‡ªå›å½’æ¨¡å‹ï¼Œå‡è½»æ›å…‰åå·®', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºxARçš„è‡ªå›å½’æ¨¡å‹æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹è¿›ä¼ ç»Ÿçš„ä»¤ç‰Œå®šä¹‰ï¼Œä»¥é€‚åº”äºŒç»´å›¾åƒç»“æ„çš„å¤æ‚æ€§ã€‚xARå°†ä»¤ç‰Œæ‰©å±•ä¸ºå®ä½“Xï¼Œå¯ä»¥è¡¨ç¤ºå•ä¸ªè¡¥ä¸ã€é‚»è¿‘è¡¥ä¸çš„ç»„åˆã€è¿œç¨‹è¡¥ä¸çš„éå±€éƒ¨åˆ†ç»„ã€ä¸åŒåˆ†è¾¨ç‡çš„å°ºåº¦ï¼Œç”šè‡³æ•´ä¸ªå›¾åƒã€‚é€šè¿‡å°†ç¦»æ•£ä»¤ç‰Œåˆ†ç±»é‡æ–°è¡¨è¿°ä¸ºè¿ç»­å®ä½“å›å½’ï¼ŒxARåˆ©ç”¨æµåŒ¹é…æ–¹æ³•åœ¨æ¯ä¸ªè‡ªå›å½’æ­¥éª¤ä¸­è¿›è¡Œè®­ç»ƒï¼Œä»è€Œå®ç°äº†å™ªå£°ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œæœ‰æ•ˆå‡è½»äº†æ›å…‰åå·®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒxARåœ¨ç”Ÿæˆå›¾åƒæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¸”æ¨ç†é€Ÿåº¦æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.17355', 'title': 'On Relation-Specific Neurons in Large Language Models', 'url': 'https://huggingface.co/papers/2502.17355', 'abstract': "In large language models (LLMs), certain neurons can store distinct pieces of knowledge learned during pretraining. While knowledge typically appears as a combination of relations and entities, it remains unclear whether some neurons focus on a relation itself -- independent of any entity. We hypothesize such neurons detect a relation in the input text and guide generation involving such a relation. To investigate this, we study the Llama-2 family on a chosen set of relations with a statistics-based method. Our experiments demonstrate the existence of relation-specific neurons. We measure the effect of selectively deactivating candidate neurons specific to relation r on the LLM's ability to handle (1) facts whose relation is r and (2) facts whose relation is a different relation r' neq r. With respect to their capacity for encoding relation information, we give evidence for the following three properties of relation-specific neurons. (i) Neuron cumulativity. The neurons for r present a cumulative effect so that deactivating a larger portion of them results in the degradation of more facts in r. (ii) Neuron versatility. Neurons can be shared across multiple closely related as well as less related relations. Some relation neurons transfer across languages. (iii) Neuron interference. Deactivating neurons specific to one relation can improve LLM generation performance for facts of other relations. We will make our code publicly available at https://github.com/cisnlp/relation-specific-neurons.", 'score': 4, 'issue_id': 2469, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': '5149cebb219a4499', 'authors': ['Yihong Liu', 'Runsheng Chen', 'Lea Hirlimann', 'Ahmad Dawar Hakimi', 'Mingyang Wang', 'Amir Hossein Kargaran', 'Sascha Rothe', 'FranÃ§ois Yvon', 'Hinrich SchÃ¼tze'], 'affiliations': ['Bosch Center for Artificial Intelligence', 'Center for Information and Language Processing, LMU Munich', 'Google DeepMind, ZÃ¼rich, Switzerland', 'Munich Center for Machine Learning (MCML)', 'Sorbonne UniversitÃ©, CNRS, ISIR, France', 'Technical University of Munich'], 'pdf_title_img': 'assets/pdf/title_img/2502.17355.jpg', 'data': {'categories': ['#transfer_learning', '#architecture', '#open_source', '#data', '#interpretability', '#multilingual'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞµĞ¹Ñ€Ğ¾Ğ½Ñ‹ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹: ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ñ‹, ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ñ‹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½ÑƒÑ Ñ ÑÑ‚Ğ¸Ğ¼Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Llama-2 Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºÑƒĞ¼ÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚, ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ LLM Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ°ĞºÑ‚Ñ‹, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Unlocking Relation-Specific Neurons in Language Models', 'desc': "This paper explores how certain neurons in large language models (LLMs) can specifically encode knowledge about relations, independent of the entities involved. The authors hypothesize that these relation-specific neurons help the model recognize and generate text based on particular relationships. Through experiments with the Llama-2 model, they demonstrate that deactivating these neurons affects the model's performance on tasks related to those specific relations. The study reveals three key properties of these neurons: they show cumulative effects, can be versatile across different relations, and their interference can enhance performance on unrelated tasks."}, 'zh': {'title': 'æ­ç¤ºç‰¹å®šå…³ç³»ç¥ç»å…ƒçš„ç§˜å¯†', 'desc': 'åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼ŒæŸäº›ç¥ç»å…ƒå¯ä»¥å­˜å‚¨åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å­¦åˆ°çš„ç‰¹å®šçŸ¥è¯†ã€‚æˆ‘ä»¬å‡è®¾æœ‰äº›ç¥ç»å…ƒä¸“æ³¨äºè¾“å…¥æ–‡æœ¬ä¸­çš„å…³ç³»ï¼Œè€Œä¸ä¾èµ–äºä»»ä½•å®ä½“ã€‚é€šè¿‡å¯¹Llama-2ç³»åˆ—æ¨¡å‹çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°äº†ç‰¹å®šäºå…³ç³»çš„ç¥ç»å…ƒï¼Œå¹¶æµ‹é‡äº†é€‰æ‹©æ€§å»æ¿€æ´»è¿™äº›ç¥ç»å…ƒå¯¹æ¨¡å‹å¤„ç†ä¸åŒå…³ç³»äº‹å®çš„å½±å“ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™äº›ç‰¹å®šäºå…³ç³»çš„ç¥ç»å…ƒå…·æœ‰ç´¯ç§¯æ€§ã€å¤šåŠŸèƒ½æ€§å’Œå¹²æ‰°æ€§ç­‰ç‰¹å¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.18197', 'title': 'Training Consistency Models with Variational Noise Coupling', 'url': 'https://huggingface.co/papers/2502.18197', 'abstract': 'Consistency Training (CT) has recently emerged as a promising alternative to diffusion models, achieving competitive performance in image generation tasks. However, non-distillation consistency training often suffers from high variance and instability, and analyzing and improving its training dynamics is an active area of research. In this work, we propose a novel CT training approach based on the Flow Matching framework. Our main contribution is a trained noise-coupling scheme inspired by the architecture of Variational Autoencoders (VAE). By training a data-dependent noise emission model implemented as an encoder architecture, our method can indirectly learn the geometry of the noise-to-data mapping, which is instead fixed by the choice of the forward process in classical CT. Empirical results across diverse image datasets show significant generative improvements, with our model outperforming baselines and achieving the state-of-the-art (SoTA) non-distillation CT FID on CIFAR-10, and attaining FID on par with SoTA on ImageNet at 64 times 64 resolution in 2-step generation. Our code is available at https://github.com/sony/vct .', 'score': 3, 'issue_id': 2469, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 25', 'zh': '2æœˆ25æ—¥'}, 'hash': '6201a478f2f81d4c', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#architecture', '#training', '#open_source', '#cv', '#optimization', '#diffusion'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğµ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ° Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ (Consistency Training) Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Flow Matching. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ°, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ ĞºĞ¾ÑĞ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ state-of-the-art Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ FID Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Revolutionizing Image Generation with Flow Matching in Consistency Training', 'desc': 'This paper introduces a new approach to Consistency Training (CT) for image generation, leveraging the Flow Matching framework. The authors address the issues of high variance and instability in non-distillation CT by proposing a noise-coupling scheme inspired by Variational Autoencoders (VAE). Their method involves training a data-dependent noise emission model, which helps to learn the noise-to-data mapping more effectively. The results demonstrate that their approach significantly improves generative performance, achieving state-of-the-art results on CIFAR-10 and competitive results on ImageNet.'}, 'zh': {'title': 'ä¸€è‡´æ€§è®­ç»ƒçš„æ–°çªç ´ï¼šæµåŒ¹é…æ¡†æ¶ä¸‹çš„å™ªå£°è€¦åˆæ–¹æ¡ˆ', 'desc': 'ä¸€è‡´æ€§è®­ç»ƒï¼ˆCTï¼‰ä½œä¸ºä¸€ç§æ–°å…´çš„å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œå±•ç°å‡ºä¸æ‰©æ•£æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œéè’¸é¦ä¸€è‡´æ€§è®­ç»ƒå¸¸å¸¸é¢ä¸´é«˜æ–¹å·®å’Œä¸ç¨³å®šæ€§çš„é—®é¢˜ï¼Œå› æ­¤åˆ†æå’Œæ”¹è¿›å…¶è®­ç»ƒåŠ¨æ€æˆä¸ºç ”ç©¶çš„çƒ­ç‚¹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæµåŒ¹é…æ¡†æ¶çš„æ–°å‹CTè®­ç»ƒæ–¹æ³•ï¼Œä¸»è¦è´¡çŒ®åœ¨äºå€Ÿé‰´å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æ¶æ„çš„å™ªå£°è€¦åˆæ–¹æ¡ˆã€‚é€šè¿‡è®­ç»ƒä¸€ä¸ªä¾èµ–äºæ•°æ®çš„å™ªå£°å‘å°„æ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿé—´æ¥å­¦ä¹ å™ªå£°åˆ°æ•°æ®æ˜ å°„çš„å‡ ä½•ç‰¹æ€§ï¼Œä»è€Œåœ¨å¤šä¸ªå›¾åƒæ•°æ®é›†ä¸Šå®ç°æ˜¾è‘—çš„ç”Ÿæˆæ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.16111', 'title': 'PlanGEN: A Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving', 'url': 'https://huggingface.co/papers/2502.16111', 'abstract': 'Recent agent frameworks and inference-time algorithms often struggle with complex planning problems due to limitations in verifying generated plans or reasoning and varying complexity of instances within a single task. Many existing methods for these tasks either perform task-level verification without considering constraints or apply inference-time algorithms without adapting to instance-level complexity. To address these limitations, we propose PlanGEN, a model-agnostic and easily scalable agent framework with three key components: constraint, verification, and selection agents. Specifically, our approach proposes constraint-guided iterative verification to enhance performance of inference-time algorithms--Best of N, Tree-of-Thought, and REBASE. In PlanGEN framework, the selection agent optimizes algorithm choice based on instance complexity, ensuring better adaptability to complex planning problems. Experimental results demonstrate significant improvements over the strongest baseline across multiple benchmarks, achieving state-of-the-art results on NATURAL PLAN (sim8%uparrow), OlympiadBench (sim4%uparrow), DocFinQA (sim7%uparrow), and GPQA (sim1%uparrow). Our key finding highlights that constraint-guided iterative verification improves inference-time algorithms, and adaptive selection further boosts performance on complex planning and reasoning problems.', 'score': 1, 'issue_id': 2474, 'pub_date': '2025-02-22', 'pub_date_card': {'ru': '22 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 22', 'zh': '2æœˆ22æ—¥'}, 'hash': '99f42232c52b1f77', 'authors': ['Mihir Parmar', 'Xin Liu', 'Palash Goyal', 'Yanfei Chen', 'Long Le', 'Swaroop Mishra', 'Hossein Mobahi', 'Jindong Gu', 'Zifeng Wang', 'Hootan Nakhost', 'Chitta Baral', 'Chen-Yu Lee', 'Tomas Pfister', 'Hamid Palangi'], 'affiliations': ['Arizona State University', 'Google'], 'pdf_title_img': 'assets/pdf/title_img/2502.16111.jpg', 'data': {'categories': ['#inference', '#benchmark', '#reasoning', '#agents', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'PlanGEN: ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ˜Ğ˜', 'desc': 'PlanGEN - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹, Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ°. PlanGEN Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Best of N, Tree-of-Thought Ğ¸ REBASE. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ NATURAL PLAN, OlympiadBench, DocFinQA Ğ¸ GPQA.'}, 'en': {'title': 'PlanGEN: Enhancing Planning with Adaptive Verification and Selection', 'desc': 'The paper introduces PlanGEN, a new agent framework designed to tackle complex planning problems in machine learning. It combines three main components: constraint agents, verification agents, and selection agents, which work together to enhance the performance of inference-time algorithms. By using constraint-guided iterative verification, PlanGEN improves the effectiveness of algorithms like Best of N and Tree-of-Thought. The framework also adapts algorithm selection based on the complexity of the task, leading to significant performance gains across various benchmarks.'}, 'zh': {'title': 'PlanGENï¼šæ™ºèƒ½ä½“æ¡†æ¶çš„åˆ›æ–°è§£å†³æ–¹æ¡ˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPlanGENçš„æ¨¡å‹æ— å…³ä¸”æ˜“äºæ‰©å±•çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤æ‚è§„åˆ’é—®é¢˜ä¸­çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šçº¦æŸä»£ç†ã€éªŒè¯ä»£ç†å’Œé€‰æ‹©ä»£ç†ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡Œçº¦æŸå¼•å¯¼çš„è¿­ä»£éªŒè¯ã€‚é€šè¿‡ä¼˜åŒ–ç®—æ³•é€‰æ‹©ï¼ŒPlanGENèƒ½å¤Ÿæ ¹æ®å®ä¾‹å¤æ‚æ€§è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ï¼Œä»è€Œæé«˜æ¨ç†æ—¶é—´ç®—æ³•çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPlanGENåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†æœ€å¼ºåŸºçº¿ï¼Œè¾¾åˆ°äº†æœ€æ–°çš„ç ”ç©¶æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.20378', 'title': 'Efficient Gaussian Splatting for Monocular Dynamic Scene Rendering via Sparse Time-Variant Attribute Modeling', 'url': 'https://huggingface.co/papers/2502.20378', 'abstract': 'Rendering dynamic scenes from monocular videos is a crucial yet challenging task. The recent deformable Gaussian Splatting has emerged as a robust solution to represent real-world dynamic scenes. However, it often leads to heavily redundant Gaussians, attempting to fit every training view at various time steps, leading to slower rendering speeds. Additionally, the attributes of Gaussians in static areas are time-invariant, making it unnecessary to model every Gaussian, which can cause jittering in static regions. In practice, the primary bottleneck in rendering speed for dynamic scenes is the number of Gaussians. In response, we introduce Efficient Dynamic Gaussian Splatting (EDGS), which represents dynamic scenes via sparse time-variant attribute modeling. Our approach formulates dynamic scenes using a sparse anchor-grid representation, with the motion flow of dense Gaussians calculated via a classical kernel representation. Furthermore, we propose an unsupervised strategy to efficiently filter out anchors corresponding to static areas. Only anchors associated with deformable objects are input into MLPs to query time-variant attributes. Experiments on two real-world datasets demonstrate that our EDGS significantly improves the rendering speed with superior rendering quality compared to previous state-of-the-art methods.', 'score': 1, 'issue_id': 2469, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 27', 'zh': '2æœˆ27æ—¥'}, 'hash': '0b7155b54c5f0829', 'authors': ['Hanyang Kong', 'Xingyi Yang', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2502.20378.jpg', 'data': {'categories': ['#cv', '#3d'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Efficient Dynamic Gaussian Splatting (EDGS) Ğ´Ğ»Ñ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. EDGS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞºĞ¾Ñ€Ğ½Ğ¾Ğ¹ ÑĞµÑ‚ĞºĞ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ MLP Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ EDGS Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Speeding Up Dynamic Scene Rendering with EDGS', 'desc': 'This paper presents Efficient Dynamic Gaussian Splatting (EDGS), a method for rendering dynamic scenes from monocular videos more efficiently. The authors address the issue of redundant Gaussians in existing methods, which slow down rendering speeds and cause jittering in static areas. EDGS utilizes a sparse anchor-grid representation to model dynamic scenes, focusing on time-variant attributes only for deformable objects. Experiments show that EDGS enhances rendering speed and quality compared to previous techniques, making it a significant advancement in the field.'}, 'zh': {'title': 'é«˜æ•ˆåŠ¨æ€é«˜æ–¯ç‚¹äº‘ï¼šæå‡æ¸²æŸ“é€Ÿåº¦ä¸è´¨é‡çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„åŠ¨æ€é«˜æ–¯ç‚¹äº‘è¡¨ç¤ºæ–¹æ³•ï¼ˆEDGSï¼‰ï¼Œæ—¨åœ¨è§£å†³ä»å•ç›®è§†é¢‘æ¸²æŸ“åŠ¨æ€åœºæ™¯çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„é«˜æ–¯ç‚¹äº‘æ–¹æ³•å¾€å¾€ä¼šå¯¼è‡´å†—ä½™çš„é«˜æ–¯åˆ†å¸ƒï¼Œå½±å“æ¸²æŸ“é€Ÿåº¦ã€‚EDGSé€šè¿‡ç¨€ç–çš„æ—¶é—´å˜åŒ–å±æ€§å»ºæ¨¡ï¼Œå‡å°‘äº†é™æ€åŒºåŸŸçš„é«˜æ–¯æ•°é‡ï¼Œä»è€Œæé«˜äº†æ¸²æŸ“æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEDGSåœ¨æ¸²æŸ“é€Ÿåº¦å’Œè´¨é‡ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (2)', '#agi', '#alignment (1)', '#architecture (7)', '#audio', '#benchmark (10)', '#cv (7)', '#data (2)', '#dataset (2)', '#diffusion (4)', '#ethics', '#games', '#graphs', '#hallucinations', '#healthcare (1)', '#inference (4)', '#interpretability (2)', '#leakage', '#long_context (2)', '#low_resource', '#machine_translation (1)', '#math (1)', '#multilingual (2)', '#multimodal (4)', '#open_source (6)', '#optimization (11)', '#plp', '#rag', '#reasoning (5)', '#rl (5)', '#rlhf (2)', '#robotics', '#science', '#security (1)', '#small_models', '#story_generation', '#survey', '#synthetic', '#training (12)', '#transfer_learning (1)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-03-01 01:50',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-01 01:50')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-01 01:50')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    