
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 11 papers. December 30.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">30 декабря</span> | <span id="title-articles-count">11 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2024-12-27.html">⬅️ <span id="prev-date">27.12</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-12-31.html">➡️ <span id="next-date">31.12</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-12.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '30 декабря', 'en': 'December 30', 'zh': '12月30日'};
        let feedDateNext = {'ru': '31.12', 'en': '12/31', 'zh': '12月31日'};
        let feedDatePrev = {'ru': '27.12', 'en': '12/27', 'zh': '12月27日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2412.18925', 'title': 'HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs', 'url': 'https://huggingface.co/papers/2412.18925', 'abstract': 'The breakthrough of OpenAI o1 highlights the potential of enhancing reasoning to improve LLM. Yet, most research in reasoning has focused on mathematical tasks, leaving domains like medicine underexplored. The medical domain, though distinct from mathematics, also demands robust reasoning to provide reliable answers, given the high standards of healthcare. However, verifying medical reasoning is challenging, unlike those in mathematics. To address this, we propose verifiable medical problems with a medical verifier to check the correctness of model outputs. This verifiable nature enables advancements in medical reasoning through a two-stage approach: (1) using the verifier to guide the search for a complex reasoning trajectory for fine-tuning LLMs, (2) applying reinforcement learning (RL) with verifier-based rewards to enhance complex reasoning further. Finally, we introduce HuatuoGPT-o1, a medical LLM capable of complex reasoning, which outperforms general and medical-specific baselines using only 40K verifiable problems. Experiments show complex reasoning improves medical problem-solving and benefits more from RL. We hope our approach inspires advancements in reasoning across medical and other specialized domains.', 'score': 50, 'issue_id': 1382, 'pub_date': '2024-12-25', 'pub_date_card': {'ru': '25 декабря', 'en': 'December 25', 'zh': '12月25日'}, 'hash': '218dd2a8c2ae478f', 'authors': ['Junying Chen', 'Zhenyang Cai', 'Ke Ji', 'Xidong Wang', 'Wanlong Liu', 'Rongsheng Wang', 'Jianye Hou', 'Benyou Wang'], 'affiliations': ['Shenzhen Research Institute of Big Data', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2412.18925.jpg', 'data': {'categories': ['#reasoning', '#healthcare', '#training', '#rl'], 'emoji': '🩺', 'ru': {'title': 'Улучшение медицинских ИИ-моделей через верифицируемые рассуждения', 'desc': 'Статья представляет новый подход к улучшению способностей языковых моделей в медицинской сфере. Авторы предлагают использовать верифицируемые медицинские задачи и специальный верификатор для проверки корректности ответов модели. На основе этого подхода разработана двухэтапная методика: сначала используется верификатор для поиска сложной траектории рассуждений при файнтюнинге языковых моделей, затем применяется обучение с подкреплением для дальнейшего улучшения сложных рассуждений. В результате создана модель HuatuoGPT-01, превосходящая базовые модели в решении медицинских задач.'}, 'en': {'title': 'Enhancing Medical Reasoning with HuatuoGPT-o1', 'desc': 'This paper discusses the development of HuatuoGPT-o1, a medical language model (LLM) that enhances reasoning capabilities specifically for medical tasks. Unlike traditional approaches that focus on mathematical reasoning, this research emphasizes the need for robust reasoning in healthcare, where accuracy is critical. The authors propose a two-stage method that includes a medical verifier to ensure the correctness of model outputs and reinforcement learning (RL) to further improve reasoning skills. The results demonstrate that HuatuoGPT-o1 significantly outperforms existing models by effectively solving complex medical problems using a limited dataset of verifiable problems.'}, 'zh': {'title': '医学推理的新突破：HuatuoGPT-o1', 'desc': '这篇论文介绍了OpenAI的o1在增强推理能力方面的突破，特别是在医学领域的应用。尽管大多数推理研究集中在数学任务上，但医学同样需要强大的推理能力以提供可靠的答案。为了验证医学推理的正确性，作者提出了一种可验证的医学问题和医学验证器，帮助检查模型输出的准确性。最终，论文介绍了HuatuoGPT-o1，这是一种能够进行复杂推理的医学大语言模型，实验表明其在解决医学问题时表现优于其他基线模型。'}}}, {'id': 'https://huggingface.co/papers/2412.18619', 'title': 'Next Token Prediction Towards Multimodal Intelligence: A Comprehensive Survey', 'url': 'https://huggingface.co/papers/2412.18619', 'abstract': 'Building on the foundations of language modeling in natural language processing, Next Token Prediction (NTP) has evolved into a versatile training objective for machine learning tasks across various modalities, achieving considerable success. As Large Language Models (LLMs) have advanced to unify understanding and generation tasks within the textual modality, recent research has shown that tasks from different modalities can also be effectively encapsulated within the NTP framework, transforming the multimodal information into tokens and predict the next one given the context. This survey introduces a comprehensive taxonomy that unifies both understanding and generation within multimodal learning through the lens of NTP. The proposed taxonomy covers five key aspects: Multimodal tokenization, MMNTP model architectures, unified task representation, datasets \\& evaluation, and open challenges. This new taxonomy aims to aid researchers in their exploration of multimodal intelligence. An associated GitHub repository collecting the latest papers and repos is available at https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction', 'score': 15, 'issue_id': 1392, 'pub_date': '2024-12-16', 'pub_date_card': {'ru': '16 декабря', 'en': 'December 16', 'zh': '12月16日'}, 'hash': '48fd5f4393f283a7', 'authors': ['Liang Chen', 'Zekun Wang', 'Shuhuai Ren', 'Lei Li', 'Haozhe Zhao', 'Yunshui Li', 'Zefan Cai', 'Hongcheng Guo', 'Lei Zhang', 'Yizhe Xiong', 'Yichi Zhang', 'Ruoyu Wu', 'Qingxiu Dong', 'Ge Zhang', 'Jian Yang', 'Lingwei Meng', 'Shujie Hu', 'Yulong Chen', 'Junyang Lin', 'Shuai Bai', 'Andreas Vlachos', 'Xu Tan', 'Minjia Zhang', 'Wen Xiao', 'Aaron Yee', 'Tianyu Liu', 'Baobao Chang'], 'affiliations': ['Beihang University', 'Peking University', 'Shenzhen Institute of Advanced Technology, China Academy of Sciences', 'Tsinghua University', 'University of Hongkong'], 'pdf_title_img': 'assets/pdf/title_img/2412.18619.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#benchmark', '#dataset', '#architecture', '#survey'], 'emoji': '🧠', 'ru': {'title': 'Единый подход к мультимодальному ИИ через предсказание следующего токена', 'desc': 'Это обзор, посвященный применению предсказания следующего токена (NTP) в мультимодальном машинном обучении. Авторы представляют таксономию, объединяющую задачи понимания и генерации в мультимодальном обучении через призму NTP. Рассматриваются пять ключевых аспектов: мультимодальная токенизация, архитектуры моделей MMNTP, унифицированное представление задач, наборы данных и оценка, а также открытые проблемы. Цель таксономии - помочь исследователям в изучении мультимодального искусственного интеллекта.'}, 'en': {'title': 'Unifying Multimodal Learning through Next Token Prediction', 'desc': 'This paper discusses the Next Token Prediction (NTP) approach in machine learning, which has become a key method for training models across different types of data. It highlights how NTP can be applied not only to text but also to other modalities by converting various forms of information into tokens for prediction. The authors present a new taxonomy that organizes the understanding and generation tasks in multimodal learning, focusing on aspects like tokenization and model architectures. This framework is designed to support researchers in advancing multimodal intelligence and addresses current challenges in the field.'}, 'zh': {'title': '下一标记预测：多模态学习的新视角', 'desc': '本文探讨了下一标记预测（NTP）在多模态学习中的应用，强调其在自然语言处理中的重要性。随着大型语言模型（LLMs）的发展，NTP不仅适用于文本任务，还能有效处理其他模态的信息。文章提出了一个全面的分类法，涵盖了多模态标记化、MMNTP模型架构、统一任务表示、数据集与评估以及开放挑战等五个关键方面。该分类法旨在帮助研究人员更好地探索多模态智能。'}}}, {'id': 'https://huggingface.co/papers/2412.19326', 'title': 'Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment', 'url': 'https://huggingface.co/papers/2412.19326', 'abstract': "Current multimodal large language models (MLLMs) struggle with fine-grained or precise understanding of visuals though they give comprehensive perception and reasoning in a spectrum of vision applications. Recent studies either develop tool-using or unify specific visual tasks into the autoregressive framework, often at the expense of overall multimodal performance. To address this issue and enhance MLLMs with visual tasks in a scalable fashion, we propose Task Preference Optimization (TPO), a novel method that utilizes differentiable task preferences derived from typical fine-grained visual tasks. TPO introduces learnable task tokens that establish connections between multiple task-specific heads and the MLLM. By leveraging rich visual labels during training, TPO significantly enhances the MLLM's multimodal capabilities and task-specific performance. Through multi-task co-training within TPO, we observe synergistic benefits that elevate individual task performance beyond what is achievable through single-task training methodologies. Our instantiation of this approach with VideoChat and LLaVA demonstrates an overall 14.6% improvement in multimodal performance compared to baseline models. Additionally, MLLM-TPO demonstrates robust zero-shot capabilities across various tasks, performing comparably to state-of-the-art supervised models. The code will be released at https://github.com/OpenGVLab/TPO", 'score': 11, 'issue_id': 1383, 'pub_date': '2024-12-26', 'pub_date_card': {'ru': '26 декабря', 'en': 'December 26', 'zh': '12月26日'}, 'hash': '84d0f2e573ba96b2', 'authors': ['Ziang Yan', 'Zhilin Li', 'Yinan He', 'Chenting Wang', 'Kunchang Li', 'Xinhao Li', 'Xiangyu Zeng', 'Zilei Wang', 'Yali Wang', 'Yu Qiao', 'Limin Wang', 'Yi Wang'], 'affiliations': ['Nanjing University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences', 'University of Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2412.19326.jpg', 'data': {'categories': ['#multimodal', '#games', '#training', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Оптимизация предпочтений задач: новый подход к улучшению мультимодальных языковых моделей', 'desc': 'Статья представляет новый метод под названием Task Preference Optimization (TPO) для улучшения мультимодальных больших языковых моделей (MLLM) в задачах компьютерного зрения. TPO использует обучаемые токены задач для связи между MLLM и специфичными для задач головами модели. Метод позволяет значительно улучшить мультимодальные возможности MLLM и производительность в конкретных задачах за счет использования богатых визуальных меток при обучении. Эксперименты показали общее улучшение мультимодальной производительности на 14.6% по сравнению с базовыми моделями.'}, 'en': {'title': 'Enhancing Multimodal Models with Task Preference Optimization', 'desc': 'This paper introduces Task Preference Optimization (TPO), a new method designed to improve multimodal large language models (MLLMs) in understanding visual tasks. TPO uses learnable task tokens to connect different task-specific heads with the MLLM, allowing for better integration of fine-grained visual information. By training with rich visual labels and employing multi-task co-training, TPO enhances both the overall multimodal performance and the performance on specific tasks. The results show a significant improvement in performance, with TPO achieving a 14.6% increase compared to baseline models, while also demonstrating strong zero-shot capabilities.'}, 'zh': {'title': '任务偏好优化：提升多模态模型的视觉理解能力', 'desc': '当前的多模态大型语言模型（MLLMs）在视觉理解方面存在一定的局限性，尤其是在细粒度的视觉任务上。为了解决这个问题，我们提出了一种新的方法，称为任务偏好优化（TPO），它通过可微分的任务偏好来增强MLLMs的视觉任务能力。TPO引入了可学习的任务标记，建立了多个特定任务头与MLLM之间的连接，从而提升了多模态能力和任务特定性能。通过多任务共同训练，TPO显著提高了模型的整体表现，尤其是在零样本任务上表现出色。'}}}, {'id': 'https://huggingface.co/papers/2412.18653', 'title': '1.58-bit FLUX', 'url': 'https://huggingface.co/papers/2412.18653', 'abstract': 'We present 1.58-bit FLUX, the first successful approach to quantizing the state-of-the-art text-to-image generation model, FLUX.1-dev, using 1.58-bit weights (i.e., values in {-1, 0, +1}) while maintaining comparable performance for generating 1024 x 1024 images. Notably, our quantization method operates without access to image data, relying solely on self-supervision from the FLUX.1-dev model. Additionally, we develop a custom kernel optimized for 1.58-bit operations, achieving a 7.7x reduction in model storage, a 5.1x reduction in inference memory, and improved inference latency. Extensive evaluations on the GenEval and T2I Compbench benchmarks demonstrate the effectiveness of 1.58-bit FLUX in maintaining generation quality while significantly enhancing computational efficiency.', 'score': 10, 'issue_id': 1396, 'pub_date': '2024-12-24', 'pub_date_card': {'ru': '24 декабря', 'en': 'December 24', 'zh': '12月24日'}, 'hash': '6476ec807199e8af', 'authors': ['Chenglin Yang', 'Celong Liu', 'Xueqing Deng', 'Dongwon Kim', 'Xing Mei', 'Xiaohui Shen', 'Liang-Chieh Chen'], 'affiliations': ['ByteDance', 'POSTECH'], 'pdf_title_img': 'assets/pdf/title_img/2412.18653.jpg', 'data': {'categories': ['#cv', '#inference', '#benchmark', '#optimization'], 'emoji': '🖼️', 'ru': {'title': 'Эффективное квантование нейросети для генерации изображений', 'desc': 'В статье представлен метод квантования модели генерации изображений FLUX.1-dev до 1,58 бит. Это позволяет значительно сократить объем памяти и ускорить инференс без существенной потери качества. Квантование выполняется без доступа к данным изображений, используя только самообучение. Разработано специальное ядро для 1,58-битных операций, что дополнительно повышает эффективность.'}, 'en': {'title': 'Efficient Image Generation with 1.58-bit FLUX', 'desc': 'This paper introduces 1.58-bit FLUX, a novel method for quantizing the FLUX.1-dev text-to-image generation model. By using only 1.58-bit weights, which can take values of -1, 0, or +1, the model achieves similar performance in generating high-resolution images while drastically reducing storage and memory requirements. The quantization process is unique as it does not require access to image data, instead utilizing self-supervision from the existing model. The authors also present a custom kernel that optimizes 1.58-bit operations, leading to significant improvements in computational efficiency without compromising image generation quality.'}, 'zh': {'title': '高效量化，图像生成新突破', 'desc': '我们提出了1.58-bit FLUX，这是第一个成功量化最先进的文本到图像生成模型FLUX.1-dev的方法，使用1.58-bit权重（即{-1, 0, +1}的值），同时保持生成1024 x 1024图像的性能。值得注意的是，我们的量化方法不依赖于图像数据，而是仅依靠FLUX.1-dev模型的自我监督。此外，我们开发了一个针对1.58-bit操作优化的自定义内核，实现了模型存储减少7.7倍，推理内存减少5.1倍，并改善了推理延迟。在GenEval和T2I Compbench基准上的广泛评估证明了1.58-bit FLUX在保持生成质量的同时显著提高了计算效率。'}}}, {'id': 'https://huggingface.co/papers/2412.18605', 'title': 'Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models', 'url': 'https://huggingface.co/papers/2412.18605', 'abstract': 'Orientation is a key attribute of objects, crucial for understanding their spatial pose and arrangement in images. However, practical solutions for accurate orientation estimation from a single image remain underexplored. In this work, we introduce Orient Anything, the first expert and foundational model designed to estimate object orientation in a single- and free-view image. Due to the scarcity of labeled data, we propose extracting knowledge from the 3D world. By developing a pipeline to annotate the front face of 3D objects and render images from random views, we collect 2M images with precise orientation annotations. To fully leverage the dataset, we design a robust training objective that models the 3D orientation as probability distributions of three angles and predicts the object orientation by fitting these distributions. Besides, we employ several strategies to improve synthetic-to-real transfer. Our model achieves state-of-the-art orientation estimation accuracy in both rendered and real images and exhibits impressive zero-shot ability in various scenarios. More importantly, our model enhances many applications, such as comprehension and generation of complex spatial concepts and 3D object pose adjustment.', 'score': 9, 'issue_id': 1381, 'pub_date': '2024-12-24', 'pub_date_card': {'ru': '24 декабря', 'en': 'December 24', 'zh': '12月24日'}, 'hash': '6bc56feed4022217', 'authors': ['Zehan Wang', 'Ziang Zhang', 'Tianyu Pang', 'Chao Du', 'Hengshuang Zhao', 'Zhou Zhao'], 'affiliations': ['Sea AI Lab', 'The University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2412.18605.jpg', 'data': {'categories': ['#synthetic', '#cv', '#training', '#dataset', '#3d', '#transfer_learning'], 'emoji': '🧭', 'ru': {'title': 'Orient Anything: точное определение ориентации объектов по одному изображению', 'desc': 'Статья представляет новую модель Orient Anything для оценки ориентации объектов на изображениях. Авторы создали датасет из 2 миллионов синтетических изображений с точными аннотациями ориентации. Модель использует вероятностный подход, моделируя ориентацию как распределения трех углов. Orient Anything достигает наилучших результатов в оценке ориентации на синтетических и реальных изображениях, демонстрируя способность к обобщению.'}, 'en': {'title': 'Revolutionizing Object Orientation Estimation from Images', 'desc': "This paper presents 'Orient Anything', a novel model for estimating the orientation of objects from single images. It addresses the challenge of limited labeled data by utilizing 3D object knowledge to create a large dataset of 2 million images with accurate orientation labels. The model employs a training objective that treats object orientation as probability distributions of angles, allowing it to predict orientations effectively. Additionally, it demonstrates strong performance in both synthetic and real-world scenarios, enhancing applications related to spatial understanding and 3D object manipulation."}, 'zh': {'title': '单图像方向估计的新突破', 'desc': '本文介绍了一种名为Orient Anything的模型，旨在从单张图像中准确估计物体的方向。由于标注数据稀缺，我们通过从3D世界提取知识，收集了200万张带有精确方向标注的图像。该模型通过将3D方向建模为三个角度的概率分布，来预测物体的方向，并采用多种策略提高合成图像到真实图像的迁移效果。最终，我们的模型在渲染和真实图像中都达到了最先进的方向估计精度，并在多种场景中展现了出色的零样本能力。'}}}, {'id': 'https://huggingface.co/papers/2412.17762', 'title': 'The Superposition of Diffusion Models Using the Itô Density Estimator', 'url': 'https://huggingface.co/papers/2412.17762', 'abstract': "The Cambrian explosion of easily accessible pre-trained diffusion models suggests a demand for methods that combine multiple different pre-trained diffusion models without incurring the significant computational burden of re-training a larger combined model. In this paper, we cast the problem of combining multiple pre-trained diffusion models at the generation stage under a novel proposed framework termed superposition. Theoretically, we derive superposition from rigorous first principles stemming from the celebrated continuity equation and design two novel algorithms tailor-made for combining diffusion models in SuperDiff. SuperDiff leverages a new scalable It\\^o density estimator for the log likelihood of the diffusion SDE which incurs no additional overhead compared to the well-known Hutchinson's estimator needed for divergence calculations. We demonstrate that SuperDiff is scalable to large pre-trained diffusion models as superposition is performed solely through composition during inference, and also enjoys painless implementation as it combines different pre-trained vector fields through an automated re-weighting scheme. Notably, we show that SuperDiff is efficient during inference time, and mimics traditional composition operators such as the logical OR and the logical AND. We empirically demonstrate the utility of using SuperDiff for generating more diverse images on CIFAR-10, more faithful prompt conditioned image editing using Stable Diffusion, and improved unconditional de novo structure design of proteins. https://github.com/necludov/super-diffusion", 'score': 8, 'issue_id': 1387, 'pub_date': '2024-12-23', 'pub_date_card': {'ru': '23 декабря', 'en': 'December 23', 'zh': '12月23日'}, 'hash': '8f698bcd1c639eb2', 'authors': ['Marta Skreta', 'Lazar Atanackovic', 'Avishek Joey Bose', 'Alexander Tong', 'Kirill Neklyudov'], 'affiliations': ['Mila - Quebec AI Institute', 'University of Oxford', 'University of Toronto', 'Université de Montréal', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2412.17762.jpg', 'data': {'categories': ['#diffusion', '#data', '#inference', '#cv'], 'emoji': '🔀', 'ru': {'title': 'SuperDiff: Эффективное объединение диффузионных моделей без переобучения', 'desc': "Статья представляет новый метод под названием SuperDiff для объединения нескольких предобученных диффузионных моделей без необходимости их повторного обучения. Авторы предлагают теоретическую основу 'суперпозиции', основанную на уравнении непрерывности, и разрабатывают два новых алгоритма для комбинирования моделей. SuperDiff использует новый масштабируемый оценщик плотности Ито для логарифмического правдоподобия диффузионного СДУ. Эмпирические результаты показывают эффективность метода в генерации более разнообразных изображений, редактировании изображений по текстовым подсказкам и улучшенном дизайне белковых структур."}, 'en': {'title': 'Effortless Fusion of Diffusion Models with SuperDiff', 'desc': 'This paper introduces a new method called SuperDiff for combining multiple pre-trained diffusion models without the need for extensive re-training. The authors derive a theoretical framework based on the continuity equation and propose two algorithms that efficiently merge these models during the generation phase. SuperDiff utilizes a scalable Itô density estimator to calculate log likelihoods, ensuring minimal computational overhead. The method is shown to enhance image diversity, improve prompt-based image editing, and aid in protein structure design, demonstrating its practical applications in various domains.'}, 'zh': {'title': '高效结合预训练扩散模型的新方法', 'desc': '本文提出了一种新的框架，称为超叠加（superposition），用于在生成阶段结合多个预训练的扩散模型，而无需重新训练更大的模型。我们从连续性方程出发，理论上推导出超叠加，并设计了两种新算法，专门用于在SuperDiff中结合扩散模型。SuperDiff利用了一种新的可扩展Itô密度估计器，能够高效计算扩散随机微分方程的对数似然，而不会增加额外的计算负担。通过自动重加权方案，SuperDiff能够轻松实现不同预训练向量场的组合，并在推理时表现出高效性，能够生成更具多样性的图像和更准确的图像编辑。'}}}, {'id': 'https://huggingface.co/papers/2412.19712', 'title': 'From Elements to Design: A Layered Approach for Automatic Graphic Design Composition', 'url': 'https://huggingface.co/papers/2412.19712', 'abstract': 'In this work, we investigate automatic design composition from multimodal graphic elements. Although recent studies have developed various generative models for graphic design, they usually face the following limitations: they only focus on certain subtasks and are far from achieving the design composition task; they do not consider the hierarchical information of graphic designs during the generation process. To tackle these issues, we introduce the layered design principle into Large Multimodal Models (LMMs) and propose a novel approach, called LaDeCo, to accomplish this challenging task. Specifically, LaDeCo first performs layer planning for a given element set, dividing the input elements into different semantic layers according to their contents. Based on the planning results, it subsequently predicts element attributes that control the design composition in a layer-wise manner, and includes the rendered image of previously generated layers into the context. With this insightful design, LaDeCo decomposes the difficult task into smaller manageable steps, making the generation process smoother and clearer. The experimental results demonstrate the effectiveness of LaDeCo in design composition. Furthermore, we show that LaDeCo enables some interesting applications in graphic design, such as resolution adjustment, element filling, design variation, etc. In addition, it even outperforms the specialized models in some design subtasks without any task-specific training.', 'score': 6, 'issue_id': 1383, 'pub_date': '2024-12-27', 'pub_date_card': {'ru': '27 декабря', 'en': 'December 27', 'zh': '12月27日'}, 'hash': '389e5dbbfa107ed6', 'authors': ['Jiawei Lin', 'Shizhao Sun', 'Danqing Huang', 'Ting Liu', 'Ji Li', 'Jiang Bian'], 'affiliations': ['Microsoft Research', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2412.19712.jpg', 'data': {'categories': ['#cv', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Послойный подход к автоматической композиции дизайна с помощью больших мультимодальных моделей', 'desc': 'В данной работе исследуется автоматическое создание дизайн-композиций из мультимодальных графических элементов с использованием больших мультимодальных моделей (LMM). Предложенный подход LaDeCo вводит принцип послойного дизайна, разделяя элементы на семантические слои и предсказывая их атрибуты. Это позволяет разбить сложную задачу на более управляемые этапы, делая процесс генерации более плавным и понятным. Эксперименты показывают эффективность LaDeCo в создании дизайн-композиций и его применимость для различных задач графического дизайна.'}, 'en': {'title': 'Layered Design for Smarter Graphic Composition', 'desc': 'This paper presents LaDeCo, a novel approach for automatic design composition using Large Multimodal Models (LMMs). It addresses limitations in existing generative models by incorporating a layered design principle, which organizes graphic elements into semantic layers. LaDeCo enhances the design generation process by predicting element attributes in a structured, layer-wise manner, allowing for better management of complex design tasks. Experimental results show that LaDeCo not only improves design composition but also outperforms specialized models in certain subtasks without requiring specific training.'}, 'zh': {'title': 'LaDeCo：智能图形设计的分层组合新方法', 'desc': '本研究探讨了从多模态图形元素自动设计组合的方法。尽管近期有多种生成模型用于图形设计，但它们通常只关注某些子任务，且未能有效处理设计组合任务。为了解决这些问题，我们将分层设计原则引入大型多模态模型（LMMs），并提出了一种新方法，称为LaDeCo。LaDeCo通过对输入元素进行层次规划，分离出不同语义层，从而使生成过程更加顺畅和清晰。'}}}, {'id': 'https://huggingface.co/papers/2412.19512', 'title': 'Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging', 'url': 'https://huggingface.co/papers/2412.19512', 'abstract': 'Fine-tuning large language models (LLMs) for downstream tasks is a widely adopted approach, but it often leads to safety degradation in safety-aligned LLMs. Currently, many solutions address this issue by incorporating additional safety data, which can be impractical in many cases. In this paper, we address the question: How can we improve downstream task performance while preserving safety in LLMs without relying on additional safety data? We propose a simple and effective method that maintains the inherent safety of LLMs while enhancing their downstream task performance: merging the weights of pre- and post-fine-tuned safety-aligned models. Experimental results across various downstream tasks, models, and merging methods demonstrate that this approach effectively mitigates safety degradation while improving downstream task performance, offering a practical solution for adapting safety-aligned LLMs.', 'score': 5, 'issue_id': 1389, 'pub_date': '2024-12-27', 'pub_date_card': {'ru': '27 декабря', 'en': 'December 27', 'zh': '12月27日'}, 'hash': '87b5eeead71b4993', 'authors': ['Hua Farn', 'Hsuan Su', 'Shachi H Kumar', 'Saurav Sahay', 'Shang-Tse Chen', 'Hung-yi Lee'], 'affiliations': ['Intel Lab', 'National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2412.19512.jpg', 'data': {'categories': ['#optimization', '#training', '#alignment'], 'emoji': '🔐', 'ru': {'title': 'Безопасная настройка языковых моделей без компромиссов', 'desc': 'Исследователи предлагают новый метод для улучшения производительности языковых моделей на конкретных задачах без ущерба для их безопасности. Метод заключается в объединении весов моделей до и после тонкой настройки, что позволяет сохранить исходные параметры безопасности. Эксперименты показали эффективность этого подхода для различных задач и моделей. Предложенный метод не требует дополнительных данных по безопасности, что делает его практичным решением для адаптации безопасных языковых моделей.'}, 'en': {'title': 'Enhancing Performance While Preserving Safety in LLMs', 'desc': 'This paper discusses the challenge of fine-tuning large language models (LLMs) for specific tasks without compromising their safety features. It highlights that traditional methods often require extra safety data, which can be difficult to obtain. The authors propose a novel technique that combines the weights of models before and after fine-tuning to enhance performance while preserving safety. Their experiments show that this merging method successfully reduces safety degradation and improves task performance, providing a practical solution for using safety-aligned LLMs.'}, 'zh': {'title': '提升下游任务性能，保障安全性！', 'desc': '本文探讨了在不依赖额外安全数据的情况下，如何提高大型语言模型（LLMs）在下游任务中的表现，同时保持其安全性。我们提出了一种简单有效的方法，通过合并预调优和后调优的安全对齐模型的权重，来增强下游任务的性能。实验结果表明，这种方法在多种下游任务和模型中有效减轻了安全性下降的问题。我们的研究为安全对齐的LLMs提供了一种实用的适应方案。'}}}, {'id': 'https://huggingface.co/papers/2412.19645', 'title': 'VideoMaker: Zero-shot Customized Video Generation with the Inherent Force of Video Diffusion Models', 'url': 'https://huggingface.co/papers/2412.19645', 'abstract': "Zero-shot customized video generation has gained significant attention due to its substantial application potential. Existing methods rely on additional models to extract and inject reference subject features, assuming that the Video Diffusion Model (VDM) alone is insufficient for zero-shot customized video generation. However, these methods often struggle to maintain consistent subject appearance due to suboptimal feature extraction and injection techniques. In this paper, we reveal that VDM inherently possesses the force to extract and inject subject features. Departing from previous heuristic approaches, we introduce a novel framework that leverages VDM's inherent force to enable high-quality zero-shot customized video generation. Specifically, for feature extraction, we directly input reference images into VDM and use its intrinsic feature extraction process, which not only provides fine-grained features but also significantly aligns with VDM's pre-trained knowledge. For feature injection, we devise an innovative bidirectional interaction between subject features and generated content through spatial self-attention within VDM, ensuring that VDM has better subject fidelity while maintaining the diversity of the generated video.Experiments on both customized human and object video generation validate the effectiveness of our framework.", 'score': 3, 'issue_id': 1383, 'pub_date': '2024-12-27', 'pub_date_card': {'ru': '27 декабря', 'en': 'December 27', 'zh': '12月27日'}, 'hash': '1728f5feb9ef107d', 'authors': ['Tao Wu', 'Yong Zhang', 'Xiaodong Cun', 'Zhongang Qi', 'Junfu Pu', 'Huanzhang Dou', 'Guangcong Zheng', 'Ying Shan', 'Xi Li'], 'affiliations': ['ARC Lab, Tencent PCG', 'College of Computer Science and Technology, Zhejiang University', 'Huawei Noahs Ark Lab', 'Tencent AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2412.19645.jpg', 'data': {'categories': ['#cv', '#video', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Раскрытие потенциала VDM для персонализированной генерации видео', 'desc': 'Статья представляет новый подход к генерации персонализированных видео с нулевым обучением. Авторы предлагают использовать внутренние возможности Видео Диффузионной Модели (VDM) для извлечения и внедрения признаков объекта, в отличие от предыдущих методов, использующих дополнительные модели. Они вводят новую архитектуру, которая напрямую подает эталонные изображения в VDM и использует двунаправленное взаимодействие через пространственное самовнимание. Эксперименты подтверждают эффективность предложенного метода для генерации персонализированных видео с людьми и объектами.'}, 'en': {'title': 'Harnessing VDM for High-Quality Zero-Shot Video Generation', 'desc': 'This paper addresses the challenge of zero-shot customized video generation, which is the ability to create videos featuring specific subjects without prior training on those subjects. The authors argue that existing methods rely too heavily on external models for feature extraction and injection, leading to inconsistencies in subject appearance. They propose a new framework that utilizes the Video Diffusion Model (VDM) itself for both extracting and injecting subject features, enhancing the quality of the generated videos. Their approach includes a novel bidirectional interaction mechanism that improves subject fidelity while preserving the diversity of the output, as demonstrated through experiments with human and object video generation.'}, 'zh': {'title': '利用VDM实现高质量零-shot定制视频生成', 'desc': '零-shot定制视频生成因其广泛的应用潜力而受到关注。现有方法依赖额外模型提取和注入参考主体特征，假设视频扩散模型（VDM）单独无法实现零-shot定制视频生成。然而，这些方法在保持主体外观一致性方面常常面临挑战。本文提出了一种新框架，利用VDM的内在特性，实现高质量的零-shot定制视频生成。'}}}, {'id': 'https://huggingface.co/papers/2412.17606', 'title': 'SBS Figures: Pre-training Figure QA from Stage-by-Stage Synthesized Images', 'url': 'https://huggingface.co/papers/2412.17606', 'abstract': 'Building a large-scale figure QA dataset requires a considerable amount of work, from gathering and selecting figures to extracting attributes like text, numbers, and colors, and generating QAs. Although recent developments in LLMs have led to efforts to synthesize figures, most of these focus primarily on QA generation. Additionally, creating figures directly using LLMs often encounters issues such as code errors, similar-looking figures, and repetitive content in figures. To address this issue, we present SBSFigures (Stage-by-Stage Synthetic Figures), a dataset for pre-training figure QA. Our proposed pipeline enables the creation of chart figures with complete annotations of the visualized data and dense QA annotations without any manual annotation process. Our stage-by-stage pipeline makes it possible to create diverse topic and appearance figures efficiently while minimizing code errors. Our SBSFigures demonstrate a strong pre-training effect, making it possible to achieve efficient training with a limited amount of real-world chart data starting from our pre-trained weights.', 'score': 2, 'issue_id': 1385, 'pub_date': '2024-12-23', 'pub_date_card': {'ru': '23 декабря', 'en': 'December 23', 'zh': '12月23日'}, 'hash': '71e72ba060f5d9bd', 'authors': ['Risa Shinoda', 'Kuniaki Saito', 'Shohei Tanaka', 'Tosho Hirasawa', 'Yoshitaka Ushiku'], 'affiliations': ['Kyoto University, Japan', 'OMRON SINIC Corp., Japan'], 'pdf_title_img': 'assets/pdf/title_img/2412.17606.jpg', 'data': {'categories': ['#training', '#dataset', '#optimization', '#data', '#synthetic'], 'emoji': '📊', 'ru': {'title': 'Автоматическое создание датасета для обучения ИИ анализу графиков', 'desc': 'Статья представляет SBSFigures - набор данных для предобучения моделей ответов на вопросы по изображениям. Авторы предлагают пайплайн для создания графиков с полными аннотациями визуализированных данных и плотными аннотациями вопросов-ответов без ручной разметки. Поэтапный подход позволяет эффективно создавать разнообразные графики, минимизируя ошибки в коде. SBSFigures демонстрирует сильный эффект предобучения, позволяя достичь эффективного обучения на ограниченном количестве реальных данных.'}, 'en': {'title': 'Automating Figure QA with SBSFigures: Efficient, Diverse, and Error-Free!', 'desc': 'This paper introduces SBSFigures, a new dataset designed for pre-training figure question answering (QA) systems. The dataset is generated through a novel stage-by-stage pipeline that automates the creation of chart figures with detailed annotations and dense QA pairs, eliminating the need for manual annotation. By addressing common issues in figure generation, such as code errors and repetitive content, SBSFigures allows for the efficient production of diverse figures. The results show that using this dataset for pre-training significantly enhances the performance of QA models, even when trained on a limited amount of real-world data.'}, 'zh': {'title': '阶段性合成图形：高效的图形问答预训练数据集', 'desc': '本文介绍了一种名为SBSFigures（阶段性合成图形）的数据集，用于预训练图形问答（QA）。该数据集通过一个阶段性流程，自动生成带有完整注释的图表，避免了手动标注的繁琐过程。与传统方法相比，SBSFigures能够高效地创建多样化的主题和外观图形，同时减少代码错误。通过使用我们的预训练权重，SBSFigures展示了强大的预训练效果，使得在有限的真实图表数据上实现高效训练成为可能。'}}}, {'id': 'https://huggingface.co/papers/2412.18702', 'title': 'CypherBench: Towards Precise Retrieval over Full-scale Modern Knowledge Graphs in the LLM Era', 'url': 'https://huggingface.co/papers/2412.18702', 'abstract': 'Retrieval from graph data is crucial for augmenting large language models (LLM) with both open-domain knowledge and private enterprise data, and it is also a key component in the recent GraphRAG system (edge et al., 2024). Despite decades of research on knowledge graphs and knowledge base question answering, leading LLM frameworks (e.g. Langchain and LlamaIndex) have only minimal support for retrieval from modern encyclopedic knowledge graphs like Wikidata. In this paper, we analyze the root cause and suggest that modern RDF knowledge graphs (e.g. Wikidata, Freebase) are less efficient for LLMs due to overly large schemas that far exceed the typical LLM context window, use of resource identifiers, overlapping relation types and lack of normalization. As a solution, we propose property graph views on top of the underlying RDF graph that can be efficiently queried by LLMs using Cypher. We instantiated this idea on Wikidata and introduced CypherBench, the first benchmark with 11 large-scale, multi-domain property graphs with 7.8 million entities and over 10,000 questions. To achieve this, we tackled several key challenges, including developing an RDF-to-property graph conversion engine, creating a systematic pipeline for text-to-Cypher task generation, and designing new evaluation metrics.', 'score': 1, 'issue_id': 1398, 'pub_date': '2024-12-24', 'pub_date_card': {'ru': '24 декабря', 'en': 'December 24', 'zh': '12月24日'}, 'hash': 'd404a0a9acade186', 'authors': ['Yanlin Feng', 'Simone Papicchio', 'Sajjadur Rahman'], 'affiliations': ['Megagon Labs', 'Politecnico di Torino'], 'pdf_title_img': 'assets/pdf/title_img/2412.18702.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#dataset', '#graphs', '#rag'], 'emoji': '🕸️', 'ru': {'title': 'Графы свойств: эффективный доступ к знаниям для LLM', 'desc': 'Статья посвящена проблеме извлечения данных из графовых баз знаний для улучшения работы больших языковых моделей (LLM). Авторы анализируют причины неэффективности современных RDF-графов для LLM и предлагают использовать представления в виде графов свойств поверх RDF. Они создали CypherBench - первый бенчмарк с крупномасштабными мультидоменными графами свойств на основе Wikidata. В работе решены ключевые задачи, включая разработку конвертера RDF в граф свойств и создание пайплайна для генерации задач text-to-Cypher.'}, 'en': {'title': 'Optimizing Knowledge Retrieval for LLMs with Property Graphs', 'desc': 'This paper addresses the challenges of retrieving information from knowledge graphs to enhance large language models (LLMs). It identifies inefficiencies in modern RDF knowledge graphs, such as complex schemas and overlapping relation types, which hinder effective querying by LLMs. The authors propose using property graph views to simplify and optimize the querying process, allowing LLMs to access knowledge more efficiently. They introduce CypherBench, a benchmark for evaluating this approach with large-scale property graphs and a systematic pipeline for generating queries.'}, 'zh': {'title': '提升LLM检索效率的创新方法', 'desc': '本论文探讨了如何从图数据中检索信息，以增强大型语言模型（LLM）的知识获取能力。我们发现，现代RDF知识图谱（如Wikidata）由于其庞大的模式和复杂的关系类型，导致LLM的检索效率低下。为了解决这个问题，我们提出了在RDF图上构建属性图视图的方法，使得LLM能够更高效地使用Cypher查询。我们还开发了CypherBench，这是第一个包含多个领域的大规模属性图基准，旨在推动这一领域的研究。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents', '#agi', '#alignment (1)', '#architecture (1)', '#audio', '#benchmark (3)', '#cv (5)', '#data (2)', '#dataset (4)', '#diffusion (2)', '#ethics', '#games (1)', '#graphs (1)', '#hallucinations', '#healthcare (1)', '#inference (2)', '#interpretability', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (3)', '#open_source (1)', '#optimization (5)', '#plp', '#rag (1)', '#reasoning (1)', '#rl (1)', '#rlhf', '#robotics', '#science', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic (2)', '#training (5)', '#transfer_learning (1)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-12-30 21:09',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-12-30 21:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-12-30 21:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    