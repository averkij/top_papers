
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 10 papers. December 23.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ</span> | <span id="title-articles-count">10 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2024-12-20.html">â¬…ï¸ <span id="prev-date">20.12</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-12-24.html">â¡ï¸ <span id="next-date">24.12</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-12.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 23', 'zh': '12æœˆ23æ—¥'};
        let feedDateNext = {'ru': '24.12', 'en': '12/24', 'zh': '12æœˆ24æ—¥'};
        let feedDatePrev = {'ru': '20.12', 'en': '12/20', 'zh': '12æœˆ20æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2412.15119', 'title': 'Parallelized Autoregressive Visual Generation', 'url': 'https://huggingface.co/papers/2412.15119', 'abstract': 'Autoregressive models have emerged as a powerful approach for visual generation but suffer from slow inference speed due to their sequential token-by-token prediction process. In this paper, we propose a simple yet effective approach for parallelized autoregressive visual generation that improves generation efficiency while preserving the advantages of autoregressive modeling. Our key insight is that parallel generation depends on visual token dependencies-tokens with weak dependencies can be generated in parallel, while strongly dependent adjacent tokens are difficult to generate together, as their independent sampling may lead to inconsistencies. Based on this observation, we develop a parallel generation strategy that generates distant tokens with weak dependencies in parallel while maintaining sequential generation for strongly dependent local tokens. Our approach can be seamlessly integrated into standard autoregressive models without modifying the architecture or tokenizer. Experiments on ImageNet and UCF-101 demonstrate that our method achieves a 3.6x speedup with comparable quality and up to 9.5x speedup with minimal quality degradation across both image and video generation tasks. We hope this work will inspire future research in efficient visual generation and unified autoregressive modeling. Project page: https://epiphqny.github.io/PAR-project.', 'score': 29, 'issue_id': 1258, 'pub_date': '2024-12-19', 'pub_date_card': {'ru': '19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 19', 'zh': '12æœˆ19æ—¥'}, 'hash': '0933582baa02f7a6', 'authors': ['Yuqing Wang', 'Shuhuai Ren', 'Zhijie Lin', 'Yujin Han', 'Haoyuan Guo', 'Zhenheng Yang', 'Difan Zou', 'Jiashi Feng', 'Xihui Liu'], 'affiliations': ['ByteDance Seed', 'Peking University', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.15119.jpg', 'data': {'categories': ['#training', '#inference', '#video', '#cv', '#architecture', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ ÑĞ»Ğ°Ğ±Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾, Ğ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸Ğ»Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 3.6 Ñ€Ğ°Ğ· Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ğ´Ğ¾ 9.5 Ñ€Ğ°Ğ· Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸ĞµĞ¹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Speeding Up Visual Generation with Smart Token Parallelization', 'desc': 'This paper addresses the slow inference speed of autoregressive models used for visual generation, which typically generate images or videos one token at a time. The authors propose a new method that allows for parallel generation of visual tokens, focusing on the dependencies between tokens to determine which can be generated simultaneously. By identifying weakly dependent tokens that can be generated in parallel, while keeping strongly dependent tokens in a sequential order, the method enhances efficiency without altering the existing model architecture. Experiments show significant speed improvements in generating images and videos, making this approach a promising direction for future research in visual generation.'}, 'zh': {'title': 'å¹¶è¡Œè‡ªå›å½’ç”Ÿæˆï¼Œæå‡è§†è§‰ç”Ÿæˆæ•ˆç‡', 'desc': 'è‡ªå›å½’æ¨¡å‹åœ¨è§†è§‰ç”Ÿæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºé€ä¸ªé¢„æµ‹çš„è¿‡ç¨‹ï¼Œæ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å¹¶è¡Œè‡ªå›å½’è§†è§‰ç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜ç”Ÿæˆæ•ˆç‡ï¼ŒåŒæ—¶ä¿ç•™è‡ªå›å½’å»ºæ¨¡çš„ä¼˜ç‚¹ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œè§†è§‰æ ‡è®°ä¹‹é—´çš„ä¾èµ–å…³ç³»å†³å®šäº†å¹¶è¡Œç”Ÿæˆçš„å¯èƒ½æ€§ï¼Œå¼±ä¾èµ–çš„æ ‡è®°å¯ä»¥å¹¶è¡Œç”Ÿæˆï¼Œè€Œå¼ºä¾èµ–çš„æ ‡è®°åˆ™éš¾ä»¥ä¸€èµ·ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†3.6å€çš„é€Ÿåº¦æå‡ï¼Œä¸”è´¨é‡ä¿æŒç›¸å½“ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹å¯è¾¾åˆ°9.5å€çš„é€Ÿåº¦æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.16145', 'title': 'Offline Reinforcement Learning for LLM Multi-Step Reasoning', 'url': 'https://huggingface.co/papers/2412.16145', 'abstract': 'Improving the multi-step reasoning ability of large language models (LLMs) with offline reinforcement learning (RL) is essential for quickly adapting them to complex tasks. While Direct Preference Optimization (DPO) has shown promise in aligning LLMs with human preferences, it is less suitable for multi-step reasoning tasks because (1) DPO relies on paired preference data, which is not readily available for multi-step reasoning tasks, and (2) it treats all tokens uniformly, making it ineffective for credit assignment in multi-step reasoning tasks, which often come with sparse reward. In this work, we propose OREO (Offline Reasoning Optimization), an offline RL method for enhancing LLM multi-step reasoning. Building on insights from previous works of maximum entropy reinforcement learning, it jointly learns a policy model and value function by optimizing the soft Bellman Equation. We show in principle that it reduces the need to collect pairwise data and enables better credit assignment. Empirically, OREO surpasses existing offline learning methods on multi-step reasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH) and embodied agent control (ALFWorld). The approach can be extended to a multi-iteration framework when additional resources are available. Furthermore, the learned value function can be leveraged to guide the tree search for free, which can further boost performance during test time.', 'score': 15, 'issue_id': 1260, 'pub_date': '2024-12-20', 'pub_date_card': {'ru': '20 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 20', 'zh': '12æœˆ20æ—¥'}, 'hash': '5779a845f782fb45', 'authors': ['Huaijie Wang', 'Shibo Hao', 'Hanze Dong', 'Shenao Zhang', 'Yilin Bao', 'Ziran Yang', 'Yi Wu'], 'affiliations': ['Northwestern University', 'Salesforce Research', 'Tsinghua University', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2412.16145.jpg', 'data': {'categories': ['#benchmark', '#rl', '#optimization', '#math', '#rlhf', '#agents', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'OREO: ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ OREO (Offline Reasoning Optimization) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ. OREO ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¼ÑĞ³ĞºĞ¾Ğµ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ‘ĞµĞ»Ğ»Ğ¼Ğ°Ğ½Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. OREO Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½ Ğ´Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½ Ğ´Ğ»Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'OREO: Enhancing Multi-Step Reasoning in LLMs with Offline RL', 'desc': "This paper introduces OREO, an offline reinforcement learning method designed to improve the multi-step reasoning capabilities of large language models (LLMs). Unlike Direct Preference Optimization, which struggles with multi-step tasks due to its reliance on paired preference data and uniform token treatment, OREO effectively addresses these challenges by optimizing the soft Bellman Equation. The method enhances credit assignment and reduces the need for extensive data collection, leading to superior performance on reasoning benchmarks. Additionally, OREO's learned value function can be utilized to enhance search strategies during testing, further improving outcomes."}, 'zh': {'title': 'OREOï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ­¥æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºOREOçš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚ä¸ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä¸åŒï¼ŒOREOä¸ä¾èµ–äºæˆå¯¹çš„åå¥½æ•°æ®ï¼Œé€‚ç”¨äºå¤šæ­¥æ¨ç†ä»»åŠ¡ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–è½¯è´å°”æ›¼æ–¹ç¨‹ï¼Œè”åˆå­¦ä¹ ç­–ç•¥æ¨¡å‹å’Œä»·å€¼å‡½æ•°ï¼Œä»è€Œæ”¹å–„äº†å¥–åŠ±ç¨€ç–æƒ…å†µä¸‹çš„ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOREOåœ¨æ•°å­¦æ¨ç†å’Œæ™ºèƒ½ä½“æ§åˆ¶ç­‰å¤šæ­¥æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰çš„ç¦»çº¿å­¦ä¹ æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.13649', 'title': 'SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation', 'url': 'https://huggingface.co/papers/2412.13649', 'abstract': 'Key-Value (KV) cache has become a bottleneck of LLMs for long-context generation. Despite the numerous efforts in this area, the optimization for the decoding phase is generally ignored. However, we believe such optimization is crucial, especially for long-output generation tasks based on the following two observations: (i) Excessive compression during the prefill phase, which requires specific full context impairs the comprehension of the reasoning task; (ii) Deviation of heavy hitters occurs in the reasoning tasks with long outputs. Therefore, SCOPE, a simple yet efficient framework that separately performs KV cache optimization during the prefill and decoding phases, is introduced. Specifically, the KV cache during the prefill phase is preserved to maintain the essential information, while a novel strategy based on sliding is proposed to select essential heavy hitters for the decoding phase. Memory usage and memory transfer are further optimized using adaptive and discontinuous strategies. Extensive experiments on LongGenBench show the effectiveness and generalization of SCOPE and its compatibility as a plug-in to other prefill-only KV compression methods.', 'score': 14, 'issue_id': 1258, 'pub_date': '2024-12-18', 'pub_date_card': {'ru': '18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 18', 'zh': '12æœˆ18æ—¥'}, 'hash': '885d11532659dd95', 'authors': ['Jialong Wu', 'Zhenglin Wang', 'Linhai Zhang', 'Yilong Lai', 'Yulan He', 'Deyu Zhou'], 'affiliations': ['Department of Informatics, Kings College London, UK', 'School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China', 'The Alan Turing Institute, UK'], 'pdf_title_img': 'assets/pdf/title_img/2412.13649.jpg', 'data': {'categories': ['#training', '#long_context', '#inference', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ KV-ĞºÑÑˆĞ° Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ² LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SCOPE - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ KV-ĞºÑÑˆĞ° Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… LLM Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² prefill Ğ¸ decoding, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ğ°Ğ¶Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ¸ÑÑ‚Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° LongGenBench Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ SCOPE, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ĞµĞ³Ğ¾ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ KV-ĞºÑÑˆĞ°.'}, 'en': {'title': 'Optimizing KV Caches for Enhanced Long-Context Generation', 'desc': 'This paper addresses the limitations of Key-Value (KV) caches in large language models (LLMs) when generating long outputs. It highlights that optimizing the decoding phase is essential, as excessive compression during the prefill phase can hinder reasoning tasks. The proposed framework, SCOPE, optimizes KV cache usage by preserving crucial information during prefill and employing a sliding strategy to select important data during decoding. Experimental results demonstrate that SCOPE improves memory efficiency and can be integrated with existing KV compression methods.'}, 'zh': {'title': 'ä¼˜åŒ–KVç¼“å­˜ï¼Œæå‡é•¿è¾“å‡ºç”Ÿæˆæ•ˆç‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSCOPEçš„æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–é•¿è¾“å‡ºç”Ÿæˆä»»åŠ¡ä¸­çš„KVç¼“å­˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨é¢„å¡«å……é˜¶æ®µè¿‡åº¦å‹ç¼©ä¼šå½±å“æ¨ç†ä»»åŠ¡çš„ç†è§£ï¼Œå› æ­¤éœ€è¦ä¿ç•™å…³é”®ä¿¡æ¯ã€‚SCOPEé€šè¿‡åœ¨é¢„å¡«å……å’Œè§£ç é˜¶æ®µåˆ†åˆ«ä¼˜åŒ–KVç¼“å­˜ï¼Œé‡‡ç”¨æ»‘åŠ¨ç­–ç•¥é€‰æ‹©é‡è¦çš„é‡å‡»é¡¹ï¼Œä»è€Œæé«˜è§£ç æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSCOPEåœ¨LongGenBenchä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”å¯ä»¥ä½œä¸ºå…¶ä»–KVå‹ç¼©æ–¹æ³•çš„æ’ä»¶ä½¿ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.16112', 'title': 'CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up', 'url': 'https://huggingface.co/papers/2412.16112', 'abstract': 'Diffusion Transformers (DiT) have become a leading architecture in image generation. However, the quadratic complexity of attention mechanisms, which are responsible for modeling token-wise relationships, results in significant latency when generating high-resolution images. To address this issue, we aim at a linear attention mechanism in this paper that reduces the complexity of pre-trained DiTs to linear. We begin our exploration with a comprehensive summary of existing efficient attention mechanisms and identify four key factors crucial for successful linearization of pre-trained DiTs: locality, formulation consistency, high-rank attention maps, and feature integrity. Based on these insights, we introduce a convolution-like local attention strategy termed CLEAR, which limits feature interactions to a local window around each query token, and thus achieves linear complexity. Our experiments indicate that, by fine-tuning the attention layer on merely 10K self-generated samples for 10K iterations, we can effectively transfer knowledge from a pre-trained DiT to a student model with linear complexity, yielding results comparable to the teacher model. Simultaneously, it reduces attention computations by 99.5% and accelerates generation by 6.3 times for generating 8K-resolution images. Furthermore, we investigate favorable properties in the distilled attention layers, such as zero-shot generalization cross various models and plugins, and improved support for multi-GPU parallel inference. Models and codes are available here: https://github.com/Huage001/CLEAR.', 'score': 9, 'issue_id': 1259, 'pub_date': '2024-12-20', 'pub_date_card': {'ru': '20 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 20', 'zh': '12æœˆ20æ—¥'}, 'hash': 'c17ca50dc03ea86c', 'authors': ['Songhua Liu', 'Zhenxiong Tan', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2412.16112.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#training', '#inference', '#architecture', '#diffusion'], 'emoji': 'ğŸš€', 'ru': {'title': 'CLEAR: Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ DiT Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Diffusion Transformers (DiT), Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ CLEAR. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… DiT Ñ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¾ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. CLEAR Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğµ ÑĞ²Ñ‘Ñ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸ÑĞ¼, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾ĞºĞ½Ğ¾Ğ¼ Ğ²Ğ¾ĞºÑ€ÑƒĞ³ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ fine-tuning ÑĞ»Ğ¾Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ²ÑĞµĞ³Ğ¾ 10 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'Linearizing Attention for Faster Image Generation with CLEAR', 'desc': 'This paper presents a new approach to improve the efficiency of Diffusion Transformers (DiT) in image generation by introducing a linear attention mechanism. The authors identify key factors necessary for linearizing DiTs, such as locality and feature integrity, and propose a local attention strategy called CLEAR. This method significantly reduces the computational complexity of attention mechanisms, achieving a 99.5% reduction in attention computations and a 6.3 times speedup in generating high-resolution images. Additionally, the study shows that the distilled model retains performance comparable to the original DiT while enabling better generalization and multi-GPU support.'}, 'zh': {'title': 'çº¿æ€§æ³¨æ„åŠ›ï¼Œå¿«é€Ÿç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ—¨åœ¨è§£å†³æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰åœ¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒæ—¶çš„å»¶è¿Ÿé—®é¢˜ã€‚æˆ‘ä»¬æ€»ç»“äº†ç°æœ‰çš„é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶ç¡®å®šäº†æˆåŠŸçº¿æ€§åŒ–é¢„è®­ç»ƒDiTçš„å››ä¸ªå…³é”®å› ç´ ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åä¸ºCLEARçš„å±€éƒ¨æ³¨æ„åŠ›ç­–ç•¥ï¼Œé™åˆ¶ç‰¹å¾äº¤äº’åœ¨æ¯ä¸ªæŸ¥è¯¢æ ‡è®°å‘¨å›´çš„å±€éƒ¨çª—å£å†…ï¼Œä»è€Œå®ç°çº¿æ€§å¤æ‚åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡å¯¹æ³¨æ„åŠ›å±‚è¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬å¯ä»¥æœ‰æ•ˆåœ°å°†çŸ¥è¯†ä»é¢„è®­ç»ƒçš„DiTè½¬ç§»åˆ°å­¦ç”Ÿæ¨¡å‹ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘è®¡ç®—é‡å¹¶åŠ é€Ÿç”Ÿæˆè¿‡ç¨‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.15322', 'title': 'Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis', 'url': 'https://huggingface.co/papers/2412.15322', 'abstract': 'We propose to synthesize high-quality and synchronized audio, given video and optional text conditions, using a novel multimodal joint training framework MMAudio. In contrast to single-modality training conditioned on (limited) video data only, MMAudio is jointly trained with larger-scale, readily available text-audio data to learn to generate semantically aligned high-quality audio samples. Additionally, we improve audio-visual synchrony with a conditional synchronization module that aligns video conditions with audio latents at the frame level. Trained with a flow matching objective, MMAudio achieves new video-to-audio state-of-the-art among public models in terms of audio quality, semantic alignment, and audio-visual synchronization, while having a low inference time (1.23s to generate an 8s clip) and just 157M parameters. MMAudio also achieves surprisingly competitive performance in text-to-audio generation, showing that joint training does not hinder single-modality performance. Code and demo are available at: https://hkchengrex.github.io/MMAudio', 'score': 8, 'issue_id': 1258, 'pub_date': '2024-12-19', 'pub_date_card': {'ru': '19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 19', 'zh': '12æœˆ19æ—¥'}, 'hash': '9d724184f50de930', 'authors': ['Ho Kei Cheng', 'Masato Ishii', 'Akio Hayakawa', 'Takashi Shibuya', 'Alexander Schwing', 'Yuki Mitsufuji'], 'affiliations': ['Sony AI', 'Sony Group Corporation', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2412.15322.jpg', 'data': {'categories': ['#small_models', '#audio', '#inference', '#video', '#multimodal', '#synthetic'], 'emoji': 'ğŸµ', 'ru': {'title': 'MMAudio: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ñƒ', 'desc': 'MMAudio - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ°ÑƒĞ´Ğ¸Ğ¾-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ·Ğ²ÑƒĞºĞ°. MMAudio Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ·Ğ²ÑƒĞºĞ°, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ.'}, 'en': {'title': 'MMAudio: High-Quality Audio Synthesis with Video and Text Integration', 'desc': 'This paper introduces MMAudio, a novel framework for generating high-quality audio that is synchronized with video and can also utilize text inputs. Unlike traditional methods that rely solely on video data, MMAudio leverages a larger dataset of text-audio pairs to enhance the semantic alignment of the generated audio. The framework includes a conditional synchronization module that ensures audio is aligned with video at the frame level, improving overall coherence. With a flow matching objective, MMAudio sets new benchmarks in audio quality and synchronization while maintaining efficient performance with a low number of parameters.'}, 'zh': {'title': 'MMAudioï¼šé«˜è´¨é‡éŸ³é¢‘åˆæˆçš„æ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€è”åˆè®­ç»ƒæ¡†æ¶MMAudioï¼Œç”¨äºåˆæˆé«˜è´¨é‡å’ŒåŒæ­¥çš„éŸ³é¢‘ï¼ŒåŸºäºè§†é¢‘å’Œå¯é€‰çš„æ–‡æœ¬æ¡ä»¶ã€‚ä¸ä»…ä¾èµ–è§†é¢‘æ•°æ®çš„å•æ¨¡æ€è®­ç»ƒä¸åŒï¼ŒMMAudioç»“åˆäº†å¤§è§„æ¨¡çš„æ–‡æœ¬-éŸ³é¢‘æ•°æ®è¿›è¡Œè”åˆè®­ç»ƒï¼Œä»¥ç”Ÿæˆè¯­ä¹‰å¯¹é½çš„é«˜è´¨é‡éŸ³é¢‘æ ·æœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡æ¡ä»¶åŒæ­¥æ¨¡å—åœ¨å¸§çº§åˆ«ä¸Šå¯¹è§†é¢‘æ¡ä»¶å’ŒéŸ³é¢‘æ½œåœ¨ç‰¹å¾è¿›è¡Œå¯¹é½ï¼Œä»è€Œæé«˜éŸ³é¢‘ä¸è§†é¢‘çš„åŒæ­¥æ€§ã€‚MMAudioåœ¨éŸ³é¢‘è´¨é‡ã€è¯­ä¹‰å¯¹é½å’ŒéŸ³é¢‘-è§†è§‰åŒæ­¥æ–¹é¢è¾¾åˆ°äº†æ–°çš„å…¬å…±æ¨¡å‹çš„æœ€ä½³æ°´å¹³ï¼ŒåŒæ—¶æ¨ç†æ—¶é—´ä½ï¼ˆç”Ÿæˆ8ç§’ç‰‡æ®µä»…éœ€1.23ç§’ï¼‰ï¼Œå‚æ•°é‡ä»…ä¸º157Mã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.14590', 'title': 'MixLLM: LLM Quantization with Global Mixed-precision between Output-features and Highly-efficient System Design', 'url': 'https://huggingface.co/papers/2412.14590', 'abstract': 'Quantization has become one of the most effective methodologies to compress LLMs into smaller size. However, the existing quantization solutions still show limitations of either non-negligible accuracy drop or system inefficiency. In this paper, we make a comprehensive analysis of the general quantization principles on their effect to the triangle of accuracy, memory consumption and system efficiency. We propose MixLLM that explores the new optimization space of mixed-precision quantization between output features based on the insight that different output features matter differently in the model. MixLLM identifies the output features with high salience in the global view rather than within each single layer, effectively assigning the larger bit-width to output features that need it most to achieve good accuracy with low memory consumption. We present the sweet spot of quantization configuration of algorithm-system co-design that leads to high accuracy and system efficiency. To address the system challenge, we design the two-step dequantization to make use of the int8 Tensor Core easily and fast data type conversion to reduce dequantization overhead significantly, and present the software pipeline to overlap the memory access, dequantization and the MatMul to the best. Extensive experiments show that with only 10% more bits, the PPL increasement can be reduced from about 0.5 in SOTA to within 0.2 for Llama 3.1 70B, while on average MMLU-Pro improves by 0.93 over the SOTA of three popular models. In addition to its superior accuracy, MixLLM also achieves state-of-the-art system efficiency.', 'score': 4, 'issue_id': 1260, 'pub_date': '2024-12-19', 'pub_date_card': {'ru': '19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 19', 'zh': '12æœˆ19æ—¥'}, 'hash': '3a5b6d590eec2c6e', 'authors': ['Zhen Zheng', 'Xiaonan Song', 'Chuanjie Liu'], 'affiliations': ['Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2412.14590.jpg', 'data': {'categories': ['#optimization', '#inference', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MixLLM: ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ MixLLM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑÑŒ Ğ½Ğ° Ğ¸Ñ… Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. MixLLM Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MixLLM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'MixLLM: Optimizing Quantization for Accuracy and Efficiency in LLMs', 'desc': 'This paper discusses a new method called MixLLM for quantizing large language models (LLMs) to make them smaller and more efficient. The authors analyze how different quantization techniques affect accuracy, memory use, and system performance. MixLLM uses mixed-precision quantization, assigning more bits to important output features, which helps maintain accuracy while reducing memory consumption. The proposed method also includes a two-step dequantization process to improve speed and efficiency, resulting in better performance compared to existing solutions.'}, 'zh': {'title': 'MixLLMï¼šé«˜æ•ˆçš„æ··åˆç²¾åº¦é‡åŒ–æ–¹æ¡ˆ', 'desc': 'é‡åŒ–æŠ€æœ¯å·²æˆä¸ºå‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¸€ç§æœ‰æ•ˆæ–¹æ³•ï¼Œä½†ç°æœ‰çš„é‡åŒ–æ–¹æ¡ˆåœ¨å‡†ç¡®æ€§å’Œç³»ç»Ÿæ•ˆç‡ä¸Šä»å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡å¯¹é‡åŒ–åŸåˆ™è¿›è¡Œäº†å…¨é¢åˆ†æï¼Œæ¢è®¨äº†å‡†ç¡®æ€§ã€å†…å­˜æ¶ˆè€—å’Œç³»ç»Ÿæ•ˆç‡ä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬æå‡ºäº†MixLLMï¼Œåˆ©ç”¨æ··åˆç²¾åº¦é‡åŒ–ä¼˜åŒ–è¾“å‡ºç‰¹å¾ï¼Œç¡®ä¿é‡è¦ç‰¹å¾è·å¾—æ›´é«˜çš„ä½å®½ï¼Œä»è€Œåœ¨ä¿æŒè‰¯å¥½å‡†ç¡®æ€§çš„åŒæ—¶é™ä½å†…å­˜æ¶ˆè€—ã€‚é€šè¿‡è®¾è®¡ä¸¤æ­¥åé‡åŒ–å’Œä¼˜åŒ–è½¯ä»¶ç®¡é“ï¼ŒMixLLMåœ¨å‡†ç¡®æ€§å’Œç³»ç»Ÿæ•ˆç‡ä¸Šéƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.15035', 'title': 'LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps', 'url': 'https://huggingface.co/papers/2412.15035', 'abstract': 'Building safe Large Language Models (LLMs) across multiple languages is essential in ensuring both safe access and linguistic diversity. To this end, we introduce M-ALERT, a multilingual benchmark that evaluates the safety of LLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT includes 15k high-quality prompts per language, totaling 75k, following the detailed ALERT taxonomy. Our extensive experiments on 10 state-of-the-art LLMs highlight the importance of language-specific safety analysis, revealing that models often exhibit significant inconsistencies in safety across languages and categories. For instance, Llama3.2 shows high unsafety in the category crime_tax for Italian but remains safe in other languages. Similar differences can be observed across all models. In contrast, certain categories, such as substance_cannabis and crime_propaganda, consistently trigger unsafe responses across models and languages. These findings underscore the need for robust multilingual safety practices in LLMs to ensure safe and responsible usage across diverse user communities.', 'score': 2, 'issue_id': 1265, 'pub_date': '2024-12-19', 'pub_date_card': {'ru': '19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 19', 'zh': '12æœˆ19æ—¥'}, 'hash': 'ec88d82d8720bf3d', 'authors': ['Felix Friedrich', 'Simone Tedeschi', 'Patrick Schramowski', 'Manuel Brack', 'Roberto Navigli', 'Huu Nguyen', 'Bo Li', 'Kristian Kersting'], 'affiliations': ['CERTAIN', 'DFKI', 'Hessian.AI', 'Ontocord.AI', 'Sapienza University of Rome', 'TU Darmstadt', 'UIUC', 'University of Chicago', 'Virtue.ai'], 'pdf_title_img': 'assets/pdf/title_img/2412.15035.jpg', 'data': {'categories': ['#low_resource', '#multilingual', '#ethics', '#benchmark'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ LLM: Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ M-ALERT - Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 10 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸ Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸. ĞĞ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Llama3.2 Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ½ĞµĞ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ¿Ñ€ĞµÑÑ‚ÑƒĞ¿Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ñ‚Ğ°Ğ»ÑŒÑĞ½ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ¾ÑÑ‚Ğ°Ğ²Ğ°ÑÑÑŒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ¹ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ğº Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ LLM.'}, 'en': {'title': 'Ensuring Multilingual Safety in Large Language Models', 'desc': 'This paper presents M-ALERT, a multilingual benchmark designed to assess the safety of Large Language Models (LLMs) in five different languages. It includes a comprehensive set of 75,000 prompts categorized according to the ALERT taxonomy, allowing for detailed safety evaluations. The study reveals that LLMs often show varying levels of safety across languages, with some models performing poorly in specific categories for certain languages. These results highlight the necessity for tailored safety measures in LLMs to accommodate linguistic diversity and ensure responsible usage.'}, 'zh': {'title': 'æ„å»ºå®‰å…¨çš„å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹', 'desc': 'æœ¬æ–‡ä»‹ç»äº†M-ALERTï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šè¯­è¨€åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å®‰å…¨æ€§ã€‚è¯¥åŸºå‡†æ¶µç›–è‹±è¯­ã€æ³•è¯­ã€å¾·è¯­ã€æ„å¤§åˆ©è¯­å’Œè¥¿ç­ç‰™è¯­ï¼Œå…±åŒ…å«75,000ä¸ªé«˜è´¨é‡æç¤ºã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¸åŒè¯­è¨€å’Œç±»åˆ«ä¹‹é—´çš„å®‰å…¨æ€§å­˜åœ¨æ˜¾è‘—ä¸ä¸€è‡´ï¼ŒæŸäº›æ¨¡å‹åœ¨ç‰¹å®šè¯­è¨€ä¸­è¡¨ç°å‡ºé«˜é£é™©ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­å®æ–½å¼ºæœ‰åŠ›çš„å®‰å…¨å®è·µçš„é‡è¦æ€§ï¼Œä»¥ç¡®ä¿ç”¨æˆ·çš„å®‰å…¨å’Œè´Ÿè´£ä»»çš„ä½¿ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.11525', 'title': 'Sequence Matters: Harnessing Video Models in 3D Super-Resolution', 'url': 'https://huggingface.co/papers/2412.11525', 'abstract': "3D super-resolution aims to reconstruct high-fidelity 3D models from low-resolution (LR) multi-view images. Early studies primarily focused on single-image super-resolution (SISR) models to upsample LR images into high-resolution images. However, these methods often lack view consistency because they operate independently on each image. Although various post-processing techniques have been extensively explored to mitigate these inconsistencies, they have yet to fully resolve the issues. In this paper, we perform a comprehensive study of 3D super-resolution by leveraging video super-resolution (VSR) models. By utilizing VSR models, we ensure a higher degree of spatial consistency and can reference surrounding spatial information, leading to more accurate and detailed reconstructions. Our findings reveal that VSR models can perform remarkably well even on sequences that lack precise spatial alignment. Given this observation, we propose a simple yet practical approach to align LR images without involving fine-tuning or generating 'smooth' trajectory from the trained 3D models over LR images. The experimental results show that the surprisingly simple algorithms can achieve the state-of-the-art results of 3D super-resolution tasks on standard benchmark datasets, such as the NeRF-synthetic and MipNeRF-360 datasets. Project page: https://ko-lani.github.io/Sequence-Matters", 'score': 2, 'issue_id': 1265, 'pub_date': '2024-12-16', 'pub_date_card': {'ru': '16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 16', 'zh': '12æœˆ16æ—¥'}, 'hash': '39960ceb17dab1a5', 'authors': ['Hyun-kyu Ko', 'Dongheok Park', 'Youngin Park', 'Byeonghyeon Lee', 'Juhee Han', 'Eunbyung Park'], 'affiliations': ['Department of Artificial Intelligence, Sungkyunkwan University, Suwon, Korea', 'Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, Korea', 'Visual Display Division, Samsung Electronics'], 'pdf_title_img': 'assets/pdf/title_img/2412.11525.jpg', 'data': {'categories': ['#video', '#3d', '#benchmark'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ’Ğ¸Ğ´ĞµĞ¾-ÑÑƒĞ¿ĞµÑ€Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ² 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° 3D ÑÑƒĞ¿ĞµÑ€Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ - Ğ¼ĞµÑ‚Ğ¾Ğ´Ñƒ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… 3D Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ· Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑÑƒĞ¿ĞµÑ€Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ (VSR) Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑÑƒĞ¿ĞµÑ€Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing 3D Models with Video Super-Resolution Techniques', 'desc': 'This paper focuses on improving 3D super-resolution, which is the process of creating high-quality 3D models from low-resolution images taken from different angles. Traditional methods often struggle with maintaining consistency across different views because they treat each image separately. The authors propose using video super-resolution (VSR) techniques to enhance spatial consistency and leverage information from surrounding images, resulting in better 3D reconstructions. Their approach is simple and effective, achieving state-of-the-art results on well-known datasets without the need for complex fine-tuning.'}, 'zh': {'title': 'åˆ©ç”¨è§†é¢‘è¶…åˆ†è¾¨ç‡æå‡3Dé‡å»ºè´¨é‡', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†3Dè¶…åˆ†è¾¨ç‡æŠ€æœ¯ï¼Œæ—¨åœ¨ä»ä½åˆ†è¾¨ç‡çš„å¤šè§†å›¾å›¾åƒä¸­é‡å»ºé«˜ä¿çœŸåº¦çš„3Dæ¨¡å‹ã€‚ä¼ ç»Ÿçš„å•å›¾åƒè¶…åˆ†è¾¨ç‡(SISR)æ–¹æ³•åœ¨å¤„ç†æ¯å¼ å›¾åƒæ—¶ç¼ºä¹è§†å›¾ä¸€è‡´æ€§ï¼Œå¯¼è‡´é‡å»ºæ•ˆæœä¸ä½³ã€‚æˆ‘ä»¬æå‡ºåˆ©ç”¨è§†é¢‘è¶…åˆ†è¾¨ç‡(VSR)æ¨¡å‹æ¥æé«˜ç©ºé—´ä¸€è‡´æ€§ï¼Œå¹¶å‚è€ƒå‘¨å›´çš„ç©ºé—´ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®å’Œè¯¦ç»†çš„é‡å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ ‡å‡†åŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„3Dè¶…åˆ†è¾¨ç‡æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.15450', 'title': 'Fietje: An open, efficient LLM for Dutch', 'url': 'https://huggingface.co/papers/2412.15450', 'abstract': 'This paper introduces Fietje, a family of small language models (SLMs) specifically designed for the Dutch language. The model is based on Phi 2, an English-centric model of 2.7 billion parameters. Fietje demonstrated competitive results with larger language models upon its release. A core emphasis of this work is transparency and reproducibility: Fietje is fully open-source, with model weights, datasets, training, and evaluation code all publicly accessible.   The paper discusses the performance of Fietje and many other models on an extensive evaluation suite of benchmarks on reasoning, sentiment analysis, world knowledge, linguistic acceptability and word sense disambiguation. Evaluation results illustrate the rapid progress in the field of LLMs, where recent small models outperform older, larger models that were fine-tuned for Dutch. This trend signals an exciting future for Dutch language processing, suggesting that even compact LLMs are becoming increasingly capable. Furthermore, ongoing and future efforts to adapt LLMs to Dutch are poised to enhance these models even further, broadening their applicability and accessibility. Fietje is only an intermediate step in improving accessibility to language technology for users of the Dutch language.', 'score': 2, 'issue_id': 1263, 'pub_date': '2024-12-19', 'pub_date_card': {'ru': '19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 19', 'zh': '12æœˆ19æ—¥'}, 'hash': '0a377666ad38be9a', 'authors': ['Bram Vanroy'], 'affiliations': ['Dutch Language Institute, Rapenburg 61, 2311 GJ Leiden, The Netherlands', 'KU Leuven, Blijde Inkomststraat 21, 3000 Leuven, Belgium'], 'pdf_title_img': 'assets/pdf/title_img/2412.15450.jpg', 'data': {'categories': ['#multilingual', '#dataset', '#reasoning', '#open_source', '#small_models', '#benchmark'], 'emoji': 'ğŸ‡³ğŸ‡±', 'ru': {'title': 'Fietje: ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ°Ğ»Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ½Ğ¸Ğ´ĞµÑ€Ğ»Ğ°Ğ½Ğ´ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Fietje - ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (SLM), ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ½Ğ¸Ğ´ĞµÑ€Ğ»Ğ°Ğ½Ğ´ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Phi 2 Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ÑĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸: Fietje Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ´. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Fietje Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ³Ğ´Ğµ Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¸Ğµ Ğ¼Ğ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ñ€Ñ‹Ğµ Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ½Ğ¸Ğ´ĞµÑ€Ğ»Ğ°Ğ½Ğ´ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°.'}, 'en': {'title': 'Fietje: Small Models, Big Impact for Dutch Language Processing', 'desc': 'This paper presents Fietje, a series of small language models tailored for the Dutch language, built upon the Phi 2 architecture. Despite its smaller size, Fietje achieves competitive performance against larger models, showcasing the advancements in small language models (SLMs). The authors emphasize the importance of transparency and reproducibility by making all resources, including model weights and training data, publicly available. The evaluation results indicate that recent small models like Fietje are outperforming older, larger models in various linguistic tasks, highlighting a promising trend in Dutch language processing.'}, 'zh': {'title': 'Fietjeï¼šè·å…°è¯­å¤„ç†çš„æ–°å¸Œæœ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Fietjeï¼Œä¸€ä¸ªä¸“ä¸ºè·å…°è¯­è®¾è®¡çš„å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ç³»åˆ—ã€‚è¯¥æ¨¡å‹åŸºäºä¸€ä¸ªæ‹¥æœ‰27äº¿å‚æ•°çš„ä»¥è‹±è¯­ä¸ºä¸­å¿ƒçš„Phi 2æ¨¡å‹ã€‚Fietjeåœ¨å‘å¸ƒæ—¶å±•ç¤ºäº†ä¸æ›´å¤§è¯­è¨€æ¨¡å‹çš„ç«äº‰æ€§ç»“æœï¼Œå¼ºè°ƒäº†é€æ˜æ€§å’Œå¯é‡å¤æ€§ï¼Œæ‰€æœ‰æ¨¡å‹æƒé‡ã€æ•°æ®é›†ã€è®­ç»ƒå’Œè¯„ä¼°ä»£ç å‡ä¸ºå¼€æºã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæœ€è¿‘çš„å°å‹æ¨¡å‹åœ¨è·å…°è¯­å¤„ç†ä¸Šè¶…è¶Šäº†è¾ƒæ—§çš„ã€æ›´å¤§çš„æ¨¡å‹ï¼Œé¢„ç¤ºç€è·å…°è¯­å¤„ç†çš„æœªæ¥å……æ»¡å¸Œæœ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.14963', 'title': 'IDOL: Instant Photorealistic 3D Human Creation from a Single Image', 'url': 'https://huggingface.co/papers/2412.14963', 'abstract': 'Creating a high-fidelity, animatable 3D full-body avatar from a single image is a challenging task due to the diverse appearance and poses of humans and the limited availability of high-quality training data. To achieve fast and high-quality human reconstruction, this work rethinks the task from the perspectives of dataset, model, and representation. First, we introduce a large-scale HUman-centric GEnerated dataset, HuGe100K, consisting of 100K diverse, photorealistic sets of human images. Each set contains 24-view frames in specific human poses, generated using a pose-controllable image-to-multi-view model. Next, leveraging the diversity in views, poses, and appearances within HuGe100K, we develop a scalable feed-forward transformer model to predict a 3D human Gaussian representation in a uniform space from a given human image. This model is trained to disentangle human pose, body shape, clothing geometry, and texture. The estimated Gaussians can be animated without post-processing. We conduct comprehensive experiments to validate the effectiveness of the proposed dataset and method. Our model demonstrates the ability to efficiently reconstruct photorealistic humans at 1K resolution from a single input image using a single GPU instantly. Additionally, it seamlessly supports various applications, as well as shape and texture editing tasks.', 'score': 1, 'issue_id': 1269, 'pub_date': '2024-12-19', 'pub_date_card': {'ru': '19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 19', 'zh': '12æœˆ19æ—¥'}, 'hash': 'ff4454342b061f2d', 'authors': ['Yiyu Zhuang', 'Jiaxi Lv', 'Hao Wen', 'Qing Shuai', 'Ailing Zeng', 'Hao Zhu', 'Shifeng Chen', 'Yujiu Yang', 'Xun Cao', 'Wei Liu'], 'affiliations': ['Nanjing University', 'Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences', 'Shenzhen University of Advanced Technology', 'Tencent', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.14963.jpg', 'data': {'categories': ['#3d', '#dataset', '#architecture'], 'emoji': 'ğŸ•º', 'ru': {'title': 'ĞœĞ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… 3D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ñ„Ğ¾Ñ‚Ğ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… 3D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ HuGe100K, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 100 Ñ‚Ñ‹ÑÑÑ‡ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»ÑĞ´ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ°Ñ… Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ñ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¾Ğ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ², Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ Ğ¿Ğ¾Ğ·Ñƒ, Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ñ‚ĞµĞ»Ğ°, Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñƒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ»ÑĞ´ĞµĞ¹ Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ 1K Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Transforming Single Images into Lifelike 3D Avatars', 'desc': 'This paper presents a novel approach to creating detailed 3D avatars from a single image by introducing a new dataset and a transformer model. The HuGe100K dataset contains 100,000 diverse human images with multiple views and poses, which helps in training the model effectively. The proposed feed-forward transformer model predicts a 3D Gaussian representation, allowing for the separation of human features like pose, shape, and texture. This method enables fast and high-quality reconstruction of photorealistic avatars, which can be animated and edited easily, demonstrating its versatility in various applications.'}, 'zh': {'title': 'ä»å•å›¾åƒç”Ÿæˆ3Då…¨èº«å¤´åƒçš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬ç ”ç©¶æ—¨åœ¨ä»å•å¼ å›¾åƒåˆ›å»ºé«˜ä¿çœŸã€å¯åŠ¨ç”»çš„3Då…¨èº«å¤´åƒï¼Œè§£å†³äº†äººç±»å¤–è§‚å’Œå§¿åŠ¿å¤šæ ·æ€§ä»¥åŠé«˜è´¨é‡è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤§å‹äººç±»ä¸­å¿ƒç”Ÿæˆæ•°æ®é›†HuGe100Kï¼ŒåŒ…å«10ä¸‡ä¸ªå¤šæ ·åŒ–çš„ã€é€¼çœŸçš„äººç±»å›¾åƒé›†ï¼Œæ¯ä¸ªå›¾åƒé›†åŒ…å«ç‰¹å®šå§¿åŠ¿ä¸‹çš„24ä¸ªè§†è§’å¸§ã€‚åŸºäºè¯¥æ•°æ®é›†çš„å¤šæ ·æ€§ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¯æ‰©å±•çš„å‰é¦ˆå˜æ¢å™¨æ¨¡å‹ï¼Œèƒ½å¤Ÿä»ç»™å®šçš„äººç±»å›¾åƒé¢„æµ‹3Dé«˜æ–¯è¡¨ç¤ºã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆé‡å»º1Kåˆ†è¾¨ç‡çš„é€¼çœŸå›¾åƒï¼Œå¹¶æ”¯æŒå¤šç§åº”ç”¨å’Œå½¢çŠ¶ã€çº¹ç†ç¼–è¾‘ä»»åŠ¡ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (1)', '#agi', '#alignment', '#architecture (3)', '#audio (1)', '#benchmark (4)', '#cv (1)', '#data', '#dataset (2)', '#diffusion (1)', '#ethics (1)', '#games', '#graphs', '#hallucinations', '#healthcare', '#inference (5)', '#interpretability', '#leakage', '#long_context (1)', '#low_resource (1)', '#machine_translation', '#math (1)', '#multilingual (2)', '#multimodal (1)', '#open_source (1)', '#optimization (5)', '#plp', '#rag', '#reasoning (2)', '#rl (1)', '#rlhf (1)', '#robotics', '#science', '#security', '#small_models (2)', '#story_generation', '#survey', '#synthetic (1)', '#training (4)', '#transfer_learning (1)', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2024-12-23 14:09',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-12-23 14:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-12-23 14:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    