
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 20 papers. February 13.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ</span> | <span id="title-articles-count">20 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-02-12.html">â¬…ï¸ <span id="prev-date">12.02</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-02-14.html">â¡ï¸ <span id="next-date">14.02</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-02.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 13', 'zh': '2æœˆ13æ—¥'};
        let feedDateNext = {'ru': '14.02', 'en': '02/14', 'zh': '2æœˆ14æ—¥'};
        let feedDatePrev = {'ru': '12.02', 'en': '02/12', 'zh': '2æœˆ12æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2502.07870', 'title': 'TextAtlas5M: A Large-scale Dataset for Dense Text Image Generation', 'url': 'https://huggingface.co/papers/2502.07870', 'abstract': 'Text-conditioned image generation has gained significant attention in recent years and are processing increasingly longer and comprehensive text prompt. In everyday life, dense and intricate text appears in contexts like advertisements, infographics, and signage, where the integration of both text and visuals is essential for conveying complex information. However, despite these advances, the generation of images containing long-form text remains a persistent challenge, largely due to the limitations of existing datasets, which often focus on shorter and simpler text. To address this gap, we introduce TextAtlas5M, a novel dataset specifically designed to evaluate long-text rendering in text-conditioned image generation. Our dataset consists of 5 million long-text generated and collected images across diverse data types, enabling comprehensive evaluation of large-scale generative models on long-text image generation. We further curate 3000 human-improved test set TextAtlasEval across 3 data domains, establishing one of the most extensive benchmarks for text-conditioned generation. Evaluations suggest that the TextAtlasEval benchmarks present significant challenges even for the most advanced proprietary models (e.g. GPT4o with DallE-3), while their open-source counterparts show an even larger performance gap. These evidences position TextAtlas5M as a valuable dataset for training and evaluating future-generation text-conditioned image generation models.', 'score': 27, 'issue_id': 2188, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 11', 'zh': '2æœˆ11æ—¥'}, 'hash': '5b552b320e2e69f0', 'authors': ['Alex Jinpeng Wang', 'Dongxing Mao', 'Jiawei Zhang', 'Weiming Han', 'Zhuobai Dong', 'Linjie Li', 'Yiqi Lin', 'Zhengyuan Yang', 'Libo Qin', 'Fuwei Zhang', 'Lijuan Wang', 'Min Li'], 'affiliations': ['Central South University', 'Microsoft', 'National University of Singapore', 'North University of China'], 'pdf_title_img': 'assets/pdf/title_img/2502.07870.jpg', 'data': {'categories': ['#dataset', '#open_source', '#long_context', '#benchmark', '#cv'], 'emoji': 'ğŸ“š', 'ru': {'title': 'TextAtlas5M: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ TextAtlas5M Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ TextAtlasEval Ğ¸Ğ· 3000 ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, GPT4o Ñ DallE-3) ÑÑ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°ÑÑ‚ÑÑ ÑĞ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ ÑÑ‚Ğ¸Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Empowering Long-Text Image Generation with TextAtlas5M', 'desc': 'This paper introduces TextAtlas5M, a new dataset aimed at improving text-conditioned image generation, particularly for long-form text. Current models struggle with generating images that include dense and intricate text, as existing datasets primarily focus on shorter text. TextAtlas5M contains 5 million images with long text, allowing for better evaluation of generative models. The paper also presents TextAtlasEval, a curated test set that highlights the challenges faced by both proprietary and open-source models in this area.'}, 'zh': {'title': 'é•¿æ–‡æœ¬å›¾åƒç”Ÿæˆçš„æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†TextAtlas5Mï¼Œæ—¨åœ¨è§£å†³æ–‡æœ¬æ¡ä»¶ä¸‹å›¾åƒç”Ÿæˆä¸­é•¿æ–‡æœ¬æ¸²æŸ“çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ•°æ®é›†é€šå¸¸åªå…³æ³¨çŸ­æ–‡æœ¬ï¼Œé™åˆ¶äº†ç”Ÿæˆæ¨¡å‹çš„èƒ½åŠ›ã€‚TextAtlas5MåŒ…å«500ä¸‡å¼ é•¿æ–‡æœ¬ç”Ÿæˆçš„å›¾åƒï¼Œè¦†ç›–å¤šç§æ•°æ®ç±»å‹ï¼Œä¸ºå¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹çš„è¯„ä¼°æä¾›äº†åŸºç¡€ã€‚é€šè¿‡å»ºç«‹3000ä¸ªç»è¿‡äººå·¥æ”¹è¿›çš„æµ‹è¯•é›†TextAtlasEvalï¼Œæœ¬æ–‡ä¸ºæ–‡æœ¬æ¡ä»¶ç”Ÿæˆæä¾›äº†ä¸€ä¸ªå¹¿æ³›çš„åŸºå‡†ï¼Œå¸®åŠ©æœªæ¥çš„ç ”ç©¶å’Œæ¨¡å‹è®­ç»ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.08590', 'title': 'Light-A-Video: Training-free Video Relighting via Progressive Light Fusion', 'url': 'https://huggingface.co/papers/2502.08590', 'abstract': "Recent advancements in image relighting models, driven by large-scale datasets and pre-trained diffusion models, have enabled the imposition of consistent lighting. However, video relighting still lags, primarily due to the excessive training costs and the scarcity of diverse, high-quality video relighting datasets. A simple application of image relighting models on a frame-by-frame basis leads to several issues: lighting source inconsistency and relighted appearance inconsistency, resulting in flickers in the generated videos. In this work, we propose Light-A-Video, a training-free approach to achieve temporally smooth video relighting. Adapted from image relighting models, Light-A-Video introduces two key techniques to enhance lighting consistency. First, we design a Consistent Light Attention (CLA) module, which enhances cross-frame interactions within the self-attention layers to stabilize the generation of the background lighting source. Second, leveraging the physical principle of light transport independence, we apply linear blending between the source video's appearance and the relighted appearance, using a Progressive Light Fusion (PLF) strategy to ensure smooth temporal transitions in illumination. Experiments show that Light-A-Video improves the temporal consistency of relighted video while maintaining the image quality, ensuring coherent lighting transitions across frames. Project page: https://bujiazi.github.io/light-a-video.github.io/.", 'score': 27, 'issue_id': 2188, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 12', 'zh': '2æœˆ12æ—¥'}, 'hash': 'dcc03282320e88b1', 'authors': ['Yujie Zhou', 'Jiazi Bu', 'Pengyang Ling', 'Pan Zhang', 'Tong Wu', 'Qidong Huang', 'Jinsong Li', 'Xiaoyi Dong', 'Yuhang Zang', 'Yuhang Cao', 'Anyi Rao', 'Jiaqi Wang', 'Li Niu'], 'affiliations': ['Hong Kong University of Science and Technology', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Stanford University', 'The Chinese University of Hong Kong', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2502.08590.jpg', 'data': {'categories': ['#video', '#diffusion', '#cv'], 'emoji': 'ğŸ’¡', 'ru': {'title': 'ĞŸĞ»Ğ°Ğ²Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Light-A-Video - Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿ĞµÑ€ĞµĞ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ: Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Consistent Light Attention (CLA) Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° ÑĞ²ĞµÑ‚Ğ° Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Progressive Light Fusion (PLF) Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. Light-A-Video Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ€ĞµÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° ÑĞ²ĞµÑ‚Ğ° Ğ¸ Ğ¼ĞµÑ€Ñ†Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Achieving Smooth Video Relighting with Light-A-Video', 'desc': 'This paper presents Light-A-Video, a novel approach for video relighting that addresses the challenges of lighting consistency and flickering in generated videos. Unlike traditional methods that apply image relighting on a frame-by-frame basis, Light-A-Video utilizes a training-free strategy to enhance temporal smoothness. It introduces a Consistent Light Attention (CLA) module to improve interactions between frames and stabilize background lighting. Additionally, the Progressive Light Fusion (PLF) technique is employed to blend the original and relighted appearances, ensuring coherent lighting transitions across video frames.'}, 'zh': {'title': 'å®ç°è§†é¢‘é‡å…‰çš„ä¸€è‡´æ€§ä¸å¹³æ»‘æ€§', 'desc': 'æœ€è¿‘ï¼Œå›¾åƒé‡å…‰æ¨¡å‹çš„è¿›å±•å¾—ç›Šäºå¤§è§„æ¨¡æ•°æ®é›†å’Œé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œä½¿å¾—ä¸€è‡´çš„å…‰ç…§æ•ˆæœå¾—ä»¥å®ç°ã€‚ç„¶è€Œï¼Œè§†é¢‘é‡å…‰ä»ç„¶æ»åï¼Œä¸»è¦æ˜¯ç”±äºè®­ç»ƒæˆæœ¬é«˜å’Œç¼ºä¹å¤šæ ·åŒ–çš„é«˜è´¨é‡è§†é¢‘é‡å…‰æ•°æ®é›†ã€‚ç®€å•åœ°å°†å›¾åƒé‡å…‰æ¨¡å‹é€å¸§åº”ç”¨ä¼šå¯¼è‡´å…‰æºä¸ä¸€è‡´å’Œé‡å…‰å¤–è§‚ä¸ä¸€è‡´ï¼Œä»è€Œåœ¨ç”Ÿæˆçš„è§†é¢‘ä¸­äº§ç”Ÿé—ªçƒç°è±¡ã€‚æˆ‘ä»¬æå‡ºäº†Light-A-Videoï¼Œè¿™æ˜¯ä¸€ç§æ— è®­ç»ƒçš„æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°æ—¶é—´ä¸Šå¹³æ»‘çš„è§†é¢‘é‡å…‰ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.07346', 'title': 'BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models', 'url': 'https://huggingface.co/papers/2502.07346', 'abstract': 'Previous multilingual benchmarks focus primarily on simple understanding tasks, but for large language models(LLMs), we emphasize proficiency in instruction following, reasoning, long context understanding, code generation, and so on. However, measuring these advanced capabilities across languages is underexplored. To address the disparity, we introduce BenchMAX, a multi-way multilingual evaluation benchmark that allows for fair comparisons of these important abilities across languages. To maintain high quality, three distinct native-speaking annotators independently annotate each sample within all tasks after the data was machine-translated from English into 16 other languages. Additionally, we present a novel translation challenge stemming from dataset construction. Extensive experiments on BenchMAX reveal varying effectiveness of core capabilities across languages, highlighting performance gaps that cannot be bridged by simply scaling up model size. BenchMAX serves as a comprehensive multilingual evaluation platform, providing a promising test bed to promote the development of multilingual language models. The dataset and code are publicly accessible.', 'score': 26, 'issue_id': 2192, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 11', 'zh': '2æœˆ11æ—¥'}, 'hash': '8a6e122fc0804618', 'authors': ['Xu Huang', 'Wenhao Zhu', 'Hanxu Hu', 'Conghui He', 'Lei Li', 'Shujian Huang', 'Fei Yuan'], 'affiliations': ['Carnegie Mellon University', 'National Key Laboratory for Novel Software Technology, Nanjing University', 'Shanghai Artificial Intelligence Laboratory', 'University of Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2502.07346.jpg', 'data': {'categories': ['#long_context', '#low_resource', '#machine_translation', '#multilingual', '#dataset', '#open_source', '#benchmark'], 'emoji': 'ğŸŒ', 'ru': {'title': 'BenchMAX: ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'BenchMAX - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² 17 ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞĞ½ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ñ‚Ğ°ĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ĞºĞ°Ğº ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ°. ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ‚Ñ€ĞµĞ¼Ñ Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑĞ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞ»ÑŒĞ·Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'BenchMAX: Bridging Multilingual Gaps in Advanced Language Model Evaluation', 'desc': 'This paper introduces BenchMAX, a new multilingual evaluation benchmark designed to assess advanced capabilities of large language models (LLMs) such as instruction following, reasoning, and code generation across multiple languages. Unlike previous benchmarks that focused on simpler tasks, BenchMAX allows for a fair comparison of these complex abilities by using machine-translated data and independent annotations from native speakers. The study reveals significant performance gaps in LLMs when evaluated on these advanced tasks, indicating that merely increasing model size is not sufficient to improve multilingual proficiency. BenchMAX aims to enhance the development of multilingual models by providing a robust platform for evaluation and research.'}, 'zh': {'title': 'BenchMAXï¼šå¤šè¯­è¨€èƒ½åŠ›è¯„ä¼°çš„æ–°åŸºå‡†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†BenchMAXï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šè¯­è¨€è¯„ä¼°åŸºå‡†ï¼Œæ—¨åœ¨æ¯”è¾ƒå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æŒ‡ä»¤éµå¾ªã€æ¨ç†ã€é•¿æ–‡æœ¬ç†è§£å’Œä»£ç ç”Ÿæˆç­‰é«˜çº§èƒ½åŠ›ä¸Šçš„è¡¨ç°ã€‚ä»¥å¾€çš„å¤šè¯­è¨€åŸºå‡†ä¸»è¦å…³æ³¨ç®€å•ç†è§£ä»»åŠ¡ï¼Œè€ŒBenchMAXåˆ™å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œå…è®¸å¯¹ä¸åŒè¯­è¨€çš„èƒ½åŠ›è¿›è¡Œå…¬å¹³æ¯”è¾ƒã€‚ä¸ºäº†ç¡®ä¿æ•°æ®è´¨é‡ï¼Œä¸‰ä½æ¯è¯­è¯„å®¡å‘˜ç‹¬ç«‹å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œæ ‡æ³¨ï¼Œç¡®ä¿è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸åŒè¯­è¨€åœ¨æ ¸å¿ƒèƒ½åŠ›ä¸Šçš„è¡¨ç°å·®å¼‚ï¼Œè¡¨æ˜ä»…ä»…å¢åŠ æ¨¡å‹è§„æ¨¡æ— æ³•è§£å†³è¿™äº›æ€§èƒ½å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.08639', 'title': 'CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation', 'url': 'https://huggingface.co/papers/2502.08639', 'abstract': 'In this work, we present CineMaster, a novel framework for 3D-aware and controllable text-to-video generation. Our goal is to empower users with comparable controllability as professional film directors: precise placement of objects within the scene, flexible manipulation of both objects and camera in 3D space, and intuitive layout control over the rendered frames. To achieve this, CineMaster operates in two stages. In the first stage, we design an interactive workflow that allows users to intuitively construct 3D-aware conditional signals by positioning object bounding boxes and defining camera movements within the 3D space. In the second stage, these control signals--comprising rendered depth maps, camera trajectories and object class labels--serve as the guidance for a text-to-video diffusion model, ensuring to generate the user-intended video content. Furthermore, to overcome the scarcity of in-the-wild datasets with 3D object motion and camera pose annotations, we carefully establish an automated data annotation pipeline that extracts 3D bounding boxes and camera trajectories from large-scale video data. Extensive qualitative and quantitative experiments demonstrate that CineMaster significantly outperforms existing methods and implements prominent 3D-aware text-to-video generation. Project page: https://cinemaster-dev.github.io/.', 'score': 25, 'issue_id': 2186, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 12', 'zh': '2æœˆ12æ—¥'}, 'hash': '3d3890b6b6bf7904', 'authors': ['Qinghe Wang', 'Yawen Luo', 'Xiaoyu Shi', 'Xu Jia', 'Huchuan Lu', 'Tianfan Xue', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Kun Gai'], 'affiliations': ['Dalian University of Technology', 'Kuaishou Technology', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.08639.jpg', 'data': {'categories': ['#dataset', '#video', '#diffusion', '#3d', '#games'], 'emoji': 'ğŸ¬', 'ru': {'title': 'CineMaster: Ğ ĞµĞ¶Ğ¸ÑÑĞ¸Ñ€ÑƒĞ¹Ñ‚Ğµ ÑĞ²Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² 3D', 'desc': 'CineMaster - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 3D-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ² ÑÑ†ĞµĞ½Ğµ, Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ² 3D-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ 3D-ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ·Ğ°Ñ‚ĞµĞ¼ ÑÑ‚Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ 3D-Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ¼Ğ¾Ğº Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹.'}, 'en': {'title': 'Empowering Video Creation with 3D Control', 'desc': 'CineMaster is a new framework designed for generating videos from text while allowing users to control 3D elements like a film director. It operates in two stages: first, users create 3D-aware signals by placing objects and defining camera movements, and second, these signals guide a text-to-video diffusion model to produce the desired video. The framework also includes an automated data annotation system to gather necessary 3D motion and camera data from existing videos. Experiments show that CineMaster outperforms current methods in generating 3D-aware videos from text descriptions.'}, 'zh': {'title': 'CineMasterï¼šè®©è§†é¢‘ç”Ÿæˆå¦‚å¯¼æ¼”èˆ¬å¯æ§', 'desc': 'CineMasteræ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå…·æœ‰3Dæ„ŸçŸ¥å’Œå¯æ§æ€§çš„æ–‡æœ¬åˆ°è§†é¢‘ã€‚å®ƒä½¿ç”¨æˆ·èƒ½å¤Ÿåƒä¸“ä¸šç”µå½±å¯¼æ¼”ä¸€æ ·ç²¾ç¡®æ§åˆ¶åœºæ™¯ä¸­çš„ç‰©ä½“ä½ç½®ã€çµæ´»æ“ä½œ3Dç©ºé—´ä¸­çš„ç‰©ä½“å’Œç›¸æœºï¼Œå¹¶ç›´è§‚åœ°å¸ƒå±€æ¸²æŸ“å¸§ã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µé€šè¿‡äº¤äº’å¼å·¥ä½œæµç¨‹æ„å»º3Dæ„ŸçŸ¥çš„æ¡ä»¶ä¿¡å·ï¼Œç¬¬äºŒé˜¶æ®µåˆ©ç”¨è¿™äº›ä¿¡å·æŒ‡å¯¼æ–‡æœ¬åˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆç”¨æˆ·æ‰€éœ€çš„è§†é¢‘å†…å®¹ã€‚æ­¤å¤–ï¼ŒCineMasterè¿˜å»ºç«‹äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æ•°æ®æ³¨é‡Šç®¡é“ï¼Œä»¥è§£å†³ç¼ºä¹3Dç‰©ä½“è¿åŠ¨å’Œç›¸æœºå§¿æ€æ ‡æ³¨çš„æ•°æ®é›†é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.08047', 'title': 'WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation', 'url': 'https://huggingface.co/papers/2502.08047', 'abstract': 'Current GUI agents have achieved outstanding performance in GUI element grounding. However, planning remains highly challenging, especially due to sensitivity to the initial state of the environment. Specifically, slight differences in the initial state-such as the target software not being open or the interface not being in its default state-often lead to planning errors. This issue is widespread in real user scenarios, but existing benchmarks fail to evaluate it. In this paper, we present WorldGUI, a novel GUI benchmark that designs GUI tasks with various initial states to simulate real computer-user interactions. The benchmark spans a wide range of tasks across 10 popular software applications, including PowerPoint, VSCode, and Adobe Acrobat. In addition, to address the challenges of dynamic GUI automation tasks, we propose GUI-Thinker, a holistic framework, leveraging a critique mechanism, that effectively manages the unpredictability and complexity of GUI interactions. Experimental results demonstrate that GUI-Thinker significantly outperforms Claude-3.5 (Computer Use) by 14.9% in success rate on WorldGUI tasks. This improvement underscores the effectiveness of our critical-thinking-based framework in enhancing GUI automation.', 'score': 19, 'issue_id': 2190, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 12', 'zh': '2æœˆ12æ—¥'}, 'hash': '0f83dccb05181f21', 'authors': ['Henry Hengyuan Zhao', 'Difei Gao', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2502.08047.jpg', 'data': {'categories': ['#agents', '#benchmark'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ GUI', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ WorldGUI - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ¾Ğ¼ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº GUI-Thinker, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ GUI. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GUI-Thinker Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Claude-3.5 (Computer Use) Ğ¿Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² WorldGUI. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸, Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ GUI.'}, 'en': {'title': 'Enhancing GUI Automation with WorldGUI and GUI-Thinker', 'desc': 'This paper addresses the challenges faced by current GUI agents in planning tasks due to their sensitivity to the initial state of the environment. It introduces WorldGUI, a new benchmark that simulates real user interactions by incorporating various initial states across multiple software applications. To tackle the complexities of dynamic GUI automation, the authors propose GUI-Thinker, a framework that utilizes a critique mechanism to improve decision-making in unpredictable scenarios. Experimental results show that GUI-Thinker outperforms existing models, highlighting its effectiveness in enhancing the success rate of GUI automation tasks.'}, 'zh': {'title': 'æå‡GUIè‡ªåŠ¨åŒ–çš„å…³é”®æ€ç»´æ¡†æ¶', 'desc': 'å½“å‰çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†åœ¨å…ƒç´ å®šä½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è§„åˆ’æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å¯¹ç¯å¢ƒåˆå§‹çŠ¶æ€çš„æ•æ„Ÿæ€§ã€‚åˆå§‹çŠ¶æ€çš„å¾®å°å·®å¼‚ï¼Œä¾‹å¦‚ç›®æ ‡è½¯ä»¶æœªæ‰“å¼€æˆ–ç•Œé¢ä¸åœ¨é»˜è®¤çŠ¶æ€ï¼Œå¸¸å¸¸å¯¼è‡´è§„åˆ’é”™è¯¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†WorldGUIï¼Œä¸€ä¸ªæ–°é¢–çš„GUIåŸºå‡†ï¼Œè®¾è®¡äº†å…·æœ‰å¤šç§åˆå§‹çŠ¶æ€çš„GUIä»»åŠ¡ï¼Œä»¥æ¨¡æ‹ŸçœŸå®çš„è®¡ç®—æœºç”¨æˆ·äº¤äº’ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†GUI-Thinkerï¼Œä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ï¼Œé€šè¿‡æ‰¹åˆ¤æœºåˆ¶æœ‰æ•ˆç®¡ç†GUIäº¤äº’çš„ä¸å¯é¢„æµ‹æ€§å’Œå¤æ‚æ€§ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶åœ¨WorldGUIä»»åŠ¡ä¸Šçš„æˆåŠŸç‡æ¯”Claude-3.5é«˜å‡º14.9%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.07563', 'title': 'LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid', 'url': 'https://huggingface.co/papers/2502.07563', 'abstract': 'Linear sequence modeling approaches, such as linear attention, provide advantages like linear-time training and constant-memory inference over sequence lengths. However, existing sequence parallelism (SP) methods are either not optimized for the right-product-first feature of linear attention or use a ring-style communication strategy, which results in lower computation parallelism, limits their scalability for longer sequences in distributed systems. In this paper, we introduce LASP-2, a new SP method to enhance both communication and computation parallelism when training linear attention transformer models with very-long input sequences. Compared to previous work LASP, LASP-2 rethinks the minimal communication requirement for SP on linear attention layers, reorganizes the whole communication-computation workflow of LASP. In this way, only one single AllGather collective communication is needed on intermediate memory states, whose sizes are independent of the sequence length, leading to significant improvements of both communication and computation parallelism, as well as their overlap. Additionally, we extend LASP-2 to LASP-2H by applying similar communication redesign to standard attention modules, offering an efficient SP solution for hybrid models that blend linear and standard attention layers. Our evaluation on a Linear-Llama3 model, a variant of Llama3 with linear attention replacing standard attention, demonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2 achieves training speed improvements of 15.2% over LASP and 36.6% over Ring Attention, with a sequence length of 2048K across 64 GPUs. The Code is released as a part of: https://github.com/OpenSparseLLMs/Linear-MoE.', 'score': 17, 'issue_id': 2188, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 11', 'zh': '2æœˆ11æ—¥'}, 'hash': 'f5a4cfd0a0d018ae', 'authors': ['Weigao Sun', 'Disen Lan', 'Yiran Zhong', 'Xiaoye Qu', 'Yu Cheng'], 'affiliations': ['Shanghai AI Laboratory', 'South China University of Technology', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.07563.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ° ÑĞ²ĞµÑ€Ñ…Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ LASP-2 Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…. LASP-2 Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ½Ñƒ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ AllGather Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ LASP-2H Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ñ… Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Linear-Llama3 Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LASP-2 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ¿Ñ€Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Boosting Efficiency in Linear Attention with LASP-2', 'desc': 'This paper presents LASP-2, a new sequence parallelism (SP) method designed to improve the efficiency of training linear attention transformer models with very long input sequences. LASP-2 optimizes communication and computation parallelism by minimizing the communication requirements for linear attention layers, allowing for a single AllGather operation on intermediate memory states. This approach leads to significant enhancements in both communication and computation parallelism, enabling better scalability in distributed systems. Additionally, LASP-2 is extended to LASP-2H, which applies similar optimizations to standard attention modules, providing an efficient solution for hybrid models that utilize both linear and standard attention.'}, 'zh': {'title': 'LASP-2ï¼šæå‡çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹çš„å¹¶è¡Œæ€§', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åºåˆ—å¹¶è¡Œæ–¹æ³•LASP-2ï¼Œæ—¨åœ¨æé«˜çº¿æ€§æ³¨æ„åŠ›å˜æ¢å™¨æ¨¡å‹åœ¨å¤„ç†éå¸¸é•¿è¾“å…¥åºåˆ—æ—¶çš„é€šä¿¡å’Œè®¡ç®—å¹¶è¡Œæ€§ã€‚ä¸ä¹‹å‰çš„LASPæ–¹æ³•ç›¸æ¯”ï¼ŒLASP-2é‡æ–°æ€è€ƒäº†çº¿æ€§æ³¨æ„åŠ›å±‚çš„æœ€å°é€šä¿¡éœ€æ±‚ï¼Œå¹¶é‡æ–°ç»„ç»‡äº†é€šä¿¡å’Œè®¡ç®—çš„å·¥ä½œæµç¨‹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒLASP-2åªéœ€åœ¨ä¸­é—´å†…å­˜çŠ¶æ€ä¸Šè¿›è¡Œä¸€æ¬¡AllGatheré›†ä½“é€šä¿¡ï¼Œæ˜¾è‘—æé«˜äº†é€šä¿¡å’Œè®¡ç®—çš„å¹¶è¡Œæ€§åŠå…¶é‡å ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒLASP-2åœ¨è®­ç»ƒé€Ÿåº¦ä¸Šæ¯”LASPæé«˜äº†15.2%ï¼Œæ¯”ç¯å½¢æ³¨æ„åŠ›æé«˜äº†36.6%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.07864', 'title': 'TransMLA: Multi-head Latent Attention Is All You Need', 'url': 'https://huggingface.co/papers/2502.07864', 'abstract': 'Modern large language models (LLMs) often encounter communication bottlenecks on current hardware, rather than purely computational constraints. Multi-head Latent Attention (MLA) tackles this challenge by using low-rank matrices in the key-value (KV) layers, thereby allowing compressed latent KV states to be cached. This approach significantly reduces the KV cache size relative to traditional multi-head attention, leading to faster inference. Moreover, MLA employs an up-projection matrix to increase expressiveness, trading additional computation for reduced communication overhead. Although MLA has demonstrated efficiency and effectiveness in Deepseek V2/V3/R1, many major model providers still rely on Group Query Attention (GQA) and have not announced any plans to adopt MLA. In this paper, we show that GQA can always be represented by MLA while maintaining the same KV cache overhead, but the converse does not hold. To encourage broader use of MLA, we introduce **TransMLA**, a post-training method that converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen, Mixtral) into MLA-based models. After conversion, the model can undergo additional training to boost expressiveness without increasing the KV cache size. Furthermore, we plan to develop MLA-specific inference acceleration techniques to preserve low latency in transformed models, thus enabling more efficient distillation of Deepseek R1.', 'score': 17, 'issue_id': 2186, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 11', 'zh': '2æœˆ11æ—¥'}, 'hash': 'dbff84dafe8c2312', 'authors': ['Fanxu Meng', 'Zengwei Yao', 'Muhan Zhang'], 'affiliations': ['Institute for Artificial Intelligence, Peking University', 'State Key Laboratory of General Artificial Intelligence, Peking University', 'Xiaomi Corp., Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.07864.jpg', 'data': {'categories': ['#optimization', '#architecture', '#inference', '#training', '#long_context'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: MLA Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Multi-head Latent Attention (MLA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑƒĞ·ĞºĞ¸Ñ… Ğ¼ĞµÑÑ‚ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. MLA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ½Ğ³Ğ° Ğ² ÑĞ»Ğ¾ÑÑ… ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¶Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ KV. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ TransMLA Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Group Query Attention (GQA) Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MLA. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ KV-ĞºÑÑˆĞ° Ğ¸ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Transforming Attention: From GQA to Efficient MLA', 'desc': 'This paper introduces Multi-head Latent Attention (MLA) as a solution to communication bottlenecks in large language models (LLMs) caused by hardware limitations. MLA utilizes low-rank matrices in the key-value (KV) layers to compress KV states, significantly reducing cache size and improving inference speed. The authors demonstrate that Group Query Attention (GQA) can be represented by MLA without increasing KV cache overhead, while MLA offers greater flexibility. To facilitate the adoption of MLA, they propose TransMLA, a method for converting GQA-based models into MLA-based ones, allowing for enhanced expressiveness without additional cache size.'}, 'zh': {'title': 'æå‡è¯­è¨€æ¨¡å‹æ•ˆç‡çš„å…³é”®ï¼šå¤šå¤´æ½œåœ¨æ³¨æ„åŠ›', 'desc': 'ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å½“å‰ç¡¬ä»¶ä¸Šå¸¸å¸¸é¢ä¸´é€šä¿¡ç“¶é¢ˆï¼Œè€Œä¸ä»…ä»…æ˜¯è®¡ç®—é™åˆ¶ã€‚å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼ˆMLAï¼‰é€šè¿‡åœ¨é”®å€¼ï¼ˆKVï¼‰å±‚ä¸­ä½¿ç”¨ä½ç§©çŸ©é˜µæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä»è€Œå…è®¸å‹ç¼©çš„æ½œåœ¨KVçŠ¶æ€è¢«ç¼“å­˜ã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—å‡å°‘äº†KVç¼“å­˜çš„å¤§å°ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„å¤šå¤´æ³¨æ„åŠ›ï¼Œæ¨ç†é€Ÿåº¦æ›´å¿«ã€‚æ­¤å¤–ï¼ŒMLAä½¿ç”¨ä¸ŠæŠ•å½±çŸ©é˜µæ¥å¢åŠ è¡¨è¾¾èƒ½åŠ›ï¼Œä»¥é¢å¤–çš„è®¡ç®—æ¢å–å‡å°‘çš„é€šä¿¡å¼€é”€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.08127', 'title': 'Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance', 'url': 'https://huggingface.co/papers/2502.08127', 'abstract': 'Recent advancements in large language models (LLMs) have shown strong general reasoning abilities, yet their effectiveness in financial reasoning remains underexplored. In this study, we comprehensively evaluate 16 powerful reasoning and general LLMs on three complex financial tasks involving financial text, tabular data, and equations, assessing numerical reasoning, tabular interpretation, financial terminology comprehension, long-context processing, and equation-based problem solving. Our results show that while better datasets and pretraining improve financial reasoning, general enhancements like CoT fine-tuning do not always yield consistent gains. Moreover, all reasoning strategies face challenges in improving performance on long-context and multi-table tasks. To address these limitations, we develop a financial reasoning-enhanced model based on Llama-3.1-8B-Instruct, by CoT fine-tuning and reinforcement learning with domain-specific reasoning paths. Even with simple fine-tuning with one financial dataset, our model achieves a consistent 10% performance improvement across tasks, surpassing all 8B models and even Llama3-70B-Instruct and Llama3.1-70B-Instruct on average. Our results highlight the need for domain-specific adaptations in financial tasks, emphasizing future directions such as multi-table reasoning, long-context processing, and financial terminology comprehension. All our datasets, models, and codes are publicly available. Furthermore, we introduce a leaderboard for benchmarking future datasets and models.', 'score': 15, 'issue_id': 2186, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 12', 'zh': '2æœˆ12æ—¥'}, 'hash': 'fa3f08993ba529cd', 'authors': ['Lingfei Qian', 'Weipeng Zhou', 'Yan Wang', 'Xueqing Peng', 'Jimin Huang', 'Qianqian Xie'], 'affiliations': ['TheFinAI'], 'pdf_title_img': 'assets/pdf/title_img/2502.08127.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#benchmark', '#rl', '#open_source', '#training', '#long_context'], 'emoji': 'ğŸ’¹', 'ru': {'title': 'Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - ĞºĞ»ÑÑ‡ Ğº ÑƒÑĞ¿ĞµÑ…Ñƒ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ 16 Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Llama-3.1-8B-Instruct, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ğ´Ğ°Ğ¶Ğµ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'Enhancing Financial Reasoning in LLMs with Domain-Specific Adaptations', 'desc': 'This paper investigates the performance of large language models (LLMs) in financial reasoning tasks, which include interpreting financial text, analyzing tabular data, and solving equations. The authors evaluate 16 advanced LLMs and find that while improved datasets and pretraining enhance financial reasoning, general techniques like Chain-of-Thought (CoT) fine-tuning do not consistently improve results. They propose a new model, enhanced with CoT fine-tuning and reinforcement learning, which shows a significant performance boost of 10% across various financial tasks. The study emphasizes the importance of domain-specific adaptations for better performance in financial reasoning and introduces a leaderboard for future benchmarking.'}, 'zh': {'title': 'é‡‘èæ¨ç†æ¨¡å‹çš„åˆ›æ–°ä¸æå‡', 'desc': 'æœ¬ç ”ç©¶è¯„ä¼°äº†16ç§å¼ºå¤§çš„è¯­è¨€æ¨¡å‹åœ¨é‡‘èæ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¿™äº›ä»»åŠ¡åŒ…æ‹¬é‡‘èæ–‡æœ¬ã€è¡¨æ ¼æ•°æ®å’Œæ–¹ç¨‹å¼ï¼Œæ¶‰åŠæ•°å€¼æ¨ç†ã€è¡¨æ ¼è§£è¯»å’Œé‡‘èæœ¯è¯­ç†è§£ç­‰æ–¹é¢ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°½ç®¡æ›´å¥½çš„æ•°æ®é›†å’Œé¢„è®­ç»ƒå¯ä»¥æå‡é‡‘èæ¨ç†èƒ½åŠ›ï¼Œä½†é€šç”¨çš„å¢å¼ºæ–¹æ³•å¦‚é“¾å¼æ¨ç†å¾®è°ƒå¹¶ä¸æ€»æ˜¯æœ‰æ•ˆã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºLlama-3.1-8B-Instructçš„é‡‘èæ¨ç†å¢å¼ºæ¨¡å‹ï¼Œç»è¿‡å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ åï¼Œåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†10%çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.06533', 'title': 'Ignore the KL Penalty! Boosting Exploration on Critical Tokens to Enhance RL Fine-Tuning', 'url': 'https://huggingface.co/papers/2502.06533', 'abstract': 'The ability to achieve long-term goals is a key challenge in the current development of large language models (LLMs). To address this, pre-trained LLMs can be fine-tuned with reinforcement learning (RL) to explore solutions that optimize a given goal. However, exploration with LLMs is difficult, as a balance has to be struck between discovering new solutions and staying close enough to the pre-trained model, so as not to degrade basic capabilities. This is typically controlled with a Kullback-Leibler (KL) penalty. In this paper, we investigate the exploration dynamics of a small language model on a simple arithmetic task. We show how varying degrees of pre-training influence exploration and demonstrate the importance of "critical tokens" which have a dramatic impact on the final outcome. Consequently, we introduce a simple modification to the KL penalty that favors exploration on critical tokens, increasing the efficiency of the RL fine-tuning stage.', 'score': 8, 'issue_id': 2192, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 10', 'zh': '2æœˆ10æ—¥'}, 'hash': '178e4717b984d64d', 'authors': ['Jean Vassoyan', 'NathanaÃ«l Beau', 'Roman Plaud'], 'affiliations': ['Institut Polytechnique de Paris', 'UniversitÃ© Paris-Saclay, CNRS, ENS Paris-Saclay, Centre Borelli, France', 'UniversitÃ© de Paris, LLF, CNRS, France', 'onepoint, France'], 'pdf_title_img': 'assets/pdf/title_img/2502.06533.jpg', 'data': {'categories': ['#rl', '#training', '#long_context', '#small_models', '#reasoning', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²', 'desc': "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ†ĞµĞ»ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… LLM, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½ÑƒÑ Ñ†ĞµĞ»ÑŒ. ĞĞ½Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ 'ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²'. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ KL-ÑˆÑ‚Ñ€Ğ°Ñ„Ğ° Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼."}, 'en': {'title': 'Enhancing Exploration in Language Models with Critical Tokens', 'desc': "This paper addresses the challenge of enabling large language models (LLMs) to achieve long-term goals through reinforcement learning (RL). It highlights the difficulty of exploration in LLMs, where a balance must be maintained between discovering new solutions and preserving the model's foundational capabilities. The authors investigate how pre-training affects exploration dynamics in a small language model tasked with simple arithmetic. They introduce a modified Kullback-Leibler (KL) penalty that enhances exploration of 'critical tokens', leading to more efficient RL fine-tuning."}, 'zh': {'title': 'ä¼˜åŒ–é•¿æœŸç›®æ ‡çš„æ¢ç´¢ç­–ç•¥', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å®ç°é•¿æœŸç›®æ ‡æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹é¢„è®­ç»ƒçš„LLMsè¿›è¡Œå¾®è°ƒï¼Œä»¥ä¼˜åŒ–ç‰¹å®šç›®æ ‡çš„è§£å†³æ–¹æ¡ˆã€‚ç ”ç©¶è¡¨æ˜ï¼Œé¢„è®­ç»ƒçš„ç¨‹åº¦å¯¹æ¢ç´¢è¿‡ç¨‹æœ‰æ˜¾è‘—å½±å“ï¼Œå°¤å…¶æ˜¯â€œå…³é”®æ ‡è®°â€åœ¨æœ€ç»ˆç»“æœä¸­èµ·ç€é‡è¦ä½œç”¨ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§å¯¹KLæƒ©ç½šçš„ç®€å•ä¿®æ”¹ï¼Œä»¥ä¿ƒè¿›å¯¹å…³é”®æ ‡è®°çš„æ¢ç´¢ï¼Œä»è€Œæé«˜RLå¾®è°ƒé˜¶æ®µçš„æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.08606', 'title': 'Distillation Scaling Laws', 'url': 'https://huggingface.co/papers/2502.08606', 'abstract': 'We provide a distillation scaling law that estimates distilled model performance based on a compute budget and its allocation between the student and teacher. Our findings reduce the risks associated with using distillation at scale; compute allocation for both the teacher and student models can now be done to maximize student performance. We provide compute optimal distillation recipes for when 1) a teacher exists, or 2) a teacher needs training. If many students are to be distilled, or a teacher already exists, distillation outperforms supervised pretraining until a compute level which grows predictably with student size. If one student is to be distilled and a teacher also needs training, supervised learning should be done instead. Additionally, we provide insights across our large scale study of distillation, which increase our understanding of distillation and inform experimental design.', 'score': 8, 'issue_id': 2188, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 12', 'zh': '2æœˆ12æ—¥'}, 'hash': '774eeded4c92c597', 'authors': ['Dan Busbridge', 'Amitis Shidani', 'Floris Weers', 'Jason Ramapuram', 'Etai Littwin', 'Russ Webb'], 'affiliations': ['Apple', 'University of Oxford, UK'], 'pdf_title_img': 'assets/pdf/title_img/2502.08606.jpg', 'data': {'categories': ['#training', '#optimization'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ°ĞºĞ¾Ğ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ¸ ĞµĞ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ñ‹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ»ÑƒÑ‡Ğ°Ğ¸ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸Ğ»Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ´Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ğ¾ Ñ€Ğ°ÑÑ‚ĞµÑ‚ Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ°. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ insights Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Maximizing Student Performance through Optimal Distillation Strategies', 'desc': 'This paper introduces a distillation scaling law that helps predict how well a distilled model will perform based on the available computing resources and how those resources are divided between the teacher and student models. The authors demonstrate that by optimizing the compute allocation, the performance of the student model can be significantly improved, especially when multiple students are distilled from a pre-trained teacher. They also provide guidelines for when to use distillation versus supervised learning, depending on whether a teacher model is available and whether it requires training. Overall, the study enhances our understanding of model distillation and offers practical strategies for effective implementation in machine learning tasks.'}, 'zh': {'title': 'ä¼˜åŒ–è’¸é¦æ¨¡å‹æ€§èƒ½çš„è®¡ç®—æ³•åˆ™', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§è’¸é¦ç¼©æ”¾æ³•åˆ™ï¼Œç”¨äºæ ¹æ®è®¡ç®—é¢„ç®—å’Œåœ¨å­¦ç”Ÿä¸æ•™å¸ˆä¹‹é—´çš„åˆ†é…æ¥ä¼°è®¡è’¸é¦æ¨¡å‹çš„æ€§èƒ½ã€‚ç ”ç©¶ç»“æœé™ä½äº†å¤§è§„æ¨¡ä½¿ç”¨è’¸é¦çš„é£é™©ï¼Œèƒ½å¤Ÿä¼˜åŒ–æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹çš„è®¡ç®—åˆ†é…ï¼Œä»¥æœ€å¤§åŒ–å­¦ç”Ÿçš„è¡¨ç°ã€‚æˆ‘ä»¬æä¾›äº†è®¡ç®—æœ€ä¼˜çš„è’¸é¦æ–¹æ¡ˆï¼Œé€‚ç”¨äºå·²æœ‰æ•™å¸ˆæˆ–éœ€è¦è®­ç»ƒæ•™å¸ˆçš„æƒ…å†µã€‚é€šè¿‡å¤§è§„æ¨¡ç ”ç©¶ï¼Œæˆ‘ä»¬å¢åŠ äº†å¯¹è’¸é¦çš„ç†è§£ï¼Œå¹¶ä¸ºå®éªŒè®¾è®¡æä¾›äº†æŒ‡å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.08168', 'title': 'SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation', 'url': 'https://huggingface.co/papers/2502.08168', 'abstract': "In the field of synthetic aperture radar (SAR) remote sensing image interpretation, although Vision language models (VLMs) have made remarkable progress in natural language processing and image understanding, their applications remain limited in professional domains due to insufficient domain expertise. This paper innovatively proposes the first large-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which contains approximately 2 million high-quality image-text pairs, encompasses diverse scenarios with detailed target annotations. This dataset not only supports several key tasks such as visual understanding and object detection tasks, but also has unique innovative aspects: this study develop a visual-language dataset and benchmark for the SAR domain, enabling and evaluating VLMs' capabilities in SAR image interpretation, which provides a paradigmatic framework for constructing multimodal datasets across various remote sensing vertical domains. Through experiments on 16 mainstream VLMs, the effectiveness of the dataset has been fully verified, and the first multi-task dialogue benchmark in the SAR field has been successfully established. The project will be released at https://github.com/JimmyMa99/SARChat, aiming to promote the in-depth development and wide application of SAR visual language models.", 'score': 8, 'issue_id': 2186, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 12', 'zh': '2æœˆ12æ—¥'}, 'hash': '1ca2b8d38e35b203', 'authors': ['Zhiming Ma', 'Xiayang Xiao', 'Sihao Dong', 'Peidong Wang', 'HaiPeng Wang', 'Qingyun Pan'], 'affiliations': ['China Mobile Group Guangdong Co., Ltd. Guangzhou Branch, Guangzhou, China', 'China Mobile Internet Company Ltd., Guangzhou, China', 'School of Computer Science and Engineering, Northeastern University, Shenyang, China', 'The Key Laboratory for Information Science of Electromagnetic Waves (Ministry of Education), School of Information Science and Technology, Fudan University, Shanghai, China', 'The School of Automation and Electrical Engineering, Inner Mongolia University of Science and Technology, Baotou, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.08168.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#dataset', '#open_source', '#synthetic'], 'emoji': 'ğŸ›°ï¸', 'ru': {'title': 'SARChat-2M: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ SAR Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ SAR, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ SARChat-2M. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¾ĞºĞ¾Ğ»Ğ¾ 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ñ†ĞµĞ»ĞµĞ¹. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ±Ñ‹Ğ»Ğ° Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ° ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° 16 Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… VLM, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ SAR.'}, 'en': {'title': 'Empowering SAR Image Interpretation with SARChat-2M!', 'desc': 'This paper introduces SARChat-2M, a large-scale multimodal dialogue dataset specifically designed for synthetic aperture radar (SAR) images. It contains around 2 million image-text pairs that cover various scenarios and include detailed annotations for effective visual understanding and object detection. The dataset serves as a benchmark for evaluating vision language models (VLMs) in the SAR domain, demonstrating their capabilities in interpreting SAR images. By conducting experiments on 16 popular VLMs, the study establishes a new multi-task dialogue benchmark, paving the way for advancements in remote sensing applications.'}, 'zh': {'title': 'æ¨åŠ¨SARå›¾åƒè§£è¯»çš„å¤šæ¨¡æ€å¯¹è¯æ•°æ®é›†', 'desc': 'åœ¨åˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰é¥æ„Ÿå›¾åƒè§£è¯»é¢†åŸŸï¼Œå°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œå›¾åƒç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç”±äºç¼ºä¹ä¸“ä¸šé¢†åŸŸçš„çŸ¥è¯†ï¼Œå…¶åº”ç”¨ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡åˆ›æ–°æ€§åœ°æå‡ºäº†ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡çš„SARå›¾åƒå¤šæ¨¡æ€å¯¹è¯æ•°æ®é›†SARChat-2Mï¼ŒåŒ…å«çº¦200ä¸‡å¯¹é«˜è´¨é‡çš„å›¾åƒ-æ–‡æœ¬é…å¯¹ï¼Œæ¶µç›–äº†å¤šç§åœºæ™¯å’Œè¯¦ç»†çš„ç›®æ ‡æ³¨é‡Šã€‚è¯¥æ•°æ®é›†ä¸ä»…æ”¯æŒè§†è§‰ç†è§£å’Œç›®æ ‡æ£€æµ‹ç­‰å…³é”®ä»»åŠ¡ï¼Œè¿˜å¼€å‘äº†SARé¢†åŸŸçš„è§†è§‰è¯­è¨€æ•°æ®é›†å’ŒåŸºå‡†ï¼Œè¯„ä¼°VLMsåœ¨SARå›¾åƒè§£è¯»ä¸­çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹16ä¸ªä¸»æµVLMçš„å®éªŒéªŒè¯ï¼Œè¯¥æ•°æ®é›†çš„æœ‰æ•ˆæ€§å¾—åˆ°äº†å……åˆ†è¯æ˜ï¼Œå¹¶æˆåŠŸå»ºç«‹äº†SARé¢†åŸŸçš„ç¬¬ä¸€ä¸ªå¤šä»»åŠ¡å¯¹è¯åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.08524', 'title': 'LLM Pretraining with Continuous Concepts', 'url': 'https://huggingface.co/papers/2502.08524', 'abstract': "Next token prediction has been the standard training objective used in large language model pretraining. Representations are learned as a result of optimizing for token-level perplexity. We propose Continuous Concept Mixing (CoCoMix), a novel pretraining framework that combines discrete next token prediction with continuous concepts. Specifically, CoCoMix predicts continuous concepts learned from a pretrained sparse autoencoder and mixes them into the model's hidden state by interleaving with token hidden representations. Through experiments on multiple benchmarks, including language modeling and downstream reasoning tasks, we show that CoCoMix is more sample efficient and consistently outperforms standard next token prediction, knowledge distillation and inserting pause tokens. We find that combining both concept learning and interleaving in an end-to-end framework is critical to performance gains. Furthermore, CoCoMix enhances interpretability and steerability by allowing direct inspection and modification of the predicted concept, offering a transparent way to guide the model's internal reasoning process.", 'score': 7, 'issue_id': 2188, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 12', 'zh': '2æœˆ12æ—¥'}, 'hash': '99ad370e7cd11e3c', 'authors': ['Jihoon Tack', 'Jack Lanchantin', 'Jane Yu', 'Andrew Cohen', 'Ilia Kulikov', 'Janice Lan', 'Shibo Hao', 'Yuandong Tian', 'Jason Weston', 'Xian Li'], 'affiliations': ['FAIR at Meta', 'KAIST', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2502.08524.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#interpretability', '#training', '#architecture', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'CoCoMix: ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Continuous Concept Mixing (CoCoMix). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°, CoCoMix ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸ÑĞ¼Ğ¸, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CoCoMix Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, CoCoMix ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Revolutionizing Language Models with Continuous Concept Mixing', 'desc': "This paper introduces Continuous Concept Mixing (CoCoMix), a new framework for pretraining large language models. Unlike traditional methods that focus solely on predicting the next token, CoCoMix integrates continuous concepts derived from a sparse autoencoder into the model's hidden states. This approach improves sample efficiency and enhances performance on various tasks, including language modeling and reasoning. Additionally, CoCoMix allows for better interpretability and control over the model's reasoning by enabling direct manipulation of the predicted concepts."}, 'zh': {'title': 'è¿ç»­æ¦‚å¿µæ··åˆï¼šæå‡è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ä¸å¯è§£é‡Šæ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œç§°ä¸ºè¿ç»­æ¦‚å¿µæ··åˆï¼ˆCoCoMixï¼‰ï¼Œå®ƒç»“åˆäº†ç¦»æ•£çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹å’Œè¿ç»­æ¦‚å¿µã€‚CoCoMixé€šè¿‡å°†ä»é¢„è®­ç»ƒç¨€ç–è‡ªç¼–ç å™¨ä¸­å­¦ä¹ çš„è¿ç»­æ¦‚å¿µä¸æ ‡è®°çš„éšè—è¡¨ç¤ºäº¤é”™æ··åˆï¼Œæ¥ä¼˜åŒ–æ¨¡å‹çš„éšè—çŠ¶æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoCoMixåœ¨æ ·æœ¬æ•ˆç‡ä¸Šè¡¨ç°æ›´ä½³ï¼Œå¹¶ä¸”åœ¨è¯­è¨€å»ºæ¨¡å’Œæ¨ç†ä»»åŠ¡ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹å’ŒçŸ¥è¯†è’¸é¦æ–¹æ³•ã€‚è¯¥æ–¹æ³•è¿˜å¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œå¯å¼•å¯¼æ€§ï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥ç›´æ¥æ£€æŸ¥å’Œä¿®æ”¹é¢„æµ‹çš„æ¦‚å¿µï¼Œä»è€Œé€æ˜åœ°å¼•å¯¼æ¨¡å‹çš„å†…éƒ¨æ¨ç†è¿‡ç¨‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.06145', 'title': 'Animate Anyone 2: High-Fidelity Character Image Animation with Environment Affordance', 'url': 'https://huggingface.co/papers/2502.06145', 'abstract': 'Recent character image animation methods based on diffusion models, such as Animate Anyone, have made significant progress in generating consistent and generalizable character animations. However, these approaches fail to produce reasonable associations between characters and their environments. To address this limitation, we introduce Animate Anyone 2, aiming to animate characters with environment affordance. Beyond extracting motion signals from source video, we additionally capture environmental representations as conditional inputs. The environment is formulated as the region with the exclusion of characters and our model generates characters to populate these regions while maintaining coherence with the environmental context. We propose a shape-agnostic mask strategy that more effectively characterizes the relationship between character and environment. Furthermore, to enhance the fidelity of object interactions, we leverage an object guider to extract features of interacting objects and employ spatial blending for feature injection. We also introduce a pose modulation strategy that enables the model to handle more diverse motion patterns. Experimental results demonstrate the superior performance of the proposed method.', 'score': 4, 'issue_id': 2192, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 10', 'zh': '2æœˆ10æ—¥'}, 'hash': '66fa48cd36ed02b0', 'authors': ['Li Hu', 'Guangyuan Wang', 'Zhen Shen', 'Xin Gao', 'Dechao Meng', 'Lian Zhuo', 'Peng Zhang', 'Bang Zhang', 'Liefeng Bo'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2502.06145.jpg', 'data': {'categories': ['#multimodal', '#cv', '#video', '#diffusion'], 'emoji': 'ğŸ­', 'ru': {'title': 'ĞĞ¶Ğ¸Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Animate Anyone 2 - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ·Ğ´ĞµÑÑŒ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ÑÑ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ğ°ÑĞºĞ¸, Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ°Ñ Ğ¾Ñ‚ Ñ„Ğ¾Ñ€Ğ¼Ñ‹, Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ° Ğ¸ ÑÑ€ĞµĞ´Ñ‹. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ².'}, 'en': {'title': 'Animating Characters with Environmental Awareness', 'desc': 'This paper presents Animate Anyone 2, an advanced character animation method that improves upon previous diffusion models by integrating environmental context into the animation process. Unlike earlier methods, which struggled to connect characters with their surroundings, this approach captures environmental representations as conditional inputs, allowing for more coherent animations. The authors introduce a shape-agnostic mask strategy to better define the relationship between characters and their environments, enhancing the realism of interactions. Additionally, they implement an object guider and spatial blending techniques to refine object interactions and a pose modulation strategy to accommodate diverse motion patterns, resulting in superior animation quality.'}, 'zh': {'title': 'è§’è‰²ä¸ç¯å¢ƒçš„å®Œç¾ç»“åˆ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§’è‰²åŠ¨ç”»æ–¹æ³•Animate Anyone 2ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºäºæ‰©æ•£æ¨¡å‹çš„åŠ¨ç”»æ–¹æ³•åœ¨è§’è‰²ä¸ç¯å¢ƒä¹‹é—´çš„å…³è”ä¸è¶³çš„é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡æå–ç¯å¢ƒè¡¨ç¤ºä½œä¸ºæ¡ä»¶è¾“å…¥ï¼Œä½¿è§’è‰²åŠ¨ç”»èƒ½å¤Ÿä¸ç¯å¢ƒçš„ç‰¹å¾ç›¸ä¸€è‡´ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å½¢çŠ¶æ— å…³çš„æ©ç ç­–ç•¥ï¼Œæ›´æœ‰æ•ˆåœ°æè¿°è§’è‰²ä¸ç¯å¢ƒä¹‹é—´çš„å…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†å§¿æ€è°ƒèŠ‚ç­–ç•¥ï¼Œä»¥å¤„ç†æ›´ä¸°å¯Œçš„è¿åŠ¨æ¨¡å¼ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.05167', 'title': 'NoLiMa: Long-Context Evaluation Beyond Literal Matching', 'url': 'https://huggingface.co/papers/2502.05167', 'abstract': 'Recent large language models (LLMs) support long contexts ranging from 128K to 1M tokens. A popular method for evaluating these capabilities is the needle-in-a-haystack (NIAH) test, which involves retrieving a "needle" (relevant information) from a "haystack" (long irrelevant context). Extensions of this approach include increasing distractors, fact chaining, and in-context reasoning. However, in these benchmarks, models can exploit existing literal matches between the needle and haystack to simplify the task. To address this, we introduce NoLiMa, a benchmark extending NIAH with a carefully designed needle set, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack. We evaluate 12 popular LLMs that claim to support contexts of at least 128K tokens. While they perform well in short contexts (<1K), performance degrades significantly as context length increases. At 32K, for instance, 10 models drop below 50% of their strong short-length baselines. Even GPT-4o, one of the top-performing exceptions, experiences a reduction from an almost-perfect baseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the increased difficulty the attention mechanism faces in longer contexts when literal matches are absent, making it harder to retrieve relevant information.', 'score': 4, 'issue_id': 2188, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 7', 'zh': '2æœˆ7æ—¥'}, 'hash': '4ff0f34526efea9f', 'authors': ['Ali Modarressi', 'Hanieh Deilamsalehy', 'Franck Dernoncourt', 'Trung Bui', 'Ryan A. Rossi', 'Seunghyun Yoon', 'Hinrich SchÃ¼tze'], 'affiliations': ['Adobe Research', 'Center for Information and Language Processing'], 'pdf_title_img': 'assets/pdf/title_img/2502.05167.jpg', 'data': {'categories': ['#benchmark', '#long_context'], 'emoji': 'ğŸ”', 'ru': {'title': 'NoLiMa: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº NoLiMa Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ², NoLiMa Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ¼ Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ 12 Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… LLM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'NoLiMa: Challenging LLMs Beyond Literal Matches', 'desc': 'This paper introduces NoLiMa, a new benchmark designed to evaluate the capabilities of large language models (LLMs) in retrieving relevant information from extensive contexts. Unlike traditional methods that allow models to rely on literal matches, NoLiMa requires models to infer deeper associations between questions and answers with minimal lexical overlap. The study evaluates 12 popular LLMs, revealing that while they perform well in shorter contexts, their performance significantly declines as context length increases, particularly beyond 1K tokens. The findings indicate that the attention mechanism struggles to effectively retrieve relevant information in longer contexts when direct matches are not available.'}, 'zh': {'title': 'é•¿ä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯æ£€ç´¢æŒ‘æˆ˜', 'desc': 'æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ”¯æŒé•¿è¾¾128Kåˆ°1Mçš„ä¸Šä¸‹æ–‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•NoLiMaï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯çš„èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„é’ˆåœ¨å¹²è‰å †ï¼ˆNIAHï¼‰æµ‹è¯•ä¸åŒï¼ŒNoLiMaè®¾è®¡äº†æœ€å°è¯æ±‡é‡å çš„é’ˆé›†ï¼Œè¦æ±‚æ¨¡å‹æ¨æ–­æ½œåœ¨å…³è”ä»¥æ‰¾åˆ°é’ˆã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå°½ç®¡è¿™äº›æ¨¡å‹åœ¨çŸ­ä¸Šä¸‹æ–‡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨é•¿ä¸Šä¸‹æ–‡ä¸­æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹å­—é¢åŒ¹é…çš„æƒ…å†µä¸‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.04411', 'title': 'Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing', 'url': 'https://huggingface.co/papers/2502.04411', 'abstract': 'Model merging aggregates Large Language Models (LLMs) finetuned on different tasks into a stronger one. However, parameter conflicts between models leads to performance degradation in averaging. While model routing addresses this issue by selecting individual models during inference, it imposes excessive storage and compute costs, and fails to leverage the common knowledge from different models. In this work, we observe that different layers exhibit varying levels of parameter conflicts. Building on this insight, we average layers with minimal parameter conflicts and use a novel task-level expert routing for layers with significant conflicts. To further reduce storage costs, inspired by task arithmetic sparsity, we decouple multiple fine-tuned experts into a dense expert and several sparse experts. Considering the out-of-distribution samples, we select and merge appropriate experts based on the task uncertainty of the input data. We conduct extensive experiments on both LLaMA and Qwen with varying parameter scales, and evaluate on real-world reasoning tasks. Results demonstrate that our method consistently achieves significant performance improvements while requiring less system cost compared to existing methods.', 'score': 3, 'issue_id': 2192, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 6', 'zh': '2æœˆ6æ—¥'}, 'hash': '9370051a307713bf', 'authors': ['Kunfeng Lai', 'Zhenheng Tang', 'Xinglin Pan', 'Peijie Dong', 'Xiang Liu', 'Haolan Chen', 'Li Shen', 'Bo Li', 'Xiaowen Chu'], 'affiliations': ['Platform and Content Group, Tencent', 'Sun Yatsen University', 'The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.04411.jpg', 'data': {'categories': ['#model merging', '#training', '#architecture', '#reasoning', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒÑÑ€ĞµĞ´Ğ½ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ»Ğ¾ĞµĞ² ÑĞ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ°Ğ¼Ğ¸. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… LLaMA Ğ¸ Qwen Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Merging Models Smartly: Harnessing Layer Insights for Better Performance', 'desc': 'This paper presents a method for merging Large Language Models (LLMs) that have been fine-tuned on different tasks, aiming to create a more powerful model. The authors identify that parameter conflicts between models can degrade performance when simply averaging them. To address this, they propose a strategy that averages layers with minimal conflicts and employs task-level expert routing for layers with significant conflicts. Their approach not only reduces storage costs by decoupling fine-tuned experts but also improves performance on real-world reasoning tasks while being more efficient than existing methods.'}, 'zh': {'title': 'æ™ºèƒ½åˆå¹¶ï¼Œæå‡æ¨¡å‹æ€§èƒ½ï¼', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ¨¡å‹åˆå¹¶çš„æ–¹æ³•ï¼Œæ—¨åœ¨å°†ä¸åŒä»»åŠ¡ä¸Šå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èšåˆæˆä¸€ä¸ªæ›´å¼ºçš„æ¨¡å‹ã€‚æˆ‘ä»¬å‘ç°ä¸åŒå±‚ä¹‹é—´çš„å‚æ•°å†²çªç¨‹åº¦ä¸åŒï¼Œå› æ­¤æˆ‘ä»¬å¯¹å‚æ•°å†²çªè¾ƒå°çš„å±‚è¿›è¡Œå¹³å‡ï¼Œè€Œå¯¹å‚æ•°å†²çªè¾ƒå¤§çš„å±‚é‡‡ç”¨æ–°é¢–çš„ä»»åŠ¡çº§ä¸“å®¶è·¯ç”±ã€‚ä¸ºäº†è¿›ä¸€æ­¥é™ä½å­˜å‚¨æˆæœ¬ï¼Œæˆ‘ä»¬å°†å¤šä¸ªå¾®è°ƒçš„ä¸“å®¶è§£è€¦ä¸ºä¸€ä¸ªç¨ å¯†ä¸“å®¶å’Œå‡ ä¸ªç¨€ç–ä¸“å®¶ï¼Œå¹¶æ ¹æ®è¾“å…¥æ•°æ®çš„ä»»åŠ¡ä¸ç¡®å®šæ€§é€‰æ‹©å’Œåˆå¹¶åˆé€‚çš„ä¸“å®¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶å‡å°‘äº†ç³»ç»Ÿæˆæœ¬ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.06872', 'title': 'Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey', 'url': 'https://huggingface.co/papers/2502.06872', 'abstract': "Retrieval-Augmented Generation (RAG) is an advanced technique designed to address the challenges of Artificial Intelligence-Generated Content (AIGC). By integrating context retrieval into content generation, RAG provides reliable and up-to-date external knowledge, reduces hallucinations, and ensures relevant context across a wide range of tasks. However, despite RAG's success and potential, recent studies have shown that the RAG paradigm also introduces new risks, including robustness issues, privacy concerns, adversarial attacks, and accountability issues. Addressing these risks is critical for future applications of RAG systems, as they directly impact their trustworthiness. Although various methods have been developed to improve the trustworthiness of RAG methods, there is a lack of a unified perspective and framework for research in this topic. Thus, in this paper, we aim to address this gap by providing a comprehensive roadmap for developing trustworthy RAG systems. We place our discussion around five key perspectives: reliability, privacy, safety, fairness, explainability, and accountability. For each perspective, we present a general framework and taxonomy, offering a structured approach to understanding the current challenges, evaluating existing solutions, and identifying promising future research directions. To encourage broader adoption and innovation, we also highlight the downstream applications where trustworthy RAG systems have a significant impact.", 'score': 3, 'issue_id': 2188, 'pub_date': '2025-02-08', 'pub_date_card': {'ru': '8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 8', 'zh': '2æœˆ8æ—¥'}, 'hash': 'f454782ce3101c66', 'authors': ['Bo Ni', 'Zheyuan Liu', 'Leyao Wang', 'Yongjia Lei', 'Yuying Zhao', 'Xueqi Cheng', 'Qingkai Zeng', 'Luna Dong', 'Yinglong Xia', 'Krishnaram Kenthapadi', 'Ryan Rossi', 'Franck Dernoncourt', 'Md Mehrab Tanjim', 'Nesreen Ahmed', 'Xiaorui Liu', 'Wenqi Fan', 'Erik Blasch', 'Yu Wang', 'Meng Jiang', 'Tyler Derr'], 'affiliations': ['Adobe Research', 'Air Force Research Lab', 'Cisco AI Research', 'Meta', 'North Carolina State University', 'Oracle Health AI', 'The Hong Kong Polytechnic University', 'University of Notre Dame', 'University of Oregon', 'Vanderbilt University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06872.jpg', 'data': {'categories': ['#hallucinations', '#interpretability', '#security', '#survey', '#rag', '#ethics'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑƒÑ‚ÑŒ Ğº Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜: ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡Ñ‘Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡Ñ‘Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ (RAG), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ¸ÑĞºĞ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ RAG, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ°Ñ ĞºĞ°Ñ€Ñ‚Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… RAG-ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ½Ğ° Ğ¿ÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ². Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾ÑĞ²ĞµÑ‰Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… RAG-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ….'}, 'en': {'title': 'Building Trust in Retrieval-Augmented Generation Systems', 'desc': 'This paper discusses Retrieval-Augmented Generation (RAG), a technique that enhances AI-generated content by incorporating external knowledge for better accuracy and relevance. While RAG improves content generation, it also brings new challenges such as robustness, privacy, and accountability issues that need to be addressed. The authors propose a comprehensive framework that focuses on five key areas: reliability, privacy, safety, fairness, and explainability, to guide future research and development of trustworthy RAG systems. By providing a structured approach, the paper aims to foster innovation and highlight the importance of trust in RAG applications.'}, 'zh': {'title': 'æ„å»ºå¯ä¿¡èµ–çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ', 'desc': 'æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯ä¸€ç§å…ˆè¿›çš„æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰é¢ä¸´çš„æŒ‘æˆ˜ã€‚é€šè¿‡å°†ä¸Šä¸‹æ–‡æ£€ç´¢ä¸å†…å®¹ç”Ÿæˆç›¸ç»“åˆï¼ŒRAG æä¾›å¯é ä¸”æœ€æ–°çš„å¤–éƒ¨çŸ¥è¯†ï¼Œå‡å°‘å¹»è§‰ç°è±¡ï¼Œå¹¶ç¡®ä¿åœ¨å„ç§ä»»åŠ¡ä¸­ä¿æŒç›¸å…³ä¸Šä¸‹æ–‡ã€‚ç„¶è€Œï¼Œå°½ç®¡ RAG å–å¾—äº†æˆåŠŸï¼Œä½†æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œè¯¥èŒƒå¼ä¹Ÿå¼•å…¥äº†æ–°çš„é£é™©ï¼ŒåŒ…æ‹¬é²æ£’æ€§é—®é¢˜ã€éšç§é—®é¢˜ã€å¯¹æŠ—æ€§æ”»å‡»å’Œé—®è´£é—®é¢˜ã€‚æœ¬æ–‡æ—¨åœ¨æä¾›ä¸€ä¸ªå…¨é¢çš„è·¯çº¿å›¾ï¼Œä»¥å¼€å‘å¯ä¿¡èµ–çš„ RAG ç³»ç»Ÿï¼Œå›´ç»•å¯é æ€§ã€éšç§ã€å®‰å…¨æ€§ã€å…¬å¹³æ€§ã€å¯è§£é‡Šæ€§å’Œé—®è´£æ€§ç­‰äº”ä¸ªå…³é”®è§†è§’è¿›è¡Œè®¨è®ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.07737', 'title': 'Next Block Prediction: Video Generation via Semi-Autoregressive Modeling', 'url': 'https://huggingface.co/papers/2502.07737', 'abstract': 'Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR) video generation, but it suffers from suboptimal unidirectional dependencies and slow inference speed. In this work, we propose a semi-autoregressive (semi-AR) framework, called Next-Block Prediction (NBP), for video generation. By uniformly decomposing video content into equal-sized blocks (e.g., rows or frames), we shift the generation unit from individual tokens to blocks, allowing each token in the current block to simultaneously predict the corresponding token in the next block. Unlike traditional AR modeling, our framework employs bidirectional attention within each block, enabling tokens to capture more robust spatial dependencies. By predicting multiple tokens in parallel, NBP models significantly reduce the number of generation steps, leading to faster and more efficient inference. Our model achieves FVD scores of 103.3 on UCF101 and 25.5 on K600, outperforming the vanilla NTP model by an average of 4.4. Furthermore, thanks to the reduced number of inference steps, the NBP model generates 8.89 frames (128x128 resolution) per second, achieving an 11x speedup. We also explored model scales ranging from 700M to 3B parameters, observing significant improvements in generation quality, with FVD scores dropping from 103.3 to 55.3 on UCF101 and from 25.5 to 19.5 on K600, demonstrating the scalability of our approach.', 'score': 3, 'issue_id': 2186, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 11', 'zh': '2æœˆ11æ—¥'}, 'hash': '8038af0ecacc031f', 'authors': ['Shuhuai Ren', 'Shuming Ma', 'Xu Sun', 'Furu Wei'], 'affiliations': ['Microsoft Research', 'National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2502.07737.jpg', 'data': {'categories': ['#optimization', '#architecture', '#video', '#inference', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'NBP: Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±Ğ»Ğ¾ĞºĞ°Ğ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Next-Block Prediction (NBP). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Next-Token Prediction, NBP Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒĞ°Ğ²Ñ‚Ğ¾Ñ€ĞµrÑ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ±Ğ»Ğ¾ĞºĞ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ NBP Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ FVD Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… UCF101 Ğ¸ K600, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Revolutionizing Video Generation with Next-Block Prediction', 'desc': 'This paper introduces a new method for generating videos called Next-Block Prediction (NBP), which improves upon the traditional Next-Token Prediction (NTP) approach. NBP changes the way video content is generated by using blocks instead of individual tokens, allowing for simultaneous predictions within each block. This method utilizes bidirectional attention to enhance the understanding of spatial relationships in the video, leading to better quality outputs. As a result, NBP not only speeds up the generation process significantly but also achieves superior performance metrics compared to the standard NTP model.'}, 'zh': {'title': 'è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´ï¼šä¸‹ä¸€å—é¢„æµ‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŠè‡ªå›å½’æ¡†æ¶ï¼Œç§°ä¸ºä¸‹ä¸€å—é¢„æµ‹ï¼ˆNBPï¼‰ï¼Œç”¨äºè§†é¢‘ç”Ÿæˆã€‚ä¸ä¼ ç»Ÿçš„è‡ªå›å½’æ–¹æ³•ä¸åŒï¼ŒNBPé€šè¿‡å°†è§†é¢‘å†…å®¹å‡åŒ€åˆ†è§£ä¸ºç›¸ç­‰å¤§å°çš„å—ï¼Œä½¿å¾—æ¯ä¸ªå—å†…çš„æ ‡è®°å¯ä»¥åŒæ—¶é¢„æµ‹ä¸‹ä¸€ä¸ªå—çš„å¯¹åº”æ ‡è®°ï¼Œä»è€Œæ•æ‰æ›´å¼ºçš„ç©ºé—´ä¾èµ–æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å¹¶è¡Œé¢„æµ‹å¤šä¸ªæ ‡è®°ï¼Œæ˜¾è‘—å‡å°‘äº†ç”Ÿæˆæ­¥éª¤ï¼Œæé«˜äº†æ¨ç†é€Ÿåº¦ï¼Œè¾¾åˆ°äº†æ¯ç§’ç”Ÿæˆ8.89å¸§çš„æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNBPåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶åœ¨ç”Ÿæˆè´¨é‡å’Œé€Ÿåº¦ä¸Šçš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.07599', 'title': 'DPO-Shift: Shifting the Distribution of Direct Preference Optimization', 'url': 'https://huggingface.co/papers/2502.07599', 'abstract': 'Direct Preference Optimization (DPO) and its variants have become increasingly popular for aligning language models with human preferences. These methods aim to teach models to better distinguish between chosen (or preferred) and rejected (or dispreferred) responses. However, prior research has identified that the probability of chosen responses often decreases during training, and this phenomenon is known as likelihood displacement. To tackle this challenge, in this work we introduce \\method to controllably shift the distribution of the chosen probability. Then, we show that \\method exhibits a fundamental trade-off between improving the chosen probability and sacrificing the reward margin, as supported by both theoretical analysis and experimental validation. Furthermore, we demonstrate the superiority of \\method over DPO on downstream tasks such as MT-Bench and a designed win rate experiment. We believe this study shows that the likelihood displacement issue of DPO can be effectively mitigated with a simple, theoretically grounded solution. Our code is available at https://github.com/Meaquadddd/DPO-Shift.', 'score': 3, 'issue_id': 2186, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 11', 'zh': '2æœˆ11æ—¥'}, 'hash': '85d178e03a57421f', 'authors': ['Xiliang Yang', 'Feng Jiang', 'Qianen Zhang', 'Lei Zhao', 'Xiao Li'], 'affiliations': ['Institute of Translational Medicine and National Center for Translational Medicine, Shanghai Jiao Tong University', 'School of Data Science, The Chinese University of Hong Kong, Shenzhen', 'School of Mathematics, South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.07599.jpg', 'data': {'categories': ['#alignment', '#training', '#rlhf'], 'emoji': 'ğŸ”€', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ DPO-Shift Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Direct Preference Optimization (DPO). DPO-Shift Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ ÑĞ¼ĞµÑ‰Ğ°Ñ‚ÑŒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ DPO-Shift Ğ½Ğ°Ğ´ DPO Ğ½Ğ° Ñ€ÑĞ´Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ MT-Bench.'}, 'en': {'title': 'Mitigating Likelihood Displacement in Language Model Training', 'desc': 'This paper addresses the issue of likelihood displacement in Direct Preference Optimization (DPO) for aligning language models with human preferences. The authors introduce a new method, referred to as \textit{method}, which aims to control the distribution of chosen response probabilities during training. They highlight a trade-off between enhancing the chosen probability and the reward margin, supported by both theoretical insights and experimental results. The findings indicate that \textit{method} outperforms DPO in various downstream tasks, providing a promising solution to the challenges posed by likelihood displacement.'}, 'zh': {'title': 'è§£å†³é€‰æ‹©æ¦‚ç‡ä¸‹é™çš„æœ‰æ•ˆæ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•\textit{method}ï¼Œæ—¨åœ¨è§£å†³ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä¸­å‡ºç°çš„é€‰æ‹©æ¦‚ç‡ä¸‹é™é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å¯¹é€‰æ‹©å“åº”çš„æ¦‚ç‡å¾€å¾€ä¼šé™ä½ï¼Œè¿™è¢«ç§°ä¸ºä¼¼ç„¶ä½ç§»ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ§åˆ¶é€‰æ‹©æ¦‚ç‡çš„åˆ†å¸ƒï¼Œä»è€Œæ”¹å–„æ¨¡å‹çš„è¡¨ç°ã€‚é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼Œæˆ‘ä»¬è¯æ˜äº†\textit{method}åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ä¼˜äºä¼ ç»Ÿçš„DPOæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.08213', 'title': 'LLM Modules: Knowledge Transfer from a Large to a Small Model using Enhanced Cross-Attention', 'url': 'https://huggingface.co/papers/2502.08213', 'abstract': 'In this work, we propose an architecture of LLM Modules that enables the transfer of knowledge from a large pre-trained model to a smaller model using an Enhanced Cross-Attention mechanism. In the proposed scheme, the Qwen2-1.5B model is frozen and its representations are passed through specially designed attention layers to the GPT-Neo-125M model, which is trained on limited computational resources. Experimental results on the Bespoke-Stratos-17k dataset demonstrate that after 15 epochs of training, the combined model generates responses comparable in quality to those obtained by distillation. We discuss the advantages of the modular approach, provide examples of input queries and comparative analysis, and outline prospects for further extension of the method.', 'score': 1, 'issue_id': 2194, 'pub_date': '2025-02-12', 'pub_date_card': {'ru': '12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 12', 'zh': '2æœˆ12æ—¥'}, 'hash': '2623177431838d6c', 'authors': ['Konstantin Kolomeitsev'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.08213.jpg', 'data': {'categories': ['#small_models', '#transfer_learning', '#optimization', '#architecture', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹ LLM, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen2-1.5B Ğ¿ĞµÑ€ĞµĞ´Ğ°ĞµÑ‚ ÑĞ²Ğ¾Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GPT-Neo-125M, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Bespoke-Stratos-17k Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ÑĞ»Ğµ 15 ÑĞ¿Ğ¾Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Empowering Smaller Models with Enhanced Knowledge Transfer', 'desc': 'This paper introduces a new architecture called LLM Modules that facilitates knowledge transfer from a large pre-trained model to a smaller model using an Enhanced Cross-Attention mechanism. The Qwen2-1.5B model is kept unchanged, and its learned representations are utilized by the smaller GPT-Neo-125M model, which is optimized for limited computational resources. The results from experiments on the Bespoke-Stratos-17k dataset show that after 15 training epochs, the performance of the combined model is on par with traditional distillation methods. The authors highlight the benefits of this modular approach and provide examples and analyses to support their findings, while also discussing future enhancements.'}, 'zh': {'title': 'çŸ¥è¯†è½¬ç§»çš„æ–°æ–¹æ³•ï¼šæ¨¡å—åŒ–æ¶æ„ä¸å¢å¼ºäº¤å‰æ³¨æ„åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§LLMæ¨¡å—æ¶æ„ï¼Œèƒ½å¤Ÿé€šè¿‡å¢å¼ºçš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å°†çŸ¥è¯†ä»å¤§å‹é¢„è®­ç»ƒæ¨¡å‹è½¬ç§»åˆ°è¾ƒå°çš„æ¨¡å‹ã€‚æˆ‘ä»¬å°†Qwen2-1.5Bæ¨¡å‹å›ºå®šï¼Œå¹¶é€šè¿‡ä¸“é—¨è®¾è®¡çš„æ³¨æ„åŠ›å±‚å°†å…¶è¡¨ç¤ºä¼ é€’ç»™åœ¨æœ‰é™è®¡ç®—èµ„æºä¸Šè®­ç»ƒçš„GPT-Neo-125Mæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨Bespoke-Stratos-17kæ•°æ®é›†ä¸Šç»è¿‡15ä¸ªè®­ç»ƒå‘¨æœŸåï¼Œç»„åˆæ¨¡å‹ç”Ÿæˆçš„å“åº”è´¨é‡ä¸è’¸é¦è·å¾—çš„ç»“æœç›¸å½“ã€‚æˆ‘ä»¬è®¨è®ºäº†æ¨¡å—åŒ–æ–¹æ³•çš„ä¼˜åŠ¿ï¼Œæä¾›äº†è¾“å…¥æŸ¥è¯¢çš„ç¤ºä¾‹å’Œæ¯”è¾ƒåˆ†æï¼Œå¹¶æ¦‚è¿°äº†è¯¥æ–¹æ³•è¿›ä¸€æ­¥æ‰©å±•çš„å‰æ™¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.07985', 'title': 'MetaSC: Test-Time Safety Specification Optimization for Language Models', 'url': 'https://huggingface.co/papers/2502.07985', 'abstract': 'We propose a novel dynamic safety framework that optimizes language model (LM) safety reasoning at inference time without modifying model weights. Building on recent advances in self-critique methods, our approach leverages a meta-critique mechanism that iteratively updates safety prompts-termed specifications-to drive the critique and revision process adaptively. This test-time optimization not only improves performance against adversarial jailbreak requests but also in diverse general safety-related tasks, such as avoiding moral harm or pursuing honest responses. Our empirical evaluations across several language models demonstrate that dynamically optimized safety prompts yield significantly higher safety scores compared to fixed system prompts and static self-critique defenses. Code to be released at https://github.com/vicgalle/meta-self-critique.git .', 'score': 1, 'issue_id': 2190, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 11', 'zh': '2æœˆ11æ—¥'}, 'hash': '68244a5483bfc513', 'authors': ['VÃ­ctor Gallego'], 'affiliations': ['Komorebi AI, Madrid, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2502.07985.jpg', 'data': {'categories': ['#optimization', '#training', '#security', '#alignment', '#inference'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğµ Ğ¼ĞµÑ‚Ğ°-ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ğ°. ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñƒ Ğ¾Ñ‚ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Dynamic Safety for Language Models: Adapting Prompts for Better Protection', 'desc': "This paper introduces a dynamic safety framework designed to enhance the safety of language models during inference without altering their underlying weights. It utilizes a meta-critique mechanism that adaptively updates safety prompts, known as specifications, to improve the model's ability to handle adversarial inputs and general safety tasks. The approach shows significant improvements in safety performance, particularly in avoiding moral harm and ensuring honest responses. Empirical results indicate that the dynamically optimized safety prompts outperform traditional fixed prompts and static defenses in various scenarios."}, 'zh': {'title': 'åŠ¨æ€ä¼˜åŒ–ï¼Œæå‡è¯­è¨€æ¨¡å‹å®‰å…¨æ€§ï¼', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŠ¨æ€å®‰å…¨æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ—¶çš„å®‰å…¨æ€§æ¨ç†ï¼Œè€Œæ— éœ€ä¿®æ”¹æ¨¡å‹æƒé‡ã€‚è¯¥æ–¹æ³•åŸºäºè‡ªæˆ‘æ‰¹è¯„æ–¹æ³•çš„æœ€æ–°è¿›å±•ï¼Œåˆ©ç”¨å…ƒæ‰¹è¯„æœºåˆ¶è¿­ä»£æ›´æ–°å®‰å…¨æç¤ºï¼ˆç§°ä¸ºè§„èŒƒï¼‰ï¼Œä»¥è‡ªé€‚åº”åœ°æ¨åŠ¨æ‰¹è¯„å’Œä¿®è®¢è¿‡ç¨‹ã€‚æ­¤æµ‹è¯•æ—¶ä¼˜åŒ–ä¸ä»…æé«˜äº†å¯¹æŠ—æ€§è¶Šç‹±è¯·æ±‚çš„æ€§èƒ½ï¼Œè¿˜åœ¨é¿å…é“å¾·ä¼¤å®³å’Œè¿½æ±‚è¯šå®å›åº”ç­‰å¤šç§å®‰å…¨ç›¸å…³ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬çš„å®è¯è¯„ä¼°æ˜¾ç¤ºï¼ŒåŠ¨æ€ä¼˜åŒ–çš„å®‰å…¨æç¤ºç›¸æ¯”äºå›ºå®šç³»ç»Ÿæç¤ºå’Œé™æ€è‡ªæˆ‘æ‰¹è¯„é˜²å¾¡ï¼Œæ˜¾è‘—æé«˜äº†å®‰å…¨è¯„åˆ†ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (1)', '#agi', '#alignment (2)', '#architecture (6)', '#audio', '#benchmark (7)', '#cv (3)', '#data', '#dataset (5)', '#diffusion (3)', '#ethics (1)', '#games (1)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (3)', '#interpretability (2)', '#leakage', '#long_context (6)', '#low_resource (1)', '#machine_translation (1)', '#math', '#multilingual (1)', '#multimodal (2)', '#open_source (4)', '#optimization (9)', '#plp', '#rag (1)', '#reasoning (4)', '#rl (2)', '#rlhf (1)', '#robotics', '#science', '#security (2)', '#small_models (2)', '#story_generation', '#survey (1)', '#synthetic (1)', '#training (11)', '#transfer_learning (1)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-02-13 12:18',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-02-13 12:18')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-02-13 12:18')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    