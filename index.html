
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 10 papers. May 14.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">14 мая</span> | <span id="title-articles-count">10 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-05-13.html">⬅️ <span id="prev-date">13.05</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-05-15.html">➡️ <span id="next-date">15.05</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-05.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '14 мая', 'en': 'May 14', 'zh': '5月14日'};
        let feedDateNext = {'ru': '15.05', 'en': '05/15', 'zh': '5月15日'};
        let feedDatePrev = {'ru': '13.05', 'en': '05/13', 'zh': '5月13日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2505.07916', 'title': 'MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with a Learnable\n  Speaker Encoder', 'url': 'https://huggingface.co/papers/2505.07916', 'abstract': 'We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. A key innovation is our learnable speaker encoder, which extracts timbre features from a reference audio without requiring its transcription. This enables MiniMax-Speech to produce highly expressive speech with timbre consistent with the reference in a zero-shot manner, while also supporting one-shot voice cloning with exceptionally high similarity to the reference voice. In addition, the overall quality of the synthesized audio is enhanced through the proposed Flow-VAE. Our model supports 32 languages and demonstrates excellent performance across multiple objective and subjective evaluations metrics. Notably, it achieves state-of-the-art (SOTA) results on objective voice cloning metrics (Word Error Rate and Speaker Similarity) and has secured the top position on the public TTS Arena leaderboard. Another key strength of MiniMax-Speech, granted by the robust and disentangled representations from the speaker encoder, is its extensibility without modifying the base model, enabling various applications such as: arbitrary voice emotion control via LoRA; text to voice (T2V) by synthesizing timbre features directly from text description; and professional voice cloning (PVC) by fine-tuning timbre features with additional data. We encourage readers to visit https://minimax-ai.github.io/tts_tech_report for more examples.', 'score': 75, 'issue_id': 3752, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '30175415a859995c', 'authors': ['Bowen Zhang', 'Congchao Guo', 'Geng Yang', 'Hang Yu', 'Haozhe Zhang', 'Heidi Lei', 'Jialong Mai', 'Junjie Yan', 'Kaiyue Yang', 'Mingqi Yang', 'Peikai Huang', 'Ruiyang Jin', 'Sitan Jiang', 'Weihua Cheng', 'Yawei Li', 'Yichen Xiao', 'Yiying Zhou', 'Yongmao Zhang', 'Yuan Lu', 'Yucen He'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07916.jpg', 'data': {'categories': ['#optimization', '#multilingual', '#games', '#audio'], 'emoji': '🗣️', 'ru': {'title': 'Революция в синтезе речи: MiniMax-Speech - универсальный голосовой клон', 'desc': 'MiniMax-Speech - это автоматическая модель синтеза речи на основе Трансформера. Ключевой инновацией является обучаемый кодировщик говорящего, который извлекает характеристики тембра из эталонного аудио без необходимости его транскрипции. Модель поддерживает 32 языка и демонстрирует отличные результаты по многим метрикам, достигая лучших показателей в клонировании голоса. MiniMax-Speech также обладает расширяемостью, позволяющей реализовать различные приложения, такие как управление эмоциями голоса и синтез тембра из текстового описания.'}, 'en': {'title': 'Revolutionizing Speech Synthesis with MiniMax-Speech', 'desc': 'MiniMax-Speech is a cutting-edge Text-to-Speech (TTS) model that utilizes an autoregressive Transformer architecture to generate high-quality speech. A significant feature of this model is its learnable speaker encoder, which captures timbre characteristics from audio samples without needing their text transcriptions. This allows the model to create expressive speech that matches the timbre of the reference audio in a zero-shot manner, and it can also perform one-shot voice cloning with remarkable accuracy. Additionally, MiniMax-Speech enhances audio quality through a Flow-VAE and supports multiple languages, achieving state-of-the-art results in voice cloning metrics and demonstrating versatility for various applications.'}, 'zh': {'title': 'MiniMax-Speech：高质量语音生成的新突破', 'desc': 'MiniMax-Speech是一种基于自回归Transformer的文本到语音（TTS）模型，能够生成高质量的语音。其创新之处在于可学习的说话人编码器，可以从参考音频中提取音色特征，而无需其转录。该模型支持零样本生成具有参考音色的表达性语音，并且在单样本语音克隆中表现出极高的相似度。此外，MiniMax-Speech在多个客观和主观评估指标上表现优异，支持32种语言，并在语音克隆的客观指标上达到了最先进的水平。'}}}, {'id': 'https://huggingface.co/papers/2505.07591', 'title': 'A Multi-Dimensional Constraint Framework for Evaluating and Improving\n  Instruction Following in Large Language Models', 'url': 'https://huggingface.co/papers/2505.07591', 'abstract': "Instruction following evaluates large language models (LLMs) on their ability to generate outputs that adhere to user-defined constraints. However, existing benchmarks often rely on templated constraint prompts, which lack the diversity of real-world usage and limit fine-grained performance assessment. To fill this gap, we propose a multi-dimensional constraint framework encompassing three constraint patterns, four constraint categories, and four difficulty levels. Building on this framework, we develop an automated instruction generation pipeline that performs constraint expansion, conflict detection, and instruction rewriting, yielding 1,200 code-verifiable instruction-following test samples. We evaluate 19 LLMs across seven model families and uncover substantial variation in performance across constraint forms. For instance, average performance drops from 77.67% at Level I to 32.96% at Level IV. Furthermore, we demonstrate the utility of our approach by using it to generate data for reinforcement learning, achieving substantial gains in instruction following without degrading general performance. In-depth analysis indicates that these gains stem primarily from modifications in the model's attention modules parameters, which enhance constraint recognition and adherence. Code and data are available in https://github.com/Junjie-Ye/MulDimIF.", 'score': 5, 'issue_id': 3750, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': 'ca7c47ccc0066e55', 'authors': ['Junjie Ye', 'Caishuang Huang', 'Zhuohan Chen', 'Wenjie Fu', 'Chenyuan Yang', 'Leyi Yang', 'Yilong Wu', 'Peng Wang', 'Meng Zhou', 'Xiaolong Yang', 'Tao Gui', 'Qi Zhang', 'Zhongchao Shi', 'Jianping Fan', 'Xuanjing Huang'], 'affiliations': ['Institute of Modern Languages and Linguistics, Fudan University', 'Lenovo Research', 'School of Computer Science, Fudan University', 'Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2505.07591.jpg', 'data': {'categories': ['#training', '#rl', '#alignment', '#optimization', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Многомерная оценка следования инструкциям для больших языковых моделей', 'desc': 'Статья представляет новый подход к оценке способности больших языковых моделей (LLM) следовать инструкциям пользователей. Авторы разработали многомерную систему ограничений, включающую три паттерна, четыре категории и четыре уровня сложности. На основе этой системы был создан автоматизированный процесс генерации инструкций, который позволил создать 1200 проверяемых тестовых примеров. Исследование 19 LLM показало значительные различия в производительности в зависимости от типа ограничений, причем средняя производительность снижалась с 77,67% на уровне I до 32,96% на уровне IV.'}, 'en': {'title': 'Enhancing LLMs with Multi-Dimensional Constraints', 'desc': 'This paper evaluates large language models (LLMs) on their ability to follow user-defined constraints using a new multi-dimensional constraint framework. The framework includes various constraint patterns, categories, and difficulty levels, allowing for a more nuanced assessment of model performance. An automated instruction generation pipeline is developed to create diverse test samples, revealing significant performance variations among different LLMs when faced with increasing constraint complexity. The study also shows that using this framework for reinforcement learning can improve instruction adherence without compromising overall model performance, primarily by adjusting attention module parameters.'}, 'zh': {'title': '多维约束框架提升语言模型性能', 'desc': '本文探讨了如何评估大型语言模型（LLMs）在遵循用户定义约束方面的能力。现有的基准测试通常依赖于模板化的约束提示，缺乏真实世界使用的多样性，限制了细致的性能评估。为了解决这个问题，我们提出了一个多维约束框架，涵盖三种约束模式、四种约束类别和四个难度级别。通过这个框架，我们开发了一个自动化指令生成管道，生成了1200个可验证的指令遵循测试样本，并评估了19个LLM在不同约束形式下的表现。'}}}, {'id': 'https://huggingface.co/papers/2505.07215', 'title': 'Measuring General Intelligence with Generated Games', 'url': 'https://huggingface.co/papers/2505.07215', 'abstract': 'We present gg-bench, a collection of game environments designed to evaluate general reasoning capabilities in language models. Unlike most static benchmarks, gg-bench is a data generating process where new evaluation instances can be generated at will. In particular, gg-bench is synthetically generated by (1) using a large language model (LLM) to generate natural language descriptions of novel games, (2) using the LLM to implement each game in code as a Gym environment, and (3) training reinforcement learning (RL) agents via self-play on the generated games. We evaluate language models by their winrate against these RL agents by prompting models with the game description, current board state, and a list of valid moves, after which models output the moves they wish to take. gg-bench is challenging: state-of-the-art LLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench using in-context learning, while reasoning models such as o1, o3-mini and DeepSeek-R1 achieve average winrates of 31-36%. We release the generated games, data generation process, and evaluation code in order to support future modeling work and expansion of our benchmark.', 'score': 4, 'issue_id': 3751, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': 'ed99ec3875dd9a95', 'authors': ['Vivek Verma', 'David Huang', 'William Chen', 'Dan Klein', 'Nicholas Tomlin'], 'affiliations': ['Computer Science Division, University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2505.07215.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#reasoning', '#rl', '#games', '#synthetic'], 'emoji': '🎮', 'ru': {'title': 'gg-bench: Новый подход к оценке способностей языковых моделей через игровые среды', 'desc': 'В статье представлен gg-bench - набор игровых сред для оценки способностей языковых моделей к общему рассуждению. gg-bench автоматически генерирует новые игры с помощью большой языковой модели (LLM), которая создает описания и код игр. Для оценки языковые модели играют против обученных с помощью обучения с подкреплением агентов. Результаты показывают, что современные LLM достигают низких результатов (7-9% побед), в то время как специализированные модели рассуждений достигают 31-36% побед.'}, 'en': {'title': 'Dynamic Game Environments for Evaluating AI Reasoning', 'desc': 'The paper introduces gg-bench, a novel benchmark for assessing the reasoning abilities of language models through interactive game environments. Unlike traditional benchmarks, gg-bench allows for the dynamic generation of new game instances using a large language model (LLM) to create game descriptions and implement them as Gym environments. The evaluation involves training reinforcement learning (RL) agents to compete against language models, measuring their performance based on win rates. The results show that while state-of-the-art LLMs achieve low win rates, specialized reasoning models perform significantly better, highlighting the challenges in evaluating general reasoning in AI.'}, 'zh': {'title': 'gg-bench：评估语言模型推理能力的新基准', 'desc': '我们提出了gg-bench，这是一个用于评估语言模型一般推理能力的游戏环境集合。与大多数静态基准测试不同，gg-bench是一个数据生成过程，可以随时生成新的评估实例。具体来说，gg-bench通过使用大型语言模型生成新游戏的自然语言描述，并将每个游戏实现为Gym环境，最后通过自我对弈训练强化学习代理。我们通过模型在这些游戏中与强化学习代理的胜率来评估语言模型，发现当前最先进的语言模型在gg-bench上的胜率仅为7-9%。'}}}, {'id': 'https://huggingface.co/papers/2505.08665', 'title': 'SkillFormer: Unified Multi-View Video Understanding for Proficiency\n  Estimation', 'url': 'https://huggingface.co/papers/2505.08665', 'abstract': 'Assessing human skill levels in complex activities is a challenging problem with applications in sports, rehabilitation, and training. In this work, we present SkillFormer, a parameter-efficient architecture for unified multi-view proficiency estimation from egocentric and exocentric videos. Building on the TimeSformer backbone, SkillFormer introduces a CrossViewFusion module that fuses view-specific features using multi-head cross-attention, learnable gating, and adaptive self-calibration. We leverage Low-Rank Adaptation to fine-tune only a small subset of parameters, significantly reducing training costs. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves state-of-the-art accuracy in multi-view settings while demonstrating remarkable computational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer training epochs than prior baselines. It excels in multiple structured tasks, confirming the value of multi-view integration for fine-grained skill assessment.', 'score': 1, 'issue_id': 3750, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': '8cbf16dc2ec90273', 'authors': ['Edoardo Bianchi', 'Antonio Liotta'], 'affiliations': ['Free University of Bozen-Bolzano Bozen-Bolzano, IT'], 'pdf_title_img': 'assets/pdf/title_img/2505.08665.jpg', 'data': {'categories': ['#architecture', '#dataset', '#training'], 'emoji': '🎥', 'ru': {'title': 'SkillFormer: эффективная оценка мастерства по мультиракурсному видео', 'desc': 'SkillFormer - это эффективная архитектура для оценки уровня мастерства в сложных действиях по видео с нескольких ракурсов. Она использует модуль CrossViewFusion для объединения признаков из разных ракурсов с помощью кросс-внимания и адаптивной самокалибровки. Благодаря технике Low-Rank Adaptation, SkillFormer дообучает лишь небольшую часть параметров, значительно сокращая вычислительные затраты. На датасете EgoExo4D модель достигает лучшей точности при использовании в 4,5 раза меньше параметров по сравнению с аналогами.'}, 'en': {'title': 'SkillFormer: Efficient Multi-View Skill Assessment', 'desc': 'This paper introduces SkillFormer, a new machine learning model designed to assess human skill levels in complex activities using videos from different perspectives. It utilizes a CrossViewFusion module that combines features from both egocentric (first-person) and exocentric (third-person) views through advanced techniques like multi-head cross-attention. The model is built on the TimeSformer architecture and employs Low-Rank Adaptation to minimize the number of parameters that need to be trained, making it more efficient. SkillFormer achieves top accuracy on the EgoExo4D dataset while using significantly fewer resources compared to previous models, highlighting the effectiveness of integrating multiple views for skill evaluation.'}, 'zh': {'title': 'SkillFormer：高效的多视角技能评估架构', 'desc': '评估人类在复杂活动中的技能水平是一项具有挑战性的任务，广泛应用于体育、康复和培训领域。本文提出了SkillFormer，这是一种高效的架构，能够从自我中心和外部视角的视频中统一估计技能水平。SkillFormer基于TimeSformer骨干网，引入了CrossViewFusion模块，通过多头交叉注意力、可学习的门控和自适应自校准来融合视角特征。通过低秩适应技术，我们仅微调少量参数，显著降低了训练成本，并在EgoExo4D数据集上实现了多视角设置下的最先进准确率。'}}}, {'id': 'https://huggingface.co/papers/2505.08751', 'title': 'Aya Vision: Advancing the Frontier of Multilingual Multimodality', 'url': 'https://huggingface.co/papers/2505.08751', 'abstract': 'Building multimodal language models is fundamentally challenging: it requires aligning vision and language modalities, curating high-quality instruction data, and avoiding the degradation of existing text-only capabilities once vision is introduced. These difficulties are further magnified in the multilingual setting, where the need for multimodal data in different languages exacerbates existing data scarcity, machine translation often distorts meaning, and catastrophic forgetting is more pronounced. To address the aforementioned challenges, we introduce novel techniques spanning both data and modeling. First, we develop a synthetic annotation framework that curates high-quality, diverse multilingual multimodal instruction data, enabling Aya Vision models to produce natural, human-preferred responses to multimodal inputs across many languages. Complementing this, we propose a cross-modal model merging technique that mitigates catastrophic forgetting, effectively preserving text-only capabilities while simultaneously enhancing multimodal generative performance. Aya-Vision-8B achieves best-in-class performance compared to strong multimodal models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which outperforms models more than twice its size, such as Molmo-72B and LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the multi-modal frontier, and provides insights into techniques that effectively bend the need for compute while delivering extremely high performance.', 'score': 0, 'issue_id': 3757, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': 'c063c125e504fa88', 'authors': ['Saurabh Dash', 'Yiyang Nan', 'John Dang', 'Arash Ahmadian', 'Shivalika Singh', 'Madeline Smith', 'Bharat Venkitesh', 'Vlad Shmyhlo', 'Viraat Aryabumi', 'Walter Beller-Morales', 'Jeremy Pekmez', 'Jason Ozuzu', 'Pierre Richemond', 'Acyr Locatelli', 'Nick Frosst', 'Phil Blunsom', 'Aidan Gomez', 'Ivan Zhang', 'Marzieh Fadaee', 'Manoj Govindassamy', 'Sudip Roy', 'Matthias Gallé', 'Beyza Ermis', 'Ahmet Üstün', 'Sara Hooker'], 'affiliations': ['Cohere', 'Cohere Labs'], 'pdf_title_img': 'assets/pdf/title_img/2505.08751.jpg', 'data': {'categories': ['#synthetic', '#data', '#low_resource', '#multimodal', '#training', '#multilingual'], 'emoji': '🌐', 'ru': {'title': 'Прорыв в создании эффективных многоязычных мультимодальных ИИ-моделей', 'desc': 'Статья представляет новые методы для создания многоязычных мультимодальных языковых моделей. Авторы разработали систему синтетической аннотации для создания качественных многоязычных мультимодальных данных для обучения. Они также предложили технику объединения кросс-модальных моделей для предотвращения катастрофического забывания. Модели Aya-Vision, созданные с использованием этих методов, показывают лучшие результаты по сравнению с более крупными конкурентами в многоязычных мультимодальных задачах.'}, 'en': {'title': 'Advancing Multimodal Language Models for Multilingual Mastery', 'desc': 'This paper addresses the challenges of building multimodal language models that integrate both vision and language, particularly in a multilingual context. It introduces a synthetic annotation framework to create high-quality, diverse multilingual multimodal instruction data, which helps the Aya Vision models generate human-like responses. Additionally, the authors propose a cross-modal model merging technique to prevent catastrophic forgetting, ensuring that text-only capabilities are maintained while improving multimodal performance. The results show that Aya-Vision models outperform existing multimodal models, demonstrating significant advancements in multilingual multimodal processing.'}, 'zh': {'title': '多模态语言模型的突破性进展', 'desc': '构建多模态语言模型面临许多挑战，包括视觉和语言模态的对齐、高质量指令数据的整理，以及在引入视觉后避免文本能力的退化。在多语言环境中，这些困难更加突出，因为不同语言的多模态数据需求加剧了数据稀缺，机器翻译常常扭曲意义，并且灾难性遗忘现象更加明显。为了解决这些问题，我们提出了新的技术，包括一个合成注释框架，用于整理高质量、多样化的多语言多模态指令数据，使Aya Vision模型能够在多种语言中对多模态输入生成自然、符合人类偏好的响应。此外，我们还提出了一种跨模态模型合并技术，有效减轻灾难性遗忘，同时增强多模态生成性能。'}}}, {'id': 'https://huggingface.co/papers/2505.08712', 'title': 'NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged\n  Information Guidance', 'url': 'https://huggingface.co/papers/2505.08712', 'abstract': "Learning navigation in dynamic open-world environments is an important yet challenging skill for robots. Most previous methods rely on precise localization and mapping or learn from expensive real-world demonstrations. In this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end framework trained solely in simulation and can zero-shot transfer to different embodiments in diverse real-world environments. The key ingredient of NavDP's network is the combination of diffusion-based trajectory generation and a critic function for trajectory selection, which are conditioned on only local observation tokens encoded from a shared policy transformer. Given the privileged information of the global environment in simulation, we scale up the demonstrations of good quality to train the diffusion policy and formulate the critic value function targets with contrastive negative samples. Our demonstration generation approach achieves about 2,500 trajectories/GPU per day, 20times more efficient than real-world data collection, and results in a large-scale navigation dataset with 363.2km trajectories across 1244 scenes. Trained with this simulation dataset, NavDP achieves state-of-the-art performance and consistently outstanding generalization capability on quadruped, wheeled, and humanoid robots in diverse indoor and outdoor environments. In addition, we present a preliminary attempt at using Gaussian Splatting to make in-domain real-to-sim fine-tuning to further bridge the sim-to-real gap. Experiments show that adding such real-to-sim data can improve the success rate by 30\\% without hurting its generalization capability.", 'score': 0, 'issue_id': 3752, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': 'a68cc40de86ece61', 'authors': ['Wenzhe Cai', 'Jiaqi Peng', 'Yuqiang Yang', 'Yujian Zhang', 'Meng Wei', 'Hanqing Wang', 'Yilun Chen', 'Tai Wang', 'Jiangmiao Pang'], 'affiliations': ['Shanghai AI Lab', 'The University of Hong Kong', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.08712.jpg', 'data': {'categories': ['#transfer_learning', '#agents', '#diffusion', '#robotics', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'NavDP: Универсальная навигация роботов из симуляции в реальность', 'desc': 'Статья представляет NavDP - систему навигации роботов в динамичной открытой среде, обученную исключительно на симуляциях. NavDP использует комбинацию генерации траектории на основе диффузии и функции критика для выбора траектории, что позволяет ей успешно работать с разными типами роботов в реальном мире. Система обучается на большом наборе данных, сгенерированном в симуляции, что значительно эффективнее сбора реальных данных. Эксперименты показывают высокую производительность и способность к обобщению для различных типов роботов в разнообразных средах.'}, 'en': {'title': 'Efficient Robot Navigation with Simulation-Driven Learning', 'desc': 'This paper introduces the Navigation Diffusion Policy (NavDP), a novel framework for robot navigation in dynamic environments. Unlike traditional methods that depend on accurate mapping or costly real-world training, NavDP is trained entirely in simulation and can adapt to various robot types in real-world settings. The framework utilizes diffusion-based trajectory generation combined with a critic function to select optimal paths based on local observations. By generating a large dataset of simulated navigation trajectories efficiently, NavDP demonstrates superior performance and generalization across different robotic embodiments and environments.'}, 'zh': {'title': '机器人导航的新突破：导航扩散策略', 'desc': '本论文提出了一种名为导航扩散策略（NavDP）的新框架，旨在帮助机器人在动态开放世界环境中进行导航。该方法完全在模拟环境中训练，能够零-shot迁移到不同的实际应用中。NavDP结合了基于扩散的轨迹生成和用于轨迹选择的评价函数，利用共享策略变换器编码的局部观察信息进行条件化。通过在模拟中获取全球环境的特权信息，我们生成了高质量的演示数据，并在多种室内外环境中实现了最先进的导航性能和出色的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.08445', 'title': 'Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter\n  Impact on Performance and Efficiency', 'url': 'https://huggingface.co/papers/2505.08445', 'abstract': 'Large language models achieve high task performance yet often hallucinate or rely on outdated knowledge. Retrieval-augmented generation (RAG) addresses these gaps by coupling generation with external search. We analyse how hyperparameters influence speed and quality in RAG systems, covering Chroma and Faiss vector stores, chunking policies, cross-encoder re-ranking, and temperature, and we evaluate six metrics: faithfulness, answer correctness, answer relevancy, context precision, context recall, and answer similarity. Chroma processes queries 13% faster, whereas Faiss yields higher retrieval precision, revealing a clear speed-accuracy trade-off. Naive fixed-length chunking with small windows and minimal overlap outperforms semantic segmentation while remaining the quickest option. Re-ranking provides modest gains in retrieval quality yet increases runtime by roughly a factor of 5, so its usefulness depends on latency constraints. These results help practitioners balance computational cost and accuracy when tuning RAG systems for transparent, up-to-date responses. Finally, we re-evaluate the top configurations with a corrective RAG workflow and show that their advantages persist when the model can iteratively request additional evidence. We obtain a near-perfect context precision (99%), which demonstrates that RAG systems can achieve extremely high retrieval accuracy with the right combination of hyperparameters, with significant implications for applications where retrieval quality directly impacts downstream task performance, such as clinical decision support in healthcare.', 'score': 0, 'issue_id': 3756, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': '7b4d8b3daa6efb6d', 'authors': ['Adel Ammar', 'Anis Koubaa', 'Omer Nacar', 'Wadii Boulila'], 'affiliations': ['Alfaisal University, P.O. Box 50927, Riyadh 11533, Saudi Arabia', 'Prince Sultan University, Rafha Street, P.O. Box 66833, Riyadh 11586, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2505.08445.jpg', 'data': {'categories': ['#optimization', '#data', '#benchmark', '#healthcare', '#hallucinations', '#rag'], 'emoji': '🔍', 'ru': {'title': 'Оптимизация RAG: баланс между скоростью и точностью', 'desc': 'Статья анализирует влияние гиперпараметров на скорость и качество в системах генерации с дополнением извлечением (RAG). Исследуются векторные хранилища Chroma и Faiss, стратегии разбиения текста, переранжирование кросс-энкодером и температура. Оценивается шесть метрик, включая достоверность, корректность и релевантность ответов. Результаты показывают компромисс между скоростью и точностью, а также демонстрируют, что RAG-системы могут достичь очень высокой точности извлечения при правильном подборе гиперпараметров.'}, 'en': {'title': 'Optimizing RAG: Balancing Speed and Accuracy for Better AI Responses', 'desc': 'This paper explores how hyperparameters affect the performance of Retrieval-Augmented Generation (RAG) systems, which combine language generation with external information retrieval. It evaluates different vector stores, chunking strategies, and re-ranking methods to find the best balance between speed and accuracy. The findings indicate that while Chroma is faster, Faiss offers better retrieval precision, highlighting a trade-off between these two factors. The study also shows that with optimal hyperparameter tuning, RAG systems can achieve very high retrieval accuracy, which is crucial for applications like healthcare decision support.'}, 'zh': {'title': '优化RAG系统，实现高效准确的检索', 'desc': '大型语言模型在任务表现上表现优异，但常常会出现幻觉或依赖过时知识。检索增强生成（RAG）通过将生成与外部搜索结合，解决了这些问题。我们分析了超参数如何影响RAG系统的速度和质量，包括向量存储、分块策略、交叉编码重排序和温度等，并评估了六个指标。研究结果帮助从业者在调整RAG系统时平衡计算成本和准确性，以实现透明且最新的响应。'}}}, {'id': 'https://huggingface.co/papers/2505.08311', 'title': 'AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale', 'url': 'https://huggingface.co/papers/2505.08311', 'abstract': 'We present AM-Thinking-v1, a 32B dense language model that advances the frontier of reasoning, embodying the collaborative spirit of open-source innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts (MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves impressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities among open-source models of similar scale.   Built entirely from the open-source Qwen2.5-32B base model and publicly available queries, AM-Thinking-v1 leverages a meticulously crafted post-training pipeline - combining supervised fine-tuning and reinforcement learning - to deliver exceptional reasoning capabilities. This work demonstrates that the open-source community can achieve high performance at the 32B scale, a practical sweet spot for deployment and fine-tuning. By striking a balance between top-tier performance and real-world usability, we hope AM-Thinking-v1 inspires further collaborative efforts to harness mid-scale models, pushing reasoning boundaries while keeping accessibility at the core of innovation. We have open-sourced our model on https://huggingface.co/a-m-team/AM-Thinking-v1{Hugging Face}.', 'score': 0, 'issue_id': 3757, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': '492d10424bc0d97d', 'authors': ['Yunjie Ji', 'Xiaoyu Tian', 'Sitong Zhao', 'Haotian Wang', 'Shuaiting Chen', 'Yiping Peng', 'Han Zhao', 'Xiangang Li'], 'affiliations': ['Beike (Ke.com)'], 'pdf_title_img': 'assets/pdf/title_img/2505.08311.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#open_source', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Открытая 32B-модель устанавливает новую планку в рассуждениях', 'desc': 'AM-Thinking-v1 - это языковая модель с 32 миллиардами параметров, которая демонстрирует передовые способности к рассуждению. Модель превосходит DeepSeek-R1 и конкурирует с ведущими MoE-моделями, показывая впечатляющие результаты в математических и кодинговых тестах. AM-Thinking-v1 была создана на основе открытой модели Qwen2.5-32B с использованием тщательно разработанного процесса дообучения, включающего supervised fine-tuning и reinforcement learning. Авторы подчеркивают, что модель с 32 миллиардами параметров представляет собой практичный баланс между производительностью и удобством использования.'}, 'en': {'title': 'Unlocking Reasoning with Open-Source Innovation', 'desc': 'AM-Thinking-v1 is a 32 billion parameter dense language model that enhances reasoning capabilities, showcasing the power of open-source collaboration. It surpasses previous models like DeepSeek-R1 and competes with advanced Mixture-of-Experts models, achieving high scores on various benchmarks. The model is built on the Qwen2.5-32B base and utilizes a combination of supervised fine-tuning and reinforcement learning in its post-training process. This work highlights the potential of mid-scale models for practical applications, aiming to inspire further innovation in the open-source community.'}, 'zh': {'title': '开源创新，推理新高度', 'desc': '我们介绍了AM-Thinking-v1，这是一个32B的密集语言模型，推动了推理的前沿，体现了开源创新的合作精神。它在AIME 2024、AIME 2025和LiveCodeBench等基准测试中表现出色，超越了DeepSeek-R1，并与领先的专家混合模型如Qwen3-235B-A22B和Seed1.5-Thinking相媲美。AM-Thinking-v1完全基于开源的Qwen2.5-32B基础模型，结合监督微调和强化学习，展现了卓越的推理能力。我们的工作证明了开源社区可以在32B规模上实现高性能，平衡顶级性能与实际可用性，激励更多合作努力。'}}}, {'id': 'https://huggingface.co/papers/2505.07416', 'title': 'ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness\n  Prediction via Human-AI Collaborative Annotation', 'url': 'https://huggingface.co/papers/2505.07416', 'abstract': 'Multimodal Review Helpfulness Prediction (MRHP) is an essential task in recommender systems, particularly in E-commerce platforms. Determining the helpfulness of user-generated reviews enhances user experience and improves consumer decision-making. However, existing datasets focus predominantly on English and Indonesian, resulting in a lack of linguistic diversity, especially for low-resource languages such as Vietnamese. In this paper, we introduce ViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), a large-scale benchmark dataset for MRHP task in Vietnamese. This dataset covers four domains, including 2K products with 46K reviews. Meanwhile, a large-scale dataset requires considerable time and cost. To optimize the annotation process, we leverage AI to assist annotators in constructing the ViMRHP dataset. With AI assistance, annotation time is reduced (90 to 120 seconds per task down to 20 to 40 seconds per task) while maintaining data quality and lowering overall costs by approximately 65%. However, AI-generated annotations still have limitations in complex annotation tasks, which we further examine through a detailed performance analysis. In our experiment on ViMRHP, we evaluate baseline models on human-verified and AI-generated annotations to assess their quality differences. The ViMRHP dataset is publicly available at https://github.com/trng28/ViMRHP', 'score': 0, 'issue_id': 3751, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '4f126d39b5476454', 'authors': ['Truc Mai-Thanh Nguyen', 'Dat Minh Nguyen', 'Son T. Luu', 'Kiet Van Nguyen'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07416.jpg', 'data': {'categories': ['#optimization', '#dataset', '#data', '#multilingual', '#low_resource', '#open_source'], 'emoji': '🇻🇳', 'ru': {'title': 'ИИ ускоряет создание датасета для анализа отзывов на вьетнамском языке', 'desc': 'Статья представляет новый набор данных ViMRHP для прогнозирования полезности отзывов на вьетнамском языке. Авторы использовали искусственный интеллект для помощи аннотаторам, что значительно ускорило процесс разметки и снизило затраты. Датасет охватывает 4 домена, включая 2000 продуктов и 46000 отзывов. Исследователи провели эксперименты с базовыми моделями машинного обучения для оценки качества аннотаций, созданных человеком и ИИ.'}, 'en': {'title': 'Empowering Vietnamese Reviews with AI-Driven Helpfulness Prediction', 'desc': 'This paper presents the ViMRHP dataset, which is designed for predicting the helpfulness of reviews in Vietnamese, addressing a gap in existing datasets that mainly focus on English and Indonesian. The dataset includes 46,000 reviews across 2,000 products, making it a significant resource for multimodal review helpfulness prediction in low-resource languages. To streamline the annotation process, the authors utilize AI, significantly reducing the time required for each annotation task while also cutting costs by about 65%. The study also evaluates the performance of baseline models using both human-verified and AI-generated annotations, highlighting the strengths and limitations of AI in complex annotation tasks.'}, 'zh': {'title': '提升越南语评论有用性的智能解决方案', 'desc': '多模态评论有用性预测（MRHP）是推荐系统中的一个重要任务，尤其是在电子商务平台上。本文介绍了ViMRHP（越南语多模态评论有用性预测），这是一个针对越南语的MRHP任务的大规模基准数据集，涵盖了四个领域，包括2000个产品和46000条评论。为了优化注释过程，我们利用人工智能辅助注释者构建ViMRHP数据集，从而显著减少了注释时间和成本，同时保持数据质量。尽管AI生成的注释在复杂任务中仍存在局限性，但我们通过详细的性能分析进一步探讨了这些问题。'}}}, {'id': 'https://huggingface.co/papers/2504.21475', 'title': 'Advancing Arabic Reverse Dictionary Systems: A Transformer-Based\n  Approach with Dataset Construction Guidelines', 'url': 'https://huggingface.co/papers/2504.21475', 'abstract': 'This study addresses the critical gap in Arabic natural language processing by developing an effective Arabic Reverse Dictionary (RD) system that enables users to find words based on their descriptions or meanings. We present a novel transformer-based approach with a semi-encoder neural network architecture featuring geometrically decreasing layers that achieves state-of-the-art results for Arabic RD tasks. Our methodology incorporates a comprehensive dataset construction process and establishes formal quality standards for Arabic lexicographic definitions. Experiments with various pre-trained models demonstrate that Arabic-specific models significantly outperform general multilingual embeddings, with ARBERTv2 achieving the best ranking score (0.0644). Additionally, we provide a formal abstraction of the reverse dictionary task that enhances theoretical understanding and develop a modular, extensible Python library (RDTL) with configurable training pipelines. Our analysis of dataset quality reveals important insights for improving Arabic definition construction, leading to eight specific standards for building high-quality reverse dictionary resources. This work contributes significantly to Arabic computational linguistics and provides valuable tools for language learning, academic writing, and professional communication in Arabic.', 'score': 0, 'issue_id': 3756, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '86e4cc7ad4a84ece', 'authors': ['Serry Sibaee', 'Samar Ahmed', 'Abdullah Al Harbi', 'Omer Nacar', 'Adel Ammar', 'Yasser Habashi', 'Wadii Boulila'], 'affiliations': ['College of Computer & Information Sciences, Prince Sultan University, Riyadh, Saudi Arabia', 'Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia', 'Independent Researcher, Riyadh, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2504.21475.jpg', 'data': {'categories': ['#training', '#data', '#machine_translation', '#dataset', '#architecture', '#multilingual', '#low_resource'], 'emoji': '📚', 'ru': {'title': 'Революция в арабской лингвистике: трансформеры на службе обратного словаря', 'desc': 'Исследование посвящено разработке эффективной системы обратного словаря для арабского языка, позволяющей находить слова по их описаниям или значениям. Авторы представляют новый подход на основе трансформеров с полуэнкодерной нейросетевой архитектурой, достигающий наилучших результатов в задачах арабского обратного словаря. Методология включает создание комплексного датасета и установление формальных стандартов качества для арабских лексикографических определений. Эксперименты показали, что арабоязычные предобученные модели значительно превосходят многоязычные эмбеддинги, причем ARBERTv2 достигает наилучшего рейтингового показателя.'}, 'en': {'title': 'Bridging the Gap in Arabic Language Processing with a Reverse Dictionary', 'desc': 'This paper presents a new system for Arabic natural language processing called the Arabic Reverse Dictionary (RD), which helps users find words based on their meanings. The authors introduce a transformer-based model with a unique semi-encoder architecture that improves performance on Arabic RD tasks. They also create a detailed dataset and establish quality standards for Arabic definitions, showing that specialized Arabic models outperform general multilingual ones. Additionally, the study offers a theoretical framework for reverse dictionaries and introduces a Python library for easy implementation and training.'}, 'zh': {'title': '阿拉伯语反向词典：提升语言处理的利器', 'desc': '本研究解决了阿拉伯语自然语言处理中的关键问题，开发了一种有效的阿拉伯语反向词典系统，用户可以根据描述或含义找到单词。我们提出了一种新颖的基于变换器的半编码神经网络架构，具有几何递减层，达到了阿拉伯语反向词典任务的最先进结果。我们的研究方法包括全面的数据集构建过程，并建立了阿拉伯语词典定义的正式质量标准。实验表明，专门针对阿拉伯语的模型显著优于通用多语言嵌入，ARBERTv2模型获得了最佳排名分数。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (1)', '#agi', '#alignment (1)', '#architecture (2)', '#audio (1)', '#benchmark (3)', '#cv', '#data (4)', '#dataset (6)', '#diffusion (1)', '#ethics', '#games (2)', '#graphs', '#hallucinations (1)', '#healthcare (1)', '#inference', '#interpretability', '#leakage', '#long_context', '#low_resource (3)', '#machine_translation (1)', '#math', '#multilingual (4)', '#multimodal (1)', '#open_source (2)', '#optimization (4)', '#plp', '#rag (1)', '#reasoning (2)', '#rl (3)', '#rlhf', '#robotics (1)', '#science', '#security', '#small_models', '#story_generation', '#survey', '#synthetic (2)', '#training (5)', '#transfer_learning (1)', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-05-14 14:12',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-05-14 14:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-05-14 14:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    