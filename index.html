
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 68 papers. February 3.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ</span> | <span id="title-articles-count">68 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2026-02-02.html">â¬…ï¸ <span id="prev-date">02.02</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2026-02-04.html">â¡ï¸ <span id="next-date">04.02</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2026-02.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 3', 'zh': '2æœˆ3æ—¥'};
        let feedDateNext = {'ru': '04.02', 'en': '02/04', 'zh': '2æœˆ4æ—¥'};
        let feedDatePrev = {'ru': '02.02', 'en': '02/02', 'zh': '2æœˆ2æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2602.00919', 'title': 'Green-VLA: Staged Vision-Language-Action Model for Generalist Robots', 'url': 'https://huggingface.co/papers/2602.00919', 'abstract': 'Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.', 'score': 169, 'issue_id': 884, 'pub_date': '2026-01-31', 'pub_date_card': {'ru': '31 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 31', 'zh': '1æœˆ31æ—¥'}, 'hash': '710fd909b8c97f71', 'authors': ['I. Apanasevich', 'M. Artemyev', 'R. Babakyan', 'P. Fedotova', 'D. Grankin', 'E. Kupryashin', 'A. Misailidi', 'D. Nerus', 'A. Nutalapati', 'G. Sidorov', 'I. Efremov', 'M. Gerasyov', 'D. Pikurov', 'Y. Senchenko', 'S. Davidenko', 'D. Kulikov', 'M. Sultankin', 'K. Askarbek', 'O. Shamanin', 'D. Statovoy', 'E. Zalyaev', 'I. Zorin', 'A. Letkin', 'E. Rusakov', 'A. Silchenko', 'V. Vorobyov', 'S. Sobolnikov', 'A. Postnikov'], 'affiliations': ['Sber Robotics Center'], 'pdf_title_img': 'assets/pdf/title_img/2602.00919.jpg', 'data': {'categories': ['#multimodal', '#data', '#rl', '#robotics', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ¼ Ğ¸ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Green-VLA Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¿ÑÑ‚Ğ¸ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Vision-Language-Action Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²: Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ¾Ğ², Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ·Ğ°ĞºÑ€ĞµĞ¿Ğ»Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ². Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ¶Ñ‘ÑÑ‚ĞºĞ¾Ğ³Ğ¾ alignment Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Empowering Robots with Green-VLA: A Unified Vision-Language-Action Framework', 'desc': 'Green-VLA is a comprehensive framework designed for deploying robots in real-world scenarios by integrating vision, language, and action. It consists of five stages that include foundational training, multimodal grounding, and reinforcement learning to adapt to various robot types. The framework utilizes a large dataset of demonstrations and a unified action interface, allowing a single policy to control different robotic embodiments effectively. Experimental results demonstrate significant improvements in generalization, success rates, and efficiency through the reinforcement learning alignment process.'}, 'zh': {'title': 'Green-VLAï¼šå®ç°æœºå™¨äººæ³›åŒ–çš„äº”é˜¶æ®µæ¡†æ¶', 'desc': 'Green-VLAæ˜¯ä¸€ä¸ªäº”é˜¶æ®µçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°æœºå™¨äººåœ¨ç°å®ä¸–ç•Œä¸­çš„éƒ¨ç½²ï¼Œå¹¶é€šè¿‡å¤šæ¨¡æ€è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ å®ç°ä¸åŒæœºå™¨äººå½¢æ€çš„æ³›åŒ–ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬åŸºç¡€çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€å¤šæ¨¡æ€åŸºç¡€ã€å¤šä¸ªå½¢æ€çš„é¢„è®­ç»ƒã€ç‰¹å®šå½¢æ€çš„é€‚åº”å’Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥å¯¹é½äº”ä¸ªé˜¶æ®µã€‚æˆ‘ä»¬ç»“åˆäº†å¯æ‰©å±•çš„æ•°æ®å¤„ç†ç®¡é“å’Œç»Ÿä¸€çš„åŠ¨ä½œæ¥å£ï¼Œä½¿å¾—å•ä¸€ç­–ç•¥èƒ½å¤Ÿæ§åˆ¶ä¸åŒç±»å‹çš„æœºå™¨äººã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGreen-VLAåœ¨æˆåŠŸç‡ã€é²æ£’æ€§å’Œé•¿æ—¶é—´æ•ˆç‡æ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.02276', 'title': 'Kimi K2.5: Visual Agentic Intelligence', 'url': 'https://huggingface.co/papers/2602.02276', 'abstract': 'Kimi K2.5 is an open-source multimodal agentic model that enhances text and vision processing through joint optimization techniques and introduces Agent Swarm for parallel task execution.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.', 'score': 140, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': 'bf258fb12cb363a6', 'authors': ['Kimi Team', 'Tongtong Bai', 'Yifan Bai', 'Yiping Bao', 'S. H. Cai', 'Yuan Cao', 'Y. Charles', 'H. S. Che', 'Cheng Chen', 'Guanduo Chen', 'Huarong Chen', 'Jia Chen', 'Jiahao Chen', 'Jianlong Chen', 'Jun Chen', 'Kefan Chen', 'Liang Chen', 'Ruijue Chen', 'Xinhao Chen', 'Yanru Chen', 'Yanxu Chen', 'Yicun Chen', 'Yimin Chen', 'Yingjiang Chen', 'Yuankun Chen', 'Yujie Chen', 'Yutian Chen', 'Zhirong Chen', 'Ziwei Chen', 'Dazhi Cheng', 'Minghan Chu', 'Jialei Cui', 'Jiaqi Deng', 'Muxi Diao', 'Hao Ding', 'Mengfan Dong', 'Mengnan Dong', 'Yuxin Dong', 'Yuhao Dong', 'Angang Du', 'Chenzhuang Du', 'Dikang Du', 'Lingxiao Du', 'Yulun Du', 'Yu Fan', 'Shengjun Fang', 'Qiulin Feng', 'Yichen Feng', 'Garimugai Fu', 'Kelin Fu', 'Hongcheng Gao', 'Tong Gao', 'Yuyao Ge', 'Shangyi Geng', 'Chengyang Gong', 'Xiaochen Gong', 'Zhuoma Gongque', 'Qizheng Gu', 'Xinran Gu', 'Yicheng Gu', 'Longyu Guan', 'Yuanying Guo', 'Xiaoru Hao', 'Weiran He', 'Wenyang He', 'Yunjia He', 'Chao Hong', 'Hao Hu', 'Jiaxi Hu', 'Yangyang Hu', 'Zhenxing Hu', 'Ke Huang', 'Ruiyuan Huang', 'Weixiao Huang', 'Zhiqi Huang', 'Tao Jiang', 'Zhejun Jiang', 'Xinyi Jin', 'Yu Jing', 'Guokun Lai', 'Aidi Li', 'C. Li', 'Cheng Li', 'Fang Li', 'Guanghe Li', 'Guanyu Li', 'Haitao Li', 'Haoyang Li', 'Jia Li', 'Jingwei Li', 'Junxiong Li', 'Lincan Li', 'Mo Li', 'Weihong Li', 'Wentao Li', 'Xinhang Li', 'Xinhao Li', 'Yang Li', 'Yanhao Li', 'Yiwei Li', 'Yuxiao Li', 'Zhaowei Li', 'Zheming Li', 'Weilong Liao', 'Jiawei Lin', 'Xiaohan Lin', 'Zhishan Lin', 'Zichao Lin', 'Cheng Liu', 'Chenyu Liu', 'Hongzhang Liu', 'Liang Liu', 'Shaowei Liu', 'Shudong Liu', 'Shuran Liu', 'Tianwei Liu', 'Tianyu Liu', 'Weizhou Liu', 'Xiangyan Liu', 'Yangyang Liu', 'Yanming Liu', 'Yibo Liu', 'Yuanxin Liu', 'Yue Liu', 'Zhengying Liu', 'Zhongnuo Liu', 'Enzhe Lu', 'Haoyu Lu', 'Zhiyuan Lu', 'Junyu Luo', 'Tongxu Luo', 'Yashuo Luo', 'Long Ma', 'Yingwei Ma', 'Shaoguang Mao', 'Yuan Mei', 'Xin Men', 'Fanqing Meng', 'Zhiyong Meng', 'Yibo Miao', 'Minqing Ni', 'Kun Ouyang', 'Siyuan Pan', 'Bo Pang', 'Yuchao Qian', 'Ruoyu Qin', 'Zeyu Qin', 'Jiezhong Qiu', 'Bowen Qu', 'Zeyu Shang', 'Youbo Shao', 'Tianxiao Shen', 'Zhennan Shen', 'Juanfeng Shi', 'Lidong Shi', 'Shengyuan Shi', 'Feifan Song', 'Pengwei Song', 'Tianhui Song', 'Xiaoxi Song', 'Hongjin Su', 'Jianlin Su', 'Zhaochen Su', 'Lin Sui', 'Jinsong Sun', 'Junyao Sun', 'Tongyu Sun', 'Flood Sung', 'Yunpeng Tai', 'Chuning Tang', 'Heyi Tang', 'Xiaojuan Tang', 'Zhengyang Tang', 'Jiawen Tao', 'Shiyuan Teng', 'Chaoran Tian', 'Pengfei Tian', 'Ao Wang', 'Bowen Wang', 'Chensi Wang', 'Chuang Wang', 'Congcong Wang', 'Dingkun Wang', 'Dinglu Wang', 'Dongliang Wang', 'Feng Wang', 'Hailong Wang', 'Haiming Wang', 'Hengzhi Wang', 'Huaqing Wang', 'Hui Wang', 'Jiahao Wang', 'Jinhong Wang', 'Jiuzheng Wang', 'Kaixin Wang', 'Linian Wang', 'Qibin Wang', 'Shengjie Wang', 'Shuyi Wang', 'Si Wang', 'Wei Wang', 'Xiaochen Wang', 'Xinyuan Wang', 'Yao Wang', 'Yejie Wang', 'Yipu Wang', 'Yiqin Wang', 'Yucheng Wang', 'Yuzhi Wang', 'Zhaoji Wang', 'Zhaowei Wang', 'Zhengtao Wang', 'Zhexu Wang', 'Zihan Wang', 'Zizhe Wang', 'Chu Wei', 'Ming Wei', 'Chuan Wen', 'Zichen Wen', 'Chengjie Wu', 'Haoning Wu', 'Junyan Wu', 'Rucong Wu', 'Wenhao Wu', 'Yuefeng Wu', 'Yuhao Wu', 'Yuxin Wu', 'Zijian Wu', 'Chenjun Xiao', 'Jin Xie', 'Xiaotong Xie', 'Yuchong Xie', 'Yifei Xin', 'Bowei Xing', 'Boyu Xu', 'Jianfan Xu', 'Jing Xu', 'Jinjing Xu', 'L. H. Xu', 'Lin Xu', 'Suting Xu', 'Weixin Xu', 'Xinbo Xu', 'Xinran Xu', 'Yangchuan Xu', 'Yichang Xu', 'Yuemeng Xu', 'Zelai Xu', 'Ziyao Xu', 'Junjie Yan', 'Yuzi Yan', 'Guangyao Yang', 'Hao Yang', 'Junwei Yang', 'Kai Yang', 'Ningyuan Yang', 'Ruihan Yang', 'Xiaofei Yang', 'Xinlong Yang', 'Ying Yang', 'Yi Yang', 'Yi Yang', 'Zhen Yang', 'Zhilin Yang', 'Zonghan Yang', 'Haotian Yao', 'Dan Ye', 'Wenjie Ye', 'Zhuorui Ye', 'Bohong Yin', 'Chengzhen Yu', 'Longhui Yu', 'Tao Yu', 'Tianxiang Yu', 'Enming Yuan', 'Mengjie Yuan', 'Xiaokun Yuan', 'Yang Yue', 'Weihao Zeng', 'Dunyuan Zha', 'Haobing Zhan', 'Dehao Zhang', 'Hao Zhang', 'Jin Zhang', 'Puqi Zhang', 'Qiao Zhang', 'Rui Zhang', 'Xiaobin Zhang', 'Y. Zhang', 'Yadong Zhang', 'Yangkun Zhang', 'Yichi Zhang', 'Yizhi Zhang', 'Yongting Zhang', 'Yu Zhang', 'Yushun Zhang', 'Yutao Zhang', 'Yutong Zhang', 'Zheng Zhang', 'Chenguang Zhao', 'Feifan Zhao', 'Jinxiang Zhao', 'Shuai Zhao', 'Xiangyu Zhao', 'Yikai Zhao', 'Zijia Zhao', 'Huabin Zheng', 'Ruihan Zheng', 'Shaojie Zheng', 'Tengyang Zheng', 'Junfeng Zhong', 'Longguang Zhong', 'Weiming Zhong', 'M. Zhou', 'Runjie Zhou', 'Xinyu Zhou', 'Zaida Zhou', 'Jinguo Zhu', 'Liya Zhu', 'Xinhao Zhu', 'Yuxuan Zhu', 'Zhen Zhu', 'Jingze Zhuang', 'Weiyu Zhuang', 'Ying Zou', 'Xinxing Zu'], 'affiliations': ['Kimi Team', 'MoonShot AI'], 'pdf_title_img': 'assets/pdf/title_img/2602.02276.jpg', 'data': {'categories': ['#multimodal', '#plp', '#cv', '#agents', '#reasoning', '#open_source', '#training'], 'emoji': 'ğŸ', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ¾ĞµĞ²ÑƒÑ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Kimi K2.5 â€” Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ğ° Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Agent Swarm â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¸Ñ… Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ğ¾ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ, Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹. ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ² Agent Swarm ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ´Ğ¾ 4,5 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ñ‹Ğ¼ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Kimi K2.5: Uniting Text and Vision for Superior Agentic Intelligence', 'desc': 'Kimi K2.5 is an innovative open-source multimodal model that integrates text and vision processing through advanced joint optimization methods. It employs techniques like joint text-vision pre-training and reinforcement learning to ensure that both modalities work together effectively. The model also features Agent Swarm, a framework that allows multiple agents to work on different parts of a task simultaneously, improving efficiency. Evaluations demonstrate that Kimi K2.5 outperforms existing models in various tasks, significantly reducing processing time while enhancing overall performance.'}, 'zh': {'title': 'Kimi K2.5ï¼šå¤šæ¨¡æ€æ™ºèƒ½çš„æœªæ¥', 'desc': 'Kimi K2.5æ˜¯ä¸€ä¸ªå¼€æºçš„å¤šæ¨¡æ€æ™ºèƒ½æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡è”åˆä¼˜åŒ–æŠ€æœ¯æå‡æ–‡æœ¬å’Œè§†è§‰å¤„ç†èƒ½åŠ›ã€‚è¯¥æ¨¡å‹å¼ºè°ƒæ–‡æœ¬å’Œè§†è§‰çš„è”åˆä¼˜åŒ–ï¼Œä½¿å¾—è¿™ä¸¤ç§æ¨¡æ€èƒ½å¤Ÿç›¸äº’å¢å¼ºã€‚K2.5å¼•å…¥äº†Agent Swarmï¼Œä¸€ä¸ªè‡ªæˆ‘å¯¼å‘çš„å¹¶è¡Œä»£ç†ç¼–æ’æ¡†æ¶ï¼Œå¯ä»¥å°†å¤æ‚ä»»åŠ¡åŠ¨æ€åˆ†è§£ä¸ºå¼‚æ„å­é—®é¢˜å¹¶åŒæ—¶æ‰§è¡Œã€‚ç»è¿‡å¹¿æ³›è¯„ä¼°ï¼ŒKimi K2.5åœ¨ç¼–ç ã€è§†è§‰ã€æ¨ç†å’Œæ™ºèƒ½ä»»åŠ¡ç­‰å¤šä¸ªé¢†åŸŸè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶ä¸”Agent Swarmå°†å»¶è¿Ÿå‡å°‘äº†å¤šè¾¾4.5å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.22060', 'title': 'Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2601.22060', 'abstract': "Vision-DeepResearch introduces a multimodal deep-research paradigm enabling multi-turn, multi-entity, and multi-scale visual and textual search with deep-research capabilities integrated through cold-start supervision and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.", 'score': 123, 'issue_id': 884, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 29', 'zh': '1æœˆ29æ—¥'}, 'hash': 'e58b4202763d68f9', 'authors': ['Wenxuan Huang', 'Yu Zeng', 'Qiuchen Wang', 'Zhen Fang', 'Shaosheng Cao', 'Zheng Chu', 'Qingyu Yin', 'Shuang Chen', 'Zhenfei Yin', 'Lin Chen', 'Zehui Chen', 'Yao Hu', 'Philip Torr', 'Feng Zhao', 'Wanli Ouyang'], 'affiliations': ['CUHK MMLab', 'East China Normal University', 'Harbin Institute of Technology', 'Shenzhen Loop Area Institute', 'University of California, Los Angeles', 'University of Oxford', 'University of Science and Technology of China', 'Xiaohongshu Inc.', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.22060.jpg', 'data': {'categories': ['#multimodal', '#cv', '#rag', '#reasoning', '#optimization', '#rl', '#open_source'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Vision-DeepResearch Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¼Ñƒ, Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ½Ñ‚Ğ¸Ñ‚ĞµÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºÑƒ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ½Ğ°Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ ĞµĞ´Ğ¸Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ¼, Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑÑŒ Ñ ÑˆÑƒĞ¼Ğ¾Ğ¼ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ…Ğ¾Ğ»Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ñ€Ñ‚ Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ¸ reinforcement learning Ğ´Ğ»Ñ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ LLM. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´ĞµÑÑÑ‚ĞºĞ¸ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ñ‚Ğ½Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Revolutionizing Multimodal Search with Vision-DeepResearch', 'desc': 'Vision-DeepResearch presents a new approach to multimodal deep research that enhances visual and textual search capabilities. It allows for multi-turn interactions, handling multiple entities and scales, which is essential for addressing complex queries in noisy environments. The model integrates cold-start supervision and reinforcement learning to improve its reasoning depth and search breadth. As a result, it significantly outperforms existing multimodal large language models in real-world applications.'}, 'zh': {'title': 'å¤šæ¨¡æ€æ·±åº¦ç ”ç©¶çš„æ–°èŒƒå¼', 'desc': 'Vision-DeepResearchæå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ·±åº¦ç ”ç©¶èŒƒå¼ï¼Œèƒ½å¤Ÿè¿›è¡Œå¤šè½®ã€å¤šå®ä½“å’Œå¤šå°ºåº¦çš„è§†è§‰ä¸æ–‡æœ¬æœç´¢ã€‚è¯¥æ–¹æ³•é€šè¿‡å†·å¯åŠ¨ç›‘ç£å’Œå¼ºåŒ–å­¦ä¹ é›†æˆæ·±åº¦ç ”ç©¶èƒ½åŠ›ï¼Œå…‹æœäº†ä¼ ç»Ÿå¤šæ¨¡æ€æœç´¢åœ¨å¤æ‚é—®é¢˜ä¸Šçš„å±€é™æ€§ã€‚å®ƒæ”¯æŒæ•°åä¸ªæ¨ç†æ­¥éª¤å’Œæ•°ç™¾æ¬¡å¼•æ“äº¤äº’ï¼Œèƒ½å¤Ÿåœ¨å™ªå£°ç¯å¢ƒä¸­æœ‰æ•ˆæ£€ç´¢ä¿¡æ¯ã€‚ä¸ç°æœ‰çš„å¤šæ¨¡æ€æ·±åº¦ç ”ç©¶æ¨¡å‹ç›¸æ¯”ï¼ŒVision-DeepResearchåœ¨æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.02185', 'title': 'Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2602.02185', 'abstract': 'Vision-DeepResearch benchmark addresses limitations in evaluating visual-textual search capabilities of multimodal models by introducing realistic evaluation conditions and improving visual retrieval through multi-round cropped-search workflow.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/Vision-DeepResearch.', 'score': 106, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': '930ef62d925e6e3a', 'authors': ['Yu Zeng', 'Wenxuan Huang', 'Zhen Fang', 'Shuang Chen', 'Yufan Shen', 'Yishuo Cai', 'Xiaoman Wang', 'Zhenfei Yin', 'Lin Chen', 'Zehui Chen', 'Shiting Huang', 'Yiming Zhao', 'Yao Hu', 'Philip Torr', 'Wanli Ouyang', 'Shaosheng Cao'], 'affiliations': ['CUHK MMLab', 'East China Normal University', 'Peking University', 'Shenzhen Loop Area Institute', 'The University of California, Los Angeles', 'University of Oxford', 'University of Science and Technology of China', 'Xiaohongshu Inc.', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2602.02185.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#dataset', '#open_source'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Vision-DeepResearch benchmark Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²: ÑƒÑ‚ĞµÑ‡ĞºĞ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ¸ Ğ½ĞµÑ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ğ³Ğ´Ğ° Ñ‚Ñ€ĞµĞ±ÑƒĞµĞ¼Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ»ĞµĞ³ĞºĞ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€ÑĞ¼Ñ‹Ğ¼ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸ĞµĞ¼. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¾Ğ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ VDR-Bench Ñ 2000 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ VQA Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ÑƒĞ½Ğ´Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'Enhancing Visual-Textual Search with Realistic Evaluation', 'desc': 'The Vision-DeepResearch benchmark (VDR-Bench) addresses the challenges in evaluating the visual-textual search capabilities of multimodal models. It highlights two main issues with existing benchmarks: the reliance on cross-textual cues and overly simplified evaluation scenarios. To overcome these limitations, VDR-Bench includes 2,000 carefully curated visual question-answering instances that reflect real-world conditions. Additionally, it introduces a multi-round cropped-search workflow to enhance the visual retrieval performance of current multimodal large language models (MLLMs).'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨¡å‹çš„è§†è§‰-æ–‡æœ¬æœç´¢èƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†Vision-DeepResearchåŸºå‡†ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€æ¨¡å‹åœ¨è§†è§‰-æ–‡æœ¬æœç´¢èƒ½åŠ›è¯„ä¼°ä¸­çš„å±€é™æ€§ã€‚ç°æœ‰åŸºå‡†å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä¸€æ˜¯ç¼ºä¹ä»¥è§†è§‰æœç´¢ä¸ºä¸­å¿ƒçš„è¯„ä¼°ï¼ŒäºŒæ˜¯è¯„ä¼°åœºæ™¯è¿‡äºç†æƒ³åŒ–ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†åŒ…å«2000ä¸ªè§†è§‰é—®ç­”å®ä¾‹çš„VDR-Benchï¼Œå¹¶é€šè¿‡å¤šé˜¶æ®µçš„ç­–åˆ’æµç¨‹å’Œä¸“å®¶å®¡æ ¸æ¥ç¡®ä¿é—®é¢˜çš„è´¨é‡ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å¤šè½®è£å‰ªæœç´¢å·¥ä½œæµç¨‹ï¼Œä»¥æé«˜å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨çœŸå®è§†è§‰æ£€ç´¢åœºæ™¯ä¸­çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.02084', 'title': 'Closing the Loop: Universal Repository Representation with RPG-Encoder', 'url': 'https://huggingface.co/papers/2602.02084', 'abstract': "RPG-Encoder framework transforms repository comprehension and generation into a unified cycle by encoding code into high-fidelity Repository Planning Graph representations that improve understanding and reconstruction accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Current repository agents encounter a reasoning disconnect due to fragmented representations, as existing methods rely on isolated API documentation or dependency graphs that lack semantic depth. We consider repository comprehension and generation to be inverse processes within a unified cycle: generation expands intent into implementation, while comprehension compresses implementation back into intent. To address this, we propose RPG-Encoder, a framework that generalizes the Repository Planning Graph (RPG) from a static generative blueprint into a unified, high-fidelity representation. RPG-Encoder closes the reasoning loop through three mechanisms: (1) Encoding raw code into the RPG that combines lifted semantic features with code dependencies; (2) Evolving the topology incrementally to decouple maintenance costs from repository scale, reducing overhead by 95.7%; and (3) Operating as a unified interface for structure-aware navigation. In evaluations, RPG-Encoder establishes state-of-the-art repository understanding on SWE-bench Verified with 93.7% Acc@5 and exceeds the best baseline by over 10% on SWE-bench Live Lite. These results highlight our superior fine-grained localization accuracy in complex codebases. Furthermore, it achieves 98.5% reconstruction coverage on RepoCraft, confirming RPG's high-fidelity capacity to mirror the original codebase and closing the loop between intent and implementation.", 'score': 74, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': '99a14bc52117cc65', 'authors': ['Jane Luo', 'Chengyu Yin', 'Xin Zhang', 'Qingtao Li', 'Steven Liu', 'Yiming Huang', 'Jie Wu', 'Hao Liu', 'Yangyu Huang', 'Yu Kang', 'Fangkai Yang', 'Ying Xin', 'Scarlett Li'], 'affiliations': ['Microsoft Research Asia', 'Tsinghua University', 'UCSD'], 'pdf_title_img': 'assets/pdf/title_img/2602.02084.jpg', 'data': {'categories': ['#agents', '#benchmark', '#plp'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RPG-Encoder â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Repository Planning Graph. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ API Ğ¸ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ±ĞµĞ· ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. RPG-Encoder ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ğ´Ğ½ÑÑ‚Ñ‹Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ñ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑĞ¼Ğ¸ ĞºĞ¾Ğ´Ğ°, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑÑÑŒ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² (93.7% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° SWE-bench) Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 98.5% Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°.'}, 'en': {'title': 'Unifying Code Comprehension and Generation with RPG-Encoder', 'desc': 'The RPG-Encoder framework enhances the understanding and generation of code repositories by transforming them into high-fidelity Repository Planning Graphs (RPGs). This approach addresses the limitations of existing methods that rely on fragmented representations, allowing for a more cohesive understanding of code. By treating repository comprehension and generation as inverse processes, RPG-Encoder effectively links intent with implementation. The framework demonstrates significant improvements in accuracy and efficiency, achieving state-of-the-art results in repository understanding and reconstruction.'}, 'zh': {'title': 'ç»Ÿä¸€å¾ªç¯ï¼šæå‡ä»£ç åº“ç†è§£ä¸ç”Ÿæˆçš„RPG-Encoder', 'desc': 'RPG-Encoderæ¡†æ¶å°†ä»£ç åº“çš„ç†è§£å’Œç”Ÿæˆè½¬åŒ–ä¸ºä¸€ä¸ªç»Ÿä¸€çš„å¾ªç¯ï¼Œé€šè¿‡å°†ä»£ç ç¼–ç ä¸ºé«˜ä¿çœŸåº¦çš„ä»£ç åº“è§„åˆ’å›¾ï¼ˆRPGï¼‰è¡¨ç¤ºï¼Œæå‡äº†ç†è§£å’Œé‡æ„çš„å‡†ç¡®æ€§ã€‚ç°æœ‰çš„ä»£ç åº“ä»£ç†ç”±äºä¾èµ–äºå­¤ç«‹çš„APIæ–‡æ¡£æˆ–ç¼ºä¹è¯­ä¹‰æ·±åº¦çš„ä¾èµ–å›¾ï¼Œå¯¼è‡´æ¨ç†æ–­è£‚ã€‚æˆ‘ä»¬è®¤ä¸ºä»£ç åº“çš„ç†è§£å’Œç”Ÿæˆæ˜¯ä¸€ä¸ªç»Ÿä¸€å¾ªç¯ä¸­çš„é€†è¿‡ç¨‹ï¼šç”Ÿæˆå°†æ„å›¾æ‰©å±•ä¸ºå®ç°ï¼Œè€Œç†è§£åˆ™å°†å®ç°å‹ç¼©å›æ„å›¾ã€‚RPG-Encoderé€šè¿‡ç¼–ç åŸå§‹ä»£ç ã€é€æ­¥æ¼”åŒ–æ‹“æ‰‘ç»“æ„å’Œä½œä¸ºç»Ÿä¸€æ¥å£æ¥å…³é—­æ¨ç†å¾ªç¯ï¼Œä»è€Œåœ¨å¤æ‚ä»£ç åº“ä¸­å®ç°äº†æ›´é«˜çš„å®šä½å‡†ç¡®æ€§å’Œé‡æ„è¦†ç›–ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.02437', 'title': 'UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing', 'url': 'https://huggingface.co/papers/2602.02437', 'abstract': 'UniReason integrates text-to-image generation and image editing through a dual reasoning paradigm that enhances planning with world knowledge and uses editing for visual refinement, achieving superior performance on reasoning-intensive benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.', 'score': 67, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': '40adab1ac54f110d', 'authors': ['Dianyi Wang', 'Chaofan Ma', 'Feng Han', 'Size Wu', 'Wei Song', 'Yibin Wang', 'Zhixiong Zhang', 'Tianhang Wang', 'Siyuan Wang', 'Zhongyu Wei', 'Jiaqi Wang'], 'affiliations': ['Fudan University', 'Nanyang Technological University', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'University of Southern California', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2602.02437.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#benchmark', '#reasoning', '#synthetic'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑƒÑ‚Ğ¾Ğ½Ñ‡ĞµĞ½Ğ¸Ğµ', 'desc': 'UniReason â€” ÑÑ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ°Ñæ¡†æ¶, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ğ¾Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¾ Ğ¼Ğ¸Ñ€Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ğ½ĞµĞ´Ñ€Ğ¸Ñ‚ÑŒ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 300k Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿ÑÑ‚ÑŒ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼Ğ¸ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Harmonizing Generation and Editing for Enhanced Reasoning in AI', 'desc': 'UniReason is a novel framework that combines text-to-image generation and image editing into a single process, enhancing reasoning capabilities. It employs a dual reasoning paradigm that integrates world knowledge to improve planning and uses editing for precise visual adjustments. By treating generation and editing as interconnected steps, UniReason mimics human cognitive processes, allowing for better synthesis of complex tasks. The framework is supported by a large dataset designed for reasoning tasks, demonstrating significant improvements in performance on various benchmarks.'}, 'zh': {'title': 'UniReasonï¼šç»Ÿä¸€ç”Ÿæˆä¸ç¼–è¾‘çš„æ™ºèƒ½æ¨ç†æ¡†æ¶', 'desc': 'UniReason æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç»“åˆäº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘ï¼Œé€šè¿‡åŒé‡æ¨ç†èŒƒå¼æ¥å¢å¼ºè§„åˆ’èƒ½åŠ›ã€‚å®ƒå°†ç”Ÿæˆè§†ä¸ºå¢å¼ºä¸–ç•ŒçŸ¥è¯†çš„è§„åˆ’ï¼Œå¹¶åˆ©ç”¨ç¼–è¾‘èƒ½åŠ›è¿›è¡Œç»†è‡´çš„è§†è§‰ä¿®æ­£ï¼Œä»è€Œçº æ­£è§†è§‰é”™è¯¯ã€‚è¯¥æ–¹æ³•åœ¨å…±äº«è¡¨ç¤ºä¸­ç»Ÿä¸€äº†ç”Ÿæˆå’Œç¼–è¾‘ï¼Œæ¨¡æ‹Ÿäº†äººç±»çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œå³è§„åˆ’åå†è¿›è¡Œä¿®æ­£ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªå¤§å‹æ¨ç†ä¸­å¿ƒæ•°æ®é›†ï¼ŒUniReason åœ¨æ¨ç†å¯†é›†å‹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶ä¿æŒäº†ä¼˜è¶Šçš„ç»¼åˆåˆæˆèƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.02053', 'title': 'WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora', 'url': 'https://huggingface.co/papers/2602.02053', 'abstract': "WildGraphBench evaluates GraphRAG performance in realistic scenarios using Wikipedia's structured content to assess multi-fact aggregation and summarization capabilities across diverse document types.  \t\t\t\t\tAI-generated summary \t\t\t\t Graph-based Retrieval-Augmented Generation (GraphRAG) organizes external knowledge as a hierarchical graph, enabling efficient retrieval and aggregation of scattered evidence across multiple documents. However, many existing benchmarks for GraphRAG rely on short, curated passages as external knowledge, failing to adequately evaluate systems in realistic settings involving long contexts and large-scale heterogeneous documents. To bridge this gap, we introduce WildGraphBench, a benchmark designed to assess GraphRAG performance in the wild. We leverage Wikipedia's unique structure, where cohesive narratives are grounded in long and heterogeneous external reference documents, to construct a benchmark reflecting real-word scenarios. Specifically, we sample articles across 12 top-level topics, using their external references as the retrieval corpus and citation-linked statements as ground truth, resulting in 1,100 questions spanning three levels of complexity: single-fact QA, multi-fact QA, and section-level summarization. Experiments across multiple baselines reveal that current GraphRAG pipelines help on multi-fact aggregation when evidence comes from a moderate number of sources, but this aggregation paradigm may overemphasize high-level statements at the expense of fine-grained details, leading to weaker performance on summarization tasks. Project page:https://github.com/BstWPY/WildGraphBench.", 'score': 39, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': '81f1879290eedf25', 'authors': ['Pengyu Wang', 'Benfeng Xu', 'Licheng Zhang', 'Shaohan Wang', 'Mingxuan Du', 'Chiwei Zhu', 'Zhendong Mao'], 'affiliations': ['Metastone Technology, Beijing, China', 'University of Science and Technology of China, Hefei, China'], 'pdf_title_img': 'assets/pdf/title_img/2602.02053.jpg', 'data': {'categories': ['#dataset', '#rag', '#benchmark', '#open_source', '#graphs', '#long_context'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ WildGraphBench â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ GraphRAG Ğ² Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ’Ğ¸ĞºĞ¸Ğ¿ĞµĞ´Ğ¸Ğ¸. GraphRAG Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ³Ñ€Ğ°Ñ„Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 1100 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ (Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸) Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ’Ğ¸ĞºĞ¸Ğ¿ĞµĞ´Ğ¸Ğ¸ Ğ¸ Ğ¸Ñ… ÑĞ¿Ñ€Ğ°Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ GraphRAG Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ², Ğ½Ğ¾ ÑĞºĞ»Ğ¾Ğ½Ğ½Ñ‹ Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ñ‰Ğ¸Ğµ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑƒÑ‰ĞµÑ€Ğ± Ğ´ĞµÑ‚Ğ°Ğ»ÑĞ¼ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Evaluating GraphRAG in Real-World Scenarios with WildGraphBench', 'desc': "WildGraphBench is a new benchmark that evaluates the performance of GraphRAG, a model that uses graph-based retrieval to enhance multi-fact aggregation and summarization. It focuses on realistic scenarios by utilizing Wikipedia's structured content, which provides a diverse set of long and heterogeneous documents. The benchmark includes 1,100 questions that test different levels of complexity, from single-fact to multi-fact and summarization tasks. Results show that while GraphRAG performs well with moderate sources, it struggles with detailed summarization due to a tendency to prioritize high-level information over finer details."}, 'zh': {'title': 'WildGraphBenchï¼šçœŸå®åœºæ™¯ä¸‹çš„å›¾å½¢å¢å¼ºç”Ÿæˆè¯„ä¼°', 'desc': 'WildGraphBench æ˜¯ä¸€ä¸ªè¯„ä¼° GraphRAG åœ¨ç°å®åœºæ™¯ä¸­è¡¨ç°çš„åŸºå‡†ï¼Œåˆ©ç”¨ç»´åŸºç™¾ç§‘çš„ç»“æ„åŒ–å†…å®¹æ¥æµ‹è¯•å¤šäº‹å®èšåˆå’Œæ‘˜è¦èƒ½åŠ›ã€‚GraphRAG é€šè¿‡å°†å¤–éƒ¨çŸ¥è¯†ç»„ç»‡ä¸ºå±‚æ¬¡å›¾ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°æ£€ç´¢å’Œèšåˆæ¥è‡ªå¤šä¸ªæ–‡æ¡£çš„åˆ†æ•£è¯æ®ã€‚ç°æœ‰çš„ GraphRAG åŸºå‡†å¤§å¤šä¾èµ–äºçŸ­å°çš„ç²¾å¿ƒæŒ‘é€‰çš„æ®µè½ï¼Œæ— æ³•å……åˆ†è¯„ä¼°ç³»ç»Ÿåœ¨é•¿ä¸Šä¸‹æ–‡å’Œå¤§è§„æ¨¡å¼‚æ„æ–‡æ¡£ä¸­çš„è¡¨ç°ã€‚WildGraphBench é€šè¿‡é‡‡æ · 12 ä¸ªé¡¶çº§ä¸»é¢˜çš„æ–‡ç« ï¼Œæ„å»ºäº†ä¸€ä¸ªåæ˜ çœŸå®åœºæ™¯çš„åŸºå‡†ï¼ŒåŒ…å« 1,100 ä¸ªé—®é¢˜ï¼Œæ¶µç›–å•äº‹å®é—®ç­”ã€å¤šäº‹å®é—®ç­”å’Œç« èŠ‚çº§æ‘˜è¦ç­‰ä¸åŒå¤æ‚åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01566', 'title': 'FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents', 'url': 'https://huggingface.co/papers/2602.01566', 'abstract': 'A file-system-based dual-agent framework enables large language model agents to perform extended research tasks beyond context window limitations by using persistent storage as external memory.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep research is emerging as a representative long-horizon task for large language model (LLM) agents. However, long trajectories in deep research often exceed model context limits, compressing token budgets for both evidence collection and report writing, and preventing effective test-time scaling. We introduce FS-Researcher, a file-system-based, dual-agent framework that scales deep research beyond the context window via a persistent workspace. Specifically, a Context Builder agent acts as a librarian which browses the internet, writes structured notes, and archives raw sources into a hierarchical knowledge base that can grow far beyond context length. A Report Writer agent then composes the final report section by section, treating the knowledge base as the source of facts. In this framework, the file system serves as a durable external memory and a shared coordination medium across agents and sessions, enabling iterative refinement beyond the context window. Experiments on two open-ended benchmarks (DeepResearch Bench and DeepConsult) show that FS-Researcher achieves state-of-the-art report quality across different backbone models. Further analyses demonstrate a positive correlation between final report quality and the computation allocated to the Context Builder, validating effective test-time scaling under the file-system paradigm. The code and data are anonymously open-sourced at https://github.com/Ignoramus0817/FS-Researcher.', 'score': 38, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': 'efb8704f7c339683', 'authors': ['Chiwei Zhu', 'Benfeng Xu', 'Mingxuan Du', 'Shaohan Wang', 'Xiaorui Wang', 'Zhendong Mao', 'Yongdong Zhang'], 'affiliations': ['Metastone Technology', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2602.01566.jpg', 'data': {'categories': ['#dataset', '#agents', '#benchmark', '#reasoning', '#open_source', '#long_context'], 'emoji': 'ğŸ“š', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹: Ğ²Ğ½ĞµÑˆĞ½ÑÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² LLM', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ FS-Researcher â€” Ğ´Ğ²ÑƒÑ…Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ³Ğ´Ğµ Ğ¾Ğ´Ğ¸Ğ½ Ğ°Ğ³ĞµĞ½Ñ‚ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ° Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ±Ğ°Ğ·Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ, Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑ ĞºĞ°Ğº Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ°Ñ€ÑŒ, Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¿Ğ¸ÑˆĞµÑ‚ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹. Ğ¤Ğ°Ğ¹Ğ»Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ° LLM. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞµĞ°Ğ½ÑĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'Unlocking Deep Research with FS-Researcher: Beyond Context Limits!', 'desc': 'This paper presents FS-Researcher, a dual-agent framework designed to enhance the capabilities of large language models (LLMs) for deep research tasks that exceed their context window limitations. The framework utilizes a file system as external memory, allowing a Context Builder agent to gather and organize information from the internet into a structured knowledge base. A Report Writer agent then uses this knowledge base to generate comprehensive reports, enabling iterative refinement and improved report quality. Experiments demonstrate that FS-Researcher outperforms existing methods, showing that effective allocation of resources to the Context Builder correlates with higher report quality.'}, 'zh': {'title': 'è¶…è¶Šä¸Šä¸‹æ–‡é™åˆ¶çš„æ·±åº¦ç ”ç©¶æ¡†æ¶', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFS-Researcherçš„åŒä»£ç†æ¡†æ¶ï¼Œæ—¨åœ¨å¸®åŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¿›è¡Œæ·±åº¦ç ”ç©¶æ—¶è¶…è¶Šä¸Šä¸‹æ–‡çª—å£çš„é™åˆ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ–‡ä»¶ç³»ç»Ÿä½œä¸ºæŒä¹…å­˜å‚¨ï¼Œå…è®¸ä¸€ä¸ªä»£ç†è´Ÿè´£æ”¶é›†å’Œæ•´ç†ä¿¡æ¯ï¼Œè€Œå¦ä¸€ä¸ªä»£ç†åˆ™è´Ÿè´£æ’°å†™æŠ¥å‘Šã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç ”ç©¶è€…å¯ä»¥åœ¨æ›´é•¿çš„æ—¶é—´è·¨åº¦å†…è¿›è¡Œæœ‰æ•ˆçš„ç ”ç©¶ï¼Œè€Œä¸å—é™äºæ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFS-Researcheråœ¨æŠ¥å‘Šè´¨é‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œè¯æ˜äº†å…¶åœ¨æµ‹è¯•æ—¶æ‰©å±•èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.02361', 'title': 'SWE-Universe: Scale Real-World Verifiable Environments to Millions', 'url': 'https://huggingface.co/papers/2602.02361', 'abstract': 'A scalable framework for constructing real-world software engineering environments from GitHub pull requests using an efficient building agent with self-verification and hacking detection capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model. This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning. Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified. Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents.', 'score': 32, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': '59a87be075e0af02', 'authors': ['Mouxiang Chen', 'Lei Zhang', 'Yunlong Feng', 'Xuwu Wang', 'Wenting Zhao', 'Ruisheng Cao', 'Jiaxi Yang', 'Jiawei Chen', 'Mingze Li', 'Zeyao Ma', 'Hao Ge', 'Zongmeng Zhang', 'Zeyu Cui', 'Dayiheng Liu', 'Jingren Zhou', 'Jianling Sun', 'Junyang Lin', 'Binyuan Hui'], 'affiliations': ['Alibaba Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2602.02361.jpg', 'data': {'categories': ['#multilingual', '#plp', '#dataset', '#agents', '#benchmark', '#rl', '#training'], 'emoji': 'ğŸ—ï¸', 'ru': {'title': 'ĞœĞ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ± Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ SWE-Universe â€” Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ pull request'Ğ¾Ğ² Ğ¸Ğ· GitHub. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ¸ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ğ°Ğ¼ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¾ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° (807,693), Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞºĞ¾Ğ´Ğ°."}, 'en': {'title': 'Building Reliable Software Environments at Scale', 'desc': 'The paper introduces SWE-Universe, a framework designed to automatically create verifiable software engineering environments from GitHub pull requests. It addresses common issues in automatic building, such as low success rates and ineffective verification processes, by using a custom-trained building agent. This agent incorporates self-verification and hacking detection to ensure the environments generated are reliable and high-quality. The framework successfully scales to nearly a million environments and demonstrates its effectiveness through applications in reinforcement learning and agent training.'}, 'zh': {'title': 'æ„å»ºå¯éªŒè¯è½¯ä»¶å·¥ç¨‹ç¯å¢ƒçš„é«˜æ•ˆæ¡†æ¶', 'desc': 'æˆ‘ä»¬æå‡ºäº†SWE-Universeï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•ä¸”é«˜æ•ˆçš„æ¡†æ¶ï¼Œç”¨äºä»GitHubæ‹‰å–è¯·æ±‚è‡ªåŠ¨æ„å»ºå¯éªŒè¯çš„çœŸå®è½¯ä»¶å·¥ç¨‹ç¯å¢ƒã€‚ä¸ºäº†å…‹æœè‡ªåŠ¨æ„å»ºä¸­çš„å¸¸è§æŒ‘æˆ˜ï¼Œå¦‚ä½ç”Ÿäº§ç‡å’Œå¼±éªŒè¯å™¨ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åˆ©ç”¨äº†ä¸€ä¸ªç”±é«˜æ•ˆå®šåˆ¶æ¨¡å‹é©±åŠ¨çš„æ„å»ºä»£ç†ã€‚è¯¥ä»£ç†é‡‡ç”¨è¿­ä»£è‡ªéªŒè¯å’Œå¾ªç¯é»‘å®¢æ£€æµ‹ï¼Œç¡®ä¿ç”Ÿæˆé«˜ä¿çœŸåº¦çš„å¯éªŒè¯ä»»åŠ¡ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å°†çœŸå®å¤šè¯­è¨€è½¯ä»¶å·¥ç¨‹ç¯å¢ƒçš„æ•°é‡æ‰©å±•åˆ°ç™¾ä¸‡è§„æ¨¡ï¼Œå¹¶åœ¨Qwen3-Max-Thinkingä¸Šå–å¾—äº†75.3%çš„å¾—åˆ†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01590', 'title': 'Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles', 'url': 'https://huggingface.co/papers/2602.01590', 'abstract': "Deep Research Agents demonstrate capabilities in autonomous information retrieval but show significant gaps when evaluated against expert-level Wikipedia articles using a new live benchmark and comprehensive evaluation framework.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep Research Agents (DRAs) have demonstrated remarkable capabilities in autonomous information retrieval and report generation, showing great potential to assist humans in complex research tasks. Current evaluation frameworks primarily rely on LLM-generated references or LLM-derived evaluation dimensions. While these approaches offer scalability, they often lack the reliability of expert-verified content and struggle to provide objective, fine-grained assessments of critical dimensions. To bridge this gap, we introduce Wiki Live Challenge (WLC), a live benchmark that leverages the newest Wikipedia Good Articles (GAs) as expert-level references. Wikipedia's strict standards for neutrality, comprehensiveness, and verifiability serve as a great challenge for DRAs, with GAs representing the pinnacle of which. We curate a dataset of 100 recent Good Articles and propose Wiki Eval, a comprehensive evaluation framework comprising a fine-grained evaluation method with 39 criteria for writing quality and rigorous metrics for factual verifiability. Extensive experiments on various DRA systems demonstrate a significant gap between current DRAs and human expert-level Wikipedia articles, validating the effectiveness of WLC in advancing agent research. We release our benchmark at https://github.com/WangShao2000/Wiki_Live_Challenge", 'score': 30, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': '1e1b08d9febd5020', 'authors': ['Shaohan Wang', 'Benfeng Xu', 'Licheng Zhang', 'Mingxuan Du', 'Chiwei Zhu', 'Xiaorui Wang', 'Zhendong Mao', 'Yongdong Zhang'], 'affiliations': ['Metastone Technology, Beijing, China', 'University of Science and Technology of China, Hefei, China'], 'pdf_title_img': 'assets/pdf/title_img/2602.01590.jpg', 'data': {'categories': ['#dataset', '#agents', '#benchmark', '#open_source', '#survey'], 'emoji': 'ğŸ“š', 'ru': {'title': 'Ğ˜Ğ·Ğ¼ĞµÑ€ÑÑ Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ benchmark Wiki Live Challenge, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Wikipedia Good Articles ĞºĞ°Ğº ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Deep Research Agents. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Wiki Eval Ñ 39 ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¸ÑÑŒĞ¼Ğ° Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾ÌĞ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ°Ñ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞŸÑ€Ğ¾Ğ²ĞµĞ´Ñ‘Ğ½Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… DRA Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑÑ‚Ğ°Ñ‚ÑŒÑĞ¼Ğ¸, Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Wikipedia. ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ñƒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğµ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Bridging the Gap: Evaluating AI with Expert Standards', 'desc': "Deep Research Agents (DRAs) are AI systems designed to autonomously retrieve information and generate reports, showing promise in aiding complex research tasks. However, when evaluated against expert-level Wikipedia articles, DRAs reveal significant shortcomings, particularly in reliability and objective assessment. The Wiki Live Challenge (WLC) introduces a new benchmark using Wikipedia's Good Articles as a standard for evaluation, emphasizing neutrality and verifiability. This framework includes a detailed evaluation method with 39 criteria, highlighting the performance gap between DRAs and human experts, thus pushing the boundaries of agent research."}, 'zh': {'title': 'æå‡æ·±åº¦ç ”ç©¶ä»£ç†çš„è¯„ä¼°æ ‡å‡†', 'desc': 'æ·±åº¦ç ”ç©¶ä»£ç†ï¼ˆDRAï¼‰åœ¨è‡ªä¸»ä¿¡æ¯æ£€ç´¢å’ŒæŠ¥å‘Šç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¸ä¸“å®¶çº§ç»´åŸºç™¾ç§‘æ–‡ç« çš„è¯„ä¼°ä¸­å­˜åœ¨æ˜¾è‘—å·®è·ã€‚å½“å‰çš„è¯„ä¼°æ¡†æ¶ä¸»è¦ä¾èµ–äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„å‚è€ƒèµ„æ–™ï¼Œç¼ºä¹ä¸“å®¶éªŒè¯å†…å®¹çš„å¯é æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç»´åŸºå®æ—¶æŒ‘æˆ˜ï¼ˆWLCï¼‰ï¼Œåˆ©ç”¨æœ€æ–°çš„ç»´åŸºç™¾ç§‘ä¼˜è´¨æ–‡ç« ä½œä¸ºä¸“å®¶çº§å‚è€ƒã€‚é€šè¿‡å¯¹100ç¯‡ä¼˜è´¨æ–‡ç« çš„è¯„ä¼°ï¼Œæˆ‘ä»¬æå‡ºäº†Wiki Evalæ¡†æ¶ï¼ŒåŒ…å«39ä¸ªå†™ä½œè´¨é‡æ ‡å‡†å’Œä¸¥æ ¼çš„äº‹å®å¯éªŒè¯æ€§æŒ‡æ ‡ï¼ŒéªŒè¯äº†DRAä¸äººç±»ä¸“å®¶ä¹‹é—´çš„å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.02493', 'title': 'PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss', 'url': 'https://huggingface.co/papers/2602.02493', 'abstract': 'PixelGen is a pixel-space diffusion framework that uses perceptual supervision through LPIPS and DINO-based losses to generate high-quality images without requiring VAEs or latent representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.', 'score': 28, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': '4772b7581ff51570', 'authors': ['Zehong Ma', 'Ruihan Xu', 'Shiliang Zhang'], 'affiliations': ['State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2602.02493.jpg', 'data': {'categories': [], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞŸÑ€ÑĞ¼Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ´Ğ·Ğ¾Ñ€', 'desc': 'PixelGen â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ½Ğ¾ÑÑÑ‚ VAE Ğ² Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ´Ğ·Ğ¾Ñ€ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²Ğµ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ: LPIPS Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼ Ğ¸ DINO-based Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ´Ğ»Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾Ğ¼ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ. PixelGen Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² (FID 5.11 Ğ½Ğ° ImageNet-256), Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ VAE Ğ¸ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼ Ğ¸ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'PixelGen: Simplifying Image Generation with Perceptual Supervision', 'desc': 'PixelGen is a novel pixel-space diffusion framework that generates high-quality images directly in pixel space, eliminating the need for variational autoencoders (VAEs) or latent representations. It utilizes perceptual supervision through two types of losses: LPIPS for enhancing local image patterns and DINO for improving global semantic understanding. This approach allows PixelGen to effectively navigate the complex high-dimensional pixel space, overcoming challenges faced by traditional pixel diffusion methods. As a result, PixelGen achieves impressive performance metrics, including a low FID score on ImageNet-256, demonstrating its capability in large-scale text-to-image generation tasks.'}, 'zh': {'title': 'PixelGenï¼šç®€åŒ–è€Œå¼ºå¤§çš„å›¾åƒç”Ÿæˆæ¡†æ¶', 'desc': 'PixelGenæ˜¯ä¸€ç§åƒç´ ç©ºé—´æ‰©æ•£æ¡†æ¶ï¼Œé€šè¿‡LPIPSå’ŒåŸºäºDINOçš„æŸå¤±è¿›è¡Œæ„ŸçŸ¥ç›‘ç£ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼Œè€Œæ— éœ€ä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æˆ–æ½œåœ¨è¡¨ç¤ºã€‚è¯¥æ–¹æ³•ç›´æ¥åœ¨åƒç´ ç©ºé—´ä¸­ç”Ÿæˆå›¾åƒï¼Œé¿å…äº†ä¸¤é˜¶æ®µæ½œåœ¨æ‰©æ•£ä¸­å¼•å…¥çš„ä¼ªå½±å’Œç“¶é¢ˆã€‚PixelGenå¼•å…¥äº†ä¸¤ç§äº’è¡¥çš„æ„ŸçŸ¥æŸå¤±ï¼Œå¸®åŠ©æ‰©æ•£æ¨¡å‹å­¦ä¹ æ›´æœ‰æ„ä¹‰çš„æ„ŸçŸ¥æµå½¢ï¼Œä»è€Œä¼˜åŒ–é«˜ç»´åƒç´ æµå½¢ã€‚é€šè¿‡æ„ŸçŸ¥ç›‘ç£ï¼ŒPixelGenåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å¼ºå¤§çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.02383', 'title': 'SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization', 'url': 'https://huggingface.co/papers/2602.02383', 'abstract': "SLIME is a novel reference-free alignment objective for large language models that decouples preference learning from generation quality through a three-pronged approach combining likelihood maximization, probability stabilization, and dual-margin constraints.  \t\t\t\t\tAI-generated summary \t\t\t\t Direct preference optimization methods have emerged as a computationally efficient alternative to Reinforcement Learning from Human Feedback (RLHF) for aligning Large Language Models (LLMs). Latest approaches have streamlined the alignment process by deriving implicit reward functions, yet they often suffer from a critical objective mismatch: optimizing the relative margin between chosen and rejected responses does not guarantee the preservation of the chosen response's absolute likelihood. This can lead to ``unlearning'', where the model degrades the probability of high-quality outputs to satisfy margin constraints, and ``formatting collapse'' caused by the over-penalization of rejected sequences. In this work, we introduce SLIME (Stabilized Likelihood Implicit Margin Enforcement), a reference-free alignment objective designed to decouple preference learning from generation quality. SLIME incorporates a three-pronged objective: (1) an anchoring term to maximize the likelihood of preferred responses; (2) a stabilizing penalty that prevents the probabilities of rejected tokens from collapsing to zero; and (3) a dual-margin mechanism that combines hard and soft constraints for precise boundary shaping. Our results demonstrate that SLIME achieves superior performance compared to state-of-the-art baselines while maintaining higher generation stability.", 'score': 26, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': '0aa05d3f99126285', 'authors': ['Maksim Afanasyev', 'Illarion Iov'], 'affiliations': ['Floating Point Sigma Lab'], 'pdf_title_img': 'assets/pdf/title_img/2602.02383.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#training'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼', 'desc': 'SLIME â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ… Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½Ñ‘Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ñ‹Ğµ margin-Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Â«Ñ€Ğ°Ğ·ÑƒÑ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸ÑÂ» Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Â«ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÂ», ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‚ Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿ĞµĞ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SLIME Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'SLIME: Aligning Preferences with Stability in Language Models', 'desc': "SLIME is a new method for aligning large language models without needing reference data. It separates the learning of user preferences from the quality of the generated text. The approach uses three key strategies: maximizing the likelihood of preferred responses, stabilizing the probabilities of less preferred options, and applying dual-margin constraints for better control. This results in improved performance and stability in the model's outputs compared to existing methods."}, 'zh': {'title': 'SLIMEï¼šè§£è€¦åå¥½å­¦ä¹ ä¸ç”Ÿæˆè´¨é‡çš„åˆ›æ–°å¯¹é½ç›®æ ‡', 'desc': 'SLIMEæ˜¯ä¸€ç§æ–°é¢–çš„æ— å‚è€ƒå¯¹é½ç›®æ ‡ï¼Œæ—¨åœ¨é€šè¿‡ä¸‰é‡æ–¹æ³•å°†åå¥½å­¦ä¹ ä¸ç”Ÿæˆè´¨é‡è§£è€¦ã€‚å®ƒç»“åˆäº†ä¼¼ç„¶æœ€å¤§åŒ–ã€æ¦‚ç‡ç¨³å®šåŒ–å’ŒåŒè¾¹ç•Œçº¦æŸï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½æ•ˆæœã€‚SLIMEçš„è®¾è®¡é¿å…äº†æ¨¡å‹åœ¨ä¼˜åŒ–ç›¸å¯¹è¾¹é™…æ—¶å¯èƒ½å¯¼è‡´çš„é«˜è´¨é‡è¾“å‡ºæ¦‚ç‡ä¸‹é™å’Œæ ¼å¼å´©æºƒé—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSLIMEåœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›åŸºçº¿ï¼ŒåŒæ—¶ä¿æŒäº†æ›´é«˜çš„ç”Ÿæˆç¨³å®šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.02488', 'title': 'RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System', 'url': 'https://huggingface.co/papers/2602.02488', 'abstract': 'RLAnything enhances reinforcement learning for LLMs and agents through dynamic model optimization and closed-loop feedback mechanisms that improve policy and reward model training.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL', 'score': 25, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': 'd1b9b86c47a0ffda', 'authors': ['Yinjie Wang', 'Tianbao Xie', 'Ke Shen', 'Mengdi Wang', 'Ling Yang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2602.02488.jpg', 'data': {'categories': ['#rl', '#agents', '#training'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'RLAnything â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ñ‹Ğµ Ñ†Ğ¸ĞºĞ»Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. ĞŸĞ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸, Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ĞµĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¾Ğ¿Ñ‹Ñ‚Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ´Ğ»Ñ LLM Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ° 9-18% Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.'}, 'en': {'title': 'Boosting Learning with RLAnything: Dynamic Optimization for LLMs and Agents', 'desc': 'RLAnything is a novel reinforcement learning framework designed to enhance the training of large language models (LLMs) and agents. It utilizes dynamic model optimization and closed-loop feedback mechanisms to improve both policy and reward model training. By integrating feedback from both step-wise actions and overall outcomes, RLAnything strengthens the learning signals, leading to better performance. The framework also adapts the training environment based on critic feedback, allowing models to learn more effectively from their experiences, resulting in significant performance improvements across various tasks.'}, 'zh': {'title': 'RLAnythingï¼šåŠ¨æ€ä¼˜åŒ–å¼ºåŒ–å­¦ä¹ çš„æœªæ¥', 'desc': 'RLAnythingæ˜¯ä¸€ä¸ªå¢å¼ºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œä¸“ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ™ºèƒ½ä½“è®¾è®¡ã€‚å®ƒé€šè¿‡åŠ¨æ€ä¼˜åŒ–ç¯å¢ƒã€ç­–ç•¥å’Œå¥–åŠ±æ¨¡å‹ï¼Œåˆ©ç”¨é—­ç¯åé¦ˆæœºåˆ¶æ¥æå‡å­¦ä¹ æ•ˆæœã€‚è¯¥æ¡†æ¶ç»“åˆäº†é€æ­¥åé¦ˆå’Œç»“æœä¿¡å·ï¼Œä¼˜åŒ–ç­–ç•¥è®­ç»ƒï¼ŒåŒæ—¶é€šè¿‡ä¸€è‡´æ€§åé¦ˆå…±åŒä¼˜åŒ–å¥–åŠ±æ¨¡å‹ï¼Œä»è€Œè¿›ä¸€æ­¥æ”¹å–„ç­–ç•¥è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRLAnythingåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01624', 'title': 'PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards', 'url': 'https://huggingface.co/papers/2602.01624', 'abstract': 'PISCES is an annotation-free text-to-video generation method that uses dual optimal transport-aligned rewards to improve visual quality and semantic alignment without human preference annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-video (T2V) generation aims to synthesize videos with high visual quality and temporal consistency that are semantically aligned with input text. Reward-based post-training has emerged as a promising direction to improve the quality and semantic alignment of generated videos. However, recent methods either rely on large-scale human preference annotations or operate on misaligned embeddings from pre-trained vision-language models, leading to limited scalability or suboptimal supervision. We present PISCES, an annotation-free post-training algorithm that addresses these limitations via a novel Dual Optimal Transport (OT)-aligned Rewards module. To align reward signals with human judgment, PISCES uses OT to bridge text and video embeddings at both distributional and discrete token levels, enabling reward supervision to fulfill two objectives: (i) a Distributional OT-aligned Quality Reward that captures overall visual quality and temporal coherence; and (ii) a Discrete Token-level OT-aligned Semantic Reward that enforces semantic, spatio-temporal correspondence between text and video tokens. To our knowledge, PISCES is the first to improve annotation-free reward supervision in generative post-training through the lens of OT. Experiments on both short- and long-video generation show that PISCES outperforms both annotation-based and annotation-free methods on VBench across Quality and Semantic scores, with human preference studies further validating its effectiveness. We show that the Dual OT-aligned Rewards module is compatible with multiple optimization paradigms, including direct backpropagation and reinforcement learning fine-tuning.', 'score': 23, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': 'cc87c0d6fe4774f9', 'authors': ['Minh-Quan Le', 'Gaurav Mittal', 'Cheng Zhao', 'David Gu', 'Dimitris Samaras', 'Mei Chen'], 'affiliations': ['Microsoft', 'Stony Brook University'], 'pdf_title_img': 'assets/pdf/title_img/2602.01624.jpg', 'data': {'categories': ['#optimization', '#rl', '#video', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'PISCES â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ…: Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° Ñ‚Ğ¸Ğ¿Ğ° Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ â€” Ğ¾Ğ´Ğ½Ğ° Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ‰ĞµĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ´Ñ€ÑƒĞ³Ğ°Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ PISCES Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ±ĞµĞ· Ğ½Ğ¸Ñ….'}, 'en': {'title': 'PISCES: Annotation-Free Video Generation with Optimal Transport Rewards', 'desc': 'PISCES is a novel method for generating videos from text without needing human annotations. It uses a technique called Dual Optimal Transport to align rewards that improve both the visual quality and the semantic relevance of the generated videos. By bridging text and video embeddings, PISCES ensures that the generated content is coherent and matches the intended meaning. Experiments demonstrate that PISCES outperforms existing methods, making it a significant advancement in text-to-video generation.'}, 'zh': {'title': 'æ— æ³¨é‡Šçš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'PISCESæ˜¯ä¸€ç§æ— æ³¨é‡Šçš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜ç”Ÿæˆè§†é¢‘çš„è§†è§‰è´¨é‡å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡åŒé‡æœ€ä¼˜ä¼ è¾“å¯¹é½å¥–åŠ±æ¨¡å—ï¼Œè§£å†³äº†ä¾èµ–äººå·¥åå¥½æ³¨é‡Šçš„å±€é™æ€§ã€‚PISCESä½¿ç”¨æœ€ä¼˜ä¼ è¾“æŠ€æœ¯åœ¨æ–‡æœ¬å’Œè§†é¢‘åµŒå…¥ä¹‹é—´å»ºç«‹è”ç³»ï¼Œä»è€Œå®ç°å¥–åŠ±ä¿¡å·ä¸äººç±»åˆ¤æ–­çš„å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPISCESåœ¨è§†é¢‘ç”Ÿæˆçš„è´¨é‡å’Œè¯­ä¹‰è¯„åˆ†ä¸Šä¼˜äºå…¶ä»–æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01756', 'title': 'Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation', 'url': 'https://huggingface.co/papers/2602.01756', 'abstract': "Mind-Brush presents a unified agentic framework for text-to-image generation that dynamically retrieves multimodal evidence and employs reasoning tools to improve understanding of implicit user intentions and complex knowledge reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t While text-to-image generation has achieved unprecedented fidelity, the vast majority of existing models function fundamentally as static text-to-pixel decoders. Consequently, they often fail to grasp implicit user intentions. Although emerging unified understanding-generation models have improved intent comprehension, they still struggle to accomplish tasks involving complex knowledge reasoning within a single model. Moreover, constrained by static internal priors, these models remain unable to adapt to the evolving dynamics of the real world. To bridge these gaps, we introduce Mind-Brush, a unified agentic framework that transforms generation into a dynamic, knowledge-driven workflow. Simulating a human-like 'think-research-create' paradigm, Mind-Brush actively retrieves multimodal evidence to ground out-of-distribution concepts and employs reasoning tools to resolve implicit visual constraints. To rigorously evaluate these capabilities, we propose Mind-Bench, a comprehensive benchmark comprising 500 distinct samples spanning real-time news, emerging concepts, and domains such as mathematical and Geo-Reasoning. Extensive experiments demonstrate that Mind-Brush significantly enhances the capabilities of unified models, realizing a zero-to-one capability leap for the Qwen-Image baseline on Mind-Bench, while achieving superior results on established benchmarks like WISE and RISE.", 'score': 21, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': '296ad3c0ea5b092d', 'authors': ['Jun He', 'Junyan Ye', 'Zilong Huang', 'Dongzhi Jiang', 'Chenjue Zhang', 'Leqi Zhu', 'Renrui Zhang', 'Xiang Zhang', 'Weijia Li'], 'affiliations': ['MMLab, The Chinese University of Hong Kong', 'Shanghai Artificial Intelligence Laboratory', 'Sun Yat-sen University', 'Tsinghua Shenzhen International Graduate School, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2602.01756.jpg', 'data': {'categories': ['#multimodal', '#agents', '#benchmark', '#rag', '#reasoning'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹', 'desc': "Mind-Brush Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ 'Ğ´ÑƒĞ¼Ğ°Ğ¹-Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞ¹-ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ğ¹', Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²ĞµĞ´Ğ»Ğ¸ Mind-Bench - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ· 500 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¿Ğ¾ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸, Ğ½Ğ¾Ğ²Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ³ĞµĞ¾Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞºĞ°Ñ‡ĞºĞ° Ğ½Ğ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen-Image."}, 'en': {'title': 'Mind-Brush: Dynamic Understanding for Text-to-Image Generation', 'desc': "Mind-Brush is a new framework for generating images from text that improves how AI understands what users really want. Unlike traditional models that just convert text to images, Mind-Brush actively gathers information from various sources to better interpret complex ideas. It uses a 'think-research-create' approach, allowing it to adapt to new information and solve intricate reasoning tasks. The framework has been tested with a new benchmark called Mind-Bench, showing significant improvements in performance over existing models."}, 'zh': {'title': 'Mind-Brushï¼šåŠ¨æ€çŸ¥è¯†é©±åŠ¨çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶', 'desc': 'Mind-Brushæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ™ºèƒ½æ¡†æ¶ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆï¼Œèƒ½å¤ŸåŠ¨æ€æ£€ç´¢å¤šæ¨¡æ€è¯æ®å¹¶è¿ç”¨æ¨ç†å·¥å…·ï¼Œä»¥æé«˜å¯¹ç”¨æˆ·éšå«æ„å›¾å’Œå¤æ‚çŸ¥è¯†æ¨ç†çš„ç†è§£ã€‚ç°æœ‰çš„å¤§å¤šæ•°æ¨¡å‹ä½œä¸ºé™æ€æ–‡æœ¬åˆ°åƒç´ çš„è§£ç å™¨ï¼Œæ— æ³•æœ‰æ•ˆæŠŠæ¡ç”¨æˆ·çš„éšå«æ„å›¾ã€‚å°½ç®¡æ–°å…´çš„ç»Ÿä¸€ç†è§£ç”Ÿæˆæ¨¡å‹æœ‰æ‰€æ”¹å–„ï¼Œä½†åœ¨å¤„ç†å¤æ‚çŸ¥è¯†æ¨ç†æ—¶ä»ç„¶å­˜åœ¨å›°éš¾ã€‚Mind-Brushé€šè¿‡æ¨¡æ‹Ÿäººç±»çš„â€œæ€è€ƒ-ç ”ç©¶-åˆ›é€ â€è¿‡ç¨‹ï¼Œè½¬å˜ä¸ºä¸€ä¸ªåŠ¨æ€çš„ã€çŸ¥è¯†é©±åŠ¨çš„å·¥ä½œæµç¨‹ï¼Œæ˜¾è‘—æå‡äº†ç»Ÿä¸€æ¨¡å‹çš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.02214', 'title': 'Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation', 'url': 'https://huggingface.co/papers/2602.02214', 'abstract': "A novel Causal Forcing method addresses the architectural gap in distilling bidirectional video diffusion models into autoregressive models by using AR teachers for ODE initialization, significantly improving video generation performance.  \t\t\t\t\tAI-generated summary \t\t\t\t To achieve real-time interactive video generation, current methods distill pretrained bidirectional video diffusion models into few-step autoregressive (AR) models, facing an architectural gap when full attention is replaced by causal attention. However, existing approaches do not bridge this gap theoretically. They initialize the AR student via ODE distillation, which requires frame-level injectivity, where each noisy frame must map to a unique clean frame under the PF-ODE of an AR teacher. Distilling an AR student from a bidirectional teacher violates this condition, preventing recovery of the teacher's flow map and instead inducing a conditional-expectation solution, which degrades performance. To address this issue, we propose Causal Forcing that uses an AR teacher for ODE initialization, thereby bridging the architectural gap. Empirical results show that our method outperforms all baselines across all metrics, surpassing the SOTA Self Forcing by 19.3\\% in Dynamic Degree, 8.7\\% in VisionReward, and 16.7\\% in Instruction Following. Project page and the code: https://thu-ml.github.io/CausalForcing.github.io/{https://thu-ml.github.io/CausalForcing.github.io/}", 'score': 19, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': 'ffbfd173cf4147bb', 'authors': ['Hongzhou Zhu', 'Min Zhao', 'Guande He', 'Hang Su', 'Chongxuan Li', 'Jun Zhu'], 'affiliations': ['Beijing Key Laboratory of Research on Large Models and Intelligent Governance', 'Dept. of Comp. Sci. & Tech., BNRist Center, THU-Bosch ML Center, Tsinghua University', 'Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE', 'Gaoling School of Artificial Intelligence Renmin University of China Beijing, China', 'Pazhou Laboratory (Huangpu)', 'ShengShu', 'The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2602.02214.jpg', 'data': {'categories': ['#architecture', '#video', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Causal Forcing Ğ´Ğ»Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¼ĞµĞ½Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ODE Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ½Ğ°Ñ€ÑƒÑˆĞ°ĞµÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğµ Ğ¸Ğ½ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ODE, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 19.3% Ğ¿Ğ¾ Dynamic Degree, 8.7% Ğ¿Ğ¾ VisionReward Ğ¸ 16.7% Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ Instruction Following.'}, 'en': {'title': 'Bridging the Gap in Video Generation with Causal Forcing', 'desc': 'This paper introduces a new method called Causal Forcing, which improves the process of converting bidirectional video diffusion models into autoregressive (AR) models for better video generation. The challenge arises from the architectural differences when switching from full attention to causal attention, which existing methods fail to address theoretically. The authors highlight that previous approaches struggle with ODE initialization due to the requirement of frame-level injectivity, leading to performance degradation. By utilizing an AR teacher for ODE initialization, Causal Forcing effectively bridges this gap, resulting in significant performance improvements over existing methods.'}, 'zh': {'title': 'å› æœå¼ºåˆ¶ï¼šæå‡è§†é¢‘ç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å› æœå¼ºåˆ¶æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å°†åŒå‘è§†é¢‘æ‰©æ•£æ¨¡å‹è’¸é¦ä¸ºè‡ªå›å½’æ¨¡å‹æ—¶çš„æ¶æ„å·®è·ã€‚è¯¥æ–¹æ³•é€šè¿‡ä½¿ç”¨è‡ªå›å½’æ•™å¸ˆè¿›è¡Œå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰åˆå§‹åŒ–ï¼Œæ˜¾è‘—æé«˜äº†è§†é¢‘ç”Ÿæˆæ€§èƒ½ã€‚ä¼ ç»Ÿæ–¹æ³•åœ¨å°†å…¨æ³¨æ„åŠ›æ›¿æ¢ä¸ºå› æœæ³¨æ„åŠ›æ—¶æœªèƒ½ç†è®ºä¸Šå¼¥è¡¥è¿™ä¸€å·®è·ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå› æœå¼ºåˆ¶æ–¹æ³•åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡ä¼˜äºåŸºçº¿ï¼Œå°¤å…¶åœ¨åŠ¨æ€åº¦ã€è§†è§‰å¥–åŠ±å’ŒæŒ‡ä»¤è·Ÿéšæ–¹é¢åˆ†åˆ«æé«˜äº†19.3%ã€8.7%å’Œ16.7%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01395', 'title': 'Rethinking Selective Knowledge Distillation', 'url': 'https://huggingface.co/papers/2602.01395', 'abstract': 'Selective knowledge distillation in autoregressive language models using student-entropy-guided position selection improves accuracy and efficiency while reducing memory and storage requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t Growing efforts to improve knowledge distillation (KD) in large language models (LLMs) replace dense teacher supervision with selective distillation, which uses a subset of token positions, vocabulary classes, or training samples for supervision. However, it remains unclear which importance signals, selection policies, and their interplay are most effective. In this work, we revisit where and how to distill in autoregressive LLMs. We disentangle selective KD along the position, class, and sample axes and systematically compare importance signals and selection policies. Then, guided by this analysis, we identify underexplored opportunities and introduce student-entropy-guided position selection (SE-KD). Across a suite of benchmarks, SE-KD often improves accuracy, downstream task adherence, and memory efficiency over dense distillation. Extending this approach across the class and sample axes (SE-KD 3X) yields complementary efficiency gains that make offline teacher caching feasible. In practice, this reduces wall time by 70% and peak memory by 18%, while cutting storage usage by 80% over prior methods without sacrificing performance.', 'score': 18, 'issue_id': 884, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 1', 'zh': '2æœˆ1æ—¥'}, 'hash': '4ef0dc85c3edbbd9', 'authors': ['Almog Tavor', 'Itay Ebenspanger', 'Neil Cnaan', 'Mor Geva'], 'affiliations': ['Blavatnik School of Computer Science and AI, Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2602.01395.jpg', 'data': {'categories': ['#inference', '#optimization', '#training', '#small_models'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ: Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ (knowledge distillation) Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ SE-KD, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ñ‚ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ²Ğ´Ğ¾Ğ»ÑŒ Ğ¾ÑĞµĞ¹ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹, ĞºĞ»Ğ°ÑÑĞ¾Ğ² Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ SE-KD Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸ĞµĞ¹, Ğ° Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ SE-KD 3X Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ¸Ğ³Ñ€Ñ‹Ñˆ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° 70%, Ğ¿Ğ¸ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° 18% Ğ¸ Ğ¾Ğ±ÑŠÑ‘Ğ¼ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ° Ğ½Ğ° 80% Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Efficient Knowledge Distillation with Selective Positioning', 'desc': "This paper presents a method called student-entropy-guided position selection (SE-KD) for improving knowledge distillation in autoregressive language models. The authors explore how to selectively distill knowledge by focusing on specific token positions, vocabulary classes, and training samples, rather than using all available data. Their approach shows that by carefully choosing which parts of the model to distill, they can enhance accuracy and efficiency while significantly reducing memory and storage needs. The results demonstrate that SE-KD can lead to faster processing times and lower resource consumption without compromising the model's performance."}, 'zh': {'title': 'é€‰æ‹©æ€§çŸ¥è¯†è’¸é¦ï¼šæå‡æ•ˆç‡ä¸å‡†ç¡®æ€§', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åœ¨è‡ªå›å½’è¯­è¨€æ¨¡å‹ä¸­è¿›è¡Œé€‰æ‹©æ€§çŸ¥è¯†è’¸é¦çš„æ–¹æ³•ã€‚é€šè¿‡é€‰æ‹©æ€§è’¸é¦ï¼Œç ”ç©¶è€…ä½¿ç”¨éƒ¨åˆ†æ ‡è®°ä½ç½®ã€è¯æ±‡ç±»åˆ«æˆ–è®­ç»ƒæ ·æœ¬æ¥æ›¿ä»£å¯†é›†çš„æ•™å¸ˆç›‘ç£ï¼Œä»è€Œæé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„é€‰æ‹©ç­–ç•¥ï¼Œç§°ä¸ºå­¦ç”Ÿç†µå¼•å¯¼çš„ä½ç½®é€‰æ‹©ï¼ˆSE-KDï¼‰ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚ç»“æœæ˜¾ç¤ºï¼ŒSE-KDåœ¨å‡å°‘å†…å­˜å’Œå­˜å‚¨éœ€æ±‚çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01058', 'title': 'Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning', 'url': 'https://huggingface.co/papers/2602.01058', 'abstract': 'Post-training of reasoning large language models can be improved by correcting distribution mismatches between supervised fine-tuning and reinforcement learning stages through importance sampling reweighting of the SFT loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Post-training of reasoning LLMs is a holistic process that typically consists of an offline SFT stage followed by an online reinforcement learning (RL) stage. However, SFT is often optimized in isolation to maximize SFT performance alone.   We show that, after identical RL training, models initialized from stronger SFT checkpoints can significantly underperform those initialized from weaker ones. We attribute this to a mismatch typical in current SFT-RL pipelines: the distribution that generates the offline SFT data can differ substantially from the policy optimized during online RL, which learns from its own rollouts.   We propose PEAR (Policy Evaluation-inspired Algorithm for Offline Learning Loss Re-weighting), an SFT-stage method that corrects this mismatch and better prepares the model for RL. PEAR uses importance sampling to reweight the SFT loss, with three variants operating at the token, block, and sequence levels. It can be used to augment standard SFT objectives and incurs little additional training overhead once probabilities for the offline data are collected.   We conduct controlled experiments on verifiable reasoning games and mathematical reasoning tasks on Qwen 2.5 and 3 and DeepSeek-distilled models. PEAR consistently improves post-RL performance over canonical SFT, with pass at 8 gains up to a 14.6 percent on AIME2025. Our results suggest that PEAR is an effective step toward more holistic LLM post-training by designing and evaluating SFT with downstream RL in mind rather than in isolation.', 'score': 17, 'issue_id': 884, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 1', 'zh': '2æœˆ1æ—¥'}, 'hash': 'f8ee0b48de46d590', 'authors': ['Dylan Zhang', 'Yufeng Xu', 'Haojin Wang', 'Qingzhi Chen', 'Hao Peng'], 'affiliations': ['New York University (Shanghai)', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2602.01058.jpg', 'data': {'categories': ['#math', '#reasoning', '#optimization', '#rl', '#training'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ğ¸ SFT Ğ¸ RL Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ PEAR Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğ°Ñ… supervised fine-tuning Ğ¸ reinforcement learning. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ SFT Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ¹, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ importance sampling Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ²ĞµÌÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ†ĞµĞ»Ğ¸ĞºĞ¾Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 14,6% Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ pass@8 Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ AIME2025.'}, 'en': {'title': 'Bridging the Gap: Enhancing LLMs with PEAR for Better Post-Training Performance', 'desc': 'This paper discusses how to improve the post-training of reasoning large language models (LLMs) by addressing the differences between supervised fine-tuning (SFT) and reinforcement learning (RL) stages. The authors introduce a method called PEAR, which uses importance sampling to adjust the SFT loss, ensuring that the model is better prepared for the RL phase. They demonstrate that models trained with PEAR show significant performance improvements in reasoning tasks compared to those trained with traditional SFT methods. The findings highlight the importance of aligning SFT and RL processes to enhance overall model effectiveness.'}, 'zh': {'title': 'ä¼˜åŒ–åè®­ç»ƒï¼Œæå‡æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åè®­ç»ƒè¿‡ç¨‹ä¸­çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é˜¶æ®µä¹‹é—´çš„åˆ†å¸ƒä¸åŒ¹é…ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ¥è‡ªå¼ºSFTæ£€æŸ¥ç‚¹çš„æ¨¡å‹åœ¨ç»è¿‡ç›¸åŒçš„RLè®­ç»ƒåï¼Œè¡¨ç°å¯èƒ½ä¸å¦‚æ¥è‡ªå¼±SFTæ£€æŸ¥ç‚¹çš„æ¨¡å‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºPEARçš„ç®—æ³•ï¼Œé€šè¿‡é‡è¦æ€§é‡‡æ ·å¯¹SFTæŸå¤±è¿›è¡Œé‡åŠ æƒï¼Œä»è€Œæ›´å¥½åœ°ä¸ºRLé˜¶æ®µåšå‡†å¤‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPEARåœ¨å¤šä¸ªæ¨ç†ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„åRLæ€§èƒ½ï¼Œè¡¨æ˜åœ¨è®¾è®¡SFTæ—¶è€ƒè™‘ä¸‹æ¸¸RLçš„å½±å“æ˜¯æœ‰æ•ˆçš„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01801', 'title': 'Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention', 'url': 'https://huggingface.co/papers/2602.01801', 'abstract': 'Autoregressive video diffusion models face efficiency challenges due to growing KV caches and redundant attention computations, which are addressed through TempCache, AnnCA, and AnnSA techniques that reduce computational demands while maintaining visual quality and stable performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive video diffusion models enable streaming generation, opening the door to long-form synthesis, video world models, and interactive neural game engines. However, their core attention layers become a major bottleneck at inference time: as generation progresses, the KV cache grows, causing both increasing latency and escalating GPU memory, which in turn restricts usable temporal context and harms long-range consistency. In this work, we study redundancy in autoregressive video diffusion and identify three persistent sources: near-duplicate cached keys across frames, slowly evolving (largely semantic) queries/keys that make many attention computations redundant, and cross-attention over long prompts where only a small subset of tokens matters per frame. Building on these observations, we propose a unified, training-free attention framework for autoregressive diffusion: TempCache compresses the KV cache via temporal correspondence to bound cache growth; AnnCA accelerates cross-attention by selecting frame-relevant prompt tokens using fast approximate nearest neighbor (ANN) matching; and AnnSA sparsifies self-attention by restricting each query to semantically matched keys, also using a lightweight ANN. Together, these modules reduce attention, compute, and memory and are compatible with existing autoregressive diffusion backbones and world models. Experiments demonstrate up to x5--x10 end-to-end speedups while preserving near-identical visual quality and, crucially, maintaining stable throughput and nearly constant peak GPU memory usage over long rollouts, where prior methods progressively slow down and suffer from increasing memory usage.', 'score': 16, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': 'bf9e6454eeb13993', 'authors': ['Dvir Samuel', 'Issar Tzachor', 'Matan Levy', 'Micahel Green', 'Gal Chechik', 'Rami Ben-Ari'], 'affiliations': ['Bar-Ilan University, Ramat-Gan, Israel', 'NVIDIA, Tel-Aviv, Israel', 'OriginAI, Tel-Aviv, Israel', 'The Hebrew University of Jerusalem, Jerusalem, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2602.01801.jpg', 'data': {'categories': ['#video', '#inference', '#diffusion', '#optimization', '#long_context'], 'emoji': 'âš¡', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ ĞºĞµÑˆĞ° Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ñ‚Ñ€Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: TempCache ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ KV-ĞºĞµÑˆ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹, AnnCA ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞ¸Ñ… ÑĞ¾ÑĞµĞ´ĞµĞ¹, Ğ¸ AnnSA Ñ€Ğ°Ğ·Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ»ÑÑ‡Ğ°Ğ¼. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² 5-10 Ñ€Ğ°Ğ· Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Boosting Efficiency in Video Diffusion Models', 'desc': 'This paper addresses the efficiency issues in autoregressive video diffusion models, which are crucial for generating long-form videos. The authors introduce three techniques: TempCache, AnnCA, and AnnSA, which optimize the attention mechanisms to reduce computational load while preserving visual quality. TempCache minimizes the growth of the key-value (KV) cache, AnnCA speeds up cross-attention by selecting relevant tokens, and AnnSA limits self-attention to semantically relevant keys. Together, these innovations lead to significant speed improvements and stable performance during video generation, making the models more practical for real-time applications.'}, 'zh': {'title': 'æå‡è‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹æ•ˆç‡çš„åˆ›æ–°æŠ€æœ¯', 'desc': 'è‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨æ•ˆç‡ä¸Šé¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºKVç¼“å­˜çš„å¢é•¿å’Œå†—ä½™çš„æ³¨æ„åŠ›è®¡ç®—ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†TempCacheã€AnnCAå’ŒAnnSAæŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯åœ¨ä¿æŒè§†è§‰è´¨é‡å’Œç¨³å®šæ€§èƒ½çš„åŒæ—¶ï¼Œå‡å°‘äº†è®¡ç®—éœ€æ±‚ã€‚TempCacheé€šè¿‡æ—¶é—´å¯¹åº”å‹ç¼©KVç¼“å­˜ï¼ŒAnnCAé€šè¿‡å¿«é€Ÿè¿‘ä¼¼æœ€è¿‘é‚»åŒ¹é…åŠ é€Ÿè·¨æ³¨æ„åŠ›ï¼ŒAnnSAåˆ™é€šè¿‡é™åˆ¶æŸ¥è¯¢åˆ°è¯­ä¹‰åŒ¹é…çš„é”®æ¥ç¨€ç–è‡ªæ³¨æ„åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ–¹æ³•åœ¨ä¿æŒå‡ ä¹ç›¸åŒçš„è§†è§‰è´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¾¾5åˆ°10å€çš„ç«¯åˆ°ç«¯åŠ é€Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.02092', 'title': 'FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space', 'url': 'https://huggingface.co/papers/2602.02092', 'abstract': 'FSVideo is a fast transformer-based image-to-video diffusion framework that uses a compressed video autoencoder, diffusion transformer architecture with enhanced layer memory, and multi-resolution generation strategy to achieve high performance with significantly reduced computation time.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce FSVideo, a fast speed transformer-based image-to-video (I2V) diffusion framework. We build our framework on the following key components: 1.) a new video autoencoder with highly-compressed latent space (64times64times4 spatial-temporal downsampling ratio), achieving competitive reconstruction quality; 2.) a diffusion transformer (DIT) architecture with a new layer memory design to enhance inter-layer information flow and context reuse within DIT, and 3.) a multi-resolution generation strategy via a few-step DIT upsampler to increase video fidelity. Our final model, which contains a 14B DIT base model and a 14B DIT upsampler, achieves competitive performance against other popular open-source models, while being an order of magnitude faster. We discuss our model design as well as training strategies in this report.', 'score': 15, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': '0632387ea043193b', 'authors': ['FSVideo Team', 'Qingyu Chen', 'Zhiyuan Fang', 'Haibin Huang', 'Xinwei Huang', 'Tong Jin', 'Minxuan Lin', 'Bo Liu', 'Celong Liu', 'Chongyang Ma', 'Xing Mei', 'Xiaohui Shen', 'Yaojie Shen', 'Fuwen Tan', 'Angtian Wang', 'Xiao Yang', 'Yiding Yang', 'Jiamin Yuan', 'Lingxi Zhang', 'Yuxin Zhang'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2602.02092.jpg', 'data': {'categories': ['#architecture', '#video', '#diffusion', '#optimization', '#open_source', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞŸĞ¾Ñ€ÑĞ´Ğ¾Ğº Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ: Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'FSVideo Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒÑ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ÑĞ»Ğ¾Ñ‘Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ñƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾ÑĞ¼Ğ¸ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ¿ÑÑĞ¼Ğ¿Ğ»ĞµÑ€Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸.'}, 'en': {'title': 'FSVideo: Fast and Efficient Image-to-Video Transformation', 'desc': 'FSVideo is a novel framework that transforms images into videos using a fast transformer-based approach. It incorporates a compressed video autoencoder that significantly reduces the data size while maintaining high-quality video reconstruction. The diffusion transformer architecture features an enhanced layer memory, which improves the flow of information between layers, allowing for better context reuse. Additionally, a multi-resolution generation strategy is employed to boost the fidelity of the generated videos, making FSVideo both efficient and effective compared to existing models.'}, 'zh': {'title': 'FSVideoï¼šå¿«é€Ÿé«˜æ•ˆçš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¡†æ¶', 'desc': 'FSVideoæ˜¯ä¸€ä¸ªåŸºäºå¿«é€Ÿå˜æ¢å™¨çš„å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¡†æ¶ã€‚å®ƒé‡‡ç”¨äº†ä¸€ç§å‹ç¼©çš„è§†é¢‘è‡ªç¼–ç å™¨ï¼Œå…·æœ‰é«˜æ•ˆçš„å±‚å†…å­˜è®¾è®¡ï¼Œèƒ½å¤Ÿå¢å¼ºä¿¡æ¯æµåŠ¨å’Œä¸Šä¸‹æ–‡é‡ç”¨ã€‚è¯¥æ¡†æ¶è¿˜ä½¿ç”¨äº†å¤šåˆ†è¾¨ç‡ç”Ÿæˆç­–ç•¥ï¼Œé€šè¿‡å°‘é‡æ­¥éª¤çš„ä¸Šé‡‡æ ·å™¨æé«˜è§†é¢‘çš„æ¸…æ™°åº¦ã€‚æœ€ç»ˆæ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¸å…¶ä»–æµè¡Œçš„å¼€æºæ¨¡å‹ç›¸å½“ï¼Œä½†è®¡ç®—é€Ÿåº¦å¿«äº†ä¸€ä¸ªæ•°é‡çº§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01541', 'title': 'Toward Cognitive Supersensing in Multimodal Large Language Model', 'url': 'https://huggingface.co/papers/2602.01541', 'abstract': 'MLLMs equipped with Cognitive Supersensing and Latent Visual Imagery Prediction demonstrate enhanced cognitive reasoning capabilities through integrated visual and textual reasoning pathways.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.', 'score': 14, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': '632e10bd4637e8b2', 'authors': ['Boyi Li', 'Yifan Shen', 'Yuanzhe Liu', 'Yifan Xu', 'Jiateng Liu', 'Xinzhuo Li', 'Zhengyuan Li', 'Jingyuan Zhu', 'Yunhan Zhong', 'Fangzhou Lan', 'Jianguo Cao', 'James M. Rehg', 'Heng Ji', 'Ismini Lourentzou', 'Xu Cao'], 'affiliations': ['PediaMed AI', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2602.01541.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#benchmark', '#reasoning', '#rl', '#open_source', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ’Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ¾ÑĞ½Ğ¾Ğ²Ğ° ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Cognitive Supersensing Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ² (LVIP). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ±ÑƒÑ„ĞµÑ€Ñƒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CogSense-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿ÑÑ‚Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… VQA Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ‹ ÑĞ²Ğ»ÑÑÑ‚ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸ĞµĞ¼ Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Enhancing MLLMs with Visual Imagery for Better Reasoning', 'desc': "This paper presents a new approach called Cognitive Supersensing for Multimodal Large Language Models (MLLMs) to enhance their cognitive reasoning abilities. It introduces a Latent Visual Imagery Prediction (LVIP) mechanism that allows these models to integrate visual and textual reasoning, mimicking human-like visual memory. The authors also propose a reinforcement learning stage to optimize reasoning paths based on visual information, improving the model's performance on complex cognitive tasks. The results show that MLLMs using this method significantly outperform existing models on a new benchmark called CogSense-Bench, indicating the importance of visual imagery in cognitive understanding."}, 'zh': {'title': 'è®¤çŸ¥è¶…æ„ŸçŸ¥ï¼šæå‡å¤šæ¨¡æ€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå®ƒé€šè¿‡è®¤çŸ¥è¶…æ„ŸçŸ¥å’Œæ½œåœ¨è§†è§‰å›¾åƒé¢„æµ‹æ¥å¢å¼ºè®¤çŸ¥æ¨ç†èƒ½åŠ›ã€‚è¿™ç§æ¨¡å‹ç»“åˆäº†è§†è§‰å’Œæ–‡æœ¬æ¨ç†è·¯å¾„ï¼Œä½¿å…¶åœ¨å¤„ç†å¤æ‚çš„è®¤çŸ¥é—®é¢˜æ—¶è¡¨ç°æ›´å¥½ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦è§†è§‰è®°å¿†çš„æƒ…å†µä¸‹ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒèŒƒå¼ï¼Œèƒ½å¤Ÿè®©MLLMå…·å¤‡ç±»ä¼¼äººç±»çš„è§†è§‰å›¾åƒèƒ½åŠ›ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ–‡æœ¬æ¨ç†è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è®¤çŸ¥è¶…æ„ŸçŸ¥è®­ç»ƒçš„MLLMåœ¨è§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºå†…éƒ¨è§†è§‰å›¾åƒåœ¨æ„ŸçŸ¥è¯†åˆ«ä¸è®¤çŸ¥ç†è§£ä¹‹é—´çš„æ¡¥æ¢ä½œç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01335', 'title': 'Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning', 'url': 'https://huggingface.co/papers/2602.01335', 'abstract': 'Visual metaphor transfer enables creative AI systems to decompose abstract conceptual relationships from reference images and reapply them to new subjects through a multi-agent framework grounded in cognitive theory.  \t\t\t\t\tAI-generated summary \t\t\t\t A visual metaphor constitutes a high-order form of human creativity, employing cross-domain semantic fusion to transform abstract concepts into impactful visual rhetoric. Despite the remarkable progress of generative AI, existing models remain largely confined to pixel-level instruction alignment and surface-level appearance preservation, failing to capture the underlying abstract logic necessary for genuine metaphorical generation. To bridge this gap, we introduce the task of Visual Metaphor Transfer (VMT), which challenges models to autonomously decouple the "creative essence" from a reference image and re-materialize that abstract logic onto a user-specified target subject. We propose a cognitive-inspired, multi-agent framework that operationalizes Conceptual Blending Theory (CBT) through a novel Schema Grammar ("G"). This structured representation decouples relational invariants from specific visual entities, providing a rigorous foundation for cross-domain logic re-instantiation. Our pipeline executes VMT through a collaborative system of specialized agents: a perception agent that distills the reference into a schema, a transfer agent that maintains generic space invariance to discover apt carriers, a generation agent for high-fidelity synthesis and a hierarchical diagnostic agent that mimics a professional critic, performing closed-loop backtracking to identify and rectify errors across abstract logic, component selection, and prompt encoding. Extensive experiments and human evaluations demonstrate that our method significantly outperforms SOTA baselines in metaphor consistency, analogy appropriateness, and visual creativity, paving the way for automated high-impact creative applications in advertising and media. Source code will be made publicly available.', 'score': 14, 'issue_id': 884, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 1', 'zh': '2æœˆ1æ—¥'}, 'hash': '92808513aa27eea4', 'authors': ['Yu Xu', 'Yuxin Zhang', 'Juan Cao', 'Lin Gao', 'Chunyu Wang', 'Oliver Deussen', 'Tong-Yee Lee', 'Fan Tang'], 'affiliations': ['National Cheng-Kung University', 'Tencent Hunyuan', 'University of Chinese Academy of Sciences', 'University of Konstanz'], 'pdf_title_img': 'assets/pdf/title_img/2602.01335.jpg', 'data': {'categories': ['#open_source'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¢Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ°Ñ„Ğ¾Ñ€ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Visual Metaphor Transfer, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞµ ĞµÑ‘ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Conceptual Blending, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Schema Grammar Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ°, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ°Ñ„Ğ¾Ñ€Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ°Ñ„Ğ¾Ñ€ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞºĞ»Ğ°Ğ¼Ğµ Ğ¸ Ğ¼ĞµĞ´Ğ¸Ğ°.'}, 'en': {'title': 'Transforming Abstract Concepts into Visual Metaphors with AI', 'desc': 'This paper introduces Visual Metaphor Transfer (VMT), a novel approach that allows AI to creatively reinterpret abstract concepts from reference images and apply them to new subjects. The authors propose a multi-agent framework inspired by cognitive theory, specifically Conceptual Blending Theory, to facilitate this process. By using a structured Schema Grammar, the framework separates the essential creative elements from specific visuals, enabling the AI to generate meaningful metaphors. The results show that this method significantly improves metaphor consistency and visual creativity compared to existing models, suggesting its potential for applications in advertising and media.'}, 'zh': {'title': 'è§†è§‰éšå–»è½¬ç§»ï¼šAIåˆ›æ„çš„æ–°å¢ƒç•Œ', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§è§†è§‰éšå–»è½¬ç§»ï¼ˆVMTï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨è®©AIæ¨¡å‹ä»å‚è€ƒå›¾åƒä¸­æå–åˆ›æ„æœ¬è´¨ï¼Œå¹¶å°†å…¶åº”ç”¨äºæ–°çš„ç›®æ ‡å¯¹è±¡ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼ŒåŸºäºè®¤çŸ¥ç†è®ºï¼Œé€šè¿‡æ¦‚å¿µèåˆç†è®ºï¼ˆCBTï¼‰å®ç°æŠ½è±¡é€»è¾‘çš„é‡æ–°æ„å»ºã€‚è¯¥æ¡†æ¶åŒ…æ‹¬å¤šä¸ªä¸“é—¨çš„ä»£ç†ï¼Œåˆ†åˆ«è´Ÿè´£æ„ŸçŸ¥ã€è½¬ç§»ã€ç”Ÿæˆå’Œè¯Šæ–­ï¼Œç¡®ä¿ç”Ÿæˆçš„éšå–»åœ¨é€»è¾‘å’Œè§†è§‰ä¸Šéƒ½å…·æœ‰ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨éšå–»ä¸€è‡´æ€§ã€ç±»æ¯”é€‚ç”¨æ€§å’Œè§†è§‰åˆ›æ„æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01851', 'title': 'How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing', 'url': 'https://huggingface.co/papers/2602.01851', 'abstract': 'Visual Instruction Benchmark for Image Editing introduces a three-level interaction hierarchy for evaluating visual instruction following capabilities in generative models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent generative models have achieved remarkable progress in image editing. However, existing systems and benchmarks remain largely text-guided. In contrast, human communication is inherently multimodal, where visual instructions such as sketches efficiently convey spatial and structural intent. To address this gap, we introduce VIBE, the Visual Instruction Benchmark for Image Editing with a three-level interaction hierarchy that captures deictic grounding, morphological manipulation, and causal reasoning. Across these levels, we curate high-quality and diverse test cases that reflect progressively increasing complexity in visual instruction following. We further propose a robust LMM-as-a-judge evaluation framework with task-specific metrics to enable scalable and fine-grained assessment. Through a comprehensive evaluation of 17 representative open-source and proprietary image editing models, we find that proprietary models exhibit early-stage visual instruction-following capabilities and consistently outperform open-source models. However, performance degrades markedly with increasing task difficulty even for the strongest systems, highlighting promising directions for future research.', 'score': 13, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': 'e226f48f4fab2c28', 'authors': ['Huanyu Zhang', 'Xuehai Bai', 'Chengzu Li', 'Chen Liang', 'Haochen Tian', 'Haodong Li', 'Ruichuan An', 'Yifan Zhang', 'Anna Korhonen', 'Zhang Zhang', 'Liang Wang', 'Tieniu Tan'], 'affiliations': ['Hangzhou Dianzi University', 'Institute of Automation, Chinese Academy of Sciences', 'Language Technology Lab, University of Cambridge', 'School of Artificial Intelligence, University of Chinese Academy of Science'], 'pdf_title_img': 'assets/pdf/title_img/2602.01851.jpg', 'data': {'categories': ['#multimodal', '#cv', '#benchmark'], 'emoji': 'âœï¸', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VIBE â€” Visual Instruction Benchmark for Image Editing, Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¿Ñ€Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹, VIBE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ñ‘Ñ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ: Ğ´ĞµĞ¹ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ, Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 17 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ, Ğ½Ğ¾ Ğ²ÑĞµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Enhancing Image Editing with Visual Instructions', 'desc': 'The paper presents the Visual Instruction Benchmark for Image Editing (VIBE), which introduces a structured way to evaluate how well generative models can follow visual instructions. It emphasizes the importance of multimodal communication, as humans often use visual cues like sketches to convey complex ideas. VIBE features a three-level interaction hierarchy that includes deictic grounding, morphological manipulation, and causal reasoning, allowing for a nuanced assessment of model capabilities. The study evaluates various image editing models, revealing that while proprietary models perform better overall, they struggle with more complex tasks, indicating areas for future improvement.'}, 'zh': {'title': 'è§†è§‰æŒ‡ä»¤åŸºå‡†ï¼šæå‡å›¾åƒç¼–è¾‘çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†è§†è§‰æŒ‡ä»¤åŸºå‡†ï¼ˆVIBEï¼‰ï¼Œç”¨äºè¯„ä¼°ç”Ÿæˆæ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä¸­çš„è§†è§‰æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚è¯¥åŸºå‡†å»ºç«‹äº†ä¸€ä¸ªä¸‰å±‚æ¬¡çš„äº¤äº’å±‚æ¬¡ç»“æ„ï¼Œæ¶µç›–æŒ‡ç¤ºæ€§åŸºç¡€ã€å½¢æ€æ“ä½œå’Œå› æœæ¨ç†ã€‚é€šè¿‡ç²¾å¿ƒç­–åˆ’çš„æµ‹è¯•æ¡ˆä¾‹ï¼ŒVIBE åæ˜ äº†è§†è§‰æŒ‡ä»¤è·Ÿéšçš„å¤æ‚æ€§é€æ­¥å¢åŠ ã€‚ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§åŸºäºä»»åŠ¡ç‰¹å®šæŒ‡æ ‡çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥ä¾¿å¯¹ä¸åŒå›¾åƒç¼–è¾‘æ¨¡å‹è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01538', 'title': 'Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars', 'url': 'https://huggingface.co/papers/2602.01538', 'abstract': 'A dual-stream framework called InteractAvatar is presented for generating talking avatars that can interact with objects in their environment, addressing challenges in grounded human-object interaction through decoupled perception and planning modules.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: https://interactavatar.github.io', 'score': 13, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': 'dd14419a0410c173', 'authors': ['Youliang Zhang', 'Zhengguang Zhou', 'Zhentao Yu', 'Ziyao Huang', 'Teng Hu', 'Sen Liang', 'Guozhen Zhang', 'Ziqiao Peng', 'Shunkai Li', 'Yi Chen', 'Zixiang Zhou', 'Yuan Zhou', 'Qinglin Lu', 'Xiu Li'], 'affiliations': ['Tencent HY', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2602.01538.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#video', '#games', '#benchmark'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ“Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ğµ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‚ Ñ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ°Ğ¼Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ´Ğ²ÑƒÑ…Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° InteractAvatar Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ² Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ ÑĞ°Ğ¼Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ—Ğ²ÑƒĞºĞ¾Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑƒĞ±ĞµĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº GroundedInter Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'InteractAvatar: Talking Avatars that Interact with Their World', 'desc': 'The paper introduces InteractAvatar, a dual-stream framework designed to create talking avatars that can interact with objects in their environment. It addresses the challenge of grounded human-object interaction (GHOI) by separating perception and planning from video synthesis. The framework includes a Perception and Interaction Module (PIM) for generating interaction motions based on environmental detection, and an Audio-Interaction Aware Generation Module (AIM) for synthesizing realistic talking avatars. By allowing parallel generation of motions and videos, InteractAvatar effectively improves the quality of interactions while establishing a benchmark for evaluating GHOI video generation.'}, 'zh': {'title': 'ç”Ÿæˆäº’åŠ¨ä¼šè¯´è¯çš„è™šæ‹Ÿå½¢è±¡æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºInteractAvatarçš„åŒæµæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆèƒ½å¤Ÿä¸ç¯å¢ƒä¸­ç‰©ä½“äº’åŠ¨çš„ä¼šè¯´è¯çš„è™šæ‹Ÿå½¢è±¡ã€‚è¯¥æ–¹æ³•é€šè¿‡è§£è€¦æ„ŸçŸ¥å’Œè§„åˆ’æ¨¡å—ï¼Œè§£å†³äº†åŸºäºæ–‡æœ¬çš„äººçš„ç‰©ä½“äº’åŠ¨ï¼ˆGHOIï¼‰ä¸­çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†æ„ŸçŸ¥ä¸äº’åŠ¨æ¨¡å—ï¼ˆPIMï¼‰æ¥å¢å¼ºç¯å¢ƒæ„ŸçŸ¥ï¼Œå¹¶ç”Ÿæˆä¸æ–‡æœ¬å¯¹é½çš„äº’åŠ¨åŠ¨ä½œã€‚åŒæ—¶ï¼ŒéŸ³é¢‘äº’åŠ¨æ„ŸçŸ¥ç”Ÿæˆæ¨¡å—ï¼ˆAIMï¼‰ç”¨äºåˆæˆç”ŸåŠ¨çš„ä¼šè¯´è¯çš„è™šæ‹Ÿå½¢è±¡ï¼Œæ‰§è¡Œç‰©ä½“äº’åŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆä¼šè¯´è¯çš„è™šæ‹Ÿå½¢è±¡ä¸ç‰©ä½“äº’åŠ¨æ–¹é¢å…·æœ‰æ˜¾è‘—æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.02486', 'title': 'RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents', 'url': 'https://huggingface.co/papers/2602.02486', 'abstract': 'Re-TRAC is an agentic framework that enhances LLM-based research agents by enabling cross-trajectory exploration and iterative reflection through structured state representations, leading to more efficient and effective problem-solving compared to traditional ReAct approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.', 'score': 11, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': '086cbae4c7c49747', 'authors': ['Jialiang Zhu', 'Gongrui Zhang', 'Xiaolong Ma', 'Lin Xu', 'Miaosen Zhang', 'Ruiqi Yang', 'Song Wang', 'Kai Qiu', 'Zhirong Wu', 'Qi Dai', 'Ruichun Ma', 'Bei Liu', 'Yifan Yang', 'Chong Luo', 'Zhengyuan Yang', 'Linjie Li', 'Lijuan Wang', 'Weizhu Chen', 'Xin Geng', 'Baining Guo'], 'affiliations': ['Brown University', 'Microsoft', 'Southeast University', 'Tsinghua University', 'Waseda University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2602.02486.jpg', 'data': {'categories': ['#long_context', '#optimization', '#reasoning'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ÑÑÑ‰Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚: Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ĞºÑ€Ğ¾ÑÑ-Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ½ÑƒÑ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ', 'desc': 'Re-TRAC â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆÑ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° ReAct, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¹Ñ‚Ğ¸ Ğ² Ñ€Ğ°Ğ½Ğ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ², Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ½ĞµÑƒĞ´Ğ°Ñ‡ Ğ¿Ğ¾ÑĞ»Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Re-TRAC Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ReAct Ğ½Ğ° 15-20% Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ° Ğ½Ğµ ĞºĞ°Ğº ÑĞ»ĞµĞ¿ÑƒÑ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ.'}, 'en': {'title': 'Re-TRAC: Enhancing Research Agents with Iterative Reflection and Exploration', 'desc': 'Re-TRAC is a new framework designed to improve the performance of large language model (LLM)-based research agents by allowing them to explore different paths and reflect on their findings. Unlike traditional ReAct methods, which follow a linear approach, Re-TRAC uses structured state representations to summarize past experiences and guide future actions. This method helps the agents avoid getting stuck in local optima and reduces unnecessary exploration, making the problem-solving process more efficient. Empirical results demonstrate that Re-TRAC outperforms ReAct by 15-20% and achieves better performance with smaller models through fine-tuning.'}, 'zh': {'title': 'Re-TRACï¼šæå‡ç ”ç©¶ä»£ç†çš„æ™ºèƒ½æ¢ç´¢ä¸åæ€', 'desc': 'Re-TRACæ˜¯ä¸€ä¸ªå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç ”ç©¶ä»£ç†çš„æ¡†æ¶ï¼Œé€šè¿‡ç»“æ„åŒ–çŠ¶æ€è¡¨ç¤ºå®ç°è·¨è½¨è¿¹æ¢ç´¢å’Œè¿­ä»£åæ€ï¼Œä»è€Œæé«˜é—®é¢˜è§£å†³çš„æ•ˆç‡å’Œæ•ˆæœã€‚ä¸ä¼ ç»Ÿçš„ReActæ–¹æ³•ç›¸æ¯”ï¼ŒRe-TRACèƒ½å¤Ÿåœ¨æ¯ä¸ªè½¨è¿¹åç”ŸæˆçŠ¶æ€è¡¨ç¤ºï¼Œæ±‡æ€»è¯æ®ã€ä¸ç¡®å®šæ€§ã€å¤±è´¥å’Œæœªæ¥è®¡åˆ’ï¼Œä½¿å¾—åç»­è½¨è¿¹èƒ½å¤ŸåŸºäºæ­¤çŠ¶æ€è¿›è¡Œè°ƒæ•´ã€‚è¯¥æ–¹æ³•ä½¿å¾—ç ”ç©¶è¿‡ç¨‹å˜å¾—æ›´åŠ æ¸è¿›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé¿å…å±€éƒ¨æœ€ä¼˜å’Œå†—ä½™æ¢ç´¢ã€‚å®éªŒè¯æ˜ï¼ŒRe-TRACåœ¨BrowseCompä¸Šæ¯”ReActçš„è¡¨ç°æé«˜äº†15-20%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01576', 'title': 'Generative Visual Code Mobile World Models', 'url': 'https://huggingface.co/papers/2602.01576', 'abstract': 'Visual world models for mobile GUI agents are improved through renderable code generation using vision-language models, achieving better performance with reduced model size compared to existing approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Mobile Graphical User Interface (GUI) World Models (WMs) offer a promising path for improving mobile GUI agent performance at train- and inference-time. However, current approaches face a critical trade-off: text-based WMs sacrifice visual fidelity, while the inability of visual WMs in precise text rendering led to their reliance on slow, complex pipelines dependent on numerous external models. We propose a novel paradigm: visual world modeling via renderable code generation, where a single Vision-Language Model (VLM) predicts the next GUI state as executable web code that renders to pixels, rather than generating pixels directly. This combines the strengths of both approaches: VLMs retain their linguistic priors for precise text rendering while their pre-training on structured web code enables high-fidelity visual generation. We introduce gWorld (8B, 32B), the first open-weight visual mobile GUI WMs built on this paradigm, along with a data generation framework (gWorld) that automatically synthesizes code-based training data. In extensive evaluation across 4 in- and 2 out-of-distribution benchmarks, gWorld sets a new pareto frontier in accuracy versus model size, outperforming 8 frontier open-weight models over 50.25x larger. Further analyses show that (1) scaling training data via gWorld yields meaningful gains, (2) each component of our pipeline improves data quality, and (3) stronger world modeling improves downstream mobile GUI policy performance.', 'score': 11, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': '3b3360675a7ef03e', 'authors': ['Woosung Koh', 'Sungjun Han', 'Segyu Lee', 'Se-Young Yun', 'Jamin Shin'], 'affiliations': ['KAIST AI', 'Trillion Labs'], 'pdf_title_img': 'assets/pdf/title_img/2602.01576.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#cv', '#agents', '#benchmark', '#small_models'], 'emoji': 'ğŸ“±', 'ru': {'title': 'ĞÑ‚ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğº ĞºĞ¾Ğ´Ñƒ: Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ²ĞµĞ±-ĞºĞ¾Ğ´Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ²ĞµĞ±-ĞºĞ¾Ğ´Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹, VLM Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ GUI Ğ² Ğ²Ğ¸Ğ´Ğµ ĞºĞ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ°Ñ‚ĞµĞ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ñ‚ÑÑ Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ gWorld â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 8B Ğ¸ 32B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğµ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing Mobile GUI Agents with Renderable Code Generation', 'desc': 'This paper presents a new approach to creating visual world models for mobile graphical user interface (GUI) agents by generating renderable code using vision-language models (VLMs). Unlike traditional methods that either compromise visual quality or rely on complex pipelines, this method allows for precise text rendering while maintaining high visual fidelity. The proposed system, called gWorld, generates executable web code that can be rendered into images, effectively combining the strengths of text-based and visual models. The results show that gWorld significantly improves performance while reducing model size, outperforming existing models in various benchmarks.'}, 'zh': {'title': 'é€šè¿‡å¯æ¸²æŸ“ä»£ç ç”Ÿæˆæå‡ç§»åŠ¨GUIæ€§èƒ½', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰ä¸–ç•Œå»ºæ¨¡æ–¹æ³•ï¼Œé€šè¿‡å¯æ¸²æŸ“ä»£ç ç”Ÿæˆæ¥æå‡ç§»åŠ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†çš„æ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç”Ÿæˆå¯æ‰§è¡Œçš„ç½‘é¡µä»£ç ï¼Œä»è€Œå®ç°æ›´é«˜çš„è§†è§‰ä¿çœŸåº¦å’Œæ›´å°çš„æ¨¡å‹å°ºå¯¸ã€‚æˆ‘ä»¬ä»‹ç»äº†gWorldï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäºè¿™ç§æ–°èŒƒå¼çš„å¼€æ”¾æƒé‡è§†è§‰ç§»åŠ¨GUIä¸–ç•Œæ¨¡å‹ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªè‡ªåŠ¨åˆæˆä»£ç è®­ç»ƒæ•°æ®çš„æ•°æ®ç”Ÿæˆæ¡†æ¶ã€‚é€šè¿‡åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å¹¿æ³›è¯„ä¼°ï¼ŒgWorldåœ¨å‡†ç¡®æ€§ä¸æ¨¡å‹å¤§å°ä¹‹é—´è®¾å®šäº†æ–°çš„å¸•ç´¯æ‰˜å‰æ²¿ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„å¤šä¸ªå¤§å‹æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.02472', 'title': 'SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning', 'url': 'https://huggingface.co/papers/2602.02472', 'abstract': 'SPARKLING is a framework for mid-stage width expansion in deep learning models that maintains signal preservation and breaks symmetry to stabilize training and reduce computational costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Progressive Learning (PL) reduces pre-training computational overhead by gradually increasing model scale. While prior work has extensively explored depth expansion, width expansion remains significantly understudied, with the few existing methods limited to the early stages of training. However, expanding width during the mid-stage is essential for maximizing computational savings, yet it remains a formidable challenge due to severe training instabilities. Empirically, we show that naive initialization at this stage disrupts activation statistics, triggering loss spikes, while copy-based initialization introduces gradient symmetry that hinders feature diversity. To address these issues, we propose SPARKLING (balancing {S}ignal {P}reservation {A}nd symmet{R}y brea{K}ing for width-progressive {L}earn{ING}), a novel framework for mid-stage width expansion. Our method achieves signal preservation via RMS-scale consistency, stabilizing activation statistics during expansion. Symmetry breaking is ensured through asymmetric optimizer state resetting and learning rate re-warmup. Extensive experiments on Mixture-of-Experts (MoE) models demonstrate that, across multiple width axes and optimizer families, SPARKLING consistently outperforms training from scratch and reduces training cost by up to 35% under 2times width expansion.', 'score': 10, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': 'a44d3797f8f8025c', 'authors': ['Qifan Yu', 'Xinyu Ma', 'Zhijian Zhuo', 'Minrui Wang', 'Deyi Liu', 'Shiyi Zhan', 'Yiyuan Ma', 'Liang Xiang', 'Xingyan Bin', 'Di He'], 'affiliations': ['ByteDance Seed', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2602.02472.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ ÑˆĞ¸Ñ€Ğ¸Ğ½Ñ‹ ÑĞµÑ‚Ğ¸: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ¸ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸', 'desc': 'SPARKLING â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ÑˆĞ¸Ñ€Ğ¸Ğ½Ñ‹ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ¸ Ğ½Ğ°Ñ€ÑƒÑˆĞ°ĞµÑ‚ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğ¸ ÑˆĞ¸Ñ€Ğ¸Ğ½Ñ‹ Ğ² ÑĞµÑ€ĞµĞ´Ğ¸Ğ½Ğµ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ RMS-Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ÑĞ±Ñ€Ğ¾Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ½Ğ°Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğ°Ñ€ÑƒÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹, Ğ¸ ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞ³Ğ¾ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ², SPARKLING Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°, Ñ‚Ğ°Ğº Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Mixture-of-Experts Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ´Ğ¾ 35% Ğ¿Ñ€Ğ¸ Ğ´Ğ²ÑƒĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğ¸ ÑˆĞ¸Ñ€Ğ¸Ğ½Ñ‹.'}, 'en': {'title': 'Enhancing Deep Learning Efficiency with Mid-Stage Width Expansion', 'desc': 'SPARKLING is a new framework designed to expand the width of deep learning models during the mid-stage of training while ensuring that the signal is preserved and symmetry is broken. This approach addresses the challenges of training instabilities that arise when increasing model width, which can lead to loss spikes and reduced feature diversity. By maintaining consistent activation statistics and employing techniques like asymmetric optimizer state resetting, SPARKLING stabilizes the training process. Experimental results show that this method can significantly reduce computational costs, achieving up to 35% savings compared to traditional training methods.'}, 'zh': {'title': 'SPARKLINGï¼šæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å®½åº¦æ‰©å±•æ–°æ–¹æ³•', 'desc': 'SPARKLINGæ˜¯ä¸€ä¸ªç”¨äºæ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­æœŸå®½åº¦æ‰©å±•çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä¿æŒä¿¡å·çš„å®Œæ•´æ€§å¹¶æ‰“ç ´å¯¹ç§°æ€§ï¼Œä»¥ç¨³å®šè®­ç»ƒå¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚è¯¥æ–¹æ³•é€šè¿‡é€æ­¥å¢åŠ æ¨¡å‹è§„æ¨¡ï¼Œå‡å°‘é¢„è®­ç»ƒçš„è®¡ç®—å¼€é”€ï¼Œå°¤å…¶æ˜¯åœ¨å®½åº¦æ‰©å±•æ–¹é¢ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚SPARKLINGé€šè¿‡RMSå°ºåº¦ä¸€è‡´æ€§å®ç°ä¿¡å·çš„ä¿æŒï¼Œå¹¶é€šè¿‡ä¸å¯¹ç§°ä¼˜åŒ–å™¨çŠ¶æ€é‡ç½®å’Œå­¦ä¹ ç‡é‡æ–°å‡æ¸©æ¥ç¡®ä¿å¯¹ç§°æ€§ç ´åã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPARKLINGåœ¨å¤šä¸ªå®½åº¦è½´å’Œä¼˜åŒ–å™¨å®¶æ—ä¸­è¡¨ç°ä¼˜äºä»å¤´è®­ç»ƒï¼Œå¹¶åœ¨å®½åº¦æ‰©å±•2å€çš„æƒ…å†µä¸‹å°†è®­ç»ƒæˆæœ¬é™ä½äº†å¤šè¾¾35%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.02343', 'title': 'Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics', 'url': 'https://huggingface.co/papers/2602.02343', 'abstract': "Large language model control methods are unified under a dynamic weight update framework, revealing a preference-utility trade-off and enabling improved steering through SPLIT approach.  \t\t\t\t\tAI-generated summary \t\t\t\t Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frames these interventions as dynamic weight updates induced by a control signal, placing them within a single conceptual framework. Building on this view, we propose a unified preference-utility analysis that separates control effects into preference, defined as the tendency toward a target concept, and utility, defined as coherent and task-valid generation, and measures both on a shared log-odds scale using polarity-paired contrastive examples. Across methods, we observe a consistent trade-off between preference and utility: stronger control increases preference while predictably reducing utility. We further explain this behavior through an activation manifold perspective, in which control shifts representations along target-concept directions to enhance preference, while utility declines primarily when interventions push representations off the model's valid-generation manifold. Finally, we introduce a new steering approach SPLIT guided by this analysis that improves preference while better preserving utility. Code is available at https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md.", 'score': 10, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': '698ec81e2428cdb9', 'authors': ['Ziwen Xu', 'Chenyan Wu', 'Hengyu Sun', 'Haiwen Hong', 'Mengru Wang', 'Yunzhi Yao', 'Longtao Huang', 'Hui Xue', 'Shumin Deng', 'Zhixuan Chu', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['Alibaba Group', 'NUS-NCS Joint Lab', 'National University of Singapore', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2602.02343.jpg', 'data': {'categories': [], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼: ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ LLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM) - Ğ¾Ñ‚ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ´Ğ¾ LoRA Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² - Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ²ĞµÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ (ÑĞºĞ»Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ñƒ) Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ (ÑĞ²ÑĞ·Ğ½Ğ°Ñ Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°), Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑ Ğ¾Ğ±Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ½Ğ° ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑˆĞºĞ°Ğ»Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ: ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ, Ğ½Ğ¾ Ğ½ĞµĞ¸Ğ·Ğ±ĞµĞ¶Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ SPLIT, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'Balancing Preference and Utility in Language Model Control', 'desc': 'This paper presents a unified framework for controlling large language models (LLMs) by viewing various control methods as dynamic weight updates influenced by a control signal. It introduces a preference-utility analysis that distinguishes between preference, which drives the model towards a desired concept, and utility, which ensures the output remains coherent and task-relevant. The study reveals a trade-off where stronger control enhances preference but can diminish utility, explained through the concept of an activation manifold. To address this, the authors propose a new steering method called SPLIT, which aims to improve preference while maintaining higher utility in model outputs.'}, 'zh': {'title': 'ç»Ÿä¸€æ§åˆ¶æ–¹æ³•ï¼Œä¼˜åŒ–åå¥½ä¸æ•ˆç”¨çš„å¹³è¡¡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¤§å‹è¯­è¨€æ¨¡å‹æ§åˆ¶æ–¹æ³•æ¡†æ¶ï¼Œå°†ä¸åŒçš„æ§åˆ¶æŠ€æœ¯è§†ä¸ºåŠ¨æ€æƒé‡æ›´æ–°ã€‚æˆ‘ä»¬åˆ†æäº†æ§åˆ¶æ•ˆæœçš„åå¥½å’Œæ•ˆç”¨ï¼Œå‘ç°å¢å¼ºæ§åˆ¶ä¼šæé«˜åå¥½ä½†é™ä½æ•ˆç”¨ã€‚é€šè¿‡æ¿€æ´»æµå½¢çš„è§†è§’ï¼Œæˆ‘ä»¬è§£é‡Šäº†è¿™ç§æƒè¡¡å…³ç³»ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„å¼•å¯¼æ–¹æ³•SPLITï¼Œä»¥æ”¹å–„åå¥½åŒæ—¶æ›´å¥½åœ°ä¿æŒæ•ˆç”¨ã€‚è¯¥ç ”ç©¶ä¸ºç†è§£å’Œæ¯”è¾ƒä¸åŒçš„æ§åˆ¶æ–¹æ³•æä¾›äº†æ–°çš„è§†è§’ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.02227', 'title': "Show, Don't Tell: Morphing Latent Reasoning into Image Generation", 'url': 'https://huggingface.co/papers/2602.02227', 'abstract': 'LatentMorph integrates implicit latent reasoning into text-to-image generation through four lightweight components that enable adaptive self-refinement and improve both efficiency and cognitive alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image (T2I) generation has achieved remarkable progress, yet existing methods often lack the ability to dynamically reason and refine during generation--a hallmark of human creativity. Current reasoning-augmented paradigms most rely on explicit thought processes, where intermediate reasoning is decoded into discrete text at fixed steps with frequent image decoding and re-encoding, leading to inefficiencies, information loss, and cognitive mismatches. To bridge this gap, we introduce LatentMorph, a novel framework that seamlessly integrates implicit latent reasoning into the T2I generation process. At its core, LatentMorph introduces four lightweight components: (i) a condenser for summarizing intermediate generation states into compact visual memory, (ii) a translator for converting latent thoughts into actionable guidance, (iii) a shaper for dynamically steering next image token predictions, and (iv) an RL-trained invoker for adaptively determining when to invoke reasoning. By performing reasoning entirely in continuous latent spaces, LatentMorph avoids the bottlenecks of explicit reasoning and enables more adaptive self-refinement. Extensive experiments demonstrate that LatentMorph (I) enhances the base model Janus-Pro by 16% on GenEval and 25% on T2I-CompBench; (II) outperforms explicit paradigms (e.g., TwiG) by 15% and 11% on abstract reasoning tasks like WISE and IPV-Txt, (III) while reducing inference time by 44% and token consumption by 51%; and (IV) exhibits 71% cognitive alignment with human intuition on reasoning invocation.', 'score': 10, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': '1315c99b8dc0ab14', 'authors': ['Harold Haodong Chen', 'Xinxiang Yin', 'Wen-Jie Shu', 'Hongfei Zhang', 'Zixin Zhang', 'Chenfei Liao', 'Litao Guo', 'Qifeng Chen', 'Ying-Cong Chen'], 'affiliations': ['HKU'], 'pdf_title_img': 'assets/pdf/title_img/2602.02227.jpg', 'data': {'categories': ['#multimodal', '#cv', '#reasoning', '#optimization', '#rl', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡ĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑƒĞ¼Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'LatentMorph Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ»Ñ‘Ğ³ĞºĞ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¾Ñ€ Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹, Ñ‚Ñ€Ğ°Ğ½ÑĞ»ÑÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ² Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾, Ñ„Ğ¾Ñ€Ğ¼Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹ reinforcement learning Ğ¸Ğ½Ğ²Ğ¾ĞºĞµÑ€ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ²ÑĞµÑ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°ĞµÑ‚ ÑƒĞ·ĞºĞ¸Ñ… Ğ¼ĞµÑÑ‚ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞ°Ğ¼Ğ¾ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ (Ğ½Ğ° 16-25% Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼), ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ½Ğ° 44% Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 51%.'}, 'en': {'title': 'Empowering Creativity in T2I with Implicit Reasoning', 'desc': 'LatentMorph is a new framework for text-to-image (T2I) generation that incorporates implicit latent reasoning to enhance the creative process. It features four key components: a condenser for summarizing generation states, a translator for turning latent thoughts into guidance, a shaper for directing image predictions, and an RL-trained invoker for deciding when to reason. This approach allows for reasoning to occur in continuous latent spaces, which improves efficiency and reduces information loss compared to traditional explicit reasoning methods. Experiments show that LatentMorph significantly boosts performance and cognitive alignment with human intuition while also cutting down on inference time and resource usage.'}, 'zh': {'title': 'éšå¼æ¨ç†ï¼Œæå‡å›¾åƒç”Ÿæˆæ•ˆç‡', 'desc': 'LatentMorph æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œå°†éšå¼æ½œåœ¨æ¨ç†æ•´åˆåˆ°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­ã€‚å®ƒé€šè¿‡å››ä¸ªè½»é‡çº§ç»„ä»¶å®ç°è‡ªæˆ‘ä¼˜åŒ–ï¼Œæé«˜äº†ç”Ÿæˆæ•ˆç‡å’Œè®¤çŸ¥ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶é¿å…äº†æ˜¾å¼æ¨ç†çš„ç“¶é¢ˆï¼Œä½¿å¾—æ¨ç†è¿‡ç¨‹åœ¨è¿ç»­çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œï¼Œä»è€Œå®ç°æ›´çµæ´»çš„è‡ªæˆ‘æ”¹è¿›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLatentMorph åœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå¹¶å‡å°‘äº†æ¨ç†æ—¶é—´å’Œèµ„æºæ¶ˆè€—ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01479', 'title': 'Ebisu: Benchmarking Large Language Models in Japanese Finance', 'url': 'https://huggingface.co/papers/2602.01479', 'abstract': 'A Japanese financial language understanding benchmark named Ebisu is introduced, featuring two expert-annotated tasks that evaluate implicit commitment recognition and hierarchical financial terminology extraction, revealing persistent challenges for current language models despite their advanced capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Japanese finance combines agglutinative, head-final linguistic structure, mixed writing systems, and high-context communication norms that rely on indirect expression and implicit commitment, posing a substantial challenge for LLMs. We introduce Ebisu, a benchmark for native Japanese financial language understanding, comprising two linguistically and culturally grounded, expert-annotated tasks: JF-ICR, which evaluates implicit commitment and refusal recognition in investor-facing Q&A, and JF-TE, which assesses hierarchical extraction and ranking of nested financial terminology from professional disclosures. We evaluate a diverse set of open-source and proprietary LLMs spanning general-purpose, Japanese-adapted, and financial models. Results show that even state-of-the-art systems struggle on both tasks. While increased model scale yields limited improvements, language- and domain-specific adaptation does not reliably improve performance, leaving substantial gaps unresolved. Ebisu provides a focused benchmark for advancing linguistically and culturally grounded financial NLP. All datasets and evaluation scripts are publicly released.', 'score': 10, 'issue_id': 884, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 1', 'zh': '2æœˆ1æ—¥'}, 'hash': '19e6213f18c89cda', 'authors': ['Xueqing Peng', 'Ruoyu Xiang', 'Fan Zhang', 'Mingzi Song', 'Mingyang Jiang', 'Yan Wang', 'Lingfei Qian', 'Taiki Hara', 'Yuqing Guo', 'Jimin Huang', 'Junichi Tsujii', 'Sophia Ananiadou'], 'affiliations': ['Meiji Gakuin University', 'National Institute of Advanced Industrial Science and Technology (AIST)', 'New York University', 'The Fin AI', 'The National Centre for Text Mining', 'The University of Tokyo', 'University of Manchester'], 'pdf_title_img': 'assets/pdf/title_img/2602.01479.jpg', 'data': {'categories': ['#multilingual', '#dataset', '#science', '#benchmark', '#low_resource', '#open_source'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ±Ğ°Ñ€ÑŒĞµÑ€: Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ LLM Ğ±Ğ¾Ñ€ÑÑ‚ÑÑ Ñ ÑĞ¿Ğ¾Ğ½ÑĞºĞ¸Ğ¼ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ebisu Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ¿Ğ¾Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚-Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ² Q&A Ğ´Ğ»Ñ Ğ¸Ğ½Ğ²ĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ LLM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑĞ¿Ğ¾Ğ½ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¾Ğ±ĞµĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ°Ñ‘Ñ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ, Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Ebisu: Bridging Gaps in Japanese Financial Language Understanding', 'desc': 'The paper introduces Ebisu, a benchmark designed to assess language understanding in Japanese finance. It includes two expert-annotated tasks: JF-ICR for recognizing implicit commitments in investor Q&A, and JF-TE for extracting hierarchical financial terminology. Despite advancements in language models, the study finds that even the best-performing models struggle with these tasks, highlighting the complexity of Japanese financial language. The results indicate that simply increasing model size or adapting to specific domains does not significantly enhance performance, revealing ongoing challenges in financial natural language processing.'}, 'zh': {'title': 'Ebisuï¼šæ¨åŠ¨æ—¥æœ¬é‡‘èè¯­è¨€ç†è§£çš„åŸºå‡†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºEbisuçš„æ—¥æœ¬é‡‘èè¯­è¨€ç†è§£åŸºå‡†ï¼ŒåŒ…å«ä¸¤ä¸ªä¸“å®¶æ³¨é‡Šçš„ä»»åŠ¡ï¼Œè¯„ä¼°éšæ€§æ‰¿è¯ºè¯†åˆ«å’Œå±‚æ¬¡é‡‘èæœ¯è¯­æå–ã€‚å°½ç®¡å½“å‰çš„è¯­è¨€æ¨¡å‹èƒ½åŠ›å…ˆè¿›ï¼Œä½†åœ¨è¿™äº›ä»»åŠ¡ä¸Šä»é¢ä¸´æŒç»­æŒ‘æˆ˜ã€‚æ—¥æœ¬é‡‘èè¯­è¨€çš„ç‰¹ç‚¹åŒ…æ‹¬ç²˜ç€æ€§ã€åç½®ç»“æ„å’Œé«˜è¯­å¢ƒäº¤æµè§„èŒƒï¼Œè¿™ä½¿å¾—è¯­è¨€æ¨¡å‹çš„ç†è§£å˜å¾—å¤æ‚ã€‚Ebisuä¸ºé‡‘èè‡ªç„¶è¯­è¨€å¤„ç†çš„è¿›æ­¥æä¾›äº†ä¸€ä¸ªèšç„¦çš„åŸºå‡†ï¼Œæ‰€æœ‰æ•°æ®é›†å’Œè¯„ä¼°è„šæœ¬å‡å·²å…¬å¼€å‘å¸ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.02156', 'title': 'LoopViT: Scaling Visual ARC with Looped Transformers', 'url': 'https://huggingface.co/papers/2602.02156', 'abstract': 'Loop-ViT introduces a recursive vision transformer architecture that decouples reasoning depth from model capacity through weight-tied recurrence and dynamic exit mechanisms, achieving superior visual reasoning performance with fewer parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in visual reasoning have leveraged vision transformers to tackle the ARC-AGI benchmark. However, we argue that the feed-forward architecture, where computational depth is strictly bound to parameter size, falls short of capturing the iterative, algorithmic nature of human induction. In this work, we propose a recursive architecture called Loop-ViT, which decouples reasoning depth from model capacity through weight-tied recurrence. Loop-ViT iterates a weight-tied Hybrid Block, combining local convolutions and global attention, to form a latent chain of thought. Crucially, we introduce a parameter-free Dynamic Exit mechanism based on predictive entropy: the model halts inference when its internal state ``crystallizes" into a low-uncertainty attractor. Empirical results on the ARC-AGI-1 benchmark validate this perspective: our 18M model achieves 65.8% accuracy, outperforming massive 73M-parameter ensembles. These findings demonstrate that adaptive iterative computation offers a far more efficient scaling axis for visual reasoning than simply increasing network width. The code is available at https://github.com/WenjieShu/LoopViT.', 'score': 8, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': 'e3f1d9d840128bb1', 'authors': ['Wen-Jie Shu', 'Xuerui Qiu', 'Rui-Jie Zhu', 'Harold Haodong Chen', 'Yexin Liu', 'Harry Yang'], 'affiliations': ['CASIA', 'HKUST', 'UC Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2602.02156.jpg', 'data': {'categories': ['#architecture', '#cv', '#benchmark', '#reasoning', '#optimization', '#open_source', '#small_models'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ÑĞµÑ‚Ğ¸', 'desc': 'Loop-ViT Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñƒ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… feed-forward Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€, Ğ³Ğ´Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ° ÑĞ²ÑĞ·Ğ°Ğ½Ğ° Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Loop-ViT Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ´Ğ¸Ğ½ Ğ±Ğ»Ğ¾Ğº, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ²Ñ‘Ñ€Ñ‚ĞºĞ¸ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Dynamic Exit Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸, Ğ¾ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ³Ğ´Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ñ‚Ñ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ARC-AGI-1 Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 18M Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 65.8% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ğ¸ Ñ 73M Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Decoupling Reasoning Depth from Model Capacity in Visual Transformers', 'desc': "Loop-ViT presents a novel recursive vision transformer architecture that separates the depth of reasoning from the model's capacity, allowing for more efficient visual reasoning. It utilizes weight-tied recurrence and a Dynamic Exit mechanism to optimize performance while maintaining fewer parameters. The architecture incorporates a Hybrid Block that combines local convolutions with global attention, creating a chain of thought that iteratively refines its predictions. Empirical results show that Loop-ViT outperforms larger models on the ARC-AGI benchmark, highlighting the advantages of adaptive iterative computation in visual reasoning tasks."}, 'zh': {'title': 'Loop-ViTï¼šé«˜æ•ˆçš„è§†è§‰æ¨ç†æ–°æ¶æ„', 'desc': 'Loop-ViTæ˜¯ä¸€ç§é€’å½’è§†è§‰å˜æ¢å™¨æ¶æ„ï¼Œé€šè¿‡æƒé‡ç»‘å®šçš„é€’å½’å’ŒåŠ¨æ€é€€å‡ºæœºåˆ¶ï¼Œå°†æ¨ç†æ·±åº¦ä¸æ¨¡å‹å®¹é‡è§£è€¦ï¼Œä»è€Œä»¥æ›´å°‘çš„å‚æ•°å®ç°æ›´ä¼˜çš„è§†è§‰æ¨ç†æ€§èƒ½ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†æƒé‡ç»‘å®šçš„æ··åˆå—ï¼Œç»“åˆå±€éƒ¨å·ç§¯å’Œå…¨å±€æ³¨æ„åŠ›ï¼Œå½¢æˆäº†ä¸€ç§æ½œåœ¨çš„æ€ç»´é“¾ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ— å‚æ•°çš„åŠ¨æ€é€€å‡ºæœºåˆ¶ï¼Œå½“æ¨¡å‹çš„å†…éƒ¨çŠ¶æ€â€œç»“æ™¶â€æˆä½ä¸ç¡®å®šæ€§å¸å¼•å­æ—¶ï¼Œåœæ­¢æ¨ç†ã€‚å®éªŒè¯æ˜ï¼ŒLoop-ViTåœ¨ARC-AGI-1åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œ18Må‚æ•°çš„æ¨¡å‹å‡†ç¡®ç‡è¾¾åˆ°65.8%ï¼Œè¶…è¶Šäº†73Må‚æ•°çš„å¤§å‹é›†æˆæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01322', 'title': 'PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding', 'url': 'https://huggingface.co/papers/2602.01322', 'abstract': 'PolySAE extends sparse autoencoders with polynomial decoding to capture feature interactions and compositional structure while maintaining linear encoders for interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t Sparse autoencoders (SAEs) have emerged as a promising method for interpreting neural network representations by decomposing activations into sparse combinations of dictionary atoms. However, SAEs assume that features combine additively through linear reconstruction, an assumption that cannot capture compositional structure: linear models cannot distinguish whether "Starbucks" arises from the composition of "star" and "coffee" features or merely their co-occurrence. This forces SAEs to allocate monolithic features for compound concepts rather than decomposing them into interpretable constituents. We introduce PolySAE, which extends the SAE decoder with higher-order terms to model feature interactions while preserving the linear encoder essential for interpretability. Through low-rank tensor factorization on a shared projection subspace, PolySAE captures pairwise and triple feature interactions with small parameter overhead (3% on GPT2). Across four language models and three SAE variants, PolySAE achieves an average improvement of approximately 8% in probing F1 while maintaining comparable reconstruction error, and produces 2-10times larger Wasserstein distances between class-conditional feature distributions. Critically, learned interaction weights exhibit negligible correlation with co-occurrence frequency (r = 0.06 vs. r = 0.82 for SAE feature covariance), suggesting that polynomial terms capture compositional structure, such as morphological binding and phrasal composition, largely independent of surface statistics.', 'score': 8, 'issue_id': 884, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 1', 'zh': '2æœˆ1æ—¥'}, 'hash': '15ada34a45c5eb0c', 'authors': ['Panagiotis Koromilas', 'Andreas D. Demou', 'James Oldfield', 'Yannis Panagakis', 'Mihalis Nicolaou'], 'affiliations': ['Archimedes AI/Athena Research Center', 'The Cyprus Institute', 'University of Athens', 'University of Cyprus', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2602.01322.jpg', 'data': {'categories': ['#interpretability', '#architecture', '#training'], 'emoji': 'ğŸ§©', 'ru': {'title': 'ĞŸĞ¾Ğ»Ğ¸Ğ½Ğ¾Ğ¼Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑÑ…', 'desc': 'PolySAE ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¸ (SAE) Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ğ½Ğ¾Ğ¼Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ SAE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ½Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ»Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ Ğ¸Ğ· ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ¸Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ñ‡Ğ°ÑÑ‚Ğ¾Ğ¹ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸. PolySAE ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸, Ğ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ñ‡Ğ»ĞµĞ½Ğ°Ğ¼Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ°. ĞĞ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° 8% Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ñ„Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ.'}, 'en': {'title': 'PolySAE: Enhancing Interpretability with Polynomial Feature Interactions', 'desc': "PolySAE is a novel approach that enhances sparse autoencoders by incorporating polynomial decoding to better understand feature interactions and compositional structures. Traditional sparse autoencoders rely on linear reconstruction, which limits their ability to differentiate between co-occurring features and those that are compositionally related. By introducing higher-order terms in the decoding process, PolySAE allows for a more nuanced representation of features while keeping the linear encoder for interpretability. The results show that PolySAE significantly improves performance in probing tasks while effectively capturing complex relationships between features without increasing the model's complexity significantly."}, 'zh': {'title': 'PolySAEï¼šæ•æ‰ç‰¹å¾äº¤äº’ä¸ç»„åˆç»“æ„çš„åˆ›æ–°æ¨¡å‹', 'desc': 'PolySAEæ˜¯ä¸€ç§æ‰©å±•ç¨€ç–è‡ªç¼–ç å™¨çš„æ¨¡å‹ï¼Œé€šè¿‡å¤šé¡¹å¼è§£ç æ¥æ•æ‰ç‰¹å¾ä¹‹é—´çš„äº¤äº’å’Œç»„åˆç»“æ„ï¼ŒåŒæ—¶ä¿æŒçº¿æ€§ç¼–ç å™¨ä»¥ä¾¿äºè§£é‡Šã€‚ä¼ ç»Ÿçš„ç¨€ç–è‡ªç¼–ç å™¨å‡è®¾ç‰¹å¾æ˜¯é€šè¿‡çº¿æ€§é‡æ„åŠ æ€§ç»„åˆçš„ï¼Œè¿™æ— æ³•æœ‰æ•ˆæ•æ‰ç»„åˆç»“æ„ã€‚PolySAEé€šè¿‡åœ¨å…±äº«æŠ•å½±å­ç©ºé—´ä¸Šè¿›è¡Œä½ç§©å¼ é‡åˆ†è§£ï¼Œèƒ½å¤Ÿä»¥è¾ƒå°çš„å‚æ•°å¼€é”€å»ºæ¨¡ç‰¹å¾çš„æˆå¯¹å’Œä¸‰å…ƒäº¤äº’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPolySAEåœ¨å¤šä¸ªè¯­è¨€æ¨¡å‹ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶ä¿æŒäº†å¯æ¯”çš„é‡æ„è¯¯å·®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.20613', 'title': 'AgentIF-OneDay: A Task-level Instruction-Following Benchmark for General AI Agents in Daily Scenarios', 'url': 'https://huggingface.co/papers/2601.20613', 'abstract': "AgentIF-OneDay evaluates AI agents' ability to handle diverse daily tasks through natural language instructions, requiring problem-solving, attachment understanding, and file-based outputs across three user-centric categories.  \t\t\t\t\tAI-generated summary \t\t\t\t The capacity of AI agents to effectively handle tasks of increasing duration and complexity continues to grow, demonstrating exceptional performance in coding, deep research, and complex problem-solving evaluations. However, in daily scenarios, the perception of these advanced AI capabilities among general users remains limited. We argue that current evaluations prioritize increasing task difficulty without sufficiently addressing the diversity of agentic tasks necessary to cover the daily work, life, and learning activities of a broad demographic. To address this, we propose AgentIF-OneDay, aimed at determining whether general users can utilize natural language instructions and AI agents to complete a diverse array of daily tasks. These tasks require not only solving problems through dialogue but also understanding various attachment types and delivering tangible file-based results. The benchmark is structured around three user-centric categories: Open Workflow Execution, which assesses adherence to explicit and complex workflows; Latent Instruction, which requires agents to infer implicit instructions from attachments; and Iterative Refinement, which involves modifying or expanding upon ongoing work. We employ instance-level rubrics and a refined evaluation pipeline that aligns LLM-based verification with human judgment, achieving an 80.1% agreement rate using Gemini-3-Pro. AgentIF-OneDay comprises 104 tasks covering 767 scoring points. We benchmarked four leading general AI agents and found that agent products built based on APIs and ChatGPT agents based on agent RL remain in the first tier simultaneously. Leading LLM APIs and open-source models have internalized agentic capabilities, enabling AI application teams to develop cutting-edge Agent products.", 'score': 7, 'issue_id': 884, 'pub_date': '2026-01-28', 'pub_date_card': {'ru': '28 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 28', 'zh': '1æœˆ28æ—¥'}, 'hash': 'e7850e15fbaed0b1', 'authors': ['Kaiyuan Chen', 'Qimin Wu', 'Taiyu Hou', 'Tianhao Tang', 'Xueyu Hu', 'Yuchen Hou', 'Bikun Li', 'Chengming Qian', 'Guoyin Wang', 'Haolin Chen', 'Haotong Tian', 'Haoye Zhang', 'Haoyu Bian', 'Hongbing Pan', 'Hongkang Zhang', 'Hongyi Zhou', 'Jiaqi Cai', 'Jiewu Rao', 'Jiyuan Ren', 'Keduan Huang', 'Lucia Zhu Huang', 'Mingyu Yuan', 'Naixu Guo', 'Qicheng Tang', 'Qinyan Zhang', 'Shuai Chen', 'Siheng Chen', 'Ting Ting Li', 'Xiaoxing Guo', 'Yaocheng Zuo', 'Yaoqi Guo', 'Yinan Wang', 'Yinzhou Yu', 'Yize Wang', 'Yuan Jiang', 'Yuan Tian', 'Yuanshuo Zhang', 'Yuxuan Liu', 'Yvette Yan Zeng', 'Zenyu Shan', 'Zihan Yin', 'Xiaobo Hu', 'Yang Liu', 'Yixin Ren', 'Yuan Gong'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2601.20613.jpg', 'data': {'categories': ['#agents', '#benchmark', '#dataset'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº AgentIF-OneDay Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ñ‡ĞµÑ€ĞµĞ· ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸: Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ², Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸Ğ· Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞÑ†ĞµĞ½ĞºĞ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 104 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ 767 ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ÑĞ¾Ğ²Ğ¼ĞµÑ‰Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ ÑÑƒĞ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ¼ (Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚Ğ° ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ 80.1%). Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ AI Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑƒĞ¶Ğµ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² API Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ°Ğ¼ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering AI Agents for Everyday Tasks', 'desc': "AgentIF-OneDay is a benchmark designed to evaluate AI agents' performance in completing a variety of daily tasks using natural language instructions. It focuses on three main categories: Open Workflow Execution, Latent Instruction, and Iterative Refinement, which test the agents' problem-solving abilities and understanding of different attachment types. The study highlights that while AI agents excel in complex tasks, their effectiveness in everyday scenarios is not fully recognized by users. By employing a refined evaluation pipeline and instance-level rubrics, the research demonstrates a high agreement rate between AI assessments and human judgment, showcasing the capabilities of leading AI models in practical applications."}, 'zh': {'title': 'è¯„ä¼°AIä»£ç†çš„æ—¥å¸¸ä»»åŠ¡å¤„ç†èƒ½åŠ›', 'desc': 'AgentIF-OneDay æ˜¯ä¸€ä¸ªè¯„ä¼°äººå·¥æ™ºèƒ½ä»£ç†å¤„ç†æ—¥å¸¸ä»»åŠ¡èƒ½åŠ›çš„åŸºå‡†ï¼Œä½¿ç”¨è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ¥å®Œæˆå¤šæ ·åŒ–çš„ä»»åŠ¡ã€‚è¿™äº›ä»»åŠ¡ä¸ä»…éœ€è¦è§£å†³é—®é¢˜ï¼Œè¿˜éœ€è¦ç†è§£ä¸åŒç±»å‹çš„é™„ä»¶ï¼Œå¹¶æä¾›åŸºäºæ–‡ä»¶çš„è¾“å‡ºã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡AIåœ¨å¤æ‚é—®é¢˜è§£å†³å’Œç¼–ç æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†æ™®é€šç”¨æˆ·å¯¹å…¶èƒ½åŠ›çš„è®¤çŸ¥ä»ç„¶æœ‰é™ã€‚æˆ‘ä»¬æå‡ºçš„è¯„ä¼°æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªç”¨æˆ·ä¸­å¿ƒçš„ç±»åˆ«ï¼Œæ—¨åœ¨æ›´å…¨é¢åœ°åæ˜ AIä»£ç†åœ¨æ—¥å¸¸å·¥ä½œã€ç”Ÿæ´»å’Œå­¦ä¹ ä¸­çš„åº”ç”¨èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.02453', 'title': 'Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling', 'url': 'https://huggingface.co/papers/2602.02453', 'abstract': 'Thinking with Comics emerges as an effective visual reasoning approach that bridges images and videos by leveraging comic structures for improved multimodal reasoning efficiency and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning.', 'score': 5, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': '09822eb6bd7dd50b', 'authors': ['Andong Chen', 'Wenxin Zhu', 'Qiuyu Ding', 'Yuchen Song', 'Muyun Yang', 'Tiejun Zhao'], 'affiliations': ['Harbin Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2602.02453.jpg', 'data': {'categories': ['#multimodal', '#video', '#cv', '#benchmark', '#reasoning', '#long_context'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¸ĞºÑÑ‹ ĞºĞ°Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ¼Ğ¸ĞºÑÑ‹ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞšĞ¾Ğ¼Ğ¸ĞºÑÑ‹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ, Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ´Ğ²Ğ° Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ¼Ğ¸ĞºÑĞ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¸Ñ… Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ ĞºĞ¾Ğ¼Ğ¸ĞºÑĞ°Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑÑ‚Ğ°Ğ²Ğ°ÑÑÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼, Ñ‡ĞµĞ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Comics: The Bridge for Smarter Visual Reasoning', 'desc': "The paper introduces 'Thinking with Comics', a novel visual reasoning method that enhances the understanding of images and videos by utilizing comic structures. This approach addresses the limitations of static images, which lack temporal context, and the inefficiencies of videos, which can be redundant and computationally expensive. By leveraging the high information density of comics, the method maintains narrative coherence and embedded text while reducing reasoning costs. Experimental results demonstrate that this comic-based reasoning outperforms traditional image-based reasoning in complex tasks, highlighting the effectiveness of comics as an intermediate visual representation for multimodal reasoning."}, 'zh': {'title': 'æ¼«ç”»æ€ç»´ï¼šæå‡å¤šæ¨¡æ€æ¨ç†çš„æœ‰æ•ˆæ¡¥æ¢', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œæ¼«ç”»æ€ç»´â€çš„è§†è§‰æ¨ç†æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡æ¼«ç”»ç»“æ„æé«˜å¤šæ¨¡æ€æ¨ç†çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚æ¼«ç”»ä½œä¸ºä¸€ç§ä¿¡æ¯å¯†åº¦é«˜çš„åª’ä»‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä¿ç•™æ—¶é—´ç»“æ„ã€åµŒå…¥æ–‡æœ¬å’Œå™äº‹è¿è´¯æ€§ï¼ŒåŒæ—¶é™ä½æ¨ç†æˆæœ¬ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ¼«ç”»æ€ç»´åœ¨å¤šæ­¥æ—¶é—´å’Œå› æœæ¨ç†ä»»åŠ¡ä¸Šä¼˜äºå›¾åƒæ€ç»´ï¼Œå¹¶ä¸”åœ¨æ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºè§†é¢‘æ€ç»´ã€‚ä¸åŒçš„æ¼«ç”»å™äº‹ç»“æ„å’Œé£æ ¼å¯¹ä»»åŠ¡è¡¨ç°æœ‰ä¸€è‡´çš„å½±å“ï¼Œè¡¨æ˜æ¼«ç”»æ˜¯æ”¹å–„å¤šæ¨¡æ€æ¨ç†çš„æœ‰æ•ˆä¸­ä»‹è§†è§‰è¡¨ç¤ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01675', 'title': 'TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios', 'url': 'https://huggingface.co/papers/2602.01675', 'abstract': 'TRIP-Bench presents a comprehensive long-horizon benchmark for travel planning that evaluates LLM agents on complex multi-turn interactions, while GTPO offers an online reinforcement learning approach to enhance constraint satisfaction and robustness in extended dialogues.  \t\t\t\t\tAI-generated summary \t\t\t\t As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions. To bridge this gap, we introduce TRIP-Bench, a long-horizon benchmark grounded in realistic travel-planning scenarios. TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation. It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\\% success on the easy split, with performance dropping below 10\\% on hard subsets. We further propose GTPO, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing. Applied to Qwen2.5-32B-Instruct, GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.', 'score': 5, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': 'c5c5d10005e6940b', 'authors': ['Yuanzhe Shen', 'Zisu Huang', 'Zhengyuan Wang', 'Muzhao Tian', 'Zhengkang Guo', 'Chenyang Zhang', 'Shuaiyu Zhou', 'Zengjie Hu', 'Dailin Li', 'Jingwen Xu', 'Kaimin Wang', 'Wenhao Liu', 'Tianlong Li', 'Fengpeng Yue', 'Feng Hong', 'Cao Liu', 'Ke Zeng'], 'affiliations': ['University 1', 'University 2', 'University 3', 'University 4', 'University 5', 'University 6'], 'pdf_title_img': 'assets/pdf/title_img/2602.01675.jpg', 'data': {'categories': ['#dataset', '#agents', '#benchmark', '#reasoning', '#optimization', '#rl', '#long_context'], 'emoji': 'âœˆï¸', 'ru': {'title': 'Ğ”Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµÑˆĞµÑÑ‚Ğ²Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ TRIP-Bench â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿ÑƒÑ‚ĞµÑˆĞµÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 150 Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ°Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ ÑĞ²Ñ‹ÑˆĞµ 200k Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ GTPO â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ 50% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ¾Ğ³Ğ´Ğ° ĞºĞ°Ğº Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ğ½Ğ¸Ğ¶Ğµ 10%. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ GTPO Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-32B-Instruct Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Gemini-3-Pro.'}, 'en': {'title': 'Enhancing Travel Planning with TRIP-Bench and GTPO', 'desc': "The paper introduces TRIP-Bench, a benchmark designed to evaluate large language model (LLM) agents in complex travel planning scenarios that require multi-turn interactions. It highlights the limitations of existing benchmarks in addressing challenges like global constraint enforcement and multi-tool coordination. The benchmark includes a variety of difficulty levels, with a focus on long, ambiguous dialogues that test the models' capabilities over extended interactions. Additionally, the paper presents GTPO, a reinforcement learning approach that enhances the performance of LLMs in these scenarios by improving constraint satisfaction and robustness during dialogues."}, 'zh': {'title': 'æå‡æ—…è¡Œè§„åˆ’çš„æ™ºèƒ½å¯¹è¯èƒ½åŠ›', 'desc': 'TRIP-Benchæ˜¯ä¸€ä¸ªå…¨é¢çš„é•¿æœŸåŸºå‡†æµ‹è¯•ï¼Œä¸“æ³¨äºæ—…è¡Œè§„åˆ’ï¼Œè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚å¤šè½®äº¤äº’ä¸­çš„è¡¨ç°ã€‚è¯¥åŸºå‡†åˆ©ç”¨çœŸå®ä¸–ç•Œæ•°æ®ï¼Œæä¾›18ç§å·¥å…·å’Œ40å¤šç§æ—…è¡Œéœ€æ±‚ï¼Œæ”¯æŒè‡ªåŠ¨åŒ–è¯„ä¼°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿æ˜¯å…ˆè¿›çš„æ¨¡å‹åœ¨ç®€å•ä»»åŠ¡ä¸Šä¹Ÿåªèƒ½è¾¾åˆ°50%çš„æˆåŠŸç‡ï¼Œè€Œåœ¨å›°éš¾ä»»åŠ¡ä¸Šè¡¨ç°æ›´å·®ï¼Œä½äº10%ã€‚æ­¤å¤–ï¼ŒGTPOæ˜¯ä¸€ç§åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ç‰¹æ®Šçš„å¥–åŠ±å½’ä¸€åŒ–å’Œå¥–åŠ±å·®å¼‚åŒ–ï¼Œæå‡äº†çº¦æŸæ»¡è¶³å’Œäº¤äº’çš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01660', 'title': 'CoDiQ: Test-Time Scaling for Controllable Difficult Question Generation', 'url': 'https://huggingface.co/papers/2602.01660', 'abstract': "A novel framework called CoDiQ enables controllable difficulty generation for competition-level questions through test-time scaling, resulting in a corpus that significantly improves large reasoning model performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) benefit substantially from training on challenging competition-level questions. However, existing automated question synthesis methods lack precise difficulty control, incur high computational costs, and struggle to generate competition-level questions at scale. In this paper, we propose CoDiQ (Controllable Difficult Question Generation), a novel framework enabling fine-grained difficulty control via test-time scaling while ensuring question solvability. Specifically, first, we identify a test-time scaling tendency (extended reasoning token budget boosts difficulty but reduces solvability) and the intrinsic properties defining the upper bound of a model's ability to generate valid, high-difficulty questions. Then, we develop CoDiQ-Generator from Qwen3-8B, which improves the upper bound of difficult question generation, making it particularly well-suited for challenging question construction. Building on the CoDiQ framework, we build CoDiQ-Corpus (44K competition-grade question sequences). Human evaluations show these questions are significantly more challenging than LiveCodeBench/AIME with over 82% solvability. Training LRMs on CoDiQ-Corpus substantially improves reasoning performance, verifying that scaling controlled-difficulty training questions enhances reasoning capabilities. We open-source CoDiQ-Corpus, CoDiQ-Generator, and implementations to support related research.", 'score': 5, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': 'cdeb2332e5eb43f6', 'authors': ['Zhongyuan Peng', 'Caijun Xu', 'Changyi Xiao', 'Shibo Hong', 'Eli Zhang', 'Stephen Huang', 'Yixin Cao'], 'affiliations': ['Fudan University', 'M-A-P', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2602.01660.jpg', 'data': {'categories': ['#synthetic', '#open_source', '#reasoning'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº CoDiQ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€Ğ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ€ĞµÑˆĞ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen3-8B Ğ¾Ğ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ CoDiQ-Generator, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ¸Ğ· 44K Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ (CoDiQ-Corpus) Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñƒ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼.'}, 'en': {'title': 'CoDiQ: Mastering Question Difficulty for Enhanced AI Reasoning', 'desc': 'The paper introduces CoDiQ, a framework designed to generate competition-level questions with controllable difficulty. It addresses the limitations of existing methods that struggle with difficulty control and high computational costs. By using test-time scaling, CoDiQ allows for fine-tuned difficulty adjustments while maintaining question solvability. The resulting CoDiQ-Corpus, containing 44,000 high-quality questions, significantly enhances the performance of large reasoning models during training.'}, 'zh': {'title': 'å¯æ§éš¾åº¦ç”Ÿæˆï¼Œæå‡æ¨ç†èƒ½åŠ›ï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶CoDiQï¼Œç”¨äºåœ¨æµ‹è¯•æ—¶ç”Ÿæˆå¯æ§éš¾åº¦çš„ç«äº‰çº§é—®é¢˜ï¼Œä»è€Œæ˜¾è‘—æå‡å¤§å‹æ¨ç†æ¨¡å‹çš„æ€§èƒ½ã€‚ç°æœ‰çš„è‡ªåŠ¨é—®é¢˜åˆæˆæ–¹æ³•åœ¨éš¾åº¦æ§åˆ¶ä¸Šä¸å¤Ÿç²¾ç¡®ï¼Œè®¡ç®—æˆæœ¬é«˜ï¼Œä¸”éš¾ä»¥å¤§è§„æ¨¡ç”Ÿæˆç«äº‰çº§é—®é¢˜ã€‚CoDiQé€šè¿‡æµ‹è¯•æ—¶çš„ç¼©æ”¾ç­–ç•¥ï¼Œå®ç°äº†å¯¹é—®é¢˜éš¾åº¦çš„ç»†ç²’åº¦æ§åˆ¶ï¼ŒåŒæ—¶ç¡®ä¿é—®é¢˜çš„å¯è§£æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨CoDiQç”Ÿæˆçš„44Kä¸ªç«äº‰çº§é—®é¢˜åºåˆ—ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜å¤§å‹æ¨ç†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01511', 'title': 'Alternating Reinforcement Learning for Rubric-Based Reward Modeling in Non-Verifiable LLM Post-Training', 'url': 'https://huggingface.co/papers/2602.01511', 'abstract': 'Rubric-ARM framework jointly optimizes rubric generation and judging through reinforcement learning to improve response quality assessment in creative and open-ended tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Standard reward models typically predict scalar scores that fail to capture the multifaceted nature of response quality in non-verifiable domains, such as creative writing or open-ended instruction following. To address this limitation, we propose Rubric-ARM, a framework that jointly optimizes a rubric generator and a judge using reinforcement learning from preference feedback. Unlike existing methods that rely on static rubrics or disjoint training pipelines, our approach treats rubric generation as a latent action learned to maximize judgment accuracy. We introduce an alternating optimization strategy to mitigate the non-stationarity of simultaneous updates, providing theoretical analysis that demonstrates how this schedule reduces gradient variance during training. Extensive experiments show that Rubric-ARM achieves state-of-the-art performance among baselines on multiple benchmarks and significantly improves downstream policy alignment in both offline and online reinforcement learning settings.', 'score': 5, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': 'f32153de89bb30a4', 'authors': ['Ran Xu', 'Tianci Liu', 'Zihan Dong', 'Tony You', 'Ilgee Hong', 'Carl Yang', 'Linjun Zhang', 'Tao Zhao', 'Haoyu Wang'], 'affiliations': ['Emory University', 'Georgia Institute of Technology', 'Purdue University', 'Rutgers University', 'University at Albany'], 'pdf_title_img': 'assets/pdf/title_img/2602.01511.jpg', 'data': {'categories': ['#rl', '#rlhf', '#benchmark'], 'emoji': 'ğŸ“‹', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Rubric-ARM â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑÑƒĞ´ÑŒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞºĞ°Ğ»ÑÑ€Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ¾Ñ‚Ñ€Ğ°Ğ·Ğ¸Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ğ½ĞµĞ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¸ÑÑŒĞ¼Ğ¾ Ğ¸Ğ»Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑĞ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ² ĞºĞ°Ğº ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰ÑƒÑÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½ĞµÑÑ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ñ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ°.'}, 'en': {'title': 'Revolutionizing Response Quality Assessment with Rubric-ARM', 'desc': 'The Rubric-ARM framework enhances the assessment of creative responses by integrating rubric generation and judging through reinforcement learning. Traditional reward models often provide simple scores that overlook the complexity of quality in creative tasks. Rubric-ARM innovatively treats rubric creation as a dynamic process, optimizing it alongside the judging mechanism to improve accuracy. By employing an alternating optimization strategy, the framework effectively reduces training variability, leading to superior performance in various benchmarks and better alignment in reinforcement learning applications.'}, 'zh': {'title': 'Rubric-ARMï¼šæå‡åˆ›æ„ä»»åŠ¡è¯„ä¼°çš„æ™ºèƒ½æ¡†æ¶', 'desc': 'Rubric-ARMæ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è”åˆä¼˜åŒ–è¯„åˆ†æ ‡å‡†ç”Ÿæˆå’Œè¯„åˆ¤ï¼Œä»¥æé«˜åˆ›æ„å’Œå¼€æ”¾å¼ä»»åŠ¡ä¸­çš„å“åº”è´¨é‡è¯„ä¼°ã€‚ä¼ ç»Ÿçš„å¥–åŠ±æ¨¡å‹é€šå¸¸é¢„æµ‹æ ‡é‡åˆ†æ•°ï¼Œæ— æ³•æ•æ‰éå¯éªŒè¯é¢†åŸŸï¼ˆå¦‚åˆ›æ„å†™ä½œï¼‰çš„å“åº”è´¨é‡çš„å¤šé¢æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†è¯„åˆ†æ ‡å‡†ç”Ÿæˆè§†ä¸ºä¸€ç§æ½œåœ¨çš„åŠ¨ä½œï¼Œæ—¨åœ¨æœ€å¤§åŒ–åˆ¤æ–­çš„å‡†ç¡®æ€§ï¼Œå¹¶å¼•å…¥äº¤æ›¿ä¼˜åŒ–ç­–ç•¥ä»¥å‡è½»åŒæ—¶æ›´æ–°çš„éå¹³ç¨³æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRubric-ARMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æ”¹å–„äº†ç¦»çº¿å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸­çš„ä¸‹æ¸¸ç­–ç•¥å¯¹é½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01382', 'title': 'PromptRL: Prompt Matters in RL for Flow-Based Image Generation', 'url': 'https://huggingface.co/papers/2602.01382', 'abstract': 'Flow matching models for text-to-image generation are enhanced through a reinforcement learning framework that addresses sample inefficiency and prompt overfitting by incorporating language models for prompt refinement, achieving superior performance with reduced computational requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t Flow matching models (FMs) have revolutionized text-to-image (T2I) generation, with reinforcement learning (RL) serving as a critical post-training strategy for alignment with reward objectives. In this research, we show that current RL pipelines for FMs suffer from two underappreciated yet important limitations: sample inefficiency due to insufficient generation diversity, and pronounced prompt overfitting, where models memorize specific training formulations and exhibit dramatic performance collapse when evaluated on semantically equivalent but stylistically varied prompts. We present PromptRL (Prompt Matters in RL for Flow-Based Image Generation), a framework that incorporates language models (LMs) as trainable prompt refinement agents directly within the flow-based RL optimization loop. This design yields two complementary benefits: rapid development of sophisticated prompt rewriting capabilities and, critically, a synergistic training regime that reshapes the optimization dynamics. PromptRL achieves state-of-the-art performance across multiple benchmarks, obtaining scores of 0.97 on GenEval, 0.98 on OCR accuracy, and 24.05 on PickScore.   Furthermore, we validate the effectiveness of our RL approach on large-scale image editing models, improving the EditReward of FLUX.1-Kontext from 1.19 to 1.43 with only 0.06 million rollouts, surpassing Gemini 2.5 Flash Image (also known as Nano Banana), which scores 1.37, and achieving comparable performance with ReasonNet (1.44), which relied on fine-grained data annotations along with a complex multi-stage training. Our extensive experiments empirically demonstrate that PromptRL consistently achieves higher performance ceilings while requiring over 2times fewer rollouts compared to naive flow-only RL. Our code is available at https://github.com/G-U-N/UniRL.', 'score': 5, 'issue_id': 884, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 1', 'zh': '2æœˆ1æ—¥'}, 'hash': '4a2019b4af9a1898', 'authors': ['Fu-Yun Wang', 'Han Zhang', 'Michael Gharbi', 'Hongsheng Li', 'Taesung Park'], 'affiliations': ['Meta Superintelligence Labs', 'Reve', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2602.01382.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#rl', '#rlhf', '#training'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞŸÑƒÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° PromptRL â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ flow matching Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ RL Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ¸Ğ»Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ†Ğ¸ĞºĞ» Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ flow-based RL, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¸Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ PromptRL Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ² Ğ´Ğ²Ğ° Ñ€Ğ°Ğ·Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ², Ñ‡ĞµĞ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ñ€ĞµĞºĞ¾Ñ€Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Enhancing Text-to-Image Generation with Prompt Refinement in RL', 'desc': "This paper introduces PromptRL, a novel framework that enhances flow matching models for text-to-image generation using reinforcement learning. It addresses two key issues: sample inefficiency, which limits the diversity of generated images, and prompt overfitting, where models fail to generalize to varied prompts. By integrating language models for prompt refinement within the reinforcement learning process, PromptRL improves the model's ability to generate diverse and high-quality images. The results show significant performance improvements on various benchmarks while reducing the computational resources needed for training."}, 'zh': {'title': 'æå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ™ºèƒ½åŒ–ï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œç§°ä¸ºPromptRLï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶æ¥è§£å†³æ ·æœ¬æ•ˆç‡ä½å’Œæç¤ºè¿‡æ‹Ÿåˆçš„é—®é¢˜ã€‚é€šè¿‡å°†è¯­è¨€æ¨¡å‹æ•´åˆåˆ°æµåŒ¹é…æ¨¡å‹çš„ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼ŒPromptRLèƒ½å¤Ÿå¿«é€Ÿæ”¹å†™æç¤ºï¼Œä»è€Œæé«˜ç”Ÿæˆçš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPromptRLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆè´¨é‡ï¼Œå¹¶å‡å°‘äº†è®¡ç®—èµ„æºçš„éœ€æ±‚ã€‚è¯¥æ–¹æ³•åœ¨å¤§è§„æ¨¡å›¾åƒç¼–è¾‘æ¨¡å‹ä¸Šä¹Ÿå–å¾—äº†è‰¯å¥½çš„æ•ˆæœï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.00986', 'title': 'Sparse Reward Subsystem in Large Language Models', 'url': 'https://huggingface.co/papers/2602.00986', 'abstract': "Research identifies a sparse reward subsystem in LLM hidden states containing value neurons that represent internal state expectations and dopamine-like neurons encoding reward prediction errors.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we identify a sparse reward subsystem within the hidden states of Large Language Models (LLMs), drawing an analogy to the biological reward subsystem in the human brain. We demonstrate that this subsystem contains value neurons that represent the model's internal expectation of state value, and through intervention experiments, we establish the importance of these neurons for reasoning. Our experiments reveal that these value neurons are robust across diverse datasets, model scales, and architectures; furthermore, they exhibit significant transferability across different datasets and models fine-tuned from the same base model. By examining cases where value predictions and actual rewards diverge, we identify dopamine neurons within the reward subsystem which encode reward prediction errors (RPE). These neurons exhibit high activation when the reward is higher than expected and low activation when the reward is lower than expected.", 'score': 5, 'issue_id': 884, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 1', 'zh': '2æœˆ1æ—¥'}, 'hash': '0306456f1e097ed2', 'authors': ['Guowei Xu', 'Mert Yuksekgonul', 'James Zou'], 'affiliations': ['Stanford University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2602.00986.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#transfer_learning', '#interpretability', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ’Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ¾Ğ´ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑÑ… Ğ±Ğ¾Ğ»ÑŒÑˆÑ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½ÑƒÑ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ·Ğ³Ğ°. Ğ’ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ñ‹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ñ‹ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ, Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ° Ğ¸Ñ… ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ€Ğ¾Ğ»ÑŒ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ñ‹ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑÑÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ´Ğ¾Ñ„Ğ°Ğ¼Ğ¸Ğ½Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ñ‹, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒÑÑ‰Ğ¸ĞµÑÑ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ»Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Unlocking the Reward System in Large Language Models', 'desc': "This paper explores a specific subsystem in the hidden states of Large Language Models (LLMs) that functions similarly to the reward system in the human brain. It identifies 'value neurons' that predict the expected value of states, which are crucial for the model's reasoning capabilities. The research shows that these neurons are consistent across various datasets and model architectures, indicating their robustness and transferability. Additionally, the study uncovers 'dopamine neurons' that signal reward prediction errors, highlighting their role in adjusting expectations based on actual rewards received."}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ç¨€ç–å¥–åŠ±å­ç³»ç»Ÿ', 'desc': 'æœ¬ç ”ç©¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„éšè—çŠ¶æ€ä¸­è¯†åˆ«å‡ºä¸€ä¸ªç¨€ç–å¥–åŠ±å­ç³»ç»Ÿï¼Œç±»ä¼¼äºäººè„‘ä¸­çš„ç”Ÿç‰©å¥–åŠ±å­ç³»ç»Ÿã€‚æˆ‘ä»¬å‘ç°è¯¥å­ç³»ç»ŸåŒ…å«ä»·å€¼ç¥ç»å…ƒï¼Œè¡¨ç¤ºæ¨¡å‹å¯¹çŠ¶æ€ä»·å€¼çš„å†…éƒ¨æœŸæœ›ï¼Œå¹¶é€šè¿‡å¹²é¢„å®éªŒè¯æ˜è¿™äº›ç¥ç»å…ƒåœ¨æ¨ç†ä¸­çš„é‡è¦æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™äº›ä»·å€¼ç¥ç»å…ƒåœ¨ä¸åŒæ•°æ®é›†ã€æ¨¡å‹è§„æ¨¡å’Œæ¶æ„ä¸­éƒ½è¡¨ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§ï¼Œå¹¶ä¸”åœ¨ä¸åŒæ•°æ®é›†å’Œä»åŒä¸€åŸºç¡€æ¨¡å‹å¾®è°ƒçš„æ¨¡å‹ä¹‹é—´å…·æœ‰æ˜¾è‘—çš„å¯è¿ç§»æ€§ã€‚æˆ‘ä»¬è¿˜å‘ç°å¥–åŠ±é¢„æµ‹è¯¯å·®ï¼ˆRPEï¼‰ç¼–ç çš„å¤šå·´èƒºç¥ç»å…ƒï¼Œå½“å¥–åŠ±é«˜äºé¢„æœŸæ—¶æ¿€æ´»å¼ºçƒˆï¼Œä½äºé¢„æœŸæ—¶æ¿€æ´»è¾ƒå¼±ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.00759', 'title': 'Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning', 'url': 'https://huggingface.co/papers/2602.00759', 'abstract': "Adaptive Ability Decomposing (AÂ²D) enhances reinforcement learning with verifiable rewards by decomposing complex questions into simpler sub-questions, improving LLM reasoning through guided exploration without requiring a teacher model.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) has shown great potential to enhance the reasoning ability of large language models (LLMs). However, due to the limited amount of information provided during the RLVR process, the model can only engage in largely blind exploration, which often results in failure on challenging problems. To provide additional information for the RLVR process without relying on a teacher model, we propose A^2D, an Adaptive Ability Decomposing method for enhancing the effectiveness of RLVR. Specifically, we first train a decomposer via RLVR without distillation, enabling it to decompose complex questions into a set of simpler sub-questions. Next, we use this decomposer to annotate sub-questions for each question in the training dataset, and then train the reasoner under RLVR with sub-question guidance. To better understand A^2D, we first compare its performance with competitive baselines, showing its effectiveness. Next, we observe that our method functions as a plug-and-play module that can be applied to different RLVR algorithms. Furthermore, we conduct an analysis of the decomposer, revealing how the RLVR process affects its performance and behavior, and which type of guidance is better suited for enhancing the reasoner's exploration and exploitation abilities.", 'score': 5, 'issue_id': 884, 'pub_date': '2026-01-31', 'pub_date_card': {'ru': '31 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 31', 'zh': '1æœˆ31æ—¥'}, 'hash': '4b82299bd9ee8e8f', 'authors': ['Zhipeng Chen', 'Xiaobo Qin', 'Wayne Xin Zhao', 'Youbin Wu', 'Ji-Rong Wen'], 'affiliations': ['ByteDance Seed', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2602.00759.jpg', 'data': {'categories': ['#optimization', '#rl', '#training', '#reasoning'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ¾Ñ‚ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğº Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Adaptive Ability Decomposing (AÂ²D) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·ĞµÑ€ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ· Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LLM Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ»ĞµĞ¿Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. AÂ²D Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğµ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Decomposing Complexity for Smarter Learning', 'desc': 'Adaptive Ability Decomposing (AÂ²D) is a method that improves reinforcement learning with verifiable rewards (RLVR) by breaking down complex questions into simpler sub-questions. This approach enhances the reasoning capabilities of large language models (LLMs) by providing structured guidance during the learning process, allowing for more effective exploration. AÂ²D trains a decomposer to create these sub-questions without needing a teacher model, which helps the main model learn better. The method has been shown to be effective compared to other techniques and can be integrated into various RLVR algorithms as a flexible module.'}, 'zh': {'title': 'è‡ªé€‚åº”èƒ½åŠ›åˆ†è§£ï¼Œæå‡æ¨ç†èƒ½åŠ›ï¼', 'desc': 'è‡ªé€‚åº”èƒ½åŠ›åˆ†è§£ï¼ˆAÂ²Dï¼‰æ˜¯ä¸€ç§å¢å¼ºå¼ºåŒ–å­¦ä¹ çš„æŠ€æœ¯ï¼Œé€šè¿‡å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºæ›´ç®€å•çš„å­é—®é¢˜ï¼Œæ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨ä¸ä¾èµ–æ•™å¸ˆæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰è¿‡ç¨‹ï¼Œæä¾›é¢å¤–çš„ä¿¡æ¯æ”¯æŒã€‚AÂ²Dé¦–å…ˆè®­ç»ƒä¸€ä¸ªåˆ†è§£å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å°†å¤æ‚é—®é¢˜åˆ†è§£ï¼Œå¹¶ä¸ºè®­ç»ƒæ•°æ®é›†ä¸­çš„æ¯ä¸ªé—®é¢˜æ ‡æ³¨å­é—®é¢˜ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒAÂ²Dèƒ½å¤Ÿä½œä¸ºä¸€ä¸ªå³æ’å³ç”¨çš„æ¨¡å—ï¼Œæå‡ä¸åŒRLVRç®—æ³•çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.02110', 'title': 'An Empirical Study of World Model Quantization', 'url': 'https://huggingface.co/papers/2602.02110', 'abstract': 'Post-training quantization effects in world models reveal unique failure modes and trade-offs between accuracy, bit-width, and planning performance, particularly in encoder-predictor module asymmetries and low-bit rollout stability.  \t\t\t\t\tAI-generated summary \t\t\t\t World models learn an internal representation of environment dynamics, enabling agents to simulate and reason about future states within a compact latent space for tasks such as planning, prediction, and inference. However, running world models rely on hevay computational cost and memory footprint, making model quantization essential for efficient deployment. To date, the effects of post-training quantization (PTQ) on world models remain largely unexamined. In this work, we present a systematic empirical study of world model quantization using DINO-WM as a representative case, evaluating diverse PTQ methods under both weight-only and joint weight-activation settings. We conduct extensive experiments on different visual planning tasks across a wide range of bit-widths, quantization granularities, and planning horizons up to 50 iterations. Our results show that quantization effects in world models extend beyond standard accuracy and bit-width trade-offs: group-wise weight quantization can stabilize low-bit rollouts, activation quantization granularity yields inconsistent benefits, and quantization sensitivity is highly asymmetric between encoder and predictor modules. Moreover, aggressive low-bit quantization significantly degrades the alignment between the planning objective and task success, leading to failures that cannot be remedied by additional optimization. These findings reveal distinct quantization-induced failure modes in world model-based planning and provide practical guidance for deploying quantized world models under strict computational constraints. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/QuantWM.', 'score': 4, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': '7cebcfb01623bc43', 'authors': ['Zhongqian Fu', 'Tianyi Zhao', 'Kai Han', 'Hang Zhou', 'Xinghao Chen', 'Yunhe Wang'], 'affiliations': ['Huawei Noahs Ark Lab'], 'pdf_title_img': 'assets/pdf/title_img/2602.02110.jpg', 'data': {'categories': ['#inference', '#rl', '#agents'], 'emoji': 'âš–ï¸', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑÑ‹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ¸Ñ€Ğ°: Ğ¾Ñ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° world models, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€ÑĞ´Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ±Ğ¸Ñ‚Ğ¾Ğ² Ğ¸ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹ Ğ¸ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑĞ¼Ğ¸ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞµÑĞ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€ÑĞ´Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ½Ğ¾ Ğ°Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ†ĞµĞ»ÑŒÑ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒÑĞ¿ĞµÑ…Ğ¾Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… world models Ğ¿Ğ¾Ğ´ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Navigating the Trade-offs of Quantization in World Models', 'desc': 'This paper investigates the impact of post-training quantization (PTQ) on world models, which are used for simulating and planning in environments. The authors conduct experiments to evaluate how different quantization methods affect model performance, particularly focusing on accuracy, bit-width, and planning capabilities. They discover that quantization can lead to unique failure modes, especially due to asymmetries between the encoder and predictor modules. The findings highlight the importance of careful quantization strategies to maintain performance while reducing computational costs.'}, 'zh': {'title': 'é‡åŒ–ä¸–ç•Œæ¨¡å‹ï¼šæ•ˆç‡ä¸å‡†ç¡®æ€§çš„å¹³è¡¡ä¹‹é“', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰å¯¹ä¸–ç•Œæ¨¡å‹çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡†ç¡®æ€§ã€ä½å®½å’Œè§„åˆ’æ€§èƒ½ä¹‹é—´çš„æƒè¡¡ã€‚ä¸–ç•Œæ¨¡å‹é€šè¿‡å­¦ä¹ ç¯å¢ƒåŠ¨æ€çš„å†…éƒ¨è¡¨ç¤ºï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨ç´§å‡‘çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ¨¡æ‹Ÿå’Œæ¨ç†ã€‚ç„¶è€Œï¼Œè¿è¡Œè¿™äº›æ¨¡å‹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œå†…å­˜ï¼Œå› æ­¤æ¨¡å‹é‡åŒ–å¯¹äºé«˜æ•ˆéƒ¨ç½²è‡³å…³é‡è¦ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œé‡åŒ–ä¸ä»…å½±å“æ ‡å‡†çš„å‡†ç¡®æ€§å’Œä½å®½ï¼Œè¿˜æ­ç¤ºäº†ç¼–ç å™¨å’Œé¢„æµ‹å™¨æ¨¡å—ä¹‹é—´çš„ä¸å¯¹ç§°æ€§ä»¥åŠä½ä½å®½å›æ»šçš„ç¨³å®šæ€§é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.02039', 'title': 'Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models', 'url': 'https://huggingface.co/papers/2602.02039', 'abstract': 'Agentic large language models require investigatory intelligence for autonomous data analysis, demonstrated through the Deep Data Research benchmark that evaluates their ability to extract insights from databases without explicit queries.  \t\t\t\t\tAI-generated summary \t\t\t\t The agency expected of Agentic Large Language Models goes beyond answering correctly, requiring autonomy to set goals and decide what to explore. We term this investigatory intelligence, distinguishing it from executional intelligence, which merely completes assigned tasks. Data Science provides a natural testbed, as real-world analysis starts from raw data rather than explicit queries, yet few benchmarks focus on it. To address this, we introduce Deep Data Research (DDR), an open-ended task where LLMs autonomously extract key insights from databases, and DDR-Bench, a large-scale, checklist-based benchmark that enables verifiable evaluation. Results show that while frontier models display emerging agency, long-horizon exploration remains challenging. Our analysis highlights that effective investigatory intelligence depends not only on agent scaffolding or merely scaling, but also on intrinsic strategies of agentic models.', 'score': 4, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': '392ca60694870d1e', 'authors': ['Wei Liu', 'Peijie Yu', 'Michele Orini', 'Yali Du', 'Yulan He'], 'affiliations': ['Kings College London', 'Tencent', 'The Alan Turing Institute'], 'pdf_title_img': 'assets/pdf/title_img/2602.02039.jpg', 'data': {'categories': ['#agents', '#benchmark', '#dataset'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚: Ğ¾Ñ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ñ†ĞµĞ»Ğ¸ Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ° Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ²Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Deep Data Research, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ñ‹ Ğ¸Ğ· Ğ±Ğ°Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ‚Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ñ‡Ğ°Ñ‚ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ñ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ¾ Ğ¸ Ğ¾Ñ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ ÑĞ°Ğ¼Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Empowering LLMs with Investigatory Intelligence for Autonomous Data Insights', 'desc': "This paper introduces the concept of investigatory intelligence in large language models (LLMs), which is the ability to autonomously analyze data and extract insights without specific queries. The authors present the Deep Data Research (DDR) benchmark, designed to evaluate LLMs' performance in this area by allowing them to explore databases and derive key insights independently. The study reveals that while advanced models show some level of agency, they struggle with long-term exploration tasks. The findings suggest that developing effective investigatory intelligence requires more than just improving model size; it also involves enhancing the intrinsic strategies used by these models."}, 'zh': {'title': 'è‡ªä¸»æ•°æ®åˆ†æçš„è°ƒæŸ¥æ™ºèƒ½', 'desc': 'æœ¬æ–‡æ¢è®¨äº†ä»£ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆAgentic LLMsï¼‰åœ¨è‡ªä¸»æ•°æ®åˆ†æä¸­æ‰€éœ€çš„è°ƒæŸ¥æ™ºèƒ½ã€‚è°ƒæŸ¥æ™ºèƒ½ä¸ä»…ä»…æ˜¯æ­£ç¡®å›ç­”é—®é¢˜ï¼Œè€Œæ˜¯èƒ½å¤Ÿè‡ªä¸»è®¾å®šç›®æ ‡å’Œå†³å®šæ¢ç´¢å†…å®¹ã€‚æˆ‘ä»¬æå‡ºäº†æ·±åº¦æ•°æ®ç ”ç©¶ï¼ˆDeep Data Research, DDRï¼‰åŸºå‡†ï¼Œè¯„ä¼°æ¨¡å‹ä»æ•°æ®åº“ä¸­æå–å…³é”®è§è§£çš„èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°½ç®¡å‰æ²¿æ¨¡å‹å±•ç°å‡ºä¸€å®šçš„è‡ªä¸»æ€§ï¼Œä½†åœ¨é•¿æ—¶é—´æ¢ç´¢æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.22588', 'title': 'Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models via Semantic Capacity Asymmetry', 'url': 'https://huggingface.co/papers/2601.22588', 'abstract': 'Small language models can effectively evaluate outputs by leveraging internal representations rather than generating responses, enabling a more efficient and interpretable evaluation approach through a probing-based framework.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are widely used as reference-free evaluators via prompting, but this "LLM-as-a-Judge" paradigm is costly, opaque, and sensitive to prompt design. In this work, we investigate whether smaller models can serve as efficient evaluators by leveraging internal representations instead of surface generation. We uncover a consistent empirical pattern: small LMs, despite with weak generative ability, encode rich evaluative signals in their hidden states. This motivates us to propose the Semantic Capacity Asymmetry Hypothesis: evaluation requires significantly less semantic capacity than generation and can be grounded in intermediate representations, suggesting that evaluation does not necessarily need to rely on large-scale generative models but can instead leverage latent features from smaller ones. Our findings motivate a paradigm shift from LLM-as-a-Judge to Representation-as-a-Judge, a decoding-free evaluation strategy that probes internal model structure rather than relying on prompted output. We instantiate this paradigm through INSPECTOR, a probing-based framework that predicts aspect-level evaluation scores from small model representations. Experiments on reasoning benchmarks (GSM8K, MATH, GPQA) show that INSPECTOR substantially outperforms prompting-based small LMs and closely approximates full LLM judges, while offering a more efficient, reliable, and interpretable alternative for scalable evaluation.', 'score': 4, 'issue_id': 884, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 30', 'zh': '1æœˆ30æ—¥'}, 'hash': '451f5716f6f71f8a', 'authors': ['Zhuochun Li', 'Yong Zhang', 'Ming Li', 'Yuelyu Ji', 'Yiming Zeng', 'Ning Cheng', 'Yun Zhu', 'Yanmeng Wang', 'Shaojun Wang', 'Jing Xiao', 'Daqing He'], 'affiliations': ['Ping An Technology (Shenzhen) Co., Ltd.', 'University of Connecticut', 'University of Maryland, College Park', 'University of Pittsburgh'], 'pdf_title_img': 'assets/pdf/title_img/2601.22588.jpg', 'data': {'categories': ['#benchmark', '#training', '#small_models'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° Ğ±ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑÑƒĞ´ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñƒ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‘Ğ¼ĞºĞ¾ÑÑ‚Ğ¸: Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ², Ñ‡ĞµĞ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ "LLM-as-a-Judge" Ğ¾Ğ½Ğ¸ Ğ²Ğ½ĞµĞ´Ñ€ÑÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ "Representation-as-a-Judge" â€” ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ· Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº INSPECTOR Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ½Ğ¾ Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'Unlocking Evaluation Efficiency with Small Models', 'desc': 'This paper explores how small language models can evaluate outputs more efficiently by using their internal representations instead of generating responses. The authors propose the Semantic Capacity Asymmetry Hypothesis, which suggests that evaluating text requires less semantic capacity than generating it. They introduce a new framework called INSPECTOR, which uses probing techniques to predict evaluation scores based on the hidden states of smaller models. The results show that INSPECTOR outperforms traditional prompting methods and provides a more interpretable and scalable evaluation approach.'}, 'zh': {'title': 'å°æ¨¡å‹çš„è¯„ä¼°æ–°æ€è·¯ï¼šå†…éƒ¨è¡¨ç¤ºä¼˜äºç”Ÿæˆ', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å°å‹è¯­è¨€æ¨¡å‹å¦‚ä½•åˆ©ç”¨å†…éƒ¨è¡¨ç¤ºè¿›è¡Œæœ‰æ•ˆè¯„ä¼°ï¼Œè€Œä¸æ˜¯ç”Ÿæˆå“åº”ã€‚æˆ‘ä»¬å‘ç°å°å‹æ¨¡å‹è™½ç„¶ç”Ÿæˆèƒ½åŠ›è¾ƒå¼±ï¼Œä½†å…¶éšè—çŠ¶æ€ä¸­ç¼–ç äº†ä¸°å¯Œçš„è¯„ä¼°ä¿¡å·ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†è¯­ä¹‰èƒ½åŠ›ä¸å¯¹ç§°å‡è®¾ï¼Œè®¤ä¸ºè¯„ä¼°æ‰€éœ€çš„è¯­ä¹‰èƒ½åŠ›è¿œä½äºç”Ÿæˆã€‚æˆ‘ä»¬çš„INSPECTORæ¡†æ¶é€šè¿‡æ¢æµ‹å°æ¨¡å‹çš„å†…éƒ¨ç»“æ„ï¼Œæä¾›äº†ä¸€ç§æ— éœ€ç”Ÿæˆçš„è¯„ä¼°ç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†è¯„ä¼°çš„æ•ˆç‡å’Œå¯è§£é‡Šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01842', 'title': 'Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models', 'url': 'https://huggingface.co/papers/2602.01842', 'abstract': "A new test-time scaling framework called Prism is introduced for discrete diffusion language models that improves reasoning performance through hierarchical trajectory search, local branching with partial remasking, and self-verified feedback mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Inference-time compute has re-emerged as a practical way to improve LLM reasoning. Most test-time scaling (TTS) algorithms rely on autoregressive decoding, which is ill-suited to discrete diffusion language models (dLLMs) due to their parallel decoding over the entire sequence. As a result, developing effective and efficient TTS methods to unlock dLLMs' full generative potential remains an underexplored challenge. To address this, we propose Prism (Pruning, Remasking, and Integrated Self-verification Method), an efficient TTS framework for dLLMs that (i) performs Hierarchical Trajectory Search (HTS) which dynamically prunes and reallocates compute in an early-to-mid denoising window, (ii) introduces Local branching with partial remasking to explore diverse implementations while preserving high-confidence tokens, and (iii) replaces external verifiers with Self-Verified Feedback (SVF) obtained via self-evaluation prompts on intermediate completions. Across four mathematical reasoning and code generation benchmarks on three dLLMs, including LLaDA 8B Instruct, Dream 7B Instruct, and LLaDA 2.0-mini, our Prism achieves a favorable performance-efficiency trade-off, matching best-of-N performance with substantially fewer function evaluations (NFE). The code is released at https://github.com/viiika/Prism.", 'score': 3, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': '8ccd8159c82c4354', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#inference', '#benchmark', '#reasoning', '#diffusion', '#optimization', '#open_source', '#training'], 'emoji': 'ğŸŒ³', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ğµ: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ„ĞµÑ€Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº', 'desc': "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Prism Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ°, Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¿ĞµÑ€Ğµmasking'Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² best-of-N Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹."}, 'en': {'title': 'Unlocking dLLMs: Efficient Reasoning with Prism', 'desc': "The paper introduces Prism, a new framework designed to enhance the reasoning capabilities of discrete diffusion language models (dLLMs) during inference. It employs a Hierarchical Trajectory Search (HTS) to optimize computational resources by dynamically pruning and reallocating them in the denoising process. Additionally, Prism utilizes Local Branching with Partial Remasking to maintain high-confidence tokens while exploring diverse outputs. Finally, it incorporates Self-Verified Feedback (SVF) to improve the model's self-evaluation, resulting in better performance with fewer function evaluations across various benchmarks."}, 'zh': {'title': 'Prismï¼šæå‡ç¦»æ•£æ‰©æ•£è¯­è¨€æ¨¡å‹æ¨ç†æ€§èƒ½çš„æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾æ¡†æ¶Prismï¼Œæ—¨åœ¨æé«˜ç¦»æ•£æ‰©æ•£è¯­è¨€æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†å±‚è½¨è¿¹æœç´¢ã€å±€éƒ¨åˆ†æ”¯ä¸éƒ¨åˆ†é‡æ©ç ä»¥åŠè‡ªæˆ‘éªŒè¯åé¦ˆæœºåˆ¶æ¥å®ç°æ›´é«˜æ•ˆçš„æ¨ç†ã€‚Prismèƒ½å¤Ÿåœ¨å»å™ªçª—å£çš„æ—©ä¸­æœŸåŠ¨æ€ä¿®å‰ªå’Œé‡æ–°åˆ†é…è®¡ç®—èµ„æºï¼ŒåŒæ—¶æ¢ç´¢å¤šæ ·åŒ–çš„å®ç°æ–¹å¼ã€‚é€šè¿‡åœ¨å¤šä¸ªæ•°å­¦æ¨ç†å’Œä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­éªŒè¯ï¼ŒPrismåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´è¾¾æˆäº†è‰¯å¥½çš„å¹³è¡¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01296', 'title': 'Interacted Planes Reveal 3D Line Mapping', 'url': 'https://huggingface.co/papers/2602.01296', 'abstract': 'LiP-Map presents a line-plane joint optimization framework that explicitly models learnable line and planar primitives for accurate 3D line mapping in man-made environments.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D line mapping from multi-view RGB images provides a compact and structured visual representation of scenes. We study the problem from a physical and topological perspective: a 3D line most naturally emerges as the edge of a finite 3D planar patch. We present LiP-Map, a line-plane joint optimization framework that explicitly models learnable line and planar primitives. This coupling enables accurate and detailed 3D line mapping while maintaining strong efficiency (typically completing a reconstruction in 3 to 5 minutes per scene). LiP-Map pioneers the integration of planar topology into 3D line mapping, not by imposing pairwise coplanarity constraints but by explicitly constructing interactions between plane and line primitives, thus offering a principled route toward structured reconstruction in man-made environments. On more than 100 scenes from ScanNetV2, ScanNet++, Hypersim, 7Scenes, and Tanks\\&Temple, LiP-Map improves both accuracy and completeness over state-of-the-art methods. Beyond line mapping quality, LiP-Map significantly advances line-assisted visual localization, establishing strong performance on 7Scenes. Our code is released at https://github.com/calmke/LiPMAP for reproducible research.', 'score': 3, 'issue_id': 884, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 1', 'zh': '2æœˆ1æ—¥'}, 'hash': '506f8c0e4443e23a', 'authors': ['Zeran Ke', 'Bin Tan', 'Gui-Song Xia', 'Yujun Shen', 'Nan Xue'], 'affiliations': ['Ant Group', 'School of Computer Science and the School of Artificial Intelligence, Wuhan University, Wuhan 430072, China', 'School of Computer Science, Wuhan University, Wuhan 430072, China'], 'pdf_title_img': 'assets/pdf/title_img/2602.01296.jpg', 'data': {'categories': ['#3d', '#cv'], 'emoji': 'ğŸ“', 'ru': {'title': 'Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ»Ğ¸Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ 3D-Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'LiP-Map Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ»Ğ¸Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ñ… RGB-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ²Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ñ‹ Ğ»Ğ¸Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‚ ĞºĞ°Ğº Ñ€Ñ‘Ğ±Ñ€Ğ° ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ¾ÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ’Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ°Ğ¼Ğ¸ Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ»Ğ¸Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ»Ğ¸Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ (Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ğ·Ğ°Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ 3-5 Ğ¼Ğ¸Ğ½ÑƒÑ‚). ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 100 ÑÑ†ĞµĞ½Ğ°Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ»Ğ¸Ğ½Ğ¸ÑÑ….'}, 'en': {'title': 'Revolutionizing 3D Line Mapping with LiP-Map', 'desc': 'LiP-Map introduces a novel framework for 3D line mapping that combines line and planar elements to enhance accuracy in man-made environments. By modeling learnable line and planar primitives, it effectively captures the relationship between lines and planes, which are essential for structured scene representation. The framework operates efficiently, completing reconstructions in just 3 to 5 minutes per scene, while significantly improving mapping quality on various datasets. Additionally, LiP-Map enhances line-assisted visual localization, demonstrating its effectiveness in practical applications.'}, 'zh': {'title': 'LiP-Mapï¼šé«˜æ•ˆç²¾ç¡®çš„3Dçº¿æ˜ å°„æ–°æ–¹æ³•', 'desc': 'LiP-Mapæå‡ºäº†ä¸€ç§çº¿é¢è”åˆä¼˜åŒ–æ¡†æ¶ï¼Œä¸“é—¨å»ºæ¨¡å¯å­¦ä¹ çš„çº¿å’Œé¢åŸè¯­ï¼Œä»¥å®ç°ç²¾ç¡®çš„3Dçº¿æ˜ å°„ã€‚è¯¥æ–¹æ³•ä»ç‰©ç†å’Œæ‹“æ‰‘çš„è§’åº¦ç ”ç©¶3Dçº¿çš„ç”Ÿæˆï¼Œè®¤ä¸º3Dçº¿æœ€è‡ªç„¶åœ°å‡ºç°åœ¨æœ‰é™çš„3Då¹³é¢ç‰‡çš„è¾¹ç¼˜ã€‚é€šè¿‡æ˜¾å¼æ„å»ºå¹³é¢å’Œçº¿åŸè¯­ä¹‹é—´çš„äº¤äº’ï¼ŒLiP-Mapåœ¨ä¿æŒé«˜æ•ˆæ€§çš„åŒæ—¶ï¼Œæä¾›äº†å‡†ç¡®ä¸”è¯¦ç»†çš„3Dçº¿æ˜ å°„ã€‚ç»è¿‡åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æµ‹è¯•ï¼ŒLiP-Mapåœ¨å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶æ˜¾è‘—æå‡äº†åŸºäºçº¿çš„è§†è§‰å®šä½æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01077', 'title': 'PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers', 'url': 'https://huggingface.co/papers/2602.01077', 'abstract': 'PISA is a novel sparse attention method that improves diffusion transformer efficiency by approximating non-critical attention blocks instead of discarding them, achieving faster processing with maintained quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Transformers are fundamental for video and image generation, but their efficiency is bottlenecked by the quadratic complexity of attention. While block sparse attention accelerates computation by attending only critical key-value blocks, it suffers from degradation at high sparsity by discarding context. In this work, we discover that attention scores of non-critical blocks exhibit distributional stability, allowing them to be approximated accurately and efficiently rather than discarded, which is essentially important for sparse attention design. Motivated by this key insight, we propose PISA, a training-free Piecewise Sparse Attention that covers the full attention span with sub-quadratic complexity. Unlike the conventional keep-or-drop paradigm that directly drop the non-critical block information, PISA introduces a novel exact-or-approximate strategy: it maintains exact computation for critical blocks while efficiently approximating the remainder through block-wise Taylor expansion. This design allows PISA to serve as a faithful proxy to full attention, effectively bridging the gap between speed and quality. Experimental results demonstrate that PISA achieves 1.91 times and 2.57 times speedups on Wan2.1-14B and Hunyuan-Video, respectively, while consistently maintaining the highest quality among sparse attention methods. Notably, even for image generation on FLUX, PISA achieves a 1.2 times acceleration without compromising visual quality. Code is available at: https://github.com/xie-lab-ml/piecewise-sparse-attention.', 'score': 3, 'issue_id': 884, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 1', 'zh': '2æœˆ1æ—¥'}, 'hash': '20464de619792c9d', 'authors': ['Haopeng Li', 'Shitong Shao', 'Wenliang Zhong', 'Zikai Zhou', 'Lichen Bai', 'Hui Xiong', 'Zeke Xie'], 'affiliations': ['The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2602.01077.jpg', 'data': {'categories': ['#multimodal', '#video', '#architecture', '#inference', '#diffusion', '#optimization'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğ³Ğ¾, Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ PISA, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ·Ğ° ÑÑ‡ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½ĞµĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸Ñ… Ğ¾Ñ‚Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ½ĞµĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±Ğ»Ğ¾ĞºĞ°Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ. PISA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ exact-or-approximate: ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¢ĞµĞ¹Ğ»Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² 1.91-2.57 Ñ€Ğ°Ğ· Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'PISA: Efficient Attention Without Compromise', 'desc': 'PISA is a new method for sparse attention in diffusion transformers that enhances efficiency by approximating non-critical attention blocks instead of discarding them. This approach addresses the inefficiencies caused by the quadratic complexity of traditional attention mechanisms, which can slow down processing. By leveraging the stable distribution of attention scores in non-critical blocks, PISA maintains high-quality outputs while significantly speeding up computations. The method achieves impressive speedups in various applications, demonstrating its effectiveness in balancing performance and quality in machine learning tasks.'}, 'zh': {'title': 'PISAï¼šæå‡æ‰©æ•£å˜æ¢å™¨æ•ˆç‡çš„æ–°æ–¹æ³•', 'desc': 'PISAæ˜¯ä¸€ç§æ–°é¢–çš„ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ï¼Œé€šè¿‡è¿‘ä¼¼éå…³é”®æ³¨æ„åŠ›å—è€Œä¸æ˜¯ç›´æ¥ä¸¢å¼ƒå®ƒä»¬ï¼Œæ¥æé«˜æ‰©æ•£å˜æ¢å™¨çš„æ•ˆç‡ã€‚è¿™ç§æ–¹æ³•åœ¨ä¿æŒè´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†æ›´å¿«çš„å¤„ç†é€Ÿåº¦ã€‚PISAé‡‡ç”¨äº†ä¸€ç§æ–°çš„ç²¾ç¡®æˆ–è¿‘ä¼¼ç­–ç•¥ï¼Œå¯¹äºå…³é”®å—ä¿æŒç²¾ç¡®è®¡ç®—ï¼Œè€Œé€šè¿‡å—çŠ¶æ³°å‹’å±•å¼€æœ‰æ•ˆåœ°è¿‘ä¼¼å…¶ä½™éƒ¨åˆ†ã€‚è¿™ä½¿å¾—PISAèƒ½å¤Ÿåœ¨é€Ÿåº¦å’Œè´¨é‡ä¹‹é—´æ¶èµ·æ¡¥æ¢ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒäº†æœ€é«˜çš„ç”Ÿæˆè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.22674', 'title': 'VisionTrim: Unified Vision Token Compression for Training-Free MLLM Acceleration', 'url': 'https://huggingface.co/papers/2601.22674', 'abstract': 'VisionTrim is a training-free framework that accelerates multimodal large language models by selecting dominant visual tokens and merging them with text-guided complementation, improving efficiency without performance loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) suffer from high computational costs due to excessive visual tokens, particularly in high-resolution and video-based scenarios. Existing token reduction methods typically focus on isolated pipeline components and often neglect textual alignment, leading to performance degradation. In this paper, we propose VisionTrim, a unified framework for training-free MLLM acceleration, integrating two effective plug-and-play modules: 1) the Dominant Vision Token Selection (DVTS) module, which preserves essential visual tokens via a global-local view, and 2) the Text-Guided Vision Complement (TGVC) module, which facilitates context-aware token merging guided by textual cues. Extensive experiments across diverse image and video multimodal benchmarks demonstrate the performance superiority of our VisionTrim, advancing practical MLLM deployment in real-world applications. The code is available at: https://github.com/hanxunyu/VisionTrim.', 'score': 3, 'issue_id': 884, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 30', 'zh': '1æœˆ30æ—¥'}, 'hash': '0da66181c47566c8', 'authors': ['Hanxun Yu', 'Wentong Li', 'Xuan Qu', 'Song Wang', 'Junbo Chen', 'Jianke Zhu'], 'affiliations': ['Nanjing University of Aeronautics and Astronautics', 'Udeer.ai', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.22674.jpg', 'data': {'categories': ['#multimodal', '#inference', '#video'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¼Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²', 'desc': 'VisionTrim Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ²Ğ° Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ: Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ MLLM Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Accelerating MLLMs with VisionTrim: Efficiency Meets Performance', 'desc': 'VisionTrim is a novel framework designed to enhance the efficiency of multimodal large language models (MLLMs) without the need for training. It addresses the challenge of high computational costs caused by an abundance of visual tokens, especially in high-resolution and video contexts. The framework introduces two key modules: the Dominant Vision Token Selection (DVTS) module, which identifies and retains the most important visual tokens, and the Text-Guided Vision Complement (TGVC) module, which merges visual tokens with textual information for better context understanding. Through extensive testing, VisionTrim has shown to improve performance while reducing resource usage, making it suitable for real-world applications.'}, 'zh': {'title': 'VisionTrimï¼šé«˜æ•ˆåŠ é€Ÿå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§£å†³æ–¹æ¡ˆ', 'desc': 'VisionTrimæ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œæ—¨åœ¨åŠ é€Ÿå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œé€šè¿‡é€‰æ‹©ä¸»è¦çš„è§†è§‰æ ‡è®°å¹¶ä¸æ–‡æœ¬å¼•å¯¼çš„è¡¥å……ç›¸ç»“åˆï¼Œæé«˜æ•ˆç‡è€Œä¸æŸå¤±æ€§èƒ½ã€‚è¯¥æ¡†æ¶è§£å†³äº†é«˜åˆ†è¾¨ç‡å’Œè§†é¢‘åœºæ™¯ä¸­è§†è§‰æ ‡è®°è¿‡å¤šå¯¼è‡´çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºçš„Dominant Vision Token Selectionï¼ˆDVTSï¼‰æ¨¡å—èƒ½å¤Ÿé€šè¿‡å…¨å±€å’Œå±€éƒ¨è§†è§’ä¿ç•™é‡è¦çš„è§†è§‰æ ‡è®°ï¼Œè€ŒText-Guided Vision Complementï¼ˆTGVCï¼‰æ¨¡å—åˆ™é€šè¿‡æ–‡æœ¬çº¿ç´¢å®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ ‡è®°åˆå¹¶ã€‚é€šè¿‡åœ¨å¤šç§å›¾åƒå’Œè§†é¢‘åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒï¼ŒVisionTrimå±•ç¤ºäº†å…¶å“è¶Šçš„æ€§èƒ½ï¼Œæ¨åŠ¨äº†MLLMåœ¨å®é™…åº”ç”¨ä¸­çš„éƒ¨ç½²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01997', 'title': 'On the Limits of Layer Pruning for Generative Reasoning in LLMs', 'url': 'https://huggingface.co/papers/2602.01997', 'abstract': 'Layer pruning compresses large language models while maintaining classification performance but causes significant degradation in generative reasoning tasks, with limited recovery possible through supervised finetuning on self-generated responses.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent works have shown that layer pruning can compress large language models (LLMs) while retaining strong performance on classification benchmarks with little or no finetuning. However, existing pruning techniques often suffer severe degradation on generative reasoning tasks. Through a systematic study across multiple model families, we find that tasks requiring multi-step reasoning are particularly sensitive to depth reduction. Beyond surface-level text degeneration, we observe degradation of critical algorithmic capabilities, including arithmetic computation for mathematical reasoning and balanced parenthesis generation for code synthesis. Under realistic post-training constraints, without access to pretraining-scale data or compute, we evaluate a simple mitigation strategy based on supervised finetuning with Self-Generated Responses. This approach achieves strong recovery on classification tasks, retaining up to 90\\% of baseline performance, and yields substantial gains of up to 20--30 percentage points on generative benchmarks compared to prior post-pruning techniques. Crucially, despite these gains, recovery for generative reasoning remains fundamentally limited relative to classification tasks and is viable primarily at lower pruning ratios. Overall, we characterize the practical limits of layer pruning for generative reasoning and provide guidance on when depth reduction can be applied effectively under constrained post-training regimes.', 'score': 2, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': 'abc666c194fd4a45', 'authors': ['Safal Shrestha', 'Anubhav Shrestha', 'Aadim Nepal', 'Minwu Kim', 'Keith Ross'], 'affiliations': ['Department of Computer Science, New York University Abu Dhabi'], 'pdf_title_img': 'assets/pdf/title_img/2602.01997.jpg', 'data': {'categories': ['#inference', '#benchmark', '#reasoning', '#optimization', '#training'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'ĞŸÑ€ĞµĞ´ĞµĞ»Ñ‹ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ, Ñ‡ĞµĞ¼ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ñ‘Ğ² Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ, Ñ‡Ñ‚Ğ¾ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾ÑĞ»Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ, Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ¸Ñ€ÑƒÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµÑ€Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ°Ğ¼Ğ¾Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ´Ğ¾ 90% Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ½Ğ° 20-30 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ°. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ… Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Balancing Compression and Reasoning in Language Models', 'desc': 'This paper investigates the effects of layer pruning on large language models (LLMs), which is a technique used to reduce model size while trying to keep performance intact. The authors find that while classification tasks can maintain performance after pruning, generative reasoning tasks suffer significantly, especially those requiring multi-step reasoning. They propose a supervised finetuning method using self-generated responses to recover some performance, achieving notable improvements in classification tasks and moderate gains in generative tasks. However, the recovery for generative reasoning remains limited, highlighting the challenges of applying layer pruning effectively in these scenarios.'}, 'zh': {'title': 'å±‚å‰ªæï¼šå‹ç¼©ä¸ç”Ÿæˆæ¨ç†çš„å¹³è¡¡', 'desc': 'å±‚å‰ªææ˜¯ä¸€ç§å‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¿æŒåˆ†ç±»æ€§èƒ½çš„åŒæ—¶å‡å°‘æ¨¡å‹çš„å¤§å°ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•åœ¨ç”Ÿæˆæ¨ç†ä»»åŠ¡ä¸­ä¼šå¯¼è‡´æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤šæ­¥æ¨ç†çš„ä»»åŠ¡ä¸­ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡é€šè¿‡è‡ªç”Ÿæˆå“åº”çš„ç›‘ç£å¾®è°ƒå¯ä»¥éƒ¨åˆ†æ¢å¤æ€§èƒ½ï¼Œä½†åœ¨ç”Ÿæˆä»»åŠ¡ä¸­çš„æ¢å¤èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡ç³»ç»Ÿè¯„ä¼°äº†å±‚å‰ªæå¯¹ç”Ÿæˆæ¨ç†çš„å½±å“ï¼Œå¹¶æä¾›äº†åœ¨åè®­ç»ƒæ¡ä»¶ä¸‹æœ‰æ•ˆåº”ç”¨æ·±åº¦å‡å°‘çš„æŒ‡å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01984', 'title': 'Enhancing Multi-Image Understanding through Delimiter Token Scaling', 'url': 'https://huggingface.co/papers/2602.01984', 'abstract': "Scaling hidden states of delimiter tokens in vision-language models reduces cross-image information leakage and improves multi-image reasoning performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance declines when multiple images are provided as input. One major reason is the cross-image information leakage, where the model struggles to distinguish information across different images. Existing LVLMs already employ delimiter tokens to mark the start and end of each image, yet our analysis reveals that these tokens fail to effectively block cross-image information leakage. To enhance their effectiveness, we propose a method that scales the hidden states of delimiter tokens. This enhances the model's ability to preserve image-specific information by reinforcing intra-image interaction and limiting undesired cross-image interactions. Consequently, the model is better able to distinguish between images and reason over them more accurately. Experiments show performance gains on multi-image benchmarks such as Mantis, MuirBench, MIRB, and QBench2. We further evaluate our method on text-only tasks that require clear distinction. The method improves performance on multi-document and multi-table understanding benchmarks, including TQABench, MultiNews, and WCEP-10. Notably, our method requires no additional training or inference cost.", 'score': 2, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': '4c3d0a95f670cefb', 'authors': ['Minyoung Lee', 'Yeji Park', 'Dongjun Hwang', 'Yejin Kim', 'Seong Joon Oh', 'Junsuk Choe'], 'affiliations': ['KAIST', 'Sogang University', 'Tubingen University'], 'pdf_title_img': 'assets/pdf/title_img/2602.01984.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#leakage', '#benchmark', '#reasoning'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²-Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ÑÑ‚ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ².'}, 'en': {'title': 'Enhancing Image Distinction in Vision-Language Models', 'desc': 'This paper addresses the issue of cross-image information leakage in Large Vision-Language Models (LVLMs) when processing multiple images. The authors propose a novel approach that scales the hidden states of delimiter tokens, which are used to separate images in the input. By enhancing these tokens, the model can better maintain image-specific information and reduce confusion between different images. The results show significant improvements in multi-image reasoning tasks and also benefit text-only tasks without incurring extra training or inference costs.'}, 'zh': {'title': 'æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å¤šå›¾åƒæ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡è°ƒæ•´åˆ†éš”ç¬¦ä»¤ç‰Œçš„éšè—çŠ¶æ€ï¼Œæ¥å‡å°‘è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„è·¨å›¾åƒä¿¡æ¯æ³„æ¼é—®é¢˜ã€‚ç°æœ‰çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å•å›¾åƒä»»åŠ¡æ—¶è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤šå›¾åƒè¾“å…¥æ—¶æ€§èƒ½ä¸‹é™ï¼Œä¸»è¦æ˜¯å› ä¸ºæ¨¡å‹æ— æ³•æœ‰æ•ˆåŒºåˆ†ä¸åŒå›¾åƒçš„ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¢å¼ºäº†å›¾åƒå†…éƒ¨çš„äº¤äº’ï¼ŒåŒæ—¶é™åˆ¶äº†ä¸å¿…è¦çš„è·¨å›¾åƒäº¤äº’ï¼Œä»è€Œæé«˜äº†æ¨¡å‹å¯¹å›¾åƒç‰¹å®šä¿¡æ¯çš„ä¿ç•™èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šå›¾åƒæ¨ç†å’Œæ–‡æœ¬ç†è§£ä»»åŠ¡ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01983', 'title': 'Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning', 'url': 'https://huggingface.co/papers/2602.01983', 'abstract': "A training-free framework enables language model agents to automatically create and optimize tools during inference, improving their reasoning capabilities through self-evolution and memory consolidation.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing Tool-Integrated Reasoning (TIR) models have effectively extended the question-answering capabilities of LLMs by incorporating external tools. However, real-world scenarios present numerous open-ended problems where fixed tools often fail to meet task requirements. Furthermore, the lack of self-optimization mechanisms means that erroneous tool outputs can mislead the LLM's responses. Additionally, the construction of existing tools entails significant manual effort, which consequently constrains their applicability. Recognizing that the reasoning traces of LLMs encapsulate implicit problem-solving capabilities, we propose UCT, a novel training-free framework that transforms agents from tool users to tool creators. This approach harvests reasoning experiences and distills them into reusable assets. This method transforms the agent from a mere tool user into a tool creator, enabling adaptive tool creation and self-updating during the inference process. We also introduce a memory consolidation mechanism to maintain the tool library, ensuring high reusability of retained experiential memory for subsequent reasoning tasks. This novel automated tool construction paradigm continuously improves tool quality during reasoning, allowing the overall agent system to progress without additional training. Extensive experiments demonstrate that our method serves as a novel paradigm for enhancing the capabilities of TIR models. In particular, the significant performance gains achieved +20.86%uparrow and +23.04%uparrow on benchmarks across multi-domain mathematical and scientific reasoning tasks validate the self-evolving capability of the agent.", 'score': 2, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': 'a375d4829a4dd2fa', 'authors': ['Xintian Shen', 'Jiawei Chen', 'Lihao Zheng', 'Hao Ma', 'Tao Wei', 'Kun Zhan'], 'affiliations': ['Li Auto'], 'pdf_title_img': 'assets/pdf/title_img/2602.01983.jpg', 'data': {'categories': [], 'emoji': 'ğŸ› ï¸', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚, ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ²Ğ¾Ğ¸ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ°æ¡†æ¶UCT, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ· Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¸Ñ… ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ÑƒÑ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 20-23% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering Language Models: From Tool Users to Tool Creators', 'desc': 'This paper introduces a training-free framework called UCT that allows language model agents to create and optimize their own tools during inference. By leveraging their reasoning experiences, these agents can evolve and improve their problem-solving capabilities without needing additional training. The framework includes a memory consolidation mechanism that helps maintain a library of tools, ensuring they are reusable for future tasks. Experimental results show that this approach significantly enhances the performance of existing Tool-Integrated Reasoning models in various reasoning tasks.'}, 'zh': {'title': 'æ— è®­ç»ƒæ¡†æ¶ï¼šä»å·¥å…·ä½¿ç”¨è€…åˆ°åˆ›é€ è€…çš„è½¬å˜', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— è®­ç»ƒæ¡†æ¶UCTï¼Œä½¿è¯­è¨€æ¨¡å‹ä»£ç†èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªåŠ¨åˆ›å»ºå’Œä¼˜åŒ–å·¥å…·ï¼Œä»è€Œæå‡å…¶æ¨ç†èƒ½åŠ›ã€‚ç°æœ‰çš„å·¥å…·é›†æˆæ¨ç†æ¨¡å‹åœ¨é—®ç­”èƒ½åŠ›ä¸Šæœ‰æ‰€æ‰©å±•ï¼Œä½†åœ¨é¢å¯¹å¼€æ”¾æ€§é—®é¢˜æ—¶ï¼Œå›ºå®šå·¥å…·å¾€å¾€æ— æ³•æ»¡è¶³éœ€æ±‚ã€‚UCTæ¡†æ¶é€šè¿‡æå–æ¨ç†ç»éªŒï¼Œå°†å…¶è½¬åŒ–ä¸ºå¯é‡ç”¨çš„èµ„äº§ï¼Œä½¿ä»£ç†ä»å·¥å…·ä½¿ç”¨è€…è½¬å˜ä¸ºå·¥å…·åˆ›é€ è€…ã€‚è¯¥æ–¹æ³•è¿˜å¼•å…¥äº†è®°å¿†å·©å›ºæœºåˆ¶ï¼Œç¡®ä¿å·¥å…·åº“çš„é«˜é‡ç”¨æ€§ï¼Œä»è€Œåœ¨æ¨ç†ä»»åŠ¡ä¸­æŒç»­æé«˜å·¥å…·è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.23000', 'title': 'Mano: Restriking Manifold Optimization for LLM Training', 'url': 'https://huggingface.co/papers/2601.23000', 'abstract': "A novel optimizer called Mano is proposed that combines manifold optimization with momentum projection onto tangent spaces, achieving superior performance over AdamW and Muon while reducing memory and computational requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t While large language models (LLMs) have emerged as a significant advancement in artificial intelligence, the hardware and computational costs for training LLMs are also significantly burdensome. Among the state-of-the-art optimizers, AdamW relies on diagonal curvature estimates and ignores structural properties, while Muon applies global spectral normalization at the expense of losing curvature information. In this study, we restriked manifold optimization methods for training LLMs, which may address both optimizers' limitations, while conventional manifold optimization methods have been largely overlooked due to the poor performance in large-scale model optimization. By innovatively projecting the momentum onto the tangent space of model parameters and constraining it on a rotational Oblique manifold, we propose a novel, powerful, and efficient optimizer **Mano** that is the first to bridge the performance gap between manifold optimization and modern optimizers. Extensive experiments on the LLaMA and Qwen3 models demonstrate that Mano consistently and significantly outperforms AdamW and Muon even with less memory consumption and computational complexity, respectively, suggesting an expanded Pareto frontier in terms of space and time efficiency.", 'score': 2, 'issue_id': 884, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 30', 'zh': '1æœˆ30æ—¥'}, 'hash': 'bfee95549424c0a0', 'authors': ['Yufei Gu', 'Zeke Xie'], 'affiliations': ['1xLeaF Lab, The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2601.23000.jpg', 'data': {'categories': ['#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'ĞœĞ°Ğ½Ğ¸Ñ„Ğ¾Ğ»ÑŒĞ´Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸ĞµĞ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Mano, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ÑÑ… Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸ĞµĞ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ° Ğ½Ğ° ĞºĞ°ÑĞ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ AdamW, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ°Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºÑ€Ğ¸Ğ²Ğ¸Ğ·Ğ½Ñ‹, Ğ¸ Muon, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‰ĞµĞ³Ğ¾ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Mano ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ĞºÑ€Ğ¸Ğ²Ğ¸Ğ·Ğ½Ğµ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… LLaMA Ğ¸ Qwen Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Mano Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ².'}, 'en': {'title': 'Mano: Bridging Efficiency and Performance in Optimizers', 'desc': 'The paper introduces a new optimizer named Mano, which integrates manifold optimization techniques with momentum projection onto tangent spaces. This approach enhances the training of large language models (LLMs) by addressing the limitations of existing optimizers like AdamW and Muon, which either overlook structural properties or sacrifice curvature information. Mano is designed to be more efficient, requiring less memory and computational resources while still achieving superior performance. Experimental results on models such as LLaMA and Qwen3 show that Mano significantly outperforms its predecessors, expanding the efficiency frontier in model training.'}, 'zh': {'title': 'Manoï¼šé«˜æ•ˆæµå½¢ä¼˜åŒ–çš„æ–°é€‰æ‹©', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹ä¼˜åŒ–å™¨Manoï¼Œå®ƒç»“åˆäº†æµå½¢ä¼˜åŒ–å’ŒåŠ¨é‡æŠ•å½±åˆ°åˆ‡ç©ºé—´çš„æ–¹æ³•ã€‚Manoåœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œè¡¨ç°ä¼˜äºç°æœ‰çš„AdamWå’ŒMuonä¼˜åŒ–å™¨ï¼ŒåŒæ—¶å‡å°‘äº†å†…å­˜å’Œè®¡ç®—éœ€æ±‚ã€‚é€šè¿‡åˆ›æ–°æ€§åœ°å°†åŠ¨é‡æŠ•å½±åˆ°æ¨¡å‹å‚æ•°çš„åˆ‡ç©ºé—´ï¼Œå¹¶åœ¨æ—‹è½¬æ–œæµå½¢ä¸Šè¿›è¡Œçº¦æŸï¼ŒManoæˆåŠŸå¼¥è¡¥äº†æµå½¢ä¼˜åŒ–ä¸ç°ä»£ä¼˜åŒ–å™¨ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒManoåœ¨LLaMAå’ŒQwen3æ¨¡å‹ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†ç©ºé—´å’Œæ—¶é—´æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.00130', 'title': 'On the Relationship Between Representation Geometry and Generalization in Deep Neural Networks', 'url': 'https://huggingface.co/papers/2602.00130', 'abstract': 'Effective dimension, an unsupervised geometric metric, strongly predicts neural network performance across different architectures and domains, showing bidirectional causality between representation geometry and accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t We investigate the relationship between representation geometry and neural network performance. Analyzing 52 pretrained ImageNet models across 13 architecture families, we show that effective dimension -- an unsupervised geometric metric -- strongly predicts accuracy. Output effective dimension achieves partial r=0.75 (p < 10^(-10)) after controlling for model capacity, while total compression achieves partial r=-0.72. These findings replicate across ImageNet and CIFAR-10, and generalize to NLP: effective dimension predicts performance for 8 encoder models on SST-2/MNLI and 15 decoder-only LLMs on AG News (r=0.69, p=0.004), while model size does not (r=0.07). We establish bidirectional causality: degrading geometry via noise causes accuracy loss (r=-0.94, p < 10^(-9)), while improving geometry via PCA maintains accuracy across architectures (-0.03pp at 95% variance). This relationship is noise-type agnostic -- Gaussian, Uniform, Dropout, and Salt-and-pepper noise all show |r| > 0.90. These results establish that effective dimension provides domain-agnostic predictive and causal information about neural network performance, computed entirely without labels.', 'score': 2, 'issue_id': 884, 'pub_date': '2026-01-28', 'pub_date_card': {'ru': '28 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 28', 'zh': '1æœˆ28æ—¥'}, 'hash': '45d9fbceb8b12d6a', 'authors': ['Sumit Yadav'], 'affiliations': ['Institute of Engineering, Pulchowk Campus, Tribhuvan University, Nepal'], 'pdf_title_img': 'assets/pdf/title_img/2602.00130.jpg', 'data': {'categories': ['#architecture', '#cv', '#benchmark', '#interpretability', '#training'], 'emoji': 'ğŸ“', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¾Ñ€ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ â€” ÑÑ‚Ğ¾ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 52 Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ImageNet, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ NLP, Ğ³Ğ´Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ¼ (r=0.69). Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ° Ğ´Ğ²ÑƒÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ÑÑ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ: Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ÑˆÑƒĞ¼ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· PCA ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Geometry Matters: Effective Dimension Predicts Neural Network Success', 'desc': "This paper explores how the geometry of representations in neural networks relates to their performance. It introduces 'effective dimension', a geometric metric that can predict the accuracy of various neural network architectures without needing labeled data. The study shows a strong correlation between effective dimension and accuracy across different datasets, including ImageNet and CIFAR-10, as well as in natural language processing tasks. Additionally, it establishes a two-way relationship where changes in representation geometry directly affect model accuracy, regardless of the type of noise introduced."}, 'zh': {'title': 'æœ‰æ•ˆç»´åº¦ï¼šé¢„æµ‹ç¥ç»ç½‘ç»œæ€§èƒ½çš„å…³é”®æŒ‡æ ‡', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†è¡¨ç¤ºå‡ ä½•ä¸ç¥ç»ç½‘ç»œæ€§èƒ½ä¹‹é—´çš„å…³ç³»ã€‚é€šè¿‡åˆ†æ52ä¸ªé¢„è®­ç»ƒçš„ImageNetæ¨¡å‹ï¼Œå‘ç°æœ‰æ•ˆç»´åº¦è¿™ä¸€æ— ç›‘ç£å‡ ä½•åº¦é‡èƒ½å¤Ÿå¼ºçƒˆé¢„æµ‹æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæœ‰æ•ˆç»´åº¦ä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´å­˜åœ¨åŒå‘å› æœå…³ç³»ï¼Œå‡ ä½•ç»“æ„çš„é€€åŒ–ä¼šå¯¼è‡´å‡†ç¡®æ€§ä¸‹é™ï¼Œè€Œé€šè¿‡ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰æ”¹å–„å‡ ä½•ç»“æ„åˆ™èƒ½ä¿æŒå‡†ç¡®æ€§ã€‚è¿™ä¸€å‘ç°é€‚ç”¨äºå¤šä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œè¡¨æ˜æœ‰æ•ˆç»´åº¦åœ¨æ²¡æœ‰æ ‡ç­¾çš„æƒ…å†µä¸‹æä¾›äº†å…³äºç¥ç»ç½‘ç»œæ€§èƒ½çš„é¢„æµ‹å’Œå› æœä¿¡æ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.02354', 'title': 'Implicit neural representation of textures', 'url': 'https://huggingface.co/papers/2602.02354', 'abstract': 'Implicit neural representations operate continuously over UV coordinate space, demonstrating good image quality while balancing memory usage and rendering time, with applications in real-time rendering and downstream tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Implicit neural representation (INR) has proven to be accurate and efficient in various domains. In this work, we explore how different neural networks can be designed as a new texture INR, which operates in a continuous manner rather than a discrete one over the input UV coordinate space. Through thorough experiments, we demonstrate that these INRs perform well in terms of image quality, with considerable memory usage and rendering inference time. We analyze the balance between these objectives. In addition, we investigate various related applications in real-time rendering and down-stream tasks, e.g. mipmap fitting and INR-space generation.', 'score': 1, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': '8ed63e5840c69fcc', 'authors': ['Albert Kwok', 'Zheyuan Hu', 'Dounia Hammou'], 'affiliations': ['University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2602.02354.jpg', 'data': {'categories': [], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ (INR) Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ UV-ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞµĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ»ĞµĞ¼Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼, Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»-Ñ‚Ğ°Ğ¹Ğ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğµ Ğ¸ ÑĞ¼ĞµĞ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ mipmaps Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ INR.'}, 'en': {'title': 'Efficient Image Quality with Implicit Neural Representations', 'desc': 'This paper discusses implicit neural representations (INRs) that work continuously over UV coordinate space, which helps in achieving high-quality images while optimizing memory and rendering time. The authors propose a new texture INR design that enhances performance in real-time rendering applications. Through extensive experiments, they show that these INRs maintain excellent image quality while being efficient in memory usage and inference speed. The study also explores various applications of INRs, including mipmap fitting and INR-space generation, highlighting their versatility in downstream tasks.'}, 'zh': {'title': 'éšå¼ç¥ç»è¡¨ç¤ºï¼šé«˜æ•ˆæ¸²æŸ“çš„æ–°æ–¹æ³•', 'desc': 'éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰åœ¨UVåæ ‡ç©ºé—´ä¸­ä»¥è¿ç»­æ–¹å¼æ“ä½œï¼Œå±•ç°å‡ºè‰¯å¥½çš„å›¾åƒè´¨é‡ï¼ŒåŒæ—¶å¹³è¡¡äº†å†…å­˜ä½¿ç”¨å’Œæ¸²æŸ“æ—¶é—´ã€‚æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•è®¾è®¡ä¸åŒçš„ç¥ç»ç½‘ç»œä½œä¸ºæ–°çš„çº¹ç†éšå¼ç¥ç»è¡¨ç¤ºï¼Œä½¿å…¶åœ¨è¾“å…¥UVåæ ‡ç©ºé—´ä¸­ä»¥è¿ç»­æ–¹å¼å·¥ä½œã€‚é€šè¿‡å…¨é¢çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™äº›éšå¼ç¥ç»è¡¨ç¤ºåœ¨å›¾åƒè´¨é‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”åœ¨å†…å­˜ä½¿ç”¨å’Œæ¨ç†æ—¶é—´ä¸Šå…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†è¿™äº›ç›®æ ‡ä¹‹é—´çš„å¹³è¡¡ï¼Œå¹¶ç ”ç©¶äº†åœ¨å®æ—¶æ¸²æŸ“å’Œä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å„ç§ç›¸å…³åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.02287', 'title': 'Cross-Lingual Stability of LLM Judges Under Controlled Generation: Evidence from Finno-Ugric Languages', 'url': 'https://huggingface.co/papers/2602.02287', 'abstract': 'Controlled cross-lingual evaluation reveals instability in LLM assessment methods when targeting morphologically rich languages, indicating unreliable zero-shot judge transfer for discourse-level tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Cross-lingual evaluation of large language models (LLMs) typically conflates two sources of variance: genuine model performance differences and measurement instability. We investigate evaluation reliability by holding generation conditions constant while varying target language. Using synthetic customer-support dialogues generated with identical parameters across Estonian, Finnish, and Hungarian, we test whether automatic metrics and LLM-as-a-judge scoring produce stable model rankings across these morphologically rich, related Finno-Ugric languages. With a small set of Estonian native speaker annotations as a reference point, we find systematic ranking instabilities: surface-level metrics (lexical diversity, surface and semantic similarity) maintain cross-language stability, but pragmatic judgments (coherence, instruction-following) exhibit rank inversions and near-zero correlations. Because generation is controlled, these inconsistencies reflect how judge scoring behaves differently across languages rather than true model differences.   This controlled design provides a diagnostic probe: evaluation methods that fail to maintain stability under identical generation conditions signal transfer failure before deployment. Our findings suggest that zero-shot judge transfer is unreliable for discourse-level assessment in morphologically rich languages, motivating language-specific calibration against targeted human baselines. We release our controlled generation protocol, synthetic data, and evaluation framework to enable replication across language families at https://github.com/isaac-chung/cross-lingual-stability-judges.', 'score': 1, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': 'fc36b450cfa4c3de', 'authors': ['Isaac Chung', 'Linda Freienthal'], 'affiliations': ['Zendesk'], 'pdf_title_img': 'assets/pdf/title_img/2602.02287.jpg', 'data': {'categories': ['#multilingual', '#machine_translation', '#dataset', '#benchmark', '#low_resource', '#data', '#open_source'], 'emoji': 'âš ï¸', 'ru': {'title': 'ĞšĞ¾Ğ³Ğ´Ğ° ÑÑƒĞ´ÑŒĞ¸ Ğ¾ÑˆĞ¸Ğ±Ğ°ÑÑ‚ÑÑ: Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ĞºÑ€Ğ¾ÑÑ-Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM Ğ½Ğ° Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ ĞºÑ€Ğ¾ÑÑ-Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°ÑÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚ Ğ½Ğ° Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ½Ğ½Ğ¾-ÑƒĞ³Ğ¾Ñ€ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… (ÑÑÑ‚Ğ¾Ğ½ÑĞºĞ¾Ğ¼, Ñ„Ğ¸Ğ½ÑĞºĞ¾Ğ¼ Ğ¸ Ğ²ĞµĞ½Ğ³ĞµÑ€ÑĞºĞ¾Ğ¼), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM-as-a-judge. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ (Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾) Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ¿Ñ€Ğ°Ğ³Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ (ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚ÑŒ, ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ² Ğ¸ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¸ÑĞºÑƒÑ€ÑĞ° Ğ½ĞµĞ½Ğ°Ğ´Ñ‘Ğ¶ĞµĞ½ Ğ´Ğ»Ñ Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞµ-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ»ÑĞ´ÑĞºĞ¸Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑÑ….'}, 'en': {'title': 'Evaluating Language Models: Stability Matters!', 'desc': 'This paper investigates the reliability of evaluating large language models (LLMs) when applied to morphologically rich languages. It highlights that while some surface-level metrics remain stable across languages, pragmatic judgments like coherence and instruction-following show significant inconsistencies. The study uses controlled generation conditions to reveal that these instabilities are due to the evaluation methods rather than actual differences in model performance. The authors advocate for language-specific calibration to improve the reliability of zero-shot assessments in discourse-level tasks.'}, 'zh': {'title': 'è¯„ä¼°æ–¹æ³•åœ¨å½¢æ€ä¸°å¯Œè¯­è¨€ä¸­çš„ä¸ç¨³å®šæ€§', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨å½¢æ€ä¸°å¯Œè¯­è¨€ä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯„ä¼°æ–¹æ³•çš„ä¸ç¨³å®šæ€§ã€‚æˆ‘ä»¬é€šè¿‡æ§åˆ¶ç”Ÿæˆæ¡ä»¶ï¼Œæ¯”è¾ƒçˆ±æ²™å°¼äºšè¯­ã€èŠ¬å…°è¯­å’ŒåŒˆç‰™åˆ©è¯­çš„å®¢æˆ·æ”¯æŒå¯¹è¯ï¼Œåˆ†æè‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å’ŒLLMè¯„åˆ†çš„ç¨³å®šæ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¡¨é¢æŒ‡æ ‡åœ¨è·¨è¯­è¨€ä¸­ä¿æŒç¨³å®šï¼Œä½†åœ¨è¯­ç”¨åˆ¤æ–­æ–¹é¢å­˜åœ¨æ’åé¢ å€’å’Œå‡ ä¹é›¶ç›¸å…³æ€§çš„é—®é¢˜ã€‚è¿™è¡¨æ˜ï¼Œåœ¨ç›¸åŒç”Ÿæˆæ¡ä»¶ä¸‹ï¼Œè¯„ä¼°æ–¹æ³•çš„ä¸ç¨³å®šæ€§åæ˜ äº†è¯„ä¼°è€…è¯„åˆ†åœ¨ä¸åŒè¯­è¨€ä¸­çš„è¡¨ç°å·®å¼‚ï¼Œè€Œéæ¨¡å‹æœ¬èº«çš„çœŸå®å·®å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01970', 'title': 'Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models', 'url': 'https://huggingface.co/papers/2602.01970', 'abstract': "Generalizable Predictive Prompt Selection (GPS) uses Bayesian inference with a lightweight generative model to efficiently select informative prompts for reinforcement learning-enhanced language models, improving training efficiency and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS's substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods.", 'score': 1, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': 'd430ac88a2ce5693', 'authors': ['Yun Qu', 'Qi Wang', 'Yixiu Mao', 'Heming Zou', 'Yuhang Jiang', 'Weijie Liu', 'Clive Bai', 'Kai Yang', 'Yangkun Chen', 'Saiyong Yang', 'Xiangyang Ji'], 'affiliations': ['Department of Automation, Tsinghua University', 'LLM Department, Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2602.01970.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#rl', '#training', '#small_models'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Generalizable Predictive Prompt Selection (GPS), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ¸Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ›ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ diversity, Ğ¿Ñ€Ğ¸Ğ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğº Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ±Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ.'}, 'en': {'title': 'Efficient Prompt Selection for Enhanced Learning', 'desc': 'This paper presents Generalizable Predictive Prompt Selection (GPS), a method that uses Bayesian inference to choose the most informative prompts for training reinforcement learning-enhanced language models. By employing a lightweight generative model, GPS reduces the computational costs associated with traditional prompt selection methods. The approach focuses on prioritizing prompts based on their difficulty and incorporates diversity to ensure a well-rounded selection. Experimental results show that GPS significantly enhances training efficiency and overall model performance compared to existing techniques.'}, 'zh': {'title': 'é€šç”¨å¯é¢„æµ‹æç¤ºé€‰æ‹©ï¼šæå‡è®­ç»ƒæ•ˆç‡çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºé€šç”¨å¯é¢„æµ‹æç¤ºé€‰æ‹©ï¼ˆGPSï¼‰çš„æ–¹æ³•ï¼Œåˆ©ç”¨è´å¶æ–¯æ¨æ–­å’Œè½»é‡çº§ç”Ÿæˆæ¨¡å‹æ¥é«˜æ•ˆé€‰æ‹©ä¿¡æ¯ä¸°å¯Œçš„æç¤ºï¼Œä»è€Œæé«˜å¼ºåŒ–å­¦ä¹ å¢å¼ºè¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚å¼ºåŒ–å­¦ä¹ è™½ç„¶èƒ½å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½†é€šå¸¸éœ€è¦é«˜æ˜‚çš„è®¡ç®—æˆæœ¬ã€‚ç°æœ‰çš„æ–¹æ³•è¦ä¹ˆä¾èµ–äºæ˜‚è´µçš„ç²¾ç¡®è¯„ä¼°ï¼Œè¦ä¹ˆæ„å»ºç¼ºä¹è·¨æç¤ºæ³›åŒ–èƒ½åŠ›çš„æç¤ºç‰¹å®šé¢„æµ‹æ¨¡å‹ã€‚GPSé€šè¿‡å¯¹æç¤ºéš¾åº¦è¿›è¡Œè´å¶æ–¯æ¨æ–­ï¼Œç»“åˆä¸­ç­‰éš¾åº¦ä¼˜å…ˆçº§å’Œå†å²é”šå®šå¤šæ ·æ€§ï¼Œæ˜¾è‘—æå‡äº†è®­ç»ƒæ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.00521', 'title': 'Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory', 'url': 'https://huggingface.co/papers/2602.00521', 'abstract': 'A two-phase diagnostic framework based on Item Response Theory and Graded Response Model is introduced to assess the reliability of LLM-as-a-Judge by examining intrinsic consistency and human alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t While LLM-as-a-Judge is widely used in automated evaluation, existing validation practices primarily operate at the level of observed outputs, offering limited insight into whether LLM judges themselves function as stable and reliable measurement instruments. To address this limitation, we introduce a two-phase diagnostic framework for assessing reliability of LLM-as-a-Judge, grounded in Item Response Theory (IRT). The framework adopts Graded Response Model (GRM) of IRT and formalizes reliability along two complementary dimensions: (1) intrinsic consistency, defined as the stability of measurement behavior under prompt variations, and (2) human alignment, capturing correspondence with human quality assessments. We empirically examine diverse LLM judges with this framework, and show that leveraging IRT-GRM yields interpretable signals for diagnosing judgments systematically. These signals provide practical guidance for verifying reliablity of LLM-as-a-Judge and identifying potential causes of unreliability.', 'score': 1, 'issue_id': 884, 'pub_date': '2026-01-31', 'pub_date_card': {'ru': '31 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 31', 'zh': '1æœˆ31æ—¥'}, 'hash': '065a43f301fa28c9', 'authors': ['Junhyuk Choi', 'Sohhyung Park', 'Chanhee Cho', 'Hyeonchu Park', 'Bugeun Kim'], 'affiliations': ['Department of Artificial Intelligence, Chung-Ang University, Seoul, Republic of Korea', 'Department of Industrial Engineering'], 'pdf_title_img': 'assets/pdf/title_img/2602.00521.jpg', 'data': {'categories': [], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ”Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ LLM-ÑÑƒĞ´ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³Ñ€Ğ°Ğ´ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ LLM-as-a-Judge Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ½Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ, Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ°Ğº ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ·Ğ¼ĞµÑ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸: ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ñ‘Ñ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ½ĞµĞ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒĞ´ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM.'}, 'en': {'title': 'Assessing LLM Reliability: A Two-Phase Diagnostic Approach', 'desc': "This paper presents a two-phase diagnostic framework that uses Item Response Theory (IRT) and the Graded Response Model (GRM) to evaluate the reliability of large language models (LLMs) acting as judges. The framework assesses two key aspects: intrinsic consistency, which measures how stable the LLM's judgments are when faced with different prompts, and human alignment, which checks how closely the LLM's evaluations match those of human judges. By applying this framework, the authors demonstrate that IRT-GRM can provide clear insights into the reliability of LLMs, helping to identify issues that may affect their judgment quality. This approach aims to enhance the understanding and trustworthiness of LLMs in automated evaluation tasks."}, 'zh': {'title': 'è¯„ä¼°LLMè¯„åˆ¤è€…å¯é æ€§çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé¡¹ç›®ååº”ç†è®ºï¼ˆIRTï¼‰å’Œåˆ†çº§ååº”æ¨¡å‹ï¼ˆGRMï¼‰çš„ä¸¤é˜¶æ®µè¯Šæ–­æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯„åˆ¤è€…çš„å¯é æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†æå†…åœ¨ä¸€è‡´æ€§å’Œäººç±»å¯¹é½ä¸¤ä¸ªç»´åº¦ï¼Œæ¥æ£€éªŒLLMçš„æµ‹é‡ç¨³å®šæ€§å’Œä¸äººç±»è¯„ä¼°çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬é€šè¿‡å®è¯ç ”ç©¶ä¸åŒçš„LLMè¯„åˆ¤è€…ï¼Œå±•ç¤ºäº†IRT-GRMæ–¹æ³•èƒ½å¤Ÿç³»ç»Ÿåœ°è¯Šæ–­åˆ¤æ–­çš„å¯é æ€§ã€‚æ­¤æ¡†æ¶ä¸ºéªŒè¯LLMä½œä¸ºè¯„åˆ¤è€…çš„å¯é æ€§æä¾›äº†å®ç”¨æŒ‡å¯¼ï¼Œå¹¶å¸®åŠ©è¯†åˆ«æ½œåœ¨çš„ä¸å¯é åŸå› ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.22801', 'title': 'Clipping-Free Policy Optimization for Large Language Models', 'url': 'https://huggingface.co/papers/2601.22801', 'abstract': 'Clipping-Free Policy Optimization replaces heuristic clipping with convex quadratic penalty to stabilize reinforcement learning training for large language models without performance loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning has become central to post-training large language models, yet dominant algorithms rely on clipping mechanisms that introduce optimization issues at scale, including zero-gradient regions, reward hacking, and training instability. We propose Clipping-Free Policy Optimization (CFPO), which replaces heuristic clipping with a convex quadratic penalty derived from Total Variation divergence constraints, yielding an everywhere-differentiable objective that enforces stable policy updates without hard boundaries. We evaluate CFPO across both reasoning and alignment settings. In reasoning, CFPO matches clipping-based methods on downstream benchmarks while extending the stable training regime. In alignment, CFPO mitigates verbosity exploitation and reduces capability degradation, while achieving competitive instruction-following performance. CFPO requires only a one-line code change and no additional hyperparameters. Our results suggest that CFPO is a promising drop-in alternative to clipping-based methods for LLM post-training.', 'score': 1, 'issue_id': 884, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 30', 'zh': '1æœˆ30æ—¥'}, 'hash': '0c3a2fe11cf35034', 'authors': ['Ã–mer Veysel Ã‡aÄŸatan', 'BarÄ±ÅŸ AkgÃ¼n', 'GÃ¶zde GÃ¼l Åahin', 'Xuandong Zhao'], 'affiliations': ['KUIS AI Center, Koc University, Istanbul, Turkiye', 'Koc University', 'University of California, Berkeley, CA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2601.22801.jpg', 'data': {'categories': ['#reasoning', '#alignment', '#optimization', '#rl', '#rlhf', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ‘ĞµĞ· Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ğ¹ â€” Ğº ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ² LLM', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Clipping-Free Policy Optimization (CFPO) â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ğµ (clipping) Ğ½Ğ° Ğ²Ñ‹Ğ¿ÑƒĞºĞ»Ñ‹Ğ¹ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ÑˆÑ‚Ñ€Ğ°Ñ„, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹, Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ¾Ğ¹ Ğ¸ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. CFPO ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ²ĞµĞ·Ğ´Ğµ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ±ĞµĞ· Ğ¶ĞµÑÑ‚ĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ½Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ĞºĞ°Ğº Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Stabilizing Reinforcement Learning with Clipping-Free Policy Optimization', 'desc': 'This paper introduces Clipping-Free Policy Optimization (CFPO), a new method for stabilizing reinforcement learning in large language models. Instead of using clipping, which can cause optimization problems, CFPO employs a convex quadratic penalty to create a smooth and differentiable objective function. This approach allows for stable policy updates without the issues associated with clipping, such as zero-gradient regions and training instability. The results show that CFPO performs comparably to traditional methods while improving training stability and reducing verbosity exploitation in alignment tasks.'}, 'zh': {'title': 'æ— å‰ªåˆ‡ç­–ç•¥ä¼˜åŒ–ï¼šç¨³å®šå¼ºåŒ–å­¦ä¹ çš„æ–°é€‰æ‹©', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ–¹æ³•ï¼Œç§°ä¸ºæ— å‰ªåˆ‡ç­–ç•¥ä¼˜åŒ–ï¼ˆCFPOï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­çš„ä¸ç¨³å®šæ€§é—®é¢˜ã€‚CFPOç”¨å‡¸äºŒæ¬¡æƒ©ç½šæ›¿ä»£äº†ä¼ ç»Ÿçš„å‰ªåˆ‡æœºåˆ¶ï¼Œä»è€Œé¿å…äº†é›¶æ¢¯åº¦åŒºåŸŸå’Œå¥–åŠ±é»‘å®¢ç­‰ä¼˜åŒ–é—®é¢˜ã€‚é€šè¿‡åœ¨æ¨ç†å’Œå¯¹é½è®¾ç½®ä¸­çš„è¯„ä¼°ï¼ŒCFPOåœ¨ä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¸å‰ªåˆ‡æ–¹æ³•ç›¸å½“çš„æ•ˆæœï¼ŒåŒæ—¶æ‰©å±•äº†ç¨³å®šè®­ç»ƒçš„èŒƒå›´ã€‚CFPOåªéœ€ä¸€è¡Œä»£ç æ›´æ”¹ï¼Œæ— éœ€é¢å¤–çš„è¶…å‚æ•°ï¼Œæ˜¾ç¤ºå‡ºå…¶ä½œä¸ºå‰ªåˆ‡æ–¹æ³•æ›¿ä»£å“çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.00192', 'title': 'AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange', 'url': 'https://huggingface.co/papers/2602.00192', 'abstract': 'VAE-based inpainting creates spectral shifts that fool detection systems, which can be mitigated through Inpainting Exchange to improve content-aware detection performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern deep learning-based inpainting enables realistic local image manipulation, raising critical challenges for reliable detection. However, we observe that current detectors primarily rely on global artifacts that appear as inpainting side effects, rather than on locally synthesized content. We show that this behavior occurs because VAE-based reconstruction induces a subtle but pervasive spectral shift across the entire image, including unedited regions. To isolate this effect, we introduce Inpainting Exchange (INP-X), an operation that restores original pixels outside the edited region while preserving all synthesized content. We create a 90K test dataset including real, inpainted, and exchanged images to evaluate this phenomenon. Under this intervention, pretrained state-of-the-art detectors, including commercial ones, exhibit a dramatic drop in accuracy (e.g., from 91\\% to 55\\%), frequently approaching chance level. We provide a theoretical analysis linking this behavior to high-frequency attenuation caused by VAE information bottlenecks. Our findings highlight the need for content-aware detection. Indeed, training on our dataset yields better generalization and localization than standard inpainting. Our dataset and code are publicly available at https://github.com/emirhanbilgic/INP-X.', 'score': 1, 'issue_id': 884, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 30', 'zh': '1æœˆ30æ—¥'}, 'hash': 'f374613cb7823e7a', 'authors': ['Elif Nebioglu', 'Emirhan BilgiÃ§', 'Adrian Popescu'], 'affiliations': ['Independent Researcher', 'Institut Polytechnique de Paris, U2IS', 'Universite Paris-Saclay, CEA, LIST', 'Universite Sorbonne, Pierre et Marie Curie, ISIR'], 'pdf_title_img': 'assets/pdf/title_img/2602.00192.jpg', 'data': {'categories': [], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¡Ğ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ»Ğ¾Ğ²ÑƒÑˆĞºĞ¸ VAE-Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ°: Ğ¾Ñ‚ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ VAE-based Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹, Ğ° Ğ½Ğµ Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ VAE-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ´Ğ²Ğ¸Ğ³ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½ĞµĞ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Inpainting Exchange Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ²Ğ½Ğµ Ğ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ·Ğ¾Ğ½Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ…ÑƒĞ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ°.'}, 'en': {'title': 'Enhancing Detection with Inpainting Exchange', 'desc': 'This paper discusses the challenges posed by Variational Autoencoder (VAE)-based inpainting techniques, which can create subtle spectral shifts in images that mislead detection systems. The authors introduce a method called Inpainting Exchange (INP-X) that restores original pixels outside the edited areas while keeping the newly synthesized content intact. They demonstrate that current detection systems often fail because they focus on global artifacts rather than local details, leading to significant drops in detection accuracy when using INP-X. The study emphasizes the importance of developing content-aware detection methods to improve the reliability of detecting inpainted images.'}, 'zh': {'title': 'æå‡å†…å®¹æ„ŸçŸ¥æ£€æµ‹çš„å…³é”®åœ¨äºInpainting Exchange', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åŸºäºå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰çš„å›¾åƒä¿®å¤æŠ€æœ¯å¦‚ä½•å¯¼è‡´é¢‘è°±åç§»ï¼Œä»è€Œå½±å“æ£€æµ‹ç³»ç»Ÿçš„æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºInpainting Exchangeï¼ˆINP-Xï¼‰çš„æ–°æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¿®å¤åŒºåŸŸå¤–æ¢å¤åŸå§‹åƒç´ ï¼ŒåŒæ—¶ä¿ç•™åˆæˆå†…å®¹ã€‚é€šè¿‡åˆ›å»ºä¸€ä¸ªåŒ…å«çœŸå®ã€ä¿®å¤å’Œäº¤æ¢å›¾åƒçš„90Kæµ‹è¯•æ•°æ®é›†ï¼Œæˆ‘ä»¬å‘ç°ç°æœ‰çš„æ£€æµ‹å™¨åœ¨è¿™ç§å¹²é¢„ä¸‹å‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†å†…å®¹æ„ŸçŸ¥æ£€æµ‹çš„é‡è¦æ€§ï¼Œå¹¶è¡¨æ˜åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šè®­ç»ƒå¯ä»¥æé«˜æ£€æµ‹çš„æ³›åŒ–èƒ½åŠ›å’Œå®šä½ç²¾åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.22599', 'title': 'A Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation', 'url': 'https://huggingface.co/papers/2601.22599', 'abstract': 'Automated pipeline for sound separation using high-purity single-event segments from in-the-wild datasets achieves competitive performance with significantly reduced data requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t Query-based universal sound separation is fundamental to intelligent auditory systems, aiming to isolate specific sources from mixtures. Despite recent advances, existing methods continue to suffer from residual interference in complex acoustic scenes. This performance limitation stems largely from a data bottleneck: in-the-wild datasets contain weak labels and severe co-occurrence of events. These flaws induce models to learn spurious correlations between background noise and target categories instead of robust acoustic features. To address this, we propose an automated pipeline that eliminates co-occurrence of events by mining high-purity single-event segments from in-the-wild datasets via a semantically consistent synthesis protocol. Utilizing this pipeline, we constructed Hive, a high-quality synthetic dataset comprising 2.4k hours of raw audio. Experimental results demonstrate that, compared with the state-of-the-art model SAM-Audio which was trained on a huge dataset sim500 times larger than Hive, certain open-source models trained on Hive achieve competitive separation accuracy and perceptual quality. Moreover, these models exhibited remarkable zero-shot generalization on out-of-distribution evaluation benchmarks. These findings highlight that prioritizing purity of supervised signals enables significant data efficiency, offering a new paradigm for training robust auditory foundation models with reduced computational costs. Code and dataset are available at https://shandaai.github.io/Hive.', 'score': 1, 'issue_id': 886, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 30', 'zh': '1æœˆ30æ—¥'}, 'hash': '8664abd1b5a52f44', 'authors': ['Kai Li', 'Jintao Cheng', 'Chang Zeng', 'Zijun Yan', 'Helin Wang', 'Zixiong Su', 'Bo Zheng', 'Xiaolin Hu'], 'affiliations': ['Chinese Institute for Brain Research (CIBR), Beijing, China', 'Department of Computer Science and Technology, Institute for AI, BNRist, Tsinghua University, Beijing, China', 'Johns Hopkins University', 'Shanda AI Research Tokyo', 'Tsinghua Laboratory of Brain and Intelligence (THBI), IDG/McGovern Institute for Brain Research, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2601.22599.jpg', 'data': {'categories': ['#data', '#dataset', '#synthetic', '#open_source', '#audio'], 'emoji': 'ğŸµ', 'ru': {'title': 'Ğ§Ğ¸ÑÑ‚Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ¾Ğ²: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ²ÑƒĞºĞ° Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ·', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ñ‡Ğ¸ÑÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ½Ğ¾Ñ„Ğ¾Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Hive Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ¾Ğ¼ 2.4 Ñ‚Ñ‹ÑÑÑ‡Ğ¸ Ñ‡Ğ°ÑĞ¾Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚Ğ¾Ğº Ğ² Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Hive, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ·Ğ²ÑƒĞºĞ°, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ SAM-Audio, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ² 500 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡Ğ¸ÑÑ‚Ğ¾Ñ‚Ñ‹ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ²ÑƒĞºĞ° Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Purity Over Quantity: Revolutionizing Sound Separation with Hive', 'desc': 'This paper presents an automated pipeline for sound separation that focuses on using high-purity single-event segments from in-the-wild datasets. The authors address the issue of residual interference in complex acoustic scenes caused by weak labels and event co-occurrence in existing datasets. By mining high-quality audio segments, they created a synthetic dataset called Hive, which is significantly smaller yet competitive in performance compared to larger datasets. The results show that models trained on Hive achieve strong separation accuracy and generalization, demonstrating the importance of data purity in training effective auditory models.'}, 'zh': {'title': 'é«˜çº¯åº¦æ•°æ®é©±åŠ¨çš„å£°éŸ³åˆ†ç¦»æ–°èŒƒå¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–çš„å£°éŸ³åˆ†ç¦»ç®¡é“ï¼Œåˆ©ç”¨é«˜çº¯åº¦çš„å•äº‹ä»¶ç‰‡æ®µï¼Œä»è‡ªç„¶ç¯å¢ƒæ•°æ®é›†ä¸­æå–å£°éŸ³ã€‚è¯¥æ–¹æ³•è§£å†³äº†ç°æœ‰æŠ€æœ¯åœ¨å¤æ‚å£°å­¦åœºæ™¯ä¸­å­˜åœ¨çš„æ®‹ä½™å¹²æ‰°é—®é¢˜ï¼Œä¸»è¦æ˜¯ç”±äºæ•°æ®ç“¶é¢ˆå¯¼è‡´çš„ã€‚é€šè¿‡è¯­ä¹‰ä¸€è‡´çš„åˆæˆåè®®ï¼Œæ„å»ºäº†ä¸€ä¸ªåä¸ºHiveçš„é«˜è´¨é‡åˆæˆæ•°æ®é›†ï¼ŒåŒ…å«2400å°æ—¶çš„åŸå§‹éŸ³é¢‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨Hiveè®­ç»ƒçš„å¼€æºæ¨¡å‹åœ¨åˆ†ç¦»å‡†ç¡®æ€§å’Œæ„ŸçŸ¥è´¨é‡ä¸Šä¸å¤§å‹æ•°æ®é›†è®­ç»ƒçš„æœ€å…ˆè¿›æ¨¡å‹ç›¸å½“ï¼Œä¸”åœ¨é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.22296', 'title': 'ParalESN: Enabling parallel information processing in Reservoir Computing', 'url': 'https://huggingface.co/papers/2601.22296', 'abstract': 'Parallel Echo State Network (ParalESN) addresses reservoir computing limitations by enabling parallel temporal processing through diagonal linear recurrence, maintaining theoretical guarantees while achieving significant computational efficiency gains.  \t\t\t\t\tAI-generated summary \t\t\t\t Reservoir Computing (RC) has established itself as an efficient paradigm for temporal processing. However, its scalability remains severely constrained by (i) the necessity of processing temporal data sequentially and (ii) the prohibitive memory footprint of high-dimensional reservoirs. In this work, we revisit RC through the lens of structured operators and state space modeling to address these limitations, introducing Parallel Echo State Network (ParalESN). ParalESN enables the construction of high-dimensional and efficient reservoirs based on diagonal linear recurrence in the complex space, enabling parallel processing of temporal data. We provide a theoretical analysis demonstrating that ParalESN preserves the Echo State Property and the universality guarantees of traditional Echo State Networks while admitting an equivalent representation of arbitrary linear reservoirs in the complex diagonal form. Empirically, ParalESN matches the predictive accuracy of traditional RC on time series benchmarks, while delivering substantial computational savings. On 1-D pixel-level classification tasks, ParalESN achieves competitive accuracy with fully trainable neural networks while reducing computational costs and energy consumption by orders of magnitude. Overall, ParalESN offers a promising, scalable, and principled pathway for integrating RC within the deep learning landscape.', 'score': 1, 'issue_id': 884, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 29', 'zh': '1æœˆ29æ—¥'}, 'hash': '6eb2c7409d1f699c', 'authors': ['Matteo Pinna', 'Giacomo Lagomarsini', 'Andrea Ceni', 'Claudio Gallicchio'], 'affiliations': ['Department of Computer Science, University of Pisa'], 'pdf_title_img': 'assets/pdf/title_img/2601.22296.jpg', 'data': {'categories': ['#architecture', '#benchmark', '#training'], 'emoji': 'âš¡', 'ru': {'title': 'ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸: ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Parallel Echo State Network (ParalESN) â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº reservoir computing, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ¸Ğ°Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ÑĞ´Ñ‹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ParalESN ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾ Echo State Property Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Echo State Networks. ĞĞ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ² Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸ ÑĞ½ĞµÑ€Ğ³Ğ¾Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ¾Ğ².'}, 'en': {'title': 'Parallel Processing for Efficient Temporal Data Handling', 'desc': 'The Parallel Echo State Network (ParalESN) improves reservoir computing by allowing parallel processing of temporal data, which overcomes the limitations of traditional methods that require sequential data handling. It utilizes diagonal linear recurrence in complex space to create efficient, high-dimensional reservoirs while maintaining the essential properties of Echo State Networks. Theoretical analysis confirms that ParalESN retains the Echo State Property and universality guarantees, making it a robust alternative to conventional approaches. Empirical results show that ParalESN achieves similar predictive accuracy to traditional reservoir computing while significantly reducing computational costs and energy usage, making it a scalable solution for deep learning applications.'}, 'zh': {'title': 'å¹¶è¡Œå›å£°çŠ¶æ€ç½‘ç»œï¼šé«˜æ•ˆçš„æ—¶é—´æ•°æ®å¤„ç†æ–°è·¯å¾„', 'desc': 'å¹¶è¡Œå›å£°çŠ¶æ€ç½‘ç»œï¼ˆParalESNï¼‰é€šè¿‡å¯¹è§’çº¿çº¿æ€§é€’å½’å®ç°äº†å¹¶è¡Œæ—¶é—´å¤„ç†ï¼Œå…‹æœäº†ä¼ ç»Ÿæ°´åº“è®¡ç®—çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•åœ¨å¤æ‚ç©ºé—´ä¸­æ„å»ºé«˜ç»´é«˜æ•ˆçš„æ°´åº“ï¼Œå…è®¸å¯¹æ—¶é—´æ•°æ®è¿›è¡Œå¹¶è¡Œå¤„ç†ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒParalESNä¿æŒäº†å›å£°çŠ¶æ€å±æ€§å’Œä¼ ç»Ÿå›å£°çŠ¶æ€ç½‘ç»œçš„é€šç”¨æ€§ä¿è¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒParalESNåœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­ä¸ä¼ ç»Ÿæ°´åº“è®¡ç®—çš„å‡†ç¡®æ€§ç›¸å½“ï¼ŒåŒæ—¶åœ¨è®¡ç®—æ•ˆç‡å’Œèƒ½è€—ä¸Šå¤§å¹…æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.21968', 'title': 'OVD: On-policy Verbal Distillation', 'url': 'https://huggingface.co/papers/2601.21968', 'abstract': "On-policy Verbal Distillation (OVD) enables efficient knowledge transfer from teacher to student models by replacing token-level probability matching with trajectory matching using discrete verbal scores, reducing memory consumption and enabling free exploration without token alignment constraints.  \t\t\t\t\tAI-generated summary \t\t\t\t Knowledge distillation offers a promising path to transfer reasoning capabilities from large teacher models to efficient student models; however, existing token-level on-policy distillation methods require token-level alignment between the student and teacher models, which restricts the student model's exploration ability, prevent effective use of interactive environment feedback, and suffer from severe memory bottlenecks in reinforcement learning. We introduce On-policy Verbal Distillation (OVD), a memory-efficient framework that replaces token-level probability matching with trajectory matching using discrete verbal scores (0--9) from teacher models. OVD dramatically reduces memory consumption while enabling on-policy distillation from teacher models with verbal feedback, and avoids token-level alignment, allowing the student model to freely explore the output space. Extensive experiments on Web question answering and mathematical reasoning tasks show that OVD substantially outperforms existing methods, delivering up to +12.9% absolute improvement in average EM on Web Q&A tasks and a up to +25.7% gain on math benchmarks (when trained with only one random samples), while also exhibiting superior training efficiency. Our project page is available at https://OVD.github.io", 'score': 1, 'issue_id': 884, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 29', 'zh': '1æœˆ29æ—¥'}, 'hash': 'e85fcc0a988521e4', 'authors': ['Jing Xiong', 'Hui Shen', 'Shansan Gong', 'Yuxin Cheng', 'Jianghan Shen', 'Chaofan Tao', 'Haochen Tan', 'Haoli Bai', 'Lifeng Shang', 'Ngai Wong'], 'affiliations': ['Huawei Technologies, China', 'Nanjing University, Nanjing, China', 'The University of Hong Kong, Hong Kong, China'], 'pdf_title_img': 'assets/pdf/title_img/2601.21968.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#transfer_learning', '#optimization', '#rl', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ’ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ: Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ On-policy Verbal Distillation (OVD), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‘Ñ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¼ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚ÑĞºĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ²ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ OVD Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ¾ÑĞ²Ğ¾Ğ±Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¸ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚ÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ°Ñ‚Ñ‡Ğ¸Ğ½Ğ³ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¾Ñ‚ 0 Ğ´Ğ¾ 9, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ, Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ +12.9% Ğ¸ +25.7% ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾.'}, 'en': {'title': 'Efficient Knowledge Transfer with On-policy Verbal Distillation', 'desc': 'On-policy Verbal Distillation (OVD) is a new method for transferring knowledge from large teacher models to smaller student models in machine learning. Instead of matching probabilities at the token level, OVD uses trajectory matching with discrete verbal scores, which helps reduce memory usage. This approach allows student models to explore freely without being constrained by token alignment, making it more effective in interactive environments. Experiments show that OVD significantly improves performance on tasks like web question answering and mathematical reasoning, achieving notable gains in accuracy and training efficiency.'}, 'zh': {'title': 'åœ¨çº¿å£å¤´è’¸é¦ï¼šé«˜æ•ˆçŸ¥è¯†è½¬ç§»çš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œç§°ä¸ºåœ¨çº¿å£å¤´è’¸é¦ï¼ˆOVDï¼‰ã€‚OVDé€šè¿‡ä½¿ç”¨ç¦»æ•£çš„å£å¤´è¯„åˆ†æ›¿ä»£ä¼ ç»Ÿçš„ä»¤ç‰Œçº§æ¦‚ç‡åŒ¹é…ï¼Œä»è€Œå®ç°äº†æ•™å¸ˆæ¨¡å‹ä¸å­¦ç”Ÿæ¨¡å‹ä¹‹é—´çš„çŸ¥è¯†è½¬ç§»ã€‚è¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘äº†å†…å­˜æ¶ˆè€—ï¼Œå¹¶å…è®¸å­¦ç”Ÿæ¨¡å‹åœ¨æ²¡æœ‰ä»¤ç‰Œå¯¹é½é™åˆ¶çš„æƒ…å†µä¸‹è‡ªç”±æ¢ç´¢è¾“å‡ºç©ºé—´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOVDåœ¨ç½‘ç»œé—®ç­”å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.21759', 'title': 'Influence Guided Sampling for Domain Adaptation of Text Retrievers', 'url': 'https://huggingface.co/papers/2601.21759', 'abstract': 'An reinforcement learning-based sampling framework adaptively reweights training datasets to improve embedding model performance while reducing GPU costs.  \t\t\t\t\tAI-generated summary \t\t\t\t General-purpose open-domain dense retrieval systems are usually trained with a large, eclectic mix of corpora and search tasks. How should these diverse corpora and tasks be sampled for training? Conventional approaches sample them uniformly, proportional to their instance population sizes, or depend on human-level expert supervision. It is well known that the training data sampling strategy can greatly impact model performance. However, how to find the optimal strategy has not been adequately studied in the context of embedding models. We propose Inf-DDS, a novel reinforcement learning driven sampling framework that adaptively reweighs training datasets guided by influence-based reward signals and is much more lightweight with respect to GPU consumption. Our technique iteratively refines the sampling policy, prioritizing datasets that maximize model performance on a target development set. We evaluate the efficacy of our sampling strategy on a wide range of text retrieval tasks, demonstrating strong improvements in retrieval performance and better adaptation compared to existing gradient-based sampling methods, while also being 1.5x to 4x cheaper in GPU compute. Our sampling strategy achieves a 5.03 absolute NDCG@10 improvement while training a multilingual bge-m3 model and an absolute NDCG@10 improvement of 0.94 while training all-MiniLM-L6-v2, even when starting from expert-assigned weights on a large pool of training datasets.', 'score': 1, 'issue_id': 884, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 29', 'zh': '1æœˆ29æ—¥'}, 'hash': '74bd4c2ddb110aa3', 'authors': ['Meet Doshi', 'Vishwajeet Kumar', 'Yulong Li', 'Jaydeep Sen'], 'affiliations': ['IBM Research AI'], 'pdf_title_img': 'assets/pdf/title_img/2601.21759.jpg', 'data': {'categories': ['#multilingual', '#data', '#rl', '#optimization', '#training', '#small_models'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€Ğµweighting: ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ° Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚, ĞºĞ°ĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒÑ‡Ğ¸Ñ‚ÑŒ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Inf-DDS, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€Ğµweighting Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²ÑƒĞµÑ‚ÑÑ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ (influence-based rewards) Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸, Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ½Ğ° 1.5-4 Ñ€Ğ°Ğ·Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² GPU Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ gradient-based Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ NDCG@10 Ğ½Ğ° 5.03 Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ BGE-m3 Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ½Ğ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Optimize Sampling, Maximize Performance!', 'desc': 'This paper introduces Inf-DDS, a reinforcement learning-based framework that optimizes the sampling of training datasets for embedding models. By using influence-based reward signals, it adaptively reweights datasets to enhance model performance while significantly reducing GPU costs. The framework iteratively refines its sampling policy, focusing on datasets that yield the best results on a specific development set. The results show substantial improvements in retrieval performance across various tasks, achieving notable gains in efficiency and effectiveness compared to traditional sampling methods.'}, 'zh': {'title': 'è‡ªé€‚åº”é‡‡æ ·ï¼Œæå‡æ¨¡å‹æ€§èƒ½ä¸é™ä½æˆæœ¬', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„é‡‡æ ·æ¡†æ¶Inf-DDSï¼Œæ—¨åœ¨é€šè¿‡è‡ªé€‚åº”é‡åŠ æƒè®­ç»ƒæ•°æ®é›†æ¥æé«˜åµŒå…¥æ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒæ—¶é™ä½GPUæˆæœ¬ã€‚ä¼ ç»Ÿçš„é‡‡æ ·æ–¹æ³•é€šå¸¸é‡‡ç”¨å‡åŒ€é‡‡æ ·æˆ–ä¾èµ–äººå·¥ä¸“å®¶ç›‘ç£ï¼Œè€ŒInf-DDSåˆ™åˆ©ç”¨åŸºäºå½±å“çš„å¥–åŠ±ä¿¡å·æ¥æŒ‡å¯¼é‡‡æ ·ç­–ç•¥çš„ä¼˜åŒ–ã€‚è¯¥æ¡†æ¶é€šè¿‡è¿­ä»£ç²¾ç‚¼é‡‡æ ·ç­–ç•¥ï¼Œä¼˜å…ˆé€‰æ‹©èƒ½å¤Ÿæœ€å¤§åŒ–æ¨¡å‹åœ¨ç›®æ ‡å¼€å‘é›†ä¸Šæ€§èƒ½çš„æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒInf-DDSåœ¨å¤šç§æ–‡æœ¬æ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶ä¸”åœ¨GPUè®¡ç®—ä¸ŠèŠ‚çœäº†1.5åˆ°4å€çš„æˆæœ¬ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.14691', 'title': 'Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation', 'url': 'https://huggingface.co/papers/2601.14691', 'abstract': "Large language models used as judges for agent performance evaluation are vulnerable to manipulation of reasoning traces, with content-based fabrications being more effective than style-based alterations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly used as judges to evaluate agent performance, particularly in non-verifiable settings where judgments rely on agent trajectories including chain-of-thought (CoT) reasoning. This paradigm implicitly assumes that the agent's CoT faithfully reflects both its internal reasoning and the underlying environment state. We show this assumption is brittle: LLM judges are highly susceptible to manipulation of agent reasoning traces. By systematically rewriting agent CoTs while holding actions and observations fixed, we demonstrate that manipulated reasoning alone can inflate false positive rates of state-of-the-art VLM judges by up to 90% across 800 trajectories spanning diverse web tasks. We study manipulation strategies spanning style-based approaches that alter only the presentation of reasoning and content-based approaches that fabricate signals of task progress, and find that content-based manipulations are consistently more effective. We evaluate prompting-based techniques and scaling judge-time compute, which reduce but do not fully eliminate susceptibility to manipulation. Our findings reveal a fundamental vulnerability in LLM-based evaluation and highlight the need for judging mechanisms that verify reasoning claims against observable evidence.", 'score': 1, 'issue_id': 885, 'pub_date': '2026-01-21', 'pub_date_card': {'ru': '21 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 21', 'zh': '1æœˆ21æ—¥'}, 'hash': '319b0b7a616805c1', 'authors': ['Muhammad Khalifa', 'Lajanugen Logeswaran', 'Jaekyeom Kim', 'Sungryull Sohn', 'Yunxiang Zhang', 'Moontae Lee', 'Hao Peng', 'Lu Wang', 'Honglak Lee'], 'affiliations': ['LG AI Research', 'University of Illinois Urbana-Champaign', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2601.14691.jpg', 'data': {'categories': ['#benchmark', '#agents'], 'emoji': 'âš ï¸', 'ru': {'title': 'Ğ£ÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ LLM-ÑÑƒĞ´ĞµĞ¹ Ğ¿ĞµÑ€ĞµĞ´ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¸Ğ·Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ… Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸ÑÑ… Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ğ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° 90%. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ„Ğ°Ğ±Ñ€Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ (ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸) ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ, Ñ‡ĞµĞ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒĞ´ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚-Ğ¸Ğ½Ğ¶Ğ¸Ğ½Ğ¸Ñ€Ğ¸Ğ½Ğ³ Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ², Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑÑ‚ ÑÑ‚Ñƒ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Unmasking Vulnerabilities in LLM-Based Performance Evaluation', 'desc': "This paper discusses the vulnerabilities of large language models (LLMs) when used as judges for evaluating agent performance. It reveals that these models can be easily manipulated by altering the reasoning traces of agents, particularly through content-based fabrications, which are more effective than style-based changes. The authors demonstrate that such manipulations can significantly inflate false positive rates in performance evaluations, indicating a serious flaw in the assumption that reasoning traces accurately reflect an agent's true capabilities. The study calls for improved evaluation methods that can verify reasoning against observable evidence to mitigate these vulnerabilities."}, 'zh': {'title': 'å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°çš„è„†å¼±æ€§', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯„ä¼°æ™ºèƒ½ä½“è¡¨ç°æ—¶å®¹æ˜“å—åˆ°æ¨ç†ç—•è¿¹çš„æ“æ§ï¼Œå†…å®¹åŸºç¡€çš„ä¼ªé€ æ¯”é£æ ¼åŸºç¡€çš„ä¿®æ”¹æ›´æœ‰æ•ˆã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLLMè¯„åˆ¤è€…å¯¹æ™ºèƒ½ä½“çš„æ¨ç†è¿‡ç¨‹éå¸¸æ•æ„Ÿï¼Œç»è¿‡ç³»ç»Ÿæ€§é‡å†™çš„æ¨ç†å¯ä»¥æ˜¾è‘—æé«˜é”™è¯¯åˆ¤æ–­çš„æ¦‚ç‡ã€‚é€šè¿‡å¯¹800ä¸ªä¸åŒç½‘ç»œä»»åŠ¡çš„è½¨è¿¹è¿›è¡Œå®éªŒï¼Œå‘ç°æ“æ§æ¨ç†å¯ä»¥ä½¿æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¯„åˆ¤è€…çš„å‡é˜³æ€§ç‡å¢åŠ å¤šè¾¾90%ã€‚è¿™é¡¹ç ”ç©¶æ­ç¤ºäº†LLMè¯„ä¼°ä¸­çš„åŸºæœ¬è„†å¼±æ€§ï¼Œå¼ºè°ƒäº†éœ€è¦å»ºç«‹èƒ½å¤Ÿæ ¹æ®å¯è§‚å¯Ÿè¯æ®éªŒè¯æ¨ç†å£°æ˜çš„è¯„åˆ¤æœºåˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.02338', 'title': 'Rethinking Generative Recommender Tokenizer: Recsys-Native Encoding and Semantic Quantization Beyond LLMs', 'url': 'https://huggingface.co/papers/2602.02338', 'abstract': 'ReSID presents a novel recommendation-native framework that improves sequential recommendation by learning predictive item representations and optimizing quantization for information preservation and sequential predictability.  \t\t\t\t\tAI-generated summary \t\t\t\t Semantic ID (SID)-based recommendation is a promising paradigm for scaling sequential recommender systems, but existing methods largely follow a semantic-centric pipeline: item embeddings are learned from foundation models and discretized using generic quantization schemes. This design is misaligned with generative recommendation objectives: semantic embeddings are weakly coupled with collaborative prediction, and generic quantization is inefficient at reducing sequential uncertainty for autoregressive modeling. To address these, we propose ReSID, a recommendation-native, principled SID framework that rethinks representation learning and quantization from the perspective of information preservation and sequential predictability, without relying on LLMs. ReSID consists of two components: (i) Field-Aware Masked Auto-Encoding (FAMAE), which learns predictive-sufficient item representations from structured features, and (ii) Globally Aligned Orthogonal Quantization (GAOQ), which produces compact and predictable SID sequences by jointly reducing semantic ambiguity and prefix-conditional uncertainty. Theoretical analysis and extensive experiments across ten datasets show the effectiveness of ReSID. ReSID consistently outperforms strong sequential and SID-based generative baselines by an average of over 10%, while reducing tokenization cost by up to 122x. Code is available at https://github.com/FuCongResearchSquad/ReSID.', 'score': 0, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': 'f2ffbd8b6f718fd7', 'authors': ['Yu Liang', 'Zhongjin Zhang', 'Yuxuan Zhu', 'Kerui Zhang', 'Zhiluohan Guo', 'Wenhang Zhou', 'Zonqi Yang', 'Kangle Wu', 'Yabo Ni', 'Anxiang Zeng', 'Cong Fu', 'Jianxin Wang', 'Jiazhi Xia'], 'affiliations': ['Alibaba Group', 'Huazhong University of Science and Technology', 'Meituan'], 'pdf_title_img': 'assets/pdf/title_img/2602.02338.jpg', 'data': {'categories': ['#architecture', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'ReSID Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸ÑĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ¾Ğ², Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ»ĞµĞ¹. ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² ReSID Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑÑ…, Ğ° Ğ½Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ReSID Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° 10% Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'ReSID: Revolutionizing Sequential Recommendations with Smart Representations', 'desc': 'ReSID is a new framework designed to enhance sequential recommendation systems by focusing on how items are represented and how information is quantized. It introduces two main components: Field-Aware Masked Auto-Encoding (FAMAE) for creating effective item representations, and Globally Aligned Orthogonal Quantization (GAOQ) for optimizing the way these representations are stored. This approach aims to improve the accuracy of predictions while minimizing the loss of important information during the quantization process. The results show that ReSID significantly outperforms existing methods, making it a promising advancement in the field of recommendation systems.'}, 'zh': {'title': 'ReSIDï¼šæå‡åºåˆ—æ¨èçš„æ–°æ¡†æ¶', 'desc': 'ReSIDæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¨èæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å­¦ä¹ é¢„æµ‹æ€§é¡¹ç›®è¡¨ç¤ºå’Œä¼˜åŒ–é‡åŒ–æ¥æé«˜åºåˆ—æ¨èçš„æ•ˆæœã€‚è¯¥æ¡†æ¶ä»ä¿¡æ¯ä¿ç•™å’Œåºåˆ—å¯é¢„æµ‹æ€§çš„è§’åº¦é‡æ–°æ€è€ƒè¡¨ç¤ºå­¦ä¹ å’Œé‡åŒ–ï¼Œé¿å…ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ReSIDåŒ…å«ä¸¤ä¸ªä¸»è¦ç»„ä»¶ï¼šåœºæ„ŸçŸ¥æ©ç è‡ªç¼–ç ï¼ˆFAMAEï¼‰å’Œå…¨å±€å¯¹é½æ­£äº¤é‡åŒ–ï¼ˆGAOQï¼‰ï¼Œå‰è€…ä»ç»“æ„ç‰¹å¾ä¸­å­¦ä¹ é¢„æµ‹æ€§é¡¹ç›®è¡¨ç¤ºï¼Œåè€…é€šè¿‡å‡å°‘è¯­ä¹‰æ¨¡ç³Šå’Œå‰ç¼€æ¡ä»¶ä¸ç¡®å®šæ€§æ¥ç”Ÿæˆç´§å‡‘çš„SIDåºåˆ—ã€‚ç†è®ºåˆ†æå’Œå¤§é‡å®éªŒè¡¨æ˜ï¼ŒReSIDåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå¹³å‡è¶…è¶Šå¼ºåŸºçº¿è¶…è¿‡10%ï¼ŒåŒæ—¶å°†æ ‡è®°åŒ–æˆæœ¬é™ä½äº†122å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01897', 'title': 'Internal Flow Signatures for Self-Checking and Refinement in LLMs', 'url': 'https://huggingface.co/papers/2602.01897', 'abstract': 'Internal flow signatures analyze depthwise dynamics in large language models to enable self-checking and targeted refinement without modifying the base model.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models can generate fluent answers that are unfaithful to the provided context, while many safeguards rely on external verification or a separate judge after generation. We introduce internal flow signatures that audit decision formation from depthwise dynamics at a fixed inter-block monitoring boundary. The method stabilizes token-wise motion via bias-centered monitoring, then summarizes trajectories in compact moving readout-aligned subspaces constructed from the top token and its close competitors within each depth window. Neighboring window frames are aligned by an orthogonal transport, yielding depth-comparable transported step lengths, turning angles, and subspace drift summaries that are invariant to within-window basis choices. A lightweight GRU validator trained on these signatures performs self-checking without modifying the base model. Beyond detection, the validator localizes a culprit depth event and enables a targeted refinement: the model rolls back to the culprit token and clamps an abnormal transported step at the identified block while preserving the orthogonal residual. The resulting pipeline provides actionable localization and low-overhead self-checking from internal decision dynamics. Code is available at github.com/EavnJeong/Internal-Flow-Signatures-for-Self-Checking-and-Refinement-in-LLMs.', 'score': 0, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': '891c90ffa8a6fd40', 'authors': ['Sungheon Jeong', 'Sanggeon Yun', 'Ryozo Masukawa', 'Wenjun Haung', 'Hanning Chen', 'Mohsen Imani'], 'affiliations': ['Department of Computer Science, University of California, Irvine'], 'pdf_title_img': 'assets/pdf/title_img/2602.01897.jpg', 'data': {'categories': ['#architecture', '#hallucinations', '#interpretability', '#open_source', '#training'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ’Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞµ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ LLM: ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ³Ğ½Ğ°Ñ‚ÑƒÑ€Ñ‹ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ÑÑ… ÑĞµÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±Ğ»Ğ¾ĞºĞ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²Ñ‹Ğ´ĞµĞ»ÑÑ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¸Ğ»Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸. Ğ›Ñ‘Ğ³ĞºĞ¸Ğ¹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ñ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GRU Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ñ‚ÑƒÑ€Ğ°Ñ… Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ±ĞµĞ· Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ğµ Ğ¼ĞµÑÑ‚Ğ°, Ğ½Ğ¾ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¼ ÑĞ»Ğ¾Ğµ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ²ĞµÑÑ‚Ğ¸ Ñ†ĞµĞ»ĞµĞ²ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¾Ñ‚ĞºĞ°Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Self-Check and Refine: Enhancing LLMs with Internal Flow Signatures', 'desc': "This paper presents a novel approach called internal flow signatures to analyze the decision-making processes in large language models (LLMs). By monitoring the dynamics of token movements within the model's architecture, the method allows for self-checking of generated outputs without altering the original model. The approach utilizes a lightweight GRU validator that identifies and localizes errors in the model's decision-making, enabling targeted refinements. This technique enhances the reliability of LLMs by providing a mechanism for auditing and correcting outputs based on internal dynamics."}, 'zh': {'title': 'å†…éƒ¨æµç­¾åï¼šè‡ªæˆ‘æ£€æŸ¥ä¸ä¼˜åŒ–çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§å†…éƒ¨æµç­¾åçš„æ–¹æ³•ï¼Œç”¨äºåˆ†æå¤§å‹è¯­è¨€æ¨¡å‹çš„æ·±åº¦åŠ¨æ€ï¼Œä»¥å®ç°è‡ªæˆ‘æ£€æŸ¥å’Œé’ˆå¯¹æ€§ä¼˜åŒ–ï¼Œè€Œæ— éœ€ä¿®æ”¹åŸºç¡€æ¨¡å‹ã€‚è¯¥æ–¹æ³•é€šè¿‡åç½®ä¸­å¿ƒç›‘æ§ç¨³å®šäº†é€è¯è¿åŠ¨ï¼Œå¹¶åœ¨æ¯ä¸ªæ·±åº¦çª—å£å†…æ„å»ºäº†ç´§å‡‘çš„ç§»åŠ¨è¯»å‡ºå¯¹é½å­ç©ºé—´ã€‚é€šè¿‡æ­£äº¤ä¼ è¾“å¯¹é‚»è¿‘çª—å£å¸§è¿›è¡Œå¯¹é½ï¼Œè·å¾—æ·±åº¦å¯æ¯”çš„ä¼ è¾“æ­¥é•¿ã€è½¬è§’å’Œå­ç©ºé—´æ¼‚ç§»æ‘˜è¦ã€‚æœ€ç»ˆï¼Œè®­ç»ƒåœ¨è¿™äº›ç­¾åä¸Šçš„è½»é‡çº§GRUéªŒè¯å™¨èƒ½å¤Ÿåœ¨ä¸ä¿®æ”¹åŸºç¡€æ¨¡å‹çš„æƒ…å†µä¸‹è¿›è¡Œè‡ªæˆ‘æ£€æŸ¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01815', 'title': 'INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery', 'url': 'https://huggingface.co/papers/2602.01815', 'abstract': "Multi-agent systems for molecular discovery that use individualized scientist profiles based on publication and molecular history outperform traditional role-based approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery. To differentiate agent behavior in the multi-agent system, current frameworks typically assign generic role-based personas such as ''reviewer'' or ''writer'' or rely on coarse grained keyword-based personas. While functional, this approach oversimplifies how human scientists operate, whose contributions are shaped by their unique research trajectories. In response, we propose INDIBATOR, a framework for molecular discovery that grounds agents in individualized scientist profiles constructed from two modalities: publication history for literature-derived knowledge and molecular history for structural priors. These agents engage in multi-turn debate through proposal, critique, and voting phases. Our evaluation demonstrates that these fine-grained individuality-grounded agents consistently outperform systems relying on coarse-grained personas, achieving competitive or state-of-the-art performance. These results validate that capturing the ``scientific DNA'' of individual agents is essential for high-quality discovery.", 'score': 0, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': 'acbc59323bb71c5b', 'authors': ['Yunhui Jang', 'Seonghyun Park', 'Jaehyung Kim', 'Sungsoo Ahn'], 'affiliations': ['Korea Advanced Institute of Science and Technology (KAIST), South Korea', 'Yonsei university, South'], 'pdf_title_img': 'assets/pdf/title_img/2602.01815.jpg', 'data': {'categories': [], 'emoji': 'ğŸ§¬', 'ru': {'title': 'ĞĞ°ÑƒÑ‡Ğ½Ğ°Ñ Ğ”ĞĞš Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ Ñ…Ğ¸Ğ¼Ğ¸Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° INDIBATOR Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğµ. ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸Ğ·ÑƒÑÑ‚ÑÑ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ÑĞ¼Ğ¸ ÑƒÑ‡Ñ‘Ğ½Ñ‹Ñ…, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ· Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ»ĞµĞ²Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½ (Ñ€ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚, Ğ¿Ğ¸ÑĞ°Ñ‚ĞµĞ»ÑŒ), ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸Ğ´ĞµĞ¹, ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ¸ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ¾Ğ»ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ°.'}, 'en': {'title': 'Harnessing Individuality for Superior Molecular Discovery', 'desc': 'This paper introduces INDIBATOR, a multi-agent system designed for molecular discovery that utilizes individualized scientist profiles. Unlike traditional systems that use generic roles, INDIBATOR tailors agent behavior based on unique publication and molecular histories. The agents engage in a structured debate process, allowing for proposal, critique, and voting, which enhances collaboration and decision-making. The results show that these personalized agents significantly outperform those using broad, role-based personas, highlighting the importance of individual research trajectories in scientific discovery.'}, 'zh': {'title': 'ä¸ªæ€§åŒ–æ™ºèƒ½ä½“åŠ©åŠ›åˆ†å­å‘ç°', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºINDIBATORçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œç”¨äºåˆ†å­å‘ç°ã€‚è¯¥ç³»ç»ŸåŸºäºä¸ªæ€§åŒ–çš„ç§‘å­¦å®¶æ¡£æ¡ˆï¼Œç»“åˆäº†å‡ºç‰ˆå†å²å’Œåˆ†å­å†å²ï¼Œä»¥æ›´å¥½åœ°æ¨¡æ‹Ÿç§‘å­¦å®¶çš„ç‹¬ç‰¹ç ”ç©¶è½¨è¿¹ã€‚ä¸ä¼ ç»Ÿçš„è§’è‰²åŸºç¡€æ–¹æ³•ç›¸æ¯”ï¼ŒINDIBATORé€šè¿‡å¤šè½®è¾©è®ºçš„æ–¹å¼è¿›è¡Œææ¡ˆã€æ‰¹è¯„å’ŒæŠ•ç¥¨ï¼Œå±•ç°å‡ºæ›´é«˜çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸ªæ€§åŒ–çš„æ™ºèƒ½ä½“åœ¨ç§‘å­¦å‘ç°ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿå®ç°æ›´é«˜è´¨é‡çš„ç ”ç©¶æˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01618', 'title': 'SEA-Guard: Culturally Grounded Multilingual Safeguard for Southeast Asia', 'url': 'https://huggingface.co/papers/2602.01618', 'abstract': 'Researchers developed a novel agentic data-generation framework to create culturally grounded safety datasets for Southeast Asia, resulting in multilingual safeguard models that outperform existing approaches in detecting regionally sensitive content while maintaining general safety performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Culturally aware safeguards are crucial for AI alignment in real-world settings, where safety extends beyond common sense and encompasses diverse local values, norms, and region-specific regulations. However, building large-scale, culturally grounded datasets is challenging due to limited resources and a scarcity of native annotators. Consequently, many safeguard models rely on machine translation of English datasets, often missing regional and cultural nuances. We present a novel agentic data-generation framework to scalably create authentic, region-specific safety datasets for Southeast Asia (SEA). On this foundation, we introduce the SEA-Guard family, the first multilingual safeguard models grounded in SEA cultural contexts. Evaluated across multiple benchmarks and cultural variants, SEA-Guard consistently outperforms existing safeguards at detecting regionally sensitive or harmful content while maintaining strong general safety performance.', 'score': 0, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': 'c0cba7ad1e707785', 'authors': ['Panuthep Tasawong', 'Jian Gang Ngui', 'Alham Fikri Aji', 'Trevor Cohn', 'Peerat Limkonchotiwat'], 'affiliations': ['AI Singapore', 'Google', 'VISTEC'], 'pdf_title_img': 'assets/pdf/title_img/2602.01618.jpg', 'data': {'categories': ['#multilingual', '#dataset', '#agents', '#benchmark', '#low_resource', '#alignment', '#synthetic', '#open_source'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞšÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾-Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ®Ğ³Ğ¾-Ğ’Ğ¾ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞĞ·Ğ¸Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ®Ğ³Ğ¾-Ğ’Ğ¾ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞĞ·Ğ¸Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¾Ğ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ SEA-Guard â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², ÑƒĞ¿ÑƒÑĞºĞ°ÑÑ‚ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ½ÑĞ°Ğ½ÑÑ‹.'}, 'en': {'title': 'Empowering AI with Culturally Grounded Safety in Southeast Asia', 'desc': 'This paper introduces a new framework for generating culturally relevant safety datasets specifically for Southeast Asia. The framework addresses the challenges of creating large-scale datasets by utilizing agentic data generation, which allows for the production of authentic, region-specific data. The resulting SEA-Guard models are multilingual and designed to better detect sensitive content that aligns with local cultural values and regulations. Evaluations show that these models outperform traditional approaches, ensuring both regional sensitivity and overall safety performance.'}, 'zh': {'title': 'æ–‡åŒ–é©±åŠ¨çš„å®‰å…¨æ¨¡å‹ï¼Œå®ˆæŠ¤ä¸œå—äºšçš„å®‰å…¨', 'desc': 'ç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ç§æ–°é¢–çš„è‡ªä¸»æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºåˆ›å»ºé€‚åˆä¸œå—äºšæ–‡åŒ–çš„å®‰å…¨æ•°æ®é›†ã€‚è¿™ç§æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°ç”ŸæˆçœŸå®çš„ã€åœ°åŒºç‰¹å®šçš„å®‰å…¨æ•°æ®ï¼Œä»è€Œæ”¯æŒå¤šè¯­è¨€çš„å®‰å…¨æ¨¡å‹ã€‚æ–°æ¨å‡ºçš„SEA-Guardæ¨¡å‹åœ¨æ£€æµ‹åœ°åŒºæ•æ„Ÿå†…å®¹æ–¹é¢è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„æ•´ä½“å®‰å…¨æ€§èƒ½ã€‚è¿™é¡¹ç ”ç©¶å¼ºè°ƒäº†åœ¨äººå·¥æ™ºèƒ½å¯¹é½ä¸­ï¼Œæ–‡åŒ–æ„è¯†çš„é‡è¦æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ¶‰åŠå¤šæ ·åŒ–çš„åœ°æ–¹ä»·å€¼è§‚å’Œè§„èŒƒæ—¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2602.01418', 'title': 'Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas', 'url': 'https://huggingface.co/papers/2602.01418', 'abstract': 'Parabolic Position Encoding (PaPE) is a novel position encoding method for vision modalities that improves upon existing approaches by incorporating translation invariance, rotation invariance, distance decay, directionality, and context awareness principles.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Parabolic Position Encoding (PaPE), a parabola-based position encoding for vision modalities in attention-based architectures. Given a set of vision tokens-such as images, point clouds, videos, or event camera streams-our objective is to encode their positions while accounting for the characteristics of vision modalities. Prior works have largely extended position encodings from 1D-sequences in language to nD-structures in vision, but only with partial account of vision characteristics. We address this gap by designing PaPE from principles distilled from prior work: translation invariance, rotation invariance (PaPE-RI), distance decay, directionality, and context awareness. We evaluate PaPE on 8 datasets that span 4 modalities. We find that either PaPE or PaPE-RI achieves the top performance on 7 out of 8 datasets. Extrapolation experiments on ImageNet-1K show that PaPE extrapolates remarkably well, improving in absolute terms by up to 10.5% over the next-best position encoding. Code is available at https://github.com/DTU-PAS/parabolic-position-encoding.', 'score': 0, 'issue_id': 886, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 1', 'zh': '2æœˆ1æ—¥'}, 'hash': 'a43d68f82a428632', 'authors': ['Christoffer Koo Ã˜hrstrÃ¸m', 'Rafael I. Cabral Muchacho', 'Yifei Dong', 'Filippos Moumtzidellis', 'Ronja GÃ¼ldenring', 'Florian T. Pokorny', 'Lazaros Nalpantidis'], 'affiliations': ['Lund University', 'Royal Institute of Technology (KTH)'], 'pdf_title_img': 'assets/pdf/title_img/2602.01418.jpg', 'data': {'categories': ['#optimization', '#cv', '#multimodal', '#architecture'], 'emoji': 'ğŸ“', 'ru': {'title': 'ĞŸĞ°Ñ€Ğ°Ğ±Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ (PaPE) Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ±Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ incorporates Ğ¿ÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ²: Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ‚Ñ€Ğ°Ğ½ÑĞ»ÑÑ†Ğ¸Ğ¸, Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ (Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹), Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ PaPE Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² ÑĞµĞ¼Ğ¸ Ğ¸Ğ· Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ ÑĞ»ÑƒÑ‡Ğ°ĞµĞ². ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ½Ğ° ImageNet-1K, Ğ³Ğ´Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ñ‹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ğ½Ğ° 10.5% Ğ² Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ğ¾Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Revolutionizing Vision with Parabolic Position Encoding', 'desc': 'Parabolic Position Encoding (PaPE) is a new method designed to improve how positions are encoded in vision tasks using attention-based models. It incorporates key principles such as translation and rotation invariance, which help the model recognize objects regardless of their position or orientation. Additionally, PaPE considers factors like distance decay, directionality, and context awareness to better capture the unique characteristics of visual data. Evaluations on multiple datasets show that PaPE significantly outperforms existing position encoding methods, demonstrating its effectiveness across various vision modalities.'}, 'zh': {'title': 'æŠ›ç‰©çº¿ä½ç½®ç¼–ç ï¼šè§†è§‰æ¨¡æ€çš„æ–°çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä½ç½®ç¼–ç æ–¹æ³•ï¼Œç§°ä¸ºæŠ›ç‰©çº¿ä½ç½®ç¼–ç ï¼ˆPaPEï¼‰ï¼Œæ—¨åœ¨æ”¹å–„è§†è§‰æ¨¡æ€ä¸­çš„ä½ç½®ç¼–ç ã€‚PaPEç»“åˆäº†å¹³ç§»ä¸å˜æ€§ã€æ—‹è½¬ä¸å˜æ€§ã€è·ç¦»è¡°å‡ã€æ–¹å‘æ€§å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥ç­‰åŸåˆ™ï¼Œä»¥æ›´å¥½åœ°é€‚åº”è§†è§‰ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨8ä¸ªæ¶µç›–4ç§æ¨¡æ€çš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†PaPEï¼Œç»“æœæ˜¾ç¤ºPaPEæˆ–PaPE-RIåœ¨7ä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPaPEåœ¨ImageNet-1Kä¸Šçš„å¤–æ¨èƒ½åŠ›æ˜¾è‘—ï¼Œæ€§èƒ½æå‡å¯è¾¾10.5%ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (14)', '#agi', '#alignment (3)', '#architecture (14)', '#audio (1)', '#benchmark (30)', '#cv (10)', '#data (4)', '#dataset (15)', '#diffusion (4)', '#ethics', '#games (1)', '#graphs (1)', '#hallucinations (1)', '#healthcare', '#inference (7)', '#interpretability (4)', '#leakage (1)', '#long_context (6)', '#low_resource (3)', '#machine_translation (1)', '#math (1)', '#multilingual (5)', '#multimodal (17)', '#open_source (17)', '#optimization (21)', '#plp (3)', '#rag (3)', '#reasoning (21)', '#rl (17)', '#rlhf (4)', '#robotics (1)', '#science (1)', '#security', '#small_models (6)', '#story_generation', '#survey (1)', '#synthetic (4)', '#training (28)', '#transfer_learning (2)', '#video (8)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2026-02-03 19:41',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2026-02-03 19:41')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2026-02-03 19:41')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    