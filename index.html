
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 13 papers. October 16.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ</span> | <span id="title-articles-count">13 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-10-15.html">â¬…ï¸ <span id="prev-date">15.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-10-17.html">â¡ï¸ <span id="next-date">17.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-10.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 16', 'zh': '10æœˆ16æ—¥'};
        let feedDateNext = {'ru': '17.10', 'en': '10/17', 'zh': '10æœˆ17æ—¥'};
        let feedDatePrev = {'ru': '15.10', 'en': '10/15', 'zh': '10æœˆ15æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2510.13621', 'title': 'The Role of Computing Resources in Publishing Foundation Model Research', 'url': 'https://huggingface.co/papers/2510.13621', 'abstract': "Increased computing resources are correlated with national funding and citations in foundation model research, but not with research environment, domain, or methodology.  \t\t\t\t\tAI-generated summary \t\t\t\t Cutting-edge research in Artificial Intelligence (AI) requires considerable resources, including Graphics Processing Units (GPUs), data, and human resources. In this paper, we evaluate of the relationship between these resources and the scientific advancement of foundation models (FM). We reviewed 6517 FM papers published between 2022 to 2024, and surveyed 229 first-authors to the impact of computing resources on scientific output. We find that increased computing is correlated with national funding allocations and citations, but our findings don't observe the strong correlations with research environment (academic or industrial), domain, or study methodology. We advise that individuals and institutions focus on creating shared and affordable computing opportunities to lower the entry barrier for under-resourced researchers. These steps can help expand participation in FM research, foster diversity of ideas and contributors, and sustain innovation and progress in AI. The data will be available at: https://mit-calc.csail.mit.edu/", 'score': 5, 'issue_id': 6444, 'pub_date': '2025-10-15', 'pub_date_card': {'ru': '15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 15', 'zh': '10æœˆ15æ—¥'}, 'hash': 'f9f73f3064dbf2e4', 'authors': ['Yuexing Hao', 'Yue Huang', 'Haoran Zhang', 'Chenyang Zhao', 'Zhenwen Liang', 'Paul Pu Liang', 'Yue Zhao', 'Lichao Sun', 'Saleh Kalantari', 'Xiangliang Zhang', 'Marzyeh Ghassemi'], 'affiliations': ['CSE, University of Notre Dame, South Bend, 46556, USA', 'Computer Science Department, Lehigh University, Bethlehem, 18015, USA', 'Computer Science Department, University of California, Los Angeles, 90095, USA', 'Cornell University, Ithaca, 14850, USA', 'EECS, MIT, Cambridge, 02135, USA', 'School of Advanced Computing, University of Southern California, Los Angeles, 90007, USA'], 'pdf_title_img': 'assets/pdf/title_img/2510.13621.jpg', 'data': {'categories': ['#open_source', '#ethics'], 'emoji': 'ğŸ’°', 'ru': {'title': 'Ğ”ĞµĞ½ÑŒĞ³Ğ¸ Ñ€ĞµÑˆĞ°ÑÑ‚: ĞºĞ°Ğº Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ foundation models', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¾Ğ¼ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ foundation models (Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ 6517 ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¸ Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¸Ğ»Ğ¸ 229 Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ² ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ¾Ğ¼ Ğº GPU Ğ¸ Ğ½Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ†Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚. Ğ˜Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ½Ğµ Ğ±Ñ‹Ğ»Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸ Ğ¸ Ñ‚Ğ¸Ğ¿Ğ¾Ğ¼ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ°Ñ), Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ¼ Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸ĞµĞ¹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒÑÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ñ‰ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ñ†ĞµĞ½Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ±Ğ°Ñ€ÑŒĞµÑ€ Ğ²Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Empowering AI Research Through Shared Computing Resources', 'desc': 'This paper investigates how computing resources, such as GPUs and funding, influence the progress of foundation model research in AI. By analyzing 6517 papers and surveying 229 authors, the study finds a strong correlation between increased computing resources and national funding and citations. However, it reveals that these resources do not significantly impact the research environment, domain, or methodology. The authors recommend creating shared computing resources to support under-resourced researchers, promoting diversity and innovation in the field.'}, 'zh': {'title': 'è®¡ç®—èµ„æºä¸åŸºç¡€æ¨¡å‹ç ”ç©¶çš„å…³ç³»', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†è®¡ç®—èµ„æºä¸åŸºç¡€æ¨¡å‹ç ”ç©¶çš„ç§‘å­¦è¿›å±•ä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬åˆ†æäº†6517ç¯‡2022è‡³2024å¹´é—´å‘è¡¨çš„åŸºç¡€æ¨¡å‹è®ºæ–‡ï¼Œå¹¶è°ƒæŸ¥äº†229ä½ç¬¬ä¸€ä½œè€…å¯¹è®¡ç®—èµ„æºå½±å“çš„çœ‹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œè®¡ç®—èµ„æºçš„å¢åŠ ä¸å›½å®¶èµ„é‡‘åˆ†é…å’Œå¼•ç”¨æ¬¡æ•°ç›¸å…³ï¼Œä½†ä¸ç ”ç©¶ç¯å¢ƒã€é¢†åŸŸæˆ–ç ”ç©¶æ–¹æ³•æ²¡æœ‰æ˜¾è‘—ç›¸å…³æ€§ã€‚æˆ‘ä»¬å»ºè®®ä¸ªäººå’Œæœºæ„åº”ä¸“æ³¨äºåˆ›å»ºå…±äº«å’Œå¯è´Ÿæ‹…çš„è®¡ç®—æœºä¼šï¼Œä»¥é™ä½èµ„æºä¸è¶³ç ”ç©¶è€…çš„è¿›å…¥é—¨æ§›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.13802', 'title': 'Trace Anything: Representing Any Video in 4D via Trajectory Fields', 'url': 'https://huggingface.co/papers/2510.13802', 'abstract': 'Trace Anything, a neural network, predicts video trajectories in a single pass, achieving state-of-the-art performance and demonstrating efficiency and emergent abilities like motion forecasting.  \t\t\t\t\tAI-generated summary \t\t\t\t Effective spatio-temporal representation is fundamental to modeling, understanding, and predicting dynamics in videos. The atomic unit of a video, the pixel, traces a continuous 3D trajectory over time, serving as the primitive element of dynamics. Based on this principle, we propose representing any video as a Trajectory Field: a dense mapping that assigns a continuous 3D trajectory function of time to each pixel in every frame. With this representation, we introduce Trace Anything, a neural network that predicts the entire trajectory field in a single feed-forward pass. Specifically, for each pixel in each frame, our model predicts a set of control points that parameterizes a trajectory (i.e., a B-spline), yielding its 3D position at arbitrary query time instants. We trained the Trace Anything model on large-scale 4D data, including data from our new platform, and our experiments demonstrate that: (i) Trace Anything achieves state-of-the-art performance on our new benchmark for trajectory field estimation and performs competitively on established point-tracking benchmarks; (ii) it offers significant efficiency gains thanks to its one-pass paradigm, without requiring iterative optimization or auxiliary estimators; and (iii) it exhibits emergent abilities, including goal-conditioned manipulation, motion forecasting, and spatio-temporal fusion. Project page: https://trace-anything.github.io/.', 'score': 4, 'issue_id': 6444, 'pub_date': '2025-10-15', 'pub_date_card': {'ru': '15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 15', 'zh': '10æœˆ15æ—¥'}, 'hash': '2a519ed83bf3da2a', 'authors': ['Xinhang Liu', 'Yuxi Xiao', 'Donny Y. Chen', 'Jiashi Feng', 'Yu-Wing Tai', 'Chi-Keung Tang', 'Bingyi Kang'], 'affiliations': ['ByteDance Seed', 'Dartmouth College', 'HKUST', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.13802.jpg', 'data': {'categories': ['#training', '#games', '#video', '#dataset', '#optimization', '#benchmark'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞÑ‚ÑĞ»ĞµĞ´Ğ¸Ñ‚ÑŒ Ğ²ÑÑ‘: Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ²ÑĞµÑ… Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Trace Anything â€” Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²ÑĞµÑ… Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº Ğ¿Ğ¾Ğ»Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ Ğ¿Ğ¸ĞºÑĞµĞ»Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ°Ñ 3D-Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ, Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ B-ÑĞ¿Ğ»Ğ°Ğ¹Ğ½Ğ°Ğ¼Ğ¸ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… 4D Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ emergent abilities, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¼ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Predicting Video Trajectories with Efficiency and Precision', 'desc': 'The paper introduces Trace Anything, a neural network designed to predict video trajectories efficiently in a single pass. It utilizes a novel representation called Trajectory Field, which maps each pixel in a video to a continuous 3D trajectory function over time. This approach allows the model to generate control points for B-splines, enabling accurate trajectory predictions at any moment. The results show that Trace Anything not only achieves state-of-the-art performance but also demonstrates significant efficiency and advanced capabilities like motion forecasting and goal-conditioned manipulation.'}, 'zh': {'title': 'å•æ¬¡é¢„æµ‹ï¼Œè½¨è¿¹è¿½è¸ªçš„æœªæ¥', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTrace Anythingçš„ç¥ç»ç½‘ç»œï¼Œç”¨äºåœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­é¢„æµ‹è§†é¢‘çš„è½¨è¿¹åœºã€‚è¯¥æ–¹æ³•é€šè¿‡å°†è§†é¢‘è¡¨ç¤ºä¸ºæ¯ä¸ªåƒç´ çš„è¿ç»­ä¸‰ç»´è½¨è¿¹å‡½æ•°ï¼Œæ¥æœ‰æ•ˆå»ºæ¨¡å’Œé¢„æµ‹è§†é¢‘ä¸­çš„åŠ¨æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTrace Anythingåœ¨è½¨è¿¹åœºä¼°è®¡çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨ç‚¹è·Ÿè¸ªåŸºå‡†ä¸Šä¹Ÿå…·æœ‰ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨æ•ˆç‡ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œèƒ½å¤Ÿå®ç°ç›®æ ‡æ¡ä»¶çš„æ“ä½œã€è¿åŠ¨é¢„æµ‹å’Œæ—¶ç©ºèåˆç­‰æ–°å…´èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.13515', 'title': 'UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning', 'url': 'https://huggingface.co/papers/2510.13515', 'abstract': 'A novel Universal Multimodal Embedding (UniME-V2) model uses MLLMs to enhance representation learning by identifying diverse, high-quality hard negatives and improving discriminative capacity through soft semantic matching scores.  \t\t\t\t\tAI-generated summary \t\t\t\t Universal multimodal embedding models are foundational to various tasks. Existing approaches typically employ in-batch negative mining by measuring the similarity of query-candidate pairs. However, these methods often struggle to capture subtle semantic differences among candidates and lack diversity in negative samples. Moreover, the embeddings exhibit limited discriminative ability in distinguishing false and hard negatives. In this paper, we leverage the advanced understanding capabilities of MLLMs to enhance representation learning and present a novel Universal Multimodal Embedding (UniME-V2) model. Our approach first constructs a potential hard negative set through global retrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes MLLMs to assess the semantic alignment of query-candidate pairs and generate soft semantic matching scores. These scores serve as a foundation for hard negative mining, mitigating the impact of false negatives and enabling the identification of diverse, high-quality hard negatives. Furthermore, the semantic matching scores are used as soft labels to mitigate the rigid one-to-one mapping constraint. By aligning the similarity matrix with the soft semantic matching score matrix, the model learns semantic distinctions among candidates, significantly enhancing its discriminative capacity. To further improve performance, we propose UniME-V2-Reranker, a reranking model trained on our mined hard negatives through a joint pairwise and listwise optimization approach. We conduct comprehensive experiments on the MMEB benchmark and multiple retrieval tasks, demonstrating that our method achieves state-of-the-art performance on average across all tasks.', 'score': 4, 'issue_id': 6444, 'pub_date': '2025-10-15', 'pub_date_card': {'ru': '15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 15', 'zh': '10æœˆ15æ—¥'}, 'hash': '4e7810d5695a73ea', 'authors': ['Tiancheng Gu', 'Kaicheng Yang', 'Kaichen Zhang', 'Xiang An', 'Ziyong Feng', 'Yueyi Zhang', 'Weidong Cai', 'Jiankang Deng', 'Lidong Bing'], 'affiliations': ['Imperial College London', 'LMMs-Lab Team', 'M.R.L. Team', 'MiroMind AI', 'The University of Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2510.13515.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#training', '#multimodal'], 'emoji': 'ğŸ¯', 'ru': {'title': 'MLLM ĞºĞ°Ğº ÑÑƒĞ´ÑŒÑ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ğ¹Ğ½Ğ¸Ğ½Ğ³Ğ° hard negatives Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ UniME-V2 â€” ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLMs) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ MLLM Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒĞ´ÑŒĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ°Ñ€ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ-ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑĞ³ĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ hard negatives, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ»ÑƒĞ¶Ğ°Ñ‚ Ğ¼ÑĞ³ĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMEB Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ñ€Ğ°Ğ½ĞºĞµÑ€ UniME-V2-Reranker Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Multimodal Learning with Smart Negative Mining', 'desc': 'The paper introduces a new model called Universal Multimodal Embedding version 2 (UniME-V2) that improves how machines understand and represent different types of data. It uses advanced machine learning language models (MLLMs) to find and evaluate hard negative examples, which are crucial for training. By generating soft semantic matching scores, the model can better distinguish between similar candidates and improve its ability to identify relevant information. The results show that UniME-V2 outperforms existing methods in various tasks, making it a significant advancement in multimodal representation learning.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ çš„åˆ›æ–°æ¨¡å‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„é€šç”¨å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹ï¼ˆUniME-V2ï¼‰ï¼Œæ—¨åœ¨é€šè¿‡è¯†åˆ«å¤šæ ·åŒ–çš„é«˜è´¨é‡å›°éš¾è´Ÿæ ·æœ¬æ¥å¢å¼ºè¡¨ç¤ºå­¦ä¹ ã€‚è¯¥æ¨¡å‹åˆ©ç”¨å¤šè¯­è¨€å¤§æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è¯„ä¼°æŸ¥è¯¢-å€™é€‰å¯¹çš„è¯­ä¹‰å¯¹é½ï¼Œå¹¶ç”Ÿæˆè½¯è¯­ä¹‰åŒ¹é…åˆ†æ•°ï¼Œä»è€Œæ”¹å–„åŒºåˆ†èƒ½åŠ›ã€‚é€šè¿‡æ„å»ºæ½œåœ¨çš„å›°éš¾è´Ÿæ ·æœ¬é›†ï¼ŒUniME-V2èƒ½å¤Ÿæœ‰æ•ˆå‡è½»å‡è´Ÿæ ·æœ¬çš„å½±å“ï¼Œå¹¶è¯†åˆ«å‡ºå¤šæ ·åŒ–çš„é«˜è´¨é‡å›°éš¾è´Ÿæ ·æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ£€ç´¢ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.13809', 'title': 'PhysMaster: Mastering Physical Representation for Video Generation via\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2510.13809', 'abstract': "PhysMaster enhances video generation by integrating physical knowledge through PhysEncoder, using reinforcement learning and Direct Preference Optimization to improve physics-awareness.  \t\t\t\t\tAI-generated summary \t\t\t\t Video generation models nowadays are capable of generating visually realistic videos, but often fail to adhere to physical laws, limiting their ability to generate physically plausible videos and serve as ''world models''. To address this issue, we propose PhysMaster, which captures physical knowledge as a representation for guiding video generation models to enhance their physics-awareness. Specifically, PhysMaster is based on the image-to-video task where the model is expected to predict physically plausible dynamics from the input image. Since the input image provides physical priors like relative positions and potential interactions of objects in the scenario, we devise PhysEncoder to encode physical information from it as an extra condition to inject physical knowledge into the video generation process. The lack of proper supervision on the model's physical performance beyond mere appearance motivates PhysEncoder to apply reinforcement learning with human feedback to physical representation learning, which leverages feedback from generation models to optimize physical representations with Direct Preference Optimization (DPO) in an end-to-end manner. PhysMaster provides a feasible solution for improving physics-awareness of PhysEncoder and thus of video generation, proving its ability on a simple proxy task and generalizability to wide-ranging physical scenarios. This implies that our PhysMaster, which unifies solutions for various physical processes via representation learning in the reinforcement learning paradigm, can act as a generic and plug-in solution for physics-aware video generation and broader applications.", 'score': 2, 'issue_id': 6444, 'pub_date': '2025-10-15', 'pub_date_card': {'ru': '15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 15', 'zh': '10æœˆ15æ—¥'}, 'hash': '74cd3b0ff8137389', 'authors': ['Sihui Ji', 'Xi Chen', 'Xin Tao', 'Pengfei Wan', 'Hengshuang Zhao'], 'affiliations': ['Kling Team, Kuaishou Technology', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.13809.jpg', 'data': {'categories': ['#rlhf', '#games', '#rl', '#video', '#multimodal', '#optimization'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ·Ğ°ĞºĞ¾Ğ½Ğ°Ğ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ', 'desc': 'PhysMaster ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ PhysEncoder. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ reinforcement learning Ğ¸ Direct Preference Optimization, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. PhysEncoder Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ (Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ) Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµÑ‘ ĞºĞ°Ğº Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ plug-in Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼, Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°Ñ video generation Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ñ€Ğ¾Ğ»Ğ¸ Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… world models.'}, 'en': {'title': 'Enhancing Video Realism with Physics-Aware Generation', 'desc': "PhysMaster is a novel approach to video generation that incorporates physical knowledge to enhance the realism of generated videos. It utilizes a component called PhysEncoder, which extracts physical information from input images to guide the video generation process. By employing reinforcement learning and Direct Preference Optimization, PhysMaster optimizes the model's understanding of physical dynamics, ensuring that the generated videos are not only visually appealing but also adhere to the laws of physics. This method demonstrates the potential for creating more accurate world models that can be applied to various physical scenarios."}, 'zh': {'title': 'PhysMasterï¼šæå‡è§†é¢‘ç”Ÿæˆçš„ç‰©ç†æ„è¯†', 'desc': 'PhysMaster æ˜¯ä¸€ç§é€šè¿‡æ•´åˆç‰©ç†çŸ¥è¯†æ¥å¢å¼ºè§†é¢‘ç”Ÿæˆçš„æ¨¡å‹ã€‚å®ƒä½¿ç”¨ PhysEncoder ç¼–ç ç‰©ç†ä¿¡æ¯ï¼Œä»¥æé«˜è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„ç‰©ç†æ„è¯†ã€‚è¯¥æ¨¡å‹é‡‡ç”¨å¼ºåŒ–å­¦ä¹ å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ¥ä¼˜åŒ–ç‰©ç†è¡¨ç¤ºï¼Œç¡®ä¿ç”Ÿæˆçš„è§†é¢‘ç¬¦åˆç‰©ç†è§„å¾‹ã€‚PhysMaster æä¾›äº†ä¸€ç§é€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œé€‚ç”¨äºå„ç§ç‰©ç†è¿‡ç¨‹çš„è¡¨ç¤ºå­¦ä¹ ï¼Œèƒ½å¤Ÿå¹¿æ³›åº”ç”¨äºç‰©ç†æ„è¯†è§†é¢‘ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.13804', 'title': 'Generative Universal Verifier as Multimodal Meta-Reasoner', 'url': 'https://huggingface.co/papers/2510.13804', 'abstract': 'Generative Universal Verifier enhances multimodal reasoning by providing reliable visual verification through ViVerBench, OmniVerifier-7B, and OmniVerifier-TTS, improving generation and refinement capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Generative Universal Verifier, a novel concept and plugin designed for next-generation multimodal reasoning in vision-language models and unified multimodal models, providing the fundamental capability of reflection and refinement on visual outcomes during the reasoning and generation process. This work makes three main contributions: (1) We build ViVerBench, a comprehensive benchmark spanning 16 categories of critical tasks for evaluating visual outcomes in multimodal reasoning. Results show that existing VLMs consistently underperform across these tasks, underscoring a substantial gap from human-level capability in reliable visual verification. (2) We design two automated pipelines to construct large-scale visual verification data and train OmniVerifier-7B, the first omni-capable generative verifier trained for universal visual verification and achieves notable gains on ViVerBench(+8.3). Through training, we identify three atomic capabilities in visual verification and demonstrate how they generalize and interact synergistically. (3) We propose OmniVerifier-TTS, a sequential test-time scaling paradigm that leverages the universal verifier to bridge image generation and editing within unified models, enhancing the upper bound of generative ability through iterative fine-grained optimization. Beyond generation, we extend universal verifier to broader world-modeling interleaved reasoning scenarios. Empirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7), and GenEval++(+4.3), outperforming existing parallel test-time scaling methods, such as Best-of-N. By endowing multimodal reasoning with reliable visual verification, OmniVerifier advances both reliable reflection during generation and scalable test-time refinement, marking a step toward more trustworthy and controllable next-generation reasoning systems.', 'score': 2, 'issue_id': 6444, 'pub_date': '2025-10-15', 'pub_date_card': {'ru': '15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 15', 'zh': '10æœˆ15æ—¥'}, 'hash': '85e2081e6a4e6525', 'authors': ['Xinchen Zhang', 'Xiaoying Zhang', 'Youbin Wu', 'Yanbin Cao', 'Renrui Zhang', 'Ruihang Chu', 'Ling Yang', 'Yujiu Yang'], 'affiliations': ['ByteDance Seed', 'Princeton University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.13804.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#multimodal', '#optimization', '#benchmark'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Generative Universal Verifier â€” Ğ½Ğ¾Ğ²ÑƒÑ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ViVerBench Ğ¸Ğ· 16 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ OmniVerifier-7B â€” Ğ¿ĞµÑ€Ğ²ÑƒÑ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ²ÑˆÑƒÑ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ +8.3 Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° OmniVerifier-TTS Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ test-time scaling, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ñ‚Ğ¸Ğ¿Ğ° Best-of-N Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… T2I-ReasonBench (+3.7) Ğ¸ GenEval++ (+4.3), Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğµ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Multimodal Reasoning with Reliable Visual Verification', 'desc': 'The Generative Universal Verifier is a new tool that improves how machines understand and generate visual information alongside text. It introduces ViVerBench, a benchmark for testing how well models can verify visual outcomes, revealing that current models struggle compared to human performance. The paper also presents OmniVerifier-7B, a generative verifier that enhances visual verification capabilities and shows significant improvements in benchmark scores. Additionally, OmniVerifier-TTS offers a method for refining image generation and editing, leading to better overall performance in multimodal reasoning tasks.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨ç†çš„å¯é æ€§ä¸ç”Ÿæˆèƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°æ¦‚å¿µâ€”â€”ç”Ÿæˆé€šç”¨éªŒè¯å™¨ï¼ˆGenerative Universal Verifierï¼‰ï¼Œæ—¨åœ¨æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æ„å»ºäº†ViVerBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–16ç±»å…³é”®ä»»åŠ¡çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€æ¨ç†ä¸­çš„è§†è§‰ç»“æœã€‚é€šè¿‡è®­ç»ƒOmniVerifier-7Bï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºè§†è§‰éªŒè¯ä¸­çš„ä¸‰ç§åŸºæœ¬èƒ½åŠ›ï¼Œå¹¶å±•ç¤ºå®ƒä»¬çš„ååŒä½œç”¨ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†OmniVerifier-TTSï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–æå‡ç”Ÿæˆèƒ½åŠ›ï¼Œæ¨åŠ¨äº†æ›´å¯é çš„å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿçš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.13678', 'title': 'FlashWorld: High-quality 3D Scene Generation within Seconds', 'url': 'https://huggingface.co/papers/2510.13678', 'abstract': "FlashWorld generates 3D scenes from single images or text prompts quickly and with high quality by combining MV-oriented and 3D-oriented generation methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose FlashWorld, a generative model that produces 3D scenes from a single image or text prompt in seconds, 10~100times faster than previous works while possessing superior rendering quality. Our approach shifts from the conventional multi-view-oriented (MV-oriented) paradigm, which generates multi-view images for subsequent 3D reconstruction, to a 3D-oriented approach where the model directly produces 3D Gaussian representations during multi-view generation. While ensuring 3D consistency, 3D-oriented method typically suffers poor visual quality. FlashWorld includes a dual-mode pre-training phase followed by a cross-mode post-training phase, effectively integrating the strengths of both paradigms. Specifically, leveraging the prior from a video diffusion model, we first pre-train a dual-mode multi-view diffusion model, which jointly supports MV-oriented and 3D-oriented generation modes. To bridge the quality gap in 3D-oriented generation, we further propose a cross-mode post-training distillation by matching distribution from consistent 3D-oriented mode to high-quality MV-oriented mode. This not only enhances visual quality while maintaining 3D consistency, but also reduces the required denoising steps for inference. Also, we propose a strategy to leverage massive single-view images and text prompts during this process to enhance the model's generalization to out-of-distribution inputs. Extensive experiments demonstrate the superiority and efficiency of our method.", 'score': 2, 'issue_id': 6444, 'pub_date': '2025-10-15', 'pub_date_card': {'ru': '15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 15', 'zh': '10æœˆ15æ—¥'}, 'hash': 'ef944b6ef4d97bd1', 'authors': ['Xinyang Li', 'Tengfei Wang', 'Zixiao Gu', 'Shengchuan Zhang', 'Chunchao Guo', 'Liujuan Cao'], 'affiliations': ['Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University', 'Tencent', 'Yes Lab, Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2510.13678.jpg', 'data': {'categories': ['#3d', '#optimization', '#diffusion'], 'emoji': 'âš¡', 'ru': {'title': 'ĞœĞ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ğ¼Ğ¸Ñ€Ğ¾Ğ²: ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¼ĞµÑÑ‚Ğµ', 'desc': 'FlashWorld â€” ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ 3D-ÑÑ†ĞµĞ½Ñ‹ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ğ·Ğ° ÑÑ‡Ğ¸Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞµĞºÑƒĞ½Ğ´Ñ‹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ğ² 10-100 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ğ¾Ñ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ multi-view Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğº Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¼Ñƒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 3D Gaussian Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° dual-mode pre-training Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¾Ğ±Ğ¾Ğ¸Ñ… Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ·Ğ°Ñ‚ĞµĞ¼ cross-mode post-training Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ â€” Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'FlashWorld: Fast and High-Quality 3D Scene Generation', 'desc': 'FlashWorld is a generative model that creates high-quality 3D scenes from single images or text prompts in a fraction of the time compared to previous methods. It innovatively combines multi-view-oriented and 3D-oriented generation techniques, allowing for faster rendering while maintaining 3D consistency. The model employs a dual-mode pre-training phase and a cross-mode post-training phase to enhance visual quality and reduce denoising steps during inference. By utilizing a large dataset of single-view images and text prompts, FlashWorld improves its ability to generalize to new inputs effectively.'}, 'zh': {'title': 'FlashWorldï¼šå¿«é€Ÿç”Ÿæˆé«˜è´¨é‡3Dåœºæ™¯çš„åˆ›æ–°æ¨¡å‹', 'desc': 'FlashWorldæ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥å¿«é€Ÿä»å•å¼ å›¾åƒæˆ–æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡çš„3Dåœºæ™¯ã€‚å®ƒç»“åˆäº†å¤šè§†è§’å¯¼å‘å’Œ3Då¯¼å‘çš„ç”Ÿæˆæ–¹æ³•ï¼Œä½¿å¾—ç”Ÿæˆé€Ÿåº¦æ¯”ä»¥å¾€å¿«10åˆ°100å€ï¼ŒåŒæ—¶ä¿æŒä¼˜è¶Šçš„æ¸²æŸ“è´¨é‡ã€‚è¯¥æ¨¡å‹é€šè¿‡åŒæ¨¡å¼é¢„è®­ç»ƒå’Œäº¤å‰æ¨¡å¼åè®­ç»ƒï¼Œæœ‰æ•ˆæ•´åˆäº†ä¸¤ç§æ–¹æ³•çš„ä¼˜ç‚¹ï¼Œç¡®ä¿äº†3Dä¸€è‡´æ€§å¹¶æå‡äº†è§†è§‰è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlashWorldåœ¨ç”Ÿæˆæ•ˆç‡å’Œæ•ˆæœä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.10977', 'title': 'Revisiting Model Interpolation for Efficient Reasoning', 'url': 'https://huggingface.co/papers/2510.10977', 'abstract': 'Model merging, typically on Instruct and Thinking models, has shown remarkable performance for efficient reasoning. In this paper, we systematically revisit the simplest merging method that interpolates two weights directly. Particularly, we observe that model interpolation follows a three-stage evolutionary paradigm with distinct behaviors on the reasoning trajectory. These dynamics provide a principled guide for navigating the performance-cost trade-off. Empirical results demonstrate that a strategically interpolated model surprisingly surpasses sophisticated model merging baselines on both efficiency and effectiveness. We further validate our findings with extensive ablation studies on model layers, modules, and decoding strategies. Ultimately, this work demystifies model interpolation and offers a practical framework for crafting models with precisely targeted reasoning capabilities. Code is available at https://github.com/wutaiqiang/MI{Github}.', 'score': 2, 'issue_id': 6444, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 13', 'zh': '10æœˆ13æ—¥'}, 'hash': 'bdc53b166ac44504', 'authors': ['Taiqiang Wu', 'Runming Yang', 'Tao Liu', 'Jiahao Wang', 'Ngai Wong'], 'affiliations': ['The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.10977.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#training', '#optimization'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ¢Ñ€Ğ¸ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸: Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Instruct Ğ¸ Thinking Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ñ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ baseline Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ñ€Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚.'}, 'en': {'title': 'Unlocking Efficient Reasoning through Model Interpolation', 'desc': 'This paper explores the concept of model merging, particularly focusing on Instruct and Thinking models, to enhance reasoning efficiency. The authors analyze a basic method of directly interpolating weights from two models, revealing a three-stage evolution in the reasoning process. Their findings indicate that a well-interpolated model can outperform more complex merging techniques in terms of both efficiency and effectiveness. The research provides a structured approach to model interpolation, enabling the development of models with specific reasoning strengths, supported by comprehensive experiments and ablation studies.'}, 'zh': {'title': 'æ¨¡å‹æ’å€¼ï¼šé«˜æ•ˆæ¨ç†çš„æ–°è·¯å¾„', 'desc': 'æœ¬æ–‡æ¢è®¨äº†æ¨¡å‹åˆå¹¶ï¼Œç‰¹åˆ«æ˜¯åœ¨æŒ‡ä»¤å’Œæ€ç»´æ¨¡å‹ä¸Šçš„åº”ç”¨ï¼Œå±•ç¤ºäº†å…¶åœ¨é«˜æ•ˆæ¨ç†æ–¹é¢çš„å“è¶Šè¡¨ç°ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°å›é¡¾äº†æœ€ç®€å•çš„åˆå¹¶æ–¹æ³•ï¼Œå³ç›´æ¥æ’å€¼ä¸¤ä¸ªæƒé‡ï¼Œå¹¶è§‚å¯Ÿåˆ°æ¨¡å‹æ’å€¼éµå¾ªä¸‰é˜¶æ®µçš„æ¼”å˜èŒƒå¼ï¼Œå…·æœ‰ä¸åŒçš„æ¨ç†è½¨è¿¹ç‰¹å¾ã€‚è¿™äº›åŠ¨æ€ä¸ºåœ¨æ€§èƒ½ä¸æˆæœ¬ä¹‹é—´çš„æƒè¡¡æä¾›äº†åŸåˆ™æ€§æŒ‡å¯¼ã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œç»è¿‡æˆ˜ç•¥æ€§æ’å€¼çš„æ¨¡å‹åœ¨æ•ˆç‡å’Œæœ‰æ•ˆæ€§ä¸Šè¶…è¶Šäº†å¤æ‚çš„æ¨¡å‹åˆå¹¶åŸºçº¿ï¼Œè¿›ä¸€æ­¥é€šè¿‡å¹¿æ³›çš„æ¶ˆèç ”ç©¶éªŒè¯äº†æˆ‘ä»¬çš„å‘ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.13778', 'title': 'InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for\n  Generalist Robot Policy', 'url': 'https://huggingface.co/papers/2510.13778', 'abstract': "A unified framework for spatial grounding and robot control, InternVLA-M1, enhances instruction-following robots through spatially guided vision-language-action training, achieving significant improvements across various tasks and simulations.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce InternVLA-M1, a unified framework for spatial grounding and robot control that advances instruction-following robots toward scalable, general-purpose intelligence. Its core idea is spatially guided vision-language-action training, where spatial grounding serves as the critical link between instructions and robot actions. InternVLA-M1 employs a two-stage pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning data to determine ``where to act'' by aligning instructions with visual, embodiment-agnostic positions, and (ii) spatially guided action post-training to decide ``how to act'' by generating embodiment-aware actions through plug-and-play spatial prompting. This spatially guided training recipe yields consistent gains: InternVLA-M1 outperforms its variant without spatial guidance by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO Franka, while demonstrating stronger spatial reasoning capability in box, point, and trace prediction. To further scale instruction following, we built a simulation engine to collect 244K generalizable pick-and-place episodes, enabling a 6.2% average improvement across 200 tasks and 3K+ objects. In real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with synthetic co-training, achieved +20.6% on unseen objects and novel configurations. Moreover, in long-horizon reasoning-intensive scenarios, it surpassed existing works by over 10%. These results highlight spatially guided training as a unifying principle for scalable and resilient generalist robots. Code and models are available at https://github.com/InternRobotics/InternVLA-M1.", 'score': 1, 'issue_id': 6444, 'pub_date': '2025-10-15', 'pub_date_card': {'ru': '15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 15', 'zh': '10æœˆ15æ—¥'}, 'hash': '9f1a9178757021cf', 'authors': ['Xinyi Chen', 'Yilun Chen', 'Yanwei Fu', 'Ning Gao', 'Jiaya Jia', 'Weiyang Jin', 'Hao Li', 'Yao Mu', 'Jiangmiao Pang', 'Yu Qiao', 'Yang Tian', 'Bin Wang', 'Bolun Wang', 'Fangjing Wang', 'Hanqing Wang', 'Tai Wang', 'Ziqin Wang', 'Xueyuan Wei', 'Chao Wu', 'Shuai Yang', 'Jinhui Ye', 'Junqiu Yu', 'Jia Zeng', 'Jingjing Zhang', 'Jinyu Zhang', 'Shi Zhang', 'Feng Zheng', 'Bowen Zhou', 'Yangkun Zhu'], 'affiliations': ['Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2510.13778.jpg', 'data': {'categories': ['#science', '#agents', '#agi', '#optimization', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ Ğ¾Ğ±Ğ¾Ñ‚Ñ‹ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°', 'desc': 'InternVLA-M1 â€” ÑÑ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Â«Ğ³Ğ´Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒÂ» Ğ½Ğ° 2.3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ·Ğ°Ñ‚ĞµĞ¼ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Â«ĞºĞ°Ğº Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒÂ» Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ 4% Ğ´Ğ¾ 20% Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ÑÑ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ â€” Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ spatial grounding ĞºĞ°Ğº Ğ¼Ğ¾ÑÑ‚Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°.'}, 'en': {'title': 'Empowering Robots with Spatially Guided Intelligence', 'desc': "The paper presents InternVLA-M1, a framework that enhances robots' ability to follow instructions by integrating spatial grounding with vision-language-action training. This approach involves a two-stage process: first, pre-training the model on a large dataset to understand where to act based on spatial reasoning, and second, fine-tuning it to determine how to act using spatial prompts. The results show significant performance improvements in various robotic tasks, demonstrating the effectiveness of spatial guidance in robot control. Overall, this work emphasizes the importance of spatially informed training for developing versatile and intelligent robots."}, 'zh': {'title': 'ç©ºé—´å¼•å¯¼è®­ç»ƒï¼šæå‡æœºå™¨äººæ™ºèƒ½çš„å…³é”®', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºInternVLA-M1çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨æå‡éµå¾ªæŒ‡ä»¤çš„æœºå™¨äººæ™ºèƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡ç©ºé—´å¼•å¯¼çš„è§†è§‰-è¯­è¨€-è¡ŒåŠ¨è®­ç»ƒï¼Œå»ºç«‹äº†æŒ‡ä»¤ä¸æœºå™¨äººåŠ¨ä½œä¹‹é—´çš„å…³é”®è”ç³»ã€‚InternVLA-M1é‡‡ç”¨ä¸¤é˜¶æ®µæµç¨‹ï¼šé¦–å…ˆè¿›è¡Œç©ºé—´å¼•å¯¼çš„é¢„è®­ç»ƒï¼Œä»¥ç¡®å®šâ€œåœ¨å“ªé‡Œè¡ŒåŠ¨â€ï¼›ç„¶åè¿›è¡Œç©ºé—´å¼•å¯¼çš„åè®­ç»ƒï¼Œä»¥ç”Ÿæˆâ€œå¦‚ä½•è¡ŒåŠ¨â€çš„å…·ä½“åŠ¨ä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡å’Œæ¨¡æ‹Ÿä¸­æ˜¾è‘—æé«˜äº†æœºå™¨äººçš„è¡¨ç°ï¼Œå±•ç¤ºäº†ç©ºé—´å¼•å¯¼è®­ç»ƒåœ¨å¯æ‰©å±•å’Œé€šç”¨æœºå™¨äººä¸­çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.13744', 'title': 'Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier\n  Math', 'url': 'https://huggingface.co/papers/2510.13744', 'abstract': 'Hard2Verify, a human-annotated benchmark, evaluates step-level verifiers for LLM-based mathematical reasoning systems, highlighting the challenges and performance gaps between open-source and closed-source models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM)-based reasoning systems have recently achieved gold medal-level performance in the IMO 2025 competition, writing mathematical proofs where, to receive full credit, each step must be not only correct but also sufficiently supported. To train LLM-based reasoners in such challenging, open-ended settings, strong verifiers capable of catching step-level mistakes are necessary prerequisites. We introduce Hard2Verify, a human-annotated, step-level verification benchmark produced with over 500 hours of human labor. Hard2Verify is designed to rigorously assess step-level verifiers at the frontier: Verifiers must provide step-level annotations or identify the first error in responses generated by frontier LLMs for very recent, challenging, and open-ended math questions. We evaluate 29 generative critics and process reward models, demonstrating that, beyond a few standouts, open-source verifiers lag closed source models. We subsequently analyze what drives poor performance in step-level verification, the impacts of scaling verifier compute, as well as fundamental questions such as self-verification and verification-generation dynamics.', 'score': 1, 'issue_id': 6444, 'pub_date': '2025-10-15', 'pub_date_card': {'ru': '15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 15', 'zh': '10æœˆ15æ—¥'}, 'hash': 'ac775fe89e3a3d8b', 'authors': ['Shrey Pandit', 'Austin Xu', 'Xuan-Phi Nguyen', 'Yifei Ming', 'Caiming Xiong', 'Shafiq Joty'], 'affiliations': ['Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.13744.jpg', 'data': {'categories': ['#reasoning', '#math', '#benchmark', '#interpretability'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ AI Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Hard2Verify â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 500 Ñ‡Ğ°ÑĞ¾Ğ² Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. Ğ’ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 29 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ñ‚ÑÑ‚Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ open-source Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¾Ñ‚ closed-source ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ AI-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Hard2Verify: Bridging the Gap in LLM Verification', 'desc': 'The paper introduces Hard2Verify, a benchmark designed to evaluate step-level verifiers for large language model (LLM)-based mathematical reasoning systems. It highlights the importance of strong verifiers that can accurately identify mistakes in mathematical proofs, which are crucial for achieving high performance in competitions like IMO 2025. The benchmark was created through extensive human annotation, involving over 500 hours of labor, to rigorously assess the capabilities of various verification models. The study reveals significant performance gaps between open-source and closed-source verifiers, while also exploring factors that contribute to these discrepancies and the dynamics of verification processes.'}, 'zh': {'title': 'Hard2Verifyï¼šè¯„ä¼°æ•°å­¦æ¨ç†çš„é€æ­¥éªŒè¯å™¨', 'desc': 'Hard2Verifyæ˜¯ä¸€ä¸ªäººç±»æ ‡æ³¨çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°å­¦æ¨ç†ç³»ç»Ÿçš„é€æ­¥éªŒè¯å™¨ã€‚è¯¥åŸºå‡†å¼ºè°ƒäº†å¼€æºæ¨¡å‹å’Œé—­æºæ¨¡å‹ä¹‹é—´çš„æŒ‘æˆ˜å’Œæ€§èƒ½å·®è·ã€‚ä¸ºäº†åœ¨å¤æ‚çš„å¼€æ”¾å¼ç¯å¢ƒä¸­è®­ç»ƒLLMæ¨ç†å™¨ï¼Œå¼ºå¤§çš„éªŒè¯å™¨æ˜¯å¿…ä¸å¯å°‘çš„ï¼Œå®ƒä»¬èƒ½å¤Ÿæ•æ‰é€æ­¥é”™è¯¯ã€‚æˆ‘ä»¬è¯„ä¼°äº†29ç§ç”Ÿæˆæ€§æ‰¹è¯„è€…å’Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œç»“æœæ˜¾ç¤ºï¼Œé™¤äº†å°‘æ•°ä¼˜ç§€çš„æ¨¡å‹å¤–ï¼Œå¼€æºéªŒè¯å™¨çš„è¡¨ç°æ™®éè½åäºé—­æºæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.13586', 'title': 'Deflanderization for Game Dialogue: Balancing Character Authenticity\n  with Task Execution in LLM-based NPCs', 'url': 'https://huggingface.co/papers/2510.13586', 'abstract': 'Participants in the CPDC 2025 used lightweight prompting and fine-tuned large models to achieve high rankings in task-oriented and context-aware dialogue challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of large language models (LLMs) has opened new opportunities for cre- ating dynamic non-player characters (NPCs) in gaming environments, enabling both func- tional task execution and persona-consistent dialogue generation. In this paper, we (Tu_Character_lab) report our participation in the Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which eval- uates agents across three tracks: task-oriented dialogue, context-aware dialogue, and their integration. Our approach combines two complementary strategies: (i) lightweight prompting techniques in the API track, including a Deflanderization prompting method to suppress excessive role-play and improve task fidelity, and (ii) fine-tuned large models in the GPU track, leveraging Qwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our best submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on Task 3 (GPU track).', 'score': 1, 'issue_id': 6444, 'pub_date': '2025-10-15', 'pub_date_card': {'ru': '15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 15', 'zh': '10æœˆ15æ—¥'}, 'hash': 'b4af6fcdd0d708b9', 'authors': ['Pasin Buakhaw', 'Kun Kerdthaisong', 'Phuree Phenhiran', 'Pitikorn Khlaisamniang', 'Supasate Vorathammathorn', 'Piyalitt Ittichaiwong', 'Nutchanon Yongsatianchot'], 'affiliations': ['Artificial Intelligence Association of Thailand', 'Department of Computer Engineering and Digital Technology, Faculty of Engineering, Chulalongkorn University', 'School of Biomedical Engineering & Imaging Sciences, Kings College London', 'Siriraj Informatics and Data Innovation Center (SIData+), Faculty of Medicine, Siriraj Hospital, Mahidol University', 'Thammasat School of Engineering, Thammasat University'], 'pdf_title_img': 'assets/pdf/title_img/2510.13586.jpg', 'data': {'categories': ['#agents', '#optimization', '#training', '#games'], 'emoji': 'ğŸ®', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğµ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³ Ğ¸ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³', 'desc': 'ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° Tu_Character_lab Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… NPC Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ² Ğ¸Ğ³Ñ€Ğ°Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ğ´Ğ²Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³ Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¾Ğ¹ Deflanderization Ğ´Ğ»Ñ API Ñ‚Ñ€ĞµĞºĞ° Ğ¸ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen3-14B Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ SFT Ğ¸ LoRA Ğ´Ğ»Ñ GPU Ñ‚Ñ€ĞµĞºĞ°. Ğ¢ĞµÑ…Ğ½Ğ¸ĞºĞ° Deflanderization Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ¸Ğ·Ğ»Ğ¸ÑˆĞ½ÑÑ Ñ€Ğ¾Ğ»ĞµĞ²ÑƒÑ Ğ¸Ğ³Ñ€Ñƒ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¼. Ğ ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ½ÑĞ»Ğ¸ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… 1 Ğ¸ 3 (API) Ğ¸ Ñ‡ĞµÑ‚Ğ²ĞµÑ€Ñ‚Ğ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ 3 (GPU) Ğ½Ğ° ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ CPDC 2025.'}, 'en': {'title': 'Elevating NPC Dialogue with Fine-Tuned Models and Smart Prompting', 'desc': 'This paper discusses the participation of the Tu_Character_lab team in the CPDC 2025, focusing on creating advanced non-player characters (NPCs) for dialogue challenges. The team utilized lightweight prompting techniques and fine-tuned large language models to enhance dialogue generation and task execution. They implemented a unique Deflanderization method to maintain task fidelity while minimizing excessive role-play. Their strategies led to impressive rankings, achieving 2nd place in both Task 1 and Task 3 of the API track, and 4th place in Task 3 of the GPU track.'}, 'zh': {'title': 'è½»é‡çº§æç¤ºä¸å¾®è°ƒæ¨¡å‹çš„å®Œç¾ç»“åˆ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†æˆ‘ä»¬åœ¨2025å¹´å¸¸è¯†äººæ ¼å¯¹è¯æŒ‘æˆ˜èµ›ï¼ˆCPDCï¼‰ä¸­çš„å‚ä¸æƒ…å†µã€‚æˆ‘ä»¬ä½¿ç”¨è½»é‡çº§æç¤ºæŠ€æœ¯å’Œå¾®è°ƒçš„å¤§å‹æ¨¡å‹ï¼ŒæˆåŠŸåœ°åœ¨ä»»åŠ¡å¯¼å‘å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥å¯¹è¯æŒ‘æˆ˜ä¸­å–å¾—äº†é«˜æ’åã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†Deflanderizationæç¤ºæ–¹æ³•æ¥æŠ‘åˆ¶è¿‡åº¦è§’è‰²æ‰®æ¼”ï¼Œå¹¶æé«˜ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬åˆ©ç”¨Qwen3-14Bæ¨¡å‹è¿›è¡Œç›‘ç£å¾®è°ƒå’Œä½ç§©é€‚åº”ï¼Œæå‡äº†å¯¹è¯ç”Ÿæˆçš„è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.11958', 'title': 'Direct Multi-Token Decoding', 'url': 'https://huggingface.co/papers/2510.11958', 'abstract': 'Direct Multi-Token Decoding (DMTD) accelerates large language model inference by using only late layers for token generation, achieving significant speedup with minimal performance loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Decoder-only transformers have become the standard architecture for large language models (LLMs) due to their strong performance. Recent studies suggest that, in pre-trained LLMs, early, middle, and late layers may serve distinct roles: Early layers focus on understanding the input context, middle layers handle task-specific processing, and late layers convert abstract representations into output tokens. We hypothesize that once representations have been processed by the early and middle layers, the resulting hidden states may encapsulate sufficient information to support the generation of multiple tokens using only the late layers, eliminating the need to repeatedly traverse the early and middle layers. We refer to this inference paradigm as Direct Multi-Token Decoding (DMTD). Unlike speculative decoding, our method introduces no additional parameters, auxiliary routines, or post-generation verification. Despite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model has already demonstrated promising results, achieving up to a 2x speedup with only minor performance loss. Moreover, as shown in our scaling analysis, its performance is expected to further improve with larger training datasets.', 'score': 1, 'issue_id': 6444, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 13', 'zh': '10æœˆ13æ—¥'}, 'hash': 'a9246dd44d38f230', 'authors': ['Xuan Luo', 'Weizhi Wang', 'Xifeng Yan'], 'affiliations': ['Department of Computer Science, UC Santa Barbara'], 'pdf_title_img': 'assets/pdf/title_img/2510.11958.jpg', 'data': {'categories': ['#architecture', '#training', '#inference', '#optimization'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Direct Multi-Token Decoding (DMTD), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ inference Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ğµ, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ½Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚, ÑÑ€ĞµĞ´Ğ½Ğ¸Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ, Ğ° Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ñ‹. ĞŸĞ¾ÑĞ»Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ½Ğ½Ğ¸Ğ¼Ğ¸ Ğ¸ ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¼Ğ¸ ÑĞ»Ğ¾ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¸, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ²ÑÑ ÑĞµÑ‚ÑŒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ´Ğ²ÑƒĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen3-4B Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ĞµĞ¹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Speed Up Language Generation with DMTD!', 'desc': 'Direct Multi-Token Decoding (DMTD) is a new method that speeds up the process of generating text with large language models by only using the late layers of the model for token generation. This approach takes advantage of the fact that early and middle layers have already processed the input, allowing the late layers to efficiently produce multiple tokens without reprocessing. DMTD does not require any extra parameters or complex routines, making it a straightforward enhancement to existing models. Initial results show that a fine-tuned DMTD model can achieve up to a 2x increase in speed with minimal impact on performance, and its effectiveness is expected to grow with larger datasets.'}, 'zh': {'title': 'ç›´æ¥å¤šæ ‡è®°è§£ç ï¼šåŠ é€Ÿæ¨ç†çš„æ–°æ–¹æ³•', 'desc': 'ç›´æ¥å¤šæ ‡è®°è§£ç ï¼ˆDMTDï¼‰é€šè¿‡ä»…ä½¿ç”¨åå±‚è¿›è¡Œæ ‡è®°ç”Ÿæˆï¼ŒåŠ é€Ÿäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†ï¼Œæ˜¾è‘—æé«˜äº†é€Ÿåº¦ä¸”æ€§èƒ½æŸå¤±æå°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ—©æœŸã€ä¸­æœŸå’ŒåæœŸå±‚å„è‡ªæ‰¿æ‹…ä¸åŒçš„è§’è‰²ã€‚æˆ‘ä»¬çš„å‡è®¾æ˜¯ï¼Œä¸€æ—¦æ—©æœŸå’Œä¸­æœŸå±‚å¤„ç†å®Œè¾“å…¥ï¼Œç”Ÿæˆçš„éšè—çŠ¶æ€å°±è¶³ä»¥æ”¯æŒä»…ä½¿ç”¨åæœŸå±‚ç”Ÿæˆå¤šä¸ªæ ‡è®°ï¼Œä»è€Œé¿å…é‡å¤éå†æ—©æœŸå’Œä¸­æœŸå±‚ã€‚DMTDæ–¹æ³•åœ¨ä¸å¢åŠ é¢å¤–å‚æ•°æˆ–è¾…åŠ©ç¨‹åºçš„æƒ…å†µä¸‹ï¼Œå·²åœ¨æœ‰é™æ•°æ®é›†ä¸Šå±•ç¤ºå‡ºè‰¯å¥½çš„æ•ˆæœï¼Œé€Ÿåº¦æå‡å¯è¾¾2å€ï¼Œä¸”æ€§èƒ½æŸå¤±å¾ˆå°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.10611', 'title': 'HyperAgent: Leveraging Hypergraphs for Topology Optimization in\n  Multi-Agent Communication', 'url': 'https://huggingface.co/papers/2510.10611', 'abstract': 'HyperAgent, a hypergraph-based framework, optimizes communication topologies and captures group collaboration patterns, improving performance and efficiency in multi-agent systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language model-powered multi-agent systems have demonstrated remarkable collective intelligence through effective communication. However, existing approaches face two primary challenges: (i) Ineffective group collaboration modeling, as they rely on pairwise edge representations in graph structures, limiting their ability to capture relationships among multiple agents; and (ii) Limited task-adaptiveness in communication topology design, leading to excessive communication cost for simple tasks and insufficient coordination for complex scenarios. These issues restrict the scalability and practical deployment of adaptive collaboration frameworks. To address these challenges, we propose HyperAgent, a hypergraph-based framework that optimizes communication topologies and effectively captures group collaboration patterns using direct hyperedge representations. Unlike edge-based approaches, HyperAgent uses hyperedges to link multiple agents within the same subtask and employs hypergraph convolutional layers to achieve one-step information aggregation in collaboration groups. Additionally, it incorporates a variational autoencoder framework with sparsity regularization to dynamically adjust hypergraph topologies based on task complexity. Experiments highlight the superiority of HyperAgent in both performance and efficiency. For instance, on GSM8K, HyperAgent achieves 95.07\\% accuracy while reducing token consumption by 25.33\\%, demonstrating the potential of hypergraph-based optimization for multi-agent communication.', 'score': 1, 'issue_id': 6444, 'pub_date': '2025-10-12', 'pub_date_card': {'ru': '12 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 12', 'zh': '10æœˆ12æ—¥'}, 'hash': '38135ea1ec6a6548', 'authors': ['Heng Zhang', 'Yuling Shi', 'Xiaodong Gu', 'Zijian Zhang', 'Haochen You', 'Lubin Gan', 'Yilei Yuan', 'Jin Huang'], 'affiliations': ['Columbia University, USA', 'Shanghai Jiao Tong University, China', 'South China Normal University, China', 'University of Michigan, USA', 'University of Pennsylvania, USA', 'University of Science and Technology of China, China'], 'pdf_title_img': 'assets/pdf/title_img/2510.10611.jpg', 'data': {'categories': ['#agents', '#optimization', '#graphs', '#games'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'Ğ“Ğ¸Ğ¿ĞµÑ€Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ HyperAgent â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ñ LLM. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ°Ñ€Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, HyperAgent Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ³Ğ¸Ğ¿ĞµÑ€Ñ€Ñ‘Ğ±Ñ€Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ GSM8K Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 95.07% Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 25.33%, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'HyperAgent: Enhancing Multi-Agent Collaboration with Hypergraphs', 'desc': 'HyperAgent is a new framework that uses hypergraphs to improve how multiple agents communicate and work together. Traditional methods struggle with modeling group collaboration and adapting communication for different tasks, which can lead to inefficiencies. By using hyperedges, HyperAgent can connect several agents at once, allowing for better information sharing and coordination. The framework also adjusts its communication structure based on the complexity of the task, resulting in better performance and reduced communication costs.'}, 'zh': {'title': 'è¶…å›¾ä¼˜åŒ–ï¼šæå‡å¤šæ™ºèƒ½ä½“åä½œçš„åˆ©å™¨', 'desc': 'HyperAgentæ˜¯ä¸€ä¸ªåŸºäºè¶…å›¾çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„é€šä¿¡æ‹“æ‰‘ï¼Œå¹¶æ•æ‰ç¾¤ä½“åä½œæ¨¡å¼ï¼Œä»è€Œæé«˜æ€§èƒ½å’Œæ•ˆç‡ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å»ºæ¨¡ç¾¤ä½“åä½œæ—¶çš„ä¸è¶³ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¡¨ç¤ºå¤šä¸ªæ™ºèƒ½ä½“ä¹‹é—´çš„å…³ç³»ã€‚HyperAgenté€šè¿‡è¶…è¾¹è¿æ¥åŒä¸€å­ä»»åŠ¡ä¸­çš„å¤šä¸ªæ™ºèƒ½ä½“ï¼Œå¹¶åˆ©ç”¨è¶…å›¾å·ç§¯å±‚å®ç°åä½œç»„å†…çš„ä¸€æ­¥ä¿¡æ¯èšåˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHyperAgentåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¸Šå‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå±•ç¤ºäº†è¶…å›¾ä¼˜åŒ–åœ¨å¤šæ™ºèƒ½ä½“é€šä¿¡ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.10581', 'title': 'GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust\n  Multi-Turn Deep Search', 'url': 'https://huggingface.co/papers/2510.10581', 'abstract': 'GraphTracer addresses multi-agent system failure attribution by constructing Information Dependency Graphs to trace information flow and improve debugging accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-agent systems powered by Large Language Models excel at complex tasks through coordinated collaboration, yet they face high failure rates in multi-turn deep search scenarios. Existing temporal attribution methods struggle to accurately diagnose root causes, particularly when errors propagate across multiple agents. Attempts to automate failure attribution by analyzing action sequences remain ineffective due to their inability to account for information dependencies that span agents. This paper identifies two core challenges: (i) distinguishing symptoms from root causes in multi-agent error propagation, and (ii) tracing information dependencies beyond temporal order. To address these issues, we introduce GraphTracer, a framework that redefines failure attribution through information flow analysis. GraphTracer constructs Information Dependency Graphs (IDGs) to explicitly capture how agents reference and build on prior outputs. It localizes root causes by tracing through these dependency structures instead of relying on temporal sequences. GraphTracer also uses graph-aware synthetic data generation to target critical nodes, creating realistic failure scenarios. Evaluations on the Who\\&When benchmark and integration into production systems demonstrate that GraphTracer-8B achieves up to 18.18\\% higher attribution accuracy compared to state-of-the-art models and enables 4.8\\% to 14.2\\% performance improvements in deployed multi-agent frameworks, establishing a robust solution for multi-agent system debugging.', 'score': 1, 'issue_id': 6444, 'pub_date': '2025-10-12', 'pub_date_card': {'ru': '12 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 12', 'zh': '10æœˆ12æ—¥'}, 'hash': '4456157e4d3ee0bf', 'authors': ['Heng Zhang', 'Yuling Shi', 'Xiaodong Gu', 'Haochen You', 'Zijian Zhang', 'Lubin Gan', 'Yilei Yuan', 'Jin Huang'], 'affiliations': ['Columbia University', 'Shanghai Jiao Tong University', 'South China Normal University', 'University of Michigan', 'University of Pennsylvania', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2510.10581.jpg', 'data': {'categories': ['#agents', '#synthetic', '#dataset', '#graphs', '#optimization', '#benchmark'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'Ğ“Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ¾Ñ€Ğ½ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GraphTracer â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ LLM. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ ÑĞ¸Ğ¼Ğ¿Ñ‚Ğ¾Ğ¼Ñ‹ Ğ¾Ñ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¾Ğ½Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑÑ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. GraphTracer ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ (IDG), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ÑÑ‚, ĞºĞ°Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ğ°, Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ½Ğ° 18% Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° 5-14%.'}, 'en': {'title': 'Revolutionizing Multi-Agent Debugging with GraphTracer', 'desc': 'GraphTracer is a novel framework designed to improve failure attribution in multi-agent systems by utilizing Information Dependency Graphs (IDGs). It addresses the challenges of identifying root causes of errors that propagate across multiple agents and distinguishing them from mere symptoms. By analyzing information flow rather than just temporal sequences, GraphTracer enhances debugging accuracy and provides a clearer understanding of how agents interact. The framework has shown significant improvements in attribution accuracy and performance in real-world applications, making it a valuable tool for developers working with complex multi-agent systems.'}, 'zh': {'title': 'GraphTracerï¼šæå‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ•…éšœå½’å› çš„å‡†ç¡®æ€§', 'desc': 'GraphTracer æ˜¯ä¸€ç§é’ˆå¯¹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ•…éšœå½’å› çš„æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºä¿¡æ¯ä¾èµ–å›¾æ¥è¿½è¸ªä¿¡æ¯æµï¼Œä»è€Œæé«˜è°ƒè¯•çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•è§£å†³äº†åœ¨å¤šæ™ºèƒ½ä½“é”™è¯¯ä¼ æ’­ä¸­åŒºåˆ†ç—‡çŠ¶ä¸æ ¹æœ¬åŸå› çš„æŒ‘æˆ˜ï¼Œå¹¶èƒ½å¤Ÿè¶…è¶Šæ—¶é—´é¡ºåºè¿½è¸ªä¿¡æ¯ä¾èµ–ã€‚GraphTracer é€šè¿‡åˆ†æä¿¡æ¯æµï¼Œé‡æ–°å®šä¹‰äº†æ•…éšœå½’å› ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å®šä½æ ¹æœ¬åŸå› ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGraphTracer åœ¨æ•…éšœå½’å› å‡†ç¡®æ€§ä¸Šæ¯”ç°æœ‰æ¨¡å‹æé«˜äº† 18.18%ï¼Œå¹¶åœ¨å¤šæ™ºèƒ½ä½“æ¡†æ¶ä¸­å®ç°äº† 4.8% åˆ° 14.2% çš„æ€§èƒ½æå‡ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (4)', '#agi (1)', '#alignment', '#architecture (2)', '#audio', '#benchmark (5)', '#cv', '#data', '#dataset (3)', '#diffusion (1)', '#ethics (1)', '#games (4)', '#graphs (2)', '#hallucinations', '#healthcare', '#inference (1)', '#interpretability (1)', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (3)', '#open_source (1)', '#optimization (11)', '#plp', '#rag', '#reasoning (3)', '#rl (1)', '#rlhf (1)', '#robotics (1)', '#science (1)', '#security', '#small_models', '#story_generation', '#survey', '#synthetic (1)', '#training (5)', '#transfer_learning', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-10-16 02:21',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-10-16 02:21')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-10-16 02:21')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    