
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 20 papers. March 28.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">28 марта</span> | <span id="title-articles-count">20 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-03-27.html">⬅️ <span id="prev-date">27.03</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-03-31.html">➡️ <span id="next-date">31.03</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-03.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'};
        let feedDateNext = {'ru': '31.03', 'en': '03/31', 'zh': '3月31日'};
        let feedDatePrev = {'ru': '27.03', 'en': '03/27', 'zh': '3月27日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.21776', 'title': 'Video-R1: Reinforcing Video Reasoning in MLLMs', 'url': 'https://huggingface.co/papers/2503.21776', 'abstract': "Inspired by DeepSeek-R1's success in eliciting reasoning abilities through rule-based reinforcement learning (RL), we introduce Video-R1 as the first attempt to systematically explore the R1 paradigm for eliciting video reasoning within multimodal large language models (MLLMs). However, directly applying RL training with the GRPO algorithm to video reasoning presents two primary challenges: (i) a lack of temporal modeling for video reasoning, and (ii) the scarcity of high-quality video-reasoning data. To address these issues, we first propose the T-GRPO algorithm, which encourages models to utilize temporal information in videos for reasoning. Additionally, instead of relying solely on video data, we incorporate high-quality image-reasoning data into the training process. We have constructed two datasets: Video-R1-COT-165k for SFT cold start and Video-R1-260k for RL training, both comprising image and video data. Experimental results demonstrate that Video-R1 achieves significant improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as well as on general video benchmarks including MVBench and TempCompass, etc. Notably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All codes, models, data are released.", 'score': 47, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': 'f88fc5679e0ee30c', 'authors': ['Kaituo Feng', 'Kaixiong Gong', 'Bohao Li', 'Zonghao Guo', 'Yibing Wang', 'Tianshuo Peng', 'Benyou Wang', 'Xiangyu Yue'], 'affiliations': ['CUHK (SZ)', 'CUHK MMLab', 'Tsinghua University', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2503.21776.jpg', 'data': {'categories': ['#reasoning', '#rl', '#open_source', '#benchmark', '#dataset', '#multimodal', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'Video-R1: Прорыв в видео-рассуждениях для мультимодальных языковых моделей', 'desc': 'Статья представляет Video-R1 - первую попытку применить парадигму R1 для развития способностей видео-рассуждений в мультимодальных больших языковых моделях (MLLM). Авторы предлагают алгоритм T-GRPO для улучшения временного моделирования при видео-рассуждениях. Для обучения используются как видео-, так и изображения-данные высокого качества. Результаты показывают значительное улучшение производительности Video-R1 на различных бенчмарках видео-рассуждений.'}, 'en': {'title': 'Enhancing Video Reasoning with Temporal Insights and Image Data', 'desc': 'This paper presents Video-R1, a novel approach to enhance video reasoning capabilities in multimodal large language models (MLLMs) using rule-based reinforcement learning (RL). The authors identify two main challenges: the need for better temporal modeling in video reasoning and the limited availability of high-quality video-reasoning data. To overcome these challenges, they introduce the T-GRPO algorithm, which leverages temporal information, and augment the training process with high-quality image-reasoning data. The results show that Video-R1 significantly outperforms existing benchmarks, achieving notable accuracy improvements in video reasoning tasks.'}, 'zh': {'title': 'Video-R1：视频推理的新突破', 'desc': '本文介绍了Video-R1，这是首次系统探索在多模态大语言模型中通过规则基础的强化学习（RL）来引发视频推理能力。我们提出了T-GRPO算法，以解决视频推理中的时间建模不足和高质量视频推理数据稀缺的问题。通过结合高质量的图像推理数据，我们构建了两个数据集，分别用于冷启动和强化学习训练。实验结果表明，Video-R1在多个视频推理基准测试中取得了显著的改进，尤其在视频空间推理基准VSI-bench上达到了35.8%的准确率，超越了商业模型GPT-4o。'}}}, {'id': 'https://huggingface.co/papers/2503.21620', 'title': 'UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2503.21620', 'abstract': 'The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities in LLMs through reinforcement learning (RL) with rule-based rewards. Building on this idea, we are the first to explore how rule-based RL can enhance the reasoning capabilities of multimodal large language models (MLLMs) for graphic user interface (GUI) action prediction tasks. To this end, we curate a small yet high-quality dataset of 136 challenging tasks, encompassing five common action types on mobile devices. We also introduce a unified rule-based action reward, enabling model optimization via policy-based algorithms such as Group Relative Policy Optimization (GRPO). Experimental results demonstrate that our proposed data-efficient model, UI-R1-3B, achieves substantial improvements on both in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the ID benchmark AndroidControl, the action type accuracy improves by 15%, while grounding accuracy increases by 10.3%, compared with the base model (i.e. Qwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our model surpasses the base model by 6.0% and achieves competitive performance with larger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning (SFT) on 76K data. These results underscore the potential of rule-based reinforcement learning to advance GUI understanding and control, paving the way for future research in this domain.', 'score': 31, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '81580448c4650ed8', 'authors': ['Zhengxi Lu', 'Yuxiang Chai', 'Yaxuan Guo', 'Xi Yin', 'Liang Liu', 'Hao Wang', 'Guanjing Xiong', 'Hongsheng Li'], 'affiliations': ['MMLab @ CUHK', 'vivo AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.21620.jpg', 'data': {'categories': ['#reasoning', '#rl', '#training', '#dataset', '#multimodal', '#optimization'], 'emoji': '🖥️', 'ru': {'title': 'Улучшение понимания GUI с помощью обучения с подкреплением', 'desc': 'Исследователи изучают, как обучение с подкреплением на основе правил может улучшить способности мультимодальных больших языковых моделей (MLLM) в задачах прогнозирования действий графического пользовательского интерфейса (GUI). Они создали небольшой, но качественный набор данных из 136 сложных задач и разработали унифицированную систему вознаграждений за действия на основе правил. Их модель UI-R1-3B показала значительные улучшения как на внутридоменных, так и на внедоменных задачах по сравнению с базовой моделью. Результаты подчеркивают потенциал обучения с подкреплением на основе правил для улучшения понимания и управления GUI.'}, 'en': {'title': 'Enhancing GUI Action Prediction with Rule-Based Reinforcement Learning', 'desc': 'This paper introduces a novel approach to enhance the reasoning capabilities of multimodal large language models (MLLMs) using rule-based reinforcement learning (RL) for predicting actions in graphic user interfaces (GUIs). The authors curate a high-quality dataset of 136 challenging tasks that involve common actions on mobile devices, allowing for effective model training. They propose a unified rule-based action reward system that optimizes the model through policy-based algorithms like Group Relative Policy Optimization (GRPO). Experimental results show that their model, UI-R1-3B, significantly outperforms the base model in both in-domain and out-of-domain tasks, highlighting the effectiveness of rule-based RL in improving GUI action prediction.'}, 'zh': {'title': '基于规则的强化学习提升GUI动作预测能力', 'desc': '最近的DeepSeek-R1展示了大型语言模型（LLMs）通过基于规则的奖励进行强化学习（RL）而展现出的推理能力。我们首次探索了基于规则的强化学习如何增强多模态大型语言模型（MLLMs）在图形用户界面（GUI）动作预测任务中的推理能力。为此，我们整理了一个小而高质量的数据集，包含136个具有挑战性的任务，涵盖五种常见的移动设备动作类型。实验结果表明，我们提出的数据高效模型UI-R1-3B在领域内和领域外任务上均取得了显著的改进，展示了基于规则的强化学习在提升GUI理解和控制方面的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.21380', 'title': 'Challenging the Boundaries of Reasoning: An Olympiad-Level Math\n  Benchmark for Large Language Models', 'url': 'https://huggingface.co/papers/2503.21380', 'abstract': "In recent years, the rapid development of large reasoning models has resulted in the saturation of existing benchmarks for evaluating mathematical reasoning, highlighting the urgent need for more challenging and rigorous evaluation frameworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level mathematical benchmark, designed to rigorously test the complex reasoning capabilities of LLMs. OlymMATH features 200 meticulously curated problems, each manually verified and available in parallel English and Chinese versions. The problems are systematically organized into two distinct difficulty tiers: (1) AIME-level problems (easy) that establish a baseline for mathematical reasoning assessment, and (2) significantly more challenging problems (hard) designed to push the boundaries of current state-of-the-art models. In our benchmark, these problems span four core mathematical fields, each including a verifiable numerical solution to enable objective, rule-based evaluation. Empirical results underscore the significant challenge presented by OlymMATH, with state-of-the-art models including DeepSeek-R1 and OpenAI's o3-mini demonstrating notably limited accuracy on the hard subset. Furthermore, the benchmark facilitates comprehensive bilingual assessment of mathematical reasoning abilities-a critical dimension that remains largely unaddressed in mainstream mathematical reasoning benchmarks. We release the OlymMATH benchmark at the STILL project: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.", 'score': 26, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '923e6e5f48ca95e4', 'authors': ['Haoxiang Sun', 'Yingqian Min', 'Zhipeng Chen', 'Wayne Xin Zhao', 'Zheng Liu', 'Zhongyuan Wang', 'Lei Fang', 'Ji-Rong Wen'], 'affiliations': ['BAAI', 'DataCanvas Alaya NeW', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'School of Information, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.21380.jpg', 'data': {'categories': ['#reasoning', '#multilingual', '#benchmark', '#low_resource', '#math'], 'emoji': '🧮', 'ru': {'title': 'OlymMATH: Новая высота в оценке математических способностей ИИ', 'desc': 'OlymMATH - это новый бенчмарк для оценки математических рассуждений моделей машинного обучения на уровне олимпиад. Он содержит 200 тщательно отобранных задач двух уровней сложности, охватывающих четыре основные области математики. Бенчмарк доступен на английском и китайском языках, что позволяет проводить двуязычную оценку. Результаты тестирования показали, что даже современные языковые модели (LLM) демонстрируют ограниченную точность на сложном подмножестве задач.'}, 'en': {'title': 'OlymMATH: Raising the Bar for Mathematical Reasoning Evaluation', 'desc': "The paper introduces OlymMATH, a new benchmark for evaluating the mathematical reasoning abilities of large language models (LLMs). It consists of 200 carefully curated problems, verified for accuracy, and presented in both English and Chinese. The problems are categorized into two difficulty levels: AIME-level (easy) and more challenging problems (hard) that test the limits of current models. Empirical results show that even advanced models struggle with the hard problems, highlighting the benchmark's effectiveness in assessing complex reasoning skills."}, 'zh': {'title': 'OlymMATH：挑战数学推理的新基准', 'desc': '近年来，大型推理模型的快速发展使得现有的数学推理评估基准趋于饱和，迫切需要更具挑战性和严格性的评估框架。为此，我们推出了OlymMATH，这是一个新颖的奥林匹克级数学基准，旨在严格测试大型语言模型的复杂推理能力。OlymMATH包含200个经过精心挑选的问题，分为AIME级（简单）和更具挑战性的（困难）两种难度，涵盖四个核心数学领域，并提供可验证的数值解。实证结果表明，OlymMATH对当前最先进的模型提出了显著挑战，尤其是在困难子集上，模型的准确性明显有限。'}}}, {'id': 'https://huggingface.co/papers/2503.21755', 'title': 'VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic\n  Faithfulness', 'url': 'https://huggingface.co/papers/2503.21755', 'abstract': 'Video generation has advanced significantly, evolving from producing unrealistic outputs to generating videos that appear visually convincing and temporally coherent. To evaluate these video generative models, benchmarks such as VBench have been developed to assess their faithfulness, measuring factors like per-frame aesthetics, temporal consistency, and basic prompt adherence. However, these aspects mainly represent superficial faithfulness, which focus on whether the video appears visually convincing rather than whether it adheres to real-world principles. While recent models perform increasingly well on these metrics, they still struggle to generate videos that are not just visually plausible but fundamentally realistic. To achieve real "world models" through video generation, the next frontier lies in intrinsic faithfulness to ensure that generated videos adhere to physical laws, commonsense reasoning, anatomical correctness, and compositional integrity. Achieving this level of realism is essential for applications such as AI-assisted filmmaking and simulated world modeling. To bridge this gap, we introduce VBench-2.0, a next-generation benchmark designed to automatically evaluate video generative models for their intrinsic faithfulness. VBench-2.0 assesses five key dimensions: Human Fidelity, Controllability, Creativity, Physics, and Commonsense, each further broken down into fine-grained capabilities. Tailored for individual dimensions, our evaluation framework integrates generalists such as state-of-the-art VLMs and LLMs, and specialists, including anomaly detection methods proposed for video generation. We conduct extensive annotations to ensure alignment with human judgment. By pushing beyond superficial faithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new standard for the next generation of video generative models in pursuit of intrinsic faithfulness.', 'score': 24, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '4bd65083c265f9a4', 'authors': ['Dian Zheng', 'Ziqi Huang', 'Hongbo Liu', 'Kai Zou', 'Yinan He', 'Fan Zhang', 'Yuanhan Zhang', 'Jingwen He', 'Wei-Shi Zheng', 'Yu Qiao', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Shanghai Artificial Intelligence Laboratory', 'Sun Yat-Sen University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.21755.jpg', 'data': {'categories': ['#video', '#alignment', '#benchmark', '#games'], 'emoji': '🎬', 'ru': {'title': 'От визуальной правдоподобности к реалистичности: новый стандарт оценки генеративных видеомоделей', 'desc': 'Статья представляет VBench-2.0 - новый эталонный тест для оценки генеративных видеомоделей. В отличие от предыдущих версий, фокусирующихся на внешней достоверности, VBench-2.0 оценивает внутреннюю достоверность видео. Тест включает оценку пяти ключевых аспектов: точность отображения людей, управляемость, креативность, физическую корректность и здравый смысл. VBench-2.0 использует как модели общего назначения (VLM, LLM), так и специализированные методы для комплексной оценки генеративных видеомоделей.'}, 'en': {'title': 'Towards Realism: VBench-2.0 for Intrinsic Faithfulness in Video Generation', 'desc': 'This paper discusses the evolution of video generation models from producing unrealistic outputs to creating visually convincing and temporally coherent videos. It highlights the limitations of current evaluation benchmarks like VBench, which focus on superficial aspects of faithfulness rather than adherence to real-world principles. The authors introduce VBench-2.0, a new benchmark that evaluates video generative models based on intrinsic faithfulness, which includes factors like physics, commonsense reasoning, and anatomical correctness. By emphasizing these deeper aspects of realism, VBench-2.0 aims to improve the quality of video generation for applications such as AI-assisted filmmaking and simulated world modeling.'}, 'zh': {'title': '追求内在可信度的下一代视频生成标准', 'desc': '视频生成技术已经取得了显著进展，从最初生成不真实的输出到现在能够生成视觉上令人信服且时间上连贯的视频。为了评估这些视频生成模型，开发了VBench等基准，主要测量每帧的美观性、时间一致性和基本提示遵循等因素。然而，这些评估主要关注表面上的可信度，而不是视频是否遵循现实世界的原则。为实现真正的“世界模型”，我们引入了VBench-2.0，旨在自动评估视频生成模型的内在可信度，确保生成的视频符合物理法则和常识推理。'}}}, {'id': 'https://huggingface.co/papers/2503.21749', 'title': 'LeX-Art: Rethinking Text Generation via Scalable High-Quality Data\n  Synthesis', 'url': 'https://huggingface.co/papers/2503.21749', 'abstract': 'We introduce LeX-Art, a comprehensive suite for high-quality text-image synthesis that systematically bridges the gap between prompt expressiveness and text rendering fidelity. Our approach follows a data-centric paradigm, constructing a high-quality data synthesis pipeline based on Deepseek-R1 to curate LeX-10K, a dataset of 10K high-resolution, aesthetically refined 1024times1024 images. Beyond dataset construction, we develop LeX-Enhancer, a robust prompt enrichment model, and train two text-to-image models, LeX-FLUX and LeX-Lumina, achieving state-of-the-art text rendering performance. To systematically evaluate visual text generation, we introduce LeX-Bench, a benchmark that assesses fidelity, aesthetics, and alignment, complemented by Pairwise Normalized Edit Distance (PNED), a novel metric for robust text accuracy evaluation. Experiments demonstrate significant improvements, with LeX-Lumina achieving a 79.81% PNED gain on CreateBench, and LeX-FLUX outperforming baselines in color (+3.18%), positional (+4.45%), and font accuracy (+3.81%). Our codes, models, datasets, and demo are publicly available.', 'score': 16, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': 'fe7d17315ae060c8', 'authors': ['Shitian Zhao', 'Qilong Wu', 'Xinyue Li', 'Bo Zhang', 'Ming Li', 'Qi Qin', 'Dongyang Liu', 'Kaipeng Zhang', 'Hongsheng Li', 'Yu Qiao', 'Peng Gao', 'Bin Fu', 'Zhen Li'], 'affiliations': ['Shanghai AI Laboratory', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.21749.jpg', 'data': {'categories': ['#cv', '#open_source', '#benchmark', '#dataset', '#data'], 'emoji': '🖼️', 'ru': {'title': 'LeX-Art: Революция в синтезе текста и изображений', 'desc': 'LeX-Art представляет собой комплексный набор инструментов для высококачественного синтеза текста и изображений, который систематически устраняет разрыв между выразительностью промпта и точностью рендеринга текста. Подход основан на парадигме, ориентированной на данные, и включает создание высококачественного конвейера синтеза данных на основе Deepseek-R1 для курирования набора данных LeX-10K. Разработаны LeX-Enhancer для обогащения промптов и две модели text-to-image: LeX-FLUX и LeX-Lumina, достигающие передового уровня производительности в рендеринге текста. Для оценки введен бенчмарк LeX-Bench и новая метрика PNED для надежной оценки точности текста.'}, 'en': {'title': "Bridging Text and Image: LeX-Art's High-Quality Synthesis Revolution", 'desc': 'LeX-Art is a new system designed to create high-quality images from text prompts, focusing on improving how well the text is rendered in the images. It builds a large dataset called LeX-10K, which contains 10,000 high-resolution images that are visually appealing. The system includes a model called LeX-Enhancer that improves the prompts used for generating images, and two advanced text-to-image models, LeX-FLUX and LeX-Lumina, which achieve top performance in rendering text. Additionally, LeX-Art introduces a benchmark called LeX-Bench to evaluate the quality of the generated images, using a new metric called Pairwise Normalized Edit Distance (PNED) to measure text accuracy.'}, 'zh': {'title': '高质量文本-图像合成的新突破', 'desc': 'LeX-Art 是一个全面的文本-图像合成工具，旨在提高提示表达能力和文本渲染的准确性。我们构建了 LeX-10K 数据集，包含 10,000 张高分辨率、经过美学优化的图像，并开发了 LeX-Enhancer 模型来增强提示效果。我们训练了两个文本到图像模型 LeX-FLUX 和 LeX-Lumina，达到了最先进的文本渲染性能。通过 LeX-Bench 基准测试，我们评估了视觉文本生成的保真度、美学和一致性，实验结果显示显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2503.21460', 'title': 'Large Language Model Agent: A Survey on Methodology, Applications and\n  Challenges', 'url': 'https://huggingface.co/papers/2503.21460', 'abstract': 'The era of intelligent agents is upon us, driven by revolutionary advancements in large language models. Large Language Model (LLM) agents, with goal-driven behaviors and dynamic adaptation capabilities, potentially represent a critical pathway toward artificial general intelligence. This survey systematically deconstructs LLM agent systems through a methodology-centered taxonomy, linking architectural foundations, collaboration mechanisms, and evolutionary pathways. We unify fragmented research threads by revealing fundamental connections between agent design principles and their emergent behaviors in complex environments. Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains. By surveying the latest developments in this rapidly evolving field, we offer researchers a structured taxonomy for understanding LLM agents and identify promising directions for future research. The collection is available at https://github.com/luo-junyu/Awesome-Agent-Papers.', 'score': 13, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '8295a3726d0cc1b8', 'authors': ['Junyu Luo', 'Weizhi Zhang', 'Ye Yuan', 'Yusheng Zhao', 'Junwei Yang', 'Yiyang Gu', 'Bohan Wu', 'Binqi Chen', 'Ziyue Qiao', 'Qingqing Long', 'Rongcheng Tu', 'Xiao Luo', 'Wei Ju', 'Zhiping Xiao', 'Yifan Wang', 'Meng Xiao', 'Chenwu Liu', 'Jingyang Yuan', 'Shichang Zhang', 'Yiqiao Jin', 'Fan Zhang', 'Xian Wu', 'Hanqing Zhao', 'Dacheng Tao', 'Philip S. Yu', 'Ming Zhang'], 'affiliations': ['Computer Network Information Center, Chinese Academy of Sciences, Beijing, China', 'Department of Computer Science, University of California, Los Angeles, USA', 'Department of Computer Science, University of Illinois at Chicago, Chicago, USA', 'Georgia Institute of Technology, Atlanta, USA', 'Harvard University, Cambridge, USA', 'Jarvis Research Center, Tencent YouTu Lab, Shenzhen, China', 'Nanyang Technological University, Singapore', 'Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, USA', 'School of Computer Science and PKU-Anker LLM Lab, Peking University, Beijing, China', 'School of Computing and Information Technology, Great Bay University, Guangdong, China', 'School of Information Technology & Management, University of International Business and Economics, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.21460.jpg', 'data': {'categories': ['#agi', '#benchmark', '#agents', '#architecture', '#survey'], 'emoji': '🤖', 'ru': {'title': 'Агенты LLM: архитектура, сотрудничество и эволюция искусственного интеллекта', 'desc': 'Эта статья представляет собой обзор систем агентов на основе больших языковых моделей (LLM). Авторы предлагают методологически-ориентированную таксономию, связывающую архитектурные основы, механизмы сотрудничества и эволюционные пути агентов LLM. В работе рассматриваются принципы проектирования агентов, их поведение в сложных средах, а также методологии оценки и практические применения. Статья предоставляет исследователям структурированный подход к пониманию агентов LLM и определяет перспективные направления для будущих исследований.'}, 'en': {'title': 'Unifying the Future of Intelligent LLM Agents', 'desc': 'This paper explores the advancements in Large Language Model (LLM) agents, which are intelligent systems capable of adapting and achieving specific goals. It presents a structured taxonomy that categorizes LLM agent systems based on their architecture, collaboration methods, and evolutionary processes. The authors aim to connect various research areas by highlighting the relationship between design principles and the behaviors of these agents in complex environments. Additionally, the paper discusses evaluation methods, practical challenges, and potential applications, providing a comprehensive overview for future research in this field.'}, 'zh': {'title': '大型语言模型代理：通向人工通用智能的关键', 'desc': '本文探讨了大型语言模型（LLM）代理的系统，强调其在实现人工通用智能方面的重要性。通过建立一个以方法论为中心的分类法，文章将代理的架构基础、协作机制和演化路径进行了系统性分析。我们揭示了代理设计原则与其在复杂环境中涌现行为之间的基本联系，从而统一了分散的研究线索。最后，本文为研究人员提供了一个结构化的分类体系，以理解LLM代理，并指出未来研究的有前景方向。'}}}, {'id': 'https://huggingface.co/papers/2503.21758', 'title': 'Lumina-Image 2.0: A Unified and Efficient Image Generative Framework', 'url': 'https://huggingface.co/papers/2503.21758', 'abstract': 'We introduce Lumina-Image 2.0, an advanced text-to-image generation framework that achieves significant progress compared to previous work, Lumina-Next. Lumina-Image 2.0 is built upon two key principles: (1) Unification - it adopts a unified architecture (Unified Next-DiT) that treats text and image tokens as a joint sequence, enabling natural cross-modal interactions and allowing seamless task expansion. Besides, since high-quality captioners can provide semantically well-aligned text-image training pairs, we introduce a unified captioning system, Unified Captioner (UniCap), specifically designed for T2I generation tasks. UniCap excels at generating comprehensive and accurate captions, accelerating convergence and enhancing prompt adherence. (2) Efficiency - to improve the efficiency of our proposed model, we develop multi-stage progressive training strategies and introduce inference acceleration techniques without compromising image quality. Extensive evaluations on academic benchmarks and public text-to-image arenas show that Lumina-Image 2.0 delivers strong performances even with only 2.6B parameters, highlighting its scalability and design efficiency. We have released our training details, code, and models at https://github.com/Alpha-VLLM/Lumina-Image-2.0.', 'score': 12, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '976a9523e7e6ba13', 'authors': ['Qi Qin', 'Le Zhuo', 'Yi Xin', 'Ruoyi Du', 'Zhen Li', 'Bin Fu', 'Yiting Lu', 'Jiakang Yuan', 'Xinyue Li', 'Dongyang Liu', 'Xiangyang Zhu', 'Manyuan Zhang', 'Will Beddow', 'Erwann Millon', 'Victor Perez', 'Wenhai Wang', 'Conghui He', 'Bo Zhang', 'Xiaohong Liu', 'Hongsheng Li', 'Yu Qiao', 'Chang Xu', 'Peng Gao'], 'affiliations': ['Krea AI', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'The University of Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2503.21758.jpg', 'data': {'categories': ['#inference', '#cv', '#open_source', '#training', '#multimodal', '#small_models', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Единство и эффективность в генерации изображений по тексту', 'desc': 'Lumina-Image 2.0 - это усовершенствованная система генерации изображений по текстовому описанию. Она основана на унифицированной архитектуре, объединяющей обработку текста и изображений, и включает специальную систему генерации подписей UniCap. Разработчики применили стратегии многоэтапного прогрессивного обучения и методы ускорения вывода для повышения эффективности модели. Несмотря на относительно небольшое количество параметров (2.6 млрд), Lumina-Image 2.0 демонстрирует высокую производительность на академических бенчмарках и в публичных соревнованиях по генерации изображений.'}, 'en': {'title': 'Revolutionizing Text-to-Image Generation with Lumina-Image 2.0', 'desc': "Lumina-Image 2.0 is a cutting-edge framework for generating images from text, significantly improving upon its predecessor, Lumina-Next. It utilizes a unified architecture that allows text and image data to interact seamlessly, enhancing the model's ability to handle various tasks. The introduction of the Unified Captioner (UniCap) enables the generation of high-quality captions that align well with images, improving training efficiency and output accuracy. Additionally, the model incorporates advanced training and inference techniques to maintain high image quality while being resource-efficient, demonstrating strong performance with a relatively small number of parameters."}, 'zh': {'title': 'Lumina-Image 2.0：高效的文本到图像生成新纪元', 'desc': 'Lumina-Image 2.0 是一个先进的文本到图像生成框架，相比于之前的 Lumina-Next 取得了显著进展。该框架基于两个关键原则：统一性和效率。统一性通过采用统一架构（Unified Next-DiT）来实现文本和图像标记的联合处理，促进了跨模态的自然交互。效率方面，我们开发了多阶段渐进训练策略和推理加速技术，确保在不降低图像质量的情况下提高模型效率。'}}}, {'id': 'https://huggingface.co/papers/2503.21729', 'title': 'ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large\n  Reasoning Models with Iterative Retrieval Augmented Generation', 'url': 'https://huggingface.co/papers/2503.21729', 'abstract': "Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely primarily on parametric knowledge, limiting factual accuracy. While recent works equip reinforcement learning (RL)-based LRMs with retrieval capabilities, they suffer from overthinking and lack robustness in reasoning, reducing their effectiveness in question answering (QA) tasks. To address this, we propose ReaRAG, a factuality-enhanced reasoning model that explores diverse queries without excessive iterations. Our solution includes a novel data construction framework with an upper bound on the reasoning chain length. Specifically, we first leverage an LRM to generate deliberate thinking, then select an action from a predefined action space (Search and Finish). For Search action, a query is executed against the RAG engine, where the result is returned as observation to guide reasoning steps later. This process iterates until a Finish action is chosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach outperforms existing baselines on multi-hop QA. Further analysis highlights its strong reflective ability to recognize errors and refine its reasoning trajectory. Our study enhances LRMs' factuality while effectively integrating robust reasoning for Retrieval-Augmented Generation (RAG).", 'score': 11, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '83959049f8af99fe', 'authors': ['Zhicheng Lee', 'Shulin Cao', 'Jinxin Liu', 'Jiajie Zhang', 'Weichuan Liu', 'Xiaoyin Che', 'Lei Hou', 'Juanzi Li'], 'affiliations': ['Siemens AG', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21729.jpg', 'data': {'categories': ['#reasoning', '#rag', '#rl', '#training'], 'emoji': '🧠', 'ru': {'title': 'ReaRAG: Улучшение фактической точности моделей рассуждений с помощью контролируемого поиска', 'desc': 'Статья представляет ReaRAG - модель рассуждений с улучшенной фактической точностью для задач вопросно-ответных систем. Авторы предлагают новый подход к построению данных с ограничением длины цепочки рассуждений. ReaRAG использует большую модель рассуждений для генерации обдуманных шагов и выбора действий из предопределенного пространства. Модель демонстрирует улучшенные результаты на задачах многошаговых вопросно-ответных систем по сравнению с существующими методами.'}, 'en': {'title': 'Enhancing Factuality in Reasoning with ReaRAG', 'desc': 'This paper introduces ReaRAG, a new reasoning model designed to improve the factual accuracy of Large Reasoning Models (LRMs) in question answering tasks. Unlike traditional RL-based LRMs that often overthink and lack robustness, ReaRAG employs a structured approach to reasoning by limiting the length of reasoning chains and allowing for diverse query exploration. The model uses a two-step action process, where it can either search for information or finish the reasoning process, enhancing its ability to refine answers based on retrieved data. Overall, ReaRAG demonstrates superior performance in multi-hop question answering by effectively combining reasoning capabilities with retrieval mechanisms.'}, 'zh': {'title': '增强事实性的推理模型ReaRAG', 'desc': '大型推理模型（LRMs）展现了卓越的推理能力，但主要依赖参数知识，限制了事实准确性。虽然最近的研究为基于强化学习的LRMs增加了检索能力，但它们在推理时容易过度思考，缺乏稳健性，从而降低了在问答任务中的有效性。为了解决这个问题，我们提出了ReaRAG，这是一种增强事实性的推理模型，能够在不进行过多迭代的情况下探索多样化的查询。我们的解决方案包括一个新颖的数据构建框架，并对推理链的长度设定上限，从而提高了LRMs的事实性和推理的稳健性。'}}}, {'id': 'https://huggingface.co/papers/2503.21696', 'title': 'Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for\n  Embodied Interactive Tasks', 'url': 'https://huggingface.co/papers/2503.21696', 'abstract': "Recent advances in deep thinking models have demonstrated remarkable reasoning capabilities on mathematical and coding tasks. However, their effectiveness in embodied domains which require continuous interaction with environments through image action interleaved trajectories remains largely -unexplored. We present Embodied Reasoner, a model that extends o1 style reasoning to interactive embodied search tasks. Unlike mathematical reasoning that relies primarily on logical deduction, embodied scenarios demand spatial understanding, temporal reasoning, and ongoing self-reflection based on interaction history. To address these challenges, we synthesize 9.3k coherent Observation-Thought-Action trajectories containing 64k interactive images and 90k diverse thinking processes (analysis, spatial reasoning, reflection, planning, and verification). We develop a three-stage training pipeline that progressively enhances the model's capabilities through imitation learning, self-exploration via rejection sampling, and self-correction through reflection tuning. The evaluation shows that our model significantly outperforms those advanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and Claude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer repeated searches and logical inconsistencies, with particular advantages in complex long-horizon tasks. Real-world environments also show our superiority while exhibiting fewer repeated searches and logical inconsistency cases.", 'score': 11, 'issue_id': 2946, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': 'a52516705bc7a122', 'authors': ['Wenqi Zhang', 'Mengna Wang', 'Gangao Liu', 'Xu Huixin', 'Yiwei Jiang', 'Yongliang Shen', 'Guiyang Hou', 'Zhe Zheng', 'Hang Zhang', 'Xin Li', 'Weiming Lu', 'Peng Li', 'Yueting Zhuang'], 'affiliations': ['Alibaba Group', 'College of Computer Science and Technology, Zhejiang University', 'DAMO Academy, Alibaba Group', 'Hohai University', 'Institute of Software, Chinese Academy of Sciences', 'Nanjing Institute of Software Technology', 'Nanjing University of Posts and Telecommunications', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.21696.jpg', 'data': {'categories': ['#cv', '#optimization', '#reasoning', '#agents', '#training'], 'emoji': '🤖', 'ru': {'title': 'Воплощённый разум: новый подход к рассуждениям в физическом мире', 'desc': 'Статья представляет модель Embodied Reasoner, которая расширяет возможности рассуждений в стиле GPT-4 для интерактивных задач поиска в физическом мире. Модель обучается на синтезированном наборе данных, содержащем траектории наблюдений, мыслей и действий. Авторы разработали трехэтапный процесс обучения, включающий имитационное обучение, самоисследование и самокоррекцию. Результаты показывают, что Embodied Reasoner превосходит современные модели визуального рассуждения, особенно в сложных долгосрочных задачах.'}, 'en': {'title': 'Empowering Reasoning in Interactive Environments', 'desc': 'This paper introduces the Embodied Reasoner, a model designed to enhance reasoning in interactive environments that require continuous engagement through visual and action-based tasks. Unlike traditional mathematical reasoning, this model focuses on spatial understanding and temporal reasoning, which are crucial for navigating real-world scenarios. The authors created a large dataset of 9.3k Observation-Thought-Action trajectories to train the model, employing a three-stage training process that includes imitation learning and self-correction. The results demonstrate that the Embodied Reasoner outperforms existing visual reasoning models, showing improved efficiency and fewer logical errors in complex tasks.'}, 'zh': {'title': '提升交互式推理能力的全新模型', 'desc': '最近深度学习模型在数学和编程任务上展现了出色的推理能力，但在需要与环境持续互动的实际应用领域中，其有效性仍未得到充分探索。我们提出了"Embodied Reasoner"模型，旨在将推理能力扩展到交互式的实际搜索任务中。与主要依赖逻辑推理的数学推理不同，实际场景需要空间理解、时间推理以及基于互动历史的自我反思。通过合成9.3千条连贯的观察-思考-行动轨迹，我们开发了一个三阶段的训练流程，显著提升了模型在复杂任务中的表现。'}}}, {'id': 'https://huggingface.co/papers/2503.21248', 'title': 'ResearchBench: Benchmarking LLMs in Scientific Discovery via\n  Inspiration-Based Task Decomposition', 'url': 'https://huggingface.co/papers/2503.21248', 'abstract': 'Large language models (LLMs) have demonstrated potential in assisting scientific research, yet their ability to discover high-quality research hypotheses remains unexamined due to the lack of a dedicated benchmark. To address this gap, we introduce the first large-scale benchmark for evaluating LLMs with a near-sufficient set of sub-tasks of scientific discovery: inspiration retrieval, hypothesis composition, and hypothesis ranking. We develop an automated framework that extracts critical components - research questions, background surveys, inspirations, and hypotheses - from scientific papers across 12 disciplines, with expert validation confirming its accuracy. To prevent data contamination, we focus exclusively on papers published in 2024, ensuring minimal overlap with LLM pretraining data. Our evaluation reveals that LLMs perform well in retrieving inspirations, an out-of-distribution task, suggesting their ability to surface novel knowledge associations. This positions LLMs as "research hypothesis mines", capable of facilitating automated scientific discovery by generating innovative hypotheses at scale with minimal human intervention.', 'score': 10, 'issue_id': 2944, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': 'c58f620a0f532dc0', 'authors': ['Yujie Liu', 'Zonglin Yang', 'Tong Xie', 'Jinjie Ni', 'Ben Gao', 'Yuqiang Li', 'Shixiang Tang', 'Wanli Ouyang', 'Erik Cambria', 'Dongzhan Zhou'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'Shanghai Artificial Intelligence Laboratory', 'University of New South Wales', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21248.jpg', 'data': {'categories': ['#benchmark', '#science', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'LLM как генератор научных гипотез: новый бенчмарк открывает потенциал ИИ в исследованиях', 'desc': 'Эта статья представляет первый масштабный бенчмарк для оценки способности больших языковых моделей (LLM) генерировать качественные научные гипотезы. Авторы разработали автоматизированную систему для извлечения ключевых компонентов из научных статей в 12 дисциплинах, включая исследовательские вопросы, обзоры литературы, источники вдохновения и гипотезы. Оценка показала, что LLM хорошо справляются с поиском вдохновения, что говорит об их способности находить новые ассоциации знаний. Это позиционирует LLM как перспективный инструмент для автоматизированного научного открытия, способный генерировать инновационные гипотезы в больших масштабах.'}, 'en': {'title': 'Unlocking Scientific Discovery with LLMs', 'desc': 'This paper introduces a new benchmark to evaluate large language models (LLMs) in the context of scientific research. It focuses on three key tasks: retrieving inspirations, composing hypotheses, and ranking them based on quality. The authors developed an automated framework that accurately extracts essential elements from scientific papers, validated by experts. The findings indicate that LLMs excel at retrieving inspirations, highlighting their potential to generate innovative research hypotheses efficiently.'}, 'zh': {'title': '大型语言模型助力科学发现的潜力', 'desc': '大型语言模型（LLMs）在科学研究中显示出潜力，但它们发现高质量研究假设的能力尚未得到验证。为了解决这个问题，我们引入了第一个大规模基准，用于评估LLMs在科学发现中的表现，包括灵感检索、假设构建和假设排序等子任务。我们开发了一个自动化框架，从12个学科的科学论文中提取关键组件，并通过专家验证确认其准确性。评估结果表明，LLMs在检索灵感方面表现良好，表明它们能够发现新的知识关联，从而推动自动化科学发现。'}}}, {'id': 'https://huggingface.co/papers/2503.21774', 'title': 'Optimal Stepsize for Diffusion Sampling', 'url': 'https://huggingface.co/papers/2503.21774', 'abstract': 'Diffusion models achieve remarkable generation quality but suffer from computational intensive sampling due to suboptimal step discretization. While existing works focus on optimizing denoising directions, we address the principled design of stepsize schedules. This paper proposes Optimal Stepsize Distillation, a dynamic programming framework that extracts theoretically optimal schedules by distilling knowledge from reference trajectories. By reformulating stepsize optimization as recursive error minimization, our method guarantees global discretization bounds through optimal substructure exploitation. Crucially, the distilled schedules demonstrate strong robustness across architectures, ODE solvers, and noise schedules. Experiments show 10x accelerated text-to-image generation while preserving 99.4% performance on GenEval. Our code is available at https://github.com/bebebe666/OptimalSteps.', 'score': 9, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '709832ee6e0a6f3f', 'authors': ['Jianning Pei', 'Han Hu', 'Shuyang Gu'], 'affiliations': ['Tencent Hunyuan Research', 'University Chinese Academic of Science'], 'pdf_title_img': 'assets/pdf/title_img/2503.21774.jpg', 'data': {'categories': ['#training', '#architecture', '#data', '#diffusion', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Ускорение диффузионных моделей без потери качества', 'desc': 'Это исследование предлагает новый метод оптимизации процесса генерации изображений с помощью диффузионных моделей. Авторы разработали фреймворк Optimal Stepsize Distillation, который использует динамическое программирование для извлечения оптимальных расписаний шагов из эталонных траекторий. Метод гарантирует глобальные границы дискретизации и демонстрирует устойчивость к различным архитектурам и шумовым расписаниям. Эксперименты показывают 10-кратное ускорение генерации изображений по тексту при сохранении 99.4% производительности на GenEval.'}, 'en': {'title': 'Accelerating Diffusion Models with Optimal Stepsize Distillation', 'desc': 'This paper introduces a new method called Optimal Stepsize Distillation to improve the efficiency of diffusion models in generating images. It focuses on optimizing the stepsize schedules used during the sampling process, which is often computationally intensive. By using dynamic programming, the method distills optimal schedules from reference trajectories, ensuring that the stepsize is effectively minimized. The results show that this approach can speed up text-to-image generation by 10 times while maintaining high performance levels.'}, 'zh': {'title': '最优步长蒸馏：加速扩散模型生成的关键', 'desc': '扩散模型在生成质量上表现出色，但由于步骤离散化不理想，导致计算开销大。本文提出了一种名为最优步长蒸馏的动态规划框架，通过从参考轨迹中提取理论最优的步长调度来解决这一问题。我们将步长优化重新表述为递归误差最小化，从而保证了全局离散化界限。实验结果表明，该方法在保持99.4%性能的同时，实现了文本到图像生成的10倍加速。'}}}, {'id': 'https://huggingface.co/papers/2503.21765', 'title': 'Exploring the Evolution of Physics Cognition in Video Generation: A\n  Survey', 'url': 'https://huggingface.co/papers/2503.21765', 'abstract': 'Recent advancements in video generation have witnessed significant progress, especially with the rapid advancement of diffusion models. Despite this, their deficiencies in physical cognition have gradually received widespread attention - generated content often violates the fundamental laws of physics, falling into the dilemma of \'\'visual realism but physical absurdity". Researchers began to increasingly recognize the importance of physical fidelity in video generation and attempted to integrate heuristic physical cognition such as motion representations and physical knowledge into generative systems to simulate real-world dynamic scenarios. Considering the lack of a systematic overview in this field, this survey aims to provide a comprehensive summary of architecture designs and their applications to fill this gap. Specifically, we discuss and organize the evolutionary process of physical cognition in video generation from a cognitive science perspective, while proposing a three-tier taxonomy: 1) basic schema perception for generation, 2) passive cognition of physical knowledge for generation, and 3) active cognition for world simulation, encompassing state-of-the-art methods, classical paradigms, and benchmarks. Subsequently, we emphasize the inherent key challenges in this domain and delineate potential pathways for future research, contributing to advancing the frontiers of discussion in both academia and industry. Through structured review and interdisciplinary analysis, this survey aims to provide directional guidance for developing interpretable, controllable, and physically consistent video generation paradigms, thereby propelling generative models from the stage of \'\'visual mimicry\'\' towards a new phase of \'\'human-like physical comprehension\'\'.', 'score': 8, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '87e887fdf8f612cf', 'authors': ['Minghui Lin', 'Xiang Wang', 'Yishan Wang', 'Shu Wang', 'Fengqi Dai', 'Pengxiang Ding', 'Cunxiang Wang', 'Zhengrong Zuo', 'Nong Sang', 'Siteng Huang', 'Donglin Wang'], 'affiliations': ['Huazhong University of Science and Technology', 'Shandong University', 'Tsinghua University', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21765.jpg', 'data': {'categories': ['#benchmark', '#survey', '#video', '#architecture', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'От визуального мимикрии к человекоподобному пониманию физики в генерации видео', 'desc': 'Этот обзор посвящен интеграции физического познания в генерацию видео с использованием генеративных моделей. Авторы предлагают трехуровневую таксономию, охватывающую восприятие базовых схем, пассивное познание физических законов и активное моделирование мира. В статье обсуждаются современные методы, классические парадигмы и эталонные тесты в этой области. Исследование направлено на развитие интерпретируемых, управляемых и физически согласованных парадигм генерации видео.'}, 'en': {'title': 'From Visual Mimicry to Human-like Physical Comprehension in Video Generation', 'desc': 'This paper discusses the recent improvements in video generation using diffusion models, highlighting their limitations in understanding physical laws. It points out that while these models can create visually appealing content, they often produce results that do not adhere to the principles of physics, leading to unrealistic scenarios. The authors propose a structured overview of how physical cognition can be integrated into video generation, categorizing it into three levels: basic perception, passive knowledge, and active simulation. The survey aims to guide future research towards creating video generation systems that are not only visually realistic but also physically coherent, moving beyond mere visual mimicry to a deeper understanding of the physical world.'}, 'zh': {'title': '推动视频生成向人类物理理解的新阶段', 'desc': '近年来，视频生成技术取得了显著进展，尤其是扩散模型的快速发展。然而，这些模型在物理认知方面的不足逐渐引起了广泛关注，生成的内容常常违反基本的物理法则，陷入了“视觉真实但物理荒谬”的困境。研究人员开始认识到物理真实感在视频生成中的重要性，并尝试将启发式的物理认知融入生成系统，以模拟现实世界的动态场景。本文综述了物理认知在视频生成中的演变过程，提出了三层次的分类法，并强调了该领域的关键挑战和未来研究的潜在方向。'}}}, {'id': 'https://huggingface.co/papers/2503.21144', 'title': 'ChatAnyone: Stylized Real-time Portrait Video Generation with\n  Hierarchical Motion Diffusion Model', 'url': 'https://huggingface.co/papers/2503.21144', 'abstract': 'Real-time interactive video-chat portraits have been increasingly recognized as the future trend, particularly due to the remarkable progress made in text and voice chat technologies. However, existing methods primarily focus on real-time generation of head movements, but struggle to produce synchronized body motions that match these head actions. Additionally, achieving fine-grained control over the speaking style and nuances of facial expressions remains a challenge. To address these limitations, we introduce a novel framework for stylized real-time portrait video generation, enabling expressive and flexible video chat that extends from talking head to upper-body interaction. Our approach consists of the following two stages. The first stage involves efficient hierarchical motion diffusion models, that take both explicit and implicit motion representations into account based on audio inputs, which can generate a diverse range of facial expressions with stylistic control and synchronization between head and body movements. The second stage aims to generate portrait video featuring upper-body movements, including hand gestures. We inject explicit hand control signals into the generator to produce more detailed hand movements, and further perform face refinement to enhance the overall realism and expressiveness of the portrait video. Additionally, our approach supports efficient and continuous generation of upper-body portrait video in maximum 512 * 768 resolution at up to 30fps on 4090 GPU, supporting interactive video-chat in real-time. Experimental results demonstrate the capability of our approach to produce portrait videos with rich expressiveness and natural upper-body movements.', 'score': 6, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '12e50e9826751c62', 'authors': ['Jinwei Qi', 'Chaonan Ji', 'Sheng Xu', 'Peng Zhang', 'Bang Zhang', 'Liefeng Bo'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.21144.jpg', 'data': {'categories': ['#cv', '#multimodal', '#video', '#diffusion'], 'emoji': '🎥', 'ru': {'title': 'Реалистичные видеопортреты с движениями тела для видеочатов в реальном времени', 'desc': 'Эта статья представляет новую систему для генерации стилизованных видеопортретов в реальном времени для видеочатов. Система использует иерархические модели диффузии движения, учитывающие как явные, так и неявные представления движения на основе аудиовхода. Она позволяет генерировать разнообразные выражения лица с контролем стиля и синхронизацией движений головы и тела. Система также включает генерацию движений верхней части тела, включая жесты рук, и дополнительное улучшение лица для повышения реалистичности.'}, 'en': {'title': 'Expressive Real-Time Video Chats with Synchronized Body Movements', 'desc': 'This paper presents a new framework for creating real-time interactive video chat portraits that include both head and upper-body movements. It utilizes hierarchical motion diffusion models to synchronize facial expressions and body motions based on audio inputs, allowing for expressive and stylistically controlled video generation. The framework also incorporates explicit hand control signals to enhance the realism of hand gestures and facial refinements. Overall, the approach enables high-quality, interactive video chats at a resolution of 512 * 768 and up to 30 frames per second.'}, 'zh': {'title': '实时互动视频聊天的未来趋势', 'desc': '本论文提出了一种新颖的实时肖像视频生成框架，旨在解决现有方法在头部动作与身体动作同步方面的不足。该框架通过高效的层次运动扩散模型，结合音频输入生成多样化的面部表情，并实现头部与身体动作的协调。第二阶段则专注于生成包含上半身动作的肖像视频，通过注入手部控制信号来增强手部动作的细节，并进行面部细化以提升视频的真实感和表现力。实验结果表明，该方法能够以高达30fps的速度生成丰富表现力和自然上半身动作的肖像视频，适用于实时互动视频聊天。'}}}, {'id': 'https://huggingface.co/papers/2503.20990', 'title': 'FinAudio: A Benchmark for Audio Large Language Models in Financial\n  Applications', 'url': 'https://huggingface.co/papers/2503.20990', 'abstract': 'Audio Large Language Models (AudioLLMs) have received widespread attention and have significantly improved performance on audio tasks such as conversation, audio understanding, and automatic speech recognition (ASR). Despite these advancements, there is an absence of a benchmark for assessing AudioLLMs in financial scenarios, where audio data, such as earnings conference calls and CEO speeches, are crucial resources for financial analysis and investment decisions. In this paper, we introduce FinAudio, the first benchmark designed to evaluate the capacity of AudioLLMs in the financial domain. We first define three tasks based on the unique characteristics of the financial domain: 1) ASR for short financial audio, 2) ASR for long financial audio, and 3) summarization of long financial audio. Then, we curate two short and two long audio datasets, respectively, and develop a novel dataset for financial audio summarization, comprising the FinAudio benchmark. Then, we evaluate seven prevalent AudioLLMs on FinAudio. Our evaluation reveals the limitations of existing AudioLLMs in the financial domain and offers insights for improving AudioLLMs. All datasets and codes will be released.', 'score': 5, 'issue_id': 2943, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': '55f780e7347209e5', 'authors': ['Yupeng Cao', 'Haohang Li', 'Yangyang Yu', 'Shashidhar Reddy Javaji', 'Yueru He', 'Jimin Huang', 'Zining Zhu', 'Qianqian Xie', 'Xiao-yang Liu', 'Koduvayur Subbalakshmi', 'Meikang Qiu', 'Sophia Ananiadou', 'Jian-Yun Nie'], 'affiliations': ['Augusta University', 'Columbia University', 'Stevens Institute of Technology', 'The Fin AI', 'The University of Manchester', 'University of Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2503.20990.jpg', 'data': {'categories': ['#audio', '#benchmark', '#dataset'], 'emoji': '🎙️', 'ru': {'title': 'FinAudio: Первый бенчмарк для оценки аудио-ИИ в финансах', 'desc': 'Статья представляет FinAudio - первый бенчмарк для оценки аудио-языковых моделей (AudioLLMs) в финансовой сфере. Авторы определяют три задачи: распознавание коротких и длинных финансовых аудиозаписей, а также их суммаризация. Для бенчмарка созданы наборы данных коротких и длинных аудио, а также уникальный датасет для суммаризации финансового аудио. Оценка семи популярных AudioLLMs на FinAudio выявила ограничения существующих моделей в финансовой области.'}, 'en': {'title': 'Benchmarking AudioLLMs for Financial Insights', 'desc': 'This paper introduces FinAudio, a benchmark specifically designed to evaluate Audio Large Language Models (AudioLLMs) in financial contexts. It identifies three key tasks: automatic speech recognition (ASR) for both short and long financial audio, and summarization of long financial audio. The authors curate datasets tailored to these tasks, highlighting the unique characteristics of financial audio data. The evaluation of seven existing AudioLLMs on this benchmark reveals their limitations and suggests areas for enhancement in their performance within the financial domain.'}, 'zh': {'title': '金融领域音频模型评估新基准', 'desc': '音频大型语言模型（AudioLLMs）在对话、音频理解和自动语音识别等音频任务上取得了显著进展。然而，目前缺乏一个专门用于评估AudioLLMs在金融场景中的基准。本文提出了FinAudio，这是第一个旨在评估AudioLLMs在金融领域能力的基准，定义了三个基于金融领域特征的任务，并创建了相应的数据集。通过对七个流行的AudioLLMs进行评估，我们揭示了现有模型在金融领域的局限性，并为改进AudioLLMs提供了见解。'}}}, {'id': 'https://huggingface.co/papers/2503.20822', 'title': 'Synthetic Video Enhances Physical Fidelity in Video Synthesis', 'url': 'https://huggingface.co/papers/2503.20822', 'abstract': 'We investigate how to enhance the physical fidelity of video generation models by leveraging synthetic videos derived from computer graphics pipelines. These rendered videos respect real-world physics, such as maintaining 3D consistency, and serve as a valuable resource that can potentially improve video generation models. To harness this potential, we propose a solution that curates and integrates synthetic data while introducing a method to transfer its physical realism to the model, significantly reducing unwanted artifacts. Through experiments on three representative tasks emphasizing physical consistency, we demonstrate its efficacy in enhancing physical fidelity. While our model still lacks a deep understanding of physics, our work offers one of the first empirical demonstrations that synthetic video enhances physical fidelity in video synthesis. Website: https://kevinz8866.github.io/simulation/', 'score': 5, 'issue_id': 2941, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': '8ad7de4de496fce6', 'authors': ['Qi Zhao', 'Xingyu Ni', 'Ziyu Wang', 'Feng Cheng', 'Ziyan Yang', 'Lu Jiang', 'Bohan Wang'], 'affiliations': ['ByteDance Seed', 'National University of Singapore', 'Peking University', 'ShanghaiTech University'], 'pdf_title_img': 'assets/pdf/title_img/2503.20822.jpg', 'data': {'categories': ['#dataset', '#video', '#data', '#synthetic', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'Синтетические видео для реалистичной генерации: шаг к физической достоверности', 'desc': 'Исследователи изучают, как улучшить физическую достоверность моделей генерации видео, используя синтетические видео из компьютерной графики. Эти рендеренные видео соблюдают законы физики реального мира и могут улучшить модели генерации видео. Авторы предлагают метод интеграции синтетических данных и передачи их физического реализма модели, значительно уменьшая нежелательные артефакты. Эксперименты на трех задачах, подчеркивающих физическую согласованность, демонстрируют эффективность метода в повышении физической достоверности.'}, 'en': {'title': 'Enhancing Video Realism with Synthetic Physics', 'desc': 'This paper explores how to improve the realism of video generation models by using synthetic videos created through computer graphics. These synthetic videos adhere to real-world physics, ensuring 3D consistency, which can enhance the training of video generation models. The authors propose a method to curate and integrate this synthetic data, allowing the model to adopt the physical realism of the videos and reduce visual artifacts. Their experiments show that this approach effectively boosts the physical fidelity of generated videos, marking a significant step in the field of video synthesis.'}, 'zh': {'title': '合成视频提升视频生成的物理真实性', 'desc': '我们研究了如何通过利用计算机图形学生成的合成视频来增强视频生成模型的物理真实性。这些渲染视频遵循现实世界的物理规律，保持三维一致性，成为改善视频生成模型的重要资源。我们提出了一种解决方案，策划和整合合成数据，同时引入了一种将物理真实感转移到模型的方法，显著减少了不必要的伪影。通过在三个强调物理一致性的代表性任务上的实验，我们证明了这种方法在提高物理真实性方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.21088', 'title': 'ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging', 'url': 'https://huggingface.co/papers/2503.21088', 'abstract': "This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4: Unlearning Sensitive Content from Large Language Models. This task aims to selectively erase sensitive knowledge from large language models, avoiding both over-forgetting and under-forgetting issues. We propose an unlearning system that leverages Model Merging (specifically TIES-Merging), combining two specialized models into a more balanced unlearned model. Our system achieves competitive results, ranking second among 26 teams, with an online score of 0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we also conduct local experiments and perform a comprehensive analysis of the unlearning process, examining performance trajectories, loss dynamics, and weight perspectives, along with several supplementary experiments, to understand the effectiveness of our method. Furthermore, we analyze the shortcomings of our method and evaluation metrics, emphasizing that MIA scores and ROUGE-based metrics alone are insufficient to fully evaluate successful unlearning. Finally, we emphasize the need for more comprehensive evaluation methodologies and rethinking of unlearning objectives in future research. Code is available at https://github.com/zjunlp/unlearn/tree/main/semeval25.", 'score': 4, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '494379ac783f4297', 'authors': ['Haoming Xu', 'Shuxun Wang', 'Yanqiu Zhao', 'Yi Zhong', 'Ziyan Jiang', 'Ningyuan Zhao', 'Shumin Deng', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['National University of Singapore', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21088.jpg', 'data': {'categories': ['#benchmark', '#training', '#data', '#leakage', '#ethics', '#hallucinations'], 'emoji': '🧠', 'ru': {'title': 'Избирательное забывание: новый подход к конфиденциальности в LLM', 'desc': "Статья представляет систему для избирательного удаления конфиденциальных знаний из больших языковых моделей (LLM). Авторы используют метод слияния моделей (Model Merging), в частности TIES-Merging, для создания сбалансированной модели с 'забытыми' данными. Система показала высокие результаты на соревновании SemEval-2025, заняв второе место среди 26 команд. В работе также проводится анализ процесса разобучения (unlearning) и обсуждаются ограничения существующих методов оценки эффективности удаления знаний из моделей."}, 'en': {'title': 'Mastering Unlearning: Balancing Sensitivity in Language Models', 'desc': "This paper discusses the ZJUKLAB team's approach to the SemEval-2025 Task 4, which focuses on unlearning sensitive information from large language models. The proposed method utilizes Model Merging, specifically TIES-Merging, to create a balanced model that effectively manages the challenges of over-forgetting and under-forgetting. The team achieved impressive results, ranking second out of 26 participants, and conducted thorough experiments to analyze the unlearning process, including performance metrics and loss dynamics. The authors also highlight the limitations of current evaluation methods and advocate for improved metrics to better assess unlearning effectiveness in future studies."}, 'zh': {'title': '选择性删除，重塑语言模型的未来', 'desc': '本文介绍了ZJUKLAB团队在SemEval-2025任务4中的提交，旨在从大型语言模型中选择性地删除敏感内容。我们提出了一种利用模型合并（特别是TIES-Merging）的方法，将两个专门模型结合成一个更平衡的未学习模型。我们的系统在26个团队中排名第二，任务聚合的在线得分为0.944，总体聚合得分为0.487。我们还进行了局部实验，全面分析了未学习过程的表现轨迹、损失动态和权重视角，并强调了现有评估指标的不足，呼吁未来研究需要更全面的评估方法。'}}}, {'id': 'https://huggingface.co/papers/2503.20776', 'title': 'Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile\n  Gaussian Feature Fields', 'url': 'https://huggingface.co/papers/2503.20776', 'abstract': 'Recent advancements in 2D and multimodal models have achieved remarkable success by leveraging large-scale training on extensive datasets. However, extending these achievements to enable free-form interactions and high-level semantic operations with complex 3D/4D scenes remains challenging. This difficulty stems from the limited availability of large-scale, annotated 3D/4D or multi-view datasets, which are crucial for generalizable vision and language tasks such as open-vocabulary and prompt-based segmentation, language-guided editing, and visual question answering (VQA). In this paper, we introduce Feature4X, a universal framework designed to extend any functionality from 2D vision foundation model into the 4D realm, using only monocular video input, which is widely available from user-generated content. The "X" in Feature4X represents its versatility, enabling any task through adaptable, model-conditioned 4D feature field distillation. At the core of our framework is a dynamic optimization strategy that unifies multiple model capabilities into a single representation. Additionally, to the best of our knowledge, Feature4X is the first method to distill and lift the features of video foundation models (e.g. SAM2, InternVideo2) into an explicit 4D feature field using Gaussian Splatting. Our experiments showcase novel view segment anything, geometric and appearance scene editing, and free-form VQA across all time steps, empowered by LLMs in feedback loops. These advancements broaden the scope of agentic AI applications by providing a foundation for scalable, contextually and spatiotemporally aware systems capable of immersive dynamic 4D scene interaction.', 'score': 4, 'issue_id': 2941, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': 'd066b6a18982ec5f', 'authors': ['Shijie Zhou', 'Hui Ren', 'Yijia Weng', 'Shuwang Zhang', 'Zhen Wang', 'Dejia Xu', 'Zhiwen Fan', 'Suya You', 'Zhangyang Wang', 'Leonidas Guibas', 'Achuta Kadambi'], 'affiliations': ['DEVCOM ARL', 'MIT', 'Stanford', 'UCLA', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2503.20776.jpg', 'data': {'categories': ['#3d', '#cv', '#agi', '#agents', '#dataset', '#multimodal', '#optimization'], 'emoji': '🌐', 'ru': {'title': 'Feature4X: Универсальный мост между 2D и 4D компьютерным зрением', 'desc': 'Статья представляет Feature4X - универсальную систему для расширения функциональности 2D моделей компьютерного зрения на 4D пространство, используя только монокулярное видео. Feature4X позволяет выполнять сегментацию, редактирование и ответы на вопросы в 4D сценах с помощью языковых подсказок. В основе подхода лежит оптимизация, объединяющая возможности нескольких моделей в единое представление. Это первый метод, который преобразует признаки видео-моделей в явное 4D поле признаков с помощью Gaussian Splatting.'}, 'en': {'title': 'Feature4X: Bridging 2D Vision to 4D Interaction', 'desc': 'This paper presents Feature4X, a novel framework that enhances 2D vision models to operate in 4D environments using only monocular video inputs. It addresses the challenge of limited annotated datasets for 3D/4D tasks by enabling versatile interactions and semantic operations in complex scenes. The framework employs a dynamic optimization strategy to unify various model capabilities into a single representation, allowing for adaptable feature extraction. Feature4X is the first to distill video foundation model features into a 4D feature field, facilitating advanced tasks like segmentation, scene editing, and visual question answering with improved context awareness.'}, 'zh': {'title': 'Feature4X：将2D视觉扩展到4D的通用框架', 'desc': '本文介绍了一种名为Feature4X的通用框架，旨在将2D视觉基础模型的功能扩展到4D领域。该框架仅使用单目视频输入，解决了3D/4D数据集稀缺的问题。Feature4X通过动态优化策略，将多种模型能力统一为单一表示，支持开放词汇和基于提示的分割、语言引导编辑和视觉问答等任务。实验结果表明，该方法能够实现新视角的分割、几何和外观场景编辑，以及跨时间步的自由形式视觉问答，推动了智能代理AI应用的发展。'}}}, {'id': 'https://huggingface.co/papers/2503.20853', 'title': 'Unified Multimodal Discrete Diffusion', 'url': 'https://huggingface.co/papers/2503.20853', 'abstract': 'Multimodal generative models that can understand and generate across multiple modalities are dominated by autoregressive (AR) approaches, which process tokens sequentially from left to right, or top to bottom. These models jointly handle images, text, video, and audio for various tasks such as image captioning, question answering, and image generation. In this work, we explore discrete diffusion models as a unified generative formulation in the joint text and image domain, building upon their recent success in text generation. Discrete diffusion models offer several advantages over AR models, including improved control over quality versus diversity of generated samples, the ability to perform joint multimodal inpainting (across both text and image domains), and greater controllability in generation through guidance. Leveraging these benefits, we present the first Unified Multimodal Discrete Diffusion (UniDisc) model which is capable of jointly understanding and generating text and images for a variety of downstream tasks. We compare UniDisc to multimodal AR models, performing a scaling analysis and demonstrating that UniDisc outperforms them in terms of both performance and inference-time compute, enhanced controllability, editability, inpainting, and flexible trade-off between inference time and generation quality. Code and additional visualizations are available at https://unidisc.github.io.', 'score': 3, 'issue_id': 2941, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': '9650b4dd1188fcc0', 'authors': ['Alexander Swerdlow', 'Mihir Prabhudesai', 'Siddharth Gandhi', 'Deepak Pathak', 'Katerina Fragkiadaki'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.20853.jpg', 'data': {'categories': ['#cv', '#training', '#audio', '#multimodal', '#video', '#diffusion'], 'emoji': '🧠', 'ru': {'title': 'UniDisc: Новый подход к мультимодальному машинному обучению', 'desc': 'В статье представлена модель UniDisc, первая унифицированная мультимодальная модель дискретной диффузии, способная совместно понимать и генерировать текст и изображения для различных задач. Модель UniDisc предлагает ряд преимуществ по сравнению с авторегрессионными моделями, включая улучшенный контроль над качеством и разнообразием генерируемых образцов, возможность совместного мультимодального инпейнтинга и большую управляемость при генерации. Авторы проводят сравнительный анализ с мультимодальными авторегрессионными моделями, демонстрируя превосходство UniDisc по производительности и вычислительным затратам при выводе. Модель также обеспечивает гибкий компромисс между временем вывода и качеством генерации.'}, 'en': {'title': 'Revolutionizing Multimodal Generation with UniDisc!', 'desc': 'This paper introduces the Unified Multimodal Discrete Diffusion (UniDisc) model, which is designed to generate and understand both text and images simultaneously. Unlike traditional autoregressive models that process data sequentially, UniDisc utilizes discrete diffusion techniques to enhance the quality and diversity of generated outputs. The model excels in tasks such as multimodal inpainting and offers improved controllability and editability during generation. Through extensive comparisons, UniDisc demonstrates superior performance and efficiency over existing multimodal autoregressive approaches.'}, 'zh': {'title': '统一多模态生成，超越自回归模型！', 'desc': '这篇论文探讨了一种新的多模态生成模型，称为统一多模态离散扩散模型（UniDisc）。与传统的自回归模型不同，UniDisc能够同时理解和生成文本与图像，适用于多种任务。该模型在生成样本的质量与多样性之间提供了更好的控制，并且能够进行跨文本和图像的联合修复。通过与自回归模型的比较，UniDisc在性能、计算效率和可控性等方面表现更优。'}}}, {'id': 'https://huggingface.co/papers/2503.21541', 'title': 'LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized\n  Text-Guided Image Editing', 'url': 'https://huggingface.co/papers/2503.21541', 'abstract': 'Text-guided image editing aims to modify specific regions of an image according to natural language instructions while maintaining the general structure and the background fidelity. Existing methods utilize masks derived from cross-attention maps generated from diffusion models to identify the target regions for modification. However, since cross-attention mechanisms focus on semantic relevance, they struggle to maintain the image integrity. As a result, these methods often lack spatial consistency, leading to editing artifacts and distortions. In this work, we address these limitations and introduce LOCATEdit, which enhances cross-attention maps through a graph-based approach utilizing self-attention-derived patch relationships to maintain smooth, coherent attention across image regions, ensuring that alterations are limited to the designated items while retaining the surrounding structure. \\method consistently and substantially outperforms existing baselines on PIE-Bench, demonstrating its state-of-the-art performance and effectiveness on various editing tasks. Code can be found on https://github.com/LOCATEdit/LOCATEdit/', 'score': 1, 'issue_id': 2950, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '38e7c3d2a3738793', 'authors': ['Achint Soni', 'Meet Soni', 'Sirisha Rambhatla'], 'affiliations': ['Stony Brook University', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2503.21541.jpg', 'data': {'categories': ['#cv', '#multimodal', '#architecture', '#open_source', '#games', '#optimization', '#graphs', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'Точное редактирование изображений с сохранением структуры', 'desc': 'LOCATEdit - это новый метод редактирования изображений на основе текстовых инструкций. Он использует графовый подход для улучшения карт внимания, полученных из диффузионных моделей. Это позволяет сохранять целостность изображения и избегать артефактов при редактировании. LOCATEdit превосходит существующие методы на бенчмарке PIE-Bench в различных задачах редактирования.'}, 'en': {'title': 'Enhancing Text-Guided Image Editing with LOCATEdit', 'desc': 'This paper presents LOCATEdit, a novel approach for text-guided image editing that improves upon existing methods by addressing issues of spatial consistency. Traditional techniques rely on cross-attention maps from diffusion models, which can lead to artifacts due to their focus on semantic relevance rather than spatial integrity. LOCATEdit enhances these maps using a graph-based method that leverages self-attention to maintain coherent attention across different regions of the image. The results show that LOCATEdit significantly outperforms current baselines on the PIE-Bench dataset, proving its effectiveness in preserving the overall structure while allowing precise modifications.'}, 'zh': {'title': '精确图像编辑，保持结构完整性', 'desc': '本文介绍了一种名为LOCATEdit的图像编辑方法，旨在根据自然语言指令修改图像的特定区域，同时保持整体结构和背景的完整性。现有方法使用来自扩散模型的交叉注意力图生成的掩码来识别目标区域，但由于交叉注意力机制关注语义相关性，导致图像完整性难以保持。LOCATEdit通过基于图的自注意力方法增强交叉注意力图，确保图像区域之间的平滑一致性，从而限制修改仅在指定项目上，同时保留周围结构。实验结果表明，LOCATEdit在PIE-Bench上显著优于现有基线，展示了其在各种编辑任务中的先进性能和有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.20578', 'title': 'LLPut: Investigating Large Language Models for Bug Report-Based Input\n  Generation', 'url': 'https://huggingface.co/papers/2503.20578', 'abstract': 'Failure-inducing inputs play a crucial role in diagnosing and analyzing software bugs. Bug reports typically contain these inputs, which developers extract to facilitate debugging. Since bug reports are written in natural language, prior research has leveraged various Natural Language Processing (NLP) techniques for automated input extraction. With the advent of Large Language Models (LLMs), an important research question arises: how effectively can generative LLMs extract failure-inducing inputs from bug reports? In this paper, we propose LLPut, a technique to empirically evaluate the performance of three open-source generative LLMs -- LLaMA, Qwen, and Qwen-Coder -- in extracting relevant inputs from bug reports. We conduct an experimental evaluation on a dataset of 206 bug reports to assess the accuracy and effectiveness of these models. Our findings provide insights into the capabilities and limitations of generative LLMs in automated bug diagnosis.', 'score': 1, 'issue_id': 2941, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': '5ee433dad4dd5d00', 'authors': ['Alif Al Hasan', 'Subarna Saha', 'Mia Mohammad Imran', 'Tarannum Shaila Zaman'], 'affiliations': ['Jahangirnagar University Dhaka, Bangladesh', 'Missouri University of Science and Technology Rolla, Missouri, USA', 'University of Maryland Baltimore County Bsltimore, Maryland, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.20578.jpg', 'data': {'categories': ['#open_source', '#science', '#dataset', '#multimodal', '#data'], 'emoji': '🐛', 'ru': {'title': 'Генеративные языковые модели на страже отладки: извлечение данных об ошибках из отчетов', 'desc': 'Это исследование посвящено оценке эффективности генеративных языковых моделей (LLM) в извлечении входных данных, вызывающих ошибки, из отчетов о багах. Авторы предлагают методику LLPut для оценки производительности трех открытых LLM: LLaMA, Qwen и Qwen-Coder. Эксперимент проводился на наборе из 206 отчетов о багах для оценки точности и эффективности этих моделей. Результаты дают представление о возможностях и ограничениях генеративных LLM в автоматизированной диагностике ошибок.'}, 'en': {'title': 'Harnessing LLMs for Smart Bug Diagnosis', 'desc': 'This paper investigates how well generative Large Language Models (LLMs) can extract failure-inducing inputs from bug reports, which are essential for diagnosing software issues. The authors introduce a technique called LLPut to evaluate the performance of three open-source LLMs: LLaMA, Qwen, and Qwen-Coder. They conduct experiments on a dataset of 206 bug reports to measure the accuracy and effectiveness of these models in identifying relevant inputs. The results offer valuable insights into the strengths and weaknesses of using generative LLMs for automated bug diagnosis.'}, 'zh': {'title': '利用LLM提升缺陷报告分析的效率', 'desc': '本文探讨了如何利用大型语言模型（LLMs）从软件缺陷报告中提取导致故障的输入。我们提出了一种名为LLPut的技术，旨在评估三种开源生成性LLM（LLaMA、Qwen和Qwen-Coder）在这一任务中的表现。通过对206个缺陷报告的数据集进行实验评估，我们分析了这些模型的准确性和有效性。研究结果揭示了生成性LLM在自动化缺陷诊断中的能力和局限性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (3)', '#agi (2)', '#alignment (1)', '#architecture (4)', '#audio (2)', '#benchmark (10)', '#cv (7)', '#data (5)', '#dataset (8)', '#diffusion (5)', '#ethics (1)', '#games (2)', '#graphs (1)', '#hallucinations (1)', '#healthcare', '#inference (1)', '#interpretability', '#leakage (1)', '#long_context', '#low_resource (1)', '#machine_translation', '#math (1)', '#multilingual (1)', '#multimodal (8)', '#open_source (5)', '#optimization (7)', '#plp', '#rag (1)', '#reasoning (5)', '#rl (3)', '#rlhf', '#robotics', '#science (2)', '#security', '#small_models (1)', '#story_generation', '#survey (2)', '#synthetic (1)', '#training (7)', '#transfer_learning', '#video (5)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-03-28 12:19',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-28 12:19')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-28 12:19')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    