
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 26 papers. January 21.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">21 ÑĞ½Ğ²Ğ°Ñ€Ñ</span> | <span id="title-articles-count">26 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2026-01-20.html">â¬…ï¸ <span id="prev-date">20.01</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2026-01-22.html">â¡ï¸ <span id="next-date">22.01</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2026-01.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '21 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 21', 'zh': '1æœˆ21æ—¥'};
        let feedDateNext = {'ru': '22.01', 'en': '01/22', 'zh': '1æœˆ22æ—¥'};
        let feedDatePrev = {'ru': '20.01', 'en': '01/20', 'zh': '1æœˆ20æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2601.12993', 'title': 'Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization', 'url': 'https://huggingface.co/papers/2601.12993', 'abstract': 'Being-H0.5 is a Vision-Language-Action model that enables robust cross-embodiment generalization through human-centric learning and a Mixture-of-Transformers architecture with specialized embodiment handling.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal "mother tongue" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms.', 'score': 50, 'issue_id': 684, 'pub_date': '2026-01-19', 'pub_date_card': {'ru': '19 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 19', 'zh': '1æœˆ19æ—¥'}, 'hash': 'ccaa8b7bf86cfa7f', 'authors': ['Hao Luo', 'Ye Wang', 'Wanpeng Zhang', 'Sipeng Zheng', 'Ziheng Xi', 'Chaoyi Xu', 'Haiweng Xu', 'Haoqi Yuan', 'Chi Zhang', 'Yiqing Wang', 'Yicheng Feng', 'Zongqing Lu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2601.12993.jpg', 'data': {'categories': ['#architecture', '#dataset', '#training', '#multimodal', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ§ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²: universal VLA Ñ ĞºÑ€Ğ¾ÑÑĞ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Being-H0.5 â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Vision-Language-Action (VLA), Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ¼Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñƒ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Ğ´Ğ»Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ UniHand-2.0 â€” ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 35 Ñ‚Ñ‹ÑÑÑ‡ Ñ‡Ğ°ÑĞ¾Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ 30 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Mixture-of-Transformers Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Mixture-of-Flow (MoF) Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¼Ğ¾Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ñ‹ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… LIBERO (98.9%) Ğ¸ RoboCasa (53.9%), Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ….'}, 'en': {'title': 'Empowering Robots with Human-Centric Learning for Cross-Embodiment Mastery', 'desc': 'Being-H0.5 is a Vision-Language-Action model that enhances the ability of robots to generalize across different physical forms. It employs a human-centric learning approach, using human interaction data as a foundational guide for robotic actions. The model is built on a Mixture-of-Transformers architecture, which allows it to effectively manage various robotic embodiments and their unique control requirements. With its innovative techniques like Manifold-Preserving Gating and Universal Async Chunking, Being-H0.5 demonstrates impressive performance on multiple robotic platforms, achieving state-of-the-art results in simulated environments.'}, 'zh': {'title': 'è·¨ä½“ç°æ³›åŒ–çš„å¼ºå¤§æ¨¡å‹', 'desc': 'Being-H0.5æ˜¯ä¸€ç§è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡ä»¥äººä¸ºä¸­å¿ƒçš„å­¦ä¹ å’Œæ··åˆå˜æ¢å™¨æ¶æ„å®ç°å¼ºå¤§çš„è·¨ä½“ç°æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åˆ©ç”¨äººç±»äº¤äº’è½¨è¿¹ä½œä¸ºç‰©ç†äº¤äº’çš„é€šç”¨â€œæ¯è¯­â€ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨å½¢æ€å¼‚è´¨æ€§å’Œæ•°æ®ç¨€ç¼ºæ–¹é¢çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºçš„UniHand-2.0æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å…·èº«é¢„è®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡35,000å°æ—¶çš„å¤šæ¨¡æ€æ•°æ®ï¼Œæ¶µç›–30ç§ä¸åŒçš„æœºå™¨äººä½“ç°ã€‚é€šè¿‡ç»Ÿä¸€çš„åŠ¨ä½œç©ºé—´å’Œå¤šä»»åŠ¡é¢„è®­ç»ƒèŒƒå¼ï¼ŒBeing-H0.5èƒ½å¤Ÿåœ¨ä¸åŒçš„æœºå™¨äººå¹³å°ä¸Šå®ç°ç¨³å®šçš„è·¨ä½“ç°ç­–ç•¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.11655', 'title': 'Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey', 'url': 'https://huggingface.co/papers/2601.11655', 'abstract': 'Large language models face significant challenges in software issue resolution, prompting the development of autonomous coding agents through various training-free and training-based methodologies.  \t\t\t\t\tAI-generated summary \t\t\t\t Issue resolution, a complex Software Engineering (SWE) task integral to real-world development, has emerged as a compelling challenge for artificial intelligence. The establishment of benchmarks like SWE-bench revealed this task as profoundly difficult for large language models, thereby significantly accelerating the evolution of autonomous coding agents. This paper presents a systematic survey of this emerging domain. We begin by examining data construction pipelines, covering automated collection and synthesis approaches. We then provide a comprehensive analysis of methodologies, spanning training-free frameworks with their modular components to training-based techniques, including supervised fine-tuning and reinforcement learning. Subsequently, we discuss critical analyses of data quality and agent behavior, alongside practical applications. Finally, we identify key challenges and outline promising directions for future research. An open-source repository is maintained at https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution to serve as a dynamic resource in this field.', 'score': 47, 'issue_id': 683, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': '3bba28a3c4ff036b', 'authors': ['Caihua Li', 'Lianghong Guo', 'Yanlin Wang', 'Daya Guo', 'Wei Tao', 'Zhenyu Shan', 'Mingwei Liu', 'Jiachi Chen', 'Haoyu Song', 'Duyu Tang', 'Hongyu Zhang', 'Zibin Zheng'], 'affiliations': ['Chongqing University', 'Hangzhou Normal University', 'Huawei Technologies Co, Ltd', 'Independent Researcher', 'Sun Yat-sen University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.11655.jpg', 'data': {'categories': ['#rl', '#benchmark', '#dataset', '#agents', '#data', '#training', '#open_source', '#plp', '#survey'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ñ‹ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ±Ğ¾Ñ€ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ñ‚ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº ĞºĞ¾Ğ´Ğ°.'}, 'en': {'title': 'Empowering Autonomous Coding Agents for Software Issue Resolution', 'desc': 'This paper explores the challenges that large language models face in resolving software issues, which is a crucial task in software engineering. It highlights the development of autonomous coding agents through both training-free and training-based methods. The authors provide a systematic survey of data construction techniques and analyze various methodologies, including supervised fine-tuning and reinforcement learning. Additionally, the paper discusses data quality, agent behavior, and future research directions, while maintaining an open-source repository for ongoing contributions in this area.'}, 'zh': {'title': 'è‡ªä¸»ç¼–ç ä»£ç†ï¼šè§£å†³è½¯ä»¶é—®é¢˜çš„æ–°æ–¹å‘', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è½¯ä»¶é—®é¢˜è§£å†³æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå› æ­¤å¼€å‘äº†è‡ªä¸»ç¼–ç ä»£ç†ï¼Œé‡‡ç”¨äº†å¤šç§æ— è®­ç»ƒå’ŒåŸºäºè®­ç»ƒçš„æ–¹æ³•ã€‚è½¯ä»¶é—®é¢˜è§£å†³æ˜¯ä¸€ä¸ªå¤æ‚çš„è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼Œå¯¹äººå·¥æ™ºèƒ½æ¥è¯´æ˜¯ä¸€ä¸ªé‡è¦çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡ç³»ç»Ÿæ€§åœ°è°ƒæŸ¥äº†è¿™ä¸€æ–°å…´é¢†åŸŸï¼Œåˆ†æäº†æ•°æ®æ„å»ºæµç¨‹å’Œå„ç§æ–¹æ³•è®ºï¼ŒåŒ…æ‹¬æ— è®­ç»ƒæ¡†æ¶å’ŒåŸºäºè®­ç»ƒçš„æŠ€æœ¯ã€‚æœ€åï¼Œè®¨è®ºäº†æ•°æ®è´¨é‡ã€ä»£ç†è¡Œä¸ºçš„å…³é”®åˆ†æä»¥åŠå®é™…åº”ç”¨ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶çš„ä¸»è¦æŒ‘æˆ˜å’Œæ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.14192', 'title': 'Toward Efficient Agents: Memory, Tool learning, and Planning', 'url': 'https://huggingface.co/papers/2601.14192', 'abstract': 'Efficiency in agentic systems is examined across memory, tool learning, and planning components, analyzing trade-offs between effectiveness and computational costs through various optimization strategies and benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.', 'score': 28, 'issue_id': 685, 'pub_date': '2026-01-20', 'pub_date_card': {'ru': '20 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 20', 'zh': '1æœˆ20æ—¥'}, 'hash': 'a98c299a62724db7', 'authors': ['Xiaofang Yang', 'Lijun Li', 'Heng Zhou', 'Tong Zhu', 'Xiaoye Qu', 'Yuchen Fan', 'Qianshan Wei', 'Rui Ye', 'Li Kang', 'Yiran Qin', 'Zhiqiang Kou', 'Daizong Liu', 'Qi Li', 'Ning Ding', 'Siheng Chen', 'Jing Shao'], 'affiliations': ['Fudan University', 'Hong Kong Polytechnic University', 'Institute of Automation, Chinese Academy of Sciences', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiaotong University', 'The Chinese University of Hong Kong (Shenzhen)', 'Tsinghua University', 'University of Science and Technology of China', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2601.14192.jpg', 'data': {'categories': ['#rl', '#optimization', '#long_context', '#agents', '#benchmark'], 'emoji': 'âš¡', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†ĞµĞ¹ ĞŸĞ°Ñ€ĞµÑ‚Ğ¾. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Optimizing Efficiency in Agentic Systems', 'desc': 'This paper explores the efficiency of agentic systems, focusing on three main components: memory, tool learning, and planning. It highlights the trade-offs between effectiveness and computational costs, emphasizing the importance of efficiency for real-world applications. The authors review various optimization strategies and benchmarks, discussing methods like context compression and reinforcement learning rewards to improve performance. Additionally, they propose a framework for evaluating efficiency through a Pareto frontier analysis, identifying challenges and future research directions in the field.'}, 'zh': {'title': 'æå‡æ™ºèƒ½ä½“ç³»ç»Ÿæ•ˆç‡çš„å…³é”®ç ”ç©¶', 'desc': 'æœ¬æ–‡æ¢è®¨äº†æ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„æ•ˆç‡ï¼Œé‡ç‚¹åˆ†æäº†è®°å¿†ã€å·¥å…·å­¦ä¹ å’Œè§„åˆ’ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ã€‚ç ”ç©¶äº†åœ¨æœ‰æ•ˆæ€§å’Œè®¡ç®—æˆæœ¬ä¹‹é—´çš„æƒè¡¡ï¼Œé‡‡ç”¨äº†å¤šç§ä¼˜åŒ–ç­–ç•¥å’ŒåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬å›é¡¾äº†ä¸åŒå®ç°æ–¹å¼çš„æœ€æ–°æ–¹æ³•ï¼Œå¼ºè°ƒäº†å‹ç¼©å’Œç®¡ç†ä¸Šä¸‹æ–‡ã€è®¾è®¡å¼ºåŒ–å­¦ä¹ å¥–åŠ±ä»¥å‡å°‘å·¥å…·è°ƒç”¨ç­‰é«˜å±‚åŸåˆ™ã€‚æœ€åï¼Œæœ¬æ–‡æ€»ç»“äº†æ•ˆç‡è¯„ä¼°çš„æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ï¼Œæ—¨åœ¨ä¸ºæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ•ˆç‡ç ”ç©¶æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.13836', 'title': 'FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs', 'url': 'https://huggingface.co/papers/2601.13836', 'abstract': "FutureOmni presents the first benchmark for evaluating multimodal models' ability to forecast future events from audio-visual data, revealing current limitations and proposing an improved training strategy for better performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Although Multimodal Large Language Models (MLLMs) demonstrate strong omni-modal perception, their ability to forecast future events from audio-visual cues remains largely unexplored, as existing benchmarks focus mainly on retrospective understanding. To bridge this gap, we introduce FutureOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio-visual environments. The evaluated models are required to perform cross-modal causal and temporal reasoning, as well as effectively leverage internal knowledge to predict future events. FutureOmni is constructed via a scalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice QA pairs across 8 primary domains. Evaluations on 13 omni-modal and 7 video-only models show that current systems struggle with audio-visual future prediction, particularly in speech-heavy scenarios, with the best accuracy of 64.8% achieved by Gemini 3 Flash. To mitigate this limitation, we curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy. Evaluations on FutureOmni and popular audio-visual and video-only benchmarks demonstrate that OFF enhances future forecasting and generalization. We publicly release all code (https://github.com/OpenMOSS/FutureOmni) and datasets (https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni).", 'score': 27, 'issue_id': 687, 'pub_date': '2026-01-20', 'pub_date_card': {'ru': '20 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 20', 'zh': '1æœˆ20æ—¥'}, 'hash': 'ec773d7256f89f4c', 'authors': ['Qian Chen', 'Jinlan Fu', 'Changsong Li', 'See-Kiong Ng', 'Xipeng Qiu'], 'affiliations': ['Fudan University', 'National University of Singapore', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2601.13836.jpg', 'data': {'categories': ['#audio', '#video', '#benchmark', '#dataset', '#training', '#multimodal'], 'emoji': 'ğŸ”®', 'ru': {'title': 'ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾: ÑƒÑ‡Ğ¸Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ğ´ĞµÑ‚ÑŒ Ğ·Ğ°Ğ²Ñ‚Ñ€Ğ°', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ FutureOmni â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ ÑĞ»Ğ°Ğ±Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ĞºĞ¾Ğ³Ğ´Ğ° Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑ‡Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 64.8%. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Omni-Modal Future Forecasting (OFF) Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸Ğ· 7000 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ Ğ¸Ñ… Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ.'}, 'en': {'title': 'FutureOmni: Advancing Multimodal Event Forecasting', 'desc': "FutureOmni introduces a new benchmark to assess how well multimodal models can predict future events using audio and visual data. It highlights the shortcomings of current models, especially in scenarios with a lot of speech, where they struggle to make accurate predictions. The paper proposes a new training strategy called Omni-Modal Future Forecasting (OFF) to improve these models' performance. By providing a large dataset and evaluation framework, FutureOmni aims to enhance the ability of models to reason across different modalities and improve their forecasting capabilities."}, 'zh': {'title': 'æœªæ¥é¢„æµ‹çš„æ–°åŸºå‡†ï¼šFutureOmni', 'desc': 'FutureOmniæ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹ä»éŸ³è§†é¢‘æ•°æ®é¢„æµ‹æœªæ¥äº‹ä»¶èƒ½åŠ›çš„åŸºå‡†ï¼Œæ­ç¤ºäº†å½“å‰çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ”¹è¿›çš„è®­ç»ƒç­–ç•¥ä»¥æé«˜æ€§èƒ½ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šç§æ„ŸçŸ¥ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨éŸ³è§†é¢‘çº¿ç´¢ä¸­é¢„æµ‹æœªæ¥äº‹ä»¶çš„èƒ½åŠ›ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚FutureOmniè¦æ±‚è¯„ä¼°çš„æ¨¡å‹è¿›è¡Œè·¨æ¨¡æ€å› æœå’Œæ—¶é—´æ¨ç†ï¼Œå¹¶æœ‰æ•ˆåˆ©ç”¨å†…éƒ¨çŸ¥è¯†æ¥é¢„æµ‹æœªæ¥äº‹ä»¶ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«919ä¸ªè§†é¢‘å’Œ1034ä¸ªå¤šé¡¹é€‰æ‹©é—®ç­”å¯¹çš„åŸºå‡†ï¼ŒFutureOmniä¸ºæœªæ¥é¢„æµ‹æä¾›äº†æ–°çš„è¯„ä¼°æ ‡å‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.14250', 'title': 'OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer', 'url': 'https://huggingface.co/papers/2601.14250', 'abstract': 'OmniTransfer presents a unified framework for spatio-temporal video transfer that enhances appearance consistency and temporal control through multi-view information and multimodal semantic guidance.  \t\t\t\t\tAI-generated summary \t\t\t\t Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.', 'score': 25, 'issue_id': 685, 'pub_date': '2026-01-20', 'pub_date_card': {'ru': '20 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 20', 'zh': '1æœˆ20æ—¥'}, 'hash': 'ea0f06a5786530a6', 'authors': ['Pengze Zhang', 'Yanze Wu', 'Mengtian Li', 'Xu Bai', 'Songtao Zhao', 'Fulong Ye', 'Chong Mou', 'Xinghui Li', 'Zhuowei Chen', 'Qian He', 'Mingyuan Gao'], 'affiliations': ['Intelligent Creation Lab, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2601.14250.jpg', 'data': {'categories': ['#video', '#transfer_learning', '#architecture', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ', 'desc': 'OmniTransfer Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ²ÑŒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Task-aware Positional Bias, Reference-decoupled Causal Learning Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ° Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¸ Task-adaptive Multimodal Alignment Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğµ ÑÑ‚Ğ¸Ğ»Ñ, Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'OmniTransfer: Revolutionizing Video Transfer with Spatio-Temporal Insights', 'desc': 'OmniTransfer is a new framework designed for improving video transfer by utilizing both spatial and temporal information. It enhances the consistency of video appearance and allows for better control over timing by using data from multiple views and different types of information. The framework introduces three innovative components: a positional bias that adapts to improve alignment, a causal learning approach that separates reference and target data for efficiency, and a multimodal alignment system that adjusts to various tasks. Overall, OmniTransfer shows significant improvements in video generation quality compared to existing methods, making it a versatile tool for high-fidelity video creation.'}, 'zh': {'title': 'OmniTransferï¼šæ—¶ç©ºè§†é¢‘ä¼ è¾“çš„æ–°èŒƒå¼', 'desc': 'OmniTransferæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ—¶ç©ºè§†é¢‘ä¼ è¾“æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¤šè§†è§’ä¿¡æ¯å’Œå¤šæ¨¡æ€è¯­ä¹‰æŒ‡å¯¼æ¥å¢å¼ºå¤–è§‚ä¸€è‡´æ€§å’Œæ—¶é—´æ§åˆ¶ã€‚è¯¥æ–¹æ³•å…‹æœäº†ç°æœ‰è§†é¢‘å®šåˆ¶æ–¹æ³•çš„å±€é™æ€§ï¼Œå……åˆ†åˆ©ç”¨è§†é¢‘ä¸­ä¸°å¯Œçš„æ—¶ç©ºä¿¡æ¯ã€‚OmniTransferç»“åˆäº†ä»»åŠ¡æ„ŸçŸ¥çš„ä½ç½®ä¿¡æ¯åå·®ã€å‚è€ƒè§£è€¦å› æœå­¦ä¹ å’Œä»»åŠ¡è‡ªé€‚åº”å¤šæ¨¡æ€å¯¹é½ç­‰è®¾è®¡ï¼Œæå‡äº†è§†é¢‘ç”Ÿæˆçš„çµæ´»æ€§å’Œé«˜ä¿çœŸåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniTransferåœ¨å¤–è§‚å’Œæ—¶é—´ä¼ è¾“æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¼€åˆ›äº†çµæ´»é«˜ä¿çœŸè§†é¢‘ç”Ÿæˆçš„æ–°èŒƒå¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.11969', 'title': 'MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models', 'url': 'https://huggingface.co/papers/2601.11969', 'abstract': "A benchmark called MemoryRewardBench is introduced to systematically evaluate reward models' ability to assess long-term memory management in large language models across various context lengths and memory patterns.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing works increasingly adopt memory-centric mechanisms to process long contexts in a segment manner, and effective memory management is one of the key capabilities that enables large language models to effectively propagate information across the entire sequence. Therefore, leveraging reward models (RMs) to automatically and reliably evaluate memory quality is critical. In this work, we introduce MemoryRewardBench, the first benchmark to systematically study the ability of RMs to evaluate long-term memory management processes. MemoryRewardBench covers both long-context comprehension and long-form generation tasks, featuring 10 distinct settings with different memory management patterns, with context length ranging from 8K to 128K tokens. Evaluations on 13 cutting-edge RMs indicate a diminishing performance gap between open-source and proprietary models, with newer-generation models consistently outperforming their predecessors regardless of parameter count. We further expose the capabilities and fundamental limitations of current RMs in evaluating LLM memory management across diverse settings.", 'score': 23, 'issue_id': 683, 'pub_date': '2026-01-17', 'pub_date_card': {'ru': '17 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 17', 'zh': '1æœˆ17æ—¥'}, 'hash': '3d51cd5cc576b6cc', 'authors': ['Zecheng Tang', 'Baibei Ji', 'Ruoxi Sun', 'Haitian Wang', 'WangJie You', 'Zhang Yijun', 'Wenpeng Zhu', 'Ji Qi', 'Juntao Li', 'Min Zhang'], 'affiliations': ['China Mobile (Suzhou)', 'LCM Laboratory', 'Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2601.11969.jpg', 'data': {'categories': ['#long_context', '#survey', '#benchmark', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸: Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ MemoryRewardBench â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ°Ğº Ğ½Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ 10 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¾Ñ‚ 8K Ğ´Ğ¾ 128K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ÑÑ‚ÑÑ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ĞºĞ°Ğº Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'Evaluating Memory Management in Language Models with MemoryRewardBench', 'desc': 'The paper introduces MemoryRewardBench, a benchmark designed to evaluate how well reward models (RMs) assess long-term memory management in large language models (LLMs). It focuses on the ability of these models to handle long contexts and manage memory effectively across various tasks. The benchmark includes 10 different settings with context lengths ranging from 8K to 128K tokens, allowing for a comprehensive analysis of memory management patterns. Results show that newer RMs outperform older ones, highlighting both the strengths and limitations of current models in evaluating memory quality.'}, 'zh': {'title': 'è¯„ä¼°é•¿æœŸè®°å¿†ç®¡ç†çš„åˆ›æ–°åŸºå‡†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºMemoryRewardBenchçš„åŸºå‡†ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°å¥–åŠ±æ¨¡å‹åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ç®¡ç†é•¿æœŸè®°å¿†çš„èƒ½åŠ›ã€‚éšç€å¯¹è®°å¿†ä¸­å¿ƒæœºåˆ¶çš„é‡‡ç”¨ï¼Œå¦‚ä½•æœ‰æ•ˆç®¡ç†è®°å¿†æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ä¼ æ’­ä¿¡æ¯çš„å…³é”®èƒ½åŠ›ã€‚MemoryRewardBenchæ˜¯ç¬¬ä¸€ä¸ªç³»ç»Ÿç ”ç©¶å¥–åŠ±æ¨¡å‹è¯„ä¼°é•¿æœŸè®°å¿†ç®¡ç†è¿‡ç¨‹çš„åŸºå‡†ï¼Œæ¶µç›–äº†é•¿ä¸Šä¸‹æ–‡ç†è§£å’Œé•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚é€šè¿‡å¯¹13ä¸ªå‰æ²¿å¥–åŠ±æ¨¡å‹çš„è¯„ä¼°ï¼Œå‘ç°å¼€æºæ¨¡å‹ä¸ä¸“æœ‰æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·é€æ¸ç¼©å°ï¼Œæ–°ä¸€ä»£æ¨¡å‹åœ¨å„ä¸ªå‚æ•°æ•°é‡ä¸‹å‡ä¼˜äºå‰ä»£æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.13029', 'title': 'Think3D: Thinking with Space for Spatial Reasoning', 'url': 'https://huggingface.co/papers/2601.13029', 'abstract': "Think3D enhances vision-language models' 3D reasoning capabilities by enabling interactive spatial exploration through 3D reconstruction and camera-based operations, improving performance without additional training.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.", 'score': 17, 'issue_id': 689, 'pub_date': '2026-01-19', 'pub_date_card': {'ru': '19 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 19', 'zh': '1æœˆ19æ—¥'}, 'hash': '2b5276573acfdd07', 'authors': ['Zaibin Zhang', 'Yuhan Wu', 'Lianjie Jia', 'Yifan Wang', 'Zhongbo Zhang', 'Yijiang Li', 'Binghao Ran', 'Fuxi Zhang', 'Zhuohan Sun', 'Zhenfei Yin', 'Lijun Wang', 'Huchuan Lu'], 'affiliations': ['Dalian University of Technology', 'University of California San Diego', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2601.13029.jpg', 'data': {'categories': ['#open_source', '#reasoning'], 'emoji': 'ğŸ§Š', 'ru': {'title': 'Ğ¢Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Think3D â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ñ‡ĞµÑ€ĞµĞ· 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ 3D-ÑÑ†ĞµĞ½Ğ¾Ğ¹, Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ÑÑÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ³Ğ¾-Ğ²Ğ¸Ğ´Ğ¾Ğ¼ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ´Ğ¾Ğ¼ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº GPT-4 Ğ¸ Gemini 2.5 Pro, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: +7.8% Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ²ÑŒÑ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ +4.7% Ğ½Ğ° VSI-Bench. Ğ”Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ +0.7% Ğ´Ğ¾ +6.8%.'}, 'en': {'title': 'Empowering Vision-Language Models with 3D Spatial Intelligence', 'desc': 'Think3D is a framework designed to enhance vision-language models (VLMs) by enabling them to perform 3D reasoning through interactive spatial exploration. It utilizes 3D reconstruction techniques to create point clouds and camera poses from images, allowing models to manipulate and understand spatial relationships in a 3D context. This approach significantly improves the spatial reasoning capabilities of advanced models like GPT-4.1 and Gemini 2.5 Pro without requiring additional training, achieving notable performance gains on various benchmarks. Additionally, smaller models benefit from a reinforcement learning policy that helps them choose effective viewpoints and operations, further enhancing their 3D reasoning abilities.'}, 'zh': {'title': 'Think3Dï¼šå¼€å¯äº’åŠ¨å¼3Dæ¨ç†çš„æ–°ç»´åº¦', 'desc': 'Think3Dæ˜¯ä¸€ä¸ªå¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹3Dæ¨ç†èƒ½åŠ›çš„æ¡†æ¶ã€‚å®ƒé€šè¿‡3Dé‡å»ºå’ŒåŸºäºç›¸æœºçš„æ“ä½œï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œäº’åŠ¨å¼ç©ºé—´æ¢ç´¢ã€‚è¯¥æ¡†æ¶æ— éœ€é¢å¤–è®­ç»ƒï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ç©ºé—´æ¨ç†æ–¹é¢çš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„å°å‹æ¨¡å‹åœ¨é€‰æ‹©ä¿¡æ¯ä¸°å¯Œçš„è§†è§’å’Œæ“ä½œæ—¶ï¼Œèƒ½å¤Ÿè·å¾—æ›´å¤§çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.11522', 'title': 'UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation', 'url': 'https://huggingface.co/papers/2601.11522', 'abstract': 'UniX presents a unified medical foundation model that decouples visual understanding and generation tasks using distinct autoregressive and diffusion branches with cross-modal attention for enhanced performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent progress, medical foundation models still struggle to unify visual understanding and generation, as these tasks have inherently conflicting goals: semantic abstraction versus pixel-level reconstruction. Existing approaches, typically based on parameter-shared autoregressive architectures, frequently lead to compromised performance in one or both tasks. To address this, we present UniX, a next-generation unified medical foundation model for chest X-ray understanding and generation. UniX decouples the two tasks into an autoregressive branch for understanding and a diffusion branch for high-fidelity generation. Crucially, a cross-modal self-attention mechanism is introduced to dynamically guide the generation process with understanding features. Coupled with a rigorous data cleaning pipeline and a multi-stage training strategy, this architecture enables synergistic collaboration between tasks while leveraging the strengths of diffusion models for superior generation. On two representative benchmarks, UniX achieves a 46.1% improvement in understanding performance (Micro-F1) and a 24.2% gain in generation quality (FD-RadDino), using only a quarter of the parameters of LLM-CXR. By achieving performance on par with task-specific models, our work establishes a scalable paradigm for synergistic medical image understanding and generation. Codes and models are available at https://github.com/ZrH42/UniX.', 'score': 15, 'issue_id': 683, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 16', 'zh': '1æœˆ16æ—¥'}, 'hash': '2e8309152ea38796', 'authors': ['Ruiheng Zhang', 'Jingfeng Yao', 'Huangxuan Zhao', 'Hao Yan', 'Xiao He', 'Lei Chen', 'Zhou Wei', 'Yong Luo', 'Zengmao Wang', 'Lefei Zhang', 'Dacheng Tao', 'Bo Du'], 'affiliations': ['Huazhong University of Science and Technology', 'Nanyang Technological University', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2601.11522.jpg', 'data': {'categories': ['#diffusion', '#training', '#data', '#architecture', '#open_source', '#multimodal', '#healthcare'], 'emoji': 'ğŸ«€', 'ru': {'title': 'Ğ¡Ğ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ: Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ²ĞµÑ‚Ğ²Ğ¸ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°', 'desc': 'UniX â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ´Ğ²Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ñ: Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸, Ğ¸Ğ·Ğ²Ğ»Ñ‘Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ğ¹ pipeline Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ²ÑƒĞ¼Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ² ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸ĞµĞ¹ Ğ¸ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ĞµĞ¹. UniX Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 46,1% Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ½Ğ° 24,2% Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ² Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ñ€Ğ°Ğ·Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ñ‡ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸.'}, 'en': {'title': 'UniX: Bridging Understanding and Generation in Medical Imaging', 'desc': 'UniX is a new medical foundation model designed to improve how machines understand and generate medical images, specifically chest X-rays. It separates the tasks of visual understanding and image generation into two different branches: one that uses autoregressive methods for understanding and another that employs diffusion techniques for generating high-quality images. This model introduces a cross-modal self-attention mechanism that helps the generation process by incorporating features from the understanding branch, enhancing overall performance. As a result, UniX shows significant improvements in both understanding and generation tasks while using fewer parameters than previous models, making it a scalable solution for medical image analysis.'}, 'zh': {'title': 'UniXï¼šåŒ»å­¦å›¾åƒç†è§£ä¸ç”Ÿæˆçš„ååŒæ–°èŒƒå¼', 'desc': 'UniXæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åŒ»å­¦åŸºç¡€æ¨¡å‹ï¼Œå®ƒå°†è§†è§‰ç†è§£å’Œç”Ÿæˆä»»åŠ¡åˆ†å¼€å¤„ç†ï¼Œä½¿ç”¨ä¸åŒçš„è‡ªå›å½’å’Œæ‰©æ•£åˆ†æ”¯ï¼Œå¹¶ç»“åˆè·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æé«˜æ€§èƒ½ã€‚ä¼ ç»Ÿçš„åŒ»å­¦æ¨¡å‹åœ¨è¿™ä¸¤é¡¹ä»»åŠ¡ä¸Šå¾€å¾€è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºå®ƒä»¬çš„ç›®æ ‡ç›¸äº’çŸ›ç›¾ï¼šè¯­ä¹‰æŠ½è±¡ä¸åƒç´ çº§é‡å»ºã€‚UniXé€šè¿‡è‡ªå›å½’åˆ†æ”¯è¿›è¡Œç†è§£ï¼Œé€šè¿‡æ‰©æ•£åˆ†æ”¯è¿›è¡Œé«˜ä¿çœŸç”Ÿæˆï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚ç»è¿‡ä¸¥æ ¼çš„æ•°æ®æ¸…ç†å’Œå¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ŒUniXåœ¨ç†è§£æ€§èƒ½å’Œç”Ÿæˆè´¨é‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†åŒ»å­¦å›¾åƒç†è§£ä¸ç”Ÿæˆçš„ååŒå·¥ä½œæ–°èŒƒå¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.12294', 'title': 'ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents', 'url': 'https://huggingface.co/papers/2601.12294', 'abstract': 'ToolPRMBench is introduced as a large-scale benchmark for evaluating process reward models in tool-using agents, featuring step-level test cases and multi-LLM verification to ensure data quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.', 'score': 13, 'issue_id': 684, 'pub_date': '2026-01-18', 'pub_date_card': {'ru': '18 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 18', 'zh': '1æœˆ18æ—¥'}, 'hash': '4ace6c0ff6a7867b', 'authors': ['Dawei Li', 'Yuguang Yao', 'Zhen Tan', 'Huan Liu', 'Ruocheng Guo'], 'affiliations': ['Arizona State University', 'Intuit AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2601.12294.jpg', 'data': {'categories': ['#agents', '#benchmark', '#dataset', '#rl'], 'emoji': 'ğŸ”§', 'ru': {'title': 'Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ¾Ğ¼ Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ToolPRMBench â€” Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ Ğ±Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑˆĞ°Ğ³Ğ¾Ğ² (process reward models) Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. Ğ”Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸Ğ· Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'ToolPRMBench: Elevating Evaluation of Process Reward Models for Tool-Using Agents', 'desc': 'This paper presents ToolPRMBench, a comprehensive benchmark designed to evaluate process reward models (PRMs) specifically for tool-using agents. It addresses the need for systematic evaluation by converting agent trajectories into detailed step-level test cases, which include correct and plausible incorrect actions. The benchmark employs both offline and online sampling methods to identify single-step and multi-step errors, ensuring a thorough assessment of PRM performance. The findings demonstrate significant variations in the effectiveness of different PRMs, emphasizing the advantages of specialized models for enhancing tool-using capabilities.'}, 'zh': {'title': 'ToolPRMBenchï¼šè¯„ä¼°å·¥å…·ä½¿ç”¨ä»£ç†çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹æ–°åŸºå‡†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ToolPRMBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å·¥å…·ä½¿ç”¨ä»£ç†çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰çš„è§„æ¨¡åŒ–åŸºå‡†ã€‚è¯¥åŸºå‡†é€šè¿‡å°†ä»£ç†è½¨è¿¹è½¬æ¢ä¸ºé€æ­¥æµ‹è¯•æ¡ˆä¾‹ï¼Œæä¾›äº†æ›´ç»†è‡´çš„ç›‘æ§å’Œè¯„ä¼°ã€‚æˆ‘ä»¬é‡‡ç”¨ç¦»çº¿é‡‡æ ·å’Œåœ¨çº¿é‡‡æ ·çš„æ–¹æ³•ï¼Œåˆ†åˆ«æ•æ‰å±€éƒ¨å•æ­¥é”™è¯¯å’ŒçœŸå®çš„å¤šæ­¥å¤±è´¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸“é—¨åŒ–çš„PRMåœ¨å·¥å…·ä½¿ç”¨ä¸­çš„æœ‰æ•ˆæ€§æ˜æ˜¾ä¼˜äºé€šç”¨PRMï¼Œå±•ç¤ºäº†å…¶æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.13247', 'title': 'Aligning Agentic World Models via Knowledgeable Experience Learning', 'url': 'https://huggingface.co/papers/2601.13247', 'abstract': 'WorldMind addresses the modal disconnect in LLMs by autonomously building a symbolic world knowledge repository that enhances physical feasibility and task optimality through experience-based learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Current Large Language Models (LLMs) exhibit a critical modal disconnect: they possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Consequently, while these agents implicitly function as world models, their simulations often suffer from physical hallucinations-generating plans that are logically sound but physically unexecutable. Existing alignment strategies predominantly rely on resource-intensive training or fine-tuning, which attempt to compress dynamic environmental rules into static model parameters. However, such parametric encapsulation is inherently rigid, struggling to adapt to the open-ended variability of physical dynamics without continuous, costly retraining. To bridge this gap, we introduce WorldMind, a framework that autonomously constructs a symbolic World Knowledge Repository by synthesizing environmental feedback. Specifically, it unifies Process Experience to enforce physical feasibility via prediction errors and Goal Experience to guide task optimality through successful trajectories. Experiments on EB-ALFRED and EB-Habitat demonstrate that WorldMind achieves superior performance compared to baselines with remarkable cross-model and cross-environment transferability.', 'score': 12, 'issue_id': 683, 'pub_date': '2026-01-19', 'pub_date_card': {'ru': '19 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 19', 'zh': '1æœˆ19æ—¥'}, 'hash': 'd94ae6ed0aceb888', 'authors': ['Baochang Ren', 'Yunzhi Yao', 'Rui Sun', 'Shuofei Qiao', 'Ningyu Zhang', 'Huajun Chen'], 'affiliations': ['University of California, Los Angeles', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.13247.jpg', 'data': {'categories': ['#transfer_learning', '#alignment', '#hallucinations', '#reasoning'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ“Ñ€ounded Ğ¼Ğ¸Ñ€ Ğ´Ğ»Ñ LLM: Ğ¾Ñ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğº Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¸Ğ¼Ñ‹Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ°Ğ¼', 'desc': 'WorldMind Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°ÑÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ğ¼Ğ¸Ñ€Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ²Ğ° Ñ‚Ğ¸Ğ¿Ğ° Ğ¾Ğ¿Ñ‹Ñ‚Ğ°: Ğ¾Ğ¿Ñ‹Ñ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‹Ñ‚ Ñ†ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº Ğ·Ğ°ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞµ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, WorldMind Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… EB-ALFRED Ğ¸ EB-Habitat Ñ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Bridging the Gap: Enhancing LLMs with World Knowledge', 'desc': "WorldMind is a framework designed to improve Large Language Models (LLMs) by creating a symbolic repository of world knowledge. This repository helps LLMs understand and respect the physical laws of the world, reducing the occurrence of unrealistic plans or 'physical hallucinations'. Instead of relying on static model parameters that require expensive retraining, WorldMind uses experience-based learning to adaptively synthesize feedback from the environment. Experiments show that WorldMind significantly outperforms existing models in various tasks, demonstrating its ability to transfer knowledge across different models and environments."}, 'zh': {'title': 'WorldMindï¼šæ‰“ç ´æ¨¡æ€è„±èŠ‚çš„æ™ºèƒ½æ¡†æ¶', 'desc': 'WorldMind è§£å†³äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„æ¨¡æ€è„±èŠ‚é—®é¢˜ï¼Œé€šè¿‡è‡ªä¸»æ„å»ºç¬¦å·ä¸–ç•ŒçŸ¥è¯†åº“ï¼Œå¢å¼ºäº†ç‰©ç†å¯è¡Œæ€§å’Œä»»åŠ¡æœ€ä¼˜æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆæˆç¯å¢ƒåé¦ˆï¼Œç»Ÿä¸€äº†è¿‡ç¨‹ç»éªŒå’Œç›®æ ‡ç»éªŒï¼Œä»¥ç¡®ä¿ç‰©ç†å¯è¡Œæ€§å’Œä»»åŠ¡å¯¼å‘ã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–èµ„æºå¯†é›†å‹è®­ç»ƒçš„å¯¹é½ç­–ç•¥ç›¸æ¯”ï¼ŒWorldMind èƒ½å¤Ÿæ›´çµæ´»åœ°é€‚åº”ç‰©ç†åŠ¨æ€çš„å˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWorldMind åœ¨ EB-ALFRED å’Œ EB-Habitat ä¸Šçš„è¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå…·æœ‰æ˜¾è‘—çš„è·¨æ¨¡å‹å’Œè·¨ç¯å¢ƒçš„è¿ç§»èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.11888', 'title': 'Agentic-R: Learning to Retrieve for Agentic Search', 'url': 'https://huggingface.co/papers/2601.11888', 'abstract': 'A novel retriever training framework for agentic search that uses both local relevance and global answer correctness metrics with iterative optimization between the search agent and retriever.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic search has recently emerged as a powerful paradigm, where an agent interleaves multi-step reasoning with on-demand retrieval to solve complex questions. Despite its success, how to design a retriever for agentic search remains largely underexplored. Existing search agents typically rely on similarity-based retrievers, while similar passages are not always useful for final answer generation. In this paper, we propose a novel retriever training framework tailored for agentic search. Unlike retrievers designed for single-turn retrieval-augmented generation (RAG) that only rely on local passage utility, we propose to use both local query-passage relevance and global answer correctness to measure passage utility in a multi-turn agentic search. We further introduce an iterative training strategy, where the search agent and the retriever are optimized bidirectionally and iteratively. Different from RAG retrievers that are only trained once with fixed questions, our retriever is continuously improved using evolving and higher-quality queries from the agent. Extensive experiments on seven single-hop and multi-hop QA benchmarks demonstrate that our retriever, termed , consistently outperforms strong baselines across different search agents. Our codes are available at: https://github.com/8421BCD/Agentic-R.', 'score': 8, 'issue_id': 687, 'pub_date': '2026-01-17', 'pub_date_card': {'ru': '17 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 17', 'zh': '1æœˆ17æ—¥'}, 'hash': '08c6136631255bb6', 'authors': ['Wenhan Liu', 'Xinyu Ma', 'Yutao Zhu', 'Yuchen Li', 'Daiting Shi', 'Dawei Yin', 'Zhicheng Dou'], 'affiliations': ['Baidu Inc., Beijing, China', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2601.11888.jpg', 'data': {'categories': ['#agents', '#benchmark', '#training', '#rag'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ”Ğ²ÑƒÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ÑÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²', 'desc': "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñæ¡†æ¶ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ retriever'Ğ°, Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ¼ Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ĞºĞ°Ğº Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ (Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ), Ñ‚Ğ°Ğº Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ (ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°ÑÑĞ°Ğ¶Ğ°. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ³Ğ´Ğµ search agent Ğ¸ retriever Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ retriever'Ñƒ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¾Ñ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞµĞ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."}, 'en': {'title': 'Enhancing Agentic Search with Iterative Retriever Training', 'desc': 'This paper introduces a new training framework for retrievers used in agentic search, which combines local relevance and global answer correctness metrics. The framework allows for iterative optimization between the search agent and the retriever, enhancing their performance in multi-step reasoning tasks. Unlike traditional retrievers that focus solely on similarity, this approach continuously improves the retriever using evolving queries from the agent. The results show that this novel retriever outperforms existing methods across various question-answering benchmarks.'}, 'zh': {'title': 'åˆ›æ–°çš„ä»£ç†æœç´¢æ£€ç´¢å™¨è®­ç»ƒæ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ£€ç´¢å™¨è®­ç»ƒæ¡†æ¶ï¼Œä¸“ä¸ºä»£ç†æœç´¢è®¾è®¡ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å±€éƒ¨ç›¸å…³æ€§å’Œå…¨å±€ç­”æ¡ˆæ­£ç¡®æ€§æŒ‡æ ‡ï¼Œé€šè¿‡æœç´¢ä»£ç†å’Œæ£€ç´¢å™¨ä¹‹é—´çš„è¿­ä»£ä¼˜åŒ–æ¥æå‡æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºç›¸ä¼¼æ€§çš„æ£€ç´¢å™¨ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šè½®ä»£ç†æœç´¢ä¸­åŒæ—¶è€ƒè™‘æŸ¥è¯¢-æ®µè½çš„ç›¸å…³æ€§å’Œç­”æ¡ˆçš„å…¨å±€æ­£ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ£€ç´¢å™¨åœ¨å¤šä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„å¼ºåŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.14232', 'title': 'KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning', 'url': 'https://huggingface.co/papers/2601.14232', 'abstract': 'KAGE-Env is a JAX-native 2D platformer environment that isolates visual shifts from underlying control problems, enabling systematic analysis of visual generalization in reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322.github.io/KAGEBench/.', 'score': 7, 'issue_id': 691, 'pub_date': '2026-01-20', 'pub_date_card': {'ru': '20 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 20', 'zh': '1æœˆ20æ—¥'}, 'hash': '64c2d01c6a3797c7', 'authors': ['Egor Cherepanov', 'Daniil Zelezetsky', 'Alexey K. Kovalev', 'Aleksandr I. Panov'], 'affiliations': ['Cognitive AI Systems Lab', 'MIRIAI, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2601.14232.jpg', 'data': {'categories': ['#optimization', '#cv', '#games', '#benchmark', '#rl'], 'emoji': 'ğŸ®', 'ru': {'title': 'Ğ§Ğ¸ÑÑ‚Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° KAGE-Env â€” ÑÑ€ĞµĞ´Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° JAX, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº KAGE-Bench Ñ 34 ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾ÑĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¹ (Ñ„Ğ¾Ğ½, Ñ„Ğ¾Ñ‚Ğ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ´Ğ²Ğ¸Ğ³Ğ¸, Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°). Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ PPO-CNN ÑĞ¸Ğ»ÑŒĞ½Ğ¾ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ´Ğ²Ğ¸Ğ³Ğ¾Ğ² (Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ñ„Ğ¾Ğ½Ğ° Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸), Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° JAX ÑÑ€ĞµĞ´Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 33 Ğ¼Ğ»Ğ½ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğµ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹.'}, 'en': {'title': 'Isolating Visual Shifts for Better AI Learning', 'desc': 'KAGE-Env is a new 2D platformer environment designed for reinforcement learning that separates visual changes from control challenges. This allows researchers to study how visual shifts affect the performance of AI agents without interference from other factors. The environment includes a benchmark called KAGE-Bench, which tests agents on different visual conditions while keeping the control tasks constant. Results show that certain visual changes can significantly impact agent performance, highlighting the importance of understanding visual generalization in machine learning.'}, 'zh': {'title': 'KAGE-Envï¼šè§†è§‰æ³›åŒ–çš„ç³»ç»Ÿåˆ†æå¹³å°', 'desc': 'KAGE-Envæ˜¯ä¸€ä¸ªåŸºäºJAXçš„äºŒç»´å¹³å°ç¯å¢ƒï¼Œæ—¨åœ¨å°†è§†è§‰å˜åŒ–ä¸æ§åˆ¶é—®é¢˜åˆ†ç¦»ï¼Œä»è€Œç³»ç»Ÿåœ°åˆ†æå¼ºåŒ–å­¦ä¹ ä¸­çš„è§†è§‰æ³›åŒ–ã€‚ä¼ ç»Ÿçš„åƒç´ çº§å¼ºåŒ–å­¦ä¹ ä»£ç†åœ¨è§†è§‰åˆ†å¸ƒå˜åŒ–æ—¶å¸¸å¸¸å¤±è´¥ï¼Œå³ä½¿æ½œåœ¨åŠ¨æ€å’Œå¥–åŠ±ä¿æŒä¸å˜ã€‚KAGE-Envé€šè¿‡å°†è§‚å¯Ÿè¿‡ç¨‹åˆ†è§£ä¸ºç‹¬ç«‹å¯æ§çš„è§†è§‰è½´ï¼Œæä¾›äº†ä¸€ä¸ªæ¸…æ™°çš„è§†è§‰æ³›åŒ–æŠ½è±¡ã€‚åŸºäºæ­¤ç¯å¢ƒï¼Œæˆ‘ä»¬å®šä¹‰äº†KAGE-Benchï¼Œä¸€ä¸ªåŒ…å«34ä¸ªè®­ç»ƒ-è¯„ä¼°é…ç½®å¯¹çš„åŸºå‡†ï¼Œä¸“æ³¨äºéš”ç¦»å•ä¸€çš„è§†è§‰å˜åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.14251', 'title': 'LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR', 'url': 'https://huggingface.co/papers/2601.14251', 'abstract': 'LightOnOCR-2-1B is a compact 1B-parameter vision-language model that performs end-to-end document image-to-text conversion with improved localization and robustness through specialized training techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t We present LightOnOCR-2-1B, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9times smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and LightOnOCR-bbox-bench evaluation under their respective licenses.', 'score': 5, 'issue_id': 691, 'pub_date': '2026-01-20', 'pub_date_card': {'ru': '20 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 20', 'zh': '1æœˆ20æ—¥'}, 'hash': '6f53103e7c33c0fb', 'authors': ['Said Taghadouini', 'Adrien CavaillÃ¨s', 'Baptiste Aubertin'], 'affiliations': ['LightOn'], 'pdf_title_img': 'assets/pdf/title_img/2601.14251.jpg', 'data': {'categories': ['#training', '#multimodal', '#small_models', '#dataset', '#cv', '#multilingual', '#benchmark'], 'emoji': 'ğŸ“„', 'ru': {'title': 'Ğ›Ñ‘Ğ³ĞºĞ°Ñ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ… Ñ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚ÑŒÑ', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ LightOnOCR-2-1B â€” ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ end-to-end Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ĞµĞ¹ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ OlmOCR-Bench Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğµ Ğ² 9 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ‡ĞµĞ¼ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… bounding box Ğ´Ğ»Ñ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ RLVR Ğ¸ IoU-based Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ ÑƒÑÑ€ĞµĞ´Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ task-arithmetic merging.'}, 'en': {'title': 'Efficient Document Image-to-Text Conversion with LightOnOCR-2-1B', 'desc': 'LightOnOCR-2-1B is a vision-language model designed for converting document images into text efficiently. It utilizes 1 billion parameters and employs advanced training techniques to enhance localization and robustness, making it more reliable than traditional OCR methods. The model is trained on a diverse dataset, achieving state-of-the-art performance while being significantly smaller and faster than previous models. Additionally, it introduces features like predicting bounding boxes for images and employs reinforcement learning for improved accuracy.'}, 'zh': {'title': 'è½»é‡çº§æ–‡æ¡£å›¾åƒè½¬æ–‡æœ¬çš„é©å‘½æ€§æ¨¡å‹', 'desc': 'LightOnOCR-2-1B æ˜¯ä¸€ä¸ªç´§å‡‘çš„1Bå‚æ•°è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå®ç°æ–‡æ¡£å›¾åƒåˆ°æ–‡æœ¬çš„ç«¯åˆ°ç«¯è½¬æ¢ã€‚é€šè¿‡ä¸“é—¨çš„è®­ç»ƒæŠ€æœ¯ï¼Œå®ƒåœ¨å®šä½å’Œé²æ£’æ€§æ–¹é¢å¾—åˆ°äº†æ”¹å–„ã€‚è¯¥æ¨¡å‹åœ¨å¤§è§„æ¨¡é«˜è´¨é‡çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§è¯­è¨€çš„æ–‡æ¡£ï¼Œå¹¶åœ¨OlmOCR-Benchä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†å½’ä¸€åŒ–è¾¹ç•Œæ¡†çš„é¢„æµ‹ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è¾“å‡ºæ ¼å¼å’Œé²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.13288', 'title': 'A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification', 'url': 'https://huggingface.co/papers/2601.13288', 'abstract': 'Lightweight probes trained on hidden states of LLMs enable efficient classification tasks without additional computational overhead, improving safety and sentiment analysis performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Production LLM systems often rely on separate models for safety and other classification-heavy steps, increasing latency, VRAM footprint, and operational complexity. We instead reuse computation already paid for by the serving LLM: we train lightweight probes on its hidden states and predict labels in the same forward pass used for generation. We frame classification as representation selection over the full token-layer hidden-state tensor, rather than committing to a fixed token or fixed layer (e.g., first-token logits or final-layer pooling). To implement this, we introduce a two-stage aggregator that (i) summarizes tokens within each layer and (ii) aggregates across layer summaries to form a single representation for classification. We instantiate this template with direct pooling, a 100K-parameter scoring-attention gate, and a downcast multi-head self-attention (MHA) probe with up to 35M trainable parameters. Across safety and sentiment benchmarks our probes improve over logit-only reuse (e.g., MULI) and are competitive with substantially larger task-specific baselines, while preserving near-serving latency and avoiding the VRAM and latency costs of a separate guard-model pipeline.', 'score': 4, 'issue_id': 683, 'pub_date': '2026-01-19', 'pub_date_card': {'ru': '19 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 19', 'zh': '1æœˆ19æ—¥'}, 'hash': '16b301dd1efbd006', 'authors': ['Gonzalo Ariel Meyoyan', 'Luciano Del Corro'], 'affiliations': ['Departamento de ComputaciÃ³n, FCEyN Universidad de Buenos Aires', 'ELIAS Lab, Departamento de IngenierÃ­a Universidad de San AndrÃ©s'], 'pdf_title_img': 'assets/pdf/title_img/2601.13288.jpg', 'data': {'categories': ['#training', '#inference', '#small_models'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ›Ñ‘Ğ³ĞºĞ¸Ğµ Ğ·Ğ¾Ğ½Ğ´Ñ‹ Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸: Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ LLM Ğ±ĞµĞ· Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ»Ñ‘Ğ³ĞºĞ¸Ñ… Ğ·Ğ¾Ğ½Ğ´Ğ¾Ğ² (probes) Ğ½Ğ° ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑÑ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ LLM, Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ² Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ·Ğ¾Ğ½Ğ´Ñ‹ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ² Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ²ÑĞµĞ³Ğ¾ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ° ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ²ÑĞµÑ… ÑĞ»Ğ¾Ñ‘Ğ² Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ¸Ğ»Ğ¸ ÑĞ»Ğ¾Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ‚Ğ¾Ñ€ Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ·Ğ¾Ğ½Ğ´Ğ¾Ğ², Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ ÑƒÑÑ€ĞµĞ´Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ğ¾Ğ² Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Efficient Classification with Lightweight Probes on LLMs', 'desc': 'This paper presents a method for improving classification tasks using lightweight probes that are trained on the hidden states of large language models (LLMs). By reusing the computation from the LLM during its generation process, the approach reduces latency and memory usage compared to traditional methods that require separate models for classification. The authors propose a two-stage aggregator that summarizes information from multiple layers of the LLM to create a single representation for classification tasks. The results show that these probes enhance performance in safety and sentiment analysis while maintaining efficiency, making them competitive with larger, task-specific models.'}, 'zh': {'title': 'åˆ©ç”¨è½»é‡çº§æ¢é’ˆæå‡åˆ†ç±»æ•ˆç‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§æ¢é’ˆï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„éšè—çŠ¶æ€è¿›è¡Œé«˜æ•ˆåˆ†ç±»ä»»åŠ¡ï¼Œé¿å…äº†é¢å¤–çš„è®¡ç®—å¼€é”€ï¼Œä»è€Œæå‡äº†å®‰å…¨æ€§å’Œæƒ…æ„Ÿåˆ†æçš„æ€§èƒ½ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é‡ç”¨LLMçš„è®¡ç®—ï¼Œè®­ç»ƒè¿™äº›æ¢é’ˆæ¥é¢„æµ‹æ ‡ç­¾ï¼Œè€Œä¸æ˜¯ä¾èµ–äºå•ç‹¬çš„æ¨¡å‹ã€‚åˆ†ç±»è¢«è§†ä¸ºåœ¨å®Œæ•´çš„éšè—çŠ¶æ€å¼ é‡ä¸Šè¿›è¡Œè¡¨ç¤ºé€‰æ‹©ï¼Œè€Œä¸æ˜¯å›ºå®šäºæŸä¸ªç‰¹å®šçš„tokenæˆ–å±‚ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä¸¤é˜¶æ®µèšåˆå™¨ï¼Œé¦–å…ˆåœ¨æ¯ä¸€å±‚å†…æ€»ç»“tokenï¼Œç„¶åè·¨å±‚æ±‡æ€»å½¢æˆå•ä¸€çš„åˆ†ç±»è¡¨ç¤ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.14046', 'title': 'PRiSM: Benchmarking Phone Realization in Speech Models', 'url': 'https://huggingface.co/papers/2601.14046', 'abstract': 'PRiSM benchmark evaluates phonetic perception in speech models through standardized transcription-based metrics and downstream applications across clinical, educational, and multilingual domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Phone recognition (PR) serves as the atomic interface for language-agnostic modeling for cross-lingual speech processing and phonetic analysis. Despite prolonged efforts in developing PR systems, current evaluations only measure surface-level transcription accuracy. We introduce PRiSM, the first open-source benchmark designed to expose blind spots in phonetic perception through intrinsic and extrinsic evaluation of PR systems. PRiSM standardizes transcription-based evaluation and assesses downstream utility in clinical, educational, and multilingual settings with transcription and representation probes. We find that diverse language exposure during training is key to PR performance, encoder-CTC models are the most stable, and specialized PR models still outperform Large Audio Language Models. PRiSM releases code, recipes, and datasets to move the field toward multilingual speech models with robust phonetic ability: https://github.com/changelinglab/prism.', 'score': 3, 'issue_id': 685, 'pub_date': '2026-01-20', 'pub_date_card': {'ru': '20 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 20', 'zh': '1æœˆ20æ—¥'}, 'hash': 'ee74254991809e26', 'authors': ['Shikhar Bharadwaj', 'Chin-Jou Li', 'Yoonjae Kim', 'Kwanghee Choi', 'Eunjung Yeo', 'Ryan Soh-Eun Shim', 'Hanyu Zhou', 'Brendon Boldt', 'Karen Rosero Jacome', 'Kalvin Chang', 'Darsh Agrawal', 'Keer Xu', 'Chao-Han Huck Yang', 'Jian Zhu', 'Shinji Watanabe', 'David R. Mortensen'], 'affiliations': ['CMU', 'Gwangju Institute of Science and Technology', 'LMU Munich', 'NVIDIA', 'UBC', 'UC Berkeley', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2601.14046.jpg', 'data': {'categories': ['#open_source', '#audio', '#dataset', '#low_resource', '#multilingual', '#benchmark'], 'emoji': 'ğŸ”¤', 'ru': {'title': 'Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ„Ğ¾Ğ½ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº PRiSM Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ñ„Ğ¾Ğ½ĞµÑ‚Ğ¸ĞºĞ¸ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¾Ğ½ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğµ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ ÑĞ»ĞµĞ¿Ñ‹Ğµ Ğ¿ÑÑ‚Ğ½Ğ° Ğ² Ñ„Ğ¾Ğ½ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ…, Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğº Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ½ĞºĞ¾Ğ´ĞµÑ€-CTC Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹, Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'PRiSM: Elevating Phonetic Perception in Speech Models', 'desc': 'The PRiSM benchmark is a new tool for evaluating how well speech models understand phonetics across different languages and applications. It goes beyond just checking if the transcription is correct by also looking at how these models perform in real-world scenarios like healthcare and education. The research shows that training with a variety of languages improves the performance of phonetic recognition systems. Additionally, specialized models still perform better than larger, more general audio language models, highlighting the importance of targeted training in phonetic perception.'}, 'zh': {'title': 'PRiSMï¼šæå‡è¯­éŸ³è¯†åˆ«çš„å¤šè¯­è¨€èƒ½åŠ›', 'desc': 'PRiSMåŸºå‡†æµ‹è¯•é€šè¿‡æ ‡å‡†åŒ–çš„è½¬å½•è¯„ä¼°æŒ‡æ ‡å’Œä¸‹æ¸¸åº”ç”¨ï¼Œè¯„ä¼°è¯­éŸ³æ¨¡å‹ä¸­çš„è¯­éŸ³æ„ŸçŸ¥èƒ½åŠ›ï¼Œæ¶µç›–ä¸´åºŠã€æ•™è‚²å’Œå¤šè¯­è¨€é¢†åŸŸã€‚å°½ç®¡åœ¨å¼€å‘è¯­éŸ³è¯†åˆ«ç³»ç»Ÿæ–¹é¢è¿›è¡Œäº†é•¿æœŸåŠªåŠ›ï¼Œä½†ç›®å‰çš„è¯„ä¼°ä»…æµ‹é‡è¡¨é¢è½¬å½•å‡†ç¡®æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†PRiSMï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¼€æºåŸºå‡†ï¼Œæ—¨åœ¨é€šè¿‡å¯¹è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„å†…åœ¨å’Œå¤–åœ¨è¯„ä¼°ï¼Œæ­ç¤ºè¯­éŸ³æ„ŸçŸ¥çš„ç›²ç‚¹ã€‚ç ”ç©¶å‘ç°ï¼Œå¤šæ ·åŒ–çš„è¯­è¨€æš´éœ²å¯¹è¯­éŸ³è¯†åˆ«æ€§èƒ½è‡³å…³é‡è¦ï¼Œç¼–ç å™¨-CTCæ¨¡å‹æœ€ä¸ºç¨³å®šï¼Œè€Œä¸“é—¨çš„è¯­éŸ³è¯†åˆ«æ¨¡å‹ä»ç„¶ä¼˜äºå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.13976', 'title': 'FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation', 'url': 'https://huggingface.co/papers/2601.13976', 'abstract': 'FantasyVLN presents a unified implicit reasoning framework for vision-and-language navigation that enhances reasoning capabilities without explicit token overhead, achieving real-time performance with improved accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.', 'score': 3, 'issue_id': 691, 'pub_date': '2026-01-20', 'pub_date_card': {'ru': '20 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 20', 'zh': '1æœˆ20æ—¥'}, 'hash': '41af27da3dab1f37', 'authors': ['Jing Zuo', 'Lingzhou Mu', 'Fan Jiang', 'Chengcheng Ma', 'Mu Xu', 'Yonggang Qi'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'Fantasy AIGC Team', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2601.13976.jpg', 'data': {'categories': ['#training', '#agents', '#interpretability', '#multimodal', '#reasoning', '#architecture'], 'emoji': 'ğŸ§­', 'ru': {'title': 'Ğ¡ĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': 'FantasyVLN Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¸Ğ¿Ğ° Chain-of-Thought Ğ±ĞµĞ· ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ĞµĞ¼Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Visual AutoRegressor, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ…, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ·Ğ½Ğ°ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸, ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ² Ğ´ĞµÑÑÑ‚ÑŒ Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ CoT.'}, 'en': {'title': 'Real-Time Navigation with Implicit Reasoning', 'desc': "FantasyVLN introduces a new framework for vision-and-language navigation that enhances reasoning abilities without adding extra computational load. It leverages a compact latent space to encode visual information, allowing for efficient processing during reasoning tasks. By integrating multimodal Chain-of-Thought (CoT) strategies, it improves the model's ability to understand and act on complex instructions in real-time. The results show significant improvements in navigation success rates and reduced latency compared to traditional methods."}, 'zh': {'title': 'æ— æ ‡è®°å¼€é”€çš„å®æ—¶æ¨ç†å¯¼èˆª', 'desc': 'FantasyVLNæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„éšå¼æ¨ç†æ¡†æ¶ï¼Œç”¨äºè§†è§‰ä¸è¯­è¨€å¯¼èˆªï¼Œå¢å¼ºäº†æ¨ç†èƒ½åŠ›è€Œæ— éœ€æ˜¾å¼çš„æ ‡è®°å¼€é”€ï¼Œä»è€Œå®ç°äº†å®æ—¶æ€§èƒ½å’Œæ›´é«˜çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰è‡ªå›å½’æ¨¡å‹ï¼ˆVARï¼‰å°†æƒ³è±¡çš„è§†è§‰æ ‡è®°ç¼–ç åˆ°ç´§å‡‘çš„æ½œåœ¨ç©ºé—´ä¸­ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­ç”±äºç”Ÿæˆè™šæ„è§†è§‰è§‚å¯Ÿè€Œå¯¼è‡´çš„æ ‡è®°è†¨èƒ€é—®é¢˜ã€‚æ¨¡å‹åœ¨æ¨ç†æ—¶èƒ½å¤Ÿç›´æ¥å°†æŒ‡ä»¤æ˜ å°„åˆ°åŠ¨ä½œï¼ŒåŒæ—¶ä¿æŒæ¨ç†æ„ŸçŸ¥çš„è¡¨ç¤ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFantasyVLNåœ¨æˆåŠŸç‡å’Œæ•ˆç‡ä¸Šéƒ½æœ‰æ˜¾è‘—æå‡ï¼Œæ¨ç†å»¶è¿Ÿæ¯”æ˜¾å¼é“¾å¼æ¨ç†æ–¹æ³•å‡å°‘äº†ä¸€ä¸ªæ•°é‡çº§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.14249', 'title': 'Which Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment', 'url': 'https://huggingface.co/papers/2601.14249', 'abstract': "Researchers introduce a novel metric called Rank-Surprisal Ratio (RSR) to better assess the suitability of reasoning trajectories for distilling knowledge from large language models, demonstrating superior performance compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Long chain-of-thought (CoT) trajectories provide rich supervision signals for distilling reasoning from teacher to student LLMs. However, both prior work and our experiments show that trajectories from stronger teachers do not necessarily yield better students, highlighting the importance of data-student suitability in distillation. Existing methods assess suitability primarily through student likelihood, favoring trajectories that closely align with the model's current behavior but overlooking more informative ones. Addressing this, we propose Rank-Surprisal Ratio (RSR), a simple metric that captures both alignment and informativeness to assess the suitability of a reasoning trajectory. RSR is motivated by the observation that effective trajectories typically combine low absolute probability with relatively high-ranked tokens under the student model, balancing learning signal strength and behavioral alignment. Concretely, RSR is defined as the ratio of a trajectory's average token-wise rank to its average negative log-likelihood, and is straightforward to compute and interpret. Across five student models and reasoning trajectories from 11 diverse teachers, RSR strongly correlates with post-training performance (average Spearman 0.86), outperforming existing metrics. We further demonstrate its practical utility in both trajectory selection and teacher selection.", 'score': 2, 'issue_id': 695, 'pub_date': '2026-01-20', 'pub_date_card': {'ru': '20 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 20', 'zh': '1æœˆ20æ—¥'}, 'hash': 'fda5a3725c2639aa', 'authors': ['Yuming Yang', 'Mingyoung Lai', 'Wanxu Zhao', 'Xiaoran Fan', 'Zhiheng Xi', 'Mingqi Wu', 'Chiyue Huang', 'Jun Zhao', 'Haijun Lv', 'Jian Tong', 'Yunhua Zhou', 'Yicheng Zou', 'Qipeng Guo', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang'], 'affiliations': ['Fudan University', 'Shanghai AI Laboratory', 'University of Sydney', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2601.14249.jpg', 'data': {'categories': ['#data', '#training', '#reasoning', '#transfer_learning'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Rank-Surprisal Ratio (RSR) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ° ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑÑƒÑÑ‚ÑÑ Ñ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¼ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. RSR Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ñ€Ğ°Ğ½Ğ³Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğº ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ñ‡ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğµ ĞºĞ°Ğº ÑĞ°Ğ¼Ğ¸Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Rank-Surprisal Ratio: A New Way to Evaluate Knowledge Transfer in LLMs', 'desc': 'This paper presents a new metric called Rank-Surprisal Ratio (RSR) designed to evaluate how well reasoning trajectories can help in transferring knowledge from large language models (LLMs). The authors highlight that simply using trajectories from stronger models does not guarantee better performance in student models, emphasizing the need for a better assessment of data suitability. RSR measures both the alignment of a trajectory with the student model and its informativeness, providing a more balanced evaluation. The results show that RSR correlates strongly with the performance of student models, making it a valuable tool for selecting effective reasoning trajectories and teachers.'}, 'zh': {'title': 'å¼•å…¥æ’åæƒŠè®¶æ¯”ç‡ï¼Œæå‡çŸ¥è¯†è’¸é¦æ•ˆæœ', 'desc': 'ç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ç§æ–°çš„åº¦é‡æ ‡å‡†ï¼Œç§°ä¸ºæ’åæƒŠè®¶æ¯”ç‡ï¼ˆRank-Surprisal Ratioï¼ŒRSRï¼‰ï¼Œç”¨äºæ›´å¥½åœ°è¯„ä¼°æ¨ç†è½¨è¿¹åœ¨ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æå–çŸ¥è¯†çš„é€‚ç”¨æ€§ã€‚é•¿é“¾æ¨ç†è½¨è¿¹ä¸ºä»æ•™å¸ˆæ¨¡å‹åˆ°å­¦ç”Ÿæ¨¡å‹çš„çŸ¥è¯†è’¸é¦æä¾›äº†ä¸°å¯Œçš„ç›‘ç£ä¿¡å·ï¼Œä½†å¼ºæ•™å¸ˆçš„è½¨è¿¹å¹¶ä¸ä¸€å®šèƒ½äº§ç”Ÿæ›´å¥½çš„å­¦ç”Ÿï¼Œå¼ºè°ƒäº†æ•°æ®ä¸å­¦ç”Ÿé€‚ç”¨æ€§çš„é‡è¦æ€§ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡å­¦ç”Ÿçš„å¯èƒ½æ€§æ¥è¯„ä¼°é€‚ç”¨æ€§ï¼Œåå‘äºä¸æ¨¡å‹å½“å‰è¡Œä¸ºç´§å¯†å¯¹é½çš„è½¨è¿¹ï¼Œè€Œå¿½è§†äº†æ›´å…·ä¿¡æ¯é‡çš„è½¨è¿¹ã€‚RSRé€šè¿‡ç»“åˆä½ç»å¯¹æ¦‚ç‡å’Œç›¸å¯¹é«˜æ’åçš„æ ‡è®°ï¼Œå¹³è¡¡å­¦ä¹ ä¿¡å·å¼ºåº¦å’Œè¡Œä¸ºå¯¹é½ï¼Œå±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.12937', 'title': 'On the Evidentiary Limits of Membership Inference for Copyright Auditing', 'url': 'https://huggingface.co/papers/2601.12937', 'abstract': 'Membership inference attacks fail to reliably detect copyrighted text usage in large language models when training data is paraphrased using structure-aware methods that preserve semantic content.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) are trained on increasingly opaque corpora, membership inference attacks (MIAs) have been proposed to audit whether copyrighted texts were used during training, despite growing concerns about their reliability under realistic conditions. We ask whether MIAs can serve as admissible evidence in adversarial copyright disputes where an accused model developer may obfuscate training data while preserving semantic content, and formalize this setting through a judge-prosecutor-accused communication protocol. To test robustness under this protocol, we introduce SAGE (Structure-Aware SAE-Guided Extraction), a paraphrasing framework guided by Sparse Autoencoders (SAEs) that rewrites training data to alter lexical structure while preserving semantic content and downstream utility. Our experiments show that state-of-the-art MIAs degrade when models are fine-tuned on SAGE-generated paraphrases, indicating that their signals are not robust to semantics-preserving transformations. While some leakage remains in certain fine-tuning regimes, these results suggest that MIAs are brittle in adversarial settings and insufficient, on their own, as a standalone mechanism for copyright auditing of LLMs.', 'score': 2, 'issue_id': 688, 'pub_date': '2026-01-19', 'pub_date_card': {'ru': '19 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 19', 'zh': '1æœˆ19æ—¥'}, 'hash': '1f0670983aaf7958', 'authors': ['Murat Bilgehan Ertan', 'Emirhan BÃ¶ge', 'Min Chen', 'Kaleel Mahmood', 'Marten van Dijk'], 'affiliations': ['Centrum Wiskunde & Informatica (CWI), Amsterdam, The Netherlands', 'Independent Researcher', 'University of Rhode Island, Kingston, RI, United States', 'Vrije Universiteit Amsterdam, Amsterdam, The Netherlands'], 'pdf_title_img': 'assets/pdf/title_img/2601.12937.jpg', 'data': {'categories': ['#training', '#security', '#leakage'], 'emoji': 'ğŸ”“', 'ru': {'title': 'Ğ¥Ñ€ÑƒĞ¿ĞºĞ¾ÑÑ‚ÑŒ Ğ°Ñ‚Ğ°Ğº Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ñ‡Ğ»ĞµĞ½ÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ´ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¾-ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸Ğ¼ Ğ¿ĞµÑ€ĞµÑ„Ñ€Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ñ‚Ğ°Ğº Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ñ‡Ğ»ĞµĞ½ÑÑ‚Ğ²Ğ° (MIA) Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ğ¼ Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº SAGE, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¸ (SAE) Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµÑ„Ñ€Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ, Ğ½Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ MIA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ SAGE, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ñ€ÑƒĞ¿ĞºĞ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¸Ñ… Ğ°Ñ‚Ğ°Ğº. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ MIA Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹ ĞºĞ°Ğº ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ»Ñ Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ°Ğ² LLM Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¾Ğ±Ğ¾Ğ¹Ğ´ĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¾-ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ÑÑ… Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'MIAs Struggle Against Paraphrased Training Data', 'desc': 'This paper investigates the effectiveness of membership inference attacks (MIAs) in detecting the use of copyrighted text in large language models (LLMs) when the training data is paraphrased using a method that maintains its meaning. The authors introduce a new framework called SAGE, which uses Sparse Autoencoders to create paraphrases that change the wording but keep the original content intact. Their experiments reveal that MIAs struggle to identify copyrighted material when models are trained on these paraphrased texts, indicating that MIAs are not reliable in adversarial situations. Overall, the findings suggest that MIAs alone cannot be trusted for copyright auditing of LLMs, especially when the training data is obfuscated.'}, 'zh': {'title': 'ä¼šå‘˜æ¨æ–­æ”»å‡»åœ¨è¯­ä¹‰ä¿æŒä¸‹çš„è„†å¼±æ€§', 'desc': 'æœ¬æ–‡æ¢è®¨äº†ä¼šå‘˜æ¨æ–­æ”»å‡»ï¼ˆMIAï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­æ£€æµ‹ç‰ˆæƒæ–‡æœ¬ä½¿ç”¨çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“è®­ç»ƒæ•°æ®é€šè¿‡ç»“æ„æ„ŸçŸ¥çš„æ–¹æ³•è¿›è¡Œæ”¹å†™æ—¶ï¼ŒMIAçš„å¯é æ€§æ˜¾è‘—ä¸‹é™ã€‚è¿™ç§æ–¹æ³•ä½¿ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰æ¥ä¿æŒè¯­ä¹‰å†…å®¹ï¼ŒåŒæ—¶æ”¹å˜è¯æ±‡ç»“æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMIAåœ¨é¢å¯¹è¿™ç§è¯­ä¹‰ä¿æŒçš„æ”¹å†™æ—¶è¡¨ç°å‡ºè„†å¼±æ€§ï¼Œæ— æ³•å•ç‹¬ä½œä¸ºç‰ˆæƒå®¡è®¡çš„æœ‰æ•ˆæœºåˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10237', 'title': 'Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD', 'url': 'https://huggingface.co/papers/2601.10237', 'abstract': "Differentially private stochastic gradient descent with shuffled sampling faces fundamental privacy-utility trade-offs that require substantial noise for meaningful privacy protection, limiting practical performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Differentially Private Stochastic Gradient Descent (DP-SGD) is the dominant paradigm for private training, but its fundamental limitations under worst-case adversarial privacy definitions remain poorly understood. We analyze DP-SGD in the f-differential privacy framework, which characterizes privacy via hypothesis-testing trade-off curves, and study shuffled sampling over a single epoch with M gradient updates. We derive an explicit suboptimal upper bound on the achievable trade-off curve. This result induces a geometric lower bound on the separation Îº which is the maximum distance between the mechanism's trade-off curve and the ideal random-guessing line. Because a large separation implies significant adversarial advantage, meaningful privacy requires small Îº. However, we prove that enforcing a small separation imposes a strict lower bound on the Gaussian noise multiplier Ïƒ, which directly limits the achievable utility. In particular, under the standard worst-case adversarial model, shuffled DP-SGD must satisfy   Ïƒge 1{2ln M} quadorquad Îºge 1{8}!left(1-1{4Ï€ln M}right),   and thus cannot simultaneously achieve strong privacy and high utility. Although this bound vanishes asymptotically as M to infty, the convergence is extremely slow: even for practically relevant numbers of updates the required noise magnitude remains substantial. We further show that the same limitation extends to Poisson subsampling up to constant factors. Our experiments confirm that the noise levels implied by this bound leads to significant accuracy degradation at realistic training settings, thus showing a critical bottleneck in DP-SGD under standard worst-case adversarial assumptions.", 'score': 2, 'issue_id': 689, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': 'a4d9f8e8bd5c2577', 'authors': ['Murat Bilgehan Ertan', 'Marten van Dijk'], 'affiliations': ['CWI Amsterdam'], 'pdf_title_img': 'assets/pdf/title_img/2601.10237.jpg', 'data': {'categories': ['#security', '#optimization', '#training', '#math'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¤ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² DP-SGD', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ ÑĞ¿ÑƒÑĞº (DP-SGD) Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… f-Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· ĞºÑ€Ğ¸Ğ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ° Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·-Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ½Ğ¸Ğ¶Ğ½Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´ĞµĞ» Ğ½Ğ° Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ½Ñƒ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ°, Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DP-SGD Ğ½Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¾ Ğ½Ğ°Ğ¸Ñ…ÑƒĞ´ÑˆĞµĞ¼ ÑĞ»ÑƒÑ‡Ğ°Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ°. Ğ¥Ğ¾Ñ‚Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ°ÑĞ¸Ğ¼Ğ¿Ñ‚Ğ¾Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ÑÑ‡ĞµĞ·Ğ°ĞµÑ‚ Ñ Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ², ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ĞºÑ€Ğ°Ğ¹Ğ½Ğµ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ·Ğ°Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼.'}, 'en': {'title': 'Balancing Privacy and Performance in DP-SGD', 'desc': "This paper investigates the limitations of Differentially Private Stochastic Gradient Descent (DP-SGD) in maintaining privacy while ensuring utility during model training. It introduces the f-differential privacy framework to analyze the trade-offs between privacy and performance, revealing that achieving strong privacy necessitates substantial noise, which degrades model accuracy. The authors derive a suboptimal upper bound on the privacy-utility trade-off curve, indicating that enforcing strict privacy leads to a lower bound on the noise multiplier, limiting the model's effectiveness. Their experiments demonstrate that the required noise levels for meaningful privacy significantly hinder the model's performance in practical scenarios."}, 'zh': {'title': 'éšç§ä¸æ•ˆç”¨çš„æƒè¡¡ï¼šDP-SGDçš„æŒ‘æˆ˜', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å·®åˆ†éšç§éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆDP-SGDï¼‰åœ¨éšç§ä¿æŠ¤ä¸å®ç”¨æ€§ä¹‹é—´çš„æƒè¡¡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¸ºäº†å®ç°æœ‰æ•ˆçš„éšç§ä¿æŠ¤ï¼Œå¿…é¡»å¼•å…¥å¤§é‡å™ªå£°ï¼Œè¿™ä¼šé™åˆ¶æ¨¡å‹çš„å®é™…æ€§èƒ½ã€‚é€šè¿‡åˆ†æf-å·®åˆ†éšç§æ¡†æ¶ï¼Œä½œè€…æ¨å¯¼å‡ºå¯å®ç°çš„æƒè¡¡æ›²çº¿çš„ä¸Šé™ï¼Œå¹¶è¯æ˜äº†å°çš„åˆ†ç¦»åº¦ä¼šå¯¼è‡´å™ªå£°ä¹˜æ•°çš„ä¸¥æ ¼ä¸‹é™ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æ ‡å‡†çš„æœ€åæƒ…å†µå¯¹æŠ—æ¨¡å‹ä¸‹ï¼ŒDP-SGDé¢ä¸´æ˜¾è‘—çš„å‡†ç¡®æ€§ä¸‹é™ï¼Œæ­ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ç“¶é¢ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.13075', 'title': 'METIS: Mentoring Engine for Thoughtful Inquiry & Solutions', 'url': 'https://huggingface.co/papers/2601.13075', 'abstract': 'AI mentor METIS outperforms GPT-5 and Claude Sonnet 4.5 in supporting undergraduate research writing across multiple stages, with higher student scores and improved document-grounded outputs, though challenges remain in tool routing and stage classification.  \t\t\t\t\tAI-generated summary \t\t\t\t Many students lack access to expert research mentorship. We ask whether an AI mentor can move undergraduates from an idea to a paper. We build METIS, a tool-augmented, stage-aware assistant with literature search, curated guidelines, methodology checks, and memory. We evaluate METIS against GPT-5 and Claude Sonnet 4.5 across six writing stages using LLM-as-a-judge pairwise preferences, student-persona rubrics, short multi-turn tutoring, and evidence/compliance checks. On 90 single-turn prompts, LLM judges preferred METIS to Claude Sonnet 4.5 in 71% and to GPT-5 in 54%. Student scores (clarity/actionability/constraint-fit; 90 prompts x 3 judges) are higher across stages. In multi-turn sessions (five scenarios/agent), METIS yields slightly higher final quality than GPT-5. Gains concentrate in document-grounded stages (D-F), consistent with stage-aware routing and groundings failure modes include premature tool routing, shallow grounding, and occasional stage misclassification.', 'score': 1, 'issue_id': 692, 'pub_date': '2026-01-19', 'pub_date_card': {'ru': '19 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 19', 'zh': '1æœˆ19æ—¥'}, 'hash': '77f5f209e8ceffcf', 'authors': ['Abhinav Rajeev Kumar', 'Dhruv Trehan', 'Paras Chopra'], 'affiliations': ['Lossfunk'], 'pdf_title_img': 'assets/pdf/title_img/2601.13075.jpg', 'data': {'categories': ['#benchmark', '#training', '#agents', '#science'], 'emoji': 'ğŸ“š', 'ru': {'title': 'Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜-Ğ½Ğ°ÑÑ‚Ğ°Ğ²Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¸ÑÑŒĞ¼Ğ° ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° METIS â€” Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ½Ğ°ÑÑ‚Ğ°Ğ²Ğ½Ğ¸Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚, Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ñ Ğ¸Ğ´ĞµĞ¸ Ğ¸ Ğ·Ğ°ĞºĞ°Ğ½Ñ‡Ğ¸Ğ²Ğ°Ñ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±ĞµÑĞµĞ´Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµÑ‘ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹, Ñ‡ĞµĞ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ LLM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ METIS Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-5 Ğ¸ Claude Sonnet 4.5 Ğ¿Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ ÑÑƒĞ´ĞµĞ¹ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğ°Ñ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ¼ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ ÑĞ²ÑĞ·Ğ°Ğ½Ñ‹ Ñ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸ Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ³Ğ¾ ÑÑ‚Ğ°Ğ¿Ğ° Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'METIS: The Next Level AI Mentor for Research Writing', 'desc': "The paper presents METIS, an AI mentor designed to assist undergraduate students in research writing by guiding them through various stages of the writing process. METIS outperforms existing models like GPT-5 and Claude Sonnet 4.5, achieving higher student scores and producing better document-grounded outputs. The evaluation involved comparing preferences and scores across multiple writing stages, demonstrating METIS's effectiveness in providing tailored support. However, the study also identifies challenges such as tool routing and stage classification that need to be addressed for further improvement."}, 'zh': {'title': 'METISï¼šè¶…è¶Šä¼ ç»ŸAIçš„ç ”ç©¶å†™ä½œåŠ©æ‰‹', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMETISçš„AIå¯¼å¸ˆï¼Œå®ƒåœ¨æ”¯æŒæœ¬ç§‘ç”Ÿç ”ç©¶å†™ä½œæ–¹é¢è¡¨ç°ä¼˜äºGPT-5å’ŒClaude Sonnet 4.5ã€‚METISèƒ½å¤Ÿå¸®åŠ©å­¦ç”Ÿä»æƒ³æ³•å‘å±•åˆ°è®ºæ–‡ï¼Œæä¾›æ–‡çŒ®æœç´¢ã€æŒ‡å¯¼æ–¹é’ˆã€æ–¹æ³•æ£€æŸ¥å’Œè®°å¿†åŠŸèƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒMETISåœ¨å¤šä¸ªå†™ä½œé˜¶æ®µä¸­ï¼Œå­¦ç”Ÿçš„å¾—åˆ†æ›´é«˜ï¼Œä¸”åœ¨æ–‡æ¡£åŸºç¡€çš„è¾“å‡ºä¸Šè¡¨ç°æ›´ä½³ã€‚å°½ç®¡å¦‚æ­¤ï¼ŒMETISåœ¨å·¥å…·è·¯ç”±å’Œé˜¶æ®µåˆ†ç±»æ–¹é¢ä»é¢ä¸´ä¸€äº›æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.12910', 'title': 'SciCoQA: Quality Assurance for Scientific Paper--Code Alignment', 'url': 'https://huggingface.co/papers/2601.12910', 'abstract': "SciCoQA is a dataset for identifying mismatches between scientific publications and code implementations, containing 611 discrepancies across multiple disciplines and demonstrating the challenge of detecting such issues even for advanced language models.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SciCoQA, a dataset for detecting discrepancies between scientific publications and their codebases to ensure faithful implementations. We construct SciCoQA from GitHub issues and reproducibility papers, and to scale our dataset, we propose a synthetic data generation method for constructing paper-code discrepancies. We analyze the paper-code discrepancies in detail and propose discrepancy types and categories to better understand the occurring mismatches. In total, our dataset consists of 611 paper-code discrepancies (81 real, 530 synthetic), spanning diverse computational science disciplines, including AI, Physics, Quantitative Biology, and others. Our evaluation of 21 LLMs highlights the difficulty of SciCoQA, particularly for instances involving omitted paper details, long-context inputs, and data outside the models' pre-training corpus. The best performing model in our evaluation, GPT-5, can only detect 45.7\\% of real-world paper-code discrepancies.", 'score': 1, 'issue_id': 687, 'pub_date': '2026-01-19', 'pub_date_card': {'ru': '19 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 19', 'zh': '1æœˆ19æ—¥'}, 'hash': '1121d6cf12e6e724', 'authors': ['Tim BaumgÃ¤rtner', 'Iryna Gurevych'], 'affiliations': ['National Research Center for Applied Cybersecurity ATHENE', 'Ubiquitous Knowledge Processing Lab (UKP Lab), Department of Computer Science, TU Darmstadt'], 'pdf_title_img': 'assets/pdf/title_img/2601.12910.jpg', 'data': {'categories': ['#long_context', '#hallucinations', '#synthetic', '#science'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº: ĞºĞ¾Ğ³Ğ´Ğ° ĞºĞ¾Ğ´ Ğ½Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ', 'desc': 'SciCoQA â€” ÑÑ‚Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸ÑÑ… Ğ¸ Ğ¸Ñ… Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² ĞºĞ¾Ğ´Ğµ, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 611 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ½Ğ°ÑƒĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· GitHub-issues Ğ¸ ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ² Ğ¸Ñ… Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞÑ†ĞµĞ½ĞºĞ° 21 LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° 45.7%, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'Bridging the Gap: Detecting Mismatches in Science and Code', 'desc': 'SciCoQA is a newly created dataset designed to identify mismatches between scientific papers and their corresponding code implementations. It includes 611 discrepancies, with a mix of real and synthetic examples, to highlight the challenges faced by language models in detecting these issues. The dataset spans various fields such as AI, Physics, and Quantitative Biology, and categorizes the types of discrepancies for better analysis. Despite testing 21 large language models, the best performer, GPT-5, only managed to detect 45.7% of the real discrepancies, showcasing the complexity of the task.'}, 'zh': {'title': 'è¯†åˆ«ç§‘å­¦å‡ºç‰ˆç‰©ä¸ä»£ç å®ç°çš„ä¸åŒ¹é…', 'desc': 'SciCoQAæ˜¯ä¸€ä¸ªç”¨äºè¯†åˆ«ç§‘å­¦å‡ºç‰ˆç‰©ä¸ä»£ç å®ç°ä¹‹é—´ä¸åŒ¹é…çš„æ•°æ®é›†ï¼ŒåŒ…å«611ä¸ªè·¨å­¦ç§‘çš„å·®å¼‚ã€‚è¿™äº›å·®å¼‚å±•ç¤ºäº†å³ä½¿æ˜¯å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ä¹Ÿéš¾ä»¥æ£€æµ‹æ­¤ç±»é—®é¢˜ã€‚æˆ‘ä»¬ä»GitHubé—®é¢˜å’Œå¯é‡å¤æ€§è®ºæ–‡ä¸­æ„å»ºSciCoQAï¼Œå¹¶æå‡ºäº†ä¸€ç§åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•æ¥æ‰©å±•æ•°æ®é›†ã€‚é€šè¿‡è¯¦ç»†åˆ†æè®ºæ–‡ä¸ä»£ç ä¹‹é—´çš„å·®å¼‚ï¼Œæˆ‘ä»¬æå‡ºäº†å·®å¼‚ç±»å‹å’Œç±»åˆ«ï¼Œä»¥æ›´å¥½åœ°ç†è§£è¿™äº›ä¸åŒ¹é…çš„æƒ…å†µã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10700', 'title': 'LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals', 'url': 'https://huggingface.co/papers/2601.10700', 'abstract': 'A framework for generating structured counterfactual pairs using LLMs and SCMs enables improved evaluation and analysis of concept-based explanations in high-stakes domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets). LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs) of the text generation, interventions on a concept propagate through the SCM until an LLM generates the counterfactual. We introduce three datasets (disease detection, CV screening, and workplace violence prediction) together with a new evaluation metric, order-faithfulness. Using them, we evaluate a wide range of methods across five models and identify substantial headroom for improving concept-based explanations. LIBERTy also enables systematic analysis of model sensitivity to interventions: we find that proprietary LLMs show markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation. Overall, LIBERTy provides a much-needed benchmark for developing faithful explainability methods.', 'score': 1, 'issue_id': 686, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': '68fa6c7fe60c9fca', 'authors': ['Gilat Toker', 'Nitay Calderon', 'Ohad Amosy', 'Roi Reichart'], 'affiliations': ['Technion Israel Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2601.10700.jpg', 'data': {'categories': ['#healthcare', '#interpretability', '#ethics', '#benchmark', '#dataset', '#synthetic'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ñ‹', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ LIBERTy â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (SCM) Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ order-faithfulness Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ LLM Ğ¼ĞµĞ½ĞµĞµ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹ Ğº Ğ´ĞµĞ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸ÑĞ¼ Ğ¸Ğ·-Ğ·Ğ° Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Explainability with LIBERTy: A New Benchmark for Counterfactual Analysis', 'desc': 'This paper presents LIBERTy, a framework that generates structured counterfactual pairs using Large Language Models (LLMs) and Structured Causal Models (SCMs) to enhance the evaluation of concept-based explanations in critical areas. Concept-based explanations help understand how high-level factors, like gender or experience, affect model decisions, which is vital for stakeholders in high-stakes situations. The framework addresses the limitations of existing benchmarks that depend on expensive human-generated counterfactuals by creating datasets that allow for systematic analysis of model behavior. By introducing new datasets and an evaluation metric called order-faithfulness, the study reveals significant opportunities for improving the reliability of concept-based explanations and highlights the sensitivity of models to demographic interventions.'}, 'zh': {'title': 'LIBERTyï¼šæå‡é«˜é£é™©é¢†åŸŸè§£é‡Šæ€§çš„æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œç»“æ„å› æœæ¨¡å‹ï¼ˆSCMsï¼‰ç”Ÿæˆç»“æ„åŒ–åäº‹å®å¯¹çš„æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„é«˜é£é™©é¢†åŸŸä¸­åŸºäºæ¦‚å¿µçš„è§£é‡Šçš„è¯„ä¼°å’Œåˆ†æã€‚åŸºäºæ¦‚å¿µçš„è§£é‡Šé‡åŒ–äº†é«˜å±‚æ¬¡æ¦‚å¿µï¼ˆå¦‚æ€§åˆ«æˆ–ç»éªŒï¼‰å¯¹æ¨¡å‹è¡Œä¸ºçš„å½±å“ï¼Œè¿™å¯¹å†³ç­–è€…è‡³å…³é‡è¦ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¾èµ–äºäººå·¥ç¼–å†™çš„åäº‹å®ï¼Œæˆæœ¬é«˜ä¸”ä¸å¤Ÿå‡†ç¡®ã€‚LIBERTyæ¡†æ¶é€šè¿‡æ„å»ºç»“æ„åäº‹å®å¯¹çš„æ•°æ®é›†ï¼Œæä¾›äº†ä¸€ç§æ–°çš„è¯„ä¼°åŸºå‡†ï¼Œå¸®åŠ©ç³»ç»Ÿåˆ†ææ¨¡å‹å¯¹å¹²é¢„çš„æ•æ„Ÿæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.13697', 'title': 'Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning', 'url': 'https://huggingface.co/papers/2601.13697', 'abstract': 'GRADFILTERING is an uncertainty-aware data selection framework for instruction tuning that uses gradient signal-to-noise ratio to improve LLM adaptation efficiency and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Instruction tuning is a standard paradigm for adapting large language models (LLMs), but modern instruction datasets are large, noisy, and redundant, making full-data fine-tuning costly and often unnecessary. Existing data selection methods either build expensive gradient datastores or assign static scores from a weak proxy, largely ignoring evolving uncertainty, and thus missing a key source of LLM interpretability. We propose GRADFILTERING, an objective-agnostic, uncertainty-aware data selection framework that utilizes a small GPT-2 proxy with a LoRA ensemble and aggregates per-example gradients into a Gradient Signal-to-Noise Ratio (G-SNR) utility. Our method matches or surpasses random subsets and strong baselines in most LLM-as-a-judge evaluations as well as in human assessment. Moreover, GRADFILTERING-selected subsets converge faster than competitive filters under the same compute budget, reflecting the benefit of uncertainty-aware scoring.', 'score': 0, 'issue_id': 695, 'pub_date': '2026-01-20', 'pub_date_card': {'ru': '20 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 20', 'zh': '1æœˆ20æ—¥'}, 'hash': '875ee852a8fada33', 'authors': ['Zhihang Yuan', 'Chengyu Yue', 'Long Huang', 'Litu Ou', 'Lei Shi'], 'affiliations': ['Alibaba Cloud Computing', 'The University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2601.13697.jpg', 'data': {'categories': ['#data', '#training', '#small_models'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ LLM', 'desc': 'GRADFILTERING â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğº ÑˆÑƒĞ¼Ñƒ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾-Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ¿Ñ€Ğ¾ĞºÑĞ¸ GPT-2 Ñ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»ĞµĞ¼ LoRA Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Gradient Signal-to-Noise Ratio (G-SNR). Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ…, Ğ½Ğ¾ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ.'}, 'en': {'title': 'Enhancing LLM Efficiency with Uncertainty-Aware Data Selection', 'desc': 'GRADFILTERING is a new framework designed to select the best data for instruction tuning of large language models (LLMs) by focusing on uncertainty. It uses a measure called Gradient Signal-to-Noise Ratio (G-SNR) to evaluate the usefulness of each data point, which helps in identifying the most informative examples. This approach is more efficient than traditional methods that often rely on static scores or expensive data storage. As a result, GRADFILTERING not only improves the performance of LLMs but also speeds up the training process by selecting high-quality data subsets.'}, 'zh': {'title': 'GRADFILTERINGï¼šæå‡LLMé€‚åº”æ€§çš„æ™ºèƒ½æ•°æ®é€‰æ‹©', 'desc': 'GRADFILTERINGæ˜¯ä¸€ç§å…³æ³¨ä¸ç¡®å®šæ€§çš„æ•°æ®æ˜¾ç¤ºé€‰æ‹©æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é€‚åº”æ•ˆç‡å’Œæ€§èƒ½ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å°å‹GPT-2ä»£ç†å’ŒLoRAé›†æˆï¼Œå°†æ¯ä¸ªç¤ºä¾‹çš„æ¢¯åº¦èšåˆæˆæ¢¯åº¦ä¿¡å™ªæ¯”ï¼ˆG-SNRï¼‰æ•ˆç”¨ï¼Œä»è€Œå®ç°æ•°æ®é€‰æ‹©ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒGRADFILTERINGåœ¨å¤§å¤šæ•°LLMè¯„ä¼°ä¸­è¡¨ç°å‡ºè‰²ï¼Œç”šè‡³åœ¨äººå·¥è¯„ä¼°ä¸­ä¹Ÿä¼˜äºéšæœºå­é›†å’Œå¼ºåŸºçº¿ã€‚é€šè¿‡å…³æ³¨ä¸ç¡®å®šæ€§è¯„åˆ†ï¼ŒGRADFILTERINGé€‰æ‹©çš„å­é›†åœ¨ç›¸åŒè®¡ç®—é¢„ç®—ä¸‹æ”¶æ•›é€Ÿåº¦æ›´å¿«ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.13677', 'title': 'Finally Outshining the Random Baseline: A Simple and Effective Solution for Active Learning in 3D Biomedical Imaging', 'url': 'https://huggingface.co/papers/2601.13677', 'abstract': 'Class-stratified Scheduled Power Predictive Entropy (ClaSP PE) is a novel active learning strategy that improves 3D biomedical image segmentation by addressing class imbalance and selection redundancy through stratified querying and power noising with decay scheduling.  \t\t\t\t\tAI-generated summary \t\t\t\t Active learning (AL) has the potential to drastically reduce annotation costs in 3D biomedical image segmentation, where expert labeling of volumetric data is both time-consuming and expensive. Yet, existing AL methods are unable to consistently outperform improved random sampling baselines adapted to 3D data, leaving the field without a reliable solution. We introduce Class-stratified Scheduled Power Predictive Entropy (ClaSP PE), a simple and effective query strategy that addresses two key limitations of standard uncertainty-based AL methods: class imbalance and redundancy in early selections. ClaSP PE combines class-stratified querying to ensure coverage of underrepresented structures and log-scale power noising with a decaying schedule to enforce query diversity in early-stage AL and encourage exploitation later. In our evaluation on 24 experimental settings using four 3D biomedical datasets within the comprehensive nnActive benchmark, ClaSP PE is the only method that generally outperforms improved random baselines in terms of both segmentation quality with statistically significant gains, whilst remaining annotation efficient. Furthermore, we explicitly simulate the real-world application by testing our method on four previously unseen datasets without manual adaptation, where all experiment parameters are set according to predefined guidelines. The results confirm that ClaSP PE robustly generalizes to novel tasks without requiring dataset-specific tuning. Within the nnActive framework, we present compelling evidence that an AL method can consistently outperform random baselines adapted to 3D segmentation, in terms of both performance and annotation efficiency in a realistic, close-to-production scenario. Our open-source implementation and clear deployment guidelines make it readily applicable in practice. Code is at https://github.com/MIC-DKFZ/nnActive.', 'score': 0, 'issue_id': 694, 'pub_date': '2026-01-20', 'pub_date_card': {'ru': '20 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 20', 'zh': '1æœˆ20æ—¥'}, 'hash': '73a660bd2cdf7cc2', 'authors': ['Carsten T. LÃ¼th', 'Jeremias Traub', 'Kim-Celine Kahl', 'Till J. Bungert', 'Lukas Klein', 'Lars KrÃ¤mer', 'Paul F. JÃ¤ger', 'Klaus Maier-Hein', 'Fabian Isensee'], 'affiliations': ['Faculty of Mathematics and Computer Science, University of Heidelberg, Germany', 'German Cancer Research Center (DKFZ) Heidelberg, Division of Intelligent Medical Systems, Germany', 'German Cancer Research Center (DKFZ) Heidelberg, Division of Medical Image Computing, Germany', 'German Cancer Research Center (DKFZ) Heidelberg, Interactive Machine Learning Group, Germany', 'Helmholtz Imaging, German Cancer Research Center (DKFZ), Heidelberg, Germany', 'Institute for Machine Learning, ETH ZÃ¼rich, Switzerland', 'National Center for Tumor Diseases (NCT) Heidelberg, Germany', 'Pattern Analysis and Learning Group, Department of Radiation Oncology, Heidelberg University Hospital, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2601.13677.jpg', 'data': {'categories': ['#data', '#cv', '#benchmark', '#healthcare'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€: Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¾Ğ² Ğ´Ğ»Ñ 3D ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ClaSP PE Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ° ĞºĞ»Ğ°ÑÑĞ¾Ğ² Ğ¸ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€, Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ½ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€, Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ ĞµĞ³Ğ¾ Ñ Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ñ‹Ğ¼ ÑˆÑƒĞ¼Ğ¾Ğ¼ Ñ ÑƒĞ±Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·Ğ¶Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ClaSP PE Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° 24 ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°Ñ… Ğ¸ 4 Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° nnActive. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ñ… Ğº production ÑÑ€ĞµĞ´Ğµ.'}, 'en': {'title': 'Enhancing 3D Segmentation with Smart Active Learning', 'desc': 'Class-stratified Scheduled Power Predictive Entropy (ClaSP PE) is an innovative active learning strategy designed to enhance 3D biomedical image segmentation. It tackles the challenges of class imbalance and selection redundancy by employing stratified querying and a decay-scheduled power noising technique. This approach ensures that underrepresented classes are adequately sampled while promoting diversity in the selection of data points during the early stages of learning. The results demonstrate that ClaSP PE outperforms traditional random sampling methods, providing significant improvements in segmentation quality and annotation efficiency across various datasets without the need for manual adjustments.'}, 'zh': {'title': 'ä¸»åŠ¨å­¦ä¹ æ–°ç­–ç•¥ï¼šClaSP PE æå‡3Då›¾åƒåˆ†å‰²æ•ˆç‡', 'desc': 'Class-stratified Scheduled Power Predictive Entropy (ClaSP PE) æ˜¯ä¸€ç§æ–°é¢–çš„ä¸»åŠ¨å­¦ä¹ ç­–ç•¥ï¼Œæ—¨åœ¨æ”¹å–„3Dç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„ç±»ä¸å¹³è¡¡å’Œé€‰æ‹©å†—ä½™é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†å±‚æŸ¥è¯¢å’Œå¸¦è¡°å‡è°ƒåº¦çš„åŠŸç‡å™ªå£°æ¥æé«˜æŸ¥è¯¢çš„å¤šæ ·æ€§ï¼Œä»è€Œåœ¨æ—©æœŸé˜¶æ®µå¢å¼ºé€‰æ‹©çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒClaSP PE åœ¨å¤šä¸ª3Dç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºæ”¹è¿›çš„éšæœºåŸºçº¿ï¼Œä¸”åœ¨åˆ†å‰²è´¨é‡å’Œæ³¨é‡Šæ•ˆç‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ã€‚è¯¥æ–¹æ³•çš„å¼€æºå®ç°å’Œæ˜ç¡®çš„éƒ¨ç½²æŒ‡å—ä½¿å…¶åœ¨å®é™…åº”ç”¨ä¸­æ˜“äºä½¿ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.13253', 'title': 'A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus', 'url': 'https://huggingface.co/papers/2601.13253', 'abstract': 'A hybrid methodology combining FastText embeddings, clustering, and AI classification generates a large-scale Turkish semantic relations dataset with high accuracy validation.  \t\t\t\t\tAI-generated summary \t\t\t\t We present a hybrid methodology for generating large-scale semantic relationship datasets in low-resource languages, demonstrated through a comprehensive Turkish semantic relations corpus. Our approach integrates three phases: (1) FastText embeddings with Agglomerative Clustering to identify semantic clusters, (2) Gemini 2.5-Flash for automated semantic relationship classification, and (3) integration with curated dictionary sources. The resulting dataset comprises 843,000 unique Turkish semantic pairs across three relationship types (synonyms, antonyms, co-hyponyms) representing a 10x scale increase over existing resources at minimal cost ($65). We validate the dataset through two downstream tasks: an embedding model achieving 90% top-1 retrieval accuracy and a classification model attaining 90% F1-macro. Our scalable protocol addresses critical data scarcity in Turkish NLP and demonstrates applicability to other low-resource languages. We publicly release the dataset and models.', 'score': 0, 'issue_id': 686, 'pub_date': '2026-01-19', 'pub_date_card': {'ru': '19 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 19', 'zh': '1æœˆ19æ—¥'}, 'hash': '49ba853e5d57b947', 'authors': ['Ebubekir Tosun', 'Mehmet Emin Buldur', 'Ã–zay Ezerceli', 'Mahmoud ElHussieni'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2601.13253.jpg', 'data': {'categories': ['#multilingual', '#data', '#low_resource', '#dataset', '#open_source', '#synthetic'], 'emoji': 'ğŸ‡¹ğŸ‡·', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ñ‚ÑƒÑ€ĞµÑ†ĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ñ‚ÑƒÑ€ĞµÑ†ĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ embeddings FastText Ğ¸ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²ÑĞ·Ğ¸ (ÑĞ¸Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ñ‹, Ğ°Ğ½Ñ‚Ğ¾Ğ½Ğ¸Ğ¼Ñ‹, ĞºĞ¾-Ğ³Ğ¸Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ñ‹) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Gemini 2.5-Flash. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 843 Ñ‚Ñ‹ÑÑÑ‡Ğ¸ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ñ‚ÑƒÑ€ĞµÑ†ĞºĞ¸Ñ… ÑĞ»Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ 90% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ´ĞµÑ„Ğ¸Ñ†Ğ¸Ñ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ° Ğº Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ğ¼.'}, 'en': {'title': 'Scaling Turkish NLP with Hybrid Semantic Relations Dataset', 'desc': 'This paper introduces a hybrid methodology that combines FastText embeddings, clustering, and AI classification to create a large-scale dataset of semantic relations in Turkish. The process involves using Agglomerative Clustering to group similar semantic concepts, followed by automated classification with Gemini 2.5-Flash. The resulting dataset includes 843,000 unique pairs of Turkish words categorized into synonyms, antonyms, and co-hyponyms, significantly expanding existing resources. Validation of the dataset shows high accuracy in both retrieval and classification tasks, addressing the challenge of data scarcity in Turkish natural language processing.'}, 'zh': {'title': 'åˆ›æ–°æ··åˆæ–¹æ³•ï¼Œæå‡åœŸè€³å…¶è¯­è¯­ä¹‰æ•°æ®é›†çš„è§„æ¨¡ä¸å‡†ç¡®æ€§', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ··åˆæ–¹æ³•ï¼Œç»“åˆäº†FastTextåµŒå…¥ã€èšç±»å’Œäººå·¥æ™ºèƒ½åˆ†ç±»ï¼Œç”Ÿæˆäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„åœŸè€³å…¶è¯­è¯­ä¹‰å…³ç³»æ•°æ®é›†ã€‚è¯¥æ–¹æ³•åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šé¦–å…ˆä½¿ç”¨FastTextåµŒå…¥å’Œèšç±»ç®—æ³•è¯†åˆ«è¯­ä¹‰ç°‡ï¼›å…¶æ¬¡åˆ©ç”¨Gemini 2.5-Flashè¿›è¡Œè‡ªåŠ¨è¯­ä¹‰å…³ç³»åˆ†ç±»ï¼›æœ€åä¸ç»è¿‡æ•´ç†çš„è¯å…¸èµ„æºæ•´åˆã€‚æœ€ç»ˆç”Ÿæˆçš„æ•°æ®é›†åŒ…å«843,000å¯¹ç‹¬ç‰¹çš„åœŸè€³å…¶è¯­è¯­ä¹‰å¯¹ï¼Œæ¶µç›–åŒä¹‰è¯ã€åä¹‰è¯å’ŒåŒä¹‰è¯ç¾¤ç­‰ä¸‰ç§å…³ç³»ç±»å‹ï¼Œè§„æ¨¡æ¯”ç°æœ‰èµ„æºå¢åŠ äº†10å€ï¼Œä¸”æˆæœ¬ä»…ä¸º65ç¾å…ƒã€‚æˆ‘ä»¬é€šè¿‡ä¸¤ä¸ªä¸‹æ¸¸ä»»åŠ¡éªŒè¯äº†æ•°æ®é›†çš„å‡†ç¡®æ€§ï¼ŒåµŒå…¥æ¨¡å‹è¾¾åˆ°äº†90%çš„æ£€ç´¢å‡†ç¡®ç‡ï¼Œåˆ†ç±»æ¨¡å‹çš„F1-macroå€¼ä¹Ÿè¾¾åˆ°äº†90%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.13251', 'title': 'Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in a 15-Million Node Turkish Synonym Graph', 'url': 'https://huggingface.co/papers/2601.13251', 'abstract': "A large-scale semantic clustering system addresses the limitation of neural embeddings in distinguishing synonyms from antonyms through a specialized three-way discriminator and novel clustering algorithm.  \t\t\t\t\tAI-generated summary \t\t\t\t Neural embeddings have a notorious blind spot: they can't reliably tell synonyms apart from antonyms. Consequently, increasing similarity thresholds often fails to prevent opposites from being grouped together. We've built a large-scale semantic clustering system specifically designed to tackle this problem head on. Our pipeline chews through 15 million lexical items, evaluates a massive 520 million potential relationships, and ultimately generates 2.9 million high-precision semantic clusters. The system makes three primary contributions. First, we introduce a labeled dataset of 843,000 concept pairs spanning synonymy, antonymy, and co-hyponymy, constructed via Gemini 2.5-Flash LLM augmentation and verified using human-curated dictionary resources. Second, we propose a specialized three-way semantic relation discriminator that achieves 90% macro-F1, enabling robust disambiguation beyond raw embedding similarity. Third, we introduce a novel soft-to-hard clustering algorithm that mitigates semantic drift preventing erroneous transitive chains (e.g., hot -> spicy -> pain -> depression) while simultaneously resolving polysemy. Our approach employs a topology-aware two-stage expansion-pruning procedure with topological voting, ensuring that each term is assigned to exactly one semantically coherent cluster. The resulting resource enables high-precision semantic search and retrieval-augmented generation, particularly for morphologically rich and low-resource languages where existing synonym databases remain sparse.", 'score': 0, 'issue_id': 686, 'pub_date': '2026-01-19', 'pub_date_card': {'ru': '19 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 19', 'zh': '1æœˆ19æ—¥'}, 'hash': 'a69fdbc81b2cacaf', 'authors': ['Ebubekir Tosun', 'Mehmet Emin Buldur', 'Ã–zay Ezerceli', 'Mahmoud ElHussieni'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2601.13251.jpg', 'data': {'categories': [], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğµ ÑĞ¸Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¾Ğ² Ğ¸ Ğ°Ğ½Ñ‚Ğ¾Ğ½Ğ¸Ğ¼Ğ¾Ğ²: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ñ€ĞµĞ¹Ñ„Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ ÑĞ¸Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ñ‹ Ğ¸ Ğ°Ğ½Ñ‚Ğ¾Ğ½Ğ¸Ğ¼Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ²ÑˆÑƒÑ 15 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞµĞ´Ğ¸Ğ½Ğ¸Ñ† Ğ¸ 520 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ÑÑ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (90% macro-F1) Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¸Ğ¸, Ğ°Ğ½Ñ‚Ğ¾Ğ½Ğ¸Ğ¼Ğ¸Ğ¸ Ğ¸ ĞºĞ¾-Ğ³Ğ¸Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¼ÑĞ³ĞºĞ¾-Ğ¶Ñ‘ÑÑ‚ĞºĞ¾Ğ¹ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ° 2,9 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ².'}, 'en': {'title': 'Revolutionizing Semantic Clustering: Distinguishing Synonyms from Antonyms', 'desc': 'This paper presents a large-scale semantic clustering system that improves the differentiation between synonyms and antonyms, which is a common limitation of neural embeddings. The system utilizes a three-way discriminator and a novel clustering algorithm to accurately group lexical items. It processes a vast dataset of 15 million items and identifies 2.9 million high-precision semantic clusters. Key innovations include a labeled dataset for semantic relationships, a robust discriminator achieving 90% macro-F1, and a clustering method that prevents semantic drift and resolves polysemy.'}, 'zh': {'title': 'çªç ´ç¥ç»åµŒå…¥çš„è¯­ä¹‰èšç±»æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§å¤§è§„æ¨¡è¯­ä¹‰èšç±»ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ç¥ç»åµŒå…¥åœ¨åŒºåˆ†åŒä¹‰è¯å’Œåä¹‰è¯æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥ç³»ç»Ÿä½¿ç”¨äº†ä¸€ä¸ªä¸“é—¨çš„ä¸‰å…ƒåˆ¤åˆ«å™¨å’Œæ–°é¢–çš„èšç±»ç®—æ³•ï¼Œèƒ½å¤Ÿå¤„ç†1500ä¸‡ä¸ªè¯æ±‡é¡¹ï¼Œå¹¶è¯„ä¼°5.2äº¿ä¸ªæ½œåœ¨å…³ç³»ï¼Œæœ€ç»ˆç”Ÿæˆ290ä¸‡ä¸ªé«˜ç²¾åº¦çš„è¯­ä¹‰èšç±»ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«843,000å¯¹æ¦‚å¿µçš„æ ‡æ³¨æ•°æ®é›†ï¼Œæ¶µç›–åŒä¹‰è¯ã€åä¹‰è¯å’Œå…±åŒä¸‹ä½è¯ï¼Œå¹¶é€šè¿‡äººç±»æ ¡å¯¹çš„å­—å…¸èµ„æºè¿›è¡ŒéªŒè¯ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿé‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„è½¯åˆ°ç¡¬èšç±»ç®—æ³•ï¼Œæœ‰æ•ˆé˜²æ­¢è¯­ä¹‰æ¼‚ç§»å’Œå¤šä¹‰è¯é—®é¢˜ï¼Œç¡®ä¿æ¯ä¸ªæœ¯è¯­è¢«åˆ†é…åˆ°ä¸€ä¸ªè¯­ä¹‰ä¸€è‡´çš„èšç±»ä¸­ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (6)', '#agi', '#alignment (1)', '#architecture (4)', '#audio (2)', '#benchmark (12)', '#cv (3)', '#data (6)', '#dataset (9)', '#diffusion (1)', '#ethics (1)', '#games (1)', '#graphs', '#hallucinations (2)', '#healthcare (3)', '#inference (1)', '#interpretability (2)', '#leakage (1)', '#long_context (3)', '#low_resource (2)', '#machine_translation', '#math (1)', '#multilingual (3)', '#multimodal (6)', '#open_source (5)', '#optimization (3)', '#plp (1)', '#rag (1)', '#reasoning (4)', '#rl (4)', '#rlhf', '#robotics (1)', '#science (2)', '#security (2)', '#small_models (3)', '#story_generation', '#survey (2)', '#synthetic (3)', '#training (13)', '#transfer_learning (3)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2026-01-21 15:35',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2026-01-21 15:35')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2026-01-21 15:35')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    