
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 18 papers. May 19.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">19 мая</span> | <span id="title-articles-count">18 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-05-16.html">⬅️ <span id="prev-date">16.05</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-05-20.html">➡️ <span id="next-date">20.05</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-05.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'};
        let feedDateNext = {'ru': '20.05', 'en': '05/20', 'zh': '5月20日'};
        let feedDatePrev = {'ru': '16.05', 'en': '05/16', 'zh': '5月16日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2505.09388', 'title': 'Qwen3 Technical Report', 'url': 'https://huggingface.co/papers/2505.09388', 'abstract': 'In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.', 'score': 105, 'issue_id': 3823, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 мая', 'en': 'May 14', 'zh': '5月14日'}, 'hash': '69a0f87bb5460e8d', 'authors': ['An Yang', 'Anfeng Li', 'Baosong Yang', 'Beichen Zhang', 'Binyuan Hui', 'Bo Zheng', 'Bowen Yu', 'Chang Gao', 'Chengen Huang', 'Chenxu Lv', 'Chujie Zheng', 'Dayiheng Liu', 'Fan Zhou', 'Fei Huang', 'Feng Hu', 'Hao Ge', 'Haoran Wei', 'Huan Lin', 'Jialong Tang', 'Jian Yang', 'Jianhong Tu', 'Jianwei Zhang', 'Jianxin Yang', 'Jiaxi Yang', 'Jing Zhou', 'Jingren Zhou', 'Junyang Lin', 'Kai Dang', 'Keqin Bao', 'Kexin Yang', 'Le Yu', 'Lianghao Deng', 'Mei Li', 'Mingfeng Xue', 'Mingze Li', 'Pei Zhang', 'Peng Wang', 'Qin Zhu', 'Rui Men', 'Ruize Gao', 'Shixuan Liu', 'Shuang Luo', 'Tianhao Li', 'Tianyi Tang', 'Wenbiao Yin', 'Xingzhang Ren', 'Xinyu Wang', 'Xinyu Zhang', 'Xuancheng Ren', 'Yang Fan', 'Yang Su', 'Yichang Zhang', 'Yinger Zhang', 'Yu Wan', 'Yuqiong Liu', 'Zekun Wang', 'Zeyu Cui', 'Zhenru Zhang', 'Zhipeng Zhou', 'Zihan Qiu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.09388.jpg', 'data': {'categories': ['#low_resource', '#agi', '#reasoning', '#multilingual', '#benchmark', '#architecture', '#training', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Qwen3: Единая модель для мышления и быстрых ответов', 'desc': 'Qwen3 - это новое семейство больших языковых моделей (LLM), разработанное для улучшения производительности, эффективности и многоязычных возможностей. Ключевой инновацией Qwen3 является интеграция режима мышления и режима без мышления в единую структуру, что позволяет динамически переключаться между ними. Модель вводит механизм бюджета мышления, позволяющий адаптивно распределять вычислительные ресурсы во время вывода. Qwen3 достигает передовых результатов в различных бенчмарках и поддерживает 119 языков и диалектов.'}, 'en': {'title': 'Qwen3: Unifying Thinking and Efficiency in Language Models', 'desc': 'Qwen3 is the latest version of the Qwen model family, featuring large language models that enhance performance, efficiency, and multilingual capabilities. It includes both dense and Mixture-of-Expert (MoE) architectures with a wide range of parameters, from 0.6 to 235 billion. A notable innovation is the integration of thinking and non-thinking modes, allowing for seamless dynamic switching based on user needs, which improves response times and reasoning capabilities. Additionally, Qwen3 supports 119 languages, significantly increasing its accessibility and effectiveness in diverse applications, while also providing a thinking budget mechanism for optimized resource allocation during inference.'}, 'zh': {'title': 'Qwen3：统一思维与响应的智能语言模型', 'desc': '本文介绍了Qwen3，这是Qwen模型系列的最新版本。Qwen3包含一系列大型语言模型，旨在提高性能、效率和多语言能力。其创新之处在于将思维模式和非思维模式整合到一个统一框架中，实现动态模式切换，适应用户查询。Qwen3还引入了思维预算机制，允许用户在推理过程中自适应分配计算资源，从而在任务复杂性基础上平衡延迟和性能。'}}}, {'id': 'https://huggingface.co/papers/2505.11049', 'title': 'GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning', 'url': 'https://huggingface.co/papers/2505.11049', 'abstract': "To enhance the safety of VLMs, this paper introduces a novel reasoning-based VLM guard model dubbed GuardReasoner-VL. The core idea is to incentivize the guard model to deliberatively reason before making moderation decisions via online RL. First, we construct GuardReasoner-VLTrain, a reasoning corpus with 123K samples and 631K reasoning steps, spanning text, image, and text-image inputs. Then, based on it, we cold-start our model's reasoning ability via SFT. In addition, we further enhance reasoning regarding moderation through online RL. Concretely, to enhance diversity and difficulty of samples, we conduct rejection sampling followed by data augmentation via the proposed safety-aware data concatenation. Besides, we use a dynamic clipping parameter to encourage exploration in early stages and exploitation in later stages. To balance performance and token efficiency, we design a length-aware safety reward that integrates accuracy, format, and token cost. Extensive experiments demonstrate the superiority of our model. Remarkably, it surpasses the runner-up by 19.27% F1 score on average. We release data, code, and models (3B/7B) of GuardReasoner-VL at https://github.com/yueliu1999/GuardReasoner-VL/", 'score': 41, 'issue_id': 3829, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': 'bedca054f1392a71', 'authors': ['Yue Liu', 'Shengfang Zhai', 'Mingzhe Du', 'Yulin Chen', 'Tri Cao', 'Hongcheng Gao', 'Cheng Wang', 'Xinfeng Li', 'Kun Wang', 'Junfeng Fang', 'Jiaheng Zhang', 'Bryan Hooi'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2505.11049.jpg', 'data': {'categories': ['#multimodal', '#rl', '#dataset', '#reasoning', '#open_source', '#training'], 'emoji': '🛡️', 'ru': {'title': 'Рассуждающий страж для безопасных визуально-языковых моделей', 'desc': 'Эта статья представляет GuardReasoner-VL - новую модель-охранник для визуально-языковых моделей (VLM), использующую рассуждения для повышения безопасности. Модель обучается с помощью онлайн-обучения с подкреплением (RL) на специально созданном корпусе GuardReasoner-VLTrain, содержащем 123 тысячи образцов с рассуждениями. Авторы применяют несколько техник для улучшения обучения, включая отбор образцов, аугментацию данных и динамическую настройку параметров исследования. Эксперименты показывают, что GuardReasoner-VL превосходит другие модели, улучшая F1-меру на 19.27% в среднем.'}, 'en': {'title': 'GuardReasoner-VL: Enhancing VLM Safety through Reasoning and Reinforcement Learning', 'desc': 'This paper presents GuardReasoner-VL, a new model designed to improve the safety of Vision-Language Models (VLMs) by incorporating reasoning into moderation decisions. The model is trained using a large reasoning corpus that includes diverse text and image inputs, allowing it to learn from 123K samples and 631K reasoning steps. To enhance its reasoning capabilities, the model employs supervised fine-tuning (SFT) and online reinforcement learning (RL), which helps it adapt and improve over time. The results show that GuardReasoner-VL significantly outperforms existing models, achieving a 19.27% higher F1 score on average, demonstrating its effectiveness in ensuring safer VLM operations.'}, 'zh': {'title': '推理驱动的安全保护：GuardReasoner-VL', 'desc': '为了提高视觉语言模型（VLM）的安全性，本文提出了一种新的基于推理的VLM保护模型，称为GuardReasoner-VL。该模型通过在线强化学习（RL）激励保护模型在做出审查决策之前进行深思熟虑的推理。我们构建了一个包含123K样本和631K推理步骤的推理语料库，并通过监督微调（SFT）来冷启动模型的推理能力。此外，我们通过在线RL进一步增强了审查推理的能力，最终实验结果显示该模型在F1分数上平均超越了第二名19.27%。'}}}, {'id': 'https://huggingface.co/papers/2505.10610', 'title': 'MMLongBench: Benchmarking Long-Context Vision-Language Models\n  Effectively and Thoroughly', 'url': 'https://huggingface.co/papers/2505.10610', 'abstract': "The rapid extension of context windows in large vision-language models has given rise to long-context vision-language models (LCVLMs), which are capable of handling hundreds of images with interleaved text tokens in a single forward pass. In this work, we introduce MMLongBench, the first benchmark covering a diverse set of long-context vision-language tasks, to evaluate LCVLMs effectively and thoroughly. MMLongBench is composed of 13,331 examples spanning five different categories of downstream tasks, such as Visual RAG and Many-Shot ICL. It also provides broad coverage of image types, including various natural and synthetic images. To assess the robustness of the models to different input lengths, all examples are delivered at five standardized input lengths (8K-128K tokens) via a cross-modal tokenization scheme that combines vision patches and text tokens. Through a thorough benchmarking of 46 closed-source and open-source LCVLMs, we provide a comprehensive analysis of the current models' vision-language long-context ability. Our results show that: i) performance on a single task is a weak proxy for overall long-context capability; ii) both closed-source and open-source models face challenges in long-context vision-language tasks, indicating substantial room for future improvement; iii) models with stronger reasoning ability tend to exhibit better long-context performance. By offering wide task coverage, various image types, and rigorous length control, MMLongBench provides the missing foundation for diagnosing and advancing the next generation of LCVLMs.", 'score': 37, 'issue_id': 3830, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': '565f788b384ce66a', 'authors': ['Zhaowei Wang', 'Wenhao Yu', 'Xiyu Ren', 'Jipeng Zhang', 'Yu Zhao', 'Rohit Saxena', 'Liang Cheng', 'Ginny Wong', 'Simon See', 'Pasquale Minervini', 'Yangqiu Song', 'Mark Steedman'], 'affiliations': ['CSE Department, HKUST', 'Miniml.AI', 'NVIDIA AI Technology Center (NVAITC), NVIDIA, Santa Clara, USA', 'Tencent AI Seattle Lab', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2505.10610.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#multimodal', '#long_context'], 'emoji': '📏', 'ru': {'title': 'MMLongBench: новый стандарт оценки мультимодальных моделей с длинным контекстом', 'desc': 'В работе представлен MMLongBench - первый бенчмарк для оценки моделей обработки длинных контекстов в области компьютерного зрения и языка (LCVLM). Бенчмарк включает 13,331 примеров из 5 категорий задач, охватывающих различные типы изображений. Оценка проводится на стандартизированных входных последовательностях длиной от 8K до 128K токенов. Результаты тестирования 46 моделей показали значительный потенциал для улучшения в задачах обработки длинных мультимодальных контекстов.'}, 'en': {'title': 'MMLongBench: Advancing Long-Context Vision-Language Models', 'desc': 'This paper introduces MMLongBench, a new benchmark designed to evaluate long-context vision-language models (LCVLMs) that can process large amounts of images and text simultaneously. It includes 13,331 examples across five task categories, ensuring a diverse assessment of model performance. The benchmark tests models at various input lengths, allowing for a detailed analysis of their capabilities in handling long-context tasks. The findings reveal that current models struggle with long-context challenges, highlighting the need for improvements and suggesting that models with better reasoning skills perform better in these scenarios.'}, 'zh': {'title': 'MMLongBench：长上下文视觉语言模型的评估新基准', 'desc': '本文介绍了MMLongBench，这是第一个针对长上下文视觉语言模型（LCVLMs）的基准测试，旨在全面评估这些模型的性能。MMLongBench包含13331个示例，涵盖五种不同类别的下游任务，并提供多种自然和合成图像类型。通过对46个闭源和开源LCVLM的深入评估，研究发现单一任务的表现并不能有效反映整体的长上下文能力。结果表明，模型的推理能力越强，长上下文表现越好，未来在这一领域仍有很大的改进空间。'}}}, {'id': 'https://huggingface.co/papers/2505.11409', 'title': "Visual Planning: Let's Think Only with Images", 'url': 'https://huggingface.co/papers/2505.11409', 'abstract': 'Recent advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have substantially enhanced machine reasoning across diverse tasks. However, these models predominantly rely on pure text as the medium for both expressing and structuring reasoning, even when visual information is present. In this work, we argue that language may not always be the most natural or effective modality for reasoning, particularly in tasks involving spatial and geometrical information. Motivated by this, we propose a new paradigm, Visual Planning, which enables planning through purely visual representations, independent of text. In this paradigm, planning is executed via sequences of images that encode step-by-step inference in the visual domain, akin to how humans sketch or visualize future actions. We introduce a novel reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models, leading to substantial improvements in planning in a selection of representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our visual planning paradigm outperforms all other planning variants that conduct reasoning in the text-only space. Our results establish Visual Planning as a viable and promising alternative to language-based reasoning, opening new avenues for tasks that benefit from intuitive, image-based inference.', 'score': 29, 'issue_id': 3825, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': '67875b7838a7b7ea', 'authors': ['Yi Xu', 'Chengzu Li', 'Han Zhou', 'Xingchen Wan', 'Caiqi Zhang', 'Anna Korhonen', 'Ivan Vulić'], 'affiliations': ['Google', 'Language Technology Lab, University of Cambridge', 'University College London'], 'pdf_title_img': 'assets/pdf/title_img/2505.11409.jpg', 'data': {'categories': ['#games', '#reasoning', '#multimodal', '#training', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Визуальное планирование: новый подход к машинному рассуждению без слов', 'desc': "Статья представляет новую парадигму под названием 'Визуальное планирование', которая позволяет осуществлять планирование с помощью чисто визуальных представлений, без использования текста. Авторы предлагают фреймворк обучения с подкреплением VPRL, усиленный методом GRPO для дообучения больших моделей компьютерного зрения. Эксперименты показывают, что визуальное планирование превосходит текстовые методы рассуждений в задачах визуальной навигации. Результаты открывают новые возможности для задач, требующих интуитивного, основанного на изображениях вывода."}, 'en': {'title': 'Visual Planning: Reasoning Beyond Text', 'desc': 'This paper introduces a new approach called Visual Planning, which focuses on using visual representations for reasoning instead of relying solely on text. The authors argue that for tasks involving spatial and geometrical information, visual reasoning can be more effective. They present a reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), which enhances planning capabilities in visual navigation tasks. The results show that this visual approach outperforms traditional text-based reasoning methods, suggesting a shift towards image-based inference in machine learning applications.'}, 'zh': {'title': '视觉规划：超越文本的推理新范式', 'desc': '最近，大型语言模型（LLMs）和多模态扩展（MLLMs）的进展显著提升了机器推理能力。然而，这些模型主要依赖纯文本来表达和构建推理，即使在存在视觉信息的情况下。我们提出了一种新的范式，称为视觉规划，允许通过纯视觉表示进行规划，而不依赖文本。我们的研究表明，视觉规划在处理空间和几何信息的任务中，比基于语言的推理更有效。'}}}, {'id': 'https://huggingface.co/papers/2505.11107', 'title': 'Group Think: Multiple Concurrent Reasoning Agents Collaborating at Token\n  Level Granularity', 'url': 'https://huggingface.co/papers/2505.11107', 'abstract': "Recent advances in large language models (LLMs) have demonstrated the power of reasoning through self-generated chains of thought. Multiple reasoning agents can collaborate to raise joint reasoning quality above individual outcomes. However, such agents typically interact in a turn-based manner, trading increased latency for improved quality. In this paper, we propose Group Think--a single LLM that acts as multiple concurrent reasoning agents, or thinkers. With shared visibility into each other's partial generation progress, Group Think introduces a new concurrent-reasoning paradigm in which multiple reasoning trajectories adapt dynamically to one another at the token level. For example, a reasoning thread may shift its generation mid-sentence upon detecting that another thread is better positioned to continue. This fine-grained, token-level collaboration enables Group Think to reduce redundant reasoning and improve quality while achieving significantly lower latency. Moreover, its concurrent nature allows for efficient utilization of idle computational resources, making it especially suitable for edge inference, where very small batch size often underutilizes local~GPUs. We give a simple and generalizable modification that enables any existing LLM to perform Group Think on a local GPU. We also present an evaluation strategy to benchmark reasoning latency and empirically demonstrate latency improvements using open-source LLMs that were not explicitly trained for Group Think. We hope this work paves the way for future LLMs to exhibit more sophisticated and more efficient collaborative behavior for higher quality generation.", 'score': 14, 'issue_id': 3828, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': 'b2ea7382367e75ba', 'authors': ['Chan-Jan Hsu', 'Davide Buffelli', 'Jamie McGowan', 'Feng-Ting Liao', 'Yi-Chang Chen', 'Sattar Vakili', 'Da-shan Shiu'], 'affiliations': ['MediaTek Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.11107.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#reasoning', '#optimization', '#agents', '#training', '#open_source', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Коллективное мышление: параллельные рассуждения в одной языковой модели', 'desc': 'В статье представлен метод Group Think, позволяющий большой языковой модели (LLM) действовать как несколько параллельных рассуждающих агентов. Эти агенты могут динамически адаптировать свои рассуждения на уровне токенов, что позволяет снизить избыточность и повысить качество при значительно меньшей задержке. Метод особенно подходит для периферийных вычислений, где часто недоиспользуются локальные GPU. Авторы предлагают простую модификацию, позволяющую любой существующей LLM выполнять Group Think на локальном GPU.'}, 'en': {'title': 'Group Think: Collaborative Reasoning for Faster, Smarter LLMs', 'desc': "This paper introduces Group Think, a novel approach that allows a single large language model (LLM) to function as multiple reasoning agents working together simultaneously. By enabling these agents to share visibility into each other's progress, they can dynamically adjust their reasoning paths at the token level, enhancing the overall quality of the output. This concurrent reasoning reduces redundancy and improves efficiency, leading to lower latency compared to traditional turn-based interactions. The authors also provide a method to adapt existing LLMs for Group Think, demonstrating its effectiveness through empirical evaluations."}, 'zh': {'title': 'Group Think：提升推理质量的新方法', 'desc': '本文提出了一种名为Group Think的新方法，它利用单个大型语言模型（LLM）作为多个并发推理代理。通过共享彼此的生成进度，这种方法允许推理线程在生成过程中动态适应，从而减少冗余推理并提高生成质量。Group Think的并发特性使得计算资源得到更有效的利用，特别适合边缘推理场景。我们还提供了一种简单的修改方法，使现有的LLM能够在本地GPU上实现Group Think。'}}}, {'id': 'https://huggingface.co/papers/2505.07675', 'title': 'Simple Semi-supervised Knowledge Distillation from Vision-Language\n  Models via texttt{D}ual-texttt{H}ead\n  texttt{O}ptimization', 'url': 'https://huggingface.co/papers/2505.07675', 'abstract': 'Vision-language models (VLMs) have achieved remarkable success across diverse tasks by leveraging rich textual information with minimal labeled data. However, deploying such large models remains challenging, particularly in resource-constrained environments. Knowledge distillation (KD) offers a well-established solution to this problem; however, recent KD approaches from VLMs often involve multi-stage training or additional tuning, increasing computational overhead and optimization complexity. In this paper, we propose texttt{D}ual-texttt{H}ead texttt{O}ptimization (texttt{DHO}) -- a simple yet effective KD framework that transfers knowledge from VLMs to compact, task-specific models in semi-supervised settings. Specifically, we introduce dual prediction heads that independently learn from labeled data and teacher predictions, and propose to linearly combine their outputs during inference. We observe that DHO mitigates gradient conflicts between supervised and distillation signals, enabling more effective feature learning than single-head KD baselines. As a result, extensive experiments show that DHO consistently outperforms baselines across multiple domains and fine-grained datasets. Notably, on ImageNet, it achieves state-of-the-art performance, improving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively, while using fewer parameters.', 'score': 13, 'issue_id': 3828, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 мая', 'en': 'May 12', 'zh': '5月12日'}, 'hash': '73f4f4dd13e67a25', 'authors': ['Seongjae Kang', 'Dong Bok Lee', 'Hyungjoon Jang', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST', 'VUNO Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.07675.jpg', 'data': {'categories': ['#small_models', '#optimization', '#transfer_learning', '#training', '#cv', '#data'], 'emoji': '🧠', 'ru': {'title': 'DHO: Эффективная дистилляция знаний для компактных моделей компьютерного зрения', 'desc': 'Статья представляет новый метод дистилляции знаний под названием Dual-Head Optimization (DHO) для эффективной передачи знаний от крупных визуально-языковых моделей (VLM) к компактным моделям для конкретных задач. DHO использует две независимые головы предсказания, которые обучаются на размеченных данных и предсказаниях учителя соответственно. Этот подход позволяет избежать конфликтов градиентов между сигналами обучения с учителем и без него, что приводит к более эффективному обучению признаков. Эксперименты показывают, что DHO превосходит базовые методы на различных наборах данных, достигая лучших результатов на ImageNet при использовании меньшего количества параметров.'}, 'en': {'title': 'Streamlining Knowledge Distillation with Dual-Head Optimization', 'desc': 'This paper introduces a new framework called Dual-Head Optimization (DHO) for knowledge distillation from vision-language models (VLMs) to smaller, task-specific models. DHO uses two prediction heads that learn from both labeled data and the predictions of a larger teacher model, which helps to reduce conflicts in learning signals. The method simplifies the distillation process, avoiding the complexity of multi-stage training while still improving feature learning. Experiments show that DHO outperforms existing methods, achieving better accuracy on datasets like ImageNet with fewer parameters.'}, 'zh': {'title': 'DHO：高效的知识蒸馏框架', 'desc': '本文提出了一种名为DHO的知识蒸馏框架，旨在将视觉语言模型（VLMs）的知识转移到紧凑的任务特定模型中。DHO通过引入双预测头，分别从标记数据和教师预测中独立学习，并在推理时线性组合它们的输出。实验结果表明，DHO有效缓解了监督信号和蒸馏信号之间的梯度冲突，从而实现了比单头知识蒸馏基线更有效的特征学习。最终，DHO在多个领域和细粒度数据集上均表现优异，尤其在ImageNet上，使用更少的参数实现了3%的准确率提升。'}}}, {'id': 'https://huggingface.co/papers/2505.11427', 'title': 'Mergenetic: a Simple Evolutionary Model Merging Library', 'url': 'https://huggingface.co/papers/2505.11427', 'abstract': 'Model merging allows combining the capabilities of existing models into a new one - post hoc, without additional training. This has made it increasingly popular thanks to its low cost and the availability of libraries that support merging on consumer GPUs. Recent work shows that pairing merging with evolutionary algorithms can boost performance, but no framework currently supports flexible experimentation with such strategies in language models. We introduce Mergenetic, an open-source library for evolutionary model merging. Mergenetic enables easy composition of merging methods and evolutionary algorithms while incorporating lightweight fitness estimators to reduce evaluation costs. We describe its design and demonstrate that Mergenetic produces competitive results across tasks and languages using modest hardware.', 'score': 10, 'issue_id': 3828, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': '759eb3fbdec85844', 'authors': ['Adrian Robert Minut', 'Tommaso Mencattini', 'Andrea Santilli', 'Donato Crisostomi', 'Emanuele Rodolà'], 'affiliations': ['Ecole Polytechnique Fédérale de Lausanne', 'Sapienza University of Rome'], 'pdf_title_img': 'assets/pdf/title_img/2505.11427.jpg', 'data': {'categories': ['#optimization', '#architecture', '#open_source', '#training'], 'emoji': '🧬', 'ru': {'title': 'Эволюционное слияние языковых моделей без переобучения', 'desc': 'Статья представляет библиотеку Mergenetic для эволюционного объединения языковых моделей. Она позволяет комбинировать существующие модели без дополнительного обучения, используя эволюционные алгоритмы. Mergenetic поддерживает гибкое экспериментирование с различными стратегиями объединения и включает легковесные оценщики качества для снижения вычислительных затрат. Результаты показывают конкурентоспособность подхода на различных задачах и языках при использовании скромных вычислительных ресурсов.'}, 'en': {'title': 'Mergenetic: Evolving Better Models Through Merging', 'desc': 'This paper presents Mergenetic, an open-source library designed for evolutionary model merging in machine learning. Model merging allows the combination of existing models into a new one without the need for additional training, making it cost-effective and efficient. Mergenetic enhances this process by integrating evolutionary algorithms, which can improve model performance. The library also includes lightweight fitness estimators to minimize evaluation costs, demonstrating competitive results across various tasks and languages using standard hardware.'}, 'zh': {'title': 'Mergenetic：进化模型合并的新选择', 'desc': '模型合并是一种将现有模型的能力结合成新模型的方法，无需额外训练。这种方法因其低成本和支持消费者GPU的库而越来越受欢迎。最近的研究表明，将合并与进化算法结合可以提高性能，但目前没有框架支持在语言模型中灵活实验这些策略。我们介绍了Mergenetic，这是一个开源库，用于进化模型合并，能够轻松组合合并方法和进化算法，同时引入轻量级的适应度评估器以降低评估成本。'}}}, {'id': 'https://huggingface.co/papers/2505.10518', 'title': 'Multi-Token Prediction Needs Registers', 'url': 'https://huggingface.co/papers/2505.10518', 'abstract': 'Multi-token prediction has emerged as a promising objective for improving language model pretraining, but its benefits have not consistently generalized to other settings such as fine-tuning. In this paper, we propose MuToR, a simple and effective approach to multi-token prediction that interleaves learnable register tokens into the input sequence, each tasked with predicting future targets. Compared to existing methods, MuToR offers several key advantages: it introduces only a negligible number of additional parameters, requires no architectural changes--ensuring compatibility with off-the-shelf pretrained language models--and remains aligned with the next-token pretraining objective, making it especially well-suited for supervised fine-tuning. Moreover, it naturally supports scalable prediction horizons. We demonstrate the effectiveness and versatility of MuToR across a range of use cases, including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining, on challenging generative tasks in both language and vision domains. Our code will be available at: https://github.com/nasosger/MuToR.', 'score': 8, 'issue_id': 3830, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': '10aef9838701ab6e', 'authors': ['Anastasios Gerontopoulos', 'Spyros Gidaris', 'Nikos Komodakis'], 'affiliations': ['Archimedes, Athena Research Center', 'IACM-Forth', 'University of Crete', 'valeo.ai'], 'pdf_title_img': 'assets/pdf/title_img/2505.10518.jpg', 'data': {'categories': ['#training', '#multimodal', '#optimization'], 'emoji': '🔮', 'ru': {'title': 'MuToR: Эффективное многомаркерное предсказание без изменения архитектуры модели', 'desc': 'В статье представлен новый подход к многомаркерному предсказанию в языковых моделях под названием MuToR. Этот метод вставляет обучаемые токены-регистры в входную последовательность для предсказания будущих целей. MuToR имеет ряд преимуществ: минимальное количество дополнительных параметров, совместимость с существующими предобученными моделями и масштабируемые горизонты предсказания. Эффективность MuToR продемонстрирована на различных задачах в области обработки естественного языка и компьютерного зрения.'}, 'en': {'title': 'MuToR: Enhancing Multi-Token Prediction for Language Models', 'desc': "This paper introduces MuToR, a novel approach to multi-token prediction that enhances language model pretraining. MuToR integrates learnable register tokens into the input sequence, allowing the model to predict multiple future tokens effectively. It maintains compatibility with existing pretrained models without requiring architectural changes and adds minimal parameters. The authors demonstrate MuToR's effectiveness across various tasks in both language and vision, showcasing its versatility in supervised and parameter-efficient fine-tuning."}, 'zh': {'title': 'MuToR：高效的多标记预测方法', 'desc': '多标记预测是一种有前景的目标，用于改善语言模型的预训练，但其优势在其他设置（如微调）中并不总是有效。本文提出了一种简单有效的多标记预测方法MuToR，它将可学习的注册标记交错到输入序列中，每个标记负责预测未来的目标。与现有方法相比，MuToR具有几个关键优势：仅引入极少的额外参数，不需要架构更改，确保与现成的预训练语言模型兼容，并且与下一个标记的预训练目标保持一致，特别适合监督微调。此外，它自然支持可扩展的预测范围。'}}}, {'id': 'https://huggingface.co/papers/2505.10962', 'title': 'MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective\n  Search and Data Curation', 'url': 'https://huggingface.co/papers/2505.10962', 'abstract': 'Automated Theorem Proving (ATP) in formal languages remains a formidable challenge in AI, demanding rigorous logical deduction and navigating vast search spaces. While large language models (LLMs) have shown promising performance, existing stepwise provers often suffer from biased search guidance, leading to inefficiencies and suboptimal proof strategies. This paper introduces the Multi-Perspective Search Prover (MPS-Prover), a novel stepwise ATP system designed to overcome these limitations. MPS-Prover incorporates two key innovations: a highly effective post-training data curation strategy that prunes approximately 40% of redundant training data without sacrificing performance, and a multi-perspective tree search mechanism. This search integrates a learned critic model with strategically designed heuristic rules to diversify tactic selection, prevent getting trapped in unproductive states, and enhance search robustness. Extensive evaluations demonstrate that MPS-Prover achieves state-of-the-art performance on multiple challenging benchmarks, including miniF2F and ProofNet, outperforming prior 7B parameter models. Furthermore, our analyses reveal that MPS-Prover generates significantly shorter and more diverse proofs compared to existing stepwise and whole-proof methods, highlighting its efficiency and efficacy. Our work advances the capabilities of LLM-based formal reasoning and offers a robust framework and a comprehensive analysis for developing more powerful theorem provers.', 'score': 7, 'issue_id': 3825, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': '07990204af30ff71', 'authors': ['Zhenwen Liang', 'Linfeng Song', 'Yang Li', 'Tao Yang', 'Feng Zhang', 'Haitao Mi', 'Dong Yu'], 'affiliations': ['Tencent AI Lab', 'Tencent LLM Department'], 'pdf_title_img': 'assets/pdf/title_img/2505.10962.jpg', 'data': {'categories': ['#architecture', '#benchmark', '#reasoning', '#data', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Многоперспективный поиск для прорыва в автоматическом доказательстве теорем', 'desc': 'Эта статья представляет инновационную систему автоматического доказательства теорем под названием MPS-Prover. Система использует две ключевые инновации: эффективную стратегию отбора данных после обучения и механизм поиска с множественными перспективами. MPS-Prover достигает наилучших результатов на нескольких сложных эталонных тестах, превосходя предыдущие модели с 7 миллиардами параметров. Анализ показывает, что MPS-Prover генерирует значительно более короткие и разнообразные доказательства по сравнению с существующими методами.'}, 'en': {'title': 'Revolutionizing Theorem Proving with Multi-Perspective Search', 'desc': 'This paper presents the Multi-Perspective Search Prover (MPS-Prover), a new system for Automated Theorem Proving (ATP) that addresses inefficiencies in existing stepwise provers. MPS-Prover utilizes a post-training data curation strategy to eliminate redundant training data, improving performance without loss of quality. It also features a multi-perspective tree search that combines a learned critic model with heuristic rules to enhance search diversity and prevent unproductive paths. The results show that MPS-Prover not only achieves state-of-the-art performance on various benchmarks but also produces shorter and more diverse proofs than previous models.'}, 'zh': {'title': '多视角搜索，提升定理证明效率', 'desc': '自动定理证明（ATP）在形式语言中仍然是人工智能中的一大挑战，需要严格的逻辑推理和广泛的搜索空间。虽然大型语言模型（LLMs）表现出良好的性能，但现有的逐步证明器常常受到偏见搜索指导的影响，导致效率低下和次优的证明策略。本文介绍了一种新颖的逐步ATP系统——多视角搜索证明器（MPS-Prover），旨在克服这些局限性。MPS-Prover结合了高效的后训练数据整理策略和多视角树搜索机制，显著提高了证明的效率和多样性。'}}}, {'id': 'https://huggingface.co/papers/2505.11140', 'title': 'Scaling Reasoning can Improve Factuality in Large Language Models', 'url': 'https://huggingface.co/papers/2505.11140', 'abstract': 'Recent studies on large language model (LLM) reasoning capabilities have demonstrated promising improvements in model performance by leveraging a lengthy thinking process and additional computational resources during inference, primarily in tasks involving mathematical reasoning (Muennighoff et al., 2025). However, it remains uncertain if longer reasoning chains inherently enhance factual accuracy, particularly beyond mathematical contexts. In this work, we thoroughly examine LLM reasoning within complex open-domain question-answering (QA) scenarios. We initially distill reasoning traces from advanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then fine-tune a variety of models ranging from smaller, instruction-tuned variants to larger architectures based on Qwen2.5. To enrich reasoning traces, we introduce factual information from knowledge graphs in the form of paths into our reasoning traces. Our experimental setup includes four baseline approaches and six different instruction-tuned models evaluated across a benchmark of six datasets, encompassing over 22.6K questions. Overall, we carry out 168 experimental runs and analyze approximately 1.7 million reasoning traces. Our findings indicate that, within a single run, smaller reasoning models achieve noticeable improvements in factual accuracy compared to their original instruction-tuned counterparts. Moreover, our analysis demonstrates that adding test-time compute and token budgets factual accuracy consistently improves by 2-8%, further confirming the effectiveness of test-time scaling for enhancing performance and consequently improving reasoning accuracy in open-domain QA tasks. We release all the experimental artifacts for further research.', 'score': 5, 'issue_id': 3827, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': '29d6b0a8040db2ff', 'authors': ['Mike Zhang', 'Johannes Bjerva', 'Russa Biswas'], 'affiliations': ['Department of Computer Science Aalborg University Copenhagen, Denmark'], 'pdf_title_img': 'assets/pdf/title_img/2505.11140.jpg', 'data': {'categories': ['#reasoning', '#graphs', '#dataset', '#benchmark', '#inference', '#training'], 'emoji': '🧠', 'ru': {'title': 'Длительные рассуждения улучшают точность ответов языковых моделей', 'desc': 'Исследование посвящено изучению влияния длительного процесса рассуждений на точность ответов больших языковых моделей (LLM) в задачах открытого вопросно-ответного поиска. Авторы провели эксперименты с различными моделями, обогащая цепочки рассуждений фактической информацией из графов знаний. Результаты показывают, что меньшие модели рассуждений достигают заметных улучшений в фактической точности по сравнению с исходными инструктированными аналогами. Добавление вычислительных ресурсов и увеличение лимита токенов во время тестирования последовательно улучшает фактическую точность на 2-8%.'}, 'en': {'title': 'Enhancing LLM Reasoning with Knowledge and Compute', 'desc': 'This paper investigates the reasoning capabilities of large language models (LLMs) in open-domain question-answering tasks. It analyzes how longer reasoning processes and additional computational resources can impact factual accuracy, especially beyond mathematical reasoning. The authors fine-tune various models and incorporate knowledge graph information to enhance reasoning traces. Their experiments reveal that smaller models can achieve better factual accuracy than larger, instruction-tuned models, and that increasing computational resources during inference can further improve performance.'}, 'zh': {'title': '提升推理准确性的关键在于模型与资源的结合', 'desc': '本研究探讨了大型语言模型（LLM）在复杂开放领域问答（QA）场景中的推理能力。我们从先进的推理模型中提取推理轨迹，并对多种模型进行微调，以提高其推理准确性。通过引入知识图谱中的事实信息，我们丰富了推理轨迹，并在多个数据集上进行了广泛的实验。结果表明，较小的推理模型在事实准确性上有显著提升，而在测试时增加计算资源和令牌预算也能进一步提高准确性。'}}}, {'id': 'https://huggingface.co/papers/2505.11011', 'title': 'Humans expect rationality and cooperation from LLM opponents in\n  strategic games', 'url': 'https://huggingface.co/papers/2505.11011', 'abstract': "As Large Language Models (LLMs) integrate into our social and economic interactions, we need to deepen our understanding of how humans respond to LLMs opponents in strategic settings. We present the results of the first controlled monetarily-incentivised laboratory experiment looking at differences in human behaviour in a multi-player p-beauty contest against other humans and LLMs. We use a within-subject design in order to compare behaviour at the individual level. We show that, in this environment, human subjects choose significantly lower numbers when playing against LLMs than humans, which is mainly driven by the increased prevalence of `zero' Nash-equilibrium choices. This shift is mainly driven by subjects with high strategic reasoning ability. Subjects who play the zero Nash-equilibrium choice motivate their strategy by appealing to perceived LLM's reasoning ability and, unexpectedly, propensity towards cooperation. Our findings provide foundational insights into the multi-player human-LLM interaction in simultaneous choice games, uncover heterogeneities in both subjects' behaviour and beliefs about LLM's play when playing against them, and suggest important implications for mechanism design in mixed human-LLM systems.", 'score': 4, 'issue_id': 3828, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': '0f12554b6b3b2b83', 'authors': ['Darija Barak', 'Miguel Costa-Gomes'], 'affiliations': ['School of Economics University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2505.11011.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#games'], 'emoji': '🤖', 'ru': {'title': 'Люди vs ИИ: новые стратегии в играх с искусственным интеллектом', 'desc': "Это исследование изучает поведение людей в стратегической игре p-beauty contest против других людей и больших языковых моделей (LLM). Эксперимент показал, что участники выбирают значительно меньшие числа при игре против LLM, чем против людей, что объясняется более частым выбором равновесия Нэша 'ноль'. Такое поведение в основном наблюдается у участников с высокими способностями к стратегическому мышлению. Результаты дают важные insights о взаимодействии человека и LLM в играх с одновременным выбором и имеют значение для проектирования механизмов в смешанных человеко-LLM системах."}, 'en': {'title': 'Understanding Human Behavior in Games Against LLMs', 'desc': "This paper investigates how humans behave when competing against Large Language Models (LLMs) in strategic games, specifically in a multi-player p-beauty contest. The study reveals that participants tend to choose lower numbers when playing against LLMs compared to human opponents, influenced by the perception of LLMs' reasoning capabilities. The results indicate that individuals with strong strategic reasoning are more likely to adopt a 'zero' Nash-equilibrium strategy, believing it aligns with the LLM's cooperative tendencies. These findings highlight the complexities of human-LLM interactions and their implications for designing effective systems that integrate both human and LLM participants."}, 'zh': {'title': '人类与大型语言模型的战略互动新视角', 'desc': '本研究探讨了人类在与大型语言模型（LLMs）进行战略互动时的行为差异。通过一个受控的实验，我们发现人类在与LLMs对战时选择的数字显著低于与其他人类对战时的选择。这种现象主要是由于高战略推理能力的参与者更倾向于选择零纳什均衡策略。我们的发现为人类与LLMs在多玩家同时选择游戏中的互动提供了基础性见解，并揭示了参与者行为和对LLMs游戏方式的信念差异。'}}}, {'id': 'https://huggingface.co/papers/2505.10852', 'title': 'MatTools: Benchmarking Large Language Models for Materials Science Tools', 'url': 'https://huggingface.co/papers/2505.10852', 'abstract': 'Large language models (LLMs) are increasingly applied to materials science questions, including literature comprehension, property prediction, materials discovery and alloy design. At the same time, a wide range of physics-based computational approaches have been developed in which materials properties can be calculated. Here, we propose a benchmark application to evaluate the proficiency of LLMs to answer materials science questions through the generation and safe execution of codes based on such physics-based computational materials science packages. MatTools is built on two complementary components: a materials simulation tool question-answer (QA) benchmark and a real-world tool-usage benchmark. We designed an automated methodology to efficiently collect real-world materials science tool-use examples. The QA benchmark, derived from the pymatgen (Python Materials Genomics) codebase and documentation, comprises 69,225 QA pairs that assess the ability of an LLM to understand materials science tools. The real-world benchmark contains 49 tasks (138 subtasks) requiring the generation of functional Python code for materials property calculations. Our evaluation of diverse LLMs yields three key insights: (1)Generalists outshine specialists;(2)AI knows AI; and (3)Simpler is better. MatTools provides a standardized framework for assessing and improving LLM capabilities for materials science tool applications, facilitating the development of more effective AI systems for materials science and general scientific research.', 'score': 4, 'issue_id': 3831, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': 'a29f8a9973b15514', 'authors': ['Siyu Liu', 'Jiamin Xu', 'Beilin Ye', 'Bo Hu', 'David J. Srolovitz', 'Tongqi Wen'], 'affiliations': ['Center for Structural Materials, Department of Mechanical Engineering, The University of Hong Kong, Hong Kong SAR, China', 'Materials Innovation Institute for Life Sciences and Energy (MILES), HKU-SIRI, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.10852.jpg', 'data': {'categories': ['#benchmark', '#data', '#dataset', '#science'], 'emoji': '🧪', 'ru': {'title': 'MatTools: бенчмарк для оценки LLM в материаловедении', 'desc': 'Статья представляет MatTools - бенчмарк для оценки способности больших языковых моделей (LLM) отвечать на вопросы в области материаловедения путем генерации и безопасного выполнения кода на основе пакетов вычислительного материаловедения. Бенчмарк состоит из двух компонентов: набора вопросов-ответов по инструментам моделирования материалов и набора реальных задач по использованию этих инструментов. Авторы разработали автоматизированную методологию для сбора примеров реального использования инструментов материаловедения. Оценка различных LLM с помощью MatTools выявила три ключевых вывода: универсальные модели превосходят специализированные, ИИ хорошо разбирается в ИИ, и более простые решения лучше работают.'}, 'en': {'title': 'Evaluating LLMs in Materials Science: Generalists Win!', 'desc': 'This paper introduces MatTools, a benchmark designed to evaluate how well large language models (LLMs) can handle materials science tasks. It combines a question-answer (QA) benchmark with a real-world tool-usage benchmark, assessing LLMs on their ability to generate and execute code for materials property calculations. The QA benchmark includes over 69,000 question-answer pairs derived from existing materials science resources, while the real-world benchmark consists of 49 tasks that require functional Python code generation. The findings suggest that generalist models perform better than specialized ones, that AI can effectively leverage other AI tools, and that simpler approaches yield better results.'}, 'zh': {'title': '评估大型语言模型在材料科学中的应用能力', 'desc': '大型语言模型（LLMs）在材料科学领域的应用越来越广泛，包括文献理解、属性预测、材料发现和合金设计。我们提出了一种基准应用，评估LLMs在材料科学问题上的能力，特别是通过生成和安全执行基于物理的计算材料科学软件包的代码。MatTools由两个互补组件构成：材料模拟工具问答基准和真实工具使用基准。我们的评估结果显示，通用模型优于专业模型，AI能够理解其他AI的能力，并且简单的方法更有效。'}}}, {'id': 'https://huggingface.co/papers/2505.11152', 'title': 'Learning Dense Hand Contact Estimation from Imbalanced Data', 'url': 'https://huggingface.co/papers/2505.11152', 'abstract': 'Hands are essential to human interaction, and understanding contact between hands and the world can promote comprehensive understanding of their function. Recently, there have been growing number of hand interaction datasets that cover interaction with object, other hand, scene, and body. Despite the significance of the task and increasing high-quality data, how to effectively learn dense hand contact estimation remains largely underexplored. There are two major challenges for learning dense hand contact estimation. First, there exists class imbalance issue from hand contact datasets where majority of samples are not in contact. Second, hand contact datasets contain spatial imbalance issue with most of hand contact exhibited in finger tips, resulting in challenges for generalization towards contacts in other hand regions. To tackle these issues, we present a framework that learns dense HAnd COntact estimation (HACO) from imbalanced data. To resolve the class imbalance issue, we introduce balanced contact sampling, which builds and samples from multiple sampling groups that fairly represent diverse contact statistics for both contact and non-contact samples. Moreover, to address the spatial imbalance issue, we propose vertex-level class-balanced (VCB) loss, which incorporates spatially varying contact distribution by separately reweighting loss contribution of each vertex based on its contact frequency across dataset. As a result, we effectively learn to predict dense hand contact estimation with large-scale hand contact data without suffering from class and spatial imbalance issue. The codes will be released.', 'score': 2, 'issue_id': 3822, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': 'caa702fa71c24606', 'authors': ['Daniel Sungho Jung', 'Kyoung Mu Lee'], 'affiliations': ['IPAI, Dept. of ECE & ASRI, Seoul National University, Korea'], 'pdf_title_img': 'assets/pdf/title_img/2505.11152.jpg', 'data': {'categories': ['#training', '#dataset', '#data'], 'emoji': '🖐️', 'ru': {'title': 'Точная оценка контактов рук: преодоление дисбаланса данных', 'desc': 'Эта статья представляет новый подход к оценке плотного контакта рук с окружающей средой, что важно для понимания взаимодействия человека с миром. Авторы разработали фреймворк HACO для обучения на несбалансированных данных, решая проблемы классового и пространственного дисбаланса в наборах данных о контактах рук. Они предложили метод сбалансированной выборки контактов и функцию потерь VCB, учитывающую пространственное распределение контактов на поверхности руки. Результаты показывают эффективность предложенного подхода для точного предсказания плотных контактов рук на основе крупномасштабных данных.'}, 'en': {'title': 'Enhancing Hand Contact Estimation with Balanced Learning Techniques', 'desc': 'This paper addresses the challenge of estimating dense hand contact in various interactions, which is crucial for understanding hand functionality. It identifies two main issues: class imbalance, where most samples do not involve contact, and spatial imbalance, where contact is primarily at the fingertips. To overcome these challenges, the authors propose a framework called HACO that utilizes balanced contact sampling to ensure diverse representation of contact data. Additionally, they introduce a vertex-level class-balanced loss to adjust the learning process based on the frequency of contact across different hand regions, leading to improved predictions in dense hand contact estimation.'}, 'zh': {'title': '提升手部接触估计的准确性', 'desc': '这篇论文探讨了手部接触估计的重要性，尤其是在与物体、其他手、场景和身体的互动中。尽管已有大量高质量的数据集，但如何有效学习密集的手部接触估计仍然是一个未被充分研究的问题。论文提出了一种新的框架，称为HACO，旨在解决类不平衡和空间不平衡的问题。通过引入平衡接触采样和顶点级类平衡损失，研究者们成功地提高了手部接触估计的准确性。'}}}, {'id': 'https://huggingface.co/papers/2505.10496', 'title': 'CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of\n  Synthetic Chest Radiographs', 'url': 'https://huggingface.co/papers/2505.10496', 'abstract': 'We introduce CheXGenBench, a rigorous and multifaceted evaluation framework for synthetic chest radiograph generation that simultaneously assesses fidelity, privacy risks, and clinical utility across state-of-the-art text-to-image generative models. Despite rapid advancements in generative AI for real-world imagery, medical domain evaluations have been hindered by methodological inconsistencies, outdated architectural comparisons, and disconnected assessment criteria that rarely address the practical clinical value of synthetic samples. CheXGenBench overcomes these limitations through standardised data partitioning and a unified evaluation protocol comprising over 20 quantitative metrics that systematically analyse generation quality, potential privacy vulnerabilities, and downstream clinical applicability across 11 leading text-to-image architectures. Our results reveal critical inefficiencies in the existing evaluation protocols, particularly in assessing generative fidelity, leading to inconsistent and uninformative comparisons. Our framework establishes a standardised benchmark for the medical AI community, enabling objective and reproducible comparisons while facilitating seamless integration of both existing and future generative models. Additionally, we release a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K radiographs generated by the top-performing model (Sana 0.6B) in our benchmark to support further research in this critical domain. Through CheXGenBench, we establish a new state-of-the-art and release our framework, models, and SynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/', 'score': 2, 'issue_id': 3833, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': 'ef303e066da24351', 'authors': ['Raman Dutt', 'Pedro Sanchez', 'Yongchen Yao', 'Steven McDonagh', 'Sotirios A. Tsaftaris', 'Timothy Hospedales'], 'affiliations': ['Samsung AI Center, Cambridge', 'Sinkove', 'The University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2505.10496.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#cv', '#open_source', '#benchmark'], 'emoji': '\U0001fa7b', 'ru': {'title': 'Комплексная оценка генерации медицинских изображений: качество, безопасность, применимость', 'desc': 'CheXGenBench представляет собой комплексную систему оценки генерации синтетических рентгеновских снимков грудной клетки. Фреймворк оценивает точность, риски конфиденциальности и клиническую полезность современных генеративных моделей текст-в-изображение. CheXGenBench включает более 20 количественных метрик для анализа качества генерации, потенциальных уязвимостей приватности и клинического применения 11 ведущих архитектур. Авторы также выпустили высококачественный синтетический датасет SynthCheX-75K из 75 000 рентгенограмм, сгенерированных лучшей моделью в их бенчмарке.'}, 'en': {'title': 'Setting New Standards for Synthetic Chest Radiograph Evaluation', 'desc': 'CheXGenBench is a comprehensive evaluation framework designed to assess the generation of synthetic chest radiographs using advanced text-to-image models. It addresses previous challenges in medical image evaluation by providing standardized metrics that measure fidelity, privacy risks, and clinical utility. The framework includes over 20 quantitative metrics and evaluates 11 leading generative architectures, revealing inefficiencies in current evaluation methods. Additionally, it introduces a high-quality synthetic dataset, SynthCheX-75K, to support ongoing research in medical AI.'}, 'zh': {'title': '建立医学AI的新标准评估框架', 'desc': 'CheXGenBench是一个全面的评估框架，用于合成胸部X光图像的生成，评估生成的真实性、隐私风险和临床实用性。该框架解决了以往医学领域评估中的方法不一致、架构比较过时和评估标准脱节等问题。通过标准化的数据划分和统一的评估协议，CheXGenBench使用超过20个定量指标系统分析生成质量和潜在隐私漏洞。我们还发布了一个高质量的合成数据集SynthCheX-75K，包含75,000张由最佳模型生成的X光图像，以支持该领域的进一步研究。'}}}, {'id': 'https://huggingface.co/papers/2505.09924', 'title': 'From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework\n  for Large Language Models', 'url': 'https://huggingface.co/papers/2505.09924', 'abstract': 'The rise of Large Language Models (LLMs) has heightened concerns about the misuse of AI-generated text, making watermarking a promising solution. Mainstream watermarking schemes for LLMs fall into two categories: logits-based and sampling-based. However, current schemes entail trade-offs among robustness, text quality, and security. To mitigate this, we integrate logits-based and sampling-based schemes, harnessing their respective strengths to achieve synergy. In this paper, we propose a versatile symbiotic watermarking framework with three strategies: serial, parallel, and hybrid. The hybrid framework adaptively embeds watermarks using token entropy and semantic entropy, optimizing the balance between detectability, robustness, text quality, and security. Furthermore, we validate our approach through comprehensive experiments on various datasets and models. Experimental results indicate that our method outperforms existing baselines and achieves state-of-the-art (SOTA) performance. We believe this framework provides novel insights into diverse watermarking paradigms. Our code is available at https://github.com/redwyd/SymMark{https://github.com/redwyd/SymMark}.', 'score': 2, 'issue_id': 3833, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': '17456d03961632a9', 'authors': ['Yidan Wang', 'Yubing Ren', 'Yanan Cao', 'Binxing Fang'], 'affiliations': ['Hainan Province Fang Binxing Academician Workstation, Hainan, China', 'Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China', 'School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.09924.jpg', 'data': {'categories': ['#data', '#optimization', '#dataset', '#security', '#open_source', '#benchmark', '#architecture'], 'emoji': '🔐', 'ru': {'title': 'Симбиотическая защита: новый подход к водяным знакам в текстах ИИ', 'desc': 'Статья посвящена разработке новой системы встраивания водяных знаков в тексты, генерируемые большими языковыми моделями (LLM). Авторы предлагают гибридный подход, объединяющий методы на основе логитов и семплирования, что позволяет достичь лучшего баланса между обнаруживаемостью, устойчивостью, качеством текста и безопасностью. Предложенная система адаптивно встраивает водяные знаки, используя энтропию токенов и семантическую энтропию. Экспериментальные результаты показывают, что метод превосходит существующие базовые линии и достигает наилучших показателей в своей области.'}, 'en': {'title': 'Synergizing Watermarking Techniques for Robust AI Text Security', 'desc': 'This paper addresses the challenges of watermarking Large Language Models (LLMs) to prevent misuse of AI-generated text. It introduces a new framework that combines logits-based and sampling-based watermarking techniques to enhance robustness and text quality while maintaining security. The proposed hybrid approach uses token and semantic entropy to adaptively embed watermarks, optimizing the balance between detectability and performance. Experimental results demonstrate that this method surpasses existing watermarking techniques, achieving state-of-the-art results across various datasets and models.'}, 'zh': {'title': '共生水印框架：优化AI文本安全性与质量的创新方案', 'desc': '随着大型语言模型（LLMs）的兴起，关于AI生成文本滥用的担忧加剧，因此水印技术成为一种有前景的解决方案。现有的水印方案主要分为基于logits和基于采样的两类，但这些方案在鲁棒性、文本质量和安全性之间存在权衡。为了解决这个问题，我们提出了一种多功能的共生水印框架，结合了这两种方案的优点，采用串行、并行和混合三种策略。实验结果表明，我们的方法在多个数据集和模型上超越了现有基准，达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.11480', 'title': 'Improving Assembly Code Performance with Large Language Models via\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.11480', 'abstract': 'Large language models (LLMs) have demonstrated strong performance across a wide range of programming tasks, yet their potential for code optimization remains underexplored. This work investigates whether LLMs can optimize the performance of assembly code, where fine-grained control over execution enables improvements that are difficult to express in high-level languages. We present a reinforcement learning framework that trains LLMs using Proximal Policy Optimization (PPO), guided by a reward function that considers both functional correctness, validated through test cases, and execution performance relative to the industry-standard compiler gcc -O3. To support this study, we introduce a benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO, achieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3 baseline, outperforming all 20 other models evaluated, including Claude-3.7-sonnet. These results indicate that reinforcement learning can unlock the potential of LLMs to serve as effective optimizers for assembly code performance.', 'score': 1, 'issue_id': 3837, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': '553b664dc4913a2e', 'authors': ['Anjiang Wei', 'Tarun Suresh', 'Huanmi Tan', 'Yinglun Xu', 'Gagandeep Singh', 'Ke Wang', 'Alex Aiken'], 'affiliations': ['Carnegie Mellon University', 'Stanford University', 'University of Illinois Urbana-Champaign', 'Visa Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.11480.jpg', 'data': {'categories': ['#rl', '#plp', '#training', '#benchmark', '#rlhf', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'LLM превосходят gcc в оптимизации ассемблера', 'desc': 'Исследование посвящено оптимизации ассемблерного кода с помощью больших языковых моделей (LLM). Авторы разработали систему обучения с подкреплением, использующую алгоритм Proximal Policy Optimization (PPO) для тренировки LLM. Модель оценивается по функциональной корректности и производительности относительно gcc -O3. Результаты показывают, что обученная модель Qwen2.5-Coder-7B-PPO достигает 96% прохождения тестов и среднего ускорения в 1.47 раза по сравнению с базовой линией gcc -O3.'}, 'en': {'title': 'Unlocking LLMs for Assembly Code Optimization', 'desc': 'This paper explores the ability of large language models (LLMs) to optimize assembly code, which allows for precise control over execution. The authors develop a reinforcement learning framework using Proximal Policy Optimization (PPO) to train LLMs, focusing on both correctness and performance improvements. They introduce a benchmark of 8,072 real-world programs to evaluate their model, Qwen2.5-Coder-7B-PPO, which achieves a high test pass rate and significant speedup compared to the standard gcc -O3 compiler. The findings suggest that LLMs, when trained with reinforcement learning, can effectively enhance the performance of assembly code.'}, 'zh': {'title': '强化学习助力大型语言模型优化汇编代码', 'desc': '大型语言模型（LLMs）在编程任务中表现出色，但在代码优化方面的潜力尚未被充分探索。本文研究了LLMs是否能够优化汇编代码的性能，因为汇编语言提供了对执行的细粒度控制。我们提出了一种强化学习框架，使用近端策略优化（PPO）训练LLMs，并通过考虑功能正确性和执行性能的奖励函数进行指导。我们的模型Qwen2.5-Coder-7B-PPO在测试中达到了96.0%的通过率，并且在速度上比行业标准编译器gcc -O3快了1.47倍，显示出强化学习可以有效提升LLMs在汇编代码优化中的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2505.11493', 'title': 'GIE-Bench: Towards Grounded Evaluation for Text-Guided Image Editing', 'url': 'https://huggingface.co/papers/2505.11493', 'abstract': 'Editing images using natural language instructions has become a natural and expressive way to modify visual content; yet, evaluating the performance of such models remains challenging. Existing evaluation approaches often rely on image-text similarity metrics like CLIP, which lack precision. In this work, we introduce a new benchmark designed to evaluate text-guided image editing models in a more grounded manner, along two critical dimensions: (i) functional correctness, assessed via automatically generated multiple-choice questions that verify whether the intended change was successfully applied; and (ii) image content preservation, which ensures that non-targeted regions of the image remain visually consistent using an object-aware masking technique and preservation scoring. The benchmark includes over 1000 high-quality editing examples across 20 diverse content categories, each annotated with detailed editing instructions, evaluation questions, and spatial object masks. We conduct a large-scale study comparing GPT-Image-1, the latest flagship in the text-guided image editing space, against several state-of-the-art editing models, and validate our automatic metrics against human ratings. Results show that GPT-Image-1 leads in instruction-following accuracy, but often over-modifies irrelevant image regions, highlighting a key trade-off in the current model behavior. GIE-Bench provides a scalable, reproducible framework for advancing more accurate evaluation of text-guided image editing.', 'score': 0, 'issue_id': 3838, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': 'd5f4f14c60b051ec', 'authors': ['Yusu Qian', 'Jiasen Lu', 'Tsu-Jui Fu', 'Xinze Wang', 'Chen Chen', 'Yinfei Yang', 'Wenze Hu', 'Zhe Gan'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets/pdf/title_img/2505.11493.jpg', 'data': {'categories': ['#benchmark', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'GIE-Bench: Точная оценка редактирования изображений по текстовым инструкциям', 'desc': 'Статья представляет новый бенчмарк для оценки моделей редактирования изображений с помощью текстовых инструкций. Бенчмарк оценивает функциональную корректность изменений и сохранение неизменяемых частей изображения. Он включает более 1000 примеров редактирования с аннотациями и масками объектов. Исследование показало, что модель GPT-Image-1 лидирует в точности следования инструкциям, но часто изменяет нерелевантные области изображения.'}, 'en': {'title': 'Enhancing Evaluation of Text-Guided Image Editing with GIE-Bench', 'desc': 'This paper addresses the challenges in evaluating models that edit images based on natural language instructions. It introduces a new benchmark called GIE-Bench, which assesses models on functional correctness and image content preservation. The benchmark includes a large dataset of editing examples with detailed instructions and evaluation metrics. The study compares the performance of GPT-Image-1 with other models, revealing strengths in instruction-following but weaknesses in preserving non-targeted image areas.'}, 'zh': {'title': '提升文本引导图像编辑的评估准确性', 'desc': '本研究提出了一种新的基准，旨在更准确地评估文本引导的图像编辑模型。我们通过自动生成的多项选择题来评估功能正确性，并使用对象感知掩膜技术确保图像内容的保留。基准包含超过1000个高质量的编辑示例，涵盖20个不同的内容类别，并附有详细的编辑指令和评估问题。我们的研究表明，尽管GPT-Image-1在遵循指令的准确性上表现优异，但在处理无关图像区域时常常过度修改，突显了当前模型行为中的一个关键权衡。'}}}, {'id': 'https://huggingface.co/papers/2505.11315', 'title': 'Improving Inference-Time Optimisation for Vocal Effects Style Transfer\n  with a Gaussian Prior', 'url': 'https://huggingface.co/papers/2505.11315', 'abstract': "Style Transfer with Inference-Time Optimisation (ST-ITO) is a recent approach for transferring the applied effects of a reference audio to a raw audio track. It optimises the effect parameters to minimise the distance between the style embeddings of the processed audio and the reference. However, this method treats all possible configurations equally and relies solely on the embedding space, which can lead to unrealistic or biased results. We address this pitfall by introducing a Gaussian prior derived from a vocal preset dataset, DiffVox, over the parameter space. The resulting optimisation is equivalent to maximum-a-posteriori estimation. Evaluations on vocal effects transfer on the MedleyDB dataset show significant improvements across metrics compared to baselines, including a blind audio effects estimator, nearest-neighbour approaches, and uncalibrated ST-ITO. The proposed calibration reduces parameter mean squared error by up to 33% and matches the reference style better. Subjective evaluations with 16 participants confirm our method's superiority, especially in limited data regimes. This work demonstrates how incorporating prior knowledge in inference time enhances audio effects transfer, paving the way for more effective and realistic audio processing systems.", 'score': 0, 'issue_id': 3836, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': 'a8b0aeed9083fba7', 'authors': ['Chin-Yun Yu', 'Marco A. Martínez-Ramírez', 'Junghyun Koo', 'Wei-Hsiang Liao', 'Yuki Mitsufuji', 'György Fazekas'], 'affiliations': ['Centre for Digital Music, Queen Mary University of London, London, UK', 'Sony AI, Tokyo, Japan', 'Sony Group Corporation, Tokyo, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2505.11315.jpg', 'data': {'categories': ['#optimization', '#inference', '#audio', '#dataset'], 'emoji': '🎵', 'ru': {'title': 'Байесовская оптимизация улучшает перенос аудиоэффектов', 'desc': 'Статья представляет новый подход к переносу аудиоэффектов с использованием байесовской оптимизации. Авторы предлагают метод ST-ITO с гауссовым априорным распределением, полученным из набора данных вокальных пресетов. Эксперименты показывают значительное улучшение качества переноса эффектов по сравнению с базовыми методами. Субъективная оценка 16 участников подтверждает превосходство предложенного метода, особенно при ограниченных данных.'}, 'en': {'title': 'Enhancing Audio Effects Transfer with Prior Knowledge', 'desc': 'This paper presents a new method called Style Transfer with Inference-Time Optimisation (ST-ITO) for applying audio effects from a reference track to a raw audio track. The approach improves upon previous methods by introducing a Gaussian prior based on a vocal preset dataset, which helps to guide the optimisation of effect parameters. This results in more realistic audio effects by reducing the mean squared error of the parameters and better matching the reference style. The method shows significant improvements in performance metrics and subjective evaluations, especially when data is limited, highlighting the importance of incorporating prior knowledge in audio processing.'}, 'zh': {'title': '引入先验知识，提升音频风格转移效果', 'desc': '本文提出了一种新的音频风格转移方法，称为推理时优化（ST-ITO），旨在将参考音频的效果应用于原始音频轨道。该方法通过优化效果参数，最小化处理后音频与参考音频的风格嵌入之间的距离。然而，传统方法对所有配置的处理是平等的，可能导致不真实或有偏差的结果。我们通过引入基于声乐预设数据集DiffVox的高斯先验，改进了参数空间的优化，从而显著提高了音频效果转移的准确性和效果。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (1)', '#agi (1)', '#alignment', '#architecture (4)', '#audio (1)', '#benchmark (10)', '#cv (3)', '#data (5)', '#dataset (7)', '#diffusion', '#ethics', '#games (2)', '#graphs (1)', '#hallucinations', '#healthcare', '#inference (3)', '#interpretability', '#leakage', '#long_context (1)', '#low_resource (1)', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (5)', '#open_source (6)', '#optimization (8)', '#plp (1)', '#rag', '#reasoning (8)', '#rl (3)', '#rlhf (2)', '#robotics', '#science (1)', '#security (1)', '#small_models (1)', '#story_generation', '#survey', '#synthetic (1)', '#training (11)', '#transfer_learning (1)', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-05-19 19:09',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-05-19 19:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-05-19 19:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    