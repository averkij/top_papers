
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 28 papers. February 2.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ</span> | <span id="title-articles-count">28 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2026-01-30.html">â¬…ï¸ <span id="prev-date">30.01</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2026-02-03.html">â¡ï¸ <span id="next-date">03.02</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2026-02.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'};
        let feedDateNext = {'ru': '03.02', 'en': '02/03', 'zh': '2æœˆ3æ—¥'};
        let feedDatePrev = {'ru': '30.01', 'en': '01/30', 'zh': '1æœˆ30æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2601.21558', 'title': 'ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas', 'url': 'https://huggingface.co/papers/2601.21558', 'abstract': 'ASTRA is an automated framework that trains tool-augmented language models using synthetic data and verifiable reinforcement learning to improve multi-step decision-making capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly used as tool-augmented agents for multi-step decision making, yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either supervised fine-tuning (SFT) or reinforcement learning (RL), and struggle with stable long-horizon, multi-turn learning. To address these challenges, we introduce ASTRA, a fully automated end-to-end framework for training tool-augmented language model agents via scalable data synthesis and verifiable reinforcement learning. ASTRA integrates two complementary components. First, a pipeline that leverages the static topology of tool-call graphs synthesizes diverse, structurally grounded trajectories, instilling broad and transferable tool-use competence. Second, an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning converts decomposed question-answer traces into independent, code-executable, and rule-verifiable environments, enabling deterministic multi-turn RL. Based on this method, we develop a unified training methodology that integrates SFT with online RL using trajectory-level rewards to balance task completion and interaction efficiency. Experiments on multiple agentic tool-use benchmarks demonstrate that ASTRA-trained models achieve state-of-the-art performance at comparable scales, approaching closed-source systems while preserving core reasoning ability. We release the full pipelines, environments, and trained models at https://github.com/LianjiaTech/astra.', 'score': 34, 'issue_id': 871, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 29', 'zh': '1æœˆ29æ—¥'}, 'hash': '6bbe252a83428e3f', 'authors': ['Xiaoyu Tian', 'Haotian Wang', 'Shuaiting Chen', 'Hao Zhou', 'Kaichi Yu', 'Yudian Zhang', 'Jade Ouyang', 'Junxi Yin', 'Jiong Chen', 'Baoyan Guo', 'Lei Zhang', 'Junjie Tao', 'Yuansheng Song', 'Ming Cui', 'Chengwei Liu'], 'affiliations': ['Beike Language and Intelligence (BLI)'], 'pdf_title_img': 'assets/pdf/title_img/2601.21558.jpg', 'data': {'categories': ['#data', '#benchmark', '#open_source', '#rl', '#agents', '#optimization', '#synthetic', '#training', '#reasoning'], 'emoji': 'ğŸ› ï¸', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ', 'desc': 'ASTRA â€” ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ»ĞµĞ´Ğ¾Ğ² Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ´Ğ»Ñ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ supervised fine-tuning Ñ online RL, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ASTRA, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ state-of-the-art Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'ASTRA: Automating Tool-Augmented Learning for Better Decision-Making', 'desc': 'ASTRA is an innovative framework designed to enhance the training of tool-augmented language models for complex decision-making tasks. It automates the process by using synthetic data and verifiable reinforcement learning, addressing the limitations of existing methods that often require manual input and lack reliable environments. The framework consists of two main components: a pipeline for generating diverse tool-use trajectories and an environment synthesis framework that creates verifiable scenarios for training. As a result, ASTRA enables models to achieve superior performance in multi-step decision-making while maintaining their reasoning capabilities.'}, 'zh': {'title': 'ASTRAï¼šè‡ªåŠ¨åŒ–å·¥å…·å¢å¼ºè¯­è¨€æ¨¡å‹è®­ç»ƒçš„åˆ›æ–°æ¡†æ¶', 'desc': 'ASTRAæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åˆæˆæ•°æ®å’Œå¯éªŒè¯çš„å¼ºåŒ–å­¦ä¹ æ¥è®­ç»ƒå¢å¼ºå·¥å…·çš„è¯­è¨€æ¨¡å‹ï¼Œä»¥æé«˜å¤šæ­¥éª¤å†³ç­–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰æ–¹æ³•ä¸­éœ€è¦äººå·¥å¹²é¢„ã€ä¾èµ–ä¸å¯éªŒè¯çš„æ¨¡æ‹Ÿç¯å¢ƒä»¥åŠåœ¨é•¿æ—¶é—´ã€å¤šè½®å­¦ä¹ ä¸­ä¸ç¨³å®šçš„é—®é¢˜ã€‚ASTRAç»“åˆäº†ä¸¤å¤§ç»„ä»¶ï¼šä¸€æ˜¯åˆ©ç”¨å·¥å…·è°ƒç”¨å›¾çš„é™æ€æ‹“æ‰‘åˆæˆå¤šæ ·åŒ–çš„è½¨è¿¹ï¼ŒäºŒæ˜¯é€šè¿‡ç¯å¢ƒåˆæˆæ¡†æ¶å°†é—®é¢˜-ç­”æ¡ˆè½¨è¿¹è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„ã€è§„åˆ™å¯éªŒè¯çš„ç¯å¢ƒã€‚å®éªŒè¡¨æ˜ï¼ŒASTRAè®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªå·¥å…·ä½¿ç”¨åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ¥è¿‘å°é—­æºç³»ç»Ÿï¼ŒåŒæ—¶ä¿æŒæ ¸å¿ƒæ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.23143', 'title': 'THINKSAFE: Self-Generated Safety Alignment for Reasoning Models', 'url': 'https://huggingface.co/papers/2601.23143', 'abstract': 'ThinkSafe is a self-aligned framework that enhances safety in large reasoning models through lightweight refusal steering and fine-tuning on self-generated responses, maintaining reasoning performance while reducing computational costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation, yet this introduces a distributional discrepancy that degrades native reasoning. We propose ThinkSafe, a self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. ThinkSafe unlocks this via lightweight refusal steering, guiding the model to generate in-distribution safety reasoning traces. Fine-tuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning proficiency. Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost. Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git.', 'score': 25, 'issue_id': 871, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 30', 'zh': '1æœˆ30æ—¥'}, 'hash': '60429122f361bf32', 'authors': ['Seanie Lee', 'Sangwoo Park', 'Yumin Choi', 'Gyeongman Kim', 'Minki Kang', 'Jihun Yun', 'Dongmin Park', 'Jongho Park', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST AI', 'KRAFTON', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2601.23143.jpg', 'data': {'categories': ['#rlhf', '#alignment', '#open_source', '#rl', '#training', '#reasoning'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¹: ÑĞ°Ğ¼Ğ¾Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'ThinkSafe â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ°Ğ¼Ğ¾Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¾Ğµ Ñ€ÑƒĞ»ĞµĞ²Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ°Ğ¼Ğ¸ (refusal steering), Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ÑÑ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸĞ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ°Ğ¼Ğ¾Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ¿ĞµÑ€ĞµĞ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞµÑ‘ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼Ñƒ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'Enhancing Safety in Reasoning Models with ThinkSafe', 'desc': 'ThinkSafe is a novel framework designed to enhance the safety of large reasoning models (LRMs) while maintaining their reasoning capabilities. It employs lightweight refusal steering and fine-tuning on responses generated by the model itself, avoiding the need for external teacher distillation which can introduce discrepancies. This approach allows the model to leverage its inherent knowledge to identify harmful prompts and generate safer responses. Experiments demonstrate that ThinkSafe significantly improves safety without compromising reasoning performance and does so with lower computational costs.'}, 'zh': {'title': 'ThinkSafeï¼šæå‡æ¨ç†æ¨¡å‹å®‰å…¨æ€§çš„è‡ªæˆ‘å¯¹é½æ¡†æ¶', 'desc': 'ThinkSafeæ˜¯ä¸€ä¸ªè‡ªæˆ‘å¯¹é½æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è½»é‡çº§æ‹’ç»å¼•å¯¼å’Œè‡ªç”Ÿæˆå“åº”çš„å¾®è°ƒæ¥å¢å¼ºå¤§å‹æ¨ç†æ¨¡å‹çš„å®‰å…¨æ€§ï¼ŒåŒæ—¶ä¿æŒæ¨ç†æ€§èƒ½å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚è¯¥æ¡†æ¶è§£å†³äº†å¤§å‹æ¨ç†æ¨¡å‹åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­å¯èƒ½å¯¼è‡´çš„å®‰å…¨æ€§ä¸‹é™é—®é¢˜ï¼Œé¿å…äº†ä¾èµ–å¤–éƒ¨æ•™å¸ˆè’¸é¦æ‰€å¸¦æ¥çš„åˆ†å¸ƒå·®å¼‚ã€‚ThinkSafeåˆ©ç”¨æ¨¡å‹å†…åœ¨çš„çŸ¥è¯†æ¥è¯†åˆ«æœ‰å®³æç¤ºï¼Œé€šè¿‡å¼•å¯¼æ¨¡å‹ç”Ÿæˆå®‰å…¨æ¨ç†è½¨è¿¹æ¥æ¢å¤å®‰å…¨å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒThinkSafeåœ¨æé«˜å®‰å…¨æ€§çš„åŒæ—¶ï¼Œä¿æŒäº†æ¨ç†èƒ½åŠ›ï¼Œå¹¶æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.22628', 'title': 'TTCS: Test-Time Curriculum Synthesis for Self-Evolving', 'url': 'https://huggingface.co/papers/2601.22628', 'abstract': "TTCS is a co-evolving test-time training framework that enhances LLM reasoning abilities by iteratively generating challenging question variants and updating a reasoning solver through self-consistency rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-Time Training offers a promising way to improve the reasoning ability of large language models (LLMs) by adapting the model using only the test questions. However, existing methods struggle with difficult reasoning problems for two reasons: raw test questions are often too difficult to yield high-quality pseudo-labels, and the limited size of test sets makes continuous online updates prone to instability. To address these limitations, we propose TTCS, a co-evolving test-time training framework. Specifically, TTCS initializes two policies from the same pretrained model: a question synthesizer and a reasoning solver. These policies evolve through iterative optimization: the synthesizer generates progressively challenging question variants conditioned on the test questions, creating a structured curriculum tailored to the solver's current capability, while the solver updates itself using self-consistency rewards computed from multiple sampled responses on both original test and synthetic questions. Crucially, the solver's feedback guides the synthesizer to generate questions aligned with the model's current capability, and the generated question variants in turn stabilize the solver's test-time training. Experiments show that TTCS consistently strengthens the reasoning ability on challenging mathematical benchmarks and transfers to general-domain tasks across different LLM backbones, highlighting a scalable path towards dynamically constructing test-time curricula for self-evolving. Our code and implementation details are available at https://github.com/XMUDeepLIT/TTCS.", 'score': 20, 'issue_id': 871, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 30', 'zh': '1æœˆ30æ—¥'}, 'hash': '6bb34e440d233d8a', 'authors': ['Chengyi Yang', 'Zhishang Xiang', 'Yunbo Tang', 'Zongpei Teng', 'Chengsong Huang', 'Fei Long', 'Yuhan Liu', 'Jinsong Su'], 'affiliations': ['Renmin University of China', 'Washington University in St. Louis', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2601.22628.jpg', 'data': {'categories': ['#benchmark', '#math', '#open_source', '#optimization', '#training', '#reasoning'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ TTCS â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğº Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑƒÑ‡ĞµĞ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñƒ. Ğ ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒĞµÑ‚ÑÑ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ½Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TTCS Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑÑ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹.'}, 'en': {'title': 'Dynamic Questioning for Enhanced Reasoning in LLMs', 'desc': "TTCS is a novel framework designed to enhance the reasoning capabilities of large language models (LLMs) during test-time training. It operates by co-evolving two components: a question synthesizer that generates increasingly difficult question variants and a reasoning solver that improves its performance based on feedback from these questions. This iterative process allows the synthesizer to tailor questions to the solver's current abilities, creating a dynamic learning environment. The results demonstrate that TTCS effectively boosts reasoning skills on complex tasks and can be applied across various LLM architectures, paving the way for adaptive test-time learning."}, 'zh': {'title': 'å…±åŒè¿›åŒ–ï¼Œæå‡æ¨ç†èƒ½åŠ›çš„æµ‹è¯•æ—¶è®­ç»ƒæ¡†æ¶', 'desc': 'TTCSæ˜¯ä¸€ç§å…±åŒè¿›åŒ–çš„æµ‹è¯•æ—¶è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è¿­ä»£ç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜å˜ä½“æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªç­–ç•¥ï¼šé—®é¢˜åˆæˆå™¨å’Œæ¨ç†æ±‚è§£å™¨ï¼Œå®ƒä»¬ä»åŒä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹åˆå§‹åŒ–ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–ä¸æ–­æ¼”åŒ–ã€‚é—®é¢˜åˆæˆå™¨æ ¹æ®æµ‹è¯•é—®é¢˜ç”Ÿæˆé€æ¸å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜å˜ä½“ï¼Œè€Œæ¨ç†æ±‚è§£å™¨åˆ™é€šè¿‡è‡ªæˆ‘ä¸€è‡´æ€§å¥–åŠ±è¿›è¡Œæ›´æ–°ã€‚å®éªŒè¡¨æ˜ï¼ŒTTCSåœ¨å¤æ‚çš„æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä¸”èƒ½å¤Ÿåœ¨ä¸åŒçš„LLMåŸºç¡€ä¸Šè½¬ç§»åˆ°ä¸€èˆ¬é¢†åŸŸä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.23265', 'title': 'PaperBanana: Automating Academic Illustration for AI Scientists', 'url': 'https://huggingface.co/papers/2601.23265', 'abstract': '_paperbanana is an agentic framework that automates the creation of publication-ready academic illustrations using advanced vision-language models and image generation techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models, PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique. To rigorously evaluate our framework, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. Comprehensive experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that our method effectively extends to the generation of high-quality statistical plots. Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations.', 'score': 7, 'issue_id': 871, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 30', 'zh': '1æœˆ30æ—¥'}, 'hash': '647cf0b7118ff73f', 'authors': ['Dawei Zhu', 'Rui Meng', 'Yale Song', 'Xiyu Wei', 'Sujian Li', 'Tomas Pfister', 'Jinsung Yoon'], 'affiliations': ['Google Cloud AI Research', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2601.23265.jpg', 'data': {'categories': ['#open_source', '#science'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ»ÑÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹', 'desc': 'PaperBanana â€” ÑÑ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ğº Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ»Ğ»ÑÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑĞ¿Ñ€Ğ°Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ², Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑÑ‚Ğ¸Ğ»Ñ, Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºÑƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ PaperBananaBench â€” Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 292 Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ¸Ğ· Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹ NeurIPS 2025, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ğ¸Ğ»Ğ¸ Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ»ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ»Ğ°ĞºĞ¾Ğ½Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑÑ‚ĞµÑ‚Ğ¸ĞºĞµ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Automating Academic Illustrations with PaperBanana', 'desc': 'PaperBanana is an innovative framework designed to automate the creation of high-quality academic illustrations using advanced vision-language models (VLMs) and image generation techniques. It addresses the challenge of generating publication-ready visuals, which is often a time-consuming task for researchers. The framework utilizes specialized agents to gather references, plan the design, create images, and refine them through self-critique. Evaluation through PaperBananaBench shows that this system outperforms existing methods in terms of accuracy, clarity, and visual appeal, making it a significant advancement in the research workflow.'}, 'zh': {'title': 'PaperBananaï¼šè‡ªåŠ¨åŒ–å­¦æœ¯æ’å›¾ç”Ÿæˆçš„æœªæ¥', 'desc': 'PaperBananaæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿ç”¨å…ˆè¿›çš„è§†è§‰-è¯­è¨€æ¨¡å‹å’Œå›¾åƒç”ŸæˆæŠ€æœ¯ï¼Œè‡ªåŠ¨åˆ›å»ºç¬¦åˆå‡ºç‰ˆæ ‡å‡†çš„å­¦æœ¯æ’å›¾ã€‚è¯¥æ¡†æ¶é€šè¿‡åè°ƒä¸“é—¨çš„ä»£ç†ï¼Œå®Œæˆå‚è€ƒèµ„æ–™æ£€ç´¢ã€å†…å®¹å’Œé£æ ¼è§„åˆ’ã€å›¾åƒæ¸²æŸ“ä»¥åŠè‡ªæˆ‘æ‰¹è¯„çš„è¿­ä»£ä¼˜åŒ–ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†PaperBananaBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«292ä¸ªæµ‹è¯•æ¡ˆä¾‹çš„æ–¹æ³•è®ºå›¾è¡¨çš„åŸºå‡†ï¼Œæ¶µç›–äº†ä¸åŒç ”ç©¶é¢†åŸŸå’Œæ’å›¾é£æ ¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPaperBananaåœ¨å‡†ç¡®æ€§ã€ç®€æ´æ€§ã€å¯è¯»æ€§å’Œç¾è§‚æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰çš„ä¸»æµæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.23182', 'title': 'FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation', 'url': 'https://huggingface.co/papers/2601.23182', 'abstract': 'Frequency-domain analysis of diffusion language models reveals that low-frequency components encode global structure while high-frequency components capture local details, enabling improved generation through FourierSampler\'s dynamic frequency-domain sliding window mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the non-autoregressive potential of diffusion language models (dLLMs), existing decoding strategies demonstrate positional bias, failing to fully unlock the potential of arbitrary generation. In this work, we delve into the inherent spectral characteristics of dLLMs and present the first frequency-domain analysis showing that low-frequency components in hidden states primarily encode global structural information and long-range dependencies, while high-frequency components are responsible for characterizing local details. Based on this observation, we propose FourierSampler, which leverages a frequency-domain sliding window mechanism to dynamically guide the model to achieve a "structure-to-detail" generation. FourierSampler outperforms other inference enhancement strategies on LLADA and SDAR, achieving relative improvements of 20.4% on LLaDA1.5-8B and 16.0% on LLaDA-8B-Instruct. It notably surpasses similarly sized autoregressive models like Llama3.1-8B-Instruct.', 'score': 7, 'issue_id': 873, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 30', 'zh': '1æœˆ30æ—¥'}, 'hash': '7ed8f5f3b04d91e1', 'authors': ['Siyang He', 'Qiqi Wang', 'Xiaoran Liu', 'Hongnan Ma', 'Yiwei Shi', 'Yuerong Song', 'Ying Zhu', 'Tianyi Liang', 'Zengfeng Huang', 'Ziwei He', 'Xipeng Qiu'], 'affiliations': ['Fudan University', 'OpenMOSS Team', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2601.23182.jpg', 'data': {'categories': ['#optimization', '#diffusion'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'ĞÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğº Ğ´ĞµÑ‚Ğ°Ğ»ÑĞ¼: Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´Ñ‘Ğ½ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ‡Ñ‚Ğ¾ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸, Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ÑÑ‚ Ğ·Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ FourierSampler - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ² Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ğ¾Ñ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğº Ğ´ĞµÑ‚Ğ°Ğ»ÑĞ¼. ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… LLADA Ğ¸ SDAR Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¾Ğ¼ 20.4% Ğ¸ 16.0% ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾. FourierSampler Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Llama3.1-8B-Instruct.'}, 'en': {'title': 'Unlocking Structure and Detail in Language Generation', 'desc': 'This paper explores how diffusion language models (dLLMs) can be improved by analyzing their frequency components. It finds that low-frequency components capture the overall structure and long-range relationships in the data, while high-frequency components focus on finer, local details. The authors introduce a new method called FourierSampler, which uses a sliding window in the frequency domain to enhance the generation process by balancing structure and detail. This approach shows significant performance improvements over existing methods and even outperforms some autoregressive models.'}, 'zh': {'title': 'é¢‘åŸŸåˆ†æåŠ©åŠ›ç”Ÿæˆæ¨¡å‹çš„ç»“æ„ä¸ç»†èŠ‚ä¼˜åŒ–', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰çš„é¢‘åŸŸç‰¹æ€§ï¼Œæ­ç¤ºäº†ä½é¢‘æˆåˆ†ä¸»è¦ç¼–ç å…¨å±€ç»“æ„ä¿¡æ¯ï¼Œè€Œé«˜é¢‘æˆåˆ†åˆ™æ•æ‰å±€éƒ¨ç»†èŠ‚ã€‚é€šè¿‡è¿™ç§åˆ†æï¼Œä½œè€…æå‡ºäº†FourierSamplerï¼Œå®ƒåˆ©ç”¨é¢‘åŸŸæ»‘åŠ¨çª—å£æœºåˆ¶åŠ¨æ€å¼•å¯¼æ¨¡å‹å®ç°ä»ç»“æ„åˆ°ç»†èŠ‚çš„ç”Ÿæˆã€‚FourierSampleråœ¨LLADAå’ŒSDARä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸æ¯”å…¶ä»–æ¨ç†å¢å¼ºç­–ç•¥ï¼ŒLLaDA1.5-8Bå’ŒLLaDA-8B-Instructåˆ†åˆ«æé«˜äº†20.4%å’Œ16.0%ã€‚æ­¤å¤–ï¼Œå®ƒçš„æ€§èƒ½æ˜¾è‘—è¶…è¿‡äº†åŒç­‰è§„æ¨¡çš„è‡ªå›å½’æ¨¡å‹ï¼Œå¦‚Llama3.1-8B-Instructã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.22491', 'title': 'SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization', 'url': 'https://huggingface.co/papers/2601.22491', 'abstract': "Sweet Spot Learning (SSL) introduces a novel reinforcement learning framework that uses tiered rewards to guide agent optimization toward optimal regions of the solution space, improving sample efficiency and cross-task transferability.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards has emerged as a powerful paradigm for training intelligent agents. However, existing methods typically employ binary rewards that fail to capture quality differences among trajectories achieving identical outcomes, thereby overlooking potential diversity within the solution space. Inspired by the ``sweet spot'' concept in tennis-the racket's core region that produces optimal hitting effects, we introduce Sweet Spot Learning (SSL), a novel framework that provides differentiated guidance for agent optimization. SSL follows a simple yet effective principle: progressively amplified, tiered rewards guide policies toward the sweet-spot region of the solution space. This principle naturally adapts across diverse tasks: visual perception tasks leverage distance-tiered modeling to reward proximity, while complex reasoning tasks reward incremental progress toward promising solutions. We theoretically demonstrate that SSL preserves optimal solution ordering and enhances the gradient signal-to-noise ratio, thereby fostering more directed optimization. Extensive experiments across GUI perception, short/long-term planning, and complex reasoning tasks show consistent improvements over strong baselines on 12 benchmarks, achieving up to 2.5X sample efficiency gains and effective cross-task transferability. Our work establishes SSL as a general principle for training capable and robust agents.", 'score': 7, 'issue_id': 871, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 30', 'zh': '1æœˆ30æ—¥'}, 'hash': 'b16b81a5b4104b74', 'authors': ['Jinyang Wu', 'Changpeng Yang', 'Yuhao Shen', 'Fangzhi Xu', 'Bolin Ni', 'Chonghua Liao', 'Yuchen Liu', 'Hongzhen Wang', 'Shuai Nie', 'Shuai Zhang', 'Haoran Luo', 'Jiaming Xu'], 'affiliations': ['Institute of Automation, Chinese Academy of Sciences', 'Nanyang Technological University', 'Tsinghua University', 'Xiaomi Corporation', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.22491.jpg', 'data': {'categories': ['#benchmark', '#transfer_learning', '#rl', '#optimization', '#agents', '#reasoning'], 'emoji': 'ğŸ¾', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ÑƒĞ´Ğ°Ñ€ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹: Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Sweet Spot Learning (SSL) Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸, SSL Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ, Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ñ…. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğº ÑˆÑƒĞ¼Ñƒ Ğ¿Ñ€Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ¾ 2.5x Ñ€Ğ°Ğ· Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Unlocking Optimal Learning with Tiered Rewards', 'desc': "Sweet Spot Learning (SSL) is a new approach in reinforcement learning that uses tiered rewards to help agents find the best solutions more efficiently. Instead of just giving a simple yes or no reward, SSL provides different levels of rewards based on how close the agent's actions are to the optimal solution. This method not only improves how quickly agents learn but also helps them apply what they've learned to different tasks. Experiments show that SSL significantly boosts performance, making agents smarter and more adaptable across various challenges."}, 'zh': {'title': 'ç”œç‚¹å­¦ä¹ ï¼šä¼˜åŒ–æ™ºèƒ½ä½“çš„æ–°æ–¹æ³•', 'desc': 'ç”œç‚¹å­¦ä¹ ï¼ˆSSLï¼‰æ˜¯ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚å¥–åŠ±æ¥å¼•å¯¼æ™ºèƒ½ä½“ä¼˜åŒ–ï¼Œå¸®åŠ©å…¶æ›´æœ‰æ•ˆåœ°æ‰¾åˆ°è§£å†³æ–¹æ¡ˆçš„æœ€ä½³åŒºåŸŸã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨äºŒå…ƒå¥–åŠ±ï¼Œæ— æ³•æ•æ‰åˆ°å®ç°ç›¸åŒç»“æœçš„è½¨è¿¹ä¹‹é—´çš„è´¨é‡å·®å¼‚ï¼Œå¿½è§†äº†è§£å†³æ–¹æ¡ˆç©ºé—´ä¸­çš„æ½œåœ¨å¤šæ ·æ€§ã€‚SSLçš„æ ¸å¿ƒç†å¿µæ˜¯é€šè¿‡é€æ­¥å¢å¼ºçš„åˆ†å±‚å¥–åŠ±ï¼ŒæŒ‡å¯¼ç­–ç•¥æœå‘è§£å†³æ–¹æ¡ˆç©ºé—´çš„â€œç”œç‚¹â€åŒºåŸŸã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒSSLåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡å’Œè·¨ä»»åŠ¡çš„è¿ç§»èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.21716', 'title': 'DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning', 'url': 'https://huggingface.co/papers/2601.21716', 'abstract': 'DreamActor-M2 presents a universal character animation framework that addresses motion injection trade-offs and pose prior limitations through in-context learning and self-bootstrapped data synthesis for improved generalization across diverse characters.  \t\t\t\t\tAI-generated summary \t\t\t\t Character image animation aims to synthesize high-fidelity videos by transferring motion from a driving sequence to a static reference image. Despite recent advancements, existing methods suffer from two fundamental challenges: (1) suboptimal motion injection strategies that lead to a trade-off between identity preservation and motion consistency, manifesting as a "see-saw", and (2) an over-reliance on explicit pose priors (e.g., skeletons), which inadequately capture intricate dynamics and hinder generalization to arbitrary, non-humanoid characters. To address these challenges, we present DreamActor-M2, a universal animation framework that reimagines motion conditioning as an in-context learning problem. Our approach follows a two-stage paradigm. First, we bridge the input modality gap by fusing reference appearance and motion cues into a unified latent space, enabling the model to jointly reason about spatial identity and temporal dynamics by leveraging the generative prior of foundational models. Second, we introduce a self-bootstrapped data synthesis pipeline that curates pseudo cross-identity training pairs, facilitating a seamless transition from pose-dependent control to direct, end-to-end RGB-driven animation. This strategy significantly enhances generalization across diverse characters and motion scenarios. To facilitate comprehensive evaluation, we further introduce AW Bench, a versatile benchmark encompassing a wide spectrum of characters types and motion scenarios. Extensive experiments demonstrate that DreamActor-M2 achieves state-of-the-art performance, delivering superior visual fidelity and robust cross-domain generalization. Project Page: https://grisoon.github.io/DreamActor-M2/', 'score': 6, 'issue_id': 871, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 29', 'zh': '1æœˆ29æ—¥'}, 'hash': '7b143dcb2fb5153f', 'authors': ['Mingshuang Luo', 'Shuang Liang', 'Zhengkun Rong', 'Yuxuan Luo', 'Tianshu Hu', 'Ruibing Hou', 'Hong Chang', 'Yong Li', 'Yuan Zhang', 'Mingyuan Gao'], 'affiliations': ['ByteDance Intelligent Creation', 'Key Lab of Intell. Info. Process., ICT, CAS', 'Southeast University', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2601.21716.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#benchmark', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ±ĞµĞ· ÑĞ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ· Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ', 'desc': 'DreamActor-M2 â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¼ Ğ²Ğ¸Ğ´Ğµ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ°Ğ¼Ğ¾Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ°Ñ€ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¹Ñ‚Ğ¸ Ğ¾Ñ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞºĞµĞ»ĞµÑ‚Ğ¾Ğ² Ğº Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ RGB-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Character Animation with DreamActor-M2', 'desc': "DreamActor-M2 is a novel framework for character animation that improves how motion is applied to static images. It tackles two main issues: the balance between keeping a character's identity and ensuring smooth motion, and the limitations of using fixed pose references like skeletons. By using in-context learning, it combines visual and motion information into a single representation, allowing for better understanding of both identity and movement. Additionally, it employs a self-bootstrapped data synthesis method to create diverse training examples, enhancing its ability to animate various character types effectively."}, 'zh': {'title': 'æ¢¦å¹»æ¼”å‘˜M2ï¼šé€šç”¨è§’è‰²åŠ¨ç”»çš„æ–°çºªå…ƒ', 'desc': 'DreamActor-M2æ˜¯ä¸€ä¸ªé€šç”¨çš„è§’è‰²åŠ¨ç”»æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ å’Œè‡ªæˆ‘å¼•å¯¼çš„æ•°æ®åˆæˆæ¥è§£å†³è¿åŠ¨æ³¨å…¥çš„æƒè¡¡å’Œå§¿æ€å…ˆéªŒçš„å±€é™æ€§ï¼Œä»è€Œæé«˜å¯¹å¤šæ ·åŒ–è§’è‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µçš„æ–¹æ³•ï¼Œé¦–å…ˆå°†å‚è€ƒå¤–è§‚å’Œè¿åŠ¨çº¿ç´¢èåˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ½œåœ¨ç©ºé—´ä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå…±åŒæ¨ç†ç©ºé—´èº«ä»½å’Œæ—¶é—´åŠ¨æ€ã€‚å…¶æ¬¡ï¼ŒDreamActor-M2å¼•å…¥äº†ä¸€ç§è‡ªæˆ‘å¼•å¯¼çš„æ•°æ®åˆæˆç®¡é“ï¼Œåˆ›å»ºä¼ªè·¨èº«ä»½è®­ç»ƒå¯¹ï¼Œä¿ƒè¿›ä»ä¾èµ–å§¿æ€çš„æ§åˆ¶åˆ°ç›´æ¥çš„ç«¯åˆ°ç«¯RGBé©±åŠ¨åŠ¨ç”»çš„æ— ç¼è¿‡æ¸¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDreamActor-M2åœ¨è§†è§‰ä¿çœŸåº¦å’Œè·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.21468', 'title': 'MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning', 'url': 'https://huggingface.co/papers/2601.21468', 'abstract': 'MemOCR is a multimodal memory agent that enhances long-horizon reasoning by adaptively compressing interaction histories into visual layouts, enabling efficient context utilization under tight budget constraints.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.', 'score': 6, 'issue_id': 871, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 29', 'zh': '1æœˆ29æ—¥'}, 'hash': 'c69e9fab65c83419', 'authors': ['Yaorui Shi', 'Shugui Liu', 'Yu Yang', 'Wenyu Mao', 'Yuxin Chen', 'Qi GU', 'Hui Su', 'Xunliang Cai', 'Xiang Wang', 'An Zhang'], 'affiliations': ['Meituan', 'National University of Singapore, School of Computing', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2601.21468.jpg', 'data': {'categories': ['#benchmark', '#long_context', '#rl', '#agents', '#multimodal', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'MemOCR â€” ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ ÑĞµÑ€Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ°ĞºĞµÑ‚, Ğ³Ğ´Ğµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ÑÑ, Ğ° Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾ ÑĞ¶Ğ¸Ğ¼Ğ°ÑÑ‚ÑÑ. ĞĞ³ĞµĞ½Ñ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹, Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ± Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑÑ… Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… ÑĞ¶Ğ°Ñ‚Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MemOCR Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ñ€Ğ¸ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'MemOCR: Smart Memory for Efficient Long-Horizon Reasoning', 'desc': 'MemOCR is a novel multimodal memory agent designed to enhance reasoning over long interactions by compressing historical data into visual formats. Unlike traditional memory systems that use text serialization, MemOCR adapts its memory allocation based on the importance of information, allowing for efficient use of limited context space. It employs a structured rich-text memory that is transformed into images, prioritizing essential details while minimizing less critical information. By utilizing reinforcement learning with budget-aware objectives, MemOCR demonstrates superior performance in question-answering tasks, effectively managing context under strict budget constraints.'}, 'zh': {'title': 'MemOCRï¼šé«˜æ•ˆçš„é•¿æ—¶é—´æ¨ç†è®°å¿†ä»£ç†', 'desc': 'MemOCRæ˜¯ä¸€ç§å¤šæ¨¡æ€è®°å¿†ä»£ç†ï¼Œæ—¨åœ¨é€šè¿‡è‡ªé€‚åº”å‹ç¼©äº¤äº’å†å²æ¥å¢å¼ºé•¿æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚å®ƒé€šè¿‡å°†å†å²ä¿¡æ¯è½¬åŒ–ä¸ºè§†è§‰å¸ƒå±€ï¼Œä¼˜åŒ–äº†åœ¨æœ‰é™ä¸Šä¸‹æ–‡é¢„ç®—ä¸‹çš„è®°å¿†ä½¿ç”¨æ•ˆç‡ã€‚ä¸ä¼ ç»Ÿçš„æ–‡æœ¬åºåˆ—åŒ–è®°å¿†ç³»ç»Ÿä¸åŒï¼ŒMemOCRèƒ½å¤Ÿä»¥ç»“æ„åŒ–çš„å¯Œæ–‡æœ¬å½¢å¼å­˜å‚¨ä¿¡æ¯ï¼Œå¹¶å°†å…¶æ¸²æŸ“ä¸ºå›¾åƒï¼Œä»¥ä¾¿åœ¨è®°å¿†è®¿é—®æ—¶ä¼˜å…ˆè€ƒè™‘é‡è¦è¯æ®ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼ŒMemOCRåœ¨ä¸åŒçš„é¢„ç®—æ¡ä»¶ä¸‹è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨é•¿ä¸Šä¸‹æ–‡çš„å¤šè·³å’Œå•è·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å¼ºå¤§çš„æ–‡æœ¬åŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.20218', 'title': 'DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment', 'url': 'https://huggingface.co/papers/2601.20218', 'abstract': 'DenseGRPO addresses sparse reward problems in flow matching models by introducing dense rewards for intermediate denoising steps and adaptive exploration calibration.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce DenseGRPO, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.', 'score': 6, 'issue_id': 871, 'pub_date': '2026-01-28', 'pub_date_card': {'ru': '28 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 28', 'zh': '1æœˆ28æ—¥'}, 'hash': 'ef07baf88e11172b', 'authors': ['Haoyou Deng', 'Keyu Yan', 'Chaojie Mao', 'Xiang Wang', 'Yu Liu', 'Changxin Gao', 'Nong Sang'], 'affiliations': ['National Key Laboratory of Multispectral Information Intelligent Processing Technology, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2601.20218.jpg', 'data': {'categories': ['#benchmark', '#rlhf', '#diffusion', '#alignment', '#optimization', '#multimodal', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞŸĞ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ° Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'DenseGRPO Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… flow matching Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ° Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ ĞºĞ¾ Ğ²ÑĞµĞ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğº Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ñ‡Ğ¸ÑÑ‚Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑˆÑƒĞ¼Ğ° Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ĞµĞ¹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'Dense Rewards for Better Flow Matching', 'desc': 'DenseGRPO is a new framework designed to tackle the sparse reward problem in flow matching models used for text-to-image generation. It introduces dense rewards for each intermediate denoising step, allowing for a more accurate evaluation of contributions at each stage. By predicting step-wise reward gains and applying a reward model to intermediate clean images, DenseGRPO aligns feedback signals with individual contributions effectively. Additionally, it calibrates the exploration space by adjusting stochasticity based on the noise intensity, ensuring optimal exploration throughout the denoising process.'}, 'zh': {'title': 'DenseGRPOï¼šè§£å†³ç¨€ç–å¥–åŠ±çš„åˆ›æ–°æ¡†æ¶', 'desc': 'DenseGRPOæ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æµåŒ¹é…æ¨¡å‹ä¸­çš„ç¨€ç–å¥–åŠ±é—®é¢˜ã€‚é€šè¿‡ä¸ºæ¯ä¸ªå»å™ªæ­¥éª¤å¼•å…¥å¯†é›†å¥–åŠ±ï¼ŒDenseGRPOèƒ½å¤Ÿæ›´å¥½åœ°è¯„ä¼°æ¯ä¸ªæ­¥éª¤çš„ç»†ç²’åº¦è´¡çŒ®ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šé¦–å…ˆï¼Œé¢„æµ‹æ¯ä¸ªå»å™ªæ­¥éª¤çš„é€æ­¥å¥–åŠ±å¢ç›Šï¼Œä»¥ç¡®ä¿åé¦ˆä¿¡å·ä¸ä¸ªåˆ«æ­¥éª¤çš„è´¡çŒ®å¯¹é½ï¼›å…¶æ¬¡ï¼Œæå‡ºäº†ä¸€ç§å¥–åŠ±æ„ŸçŸ¥æ–¹æ¡ˆï¼Œé€šè¿‡è‡ªé€‚åº”è°ƒæ•´éšæœºæ€§æ³¨å…¥ï¼Œæ ¡å‡†æ¢ç´¢ç©ºé—´ï¼Œä»è€Œä¼˜åŒ–è®­ç»ƒæ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.22642', 'title': 'Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification', 'url': 'https://huggingface.co/papers/2601.22642', 'abstract': 'A formal logic verification-guided framework dynamically interleaves symbolic verification with natural language generation to improve reasoning accuracy and reduce errors in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification-guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neuro-symbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain. We operationalize this framework via a novel two-stage training pipeline that synergizes formal logic verification-guided supervised fine-tuning and policy optimization. Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as a scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning.', 'score': 5, 'issue_id': 871, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 30', 'zh': '1æœˆ30æ—¥'}, 'hash': '7cfd51e101bac332', 'authors': ['Chuxue Cao', 'Jinluan Yang', 'Haoran Li', 'Kunhao Pan', 'Zijian Zhao', 'Zhengyu Chen', 'Yuchen Tian', 'Lijun Wu', 'Conghui He', 'Sirui Han', 'Yike Guo'], 'affiliations': ['Hong Kong Baptist University', 'Hong Kong University of Science and Technology', 'Shanghai Artificial Intelligence Laboratory', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.22642.jpg', 'data': {'categories': ['#benchmark', '#training', '#rlhf'], 'emoji': 'âš™ï¸', 'ru': {'title': 'Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ĞºĞ°Ğº Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ°Ñ ÑĞ¸Ğ»Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑˆĞ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ÑÑ‚Ñ„Ğ°ĞºÑ‚ÑƒĞ¼ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑˆÑ‚Ñ€Ğ°Ñ„ÑƒĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ supervised fine-tuning Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ policy optimization.'}, 'en': {'title': 'Enhancing LLM Reasoning with Real-Time Logic Verification', 'desc': 'This paper presents a new framework that combines formal logic verification with natural language generation to enhance the reasoning capabilities of large language models (LLMs). By integrating symbolic verification into the generation process, the framework provides immediate feedback to identify and correct logical errors in real-time. Unlike previous methods that only validate reasoning after it has occurred, this approach actively penalizes mistakes during the reasoning process. The authors demonstrate that their models, trained with this framework, significantly outperform existing models on various reasoning tasks, showcasing the effectiveness of formal verification in improving LLM performance.'}, 'zh': {'title': 'å½¢å¼é€»è¾‘éªŒè¯æå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå½¢å¼é€»è¾‘éªŒè¯çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†å‡†ç¡®æ€§å¹¶å‡å°‘é”™è¯¯ã€‚è¯¥æ¡†æ¶å°†å½¢å¼ç¬¦å·éªŒè¯ä¸è‡ªç„¶è¯­è¨€ç”ŸæˆåŠ¨æ€äº¤ç»‡ï¼Œå®æ—¶åé¦ˆä»¥æ£€æµ‹å’Œçº æ­£é”™è¯¯ã€‚ä¸ä»¥å¾€çš„ç¥ç»ç¬¦å·æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¨ç†é“¾ä¸­ä¸»åŠ¨æƒ©ç½šä¸­é—´è°¬è¯¯ã€‚é€šè¿‡æ–°é¢–çš„ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†å½¢å¼éªŒè¯ä½œä¸ºæå‡LLMæ¨ç†æ€§èƒ½çš„å¯æ‰©å±•æœºåˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.22636', 'title': 'Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling', 'url': 'https://huggingface.co/papers/2601.22636', 'abstract': 'A scaling-aware risk estimation method called SABER is introduced for predicting large-scale adversarial vulnerability in language models through Best-of-N sampling, enabling accurate assessment with reduced computational costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are typically evaluated for safety under single-shot or low-budget adversarial prompting, which underestimates real-world risk. In practice, attackers can exploit large-scale parallel sampling to repeatedly probe a model until a harmful response is produced. While recent work shows that attack success increases with repeated sampling, principled methods for predicting large-scale adversarial risk remain limited. We propose a scaling-aware Best-of-N estimation of risk, SABER, for modeling jailbreak vulnerability under Best-of-N sampling. We model sample-level success probabilities using a Beta distribution, the conjugate prior of the Bernoulli distribution, and derive an analytic scaling law that enables reliable extrapolation of large-N attack success rates from small-budget measurements. Using only n=100 samples, our anchored estimator predicts ASR@1000 with a mean absolute error of 1.66, compared to 12.04 for the baseline, which is an 86.2% reduction in estimation error. Our results reveal heterogeneous risk scaling profiles and show that models appearing robust under standard evaluation can experience rapid nonlinear risk amplification under parallel adversarial pressure. This work provides a low-cost, scalable methodology for realistic LLM safety assessment. We will release our code and evaluation scripts upon publication to future research.', 'score': 5, 'issue_id': 873, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 30', 'zh': '1æœˆ30æ—¥'}, 'hash': 'c8d4ce6d36364c09', 'authors': ['Mingqian Feng', 'Xiaodong Liu', 'Weiwei Yang', 'Chenliang Xu', 'Christopher White', 'Jianfeng Gao'], 'affiliations': ['Microsoft Research', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2601.22636.jpg', 'data': {'categories': ['#alignment', '#security'], 'emoji': 'ğŸ”“', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰ÑƒÑ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ SABER Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº adversarial Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ sampling. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ±ĞµÑ‚Ğ°-Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑƒÑĞ¿ĞµÑ…Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´ÑÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ scaling law, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ°Ñ‚Ğ°Ğº Ñ Ğ¼Ğ°Ğ»Ğ¾Ğ³Ğ¾ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ N. ĞœĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒÑĞ¿ĞµÑ…Ğ° Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ñ‚Ñ‹ÑÑÑ‡Ğµ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 100 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¾Ğ¹ Ğ² 1.66 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ²Ğ¼ĞµÑÑ‚Ğ¾ 12.04 Ğ´Ğ»Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ°Ğ¶ÑƒÑ‰Ğ¸ĞµÑÑ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ, Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹ Ğº Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¼Ñƒ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¼Ñƒ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¸ÑĞºĞ° Ğ¿Ğ¾Ğ´ Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… adversarial Ğ°Ñ‚Ğ°Ğº.'}, 'en': {'title': 'SABER: Scalable Risk Estimation for Safer Language Models', 'desc': 'The paper introduces SABER, a method for estimating the risk of adversarial attacks on large language models (LLMs) using a Best-of-N sampling approach. Traditional evaluations often underestimate risks by using limited sampling, but SABER allows for accurate predictions of vulnerability by modeling success probabilities with a Beta distribution. This method significantly reduces computational costs while providing reliable estimates of attack success rates, even with a small number of samples. The findings highlight that models may seem safe under standard tests but can show increased vulnerability when subjected to extensive adversarial probing.'}, 'zh': {'title': 'SABERï¼šé«˜æ•ˆè¯„ä¼°è¯­è¨€æ¨¡å‹å¯¹æŠ—æ€§é£é™©çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSABERçš„é£é™©ä¼°è®¡æ–¹æ³•ï¼Œç”¨äºé¢„æµ‹è¯­è¨€æ¨¡å‹åœ¨å¤§è§„æ¨¡å¯¹æŠ—æ€§æ”»å‡»ä¸‹çš„è„†å¼±æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡Best-of-Né‡‡æ ·å®ç°äº†é«˜æ•ˆçš„é£é™©è¯„ä¼°ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ”»å‡»æˆåŠŸç‡éšç€é‡å¤é‡‡æ ·è€Œå¢åŠ ï¼Œè€ŒSABERèƒ½å¤Ÿé€šè¿‡è´å¡”åˆ†å¸ƒå»ºæ¨¡æ ·æœ¬çº§æˆåŠŸæ¦‚ç‡ï¼Œä»è€Œå¯é åœ°æ¨æ–­å¤§è§„æ¨¡æ”»å‡»çš„æˆåŠŸç‡ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSABERåœ¨å°æ ·æœ¬ä¸‹çš„ä¼°è®¡è¯¯å·®æ˜¾è‘—ä½äºåŸºçº¿ï¼Œæä¾›äº†ä¸€ç§ä½æˆæœ¬ã€å¯æ‰©å±•çš„è¯­è¨€æ¨¡å‹å®‰å…¨è¯„ä¼°æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.21358', 'title': 'Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization', 'url': 'https://huggingface.co/papers/2601.21358', 'abstract': 'PLaT introduces a latent reasoning framework that decouples reasoning from verbalization, enabling dynamic termination and improved scalability over traditional approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search.', 'score': 4, 'issue_id': 871, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 29', 'zh': '1æœˆ29æ—¥'}, 'hash': '271318646c5be3a2', 'authors': ['Jiecong Wang', 'Hao Peng', 'Chunyang Liu'], 'affiliations': ['Beihang University', 'Didi Chuxing'], 'pdf_title_img': 'assets/pdf/title_img/2601.21358.jpg', 'data': {'categories': ['#inference', '#interpretability', '#training', '#reasoning', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ‚Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ²: Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ²', 'desc': 'PLaT Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ ĞµĞ³Ğ¾ Ğ²ĞµÑ€Ğ±Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ ÑÑ‚Ğ¸ Ğ¼Ñ‹ÑĞ»Ğ¸ Ğ² Ñ‚ĞµĞºÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ³Ğ´Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ‚Ğ°ĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ·Ğ°Ñ€Ğ°Ğ½ĞµĞµ. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ¸Ğ±ĞºÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°.'}, 'en': {'title': 'Decoupling Reasoning for Scalable AI Solutions', 'desc': 'PLaT introduces a novel framework for latent reasoning that separates the reasoning process from the generation of text. This allows the model to dynamically decide when to stop reasoning, improving efficiency compared to traditional methods that rely on fixed steps. By modeling reasoning as a series of latent planning states, PLaT enhances scalability and diversity in problem-solving. Empirical results show that while it may have lower immediate accuracy, it offers a more flexible and robust approach to reasoning in complex tasks.'}, 'zh': {'title': 'åŠ¨æ€æ¨ç†ä¸è¯­è¨€è¡¨è¾¾çš„è§£è€¦', 'desc': 'PLaTï¼ˆæ½œåœ¨æ€ç»´è§„åˆ’ï¼‰å¼•å…¥äº†ä¸€ç§æ½œåœ¨æ¨ç†æ¡†æ¶ï¼Œå°†æ¨ç†ä¸è¯­è¨€è¡¨è¾¾è§£è€¦ï¼Œä»è€Œå®ç°åŠ¨æ€ç»ˆæ­¢å’Œæ¯”ä¼ ç»Ÿæ–¹æ³•æ›´å¥½çš„å¯æ‰©å±•æ€§ã€‚è¯¥æ¡†æ¶å°†æ¨ç†å»ºæ¨¡ä¸ºæ½œåœ¨è§„åˆ’çŠ¶æ€çš„ç¡®å®šæ€§è½¨è¿¹ï¼Œè€Œå•ç‹¬çš„è§£ç å™¨åœ¨å¿…è¦æ—¶å°†è¿™äº›æ€ç»´è½¬åŒ–ä¸ºæ–‡æœ¬ã€‚è¿™ç§è§£è€¦ä½¿æ¨¡å‹èƒ½å¤ŸåŠ¨æ€å†³å®šä½•æ—¶ç»ˆæ­¢æ¨ç†ï¼Œè€Œä¸ä¾èµ–äºå›ºå®šçš„è¶…å‚æ•°ã€‚å®éªŒè¯æ˜ï¼Œå°½ç®¡PLaTåœ¨è´ªå©ªå‡†ç¡®æ€§ä¸Šä½äºåŸºçº¿ï¼Œä½†åœ¨æ¨ç†å¤šæ ·æ€§æ–¹é¢è¡¨ç°å‡ºæ›´å¥½çš„å¯æ‰©å±•æ€§ï¼Œè¡¨æ˜å…¶å­¦ä¹ äº†æ›´å¼ºå¤§ã€æ›´å¹¿æ³›çš„è§£å†³æ–¹æ¡ˆç©ºé—´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.23184', 'title': 'ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought', 'url': 'https://huggingface.co/papers/2601.23184', 'abstract': 'ReGuLaR introduces a variational auto-encoding framework that compresses reasoning processes into latent space while maintaining performance through image-rendered explicit reasoning chains for guidance.  \t\t\t\t\tAI-generated summary \t\t\t\t While Chain-of-Thought (CoT) significantly enhances the performance of Large Language Models (LLMs), explicit reasoning chains introduce substantial computational redundancy. Recent latent reasoning methods attempt to mitigate this by compressing reasoning processes into latent space, but often suffer from severe performance degradation due to the lack of appropriate compression guidance. In this study, we propose Rendered CoT-Guided variational Latent Reasoning (ReGuLaR), a simple yet novel latent learning paradigm resolving this issue. Fundamentally, we formulate latent reasoning within the Variational Auto-Encoding (VAE) framework, sampling the current latent reasoning state from the posterior distribution conditioned on previous ones. Specifically, when learning this variational latent reasoning model, we render explicit reasoning chains as images, from which we extract dense visual-semantic representations to regularize the posterior distribution, thereby achieving efficient compression with minimal information loss. Extensive experiments demonstrate that ReGuLaR significantly outperforms existing latent reasoning methods across both computational efficiency and reasoning effectiveness, and even surpasses CoT through multi-modal reasoning, providing a new and insightful solution to latent reasoning. Code: https://github.com/FanmengWang/ReGuLaR.', 'score': 3, 'issue_id': 871, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 30', 'zh': '1æœˆ30æ—¥'}, 'hash': 'a0cd48968fea2d79', 'authors': ['Fanmeng Wang', 'Haotian Liu', 'Guojiang Zhao', 'Hongteng Xu', 'Zhifeng Gao'], 'affiliations': ['Beijing Key Laboratory of Research on Large Models and Intelligent Governance', 'DP Technology', 'Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2601.23184.jpg', 'data': {'categories': ['#open_source', '#optimization', '#multimodal', '#training', '#reasoning', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ¶Ğ°Ñ‚Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ', 'desc': 'ReGuLaR Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¯Ğ²Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ ĞºĞ°Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ÑÑ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑĞ¼Ğ¸, Ñ€ĞµÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ… ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ReGuLaR Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Chain-of-Thought Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´.'}, 'en': {'title': 'Efficient Latent Reasoning with Visual Guidance', 'desc': "ReGuLaR presents a new approach to latent reasoning by integrating a variational auto-encoding framework that compresses reasoning processes into a latent space. It utilizes rendered images of explicit reasoning chains to guide the compression, ensuring that performance is maintained while reducing computational redundancy. This method allows for efficient sampling of latent states based on previous reasoning, enhancing the model's ability to learn effectively. Experimental results show that ReGuLaR outperforms existing methods in both efficiency and reasoning capabilities, even exceeding traditional Chain-of-Thought techniques."}, 'zh': {'title': 'å˜åˆ†æ¨ç†çš„æ–°çªç ´ï¼šReGuLaR', 'desc': 'ReGuLaRæå‡ºäº†ä¸€ç§å˜åˆ†è‡ªç¼–ç æ¡†æ¶ï¼Œå°†æ¨ç†è¿‡ç¨‹å‹ç¼©åˆ°æ½œåœ¨ç©ºé—´ï¼ŒåŒæ—¶é€šè¿‡å›¾åƒæ¸²æŸ“çš„æ˜¾å¼æ¨ç†é“¾è¿›è¡ŒæŒ‡å¯¼ï¼Œä»¥ä¿æŒæ€§èƒ½ã€‚è¯¥æ–¹æ³•è§£å†³äº†ç°æœ‰æ½œåœ¨æ¨ç†æ–¹æ³•åœ¨å‹ç¼©è¿‡ç¨‹ä¸­æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚é€šè¿‡å°†æ˜¾å¼æ¨ç†é“¾æ¸²æŸ“ä¸ºå›¾åƒï¼Œæå–å¯†é›†çš„è§†è§‰-è¯­ä¹‰è¡¨ç¤ºæ¥è§„èŒƒåéªŒåˆ†å¸ƒï¼Œä»è€Œå®ç°é«˜æ•ˆå‹ç¼©å’Œæœ€å°ä¿¡æ¯æŸå¤±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReGuLaRåœ¨è®¡ç®—æ•ˆç‡å’Œæ¨ç†æ•ˆæœä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç”šè‡³åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­è¶…è¶Šäº†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.21957', 'title': 'PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing', 'url': 'https://huggingface.co/papers/2601.21957', 'abstract': "A compact vision-language model achieves state-of-the-art accuracy on document understanding tasks while maintaining efficiency through specialized benchmarking and extended functionality.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce PaddleOCR-VL-1.5, an upgraded model achieving a new state-of-the-art (SOTA) accuracy of 94.5% on OmniDocBench v1.5. To rigorously evaluate robustness against real-world physical distortions, including scanning, skew, warping, screen-photography, and illumination, we propose the Real5-OmniDocBench benchmark. Experimental results demonstrate that this enhanced model attains SOTA performance on the newly curated benchmark. Furthermore, we extend the model's capabilities by incorporating seal recognition and text spotting tasks, while remaining a 0.9B ultra-compact VLM with high efficiency. Code: https://github.com/PaddlePaddle/PaddleOCR", 'score': 3, 'issue_id': 873, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 29', 'zh': '1æœˆ29æ—¥'}, 'hash': 'dbdb8e8fbbdbcc92', 'authors': ['Cheng Cui', 'Ting Sun', 'Suyin Liang', 'Tingquan Gao', 'Zelun Zhang', 'Jiaxuan Liu', 'Xueqing Wang', 'Changda Zhou', 'Hongen Liu', 'Manhui Lin', 'Yue Zhang', 'Yubo Zhang', 'Yi Liu', 'Dianhai Yu', 'Yanjun Ma'], 'affiliations': ['Baidu Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2601.21957.jpg', 'data': {'categories': ['#small_models', '#cv', '#multimodal', '#open_source', '#dataset', '#benchmark'], 'emoji': 'ğŸ“„', 'ru': {'title': 'Ğ£Ğ»ÑŒÑ‚Ñ€Ğ°ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ PaddleOCR-VL-1.5, ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 0.9B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° 94.5% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ”Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ (ÑĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¿ĞµÑ€ĞµĞºĞ¾Ñ, Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ Ñ ÑĞºÑ€Ğ°Ğ½Ğ°, Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ), Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Real5-OmniDocBench. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿ĞµÑ‡Ğ°Ñ‚ĞµĞ¹ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ¸ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Compact Model, Big Achievements in Document Understanding!', 'desc': "The paper presents PaddleOCR-VL-1.5, a compact vision-language model that achieves a remarkable accuracy of 94.5% on the OmniDocBench v1.5 benchmark for document understanding tasks. It introduces the Real5-OmniDocBench benchmark to test the model's robustness against various real-world distortions like scanning and illumination. The model not only excels in performance but also incorporates additional functionalities such as seal recognition and text spotting. Despite its advanced capabilities, it maintains a lightweight architecture with only 0.9 billion parameters, ensuring high efficiency."}, 'zh': {'title': 'ç´§å‡‘é«˜æ•ˆçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œæ–‡æ¡£ç†è§£çš„æ–°æ ‡æ†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç´§å‡‘çš„è§†è§‰-è¯­è¨€æ¨¡å‹PaddleOCR-VL-1.5ï¼Œè¯¥æ¨¡å‹åœ¨æ–‡æ¡£ç†è§£ä»»åŠ¡ä¸Šè¾¾åˆ°äº†94.5%çš„æœ€æ–°æœ€ä½³å‡†ç¡®ç‡ã€‚ä¸ºäº†è¯„ä¼°æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œä¸­çš„é²æ£’æ€§ï¼Œä½œè€…æå‡ºäº†Real5-OmniDocBenchåŸºå‡†ï¼Œæ¶µç›–äº†æ‰«æã€å€¾æ–œã€å˜å½¢ã€å±å¹•æ‘„å½±å’Œå…‰ç…§ç­‰ç‰©ç†æ‰­æ›²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ–°åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”åœ¨åŠŸèƒ½ä¸Šæ‰©å±•äº†å°ç« è¯†åˆ«å’Œæ–‡æœ¬æ£€æµ‹ä»»åŠ¡ã€‚å°½ç®¡åŠŸèƒ½å¼ºå¤§ï¼ŒPaddleOCR-VL-1.5ä»ä¿æŒä¸º0.9Bçš„è¶…ç´§å‡‘è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰é«˜æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.21419', 'title': 'Revisiting Diffusion Model Predictions Through Dimensionality', 'url': 'https://huggingface.co/papers/2601.21419', 'abstract': "Diffusion models using direct data prediction outperform traditional noise or velocity prediction in high-dimensional settings, with a proposed framework automatically learning optimal prediction parameters from data.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion and flow matching models have highlighted a shift in the preferred prediction target -- moving from noise (varepsilon) and velocity (v) to direct data (x) prediction -- particularly in high-dimensional settings. However, a formal explanation of why the optimal target depends on the specific properties of the data remains elusive. In this work, we provide a theoretical framework based on a generalized prediction formulation that accommodates arbitrary output targets, of which varepsilon-, v-, and x-prediction are special cases. We derive the analytical relationship between data's geometry and the optimal prediction target, offering a rigorous justification for why x-prediction becomes superior when the ambient dimension significantly exceeds the data's intrinsic dimension. Furthermore, while our theory identifies dimensionality as the governing factor for the optimal prediction target, the intrinsic dimension of manifold-bound data is typically intractable to estimate in practice. To bridge this gap, we propose k-Diff, a framework that employs a data-driven approach to learn the optimal prediction parameter k directly from data, bypassing the need for explicit dimension estimation. Extensive experiments in both latent-space and pixel-space image generation demonstrate that k-Diff consistently outperforms fixed-target baselines across varying architectures and data scales, providing a principled and automated approach to enhancing generative performance.", 'score': 3, 'issue_id': 873, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 29', 'zh': '1æœˆ29æ—¥'}, 'hash': '61baa9cea285e3c0', 'authors': ['Qing Jin', 'Chaoyang Wang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2601.21419.jpg', 'data': {'categories': ['#optimization', '#diffusion'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚, Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ĞµĞ¼ ÑˆÑƒĞ¼Ğ° Ğ¸Ğ»Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹. ĞĞ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼, ĞºĞ¾Ğ³Ğ´Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ k-Diff, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· ÑĞ°Ğ¼Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑĞ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Direct Data Prediction: The Key to Better Diffusion Models', 'desc': "This paper discusses how diffusion models can improve performance in high-dimensional data settings by shifting from traditional noise or velocity predictions to direct data predictions. The authors introduce a theoretical framework that explains why the best prediction target depends on the data's geometric properties, particularly when the data's intrinsic dimension is much lower than the ambient dimension. They propose a new method called k-Diff, which automatically learns the optimal prediction parameters from the data, eliminating the need for manual dimension estimation. Experimental results show that k-Diff outperforms existing methods in generating images, demonstrating its effectiveness across different architectures and data scales."}, 'zh': {'title': 'ç›´æ¥æ•°æ®é¢„æµ‹ï¼šæ‰©æ•£æ¨¡å‹çš„æ–°æ–¹å‘', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹åœ¨é«˜ç»´è®¾ç½®ä¸­ç›´æ¥æ•°æ®é¢„æµ‹çš„ä¼˜åŠ¿ï¼Œä¼˜äºä¼ ç»Ÿçš„å™ªå£°æˆ–é€Ÿåº¦é¢„æµ‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç†è®ºæ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®æ•°æ®çš„å‡ ä½•ç‰¹æ€§è‡ªåŠ¨å­¦ä¹ æœ€ä½³é¢„æµ‹å‚æ•°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“ç¯å¢ƒç»´åº¦è¿œå¤§äºæ•°æ®çš„å†…åœ¨ç»´åº¦æ—¶ï¼Œç›´æ¥æ•°æ®é¢„æµ‹çš„æ•ˆæœæ›´ä½³ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†k-Diffæ¡†æ¶ï¼Œé€šè¿‡æ•°æ®é©±åŠ¨çš„æ–¹æ³•ç›´æ¥ä»æ•°æ®ä¸­å­¦ä¹ æœ€ä½³é¢„æµ‹å‚æ•°ï¼Œé¿å…äº†æ˜¾å¼ç»´åº¦ä¼°è®¡çš„éœ€æ±‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.18241', 'title': 'TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance', 'url': 'https://huggingface.co/papers/2601.18241', 'abstract': 'TAM-Eval is a framework and benchmark for evaluating large language models on comprehensive test suite maintenance tasks including creation, repair, and updating across multiple programming languages.  \t\t\t\t\tAI-generated summary \t\t\t\t While Large Language Models (LLMs) have shown promise in software engineering, their application to unit testing remains largely confined to isolated test generation or oracle prediction, neglecting the broader challenge of test suite maintenance. We introduce TAM-Eval (Test Automated Maintenance Evaluation), a framework and benchmark designed to evaluate model performance across three core test maintenance scenarios: creation, repair, and updating of test suites. Unlike prior work limited to function-level tasks, TAM-Eval operates at the test file level, while maintaining access to full repository context during isolated evaluation, better reflecting real-world maintenance workflows. Our benchmark comprises 1,539 automatically extracted and validated scenarios from Python, Java, and Go projects. TAM-Eval supports system-agnostic evaluation of both raw LLMs and agentic workflows, using a reference-free protocol based on test suite pass rate, code coverage, and mutation testing. Empirical results indicate that state-of-the-art LLMs have limited capabilities in realistic test maintenance processes and yield only marginal improvements in test effectiveness. We release TAM-Eval as an open-source framework to support future research in automated software testing. Our data and code are publicly available at https://github.com/trndcenter/TAM-Eval.', 'score': 3, 'issue_id': 874, 'pub_date': '2026-01-26', 'pub_date_card': {'ru': '26 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 26', 'zh': '1æœˆ26æ—¥'}, 'hash': 'd7bb7f20fdf1d2d5', 'authors': ['Elena Bruches', 'Vadim Alperovich', 'Dari Baturova', 'Roman Derunets', 'Daniil Grebenkin', 'Georgy Mkrtchyan', 'Oleg Sedukhin', 'Mikhail Klementev', 'Ivan Bondarenko', 'Nikolay Bushkov', 'Stanislav Moiseev'], 'affiliations': ['Novosibirsk State University, Novosibirsk, Russia', 'Siberian Neuronets LLC, Novosibirsk, Russia', 'T-Technologies, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2601.18241.jpg', 'data': {'categories': ['#plp', '#agents', '#dataset', '#open_source', '#benchmark'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğº Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼Ñƒ Ğ¶Ğ¸Ğ·Ğ½ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ†Ğ¸ĞºĞ»Ñƒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ²', 'desc': 'TAM-Eval â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ, Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚, ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ², Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ¾Ğ¼ Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 1539 Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ¸Ğ· Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Python, Java Ğ¸ Go, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğµ Ğ¿Ñ€Ğ¾Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ², Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¼ÑƒÑ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ¸Ğ¼ĞµÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ»Ğ¸ÑˆÑŒ Ğ¼Ğ°Ñ€Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'TAM-Eval: Advancing Test Suite Maintenance with LLMs', 'desc': 'TAM-Eval is a new framework designed to assess how well large language models (LLMs) can handle the maintenance of software test suites, which includes creating, repairing, and updating tests across different programming languages. Unlike previous studies that focused on smaller, function-level tasks, TAM-Eval evaluates performance at the test file level, providing a more realistic view of how models perform in actual software development scenarios. The benchmark includes over 1,500 scenarios from popular programming languages like Python, Java, and Go, and uses metrics such as test pass rates and code coverage to measure effectiveness. Results show that current LLMs struggle with complex test maintenance tasks, highlighting the need for further research in this area.'}, 'zh': {'title': 'TAM-Evalï¼šå…¨é¢è¯„ä¼°æµ‹è¯•å¥—ä»¶ç»´æŠ¤çš„æ¡†æ¶', 'desc': 'TAM-Evalæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è½¯ä»¶æµ‹è¯•ç»´æŠ¤ä»»åŠ¡ä¸­çš„æ¡†æ¶å’ŒåŸºå‡†ã€‚å®ƒå…³æ³¨äºæµ‹è¯•å¥—ä»¶çš„åˆ›å»ºã€ä¿®å¤å’Œæ›´æ–°ï¼Œè¶…è¶Šäº†ä»¥å¾€ä»…é™äºå•ä¸€åŠŸèƒ½çš„æµ‹è¯•ç”Ÿæˆã€‚è¯¥æ¡†æ¶åœ¨æµ‹è¯•æ–‡ä»¶çº§åˆ«è¿›è¡Œè¯„ä¼°ï¼Œå¹¶åœ¨éš”ç¦»è¯„ä¼°ä¸­ä¿æŒå¯¹å®Œæ•´ä»£ç åº“çš„è®¿é—®ï¼Œæ›´å¥½åœ°åæ˜ äº†ç°å®ä¸­çš„ç»´æŠ¤å·¥ä½œæµç¨‹ã€‚é€šè¿‡å¯¹1,539ä¸ªè‡ªåŠ¨æå–å’ŒéªŒè¯çš„åœºæ™¯è¿›è¡Œè¯„ä¼°ï¼ŒTAM-Evalæ˜¾ç¤ºå‡ºå½“å‰æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹åœ¨å®é™…æµ‹è¯•ç»´æŠ¤è¿‡ç¨‹ä¸­çš„èƒ½åŠ›æœ‰é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.13097', 'title': 'RM -RF: Reward Model for Run-Free Unit Test Evaluation', 'url': 'https://huggingface.co/papers/2601.13097', 'abstract': 'RM-RF is a lightweight reward model that predicts execution outcomes from source code alone, offering faster and more cost-effective evaluation than traditional compile-and-run methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We present RM-RF, a lightweight reward model for run-free evaluation of automatically generated unit tests. Instead of repeatedly compiling and executing candidate tests, RM-RF predicts - from source and test code alone - three execution-derived signals: (1) whether the augmented test suite compiles and runs successfully, (2) whether the generated test cases increase code coverage, and (3) whether the generated test cases improve the mutation kill rate. To train and evaluate RM-RF we assemble a multilingual dataset (Java, Python, Go) of focal files, test files, and candidate test additions labeled by an execution-based pipeline, and we release an associated dataset and methodology for comparative evaluation. We tested multiple model families and tuning regimes (zero-shot, full fine-tuning, and PEFT via LoRA), achieving an average F1 of 0.69 across the three targets. Compared to conventional compile-and-run instruments, RM-RF provides substantially lower latency and infrastructure cost while delivering competitive predictive fidelity, enabling fast, scalable feedback for large-scale test generation and RL-based code optimization.', 'score': 3, 'issue_id': 874, 'pub_date': '2026-01-19', 'pub_date_card': {'ru': '19 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 19', 'zh': '1æœˆ19æ—¥'}, 'hash': '3b4716c0678cae9f', 'authors': ['Elena Bruches', 'Daniil Grebenkin', 'Mikhail Klementev', 'Vadim Alperovich', 'Roman Derunets', 'Dari Baturova', 'Georgy Mkrtchyan', 'Oleg Sedukhin', 'Ivan Bondarenko', 'Nikolay Bushkov', 'Stanislav Moiseev'], 'affiliations': ['Novosibirsk State University, Novosibirsk, Russia', 'Siberian Neuronets LLC, Novosibirsk, Russia', 'T-Technologies, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2601.13097.jpg', 'data': {'categories': ['#optimization', '#training', '#science', '#plp', '#dataset', '#multilingual', '#open_source', '#small_models', '#data'], 'emoji': 'âš¡', 'ru': {'title': 'ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ±ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ğ¸ â€” ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° RM-RF â€” Ğ»Ñ‘Ğ³ĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ¸Ğ· Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°, Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°: ÑƒÑĞ¿ĞµÑˆĞ½ÑƒÑ ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ñ‚ĞµÑÑ‚Ğ¾Ğ², ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ ÑƒĞ±Ğ¸Ğ¹ÑÑ‚Ğ²Ğ° Ğ¼ÑƒÑ‚Ğ°Ñ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ (Java, Python, Go) Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ¸ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ F1-Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ 0,69 Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (zero-shot, Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ fine-tuning, LoRA). ĞŸĞ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼ Â«ÑĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»Ğ¸Ñ€ÑƒĞ¹ Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Â», RM-RF Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆÑƒÑ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¸ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Fast and Cost-Effective Test Evaluation with RM-RF', 'desc': 'RM-RF is a novel lightweight reward model designed to evaluate automatically generated unit tests without the need for execution. It predicts three key execution outcomes based solely on source and test code: successful compilation and execution, code coverage improvement, and mutation kill rate enhancement. The model is trained on a diverse dataset that includes multiple programming languages, ensuring broad applicability. By offering faster and more cost-effective evaluations than traditional methods, RM-RF facilitates efficient feedback for large-scale test generation and reinforcement learning in code optimization.'}, 'zh': {'title': 'è½»é‡çº§å¥–åŠ±æ¨¡å‹RM-RFï¼šå¿«é€Ÿè¯„ä¼°è‡ªåŠ¨ç”Ÿæˆçš„å•å…ƒæµ‹è¯•', 'desc': 'RM-RFæ˜¯ä¸€ç§è½»é‡çº§çš„å¥–åŠ±æ¨¡å‹ï¼Œèƒ½å¤Ÿä»…é€šè¿‡æºä»£ç é¢„æµ‹æ‰§è¡Œç»“æœï¼Œä»è€Œæ¯”ä¼ ç»Ÿçš„ç¼–è¯‘å’Œè¿è¡Œæ–¹æ³•æ›´å¿«ä¸”æ›´å…·æˆæœ¬æ•ˆç›Šã€‚è¯¥æ¨¡å‹å¯ä»¥åœ¨ä¸åå¤ç¼–è¯‘å’Œæ‰§è¡Œå€™é€‰æµ‹è¯•çš„æƒ…å†µä¸‹ï¼Œé¢„æµ‹ä¸‰ä¸ªæ‰§è¡Œç›¸å…³çš„ä¿¡å·ï¼šæµ‹è¯•å¥—ä»¶æ˜¯å¦æˆåŠŸç¼–è¯‘å’Œè¿è¡Œã€ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹æ˜¯å¦æé«˜äº†ä»£ç è¦†ç›–ç‡ï¼Œä»¥åŠæ˜¯å¦æ”¹å–„äº†çªå˜æ€æ­»ç‡ã€‚ä¸ºäº†è®­ç»ƒå’Œè¯„ä¼°RM-RFï¼Œæˆ‘ä»¬ç»„å»ºäº†ä¸€ä¸ªå¤šè¯­è¨€æ•°æ®é›†ï¼ŒåŒ…æ‹¬Javaã€Pythonå’ŒGoçš„ç„¦ç‚¹æ–‡ä»¶ã€æµ‹è¯•æ–‡ä»¶å’Œå€™é€‰æµ‹è¯•æ·»åŠ ï¼Œå¹¶é€šè¿‡åŸºäºæ‰§è¡Œçš„ç®¡é“è¿›è¡Œæ ‡æ³¨ã€‚ä¸ä¼ ç»Ÿçš„ç¼–è¯‘å’Œè¿è¡Œå·¥å…·ç›¸æ¯”ï¼ŒRM-RFæ˜¾è‘—é™ä½äº†å»¶è¿Ÿå’ŒåŸºç¡€è®¾æ–½æˆæœ¬ï¼ŒåŒæ—¶æä¾›äº†ç«äº‰åŠ›çš„é¢„æµ‹å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿä¸ºå¤§è§„æ¨¡æµ‹è¯•ç”Ÿæˆå’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„ä»£ç ä¼˜åŒ–æä¾›å¿«é€Ÿã€å¯æ‰©å±•çš„åé¦ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.23188', 'title': 'Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience', 'url': 'https://huggingface.co/papers/2601.23188', 'abstract': 'Deep search agents with hierarchical metacognitive monitoring enhance reasoning and retrieval performance through fast consistency checks and experience-driven corrective interventions.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep search agents powered by large language models have demonstrated strong capabilities in multi-step retrieval, reasoning, and long-horizon task execution. However, their practical failures often stem from the lack of mechanisms to monitor and regulate reasoning and retrieval states as tasks evolve under uncertainty. Insights from cognitive neuroscience suggest that human metacognition is hierarchically organized, integrating fast anomaly detection with selectively triggered, experience-driven reflection. In this work, we propose Deep Search with Meta-Cognitive Monitoring (DS-MCM), a deep search framework augmented with an explicit hierarchical metacognitive monitoring mechanism. DS-MCM integrates a Fast Consistency Monitor, which performs lightweight checks on the alignment between external evidence and internal reasoning confidence, and a Slow Experience-Driven Monitor, which is selectively activated to guide corrective intervention based on experience memory from historical agent trajectories. By embedding monitoring directly into the reasoning-retrieval loop, DS-MCM determines both when intervention is warranted and how corrective actions should be informed by prior experience. Experiments across multiple deep search benchmarks and backbone models demonstrate that DS-MCM consistently improves performance and robustness.', 'score': 2, 'issue_id': 872, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 30', 'zh': '1æœˆ30æ—¥'}, 'hash': 'a87289443af8a0ac', 'authors': ['Zhongxiang Sun', 'Qipeng Wang', 'Weijie Yu', 'Jingxuan Yang', 'Haolang Lu', 'Jun Xu'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'Gaoling School of Artificial Intelligence Renmin University of China', 'School of Information Technology and Management University of International Business and Economics', 'Search Applications Department, Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2601.23188.jpg', 'data': {'categories': ['#rag', '#benchmark', '#agents'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ‚Ğ°ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Deep Search with Meta-Cognitive Monitoring (DS-MCM) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼ĞµÑ‚Ğ°ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ°, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ°ÑƒĞºĞ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€, Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ¸Ğ· Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ñ†Ğ¸ĞºĞ» Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ñ‹, ĞºĞ¾Ğ³Ğ´Ğ° Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾, Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ DS-MCM ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Deep Search Agents with Smart Self-Monitoring', 'desc': "This paper introduces a new framework called Deep Search with Meta-Cognitive Monitoring (DS-MCM) that enhances the performance of deep search agents. It incorporates a hierarchical metacognitive monitoring system that helps these agents check their reasoning and retrieval processes in real-time. The framework includes a Fast Consistency Monitor for quick checks and a Slow Experience-Driven Monitor for deeper reflection based on past experiences. Experiments show that DS-MCM significantly improves the agents' ability to handle complex tasks and uncertainties."}, 'zh': {'title': 'æ·±åº¦æœç´¢ä¸å…ƒè®¤çŸ¥ç›‘æ§ï¼šæå‡æ¨ç†ä¸æ£€ç´¢çš„æ™ºèƒ½æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ·±åº¦æœç´¢ä¸å…ƒè®¤çŸ¥ç›‘æ§ï¼ˆDS-MCMï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨æå‡æ·±åº¦æœç´¢ä»£ç†çš„æ¨ç†å’Œæ£€ç´¢æ€§èƒ½ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¿«é€Ÿä¸€è‡´æ€§ç›‘æ§å’Œç»éªŒé©±åŠ¨çš„æ…¢ç›‘æ§æœºåˆ¶ï¼Œä»¥åº”å¯¹ä»»åŠ¡åœ¨ä¸ç¡®å®šæ€§ä¸‹çš„æ¼”å˜ã€‚é€šè¿‡åœ¨æ¨ç†-æ£€ç´¢å¾ªç¯ä¸­åµŒå…¥ç›‘æ§ï¼ŒDS-MCMèƒ½å¤Ÿåˆ¤æ–­ä½•æ—¶éœ€è¦å¹²é¢„ï¼Œå¹¶æ ¹æ®å†å²ç»éªŒæŒ‡å¯¼çº æ­£æªæ–½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDS-MCMåœ¨å¤šä¸ªæ·±åº¦æœç´¢åŸºå‡†å’Œæ¨¡å‹ä¸Šå‡è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.22664', 'title': 'Real-Time Aligned Reward Model beyond Semantics', 'url': 'https://huggingface.co/papers/2601.22664', 'abstract': 'RLHF suffers from reward overoptimization due to misalignment between reward models and policy models, which R2M addresses by incorporating real-time policy feedback to dynamically adapt reward modeling during training.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward overoptimization, in which policy models overfit to the reward model, exploit spurious reward patterns instead of faithfully capturing human intent. Prior mitigations primarily relies on surface semantic information and fails to efficiently address the misalignment between the reward model (RM) and the policy model caused by continuous policy distribution shifts. This inevitably leads to an increasing reward discrepancy, exacerbating reward overoptimization. To address these limitations, we introduce R2M (Real-Time Aligned Reward Model), a novel lightweight RLHF framework. R2M goes beyond vanilla reward models that solely depend on the semantic representations of a pretrained LLM. Instead, it leverages the evolving hidden states of the policy (namely policy feedback) to align with the real-time distribution shift of the policy during the RL process. This work points to a promising new direction for improving the performance of reward models through real-time utilization of feedback from policy models.', 'score': 2, 'issue_id': 871, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 30', 'zh': '1æœˆ30æ—¥'}, 'hash': 'f8dfe7a629e6c33e', 'authors': ['Zixuan Huang', 'Xin Xia', 'Yuxi Ren', 'Jianbin Zheng', 'Xuefeng Xiao', 'Hongyan Xie', 'Li Huaqiu', 'Songshi Liang', 'Zhongxiang Dai', 'Fuzhen Zhuang', 'Jianxin Li', 'Yikun Ban', 'Deqing Wang'], 'affiliations': ['Beihang University', 'ByteDance', 'Renmin University of China', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2601.22664.jpg', 'data': {'categories': ['#training', '#rlhf', '#alignment'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğµ RLHF, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ¸Ğ·-Ğ·Ğ° Ñ€Ğ°ÑÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ R2M â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ² Ğ²Ğ¸Ğ´Ğµ ĞµÑ‘ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ¿Ğ¾Ğ´ ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'R2M: Real-Time Feedback for Better Reward Alignment', 'desc': 'This paper discusses a problem in Reinforcement Learning from Human Feedback (RLHF) where reward models can lead to overoptimization, causing policy models to misalign with human preferences. The authors introduce R2M, a new framework that incorporates real-time feedback from policy models to improve reward modeling during training. By using the evolving hidden states of the policy, R2M dynamically adjusts the reward model to better match the current policy distribution. This approach aims to reduce the reward discrepancy and enhance the alignment between the reward model and the policy model, ultimately improving the performance of large language models.'}, 'zh': {'title': 'å®æ—¶å¯¹é½å¥–åŠ±æ¨¡å‹ï¼šè§£å†³å¥–åŠ±è¿‡åº¦ä¼˜åŒ–çš„åˆ›æ–°æ–¹æ³•', 'desc': 'å¼ºåŒ–å­¦ä¹ ä¸­çš„äººç±»åé¦ˆï¼ˆRLHFï¼‰æ˜¯ä¸€ç§é‡è¦æŠ€æœ¯ï¼Œç”¨äºä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½å¯¹é½ã€‚ç„¶è€Œï¼ŒRLHFå®¹æ˜“å‡ºç°å¥–åŠ±è¿‡åº¦ä¼˜åŒ–çš„é—®é¢˜ï¼Œå³ç­–ç•¥æ¨¡å‹è¿‡åº¦æ‹Ÿåˆå¥–åŠ±æ¨¡å‹ï¼Œå¯¼è‡´æ•æ‰ä¸åˆ°çœŸå®çš„äººç±»æ„å›¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†R2Mï¼ˆå®æ—¶å¯¹é½å¥–åŠ±æ¨¡å‹ï¼‰ï¼Œå®ƒé€šè¿‡å®æ—¶æ”¿ç­–åé¦ˆåŠ¨æ€è°ƒæ•´å¥–åŠ±å»ºæ¨¡ï¼Œä»è€Œæ”¹å–„å¥–åŠ±æ¨¡å‹ä¸ç­–ç•¥æ¨¡å‹ä¹‹é—´çš„å¯¹é½ã€‚R2Måˆ©ç”¨ç­–ç•¥çš„éšè—çŠ¶æ€å˜åŒ–ï¼Œæä¾›äº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥æé«˜å¥–åŠ±æ¨¡å‹çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.22141', 'title': 'Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data', 'url': 'https://huggingface.co/papers/2601.22141', 'abstract': 'Routing the Lottery framework discovers multiple specialized subnetworks tailored to different data conditions, outperforming traditional pruning methods while using fewer parameters and identifying subnetwork collapse issues.  \t\t\t\t\tAI-generated summary \t\t\t\t In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.', 'score': 2, 'issue_id': 871, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 29', 'zh': '1æœˆ29æ—¥'}, 'hash': 'd5d68fbfe04ee90b', 'authors': ['Grzegorz Stefanski', 'Alberto Presta', 'Michal Byra'], 'affiliations': ['Institute of Fundamental Technological Research, Polish Academy of Sciences, Poland', 'Samsung AI Center Warsaw, Poland'], 'pdf_title_img': 'assets/pdf/title_img/2601.22141.jpg', 'data': {'categories': ['#inference', '#training'], 'emoji': 'ğŸ«', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ¸Ğ»ĞµÑ‚Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Â«Routing the LotteryÂ», ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞµÑ‚ĞµĞ¹, ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ¿Ğ¾Ğ´ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ ĞºĞ»Ğ°ÑÑÑ‹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ğ´Ğ½Ñƒ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ¾Ğ´ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ¾ 10 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ´ÑĞµÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ÑĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ĞºĞ°Ğº Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒÑ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… ÑĞµÑ‚ĞµĞ¹.'}, 'en': {'title': 'Unlocking Adaptive Subnetworks for Diverse Data Conditions', 'desc': 'The Routing the Lottery (RTL) framework enhances traditional pruning methods by identifying multiple specialized subnetworks, known as adaptive tickets, that are optimized for different data conditions. Unlike previous approaches that rely on a single winning ticket, RTL acknowledges the diversity in real-world data by tailoring subnetworks to specific classes or environments. This method not only improves performance metrics like balanced accuracy and recall but also significantly reduces the number of parameters needed, making models more efficient. Additionally, RTL addresses the issue of subnetwork collapse, providing a new similarity score to diagnose oversparsification without requiring labels.'}, 'zh': {'title': 'è‡ªé€‚åº”å‰ªæï¼Œæå‡æ¨¡å‹æ€§èƒ½çš„å…³é”®', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºRouting the Lotteryï¼ˆRTLï¼‰çš„è‡ªé€‚åº”å‰ªææ¡†æ¶ï¼Œæ—¨åœ¨å‘ç°å¤šä¸ªä¸“é—¨é’ˆå¯¹ä¸åŒæ•°æ®æ¡ä»¶çš„å­ç½‘ç»œã€‚è¿™äº›å­ç½‘ç»œè¢«ç§°ä¸ºè‡ªé€‚åº”ç¥¨è¯ï¼Œèƒ½å¤Ÿåœ¨å¤šç§æ•°æ®é›†å’Œä»»åŠ¡ä¸­è¶…è¶Šä¼ ç»Ÿçš„å•ä¸€æ¨¡å‹å’Œå¤šæ¨¡å‹åŸºçº¿ã€‚RTLåœ¨ä¿æŒå¹³è¡¡å‡†ç¡®ç‡å’Œå¬å›ç‡çš„åŒæ—¶ï¼Œä½¿ç”¨çš„å‚æ•°æ•°é‡æ¯”ç‹¬ç«‹æ¨¡å‹å°‘å¤šè¾¾10å€ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è¯†åˆ«äº†å­ç½‘ç»œå´©æºƒçš„é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†å­ç½‘ç»œç›¸ä¼¼åº¦è¯„åˆ†ï¼Œä»¥ä¾¿åœ¨æ²¡æœ‰æ ‡ç­¾çš„æƒ…å†µä¸‹è¯Šæ–­è¿‡åº¦ç¨€ç–åŒ–ç°è±¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.21525', 'title': 'LMK > CLS: Landmark Pooling for Dense Embeddings', 'url': 'https://huggingface.co/papers/2601.21525', 'abstract': 'Landmark pooling improves long-context representation learning by partitioning sequences into chunks and using landmark tokens to preserve both global and local information more effectively than traditional pooling methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Representation learning is central to many downstream tasks such as search, clustering, classification, and reranking. State-of-the-art sequence encoders typically collapse a variable-length token sequence to a single vector using a pooling operator, most commonly a special [CLS] token or mean pooling over token embeddings. In this paper, we identify systematic weaknesses of these pooling strategies: [CLS] tends to concentrate information toward the initial positions of the sequence and can under-represent distributed evidence, while mean pooling can dilute salient local signals, sometimes leading to worse short-context performance. To address these issues, we introduce Landmark (LMK) pooling, which partitions a sequence into chunks, inserts landmark tokens between chunks, and forms the final representation by mean-pooling the landmark token embeddings. This simple mechanism improves long-context extrapolation without sacrificing local salient features, at the cost of introducing a small number of special tokens. We empirically demonstrate that LMK pooling matches existing methods on short-context retrieval tasks and yields substantial improvements on long-context tasks, making it a practical and scalable alternative to existing pooling methods.', 'score': 2, 'issue_id': 871, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 29', 'zh': '1æœˆ29æ—¥'}, 'hash': 'b6eff2a3ffcaf0bf', 'authors': ['Meet Doshi', 'Aashka Trivedi', 'Vishwajeet Kumar', 'Parul Awasthy', 'Yulong Li', 'Jaydeep Sen', 'Radu Florian', 'Sachindra Joshi'], 'affiliations': ['IBM Research'], 'pdf_title_img': 'assets/pdf/title_img/2601.21525.jpg', 'data': {'categories': ['#training', '#architecture', '#long_context'], 'emoji': 'ğŸ§©', 'ru': {'title': 'ĞÑ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Landmark pooling, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº ÑĞ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ (pooling) Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ [CLS] Ğ½ĞµÑ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ° ÑƒÑÑ€ĞµĞ´Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ¼Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹. Landmark pooling Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…, Ğ½Ğµ Ñ‚ĞµÑ€ÑÑ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ….'}, 'en': {'title': 'Enhancing Long-Context Learning with Landmark Pooling', 'desc': 'This paper introduces Landmark (LMK) pooling, a new method for improving representation learning in long-context sequences. Traditional pooling methods, like [CLS] tokens and mean pooling, often fail to capture important information from both local and global contexts. LMK pooling addresses these weaknesses by dividing sequences into chunks and using landmark tokens to maintain critical information. The results show that LMK pooling performs well on short-context tasks and significantly enhances performance on long-context tasks, making it a valuable alternative to existing methods.'}, 'zh': {'title': 'åœ°æ ‡æ± åŒ–ï¼šæå‡é•¿ä¸Šä¸‹æ–‡è¡¨ç¤ºçš„æœ‰æ•ˆæ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ± åŒ–æ–¹æ³•ï¼Œç§°ä¸ºåœ°æ ‡æ± åŒ–ï¼ˆLandmark poolingï¼‰ï¼Œæ—¨åœ¨æ”¹å–„é•¿ä¸Šä¸‹æ–‡çš„è¡¨ç¤ºå­¦ä¹ ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†åºåˆ—åˆ†å‰²æˆå¤šä¸ªå—ï¼Œå¹¶åœ¨å—ä¹‹é—´æ’å…¥åœ°æ ‡æ ‡è®°ï¼Œæœ‰æ•ˆåœ°ä¿ç•™å…¨å±€å’Œå±€éƒ¨ä¿¡æ¯ã€‚ä¸ä¼ ç»Ÿçš„æ± åŒ–æ–¹æ³•ç›¸æ¯”ï¼Œåœ°æ ‡æ± åŒ–èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†é•¿ä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶ä¸ä¼šç‰ºç‰²å±€éƒ¨æ˜¾è‘—ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ°æ ‡æ± åŒ–åœ¨çŸ­ä¸Šä¸‹æ–‡æ£€ç´¢ä»»åŠ¡ä¸Šè¡¨ç°ä¸ç°æœ‰æ–¹æ³•ç›¸å½“ï¼Œè€Œåœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šåˆ™æœ‰æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.15625', 'title': 'Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors', 'url': 'https://huggingface.co/papers/2601.15625', 'abstract': "A framework called Fission-GRPO is introduced to improve multi-turn tool execution in large language models by converting execution errors into corrective supervision during reinforcement learning training.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.", 'score': 2, 'issue_id': 873, 'pub_date': '2026-01-22', 'pub_date_card': {'ru': '22 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 22', 'zh': '1æœˆ22æ—¥'}, 'hash': '2ad0dedebab3f94a', 'authors': ['Zhiwei Zhang', 'Fei Zhao', 'Rui Wang', 'Zezhong Wang', 'Bin Liang', 'Jiakang Wang', 'Yao Hu', 'Shaosheng Cao', 'Kam-Fai Wong'], 'affiliations': ['The Chinese University of Hong Kong', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2601.15625.jpg', 'data': {'categories': ['#training', '#agents', '#rl'], 'emoji': 'ğŸ”§', 'ru': {'title': 'ĞŸÑ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ: ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰ĞµĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ tool-use Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¤ission-GRPO â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ reinforcement learning Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Error Simulator Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¾Ñ‚ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ğ° Ğ½Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen3-8B ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ»Ğ°ÑÑŒ Ğ½Ğ° 4% Ğ² Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¼Ñƒ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº.'}, 'en': {'title': 'Transforming Errors into Learning Opportunities with Fission-GRPO', 'desc': "The paper introduces Fission-GRPO, a novel framework designed to enhance multi-turn tool execution in large language models (LLMs) by transforming execution errors into corrective supervision during reinforcement learning (RL) training. Traditional RL methods treat errors as sparse negative rewards, which do not provide effective recovery guidance, leading to repetitive mistakes. Fission-GRPO addresses this by fissioning failed trajectories into new training instances, incorporating feedback from an Error Simulator to guide the model in learning from its specific errors. This approach significantly improves the model's error recovery rate and overall accuracy, demonstrating its effectiveness in real-world tool interactions."}, 'zh': {'title': 'Fission-GRPOï¼šæå‡å¤šè½®å·¥å…·æ‰§è¡Œçš„æ™ºèƒ½æ¡†æ¶', 'desc': 'Fission-GRPOæ˜¯ä¸€ä¸ªæ–°æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­å°†æ‰§è¡Œé”™è¯¯è½¬åŒ–ä¸ºçº æ­£ç›‘ç£ï¼Œæ¥æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šè½®å·¥å…·æ‰§è¡Œèƒ½åŠ›ã€‚å½“å‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å°†é”™è¯¯è§†ä¸ºç¨€ç–çš„è´Ÿå¥–åŠ±ï¼Œç¼ºä¹æ¢å¤æŒ‡å¯¼ï¼Œè€ŒFission-GRPOé€šè¿‡å°†æ¯ä¸ªå¤±è´¥çš„æ‰§è¡Œè½¨è¿¹ä¸è¯Šæ–­åé¦ˆç»“åˆï¼Œç”Ÿæˆæ–°çš„è®­ç»ƒå®ä¾‹ï¼Œä»è€Œå¸®åŠ©æ¨¡å‹å­¦ä¹ å¦‚ä½•ä»é”™è¯¯ä¸­æ¢å¤ã€‚è¯¥æ¡†æ¶ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¢ç´¢è¿‡ç¨‹ä¸­å­¦ä¹ å…¶å…·ä½“é”™è¯¯ï¼Œè€Œä¸æ˜¯ä¾èµ–é™æ€çš„é¢„æ”¶é›†é”™è¯¯æ¡ˆä¾‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFission-GRPOæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„é”™è¯¯æ¢å¤ç‡å’Œæ•´ä½“å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.23161', 'title': 'DIFFA-2: A Practical Diffusion Large Language Model for General Audio Understanding', 'url': 'https://huggingface.co/papers/2601.23161', 'abstract': 'DIFFA-2, a diffusion-based large audio language model, achieves competitive audio understanding performance with improved efficiency over autoregressive counterparts through enhanced encoding, dual adapters, and staged training.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive (AR) large audio language models (LALMs) such as Qwen-2.5-Omni have achieved strong performance on audio understanding and interaction, but scaling them remains costly in data and computation, and strictly sequential decoding limits inference efficiency. Diffusion large language models (dLLMs) have recently been shown to make effective use of limited training data, and prior work on DIFFA indicates that replacing an AR backbone with a diffusion counterpart can substantially improve audio understanding under matched settings, albeit at a proof-of-concept scale without large-scale instruction tuning, preference alignment, or practical decoding schemes. We introduce DIFFA-2, a practical diffusion-based LALM for general audio understanding. DIFFA-2 upgrades the speech encoder, employs dual semantic and acoustic adapters, and is trained with a four-stage curriculum that combines semantic and acoustic alignment, large-scale supervised fine-tuning, and variance-reduced preference optimization, using only fully open-source corpora. Experiments on MMSU, MMAU, and MMAR show that DIFFA-2 consistently improves over DIFFA and is competitive to strong AR LALMs under practical training budgets, supporting diffusion-based modeling is a viable backbone for large-scale audio understanding. Our code is available at https://github.com/NKU-HLT/DIFFA.git.', 'score': 1, 'issue_id': 871, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 30', 'zh': '1æœˆ30æ—¥'}, 'hash': 'b261594d096c0128', 'authors': ['Jiaming Zhou', 'Xuxin Cheng', 'Shiwan Zhao', 'Yuhang Jia', 'Cao Liu', 'Ke Zeng', 'Xunliang Cai', 'Yong Qin'], 'affiliations': ['College of Computer Science, Nankai University', 'Meituan LongCat Interaction Team'], 'pdf_title_img': 'assets/pdf/title_img/2601.23161.jpg', 'data': {'categories': ['#rlhf', '#diffusion', '#audio', '#training', '#open_source', '#architecture'], 'emoji': 'ğŸµ', 'ru': {'title': 'Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ°ÑƒĞ´Ğ¸Ğ¾', 'desc': 'DIFFA-2 â€” ÑÑ‚Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ñ€ĞµÑ‡Ğ¸, Ğ´Ğ²Ğ¾Ğ¹Ğ½Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸ Ğ°ĞºÑƒÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DIFFA-2 ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ Ğ¶Ğ¸Ğ·Ğ½ĞµÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾.'}, 'en': {'title': 'Revolutionizing Audio Understanding with Diffusion Models', 'desc': 'DIFFA-2 is a diffusion-based large audio language model that enhances audio understanding while being more efficient than traditional autoregressive models. It incorporates an upgraded speech encoder and dual adapters to better capture both semantic and acoustic features. The model is trained using a four-stage curriculum that optimizes performance through a combination of alignment and fine-tuning techniques. Experiments demonstrate that DIFFA-2 outperforms its predecessor and competes effectively with leading autoregressive models, showcasing the potential of diffusion-based approaches in audio processing.'}, 'zh': {'title': 'DIFFA-2ï¼šé«˜æ•ˆçš„éŸ³é¢‘ç†è§£æ–°é€‰æ‹©', 'desc': 'DIFFA-2æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨éŸ³é¢‘ç†è§£æ–¹é¢å®ç°ç«äº‰åŠ›çš„è¡¨ç°ï¼ŒåŒæ—¶åœ¨æ•ˆç‡ä¸Šä¼˜äºè‡ªå›å½’æ¨¡å‹ã€‚å®ƒé€šè¿‡æ”¹è¿›çš„ç¼–ç å™¨ã€åŒé€‚é…å™¨å’Œåˆ†é˜¶æ®µè®­ç»ƒæ¥æå‡æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„è‡ªå›å½’æ¨¡å‹ç›¸æ¯”ï¼ŒDIFFA-2åœ¨è®­ç»ƒæ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDIFFA-2åœ¨å¤šä¸ªéŸ³é¢‘ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºå‰ä¸€ç‰ˆæœ¬DIFFAï¼Œå¹¶ä¸”åœ¨å®é™…è®­ç»ƒé¢„ç®—ä¸‹ä¸å¼ºå¤§çš„è‡ªå›å½’æ¨¡å‹ç›¸ç«äº‰ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.23134', 'title': 'Machine Learning for Energy-Performance-aware Scheduling', 'url': 'https://huggingface.co/papers/2601.23134', 'abstract': 'A Bayesian Optimization approach using Gaussian Processes automates scheduling configuration optimization on heterogeneous multi-core systems while approximating the Pareto Frontier for energy-time trade-offs.  \t\t\t\t\tAI-generated summary \t\t\t\t In the post-Dennard era, optimizing embedded systems requires navigating complex trade-offs between energy efficiency and latency. Traditional heuristic tuning is often inefficient in such high-dimensional, non-smooth landscapes. In this work, we propose a Bayesian Optimization framework using Gaussian Processes to automate the search for optimal scheduling configurations on heterogeneous multi-core architectures. We explicitly address the multi-objective nature of the problem by approximating the Pareto Frontier between energy and time. Furthermore, by incorporating Sensitivity Analysis (fANOVA) and comparing different covariance kernels (e.g., MatÃ©rn vs. RBF), we provide physical interpretability to the black-box model, revealing the dominant hardware parameters driving system performance.', 'score': 1, 'issue_id': 873, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 30', 'zh': '1æœˆ30æ—¥'}, 'hash': '5f6adff0b514ffa6', 'authors': ['Zheyuan Hu', 'Yifei Shi'], 'affiliations': ['Department of Computer Science and Technology, University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2601.23134.jpg', 'data': {'categories': ['#optimization'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ´ĞµÑ€Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ´ĞµÑ€Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ²Ğ½Ğ¾ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ¸Ñ€ÑƒÑ Ñ„Ñ€Ğ¾Ğ½Ñ‚ ĞŸĞ°Ñ€ĞµÑ‚Ğ¾ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ½ĞµÑ€Ğ³Ğ¾Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ñ‡Ñ‘Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‰Ğ¸ĞºĞ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ (fANOVA) Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ´Ñ€Ğ° ĞºĞ¾Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¸ (ĞœĞ°Ñ‚ĞµÑ€Ğ½ Ğ¸ RBF). Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹ÑĞ²Ğ¸Ñ‚ÑŒ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ, Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹.'}, 'en': {'title': 'Optimizing Energy-Time Trade-offs in Multi-Core Systems with Bayesian Optimization', 'desc': 'This paper presents a method for optimizing scheduling configurations in multi-core systems using Bayesian Optimization with Gaussian Processes. It focuses on balancing energy efficiency and latency, which are often conflicting objectives in embedded systems. The approach approximates the Pareto Frontier, allowing for a better understanding of trade-offs between energy consumption and processing time. Additionally, it enhances model interpretability through Sensitivity Analysis and the comparison of different covariance kernels, identifying key hardware parameters that influence performance.'}, 'zh': {'title': 'è‡ªåŠ¨åŒ–è°ƒåº¦ä¼˜åŒ–ï¼šèƒ½é‡ä¸æ—¶é—´çš„å¹³è¡¡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè´å¶æ–¯ä¼˜åŒ–çš„æ¡†æ¶ï¼Œåˆ©ç”¨é«˜æ–¯è¿‡ç¨‹è‡ªåŠ¨åŒ–è°ƒåº¦é…ç½®ä¼˜åŒ–ï¼Œé€‚ç”¨äºå¼‚æ„å¤šæ ¸ç³»ç»Ÿã€‚æˆ‘ä»¬è§£å†³äº†èƒ½é‡æ•ˆç‡ä¸å»¶è¿Ÿä¹‹é—´çš„å¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œé€šè¿‡è¿‘ä¼¼å¸•ç´¯æ‰˜å‰æ²¿æ¥å¹³è¡¡è¿™ä¸¤è€…ã€‚ä¼ ç»Ÿçš„å¯å‘å¼è°ƒä¼˜åœ¨é«˜ç»´éå…‰æ»‘çš„æœç´¢ç©ºé—´ä¸­æ•ˆç‡ä½ä¸‹ï¼Œè€Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡Œæœç´¢ã€‚é€šè¿‡å¼•å…¥æ•æ„Ÿæ€§åˆ†æå’Œæ¯”è¾ƒä¸åŒçš„åæ–¹å·®æ ¸ï¼Œæˆ‘ä»¬ä¸ºé»‘ç®±æ¨¡å‹æä¾›äº†ç‰©ç†å¯è§£é‡Šæ€§ï¼Œæ­ç¤ºäº†å½±å“ç³»ç»Ÿæ€§èƒ½çš„ä¸»è¦ç¡¬ä»¶å‚æ•°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.22904', 'title': 'DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation', 'url': 'https://huggingface.co/papers/2601.22904', 'abstract': 'A novel vision autoencoder framework combines semantic representation with pixel-level reconstruction using spherical latent space and Riemannian flow matching for improved fidelity and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have explored using pretrained Vision Foundation Models (VFMs) such as DINO for generative autoencoders, showing strong generative performance. Unfortunately, existing approaches often suffer from limited reconstruction fidelity due to the loss of high-frequency details. In this work, we present the DINO Spherical Autoencoder (DINO-SAE), a framework that bridges semantic representation and pixel-level reconstruction. Our key insight is that semantic information in contrastive representations is primarily encoded in the direction of feature vectors, while forcing strict magnitude matching can hinder the encoder from preserving fine-grained details. To address this, we introduce Hierarchical Convolutional Patch Embedding module that enhances local structure and texture preservation, and Cosine Similarity Alignment objective that enforces semantic consistency while allowing flexible feature magnitudes for detail retention. Furthermore, leveraging the observation that SSL-based foundation model representations intrinsically lie on a hypersphere, we employ Riemannian Flow Matching to train a Diffusion Transformer (DiT) directly on this spherical latent manifold. Experiments on ImageNet-1K demonstrate that our approach achieves state-of-the-art reconstruction quality, reaching 0.37 rFID and 26.2 dB PSNR, while maintaining strong semantic alignment to the pretrained VFM. Notably, our Riemannian Flow Matching-based DiT exhibits efficient convergence, achieving a gFID of 3.47 at 80 epochs.', 'score': 1, 'issue_id': 871, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 30', 'zh': '1æœˆ30æ—¥'}, 'hash': '36974d14be561b91', 'authors': ['Hun Chang', 'Byunghee Cha', 'Jong Chul Ye'], 'affiliations': ['Graduate School of AI, KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2601.22904.jpg', 'data': {'categories': [], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ¡Ñ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€: Ğ¾Ñ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğº Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ DINO-SAE â€” Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Vision Foundation Models Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ĞµĞ¹ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ° Ğ² ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ… ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ĞºĞ¾ÑĞ¸Ğ½ÑƒÑĞ½Ğ¾Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ½, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¼ĞµĞ»ĞºĞ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸. Ğ”Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ² ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Riemannian Flow Matching Ñ Diffusion Transformer, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ³Ğ¸Ğ¿ĞµÑ€ÑÑ„ĞµÑ€Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞºĞ¾Ñ€Ğ´Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ (0.37 rFID, 26.2 dB PSNR) Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ.'}, 'en': {'title': 'Bridging Semantic and Pixel-Level Reconstruction with DINO-SAE', 'desc': 'This paper introduces the DINO Spherical Autoencoder (DINO-SAE), a new framework that enhances image reconstruction by combining semantic representation with pixel-level details. It addresses the common issue of losing high-frequency details in existing generative models by using a Hierarchical Convolutional Patch Embedding module and a Cosine Similarity Alignment objective. The framework leverages Riemannian Flow Matching to train a Diffusion Transformer on a spherical latent space, which improves both fidelity and efficiency in reconstruction tasks. Experimental results show that DINO-SAE achieves state-of-the-art performance on ImageNet-1K, demonstrating superior reconstruction quality and semantic alignment with pretrained Vision Foundation Models.'}, 'zh': {'title': 'æå‡é‡å»ºè´¨é‡çš„çƒé¢è‡ªç¼–ç å™¨', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†è§‰è‡ªç¼–ç å™¨æ¡†æ¶ï¼Œç§°ä¸ºDINOçƒé¢è‡ªç¼–ç å™¨ï¼ˆDINO-SAEï¼‰ï¼Œå®ƒç»“åˆäº†è¯­ä¹‰è¡¨ç¤ºå’Œåƒç´ çº§é‡å»ºã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥å±‚æ¬¡å·ç§¯è¡¥ä¸åµŒå…¥æ¨¡å—ï¼Œå¢å¼ºäº†å±€éƒ¨ç»“æ„å’Œçº¹ç†çš„ä¿ç•™ï¼ŒåŒæ—¶é‡‡ç”¨ä½™å¼¦ç›¸ä¼¼æ€§å¯¹é½ç›®æ ‡ï¼Œç¡®ä¿è¯­ä¹‰ä¸€è‡´æ€§å¹¶çµæ´»è°ƒæ•´ç‰¹å¾å¹…åº¦ã€‚æˆ‘ä»¬è¿˜åˆ©ç”¨RiemannianæµåŒ¹é…æ–¹æ³•ï¼Œåœ¨çƒé¢æ½œåœ¨æµå½¢ä¸Šç›´æ¥è®­ç»ƒæ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰ï¼Œä»è€Œæé«˜äº†é‡å»ºçš„ä¿çœŸåº¦å’Œæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDINO-SAEåœ¨ImageNet-1Kæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡ï¼Œå±•ç°äº†å¼ºå¤§çš„è¯­ä¹‰å¯¹é½èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.22837', 'title': 'NativeTok: Native Visual Tokenization for Improved Image Generation', 'url': 'https://huggingface.co/papers/2601.22837', 'abstract': 'NativeTok introduces a novel visual tokenization approach that enforces causal dependencies during image encoding, using a Meta Image Transformer and Mixture of Causal Expert Transformer for efficient and coherent image generation.  \t\t\t\t\tAI-generated summary \t\t\t\t VQ-based image generation typically follows a two-stage pipeline: a tokenizer encodes images into discrete tokens, and a generative model learns their dependencies for reconstruction. However, improved tokenization in the first stage does not necessarily enhance the second-stage generation, as existing methods fail to constrain token dependencies. This mismatch forces the generative model to learn from unordered distributions, leading to bias and weak coherence. To address this, we propose native visual tokenization, which enforces causal dependencies during tokenization. Building on this idea, we introduce NativeTok, a framework that achieves efficient reconstruction while embedding relational constraints within token sequences. NativeTok consists of: (1) a Meta Image Transformer (MIT) for latent image modeling, and (2) a Mixture of Causal Expert Transformer (MoCET), where each lightweight expert block generates a single token conditioned on prior tokens and latent features. We further design a Hierarchical Native Training strategy that updates only new expert blocks, ensuring training efficiency. Extensive experiments demonstrate the effectiveness of NativeTok.', 'score': 1, 'issue_id': 871, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 30', 'zh': '1æœˆ30æ—¥'}, 'hash': '7320bcaf27ae941f', 'authors': ['Bin Wu', 'Mengqi Huang', 'Weinan Jia', 'Zhendong Mao'], 'affiliations': ['University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2601.22837.jpg', 'data': {'categories': [], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'NativeTok Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Meta Image Transformer Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Mixture of Causal Expert Transformer, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ±Ğ»Ğ¾Ğº Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° ÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑÑ… Ñ ÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Enhancing Image Generation with Causal Tokenization', 'desc': 'NativeTok presents a new method for visual tokenization that maintains causal relationships during the encoding of images. This approach utilizes a Meta Image Transformer for modeling latent images and a Mixture of Causal Expert Transformer to generate tokens based on previous tokens and features. By enforcing these dependencies, NativeTok improves the coherence and quality of generated images compared to traditional methods that treat tokens as unordered. The framework also incorporates a Hierarchical Native Training strategy to enhance training efficiency by updating only new expert blocks.'}, 'zh': {'title': 'NativeTokï¼šé«˜æ•ˆå›¾åƒç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'NativeTokæå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†è§‰æ ‡è®°åŒ–æ–¹æ³•ï¼Œåœ¨å›¾åƒç¼–ç è¿‡ç¨‹ä¸­å¼ºåˆ¶æ‰§è¡Œå› æœä¾èµ–å…³ç³»ã€‚è¯¥æ–¹æ³•ä½¿ç”¨Meta Image Transformerå’ŒMixture of Causal Expert Transformeræ¥å®ç°é«˜æ•ˆä¸”è¿è´¯çš„å›¾åƒç”Ÿæˆã€‚ä¼ ç»Ÿçš„VQåŸºç¡€å›¾åƒç”Ÿæˆæ–¹æ³•åœ¨æ ‡è®°åŒ–é˜¶æ®µå’Œç”Ÿæˆé˜¶æ®µä¹‹é—´å­˜åœ¨ä¸åŒ¹é…ï¼Œå¯¼è‡´ç”Ÿæˆæ¨¡å‹å­¦ä¹ åˆ°æ— åºçš„åˆ†å¸ƒã€‚NativeToké€šè¿‡åœ¨æ ‡è®°åŒ–è¿‡ç¨‹ä¸­åµŒå…¥å…³ç³»çº¦æŸï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œä»è€Œæé«˜äº†é‡å»ºçš„æ•ˆç‡å’Œä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.20732', 'title': 'Continual GUI Agents', 'url': 'https://huggingface.co/papers/2601.20732', 'abstract': 'Continual GUI Agents framework addresses performance degradation in dynamic digital environments through reinforcement fine-tuning with novel anchoring rewards that stabilize learning across shifting UI domains and resolutions.  \t\t\t\t\tAI-generated summary \t\t\t\t As digital environments (data distribution) are in flux, with new GUI data arriving over time-introducing new domains or resolutions-agents trained on static environments deteriorate in performance. In this work, we introduce Continual GUI Agents, a new task that requires GUI agents to perform continual learning under shifted domains and resolutions. We find existing methods fail to maintain stable grounding as GUI distributions shift over time, due to the diversity of UI interaction points and regions in fluxing scenarios. To address this, we introduce GUI-Anchoring in Flux (GUI-AiF), a new reinforcement fine-tuning framework that stabilizes continual learning through two novel rewards: Anchoring Point Reward in Flux (APR-iF) and Anchoring Region Reward in Flux (ARR-iF). These rewards guide the agents to align with shifting interaction points and regions, mitigating the tendency of existing reward strategies to over-adapt to static grounding cues (e.g., fixed coordinates or element scales). Extensive experiments show GUI-AiF surpasses state-of-the-art baselines. Our work establishes the first continual learning framework for GUI agents, revealing the untapped potential of reinforcement fine-tuning for continual GUI Agents.', 'score': 1, 'issue_id': 873, 'pub_date': '2026-01-28', 'pub_date_card': {'ru': '28 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 28', 'zh': '1æœˆ28æ—¥'}, 'hash': '90be57765bcaf0a1', 'authors': ['Ziwei Liu', 'Borui Kang', 'Hangjie Yuan', 'Zixiang Zhao', 'Wei Li', 'Yifan Zhu', 'Tao Feng'], 'affiliations': ['College of Computer Science, Zhejiang University, China', 'Department of Computer Science and Technology, Tsinghua University, China', 'Photogrammetry and Remote Sensing Lab, ETH Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2601.20732.jpg', 'data': {'categories': ['#training', '#agents', '#rl'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¯ĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞµ: ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ´Ğ²Ğ¸Ğ³Ğ¸ Ğ² Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ… Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ… ÑĞºÑ€Ğ°Ğ½Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Continual GUI Agents, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰ÑƒÑ Ğ¾Ñ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ½ĞµÑÑ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ½Ğ° Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞµ GUI-AiF, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ĞµĞ¼ Ğ´Ğ²Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ (APR-iF Ğ¸ ARR-iF), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ° ÑĞ´Ğ²Ğ¸Ğ³Ğ°ÑÑ‰ĞµĞ¼ÑÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ reinforcement learning Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ continual learning Ğ² GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ….'}, 'en': {'title': 'Stabilizing Learning in Dynamic GUI Environments', 'desc': 'The paper presents a new framework called Continual GUI Agents, which aims to improve the performance of agents in changing digital environments. It highlights the problem of performance degradation when agents trained on static data encounter new graphical user interface (GUI) domains and resolutions. To tackle this issue, the authors introduce a reinforcement fine-tuning method called GUI-Anchoring in Flux (GUI-AiF), which uses two innovative rewards to help agents adapt to shifting interaction points. The results demonstrate that this approach significantly outperforms existing methods, marking a significant advancement in continual learning for GUI agents.'}, 'zh': {'title': 'æŒç»­å­¦ä¹ ï¼Œç¨³å®šè¡¨ç°ï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºæŒç»­GUIä»£ç†ï¼ˆContinual GUI Agentsï¼‰ï¼Œæ—¨åœ¨è§£å†³åŠ¨æ€æ•°å­—ç¯å¢ƒä¸­æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚éšç€æ–°çš„GUIæ•°æ®ä¸æ–­æ¶Œç°ï¼Œç°æœ‰çš„ä»£ç†åœ¨é™æ€ç¯å¢ƒä¸­è®­ç»ƒï¼Œå¯¼è‡´å…¶åœ¨å˜åŒ–çš„é¢†åŸŸå’Œåˆ†è¾¨ç‡ä¸‹è¡¨ç°ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…å¼•å…¥äº†GUI-é”šå®šåœ¨æµåŠ¨ä¸­ï¼ˆGUI-AiFï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡ä¸¤ç§æ–°é¢–çš„å¥–åŠ±æœºåˆ¶æ¥ç¨³å®šå­¦ä¹ è¿‡ç¨‹ã€‚è¿™äº›å¥–åŠ±æœºåˆ¶å¸®åŠ©ä»£ç†é€‚åº”ä¸æ–­å˜åŒ–çš„äº¤äº’ç‚¹å’ŒåŒºåŸŸï¼Œä»è€Œæé«˜äº†æŒç»­å­¦ä¹ çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.21709', 'title': 'Why Attention Patterns Exist: A Unifying Temporal Perspective Analysis', 'url': 'https://huggingface.co/papers/2601.21709', 'abstract': 'Temporal Attention Pattern Predictability Analysis (TAPPA) provides a unified framework for understanding attention patterns in large language models by analyzing their mathematical formulations from a temporal perspective, distinguishing predictable from unpredictable patterns based on query self-similarity.  \t\t\t\t\tAI-generated summary \t\t\t\t Attention patterns play a crucial role in both training and inference of large language models (LLMs). Prior works have identified individual patterns such as retrieval heads, sink heads, and diagonal traces, yet these observations remain fragmented and lack a unifying explanation. To bridge this gap, we introduce Temporal Attention Pattern Predictability Analysis (TAPPA), a unifying framework that explains diverse attention patterns by analyzing their underlying mathematical formulations from a temporally continuous perspective. TAPPA both deepens the understanding of attention behavior and guides inference acceleration approaches. Specifically, TAPPA characterizes attention patterns as predictable patterns with clear regularities and unpredictable patterns that appear effectively random. Our analysis further reveals that this distinction can be explained by the degree of query self-similarity along the temporal dimension. Focusing on the predictable patterns, we further provide a detailed mathematical analysis of three representative cases through the joint effect of queries, keys, and Rotary Positional Embeddings (RoPE). We validate TAPPA by applying its insights to KV cache compression and LLM pruning tasks. Across these tasks, a simple metric motivated by TAPPA consistently improves performance over baseline methods. The code is available at https://github.com/MIRALab-USTC/LLM-TAPPA.', 'score': 0, 'issue_id': 874, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 29', 'zh': '1æœˆ29æ—¥'}, 'hash': '79cfcdca1b2494db', 'authors': ['Qingyue Yang', 'Jie Wang', 'Xing Li', 'Yinqi Bai', 'Xialiang Tong', 'Huiling Zhen', 'Jianye Hao', 'Mingxuan Yuan', 'Bin Li'], 'affiliations': ['Huawei Technologies Co., Ltd.', 'Tianjin University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2601.21709.jpg', 'data': {'categories': [], 'emoji': 'â°', 'ru': {'title': 'ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ĞµĞ´Ğ¸Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° TAPPA Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ñ‹Ğµ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ° Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ÑÑ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒÑ ÑĞ°Ğ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¸. Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², ĞºĞ»ÑÑ‡ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ğº RoPE Ğ½Ğ° Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ². ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ TAPPA Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¶Ğ°Ñ‚Ğ¸Ñ KV ĞºÑÑˆĞ° Ğ¸ Ğ¿Ñ€ÑƒĞ½Ğ¸Ğ½Ğ³Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ³Ğ´Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°, Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ².'}, 'en': {'title': 'Unlocking Predictability in Attention Patterns of Language Models', 'desc': 'Temporal Attention Pattern Predictability Analysis (TAPPA) introduces a new way to understand attention patterns in large language models (LLMs) by examining their mathematical structures over time. It categorizes these patterns into predictable and unpredictable types based on how similar the queries are to themselves over time. This framework not only enhances our understanding of how attention works but also aids in speeding up inference processes. By applying TAPPA, researchers can improve tasks like KV cache compression and LLM pruning, leading to better performance than traditional methods.'}, 'zh': {'title': 'æ­ç¤ºæ³¨æ„æ¨¡å¼çš„å¯é¢„æµ‹æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†æ—¶é—´æ³¨æ„æ¨¡å¼å¯é¢„æµ‹æ€§åˆ†æï¼ˆTAPPAï¼‰ï¼Œä¸ºç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ³¨æ„æ¨¡å¼æä¾›äº†ç»Ÿä¸€æ¡†æ¶ã€‚é€šè¿‡ä»æ—¶é—´è§’åº¦åˆ†ææ•°å­¦å…¬å¼ï¼ŒTAPPAåŒºåˆ†äº†å¯é¢„æµ‹å’Œä¸å¯é¢„æµ‹çš„æ³¨æ„æ¨¡å¼ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ³¨æ„æ¨¡å¼çš„å¯é¢„æµ‹æ€§ä¸æŸ¥è¯¢è‡ªç›¸ä¼¼åº¦æœ‰å…³ï¼Œå…·æœ‰æ˜æ˜¾è§„å¾‹çš„æ¨¡å¼è¢«è§†ä¸ºå¯é¢„æµ‹æ¨¡å¼ã€‚TAPPAçš„åˆ†æä¸ä»…åŠ æ·±äº†å¯¹æ³¨æ„è¡Œä¸ºçš„ç†è§£ï¼Œè¿˜ä¸ºæ¨ç†åŠ é€Ÿæ–¹æ³•æä¾›äº†æŒ‡å¯¼ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (7)', '#agi', '#alignment (4)', '#architecture (4)', '#audio (1)', '#benchmark (10)', '#cv (1)', '#data (2)', '#dataset (4)', '#diffusion (4)', '#ethics', '#games', '#graphs', '#hallucinations', '#healthcare', '#inference (2)', '#interpretability (1)', '#leakage', '#long_context (2)', '#low_resource', '#machine_translation', '#math (1)', '#multilingual (1)', '#multimodal (5)', '#open_source (9)', '#optimization (9)', '#plp (2)', '#rag (1)', '#reasoning (7)', '#rl (6)', '#rlhf (5)', '#robotics', '#science (2)', '#security (1)', '#small_models (2)', '#story_generation', '#survey', '#synthetic (1)', '#training (14)', '#transfer_learning (1)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2026-02-02 08:45',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2026-02-02 08:45')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2026-02-02 08:45')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    