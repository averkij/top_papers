
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 8 papers. May 15.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">15 мая</span> | <span id="title-articles-count">8 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-05-14.html">⬅️ <span id="prev-date">14.05</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-05-16.html">➡️ <span id="next-date">16.05</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-05.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'};
        let feedDateNext = {'ru': '16.05', 'en': '05/16', 'zh': '5月16日'};
        let feedDatePrev = {'ru': '14.05', 'en': '05/14', 'zh': '5月14日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2505.04410', 'title': 'DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception', 'url': 'https://huggingface.co/papers/2505.04410', 'abstract': "Dense visual prediction tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. While Vision-Language Models (VLMs) like CLIP have shown promise in open-vocabulary tasks, their direct application to dense prediction often leads to suboptimal performance due to limitations in local feature representation. In this work, we present our observation that CLIP's image tokens struggle to effectively aggregate information from spatially or semantically related regions, resulting in features that lack local discriminability and spatial consistency. To address this issue, we propose DeCLIP, a novel framework that enhances CLIP by decoupling the self-attention module to obtain ``content'' and ``context'' features respectively. The ``content'' features are aligned with image crop representations to improve local discriminability, while ``context'' features learn to retain the spatial correlations under the guidance of vision foundation models, such as DINO. Extensive experiments demonstrate that DeCLIP significantly outperforms existing methods across multiple open-vocabulary dense prediction tasks, including object detection and semantic segmentation. Code is available at magenta{https://github.com/xiaomoguhz/DeCLIP}.", 'score': 28, 'issue_id': 3774, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': '24fee436fe24f861', 'authors': ['Junjie Wang', 'Bin Chen', 'Yulin Li', 'Bin Kang', 'Yichi Chen', 'Zhuotao Tian'], 'affiliations': ['International Research Institute for Artificial Intelligence, HIT, Shenzhen', 'School of Computer Science and Technology, HIT, Shenzhen', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2505.04410.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#cv', '#optimization', '#open_source'], 'emoji': '🔍', 'ru': {'title': 'DeCLIP: Новый шаг к универсальному компьютерному зрению', 'desc': "Статья представляет новый подход DeCLIP для улучшения возможностей моделей компьютерного зрения в задачах плотного предсказания с открытым словарем. Авторы предлагают разделить модуль самовнимания CLIP на 'содержательные' и 'контекстные' признаки для повышения локальной различимости и пространственной согласованности. DeCLIP показывает значительное улучшение результатов в задачах обнаружения объектов и семантической сегментации по сравнению с существующими методами. Этот подход позволяет преодолеть ограничения предопределенных категорий в задачах компьютерного зрения."}, 'en': {'title': 'Enhancing Dense Visual Predictions with DeCLIP', 'desc': "This paper introduces DeCLIP, a new framework designed to improve dense visual prediction tasks by enhancing the capabilities of Vision-Language Models (VLMs) like CLIP. The authors identify that CLIP's image tokens fail to effectively gather information from related regions, leading to poor local feature representation. DeCLIP addresses this by separating the self-attention mechanism into 'content' and 'context' features, which improves local discriminability and maintains spatial relationships. The results show that DeCLIP outperforms existing methods in various open-vocabulary tasks such as object detection and semantic segmentation."}, 'zh': {'title': 'DeCLIP：提升视觉语言模型的密集预测能力', 'desc': '本论文提出了一种新的框架DeCLIP，旨在改善视觉语言模型CLIP在密集视觉预测任务中的表现。我们发现CLIP的图像标记在聚合空间或语义相关区域的信息时存在困难，导致特征缺乏局部可区分性和空间一致性。DeCLIP通过解耦自注意力模块，分别提取“内容”和“上下文”特征，从而提高局部可区分性并保持空间相关性。实验结果表明，DeCLIP在多个开放词汇密集预测任务中显著优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2505.09568', 'title': 'BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,\n  Training and Dataset', 'url': 'https://huggingface.co/papers/2505.09568', 'abstract': 'Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain underexplored. Motivated by the strong potential of autoregressive and diffusion models for high-quality generation and scalability, we conduct a comprehensive study of their use in unified multimodal settings, with emphasis on image representations, modeling objectives, and training strategies. Grounded in these investigations, we introduce a novel approach that employs a diffusion transformer to generate semantically rich CLIP image features, in contrast to conventional VAE-based representations. This design yields both higher training efficiency and improved generative quality. Furthermore, we demonstrate that a sequential pretraining strategy for unified models-first training on image understanding and subsequently on image generation-offers practical advantages by preserving image understanding capability while developing strong image generation ability. Finally, we carefully curate a high-quality instruction-tuning dataset BLIP3o-60k for image generation by prompting GPT-4o with a diverse set of captions covering various scenes, objects, human gestures, and more. Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models. BLIP3-o achieves superior performance across most of the popular benchmarks spanning both image understanding and generation tasks. To facilitate future research, we fully open-source our models, including code, model weights, training scripts, and pretraining and instruction tuning datasets.', 'score': 16, 'issue_id': 3768, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 мая', 'en': 'May 14', 'zh': '5月14日'}, 'hash': '822f8dd79d39211b', 'authors': ['Jiuhai Chen', 'Zhiyang Xu', 'Xichen Pan', 'Yushi Hu', 'Can Qin', 'Tom Goldstein', 'Lifu Huang', 'Tianyi Zhou', 'Saining Xie', 'Silvio Savarese', 'Le Xue', 'Caiming Xiong', 'Ran Xu'], 'affiliations': ['New York University', 'Salesforce Research', 'UC Davis', 'University of Maryland', 'University of Washington', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2505.09568.jpg', 'data': {'categories': ['#dataset', '#diffusion', '#open_source', '#multimodal', '#training', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'Объединение понимания и генерации изображений с помощью диффузионных трансформеров', 'desc': 'Статья представляет новый подход к объединению понимания и генерации изображений в мультимодальных моделях. Авторы предлагают использовать диффузионный трансформер для генерации семантически богатых CLIP-признаков изображений вместо традиционных VAE-представлений. Исследование также показывает преимущества последовательного предобучения: сначала на задаче понимания изображений, затем на генерации. В результате разработана модель BLIP3-o, достигающая высоких результатов в задачах как понимания, так и генерации изображений.'}, 'en': {'title': 'Unifying Image Understanding and Generation with BLIP3-o', 'desc': 'This paper explores the integration of image understanding and generation in multimodal models, focusing on the use of autoregressive and diffusion models. The authors propose a new architecture that utilizes a diffusion transformer to create high-quality CLIP image features, which enhances both training efficiency and generative quality compared to traditional VAE methods. They introduce a sequential pretraining strategy that first develops image understanding before transitioning to image generation, ensuring that both capabilities are effectively preserved. Additionally, they present a curated dataset, BLIP3o-60k, for instruction tuning, which supports the development of their state-of-the-art unified multimodal model, BLIP3-o, achieving top performance on various benchmarks.'}, 'zh': {'title': '统一图像理解与生成的创新模型', 'desc': '本论文探讨了图像理解与生成的统一模型，强调了自回归和扩散模型在高质量生成中的潜力。我们提出了一种新方法，使用扩散变换器生成语义丰富的CLIP图像特征，提升了训练效率和生成质量。通过先进行图像理解训练，再进行图像生成训练的顺序预训练策略，保持了图像理解能力的同时增强了生成能力。最后，我们创建了高质量的指令调优数据集BLIP3o-60k，以支持图像生成任务的研究。'}}}, {'id': 'https://huggingface.co/papers/2505.09343', 'title': 'Insights into DeepSeek-V3: Scaling Challenges and Reflections on\n  Hardware for AI Architectures', 'url': 'https://huggingface.co/papers/2505.09343', 'abstract': "The rapid scaling of large language models (LLMs) has unveiled critical limitations in current hardware architectures, including constraints in memory capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3, trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model co-design can effectively address these challenges, enabling cost-efficient training and inference at scale. This paper presents an in-depth analysis of the DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting key innovations such as Multi-head Latent Attention (MLA) for enhanced memory efficiency, Mixture of Experts (MoE) architectures for optimized computation-communication trade-offs, FP8 mixed-precision training to unlock the full potential of hardware capabilities, and a Multi-Plane Network Topology to minimize cluster-level network overhead. Building on the hardware bottlenecks encountered during DeepSeek-V3's development, we engage in a broader discussion with academic and industry peers on potential future hardware directions, including precise low-precision computation units, scale-up and scale-out convergence, and innovations in low-latency communication fabrics. These insights underscore the critical role of hardware and model co-design in meeting the escalating demands of AI workloads, offering a practical blueprint for innovation in next-generation AI systems.", 'score': 12, 'issue_id': 3773, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 мая', 'en': 'May 14', 'zh': '5月14日'}, 'hash': '3c249078ec32a334', 'authors': ['Chenggang Zhao', 'Chengqi Deng', 'Chong Ruan', 'Damai Dai', 'Huazuo Gao', 'Jiashi Li', 'Liyue Zhang', 'Panpan Huang', 'Shangyan Zhou', 'Shirong Ma', 'Wenfeng Liang', 'Ying He', 'Yuqing Wang', 'Yuxuan Liu', 'Y. X. Wei'], 'affiliations': ['DeepSeek-AI Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.09343.jpg', 'data': {'categories': ['#training', '#inference', '#architecture', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Совместное проектирование моделей и оборудования для масштабирования ИИ', 'desc': 'Статья описывает архитектуру модели DeepSeek-V3/R1 и инфраструктуру ИИ, разработанные для преодоления ограничений современного аппаратного обеспечения при обучении больших языковых моделей (LLM). Авторы представляют ключевые инновации, включая Multi-head Latent Attention (MLA) для повышения эффективности использования памяти и архитектуру Mixture of Experts (MoE) для оптимизации баланса между вычислениями и коммуникацией. В работе также обсуждается применение смешанной точности FP8 для максимального использования возможностей оборудования и использование многоплоскостной сетевой топологии для минимизации накладных расходов на уровне кластера. На основе опыта разработки DeepSeek-V3 авторы предлагают направления для будущих исследований в области аппаратного обеспечения для ИИ.'}, 'en': {'title': 'Innovating AI: Bridging Hardware and Model Design for Scalable Solutions', 'desc': 'This paper discusses the limitations of current hardware when training large language models (LLMs) and introduces DeepSeek-V3 as a solution. It emphasizes hardware-aware model co-design, which improves memory efficiency and computational performance. Key innovations include Multi-head Latent Attention for better memory use, Mixture of Experts for efficient computation, and FP8 mixed-precision training to maximize hardware capabilities. The authors also explore future hardware advancements needed to support the growing demands of AI workloads, highlighting the importance of integrating hardware and model design.'}, 'zh': {'title': '硬件与模型共同设计，推动AI创新', 'desc': '这篇论文讨论了大型语言模型（LLMs）在硬件架构上的限制，包括内存容量、计算效率和互连带宽等问题。DeepSeek-V3模型在2048个NVIDIA H800 GPU上训练，展示了硬件感知模型共同设计如何有效解决这些挑战。论文分析了DeepSeek-V3/R1模型架构及其AI基础设施，介绍了多头潜在注意力（MLA）、专家混合（MoE）架构和FP8混合精度训练等创新。最后，作者与学术界和工业界同行探讨了未来硬件的发展方向，强调了硬件与模型共同设计在满足AI工作负载需求中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2505.09358', 'title': 'Marigold: Affordable Adaptation of Diffusion-Based Image Generators for\n  Image Analysis', 'url': 'https://huggingface.co/papers/2505.09358', 'abstract': "The success of deep learning in computer vision over the past decade has hinged on large labeled datasets and strong pretrained models. In data-scarce settings, the quality of these pretrained models becomes crucial for effective transfer learning. Image classification and self-supervised learning have traditionally been the primary methods for pretraining CNNs and transformer-based architectures. Recently, the rise of text-to-image generative models, particularly those using denoising diffusion in a latent space, has introduced a new class of foundational models trained on massive, captioned image datasets. These models' ability to generate realistic images of unseen content suggests they possess a deep understanding of the visual world. In this work, we present Marigold, a family of conditional generative models and a fine-tuning protocol that extracts the knowledge from pretrained latent diffusion models like Stable Diffusion and adapts them for dense image analysis tasks, including monocular depth estimation, surface normals prediction, and intrinsic decomposition. Marigold requires minimal modification of the pre-trained latent diffusion model's architecture, trains with small synthetic datasets on a single GPU over a few days, and demonstrates state-of-the-art zero-shot generalization. Project page: https://marigoldcomputervision.github.io", 'score': 7, 'issue_id': 3777, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 мая', 'en': 'May 14', 'zh': '5月14日'}, 'hash': '88afa5d56831ceb4', 'authors': ['Bingxin Ke', 'Kevin Qu', 'Tianfu Wang', 'Nando Metzger', 'Shengyu Huang', 'Bo Li', 'Anton Obukhov', 'Konrad Schindler'], 'affiliations': ['Photogrammetry and Remote Sensing Laboratory, ETH Zurich, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2505.09358.jpg', 'data': {'categories': ['#cv', '#dataset', '#diffusion', '#synthetic', '#transfer_learning', '#training'], 'emoji': '🌼', 'ru': {'title': 'Marigold: Раскрытие потенциала генеративных моделей для плотного анализа изображений', 'desc': 'Статья представляет Marigold - семейство условных генеративных моделей и протокол дообучения, которые извлекают знания из предобученных моделей латентной диффузии, таких как Stable Diffusion. Marigold адаптирует эти модели для задач плотного анализа изображений, включая оценку монокулярной глубины, предсказание нормалей поверхности и внутреннюю декомпозицию. Модель требует минимальной модификации архитектуры предобученной модели латентной диффузии и обучается на небольших синтетических наборах данных на одном GPU в течение нескольких дней. Marigold демонстрирует передовую обобщающую способность в режиме zero-shot.'}, 'en': {'title': 'Unlocking Image Analysis with Pretrained Generative Models', 'desc': 'This paper introduces Marigold, a set of conditional generative models designed to leverage pretrained latent diffusion models for various dense image analysis tasks. By fine-tuning these models, Marigold can effectively adapt to tasks like monocular depth estimation and surface normals prediction with minimal changes to the original architecture. The approach allows for training on small synthetic datasets, making it efficient and accessible for users with limited resources. Notably, Marigold achieves impressive zero-shot generalization, showcasing its potential in data-scarce environments.'}, 'zh': {'title': 'Marigold：高效的图像分析生成模型', 'desc': '深度学习在计算机视觉领域的成功依赖于大量标注数据集和强大的预训练模型。在数据稀缺的情况下，这些预训练模型的质量对有效的迁移学习至关重要。最近，文本到图像的生成模型，特别是使用去噪扩散的潜在空间模型，开创了一类新的基础模型，这些模型在大量带注释的图像数据集上进行训练。本文介绍了Marigold，一个条件生成模型的家族及其微调协议，能够提取预训练潜在扩散模型的知识，并将其适应于密集图像分析任务。'}}}, {'id': 'https://huggingface.co/papers/2505.07849', 'title': 'SweRank: Software Issue Localization with Code Ranking', 'url': 'https://huggingface.co/papers/2505.07849', 'abstract': "Software issue localization, the task of identifying the precise code locations (files, classes, or functions) relevant to a natural language issue description (e.g., bug report, feature request), is a critical yet time-consuming aspect of software development. While recent LLM-based agentic approaches demonstrate promise, they often incur significant latency and cost due to complex multi-step reasoning and relying on closed-source LLMs. Alternatively, traditional code ranking models, typically optimized for query-to-code or code-to-code retrieval, struggle with the verbose and failure-descriptive nature of issue localization queries. To bridge this gap, we introduce SweRank, an efficient and effective retrieve-and-rerank framework for software issue localization. To facilitate training, we construct SweLoc, a large-scale dataset curated from public GitHub repositories, featuring real-world issue descriptions paired with corresponding code modifications. Empirical results on SWE-Bench-Lite and LocBench show that SweRank achieves state-of-the-art performance, outperforming both prior ranking models and costly agent-based systems using closed-source LLMs like Claude-3.5. Further, we demonstrate SweLoc's utility in enhancing various existing retriever and reranker models for issue localization, establishing the dataset as a valuable resource for the community.", 'score': 3, 'issue_id': 3777, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': 'b9c609d9756c1056', 'authors': ['Revanth Gangi Reddy', 'Tarun Suresh', 'JaeHyeok Doo', 'Ye Liu', 'Xuan Phi Nguyen', 'Yingbo Zhou', 'Semih Yavuz', 'Caiming Xiong', 'Heng Ji', 'Shafiq Joty'], 'affiliations': ['KAIST AI', 'Salesforce Research', 'University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.07849.jpg', 'data': {'categories': ['#data', '#benchmark', '#optimization', '#dataset', '#survey', '#agents'], 'emoji': '🔍', 'ru': {'title': 'SweRank: эффективная локализация проблем в коде с помощью извлечения и переранжирования', 'desc': 'SweRank - это эффективная система для локализации проблем в программном обеспечении, использующая подход извлечения и переранжирования. Авторы создали большой датасет SweLoc, содержащий описания реальных проблем и соответствующие изменения кода из репозиториев GitHub. Эмпирические результаты показывают, что SweRank превосходит как предыдущие модели ранжирования, так и дорогостоящие системы на основе агентов, использующие закрытые языковые модели. SweLoc также демонстрирует свою полезность для улучшения существующих моделей извлечения и переранжирования в задаче локализации проблем.'}, 'en': {'title': 'SweRank: Efficient Software Issue Localization with SweLoc Dataset', 'desc': 'This paper presents SweRank, a new framework designed to improve software issue localization by efficiently retrieving and re-ranking code relevant to natural language issue descriptions. Unlike traditional models that struggle with the complexity of verbose queries, SweRank leverages a large-scale dataset called SweLoc, which contains real-world issue descriptions and their corresponding code changes. The empirical results indicate that SweRank outperforms existing models, including those based on costly closed-source large language models, in terms of accuracy and efficiency. This work not only introduces a novel approach to issue localization but also provides a valuable dataset for future research in the field.'}, 'zh': {'title': '高效软件问题定位的新方法', 'desc': '软件问题定位是识别与自然语言问题描述（如错误报告、功能请求）相关的代码位置（文件、类或函数）的任务，这在软件开发中至关重要但耗时。尽管最近基于大型语言模型（LLM）的代理方法显示出潜力，但由于复杂的多步骤推理和依赖于封闭源LLM，往往会导致显著的延迟和成本。传统的代码排名模型通常针对查询到代码或代码到代码的检索进行优化，但在处理冗长和描述性失败的定位查询时表现不佳。为了解决这个问题，我们提出了SweRank，一个高效且有效的软件问题定位检索与重排名框架，并构建了SweLoc，一个来自公共GitHub库的大规模数据集，包含真实问题描述及相应的代码修改。'}}}, {'id': 'https://huggingface.co/papers/2505.08455', 'title': 'VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large\n  Video Language Models', 'url': 'https://huggingface.co/papers/2505.08455', 'abstract': 'Despite recent advances in video understanding, the capabilities of Large Video Language Models (LVLMs) to perform video-based causal reasoning remains underexplored, largely due to the absence of relevant and dedicated benchmarks for evaluating causal reasoning in visually grounded and goal-driven settings. To fill this gap, we introduce a novel benchmark named Video-based long-form Causal Reasoning (VCRBench). We create VCRBench using procedural videos of simple everyday activities, where the steps are deliberately shuffled with each clip capturing a key causal event, to test whether LVLMs can identify, reason about, and correctly sequence the events needed to accomplish a specific goal. Moreover, the benchmark is carefully designed to prevent LVLMs from exploiting linguistic shortcuts, as seen in multiple-choice or binary QA formats, while also avoiding the challenges associated with evaluating open-ended QA. Our evaluation of state-of-the-art LVLMs on VCRBench suggests that these models struggle with video-based long-form causal reasoning, primarily due to their difficulty in modeling long-range causal dependencies directly from visual observations. As a simple step toward enabling such capabilities, we propose Recognition-Reasoning Decomposition (RRD), a modular approach that breaks video-based causal reasoning into two sub-tasks of video recognition and causal reasoning. Our experiments on VCRBench show that RRD significantly boosts accuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysis reveals interesting insights, for instance, that LVLMs primarily rely on language knowledge for complex video-based long-form causal reasoning tasks.', 'score': 2, 'issue_id': 3773, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 мая', 'en': 'May 13', 'zh': '5月13日'}, 'hash': 'b7bc69bb40029690', 'authors': ['Pritam Sarkar', 'Ali Etemad'], 'affiliations': ['Queens University, Canada', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.08455.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#long_context', '#video'], 'emoji': '🎬', 'ru': {'title': 'Новый бенчмарк для оценки причинно-следственного рассуждения в видео-языковых моделях', 'desc': 'Исследователи представили новый бенчмарк VCRBench для оценки способностей Больших Видео-Языковых Моделей (LVLM) к причинно-следственному рассуждению на основе видео. VCRBench использует процедурные видео повседневных действий с перемешанными шагами для проверки способности моделей идентифицировать и упорядочивать ключевые причинные события. Оценка современных LVLM на VCRBench показала их трудности с моделированием долгосрочных причинно-следственных зависимостей напрямую из визуальных наблюдений. Авторы предложили модульный подход Recognition-Reasoning Decomposition (RRD), который значительно повысил точность на VCRBench.'}, 'en': {'title': 'Enhancing Video Causal Reasoning with RRD', 'desc': 'This paper addresses the limitations of Large Video Language Models (LVLMs) in performing causal reasoning with videos. It introduces a new benchmark called Video-based long-form Causal Reasoning (VCRBench), which tests LVLMs on their ability to identify and sequence causal events in procedural videos. The benchmark is designed to challenge models by preventing them from using linguistic shortcuts and focuses on long-range causal dependencies. The authors propose a method called Recognition-Reasoning Decomposition (RRD) that improves the performance of LVLMs on VCRBench by separating the tasks of video recognition and causal reasoning, resulting in significant accuracy gains.'}, 'zh': {'title': '提升视频因果推理能力的关键', 'desc': '尽管视频理解技术有所进步，但大型视频语言模型（LVLMs）在视频基础的因果推理方面的能力仍未得到充分探索。为了解决这个问题，我们提出了一个新的基准测试，名为视频基础的长形式因果推理（VCRBench），通过对日常活动的视频进行处理，测试LVLMs能否识别、推理并正确排序实现特定目标所需的事件。我们还提出了一种模块化的方法，称为识别-推理分解（RRD），将视频基础的因果推理分为视频识别和因果推理两个子任务，从而显著提高了模型的准确性。我们的分析表明，LVLMs在复杂的长形式因果推理任务中主要依赖语言知识。'}}}, {'id': 'https://huggingface.co/papers/2505.09439', 'title': 'Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?', 'url': 'https://huggingface.co/papers/2505.09439', 'abstract': 'We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni, on an audio question answering dataset with the reinforcement learning method GRPO. This leads to new State-of-the-Art performance on the recent MMAU benchmark. Omni-R1 achieves the highest accuracies on the sounds, music, speech, and overall average categories, both on the Test-mini and Test-full splits. To understand the performance improvement, we tested models both with and without audio and found that much of the performance improvement from GRPO could be attributed to better text-based reasoning. We also made a surprising discovery that fine-tuning without audio on a text-only dataset was effective at improving the audio-based performance.', 'score': 1, 'issue_id': 3778, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 мая', 'en': 'May 14', 'zh': '5月14日'}, 'hash': '95b580ea2dcf9967', 'authors': ['Andrew Rouditchenko', 'Saurabhchand Bhati', 'Edson Araujo', 'Samuel Thomas', 'Hilde Kuehne', 'Rogerio Feris', 'James Glass'], 'affiliations': ['Goethe University of Frankfurt', 'IBM Research AI', 'MIT CSAIL', 'MIT-IBM Watson AI Lab', 'Tuebingen AI Center/University of Tuebingen'], 'pdf_title_img': 'assets/pdf/title_img/2505.09439.jpg', 'data': {'categories': ['#training', '#multimodal', '#rl', '#reasoning', '#optimization', '#benchmark', '#rag'], 'emoji': '🎧', 'ru': {'title': 'Революция в аудио-ИИ: Omni-R1 покоряет новые вершины мультимодального анализа', 'desc': 'Исследователи представили Omni-R1 - мультимодальную языковую модель, улучшенную с помощью обучения с подкреплением. Модель достигла наилучших результатов в бенчмарке MMAU по распознаванию звуков, музыки и речи. Анализ показал, что улучшение производительности в основном связано с более качественным текстовым рассуждением. Интересно, что дообучение на текстовых данных без аудио также повысило эффективность модели в аудиозадачах.'}, 'en': {'title': 'Enhancing Audio Understanding through Text-Based Reasoning', 'desc': "The paper introduces Omni-R1, a model that enhances the Qwen2.5-Omni multi-modal large language model (LLM) by fine-tuning it on an audio question answering dataset using the GRPO reinforcement learning method. This approach achieves state-of-the-art results on the MMAU benchmark, excelling in categories such as sounds, music, and speech. The authors found that the performance gains were largely due to improved text-based reasoning capabilities, even when audio data was not used during fine-tuning. Interestingly, they discovered that training on a text-only dataset could still boost the model's performance on audio tasks."}, 'zh': {'title': 'Omni-R1：音频问答的新突破', 'desc': '我们提出了Omni-R1，它在音频问答数据集上对最新的多模态大语言模型Qwen2.5-Omni进行了微调，采用了强化学习方法GRPO。这使得Omni-R1在最近的MMAU基准测试中达到了新的最先进性能。Omni-R1在声音、音乐、语音和整体平均类别上都取得了最高的准确率，无论是在Test-mini还是Test-full分割上。我们还发现，在没有音频的情况下进行微调，竟然也能有效提高音频相关的性能，这表明文本推理能力的提升对整体表现有重要影响。'}}}, {'id': 'https://huggingface.co/papers/2505.04793', 'title': 'DetReIDX: A Stress-Test Dataset for Real-World UAV-Based Person\n  Recognition', 'url': 'https://huggingface.co/papers/2505.04793', 'abstract': 'Person reidentification (ReID) technology has been considered to perform relatively well under controlled, ground-level conditions, but it breaks down when deployed in challenging real-world settings. Evidently, this is due to extreme data variability factors such as resolution, viewpoint changes, scale variations, occlusions, and appearance shifts from clothing or session drifts. Moreover, the publicly available data sets do not realistically incorporate such kinds and magnitudes of variability, which limits the progress of this technology. This paper introduces DetReIDX, a large-scale aerial-ground person dataset, that was explicitly designed as a stress test to ReID under real-world conditions. DetReIDX is a multi-session set that includes over 13 million bounding boxes from 509 identities, collected in seven university campuses from three continents, with drone altitudes between 5.8 and 120 meters. More important, as a key novelty, DetReIDX subjects were recorded in (at least) two sessions on different days, with changes in clothing, daylight and location, making it suitable to actually evaluate long-term person ReID. Plus, data were annotated from 16 soft biometric attributes and multitask labels for detection, tracking, ReID, and action recognition. In order to provide empirical evidence of DetReIDX usefulness, we considered the specific tasks of human detection and ReID, where SOTA methods catastrophically degrade performance (up to 80% in detection accuracy and over 70% in Rank-1 ReID) when exposed to DetReIDXs conditions. The dataset, annotations, and official evaluation protocols are publicly available at https://www.it.ubi.pt/DetReIDX/', 'score': 1, 'issue_id': 3778, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'}, 'hash': '76d34246ac682dfd', 'pdf_title_img': 'assets/pdf/title_img/2505.04793.jpg', 'data': {'categories': ['#cv', '#dataset', '#synthetic'], 'emoji': '🎯', 'ru': {'title': 'DetReIDX: Стресс-тест для реидентификации людей в реальном мире', 'desc': 'Статья представляет DetReIDX - крупномасштабный набор данных для задачи реидентификации людей в воздушно-наземных условиях. Набор данных включает более 13 миллионов ограничивающих рамок для 509 личностей, собранных в семи университетских кампусах на трех континентах, с высотой съемки от 5,8 до 120 метров. DetReIDX специально разработан для стресс-тестирования алгоритмов реидентификации в реальных условиях, включая изменения одежды, освещения и местоположения между сессиями. Эксперименты показали, что современные методы обнаружения и реидентификации людей значительно ухудшают свою производительность при работе с данными DetReIDX.'}, 'en': {'title': 'DetReIDX: A Game-Changer for Real-World Person ReID Challenges', 'desc': 'This paper presents DetReIDX, a new dataset designed to improve person reidentification (ReID) technology in real-world scenarios. It addresses the limitations of existing datasets by incorporating significant variability factors such as changes in clothing, viewpoint, and environmental conditions. DetReIDX includes over 13 million bounding boxes from 509 identities, collected across multiple sessions in diverse locations, making it a comprehensive resource for evaluating ReID systems. The authors demonstrate that current state-of-the-art methods struggle significantly under the conditions presented by DetReIDX, highlighting the need for robust solutions in person reidentification.'}, 'zh': {'title': 'DetReIDX：真实世界行人重识别的新挑战', 'desc': '本文介绍了一种名为DetReIDX的大规模空中-地面行人重识别（ReID）数据集，旨在测试ReID技术在真实世界条件下的表现。该数据集包含来自509个身份的1300多万个边界框，数据收集自三个大洲的七个大学校园，涵盖了不同的服装、光照和位置变化。DetReIDX的独特之处在于，它记录了至少两个不同日期的会话，能够有效评估长期行人重识别的能力。研究表明，当前最先进的方法在DetReIDX条件下的表现显著下降，检测准确率下降高达80%，Rank-1 ReID下降超过70%。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (1)', '#agi', '#alignment', '#architecture (3)', '#audio', '#benchmark (3)', '#cv (3)', '#data (1)', '#dataset (4)', '#diffusion (2)', '#ethics', '#games', '#graphs', '#hallucinations', '#healthcare', '#inference (1)', '#interpretability', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (3)', '#open_source (2)', '#optimization (4)', '#plp', '#rag (1)', '#reasoning (2)', '#rl (1)', '#rlhf', '#robotics', '#science', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic (2)', '#training (4)', '#transfer_learning (1)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-05-15 12:21',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-05-15 12:21')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-05-15 12:21')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    