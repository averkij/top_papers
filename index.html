
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 27 papers. March 27.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">27 Ğ¼Ğ°Ñ€Ñ‚Ğ°</span> | <span id="title-articles-count">27 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-03-26.html">â¬…ï¸ <span id="prev-date">26.03</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-03-28.html">â¡ï¸ <span id="next-date">28.03</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-03.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'};
        let feedDateNext = {'ru': '28.03', 'en': '03/28', 'zh': '3æœˆ28æ—¥'};
        let feedDatePrev = {'ru': '26.03', 'en': '03/26', 'zh': '3æœˆ26æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.20215', 'title': 'Qwen2.5-Omni Technical Report', 'url': 'https://huggingface.co/papers/2503.20215', 'abstract': "In this report, we present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. To synchronize the timestamps of video inputs with audio, we organize the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose Thinker-Talker architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni is comparable with the similarly sized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni's performance in end-to-end speech instruction following is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming Talker outperforms most existing streaming and non-streaming alternatives in robustness and naturalness.", 'score': 55, 'issue_id': 2924, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': 'dd7a3c8e8564b973', 'authors': ['Jin Xu', 'Zhifang Guo', 'Jinzheng He', 'Hangrui Hu', 'Ting He', 'Shuai Bai', 'Keqin Chen', 'Jialin Wang', 'Yang Fan', 'Kai Dang', 'Bin Zhang', 'Xiong Wang', 'Yunfei Chu', 'Junyang Lin'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.20215.jpg', 'data': {'categories': ['#architecture', '#agi', '#benchmark', '#multimodal', '#video', '#games', '#audio'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Qwen2.5-Omni: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'Qwen2.5-Omni - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¸ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ² Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¼ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ»Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ´Ğ»Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ TMRoPE Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Thinker-Talker Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ñ€ĞµÑ‡ÑŒ Ğ±ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¼ĞµÑ…. Qwen2.5-Omni Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Omni-Bench Ğ¸ MMLU.'}, 'en': {'title': 'Streamlining Multimodal Interaction with Qwen2.5-Omni', 'desc': 'Qwen2.5-Omni is a cutting-edge multimodal model that can process and generate responses across various formats, including text, images, audio, and video. It employs a unique block-wise processing method for audio and visual data to facilitate real-time streaming. The model features a dual architecture, where the Thinker generates text and the Talker produces audio, ensuring smooth interaction between the two. With innovative techniques like TMRoPE for timestamp alignment and a sliding-window DiT for audio decoding, Qwen2.5-Omni sets new benchmarks in multimodal performance and speech generation.'}, 'zh': {'title': 'å¤šæ¨¡æ€æµå¼ç”Ÿæˆçš„æœªæ¥', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Qwen2.5-Omniï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç­‰å¤šç§è¾“å…¥ï¼ŒåŒæ—¶ç”Ÿæˆæ–‡æœ¬å’Œè‡ªç„¶è¯­éŸ³å“åº”ã€‚ä¸ºäº†å®ç°å¤šæ¨¡æ€ä¿¡æ¯çš„æµå¼å¤„ç†ï¼ŒéŸ³é¢‘å’Œè§†è§‰ç¼–ç å™¨é‡‡ç”¨äº†å—å¤„ç†çš„æ–¹æ³•ï¼Œå¹¶é€šè¿‡ä¸€ç§æ–°é¢–çš„ä½ç½®åµŒå…¥æ–¹æ³•TMRoPEæ¥åŒæ­¥éŸ³é¢‘å’Œè§†é¢‘çš„æ—¶é—´æˆ³ã€‚è¯¥æ¨¡å‹çš„Thinker-Talkeræ¶æ„ä½¿å¾—æ–‡æœ¬ç”Ÿæˆå’Œè¯­éŸ³ç”Ÿæˆå¯ä»¥å¹¶è¡Œè¿›è¡Œï¼Œé¿å…äº†ä¸¤è€…ä¹‹é—´çš„å¹²æ‰°ã€‚Qwen2.5-Omniåœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨æµå¼è¯­éŸ³ç”Ÿæˆæ–¹é¢ï¼Œå…¶æ€§èƒ½ä¼˜äºå¤§å¤šæ•°ç°æœ‰çš„æ›¿ä»£æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19757', 'title': 'Dita: Scaling Diffusion Transformer for Generalist\n  Vision-Language-Action Policy', 'url': 'https://huggingface.co/papers/2503.19757', 'abstract': "While recent vision-language-action models trained on diverse robot datasets exhibit promising generalization capabilities with limited in-domain data, their reliance on compact action heads to predict discretized or continuous actions constrains adaptability to heterogeneous action spaces. We present Dita, a scalable framework that leverages Transformer architectures to directly denoise continuous action sequences through a unified multimodal diffusion process. Departing from prior methods that condition denoising on fused embeddings via shallow networks, Dita employs in-context conditioning -- enabling fine-grained alignment between denoised actions and raw visual tokens from historical observations. This design explicitly models action deltas and environmental nuances. By scaling the diffusion action denoiser alongside the Transformer's scalability, Dita effectively integrates cross-embodiment datasets across diverse camera perspectives, observation scenes, tasks, and action spaces. Such synergy enhances robustness against various variances and facilitates the successful execution of long-horizon tasks. Evaluations across extensive benchmarks demonstrate state-of-the-art or comparative performance in simulation. Notably, Dita achieves robust real-world adaptation to environmental variances and complex long-horizon tasks through 10-shot finetuning, using only third-person camera inputs. The architecture establishes a versatile, lightweight and open-source baseline for generalist robot policy learning. Project Page: https://robodita.github.io.", 'score': 39, 'issue_id': 2920, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': 'f481410d892c371a', 'authors': ['Zhi Hou', 'Tianyi Zhang', 'Yuwen Xiong', 'Haonan Duan', 'Hengjun Pu', 'Ronglei Tong', 'Chengyang Zhao', 'Xizhou Zhu', 'Yu Qiao', 'Jifeng Dai', 'Yuntao Chen'], 'affiliations': ['Center for Artificial Intelligence and Robotics, HKISI, CAS', 'College of Computer Science and Technology, Zhejiang University', 'MMLab, The Chinese University of Hong Kong', 'Peking University', 'SenseTime Research', 'Shanghai AI Lab', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.19757.jpg', 'data': {'categories': ['#cv', '#open_source', '#multimodal', '#architecture', '#diffusion', '#agents', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Dita - Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ° Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Dita Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸ÑĞ¼ ÑÑ€ĞµĞ´Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Dita ĞºĞ°Ğº Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ Ğ¿Ñ€Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼.'}, 'en': {'title': 'Dita: Transforming Robot Action Learning with Multimodal Diffusion', 'desc': 'The paper introduces Dita, a new framework that improves how robots learn to perform actions by using advanced Transformer models. Unlike previous methods that used simple networks to predict actions, Dita employs a sophisticated approach called in-context conditioning, which helps align actions more closely with visual information from past experiences. This allows Dita to effectively handle a wide range of actions and environments, making it adaptable to different tasks and camera views. The results show that Dita not only performs well in simulations but also adapts successfully to real-world scenarios with minimal additional training.'}, 'zh': {'title': 'Ditaï¼šæå‡æœºå™¨äººé€‚åº”èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDitaçš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æœºå™¨äººåœ¨å¤šæ ·åŒ–åŠ¨ä½œç©ºé—´ä¸­çš„é€‚åº”èƒ½åŠ›ã€‚Ditaåˆ©ç”¨Transformeræ¶æ„ï¼Œé€šè¿‡ç»Ÿä¸€çš„å¤šæ¨¡æ€æ‰©æ•£è¿‡ç¨‹ç›´æ¥å»å™ªè¿ç»­åŠ¨ä½œåºåˆ—ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸Šä¸‹æ–‡æ¡ä»¶åŒ–å®ç°äº†å»å™ªåŠ¨ä½œä¸å†å²è§‚å¯Ÿçš„åŸå§‹è§†è§‰æ ‡è®°ä¹‹é—´çš„ç²¾ç»†å¯¹é½ï¼Œä»è€Œæ›´å¥½åœ°å»ºæ¨¡åŠ¨ä½œå˜åŒ–å’Œç¯å¢ƒç»†èŠ‚ã€‚Ditaåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé€‚åº”çœŸå®ä¸–ç•Œçš„ç¯å¢ƒå˜åŒ–ï¼Œå¹¶æˆåŠŸæ‰§è¡Œå¤æ‚çš„é•¿æœŸä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20314', 'title': 'Wan: Open and Advanced Large-Scale Video Generative Models', 'url': 'https://huggingface.co/papers/2503.20314', 'abstract': "This report presents Wan, a comprehensive and open suite of video foundation models designed to push the boundaries of video generation. Built upon the mainstream diffusion transformer paradigm, Wan achieves significant advancements in generative capabilities through a series of innovations, including our novel VAE, scalable pre-training strategies, large-scale data curation, and automated evaluation metrics. These contributions collectively enhance the model's performance and versatility. Specifically, Wan is characterized by four key features: Leading Performance: The 14B model of Wan, trained on a vast dataset comprising billions of images and videos, demonstrates the scaling laws of video generation with respect to both data and model size. It consistently outperforms the existing open-source models as well as state-of-the-art commercial solutions across multiple internal and external benchmarks, demonstrating a clear and significant performance superiority. Comprehensiveness: Wan offers two capable models, i.e., 1.3B and 14B parameters, for efficiency and effectiveness respectively. It also covers multiple downstream applications, including image-to-video, instruction-guided video editing, and personal video generation, encompassing up to eight tasks. Consumer-Grade Efficiency: The 1.3B model demonstrates exceptional resource efficiency, requiring only 8.19 GB VRAM, making it compatible with a wide range of consumer-grade GPUs. Openness: We open-source the entire series of Wan, including source code and all models, with the goal of fostering the growth of the video generation community. This openness seeks to significantly expand the creative possibilities of video production in the industry and provide academia with high-quality video foundation models. All the code and models are available at https://github.com/Wan-Video/Wan2.1.", 'score': 28, 'issue_id': 2920, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': 'e9770d8e9d313979', 'authors': ['WanTeam', ':', 'Ang Wang', 'Baole Ai', 'Bin Wen', 'Chaojie Mao', 'Chen-Wei Xie', 'Di Chen', 'Feiwu Yu', 'Haiming Zhao', 'Jianxiao Yang', 'Jianyuan Zeng', 'Jiayu Wang', 'Jingfeng Zhang', 'Jingren Zhou', 'Jinkai Wang', 'Jixuan Chen', 'Kai Zhu', 'Kang Zhao', 'Keyu Yan', 'Lianghua Huang', 'Mengyang Feng', 'Ningyi Zhang', 'Pandeng Li', 'Pingyu Wu', 'Ruihang Chu', 'Ruili Feng', 'Shiwei Zhang', 'Siyang Sun', 'Tao Fang', 'Tianxing Wang', 'Tianyi Gui', 'Tingyu Weng', 'Tong Shen', 'Wei Lin', 'Wei Wang', 'Wei Wang', 'Wenmeng Zhou', 'Wente Wang', 'Wenting Shen', 'Wenyuan Yu', 'Xianzhong Shi', 'Xiaoming Huang', 'Xin Xu', 'Yan Kou', 'Yangyu Lv', 'Yifei Li', 'Yijing Liu', 'Yiming Wang', 'Yingya Zhang', 'Yitong Huang', 'Yong Li', 'You Wu', 'Yu Liu', 'Yulin Pan', 'Yun Zheng', 'Yuntao Hong', 'Yupeng Shi', 'Yutong Feng', 'Zeyinzi Jiang', 'Zhen Han', 'Zhi-Fan Wu', 'Ziyu Liu'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.20314.jpg', 'data': {'categories': ['#video', '#open_source', '#multimodal', '#architecture', '#diffusion', '#benchmark', '#dataset'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Wan: ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Wan - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°. Wan Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ÑĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ VAE, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Wan Ñ 14 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸. Wan Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ Ğ´Ğ»Ñ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'Wan: Revolutionizing Video Generation with Open-Source Models', 'desc': 'This paper introduces Wan, a suite of video foundation models that enhances video generation using a diffusion transformer approach. Wan features a novel Variational Autoencoder (VAE) and scalable pre-training strategies, which improve its generative capabilities. The 14B model, trained on a massive dataset, showcases superior performance compared to existing models, while the 1.3B model offers efficiency for consumer-grade hardware. By open-sourcing the models and code, Wan aims to support the video generation community and expand creative opportunities in video production.'}, 'zh': {'title': 'æ¨åŠ¨è§†é¢‘ç”Ÿæˆçš„å¼€æ”¾æ¨¡å‹â€”â€”Wan', 'desc': 'æœ¬æŠ¥å‘Šä»‹ç»äº†Wanï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢ä¸”å¼€æ”¾çš„è§†é¢‘åŸºç¡€æ¨¡å‹å¥—ä»¶ï¼Œæ—¨åœ¨æ¨åŠ¨è§†é¢‘ç”Ÿæˆçš„è¾¹ç•Œã€‚WanåŸºäºä¸»æµçš„æ‰©æ•£å˜æ¢å™¨èŒƒå¼ï¼Œé€šè¿‡åˆ›æ–°çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ã€å¯æ‰©å±•çš„é¢„è®­ç»ƒç­–ç•¥ã€å¤§è§„æ¨¡æ•°æ®æ•´ç†å’Œè‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆèƒ½åŠ›ã€‚Wançš„14Bæ¨¡å‹åœ¨æ•°åäº¿å›¾åƒå’Œè§†é¢‘çš„æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå±•ç¤ºäº†è§†é¢‘ç”Ÿæˆåœ¨æ•°æ®å’Œæ¨¡å‹è§„æ¨¡æ–¹é¢çš„æ‰©å±•è§„å¾‹ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¼€æºæ¨¡å‹å’Œå•†ä¸šè§£å†³æ–¹æ¡ˆã€‚è¯¥æ¨¡å‹ä¸ä»…é«˜æ•ˆä¸”å¤šåŠŸèƒ½ï¼Œæ”¯æŒå¤šç§ä¸‹æ¸¸åº”ç”¨ï¼Œä¸”æ‰€æœ‰ä»£ç å’Œæ¨¡å‹å‡å·²å¼€æºï¼Œæ—¨åœ¨ä¿ƒè¿›è§†é¢‘ç”Ÿæˆç¤¾åŒºçš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19990', 'title': 'LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?', 'url': 'https://huggingface.co/papers/2503.19990', 'abstract': "Multi-step spatial reasoning entails understanding and reasoning about spatial relationships across multiple sequential steps, which is crucial for tackling complex real-world applications, such as robotic manipulation, autonomous navigation, and automated assembly. To assess how well current Multimodal Large Language Models (MLLMs) have acquired this fundamental capability, we introduce LEGO-Puzzles, a scalable benchmark designed to evaluate both spatial understanding and sequential reasoning in MLLMs through LEGO-based tasks. LEGO-Puzzles consists of 1,100 carefully curated visual question-answering (VQA) samples spanning 11 distinct tasks, ranging from basic spatial understanding to complex multi-step reasoning. Based on LEGO-Puzzles, we conduct a comprehensive evaluation of state-of-the-art MLLMs and uncover significant limitations in their spatial reasoning capabilities: even the most powerful MLLMs can answer only about half of the test cases, whereas human participants achieve over 90\\% accuracy. In addition to VQA tasks, we evaluate MLLMs' abilities to generate LEGO images following assembly illustrations. Our experiments show that only Gemini-2.0-Flash and GPT-4o exhibit a limited ability to follow these instructions, while other MLLMs either replicate the input image or generate completely irrelevant outputs. Overall, LEGO-Puzzles exposes critical deficiencies in existing MLLMs' spatial understanding and sequential reasoning capabilities, and underscores the need for further advancements in multimodal spatial reasoning.", 'score': 26, 'issue_id': 2922, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': 'ec85f1936ae0edd9', 'authors': ['Kexian Tang', 'Junyao Gao', 'Yanhong Zeng', 'Haodong Duan', 'Yanan Sun', 'Zhening Xing', 'Wenran Liu', 'Kaifeng Lyu', 'Kai Chen'], 'affiliations': ['Shanghai AI Laboratory', 'Simons Institute, UC Berkeley', 'Tongji University'], 'pdf_title_img': 'assets/pdf/title_img/2503.19990.jpg', 'data': {'categories': ['#robotics', '#benchmark', '#multimodal', '#reasoning'], 'emoji': 'ğŸ§©', 'ru': {'title': 'LEGO-Puzzles: Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LEGO-Puzzles - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 1100 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LEGO, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ MLLM ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ğ»Ğ¸ÑˆÑŒ Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ğ¾Ğ¹ Ñ‚ĞµÑÑ‚Ğ¾Ğ², Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ»ÑĞ´Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 90% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ MLLM Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ LEGO Ğ¿Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ ÑĞ±Ğ¾Ñ€ĞºĞ¸.'}, 'en': {'title': 'LEGO-Puzzles: Unveiling Spatial Reasoning Gaps in MLLMs', 'desc': "This paper introduces LEGO-Puzzles, a benchmark designed to evaluate the spatial reasoning and sequential understanding capabilities of Multimodal Large Language Models (MLLMs). The benchmark consists of 1,100 visual question-answering samples across 11 tasks, ranging from basic to complex reasoning. The evaluation reveals that current MLLMs struggle with spatial reasoning, achieving only about 50% accuracy compared to over 90% for humans. Additionally, the study assesses MLLMs' ability to generate images based on assembly instructions, finding that only a couple of models perform adequately, highlighting significant gaps in MLLMs' spatial understanding."}, 'zh': {'title': 'LEGO-Puzzlesï¼šè¯„ä¼°å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›', 'desc': 'å¤šæ­¥éª¤ç©ºé—´æ¨ç†æ˜¯ç†è§£å’Œæ¨ç†ç©ºé—´å…³ç³»çš„é‡è¦èƒ½åŠ›ï¼Œå°¤å…¶åœ¨å¤æ‚çš„ç°å®åº”ç”¨ä¸­ï¼Œå¦‚æœºå™¨äººæ“ä½œå’Œè‡ªåŠ¨å¯¼èˆªã€‚ä¸ºè¯„ä¼°å½“å‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è¿™ä¸€èƒ½åŠ›ä¸Šçš„è¡¨ç°ï¼Œæˆ‘ä»¬å¼•å…¥äº†LEGO-Puzzlesï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„åŸºå‡†ï¼Œæ—¨åœ¨é€šè¿‡åŸºäºLEGOçš„ä»»åŠ¡è¯„ä¼°ç©ºé—´ç†è§£å’Œé¡ºåºæ¨ç†ã€‚LEGO-PuzzlesåŒ…å«1100ä¸ªç²¾å¿ƒç­–åˆ’çš„è§†è§‰é—®ç­”æ ·æœ¬ï¼Œæ¶µç›–ä»åŸºæœ¬ç©ºé—´ç†è§£åˆ°å¤æ‚å¤šæ­¥éª¤æ¨ç†çš„11ä¸ªä¸åŒä»»åŠ¡ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œç°æœ‰çš„MLLMsåœ¨ç©ºé—´æ¨ç†èƒ½åŠ›ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œæœ€å¼ºçš„æ¨¡å‹ä»…èƒ½å›ç­”çº¦ä¸€åŠçš„æµ‹è¯•æ¡ˆä¾‹ï¼Œè€Œäººç±»å‚ä¸è€…çš„å‡†ç¡®ç‡è¶…è¿‡90%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20201', 'title': 'Open Deep Search: Democratizing Search with Open-source Reasoning Agents', 'url': 'https://huggingface.co/papers/2503.20201', 'abstract': "We introduce Open Deep Search (ODS) to close the increasing gap between the proprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and OpenAI's GPT-4o Search Preview, and their open-source counterparts. The main innovation introduced in ODS is to augment the reasoning capabilities of the latest open-source LLMs with reasoning agents that can judiciously use web search tools to answer queries. Concretely, ODS consists of two components that work with a base LLM chosen by the user: Open Search Tool and Open Reasoning Agent. Open Reasoning Agent interprets the given task and completes it by orchestrating a sequence of actions that includes calling tools, one of which is the Open Search Tool. Open Search Tool is a novel web search tool that outperforms proprietary counterparts. Together with powerful open-source reasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses the existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES. For example, on the FRAMES evaluation benchmark, ODS improves the best existing baseline of the recently released GPT-4o Search Preview by 9.7% in accuracy. ODS is a general framework for seamlessly augmenting any LLMs -- for example, DeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search and reasoning capabilities to achieve state-of-the-art performance: 88.3% on SimpleQA and 75.3% on FRAMES.", 'score': 22, 'issue_id': 2919, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': 'a9b1bed8d26f5055', 'authors': ['Salaheddin Alzubi', 'Creston Brooks', 'Purva Chiniya', 'Edoardo Contente', 'Chiara von Gerlach', 'Lucas Irwin', 'Yihan Jiang', 'Arda Kaz', 'Windsor Nguyen', 'Sewoong Oh', 'Himanshu Tyagi', 'Pramod Viswanath'], 'affiliations': ['Princeton University', 'Sentient', 'UC Berkeley', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2503.20201.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#agents', '#benchmark', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'ODS: Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¸ÑĞº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹', 'desc': 'Open Deep Search (ODS) - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹. ODS ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Open Search Tool (Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ¿Ğ¾Ğ¸ÑĞºĞ°) Ğ¸ Open Reasoning Agent (Ğ°Ğ³ĞµĞ½Ñ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¿Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ñƒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… SimpleQA Ğ¸ FRAMES, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ODS Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»ÑĞ±Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Ğ¸Ğ¼ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Empowering Open-Source LLMs with Advanced Search and Reasoning', 'desc': 'Open Deep Search (ODS) is a new framework designed to enhance the reasoning abilities of open-source large language models (LLMs) by integrating them with advanced web search tools. It features two main components: the Open Search Tool, which performs web searches, and the Open Reasoning Agent, which interprets tasks and coordinates actions, including using the search tool. ODS has shown to significantly improve performance on benchmarks like SimpleQA and FRAMES, even surpassing proprietary models like GPT-4o Search Preview in accuracy. This innovation allows users to leverage powerful open-source LLMs while achieving state-of-the-art results in query answering tasks.'}, 'zh': {'title': 'å¼€æ”¾æ·±åº¦æœç´¢ï¼šæå‡å¼€æºLLMçš„æ¨ç†ä¸æœç´¢èƒ½åŠ›', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†å¼€æ”¾æ·±åº¦æœç´¢ï¼ˆODSï¼‰ï¼Œæ—¨åœ¨ç¼©å°ä¸“æœ‰æœç´¢äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆä¸å¼€æºè§£å†³æ–¹æ¡ˆä¹‹é—´çš„å·®è·ã€‚ODSçš„ä¸»è¦åˆ›æ–°æ˜¯é€šè¿‡æ¨ç†ä»£ç†å¢å¼ºæœ€æ–°å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ˜æ™ºåœ°ä½¿ç”¨ç½‘ç»œæœç´¢å·¥å…·æ¥å›ç­”æŸ¥è¯¢ã€‚ODSç”±ä¸¤ä¸ªç»„ä»¶ç»„æˆï¼šå¼€æ”¾æœç´¢å·¥å…·å’Œå¼€æ”¾æ¨ç†ä»£ç†ï¼Œåè€…è´Ÿè´£è§£é‡Šä»»åŠ¡å¹¶åè°ƒä¸€ç³»åˆ—æ“ä½œï¼ŒåŒ…æ‹¬è°ƒç”¨å·¥å…·ã€‚é€šè¿‡ä¸å¼ºå¤§çš„å¼€æºæ¨ç†LLMï¼ˆå¦‚DeepSeek-R1ï¼‰ç»“åˆï¼ŒODSåœ¨SimpleQAå’ŒFRAMESä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡ ä¹è¾¾åˆ°äº†ç°æœ‰çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œç”šè‡³åœ¨FRAMESåŸºå‡†ä¸Šæé«˜äº†9.7%çš„å‡†ç¡®ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20240', 'title': 'Unconditional Priors Matter! Improving Conditional Generation of\n  Fine-Tuned Diffusion Models', 'url': 'https://huggingface.co/papers/2503.20240', 'abstract': 'Classifier-Free Guidance (CFG) is a fundamental technique in training conditional diffusion models. The common practice for CFG-based training is to use a single network to learn both conditional and unconditional noise prediction, with a small dropout rate for conditioning. However, we observe that the joint learning of unconditional noise with limited bandwidth in training results in poor priors for the unconditional case. More importantly, these poor unconditional noise predictions become a serious reason for degrading the quality of conditional generation. Inspired by the fact that most CFG-based conditional models are trained by fine-tuning a base model with better unconditional generation, we first show that simply replacing the unconditional noise in CFG with that predicted by the base model can significantly improve conditional generation. Furthermore, we show that a diffusion model other than the one the fine-tuned model was trained on can be used for unconditional noise replacement. We experimentally verify our claim with a range of CFG-based conditional models for both image and video generation, including Zero-1-to-3, Versatile Diffusion, DiT, DynamiCrafter, and InstructPix2Pix.', 'score': 19, 'issue_id': 2919, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': '4add99d8d7510263', 'authors': ['Prin Phunyaphibarn', 'Phillip Y. Lee', 'Jaihoon Kim', 'Minhyuk Sung'], 'affiliations': ['KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.20240.jpg', 'data': {'categories': ['#cv', '#training', '#diffusion', '#video'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ CFG: Ğ·Ğ°Ğ¼ĞµĞ½Ğ° Ğ±ĞµĞ·ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ° Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Classifier-Free Guidance (CFG) Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ±ĞµĞ·ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ·Ğ°Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ±ĞµĞ·ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¹ ÑˆÑƒĞ¼ Ğ² CFG Ğ½Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Enhancing Conditional Generation with Improved Unconditional Noise', 'desc': 'This paper discusses Classifier-Free Guidance (CFG), a technique used in training conditional diffusion models. The authors identify that using a single network for both conditional and unconditional noise prediction leads to poor performance in generating unconditional noise, which negatively impacts the quality of conditional outputs. They propose a solution where the unconditional noise predictions are replaced with those from a better-performing base model, resulting in improved conditional generation. The findings are validated through experiments on various CFG-based models for generating images and videos, demonstrating the effectiveness of their approach.'}, 'zh': {'title': 'æå‡æ¡ä»¶ç”Ÿæˆè´¨é‡çš„æ— æ¡ä»¶å™ªå£°æ›¿ä»£', 'desc': 'æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰æ˜¯ä¸€ç§åœ¨è®­ç»ƒæ¡ä»¶æ‰©æ•£æ¨¡å‹ä¸­ä½¿ç”¨çš„åŸºæœ¬æŠ€æœ¯ã€‚ä¼ ç»Ÿçš„CFGè®­ç»ƒæ–¹æ³•æ˜¯ä½¿ç”¨å•ä¸€ç½‘ç»œåŒæ—¶å­¦ä¹ æ¡ä»¶å’Œæ— æ¡ä»¶å™ªå£°é¢„æµ‹ï¼Œä½†è¿™ç§è”åˆå­¦ä¹ ä¼šå¯¼è‡´æ— æ¡ä»¶å™ªå£°çš„å…ˆéªŒè´¨é‡è¾ƒå·®ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨æ›´å¥½çš„æ— æ¡ä»¶ç”Ÿæˆæ¨¡å‹çš„å™ªå£°æ›¿ä»£å¯ä»¥æ˜¾è‘—æé«˜æ¡ä»¶ç”Ÿæˆçš„è´¨é‡ã€‚æˆ‘ä»¬é€šè¿‡å®éªŒéªŒè¯äº†è¿™ä¸€ç‚¹ï¼Œé€‚ç”¨äºå¤šç§åŸºäºCFGçš„æ¡ä»¶æ¨¡å‹ï¼ŒåŒ…æ‹¬å›¾åƒå’Œè§†é¢‘ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19480', 'title': 'GenHancer: Imperfect Generative Models are Secretly Strong\n  Vision-Centric Enhancers', 'url': 'https://huggingface.co/papers/2503.19480', 'abstract': "The synergy between generative and discriminative models receives growing attention. While discriminative Contrastive Language-Image Pre-Training (CLIP) excels in high-level semantics, it struggles with perceiving fine-grained visual details. Generally, to enhance representations, generative models take CLIP's visual features as conditions for reconstruction. However, the underlying principle remains underexplored. In this work, we empirically found that visually perfect generations are not always optimal for representation enhancement. The essence lies in effectively extracting fine-grained knowledge from generative models while mitigating irrelevant information. To explore critical factors, we delve into three aspects: (1) Conditioning mechanisms: We found that even a small number of local tokens can drastically reduce the difficulty of reconstruction, leading to collapsed training. We thus conclude that utilizing only global visual tokens as conditions is the most effective strategy. (2) Denoising configurations: We observed that end-to-end training introduces extraneous information. To address this, we propose a two-stage training strategy to prioritize learning useful visual knowledge. Additionally, we demonstrate that lightweight denoisers can yield remarkable improvements. (3) Generation paradigms: We explore both continuous and discrete denoisers with desirable outcomes, validating the versatility of our method. Through our in-depth explorations, we have finally arrived at an effective method, namely GenHancer, which consistently outperforms prior arts on the MMVP-VLM benchmark, e.g., 6.0% on OpenAICLIP. The enhanced CLIP can be further plugged into multimodal large language models for better vision-centric performance. All the models and codes are made publicly available.", 'score': 14, 'issue_id': 2920, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': '0dc97844010f5fb0', 'authors': ['Shijie Ma', 'Yuying Ge', 'Teng Wang', 'Yuxin Guo', 'Yixiao Ge', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'Institute of Automation, CAS'], 'pdf_title_img': 'assets/pdf/title_img/2503.19480.jpg', 'data': {'categories': ['#cv', '#open_source', '#multimodal', '#architecture', '#optimization', '#benchmark', '#training'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'GenHancer: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ GenHancer. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. GenHancer Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMVP-VLM, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 6.0% Ğ´Ğ»Ñ OpenAICLIP.'}, 'en': {'title': 'Enhancing CLIP with GenHancer: Bridging Generative and Discriminative Models', 'desc': 'This paper investigates the relationship between generative and discriminative models in machine learning, specifically focusing on enhancing the performance of the Contrastive Language-Image Pre-Training (CLIP) model. The authors found that while generative models can improve representations, perfect visual generations do not always lead to optimal results. They propose a method called GenHancer, which effectively extracts fine-grained knowledge while reducing irrelevant information through careful conditioning and denoising strategies. Their approach outperforms existing methods on the MMVP-VLM benchmark, demonstrating its potential for improving vision-centric tasks in multimodal large language models.'}, 'zh': {'title': 'ç”Ÿæˆä¸åˆ¤åˆ«æ¨¡å‹çš„å®Œç¾ç»“åˆ', 'desc': 'æœ¬æ–‡æ¢è®¨äº†ç”Ÿæˆæ¨¡å‹ä¸åˆ¤åˆ«æ¨¡å‹ä¹‹é—´çš„ååŒä½œç”¨ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•åˆ©ç”¨ç”Ÿæˆæ¨¡å‹æ¥å¢å¼ºåˆ¤åˆ«æ¨¡å‹CLIPçš„è¡¨ç¤ºèƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œè§†è§‰ä¸Šå®Œç¾çš„ç”Ÿæˆå¹¶ä¸æ€»æ˜¯æœ€ä¼˜çš„è¡¨ç¤ºå¢å¼ºæ–¹å¼ï¼Œå…³é”®åœ¨äºæœ‰æ•ˆæå–ç»†ç²’åº¦çŸ¥è¯†å¹¶å‡å°‘æ— å…³ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºäº†GenHanceræ–¹æ³•ï¼Œé€šè¿‡ä¼˜åŒ–æ¡ä»¶æœºåˆ¶ã€å»å™ªé…ç½®å’Œç”ŸæˆèŒƒå¼ï¼Œæ˜¾è‘—æå‡äº†CLIPåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æœ€ç»ˆï¼ŒGenHanceråœ¨MMVP-VLMåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ä¹‹å‰çš„ç ”ç©¶æˆæœï¼Œå±•ç¤ºäº†å…¶åœ¨è§†è§‰ä¸­å¿ƒæ€§èƒ½ä¸Šçš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20020', 'title': 'Gemini Robotics: Bringing AI into the Physical World', 'url': 'https://huggingface.co/papers/2503.20020', 'abstract': "Recent advancements in large multimodal models have led to the emergence of remarkable generalist capabilities in digital domains, yet their translation to physical agents such as robots remains a significant challenge. This report introduces a new family of AI models purposefully designed for robotics and built upon the foundation of Gemini 2.0. We present Gemini Robotics, an advanced Vision-Language-Action (VLA) generalist model capable of directly controlling robots. Gemini Robotics executes smooth and reactive movements to tackle a wide range of complex manipulation tasks while also being robust to variations in object types and positions, handling unseen environments as well as following diverse, open vocabulary instructions. We show that with additional fine-tuning, Gemini Robotics can be specialized to new capabilities including solving long-horizon, highly dexterous tasks, learning new short-horizon tasks from as few as 100 demonstrations and adapting to completely novel robot embodiments. This is made possible because Gemini Robotics builds on top of the Gemini Robotics-ER model, the second model we introduce in this work. Gemini Robotics-ER (Embodied Reasoning) extends Gemini's multimodal reasoning capabilities into the physical world, with enhanced spatial and temporal understanding. This enables capabilities relevant to robotics including object detection, pointing, trajectory and grasp prediction, as well as multi-view correspondence and 3D bounding box predictions. We show how this novel combination can support a variety of robotics applications. We also discuss and address important safety considerations related to this new class of robotics foundation models. The Gemini Robotics family marks a substantial step towards developing general-purpose robots that realizes AI's potential in the physical world.", 'score': 13, 'issue_id': 2919, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': '5edeeaed81b90426', 'authors': ['Gemini Robotics Team', 'Saminda Abeyruwan', 'Joshua Ainslie', 'Jean-Baptiste Alayrac', 'Montserrat Gonzalez Arenas', 'Travis Armstrong', 'Ashwin Balakrishna', 'Robert Baruch', 'Maria Bauza', 'Michiel Blokzijl', 'Steven Bohez', 'Konstantinos Bousmalis', 'Anthony Brohan', 'Thomas Buschmann', 'Arunkumar Byravan', 'Serkan Cabi', 'Ken Caluwaerts', 'Federico Casarini', 'Oscar Chang', 'Jose Enrique Chen', 'Xi Chen', 'Hao-Tien Lewis Chiang', 'Krzysztof Choromanski', "David D'Ambrosio", 'Sudeep Dasari', 'Todor Davchev', 'Coline Devin', 'Norman Di Palo', 'Tianli Ding', 'Adil Dostmohamed', 'Danny Driess', 'Yilun Du', 'Debidatta Dwibedi', 'Michael Elabd', 'Claudio Fantacci', 'Cody Fong', 'Erik Frey', 'Chuyuan Fu', 'Marissa Giustina', 'Keerthana Gopalakrishnan', 'Laura Graesser', 'Leonard Hasenclever', 'Nicolas Heess', 'Brandon Hernaez', 'Alexander Herzog', 'R. Alex Hofer', 'Jan Humplik', 'Atil Iscen', 'Mithun George Jacob', 'Deepali Jain', 'Ryan Julian', 'Dmitry Kalashnikov', 'M. Emre Karagozler', 'Stefani Karp', 'Chase Kew', 'Jerad Kirkland', 'Sean Kirmani', 'Yuheng Kuang', 'Thomas Lampe', 'Antoine Laurens', 'Isabel Leal', 'Alex X. Lee', 'Tsang-Wei Edward Lee', 'Jacky Liang', 'Yixin Lin', 'Sharath Maddineni', 'Anirudha Majumdar', 'Assaf Hurwitz Michaely', 'Robert Moreno', 'Michael Neunert', 'Francesco Nori', 'Carolina Parada', 'Emilio Parisotto', 'Peter Pastor', 'Acorn Pooley', 'Kanishka Rao', 'Krista Reymann', 'Dorsa Sadigh', 'Stefano Saliceti', 'Pannag Sanketi', 'Pierre Sermanet', 'Dhruv Shah', 'Mohit Sharma', 'Kathryn Shea', 'Charles Shu', 'Vikas Sindhwani', 'Sumeet Singh', 'Radu Soricut', 'Jost Tobias Springenberg', 'Rachel Sterneck', 'Razvan Surdulescu', 'Jie Tan', 'Jonathan Tompson', 'Vincent Vanhoucke', 'Jake Varley', 'Grace Vesom', 'Giulia Vezzani', 'Oriol Vinyals', 'Ayzaan Wahid', 'Stefan Welker', 'Paul Wohlhart', 'Fei Xia', 'Ted Xiao', 'Annie Xie', 'Jinyu Xie', 'Peng Xu', 'Sichun Xu', 'Ying Xu', 'Zhuo Xu', 'Yuxiang Yang', 'Rui Yao', 'Sergey Yaroshenko', 'Wenhao Yu', 'Wentao Yuan', 'Jingwei Zhang', 'Tingnan Zhang', 'Allan Zhou', 'Yuxiang Zhou'], 'affiliations': ['Gemini Robotics Team, Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2503.20020.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#agents', '#agi', '#ethics', '#games', '#reasoning', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Gemini Robotics: Ğ˜Ğ˜ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¸Ñ€', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Gemini Robotics, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Gemini 2.0 Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸. Gemini Robotics - ÑÑ‚Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ¸Ğ¿Ğ° Vision-Language-Action, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Gemini Robotics-ER Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Gemini Ğ½Ğ° Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼Ğ¸Ñ€, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, ĞºĞ°Ğº ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ¸ÑĞ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Empowering Robots with Gemini Robotics: A Leap in Multimodal AI', 'desc': "This paper presents Gemini Robotics, a new family of AI models designed specifically for robotic applications, building on the Gemini 2.0 framework. The model integrates Vision-Language-Action (VLA) capabilities, allowing robots to perform complex manipulation tasks with smooth and adaptive movements. It can learn from limited demonstrations and adapt to new tasks and robot designs, showcasing its versatility in various environments. Additionally, the paper introduces Gemini Robotics-ER, which enhances the model's reasoning abilities in physical contexts, addressing safety and practical applications in robotics."}, 'zh': {'title': 'Gemini Roboticsï¼šé€šç”¨æœºå™¨äººçš„æ–°çºªå…ƒ', 'desc': 'æœ€è¿‘å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„è¿›å±•ä½¿å¾—æ•°å­—é¢†åŸŸçš„é€šç”¨èƒ½åŠ›æ˜¾è‘—æå‡ï¼Œä½†å°†å…¶åº”ç”¨äºæœºå™¨äººç­‰ç‰©ç†ä»£ç†ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œä¸“ä¸ºæœºå™¨äººè®¾è®¡ï¼ŒåŸºäºGemini 2.0æ„å»ºã€‚Gemini Roboticsæ˜¯ä¸€ä¸ªå…ˆè¿›çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰é€šç”¨æ¨¡å‹ï¼Œèƒ½å¤Ÿç›´æ¥æ§åˆ¶æœºå™¨äººï¼Œæ‰§è¡Œå¤æ‚çš„æ“ä½œä»»åŠ¡ï¼Œå¹¶å¯¹ç‰©ä½“ç±»å‹å’Œä½ç½®çš„å˜åŒ–å…·æœ‰é²æ£’æ€§ã€‚é€šè¿‡é¢å¤–çš„å¾®è°ƒï¼ŒGemini Roboticså¯ä»¥ä¸“é—¨åŒ–ä¸ºæ–°çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬è§£å†³é•¿æ—¶é—´è·¨åº¦çš„é«˜çµå·§ä»»åŠ¡ï¼Œä»¥åŠä»å°‘é‡ç¤ºä¾‹ä¸­å­¦ä¹ æ–°ä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19786', 'title': 'Gemma 3 Technical Report', 'url': 'https://huggingface.co/papers/2503.19786', 'abstract': 'We introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters. This version introduces vision understanding abilities, a wider coverage of languages and longer context - at least 128K tokens. We also change the architecture of the model to reduce the KV-cache memory that tends to explode with long context. This is achieved by increasing the ratio of local to global attention layers, and keeping the span on local attention short. The Gemma 3 models are trained with distillation and achieve superior performance to Gemma 2 for both pre-trained and instruction finetuned versions. In particular, our novel post-training recipe significantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across benchmarks. We release all our models to the community.', 'score': 12, 'issue_id': 2934, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': 'df574ff057c95baa', 'authors': ['Gemma Team', 'Aishwarya Kamath', 'Johan Ferret', 'Shreya Pathak', 'Nino Vieillard', 'Ramona Merhej', 'Sarah Perrin', 'Tatiana Matejovicova', 'Alexandre RamÃ©', 'Morgane RiviÃ¨re', 'Louis Rouillard', 'Thomas Mesnard', 'Geoffrey Cideron', 'Jean-bastien Grill', 'Sabela Ramos', 'Edouard Yvinec', 'Michelle Casbon', 'Etienne Pot', 'Ivo Penchev', 'GaÃ«l Liu', 'Francesco Visin', 'Kathleen Kenealy', 'Lucas Beyer', 'Xiaohai Zhai', 'Anton Tsitsulin', 'Robert Busa-Fekete', 'Alex Feng', 'Noveen Sachdeva', 'Benjamin Coleman', 'Yi Gao', 'Basil Mustafa', 'Iain Barr', 'Emilio Parisotto', 'David Tian', 'Matan Eyal', 'Colin Cherry', 'Jan-Thorsten Peter', 'Danila Sinopalnikov', 'Surya Bhupatiraju', 'Rishabh Agarwal', 'Mehran Kazemi', 'Dan Malkin', 'Ravin Kumar', 'David Vilar', 'Idan Brusilovsky', 'Jiaming Luo', 'Andreas Steiner', 'Abe Friesen', 'Abhanshu Sharma', 'Abheesht Sharma', 'Adi Mayrav Gilady', 'Adrian Goedeckemeyer', 'Alaa Saade', 'Alex Feng', 'Alexander Kolesnikov', 'Alexei Bendebury', 'Alvin Abdagic', 'Amit Vadi', 'AndrÃ¡s GyÃ¶rgy', 'AndrÃ© Susano Pinto', 'Anil Das', 'Ankur Bapna', 'Antoine Miech', 'Antoine Yang', 'Antonia Paterson', 'Ashish Shenoy', 'Ayan Chakrabarti', 'Bilal Piot', 'Bo Wu', 'Bobak Shahriari', 'Bryce Petrini', 'Charlie Chen', 'Charline Le Lan', 'Christopher A. Choquette-Choo', 'CJ Carey', 'Cormac Brick', 'Daniel Deutsch', 'Danielle Eisenbud', 'Dee Cattle', 'Derek Cheng', 'Dimitris Paparas', 'Divyashree Shivakumar Sreepathihalli', 'Doug Reid', 'Dustin Tran', 'Dustin Zelle', 'Eric Noland', 'Erwin Huizenga', 'Eugene Kharitonov', 'Frederick Liu', 'Gagik Amirkhanyan', 'Glenn Cameron', 'Hadi Hashemi', 'Hanna Klimczak-PluciÅ„ska', 'Harman Singh', 'Harsh Mehta', 'Harshal Tushar Lehri', 'Hussein Hazimeh', 'Ian Ballantyne', 'Idan Szpektor', 'Ivan Nardini', 'Jean Pouget-Abadie', 'Jetha Chan', 'Joe Stanton', 'John Wieting', 'Jonathan Lai', 'Jordi Orbay', 'Joseph Fernandez', 'Josh Newlan', 'Ju-yeong Ji', 'Jyotinder Singh', 'Kat Black', 'Kathy Yu', 'Kevin Hui', 'Kiran Vodrahalli', 'Klaus Greff', 'Linhai Qiu', 'Marcella Valentine', 'Marina Coelho', 'Marvin Ritter', 'Matt Hoffman', 'Matthew Watson', 'Mayank Chaturvedi', 'Michael Moynihan', 'Min Ma', 'Nabila Babar', 'Natasha Noy', 'Nathan Byrd', 'Nick Roy', 'Nikola Momchev', 'Nilay Chauhan', 'Noveen Sachdeva', 'Oskar Bunyan', 'Pankil Botarda', 'Paul Caron', 'Paul Kishan Rubenstein', 'Phil Culliton', 'Philipp Schmid', 'Pier Giuseppe Sessa', 'Pingmei Xu', 'Piotr Stanczyk', 'Pouya Tafti', 'Rakesh Shivanna', 'Renjie Wu', 'Renke Pan', 'Reza Rokni', 'Rob Willoughby', 'Rohith Vallu', 'Ryan Mullins', 'Sammy Jerome', 'Sara Smoot', 'Sertan Girgin', 'Shariq Iqbal', 'Shashir Reddy', 'Shruti Sheth', 'Siim PÃµder', 'Sijal Bhatnagar', 'Sindhu Raghuram Panyam', 'Sivan Eiger', 'Susan Zhang', 'Tianqi Liu', 'Trevor Yacovone', 'Tyler Liechty', 'Uday Kalra', 'Utku Evci', 'Vedant Misra', 'Vincent Roseberry', 'Vlad Feinberg', 'Vlad Kolesnikov', 'Woohyun Han', 'Woosuk Kwon', 'Xi Chen', 'Yinlam Chow', 'Yuvein Zhu', 'Zichuan Wei', 'Zoltan Egyed', 'Victor Cotruta', 'Minh Giang', 'Phoebe Kirk', 'Anand Rao', 'Kat Black', 'Nabila Babar', 'Jessica Lo', 'Erica Moreira', 'Luiz Gustavo Martins', 'Omar Sanseviero', 'Lucas Gonzalez', 'Zach Gleicher', 'Tris Warkentin', 'Vahab Mirrokni', 'Evan Senter', 'Eli Collins', 'Joelle Barral', 'Zoubin Ghahramani', 'Raia Hadsell', 'Yossi Matias', 'D. Sculley', 'Slav Petrov', 'Noah Fiedel', 'Noam Shazeer', 'Oriol Vinyals', 'Jeff Dean', 'Demis Hassabis', 'Koray Kavukcuoglu', 'Clement Farabet', 'Elena Buchatskaya', 'Jean-Baptiste Alayrac', 'Rohan Anil', 'Dmitry', 'Lepikhin', 'Sebastian Borgeaud', 'Olivier Bachem', 'Armand Joulin', 'Alek Andreev', 'Cassidy Hardin', 'Robert Dadashi', 'LÃ©onard Hussenot'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2503.19786.jpg', 'data': {'categories': ['#training', '#multimodal', '#architecture', '#open_source', '#multilingual', '#long_context'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Gemma 3: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Gemma 3 - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Gemma Ñ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ¾Ğ¼ Ğ¾Ñ‚ 1 Ğ´Ğ¾ 27 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ»Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºÑƒ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ¾ 128Ğš Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ±Ñ‹Ğ»Ğ° Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ° Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ KV-ĞºÑÑˆĞ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿ÑƒÑ‚ĞµĞ¼ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Gemma 3 Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Gemma 2 ĞºĞ°Ğº Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµÑ€ÑĞ¸ÑÑ….'}, 'en': {'title': 'Gemma 3: Multimodal Mastery with Extended Context!', 'desc': 'Gemma 3 is a new version of the Gemma model family that enhances multimodal capabilities, allowing it to understand both text and images. It features a larger scale, with models ranging from 1 to 27 billion parameters, and supports longer context lengths of at least 128K tokens. The architecture has been optimized to manage memory usage better during long contexts by adjusting the balance of local and global attention layers. With improved training techniques, Gemma 3 outperforms its predecessor, Gemma 2, in various tasks including math, chat, and multilingual processing, making it a strong competitor in the field.'}, 'zh': {'title': 'Gemma 3ï¼šå¤šæ¨¡æ€è½»é‡çº§æ¨¡å‹çš„çªç ´', 'desc': 'Gemma 3 æ˜¯ Gemma ç³»åˆ—è½»é‡çº§å¼€æ”¾æ¨¡å‹çš„å¤šæ¨¡æ€ç‰ˆæœ¬ï¼Œå‚æ•°è§„æ¨¡ä» 1 åˆ° 270 äº¿ä¸ç­‰ã€‚è¯¥ç‰ˆæœ¬å¼•å…¥äº†è§†è§‰ç†è§£èƒ½åŠ›ï¼Œæ”¯æŒæ›´å¤šè¯­è¨€ï¼Œå¹¶ä¸”èƒ½å¤Ÿå¤„ç†æ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼Œè‡³å°‘è¾¾åˆ° 128K ä¸ªæ ‡è®°ã€‚æˆ‘ä»¬é€šè¿‡è°ƒæ•´æ¨¡å‹æ¶æ„ï¼Œå¢åŠ å±€éƒ¨æ³¨æ„åŠ›å±‚ä¸å…¨å±€æ³¨æ„åŠ›å±‚çš„æ¯”ä¾‹ï¼Œæ¥å‡å°‘é•¿ä¸Šä¸‹æ–‡ä¸‹ KV-cache å†…å­˜çš„çˆ†ç‚¸ã€‚Gemma 3 æ¨¡å‹ç»è¿‡è’¸é¦è®­ç»ƒï¼Œè¡¨ç°ä¼˜äº Gemma 2ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦ã€å¯¹è¯ã€æŒ‡ä»¤è·Ÿéšå’Œå¤šè¯­è¨€èƒ½åŠ›æ–¹é¢æœ‰æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20672', 'title': 'BizGen: Advancing Article-level Visual Text Rendering for Infographics\n  Generation', 'url': 'https://huggingface.co/papers/2503.20672', 'abstract': 'Recently, state-of-the-art text-to-image generation models, such as Flux and Ideogram 2.0, have made significant progress in sentence-level visual text rendering. In this paper, we focus on the more challenging scenarios of article-level visual text rendering and address a novel task of generating high-quality business content, including infographics and slides, based on user provided article-level descriptive prompts and ultra-dense layouts. The fundamental challenges are twofold: significantly longer context lengths and the scarcity of high-quality business content data.   In contrast to most previous works that focus on a limited number of sub-regions and sentence-level prompts, ensuring precise adherence to ultra-dense layouts with tens or even hundreds of sub-regions in business content is far more challenging. We make two key technical contributions: (i) the construction of scalable, high-quality business content dataset, i.e., Infographics-650K, equipped with ultra-dense layouts and prompts by implementing a layer-wise retrieval-augmented infographic generation scheme; and (ii) a layout-guided cross attention scheme, which injects tens of region-wise prompts into a set of cropped region latent space according to the ultra-dense layouts, and refine each sub-regions flexibly during inference using a layout conditional CFG.   We demonstrate the strong results of our system compared to previous SOTA systems such as Flux and SD3 on our BizEval prompt set. Additionally, we conduct thorough ablation experiments to verify the effectiveness of each component. We hope our constructed Infographics-650K and BizEval can encourage the broader community to advance the progress of business content generation.', 'score': 11, 'issue_id': 2920, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': 'b04cbfb976ce4e45', 'authors': ['Yuyang Peng', 'Shishi Xiao', 'Keming Wu', 'Qisheng Liao', 'Bohan Chen', 'Kevin Lin', 'Danqing Huang', 'Ji Li', 'Yuhui Yuan'], 'affiliations': ['Brown University', 'Microsoft', 'Microsoft Research Asia', 'Tsinghua University', 'University of Liverpool'], 'pdf_title_img': 'assets/pdf/title_img/2503.20672.jpg', 'data': {'categories': ['#long_context', '#cv', '#synthetic', '#dataset', '#rag'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ğ¸Ğ½Ñ„Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºÑƒ Ğ¸ ÑĞ»Ğ°Ğ¹Ğ´Ñ‹, Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ¸ ÑĞ²ĞµÑ€Ñ…Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ°ĞºĞµÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Infographics-650K Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ…ĞµĞ¼Ñƒ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¼Ğ°ĞºĞµÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ Ğ´ĞµÑÑÑ‚ĞºĞ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ ÑĞ²ĞµÑ€Ñ…Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼ Ğ¼Ğ°ĞºĞµÑ‚Ğ°Ğ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº Flux Ğ¸ SD3, Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² BizEval.'}, 'en': {'title': 'Revolutionizing Business Content Generation with Ultra-Dense Layouts', 'desc': 'This paper presents advancements in text-to-image generation, specifically targeting the creation of business content like infographics and slides from article-level prompts. The authors introduce a new dataset, Infographics-650K, which includes ultra-dense layouts and is designed to address the challenges of longer context lengths and limited high-quality data. They propose a layout-guided cross attention mechanism that allows for precise generation across multiple sub-regions, enhancing the fidelity of the output. The results show significant improvements over existing models, encouraging further research in business content generation.'}, 'zh': {'title': 'æ¨åŠ¨å•†ä¸šå†…å®¹ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'æœ¬æ–‡å…³æ³¨äºæ–‡ç« çº§è§†è§‰æ–‡æœ¬æ¸²æŸ“çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆé«˜è´¨é‡å•†ä¸šå†…å®¹æ–¹é¢ï¼Œå¦‚ä¿¡æ¯å›¾å’Œå¹»ç¯ç‰‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡ï¼Œæ—¨åœ¨æ ¹æ®ç”¨æˆ·æä¾›çš„æè¿°æ€§æç¤ºå’Œè¶…å¯†é›†å¸ƒå±€ç”Ÿæˆè¿™äº›å†…å®¹ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„é«˜è´¨é‡å•†ä¸šå†…å®¹æ•°æ®é›†Infographics-650Kï¼Œå¹¶å®ç°äº†ä¸€ç§åŸºäºå¸ƒå±€çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥å¤„ç†å¤æ‚çš„åŒºåŸŸæç¤ºã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨ä¸ç°æœ‰æœ€å…ˆè¿›ç³»ç»Ÿçš„æ¯”è¾ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶é€šè¿‡æ¶ˆèå®éªŒéªŒè¯äº†å„ä¸ªç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20757', 'title': 'MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree\n  Search', 'url': 'https://huggingface.co/papers/2503.20757', 'abstract': 'We introduce MCTS-RAG, a novel approach that enhances the reasoning capabilities of small language models on knowledge-intensive tasks by leveraging retrieval-augmented generation (RAG) to provide relevant context and Monte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically integrates retrieval and reasoning through an iterative decision-making process. Unlike standard RAG methods, which typically retrieve information independently from reasoning and thus integrate knowledge suboptimally, or conventional MCTS reasoning, which depends solely on internal model knowledge without external facts, MCTS-RAG combines structured reasoning with adaptive retrieval. This integrated approach enhances decision-making, reduces hallucinations, and ensures improved factual accuracy and response consistency. The experimental results on multiple reasoning and knowledge-intensive datasets datasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method enables small-scale LMs to achieve performance comparable to frontier LLMs like GPT-4o by effectively scaling inference-time compute, setting a new standard for reasoning in small-scale models.', 'score': 7, 'issue_id': 2920, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': 'd4c3b116518a2b0f', 'authors': ['Yunhai Hu', 'Yilun Zhao', 'Chen Zhao', 'Arman Cohan'], 'affiliations': ['New York University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2503.20757.jpg', 'data': {'categories': ['#hallucinations', '#reasoning', '#rag', '#small_models'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MCTS-RAG: Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'MCTS-RAG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ retrieval-augmented generation (RAG) Ğ¸ Monte Carlo Tree Search (MCTS) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. MCTS-RAG ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° GPT-4, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Enhancing Small Models with Smart Retrieval and Reasoning', 'desc': 'MCTS-RAG is a new method that improves how small language models handle complex tasks that require knowledge. It combines retrieval-augmented generation (RAG) with Monte Carlo Tree Search (MCTS) to enhance reasoning by providing relevant information during the decision-making process. This approach allows the model to access external facts while reasoning, leading to better accuracy and consistency in responses. Experiments show that MCTS-RAG enables smaller models to perform as well as larger models like GPT-4o on challenging datasets.'}, 'zh': {'title': 'MCTS-RAGï¼šå°å‹æ¨¡å‹æ¨ç†çš„æ–°æ ‡å‡†', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•MCTS-RAGï¼Œå®ƒé€šè¿‡ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ï¼Œæå‡å°å‹è¯­è¨€æ¨¡å‹åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚MCTS-RAGé€šè¿‡è¿­ä»£å†³ç­–è¿‡ç¨‹åŠ¨æ€æ•´åˆæ£€ç´¢å’Œæ¨ç†ï¼Œå…‹æœäº†ä¼ ç»ŸRAGæ–¹æ³•å’ŒMCTSæ¨ç†çš„å±€é™æ€§ã€‚ä¸æ ‡å‡†RAGæ–¹æ³•ä¸åŒï¼ŒMCTS-RAGèƒ½å¤Ÿæ›´å¥½åœ°ç»“åˆç»“æ„åŒ–æ¨ç†å’Œè‡ªé€‚åº”æ£€ç´¢ï¼Œä»è€Œæé«˜å†³ç­–è´¨é‡ï¼Œå‡å°‘å¹»è§‰ç°è±¡ï¼Œå¹¶ç¡®ä¿æ›´é«˜çš„äº‹å®å‡†ç¡®æ€§å’Œå“åº”ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä½¿å°å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å¯ä¸å‰æ²¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰ç›¸åª²ç¾ï¼Œæ ‘ç«‹äº†å°å‹æ¨¡å‹æ¨ç†çš„æ–°æ ‡å‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19950', 'title': 'LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation', 'url': 'https://huggingface.co/papers/2503.19950', 'abstract': "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV Cache in large language model (LLM) inference, delivering substantial memory savings while preserving superior performance. Previous methods either assume that later tokens are more important or attempt to predict important tokens based on earlier attention patterns. Both approaches, however, can result in performance bottlenecks or frequent mispredictions.   LogQuant takes a different approach. By applying a log-based filtering mechanism, it selectively compresses the KV Cache across the entire context, achieving better performance with the same or even reduced memory footprint compared to existing methods. In benchmark tests, it enhances throughput by 25% and boosts batch size by 60% without increasing memory consumption. For challenging tasks such as Math and Code Completion, LogQuant improves accuracy by 40% to 200% at the same compression ratio, outperforming comparable techniques.LogQuant integrates effortlessly with popular inference frameworks like Python's transformers library. Implementation can be available in https://github.com/Concyclics/LogQuantKV.", 'score': 7, 'issue_id': 2921, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': 'b75e0acc68cc5153', 'authors': ['Han Chen', 'Zicong Jiang', 'Zining Zhang', 'Bingsheng He', 'Pingyi Luo', 'Mian Lu', 'Yuqiang Chen'], 'affiliations': ['4Paradigm', 'School of Computing National University of Singapore', 'School of Electronic and Information Engineering South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.19950.jpg', 'data': {'categories': ['#benchmark', '#inference', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'LogQuant - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° 2-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ KV-ĞºÑÑˆĞ° Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ KV-ĞºÑÑˆĞ° Ğ¿Ğ¾ Ğ²ÑĞµĞ¼Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ. Ğ’ Ñ‚ĞµÑÑ‚Ğ°Ñ… LogQuant Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 25% Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ±Ğ°Ñ‚Ñ‡Ğ° Ğ½Ğ° 60% Ğ±ĞµĞ· Ñ€Ğ¾ÑÑ‚Ğ° Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ”Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ° Ğ¸ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ°, LogQuant ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 40-200% Ğ¿Ñ€Ğ¸ Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ.'}, 'en': {'title': 'LogQuant: Efficient 2-Bit Quantization for Enhanced LLM Performance', 'desc': 'LogQuant is a novel 2-bit quantization method designed for efficiently managing the KV Cache in large language model inference. Unlike previous techniques that prioritize later tokens or rely on early attention patterns, LogQuant employs a log-based filtering mechanism to compress the KV Cache more effectively. This approach not only reduces memory usage but also enhances performance, achieving a 25% increase in throughput and a 60% increase in batch size without additional memory costs. In challenging tasks like Math and Code Completion, LogQuant significantly boosts accuracy by 40% to 200% while maintaining the same compression ratio, making it a superior choice for LLM applications.'}, 'zh': {'title': 'LogQuantï¼šé«˜æ•ˆçš„KVç¼“å­˜é‡åŒ–æŠ€æœ¯', 'desc': 'LogQuantæ˜¯ä¸€ç§åˆ›æ–°çš„2ä½é‡åŒ–æŠ€æœ¯ï¼Œä¸“ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä¸­çš„KVç¼“å­˜è®¾è®¡ã€‚å®ƒé€šè¿‡åº”ç”¨åŸºäºå¯¹æ•°çš„è¿‡æ»¤æœºåˆ¶ï¼Œé€‰æ‹©æ€§åœ°å‹ç¼©KVç¼“å­˜ï¼Œä»è€Œåœ¨ä¿æŒä¼˜è¶Šæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—èŠ‚çœå†…å­˜ã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼ŒLogQuanté¿å…äº†æ€§èƒ½ç“¶é¢ˆå’Œé¢‘ç¹çš„é”™è¯¯é¢„æµ‹ï¼Œæå‡äº†25%çš„ååé‡å’Œ60%çš„æ‰¹å¤„ç†å¤§å°ã€‚å¯¹äºæ•°å­¦å’Œä»£ç è¡¥å…¨ç­‰å¤æ‚ä»»åŠ¡ï¼ŒLogQuantåœ¨ç›¸åŒå‹ç¼©æ¯”ä¸‹æé«˜äº†40%åˆ°200%çš„å‡†ç¡®æ€§ï¼Œè¶…è¶Šäº†ç±»ä¼¼æŠ€æœ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20271', 'title': 'ViLBench: A Suite for Vision-Language Process Reward Modeling', 'url': 'https://huggingface.co/papers/2503.20271', 'abstract': "Process-supervised reward models serve as a fine-grained function that provides detailed step-wise feedback to model responses, facilitating effective selection of reasoning trajectories for complex tasks. Despite its advantages, evaluation on PRMs remains less explored, especially in the multimodal domain. To address this gap, this paper first benchmarks current vision large language models (VLLMs) as two types of reward models: output reward models (ORMs) and process reward models (PRMs) on multiple vision-language benchmarks, which reveal that neither ORM nor PRM consistently outperforms across all tasks, and superior VLLMs do not necessarily yield better rewarding performance. To further advance evaluation, we introduce ViLBench, a vision-language benchmark designed to require intensive process reward signals. Notably, OpenAI's GPT-4o with Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the benchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a promising pathway towards bridging the gap between general VLLMs and reward models -- by collecting 73.6K vision-language process reward data using an enhanced tree-search algorithm, our 3B model is able to achieve an average improvement of 3.3% over standard CoT and up to 2.5% compared to its untrained counterpart on ViLBench by selecting OpenAI o1's generations. We release the implementations at https://ucsc-vlaa.github.io/ViLBench with our code, model, and data.", 'score': 6, 'issue_id': 2925, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': '53462508b8990597', 'authors': ['Haoqin Tu', 'Weitao Feng', 'Hardy Chen', 'Hui Liu', 'Xianfeng Tang', 'Cihang Xie'], 'affiliations': ['Amazon Research', 'UC Santa Cruz', 'UT Dallas'], 'pdf_title_img': 'assets/pdf/title_img/2503.20271.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#benchmark', '#optimization', '#multimodal', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ (PRM), Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLLM) Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ViLBench, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ VLLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ.'}, 'en': {'title': 'Enhancing Multimodal Learning with Process-Supervised Rewards', 'desc': 'This paper explores the use of process-supervised reward models (PRMs) to provide detailed feedback for complex reasoning tasks in the multimodal domain. It benchmarks vision large language models (VLLMs) as output reward models (ORMs) and PRMs across various vision-language tasks, finding that neither consistently outperforms the other. The authors introduce ViLBench, a challenging benchmark that emphasizes the need for process reward signals, revealing that even advanced models like GPT-4o struggle with its complexity. Additionally, they demonstrate a method to enhance reward model performance by collecting a large dataset of vision-language process rewards, leading to measurable improvements in model accuracy.'}, 'zh': {'title': 'æå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„å¥–åŠ±è¯„ä¼°', 'desc': 'æœ¬æ–‡æ¢è®¨äº†è¿‡ç¨‹ç›‘ç£å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œæä¾›äº†è¯¦ç»†çš„é€æ­¥åé¦ˆä»¥å¸®åŠ©é€‰æ‹©æ¨ç†è·¯å¾„ã€‚å°½ç®¡PRMså…·æœ‰ä¼˜åŠ¿ï¼Œä½†åœ¨å¤šæ¨¡æ€é¢†åŸŸçš„è¯„ä¼°ä»ç„¶è¾ƒå°‘ã€‚æˆ‘ä»¬é¦–æ¬¡å¯¹å½“å‰çš„è§†è§‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆVLLMsï¼‰è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå‘ç°è¾“å‡ºå¥–åŠ±æ¨¡å‹ï¼ˆORMsï¼‰å’Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰åœ¨ä¸åŒä»»åŠ¡ä¸­çš„è¡¨ç°å¹¶ä¸ä¸€è‡´ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ViLBenchï¼Œä¸€ä¸ªéœ€è¦å¼ºçƒˆè¿‡ç¨‹å¥–åŠ±ä¿¡å·çš„è§†è§‰-è¯­è¨€åŸºå‡†ï¼Œå±•ç¤ºäº†å½“å‰VLLMsåœ¨æ­¤åŸºå‡†ä¸Šçš„æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19462', 'title': 'AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset', 'url': 'https://huggingface.co/papers/2503.19462', 'abstract': 'Diffusion models have achieved remarkable progress in the field of video generation. However, their iterative denoising nature requires a large number of inference steps to generate a video, which is slow and computationally expensive. In this paper, we begin with a detailed analysis of the challenges present in existing diffusion distillation methods and propose a novel efficient method, namely AccVideo, to reduce the inference steps for accelerating video diffusion models with synthetic dataset. We leverage the pretrained video diffusion model to generate multiple valid denoising trajectories as our synthetic dataset, which eliminates the use of useless data points during distillation. Based on the synthetic dataset, we design a trajectory-based few-step guidance that utilizes key data points from the denoising trajectories to learn the noise-to-video mapping, enabling video generation in fewer steps. Furthermore, since the synthetic dataset captures the data distribution at each diffusion timestep, we introduce an adversarial training strategy to align the output distribution of the student model with that of our synthetic dataset, thereby enhancing the video quality. Extensive experiments demonstrate that our model achieves 8.5x improvements in generation speed compared to the teacher model while maintaining comparable performance. Compared to previous accelerating methods, our approach is capable of generating videos with higher quality and resolution, i.e., 5-seconds, 720x1280, 24fps.', 'score': 5, 'issue_id': 2919, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': '721d2bb59c963434', 'authors': ['Haiyu Zhang', 'Xinyuan Chen', 'Yaohui Wang', 'Xihui Liu', 'Yunhong Wang', 'Yu Qiao'], 'affiliations': ['Beihang University', 'Shanghai AI Laboratory', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.19462.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#inference', '#video', '#dataset', '#synthetic'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ AccVideo Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ Ğ¼Ğ°Ğ»Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ 8.5-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Accelerating Video Generation with AccVideo', 'desc': 'This paper addresses the slow and resource-intensive process of video generation using diffusion models, which require many steps for denoising. The authors introduce AccVideo, a new method that reduces the number of inference steps needed by utilizing a synthetic dataset generated from a pretrained video diffusion model. By focusing on key data points from denoising trajectories, the model learns to map noise to video more efficiently. Additionally, an adversarial training strategy is employed to improve the quality of the generated videos, resulting in significant speed improvements while maintaining high resolution and quality.'}, 'zh': {'title': 'åŠ é€Ÿè§†é¢‘ç”Ÿæˆï¼Œæå‡è´¨é‡ä¸æ•ˆç‡', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘ç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶è¿­ä»£å»å™ªçš„ç‰¹æ€§å¯¼è‡´ç”Ÿæˆè§†é¢‘éœ€è¦å¤§é‡æ¨ç†æ­¥éª¤ï¼Œé€Ÿåº¦æ…¢ä¸”è®¡ç®—æˆæœ¬é«˜ã€‚æœ¬æ–‡åˆ†æäº†ç°æœ‰æ‰©æ•£è’¸é¦æ–¹æ³•ä¸­çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„é«˜æ•ˆæ–¹æ³•AccVideoï¼Œä»¥å‡å°‘æ¨ç†æ­¥éª¤ï¼ŒåŠ é€Ÿè§†é¢‘æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šä¸ªæœ‰æ•ˆçš„å»å™ªè½¨è¿¹ä½œä¸ºåˆæˆæ•°æ®é›†ï¼Œä»è€Œåœ¨è’¸é¦è¿‡ç¨‹ä¸­æ¶ˆé™¤æ— ç”¨æ•°æ®ç‚¹ã€‚é€šè¿‡è®¾è®¡åŸºäºè½¨è¿¹çš„å°‘æ­¥å¼•å¯¼ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨æ›´å°‘çš„æ­¥éª¤ä¸­å®ç°è§†é¢‘ç”Ÿæˆï¼ŒåŒæ—¶å¼•å…¥å¯¹æŠ—è®­ç»ƒç­–ç•¥ä»¥æé«˜è§†é¢‘è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19846', 'title': 'Attention IoU: Examining Biases in CelebA using Attention Maps', 'url': 'https://huggingface.co/papers/2503.19846', 'abstract': "Computer vision models have been shown to exhibit and amplify biases across a wide array of datasets and tasks. Existing methods for quantifying bias in classification models primarily focus on dataset distribution and model performance on subgroups, overlooking the internal workings of a model. We introduce the Attention-IoU (Attention Intersection over Union) metric and related scores, which use attention maps to reveal biases within a model's internal representations and identify image features potentially causing the biases. First, we validate Attention-IoU on the synthetic Waterbirds dataset, showing that the metric accurately measures model bias. We then analyze the CelebA dataset, finding that Attention-IoU uncovers correlations beyond accuracy disparities. Through an investigation of individual attributes through the protected attribute of Male, we examine the distinct ways biases are represented in CelebA. Lastly, by subsampling the training set to change attribute correlations, we demonstrate that Attention-IoU reveals potential confounding variables not present in dataset labels.", 'score': 4, 'issue_id': 2921, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': 'a4039050131ab9d6', 'authors': ['Aaron Serianni', 'Tyler Zhu', 'Olga Russakovsky', 'Vikram V. Ramaswamy'], 'affiliations': ['Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2503.19846.jpg', 'data': {'categories': ['#ethics', '#benchmark', '#cv', '#dataset', '#interpretability'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ - Attention-IoU. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Attention-IoU Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Waterbirds Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ ĞµĞµ Ğº Ğ½Ğ°Ğ±Ğ¾Ñ€Ñƒ CelebA, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸, Ğ²Ñ‹Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ·Ğ° Ñ€Ğ°Ğ¼ĞºĞ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¹ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Attention-IoU Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ÑĞºĞ°Ğ¶Ğ°ÑÑ‰Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ, Ğ½Ğµ Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ² Ğ¼ĞµÑ‚ĞºĞ°Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Unveiling Biases with Attention-IoU in Computer Vision Models', 'desc': "This paper addresses the issue of bias in computer vision models, which can be amplified by the datasets they are trained on. The authors introduce a new metric called Attention-IoU, which utilizes attention maps to uncover biases in a model's internal representations rather than just focusing on dataset distribution or performance metrics. They validate this metric using the Waterbirds dataset and further analyze the CelebA dataset to reveal hidden correlations that go beyond mere accuracy differences. By manipulating the training set, they demonstrate that Attention-IoU can identify confounding variables that are not explicitly labeled in the dataset, providing deeper insights into model biases."}, 'zh': {'title': 'æ­ç¤ºæ¨¡å‹å†…éƒ¨åè§çš„æ³¨æ„åŠ›äº¤å¹¶æ¯”', 'desc': 'æœ¬æ–‡æ¢è®¨äº†è®¡ç®—æœºè§†è§‰æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†å’Œä»»åŠ¡ä¸­è¡¨ç°å‡ºçš„åè§ã€‚ç°æœ‰çš„é‡åŒ–åˆ†ç±»æ¨¡å‹åè§çš„æ–¹æ³•ä¸»è¦å…³æ³¨æ•°æ®é›†åˆ†å¸ƒå’Œæ¨¡å‹åœ¨å­ç¾¤ä½“ä¸Šçš„è¡¨ç°ï¼Œè€Œå¿½è§†äº†æ¨¡å‹å†…éƒ¨çš„å·¥ä½œæœºåˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†æ³¨æ„åŠ›äº¤å¹¶æ¯”ï¼ˆAttention-IoUï¼‰æŒ‡æ ‡ï¼Œé€šè¿‡æ³¨æ„åŠ›å›¾æ­ç¤ºæ¨¡å‹å†…éƒ¨è¡¨ç¤ºä¸­çš„åè§ï¼Œå¹¶è¯†åˆ«å¯èƒ½å¯¼è‡´åè§çš„å›¾åƒç‰¹å¾ã€‚é€šè¿‡å¯¹Waterbirdså’ŒCelebAæ•°æ®é›†çš„åˆ†æï¼Œæˆ‘ä»¬éªŒè¯äº†Attention-IoUçš„æœ‰æ•ˆæ€§ï¼Œå¹¶å‘ç°å…¶èƒ½å¤Ÿæ­ç¤ºè¶…å‡ºå‡†ç¡®æ€§å·®å¼‚çš„ç›¸å…³æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20756', 'title': 'ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving\n  Systems', 'url': 'https://huggingface.co/papers/2503.20756', 'abstract': "Recent advancements in Large Multimodal Models (LMMs) have shown promise in Autonomous Driving Systems (ADS). However, their direct application to ADS is hindered by challenges such as misunderstanding of traffic knowledge, complex road conditions, and diverse states of vehicle. To address these challenges, we propose the use of Knowledge Editing, which enables targeted modifications to a model's behavior without the need for full retraining. Meanwhile, we introduce ADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS, which includes various real-world scenarios, multiple data types, and comprehensive evaluation metrics. We conduct comprehensive experiments and derive several interesting conclusions. We hope that our work will contribute to the further advancement of knowledge editing applications in the field of autonomous driving. Code and data are available in https://github.com/zjunlp/EasyEdit.", 'score': 3, 'issue_id': 2919, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': '6b8affbfbdd5a426', 'authors': ['Chenxi Wang', 'Jizhan Fang', 'Xiang Chen', 'Bozhong Tian', 'Ziwen Xu', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['Nanjing University of Aeronautics and Astronautics', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.20756.jpg', 'data': {'categories': ['#multimodal', '#agents', '#dataset'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM) Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (ADS). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ADS-Edit Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸÑ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´ÑÑ‚Ğ².'}, 'en': {'title': 'Enhancing Autonomous Driving with Targeted Knowledge Editing', 'desc': "This paper discusses the use of Large Multimodal Models (LMMs) in Autonomous Driving Systems (ADS) and the challenges they face, such as traffic knowledge misunderstanding and complex road conditions. To overcome these issues, the authors propose a method called Knowledge Editing, which allows for specific adjustments to a model's behavior without needing to retrain it entirely. They also introduce ADS-Edit, a specialized dataset for multimodal knowledge editing in ADS, which includes diverse real-world scenarios and various data types. The paper presents experimental results that highlight the potential of knowledge editing to enhance the performance of autonomous driving systems."}, 'zh': {'title': 'çŸ¥è¯†ç¼–è¾‘åŠ©åŠ›è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„è¿›æ­¥', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿï¼ˆADSï¼‰ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚å°½ç®¡LMMsæœ‰å‰æ™¯ï¼Œä½†åœ¨äº¤é€šçŸ¥è¯†ç†è§£ã€å¤æ‚è·¯å†µå’Œè½¦è¾†å¤šæ ·æ€§ç­‰æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†çŸ¥è¯†ç¼–è¾‘çš„æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸å®Œå…¨é‡è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œé’ˆå¯¹æ€§åœ°ä¿®æ”¹æ¨¡å‹çš„è¡Œä¸ºã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ADS-Editï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºè‡ªåŠ¨é©¾é©¶è®¾è®¡çš„å¤šæ¨¡æ€çŸ¥è¯†ç¼–è¾‘æ•°æ®é›†ï¼ŒåŒ…å«å¤šç§çœŸå®åœºæ™¯å’Œæ•°æ®ç±»å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20220', 'title': 'DINeMo: Learning Neural Mesh Models with no 3D Annotations', 'url': 'https://huggingface.co/papers/2503.20220', 'abstract': 'Category-level 3D/6D pose estimation is a crucial step towards comprehensive 3D scene understanding, which would enable a broad range of applications in robotics and embodied AI. Recent works explored neural mesh models that approach a range of 2D and 3D tasks from an analysis-by-synthesis perspective. Despite the largely enhanced robustness to partial occlusion and domain shifts, these methods depended heavily on 3D annotations for part-contrastive learning, which confines them to a narrow set of categories and hinders efficient scaling. In this work, we present DINeMo, a novel neural mesh model that is trained with no 3D annotations by leveraging pseudo-correspondence obtained from large visual foundation models. We adopt a bidirectional pseudo-correspondence generation method, which produce pseudo correspondence utilize both local appearance features and global context information. Experimental results on car datasets demonstrate that our DINeMo outperforms previous zero- and few-shot 3D pose estimation by a wide margin, narrowing the gap with fully-supervised methods by 67.3%. Our DINeMo also scales effectively and efficiently when incorporating more unlabeled images during training, which demonstrate the advantages over supervised learning methods that rely on 3D annotations. Our project page is available at https://analysis-by-synthesis.github.io/DINeMo/.', 'score': 3, 'issue_id': 2919, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': 'f65c515bbb6af49b', 'authors': ['Weijie Guo', 'Guofeng Zhang', 'Wufei Ma', 'Alan Yuille'], 'affiliations': ['Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2503.20220.jpg', 'data': {'categories': ['#transfer_learning', '#3d', '#robotics', '#optimization'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ 3D-Ğ¿Ğ¾Ğ·Ğ° Ğ±ĞµĞ· 3D-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸', 'desc': 'DINeMo - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞµÑ‚ĞµĞ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 3D/6D Ğ¿Ğ¾Ğ·Ñ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ÑĞµĞ²Ğ´Ğ¾-ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ· ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. DINeMo Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 3D-Ğ¿Ğ¾Ğ·Ñ‹ Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¸ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° 67.3%. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing 3D Pose Estimation with Unlabeled Data', 'desc': 'This paper introduces DINeMo, a new neural mesh model designed for category-level 3D/6D pose estimation without relying on 3D annotations. It utilizes pseudo-correspondence generated from large visual foundation models, allowing it to learn from unlabeled data effectively. The model employs a bidirectional approach to generate pseudo correspondences, combining local appearance features with global context. Experimental results show that DINeMo significantly improves performance in zero- and few-shot 3D pose estimation, achieving results close to fully-supervised methods while being more scalable and efficient.'}, 'zh': {'title': 'æ— æ ‡æ³¨3Då§¿æ€ä¼°è®¡çš„æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç¥ç»ç½‘æ ¼æ¨¡å‹DINeMoï¼Œç”¨äºç±»åˆ«çº§çš„3D/6Då§¿æ€ä¼°è®¡ï¼Œæ—¨åœ¨æé«˜3Dåœºæ™¯ç†è§£çš„èƒ½åŠ›ã€‚ä¸ä»¥å¾€ä¾èµ–3Dæ ‡æ³¨çš„å­¦ä¹ æ–¹æ³•ä¸åŒï¼ŒDINeMoé€šè¿‡åˆ©ç”¨å¤§å‹è§†è§‰åŸºç¡€æ¨¡å‹è·å¾—çš„ä¼ªå¯¹åº”å…³ç³»è¿›è¡Œè®­ç»ƒï¼Œä»è€Œé¿å…äº†å¯¹3Dæ ‡æ³¨çš„ä¾èµ–ã€‚æˆ‘ä»¬é‡‡ç”¨åŒå‘ä¼ªå¯¹åº”ç”Ÿæˆæ–¹æ³•ï¼Œç»“åˆå±€éƒ¨å¤–è§‚ç‰¹å¾å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDINeMoåœ¨è½¦ç±»æ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ä¹‹å‰çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬3Då§¿æ€ä¼°è®¡æ–¹æ³•ï¼Œç¼©å°äº†ä¸å®Œå…¨ç›‘ç£æ–¹æ³•çš„å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20198', 'title': 'Beyond Words: Advancing Long-Text Image Generation via Multimodal\n  Autoregressive Models', 'url': 'https://huggingface.co/papers/2503.20198', 'abstract': 'Recent advancements in autoregressive and diffusion models have led to strong performance in image generation with short scene text words. However, generating coherent, long-form text in images, such as paragraphs in slides or documents, remains a major challenge for current generative models. We present the first work specifically focused on long text image generation, addressing a critical gap in existing text-to-image systems that typically handle only brief phrases or single sentences. Through comprehensive analysis of state-of-the-art autoregressive generation models, we identify the image tokenizer as a critical bottleneck in text generating quality. To address this, we introduce a novel text-focused, binary tokenizer optimized for capturing detailed scene text features. Leveraging our tokenizer, we develop \\ModelName, a multimodal autoregressive model that excels in generating high-quality long-text images with unprecedented fidelity. Our model offers robust controllability, enabling customization of text properties such as font style, size, color, and alignment. Extensive experiments demonstrate that \\ModelName~significantly outperforms SD3.5 Large~sd3 and GPT4o~gpt4o with DALL-E 3~dalle3 in generating long text accurately, consistently, and flexibly. Beyond its technical achievements, \\ModelName~opens up exciting opportunities for innovative applications like interleaved document and PowerPoint generation, establishing a new frontier in long-text image generating.', 'score': 3, 'issue_id': 2920, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': 'b2b966b253011624', 'authors': ['Alex Jinpeng Wang', 'Linjie Li', 'Zhengyuan Yang', 'Lijuan Wang', 'Min Li'], 'affiliations': ['Central South University', 'Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2503.20198.jpg', 'data': {'categories': ['#long_context', '#cv', '#multimodal'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞĞ¾Ğ²Ğ°Ñ ÑÑ€Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸', 'desc': 'Ğ¡Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ autoregressive Ğ¸ diffusion Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº Ğ°Ğ±Ğ·Ğ°Ñ†Ñ‹ Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…. Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ» Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… text-to-image. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ¾Ğ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ image tokenizer, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¹ tokenizer, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ÑƒÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing Long-Text Image Generation with \\ModelName', 'desc': 'This paper introduces a new approach to generating long-form text in images, which has been a challenge for existing generative models. The authors identify that the image tokenizer is a key limitation in the quality of text generation. To overcome this, they propose a novel binary tokenizer that focuses on capturing detailed features of scene text. Their multimodal autoregressive model, named \\ModelName, demonstrates superior performance in generating high-quality long-text images, allowing for customizable text properties and paving the way for new applications in document and presentation generation.'}, 'zh': {'title': 'é•¿æ–‡æœ¬å›¾åƒç”Ÿæˆçš„æ–°çªç ´', 'desc': 'æœ€è¿‘ï¼Œè‡ªå›å½’å’Œæ‰©æ•£æ¨¡å‹çš„è¿›å±•ä½¿å¾—çŸ­æ–‡æœ¬å›¾åƒç”Ÿæˆè¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œç”Ÿæˆè¿è´¯çš„é•¿æ–‡æœ¬å›¾åƒï¼ˆå¦‚å¹»ç¯ç‰‡æˆ–æ–‡æ¡£ä¸­çš„æ®µè½ï¼‰ä»ç„¶æ˜¯å½“å‰ç”Ÿæˆæ¨¡å‹é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ã€‚æˆ‘ä»¬é¦–æ¬¡ä¸“æ³¨äºé•¿æ–‡æœ¬å›¾åƒç”Ÿæˆï¼Œå¡«è¡¥äº†ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒç³»ç»Ÿçš„å…³é”®ç©ºç™½ã€‚é€šè¿‡åˆ†ææœ€å…ˆè¿›çš„è‡ªå›å½’ç”Ÿæˆæ¨¡å‹ï¼Œæˆ‘ä»¬å‘ç°å›¾åƒåˆ†è¯å™¨æ˜¯å½±å“æ–‡æœ¬ç”Ÿæˆè´¨é‡çš„å…³é”®ç“¶é¢ˆï¼Œå› æ­¤æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬ä¸“æ³¨çš„äºŒè¿›åˆ¶åˆ†è¯å™¨ï¼Œä»¥ä¼˜åŒ–ç»†èŠ‚åœºæ™¯æ–‡æœ¬ç‰¹å¾çš„æ•æ‰ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.17358', 'title': 'Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred\n  Image', 'url': 'https://huggingface.co/papers/2503.17358', 'abstract': 'In many robotics and VR/AR applications, fast camera motions cause a high level of motion blur, causing existing camera pose estimation methods to fail. In this work, we propose a novel framework that leverages motion blur as a rich cue for motion estimation rather than treating it as an unwanted artifact. Our approach works by predicting a dense motion flow field and a monocular depth map directly from a single motion-blurred image. We then recover the instantaneous camera velocity by solving a linear least squares problem under the small motion assumption. In essence, our method produces an IMU-like measurement that robustly captures fast and aggressive camera movements. To train our model, we construct a large-scale dataset with realistic synthetic motion blur derived from ScanNet++v2 and further refine our model by training end-to-end on real data using our fully differentiable pipeline. Extensive evaluations on real-world benchmarks demonstrate that our method achieves state-of-the-art angular and translational velocity estimates, outperforming current methods like MASt3R and COLMAP.', 'score': 3, 'issue_id': 2927, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': '491a350f070233ec', 'authors': ['Jerred Chen', 'Ronald Clark'], 'affiliations': ['University of Oxford Department of Computer Science'], 'pdf_title_img': 'assets/pdf/title_img/2503.17358.jpg', 'data': {'categories': ['#dataset', '#robotics', '#training', '#cv', '#benchmark'], 'emoji': 'ğŸ“·', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ğµ Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ°Ñ€Ñ‚Ñƒ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ°Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ°Ğ¸Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ°.'}, 'en': {'title': 'Harnessing Motion Blur for Enhanced Camera Motion Estimation', 'desc': "This paper introduces a new method for estimating camera motion in situations where fast movements cause motion blur, which typically hinders existing techniques. Instead of viewing motion blur as a problem, the authors utilize it to predict a dense motion flow field and a depth map from a single blurred image. They calculate the camera's instantaneous velocity by solving a linear least squares problem, effectively treating the motion blur as valuable information. The proposed framework is trained on a large dataset and shows superior performance in estimating camera velocities compared to traditional methods."}, 'zh': {'title': 'åˆ©ç”¨è¿åŠ¨æ¨¡ç³Šæå‡ç›¸æœºè¿åŠ¨ä¼°è®¡çš„åˆ›æ–°æ–¹æ³•', 'desc': 'åœ¨è®¸å¤šæœºå™¨äººå’Œè™šæ‹Ÿç°å®/å¢å¼ºç°å®åº”ç”¨ä¸­ï¼Œå¿«é€Ÿçš„ç›¸æœºè¿åŠ¨ä¼šå¯¼è‡´ä¸¥é‡çš„è¿åŠ¨æ¨¡ç³Šï¼Œç°æœ‰çš„ç›¸æœºå§¿æ€ä¼°è®¡æ–¹æ³•å› æ­¤å¤±æ•ˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œå°†è¿åŠ¨æ¨¡ç³Šè§†ä¸ºè¿åŠ¨ä¼°è®¡çš„ä¸°å¯Œçº¿ç´¢ï¼Œè€Œä¸æ˜¯ä¸å¿…è¦çš„ä¼ªå½±ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»å•ä¸€çš„è¿åŠ¨æ¨¡ç³Šå›¾åƒä¸­ç›´æ¥é¢„æµ‹å¯†é›†çš„è¿åŠ¨æµåœºå’Œå•ç›®æ·±åº¦å›¾æ¥å·¥ä½œã€‚æˆ‘ä»¬çš„æ¨¡å‹ç»è¿‡å¤§è§„æ¨¡åˆæˆè¿åŠ¨æ¨¡ç³Šæ•°æ®é›†è®­ç»ƒï¼Œå¹¶åœ¨çœŸå®æ•°æ®ä¸Šè¿›è¡Œç«¯åˆ°ç«¯çš„å¾®è°ƒï¼Œæœ€ç»ˆåœ¨çœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„MASt3Rå’ŒCOLMAPç­‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20641', 'title': 'Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging', 'url': 'https://huggingface.co/papers/2503.20641', 'abstract': "The transition from System 1 to System 2 reasoning in large language models (LLMs) has marked significant advancements in handling complex tasks through deliberate, iterative thinking. However, this progress often comes at the cost of efficiency, as models tend to overthink, generating redundant reasoning steps without proportional improvements in output quality. Long-to-Short (L2S) reasoning has emerged as a promising solution to this challenge, aiming to balance reasoning depth with practical efficiency. While existing approaches, such as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt engineering, have shown potential, they are either computationally expensive or unstable. Model merging, on the other hand, offers a cost-effective and robust alternative by integrating the quick-thinking capabilities of System 1 models with the methodical reasoning of System 2 models. In this work, we present a comprehensive empirical study on model merging for L2S reasoning, exploring diverse methodologies, including task-vector-based, SVD-based, and activation-informed merging. Our experiments reveal that model merging can reduce average response length by up to 55% while preserving or even improving baseline performance. We also identify a strong correlation between model scale and merging efficacy with extensive evaluations on 1.5B/7B/14B/32B models. Furthermore, we investigate the merged model's ability to self-critique and self-correct, as well as its adaptive response length based on task complexity. Our findings highlight model merging as a highly efficient and effective paradigm for L2S reasoning, offering a practical solution to the overthinking problem while maintaining the robustness of System 2 reasoning. This work can be found on Github https://github.com/hahahawu/Long-to-Short-via-Model-Merging.", 'score': 2, 'issue_id': 2932, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': '52b4dbdb179d7229', 'authors': ['Han Wu', 'Yuxuan Yao', 'Shuqi Liu', 'Zehua Liu', 'Xiaojin Fu', 'Xiongwei Han', 'Xing Li', 'Hui-Ling Zhen', 'Tao Zhong', 'Mingxuan Yuan'], 'affiliations': ['Huawei Noahs Ark Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.20641.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² LLM: Ğ¾Ñ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğº ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¾Ğ¼Ñƒ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ğ¾Ñ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ (System 1) Ğº Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ (System 2). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (model merging) Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ÑÑ€ĞµĞ´Ğ½ÑÑ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° 55% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Efficient Reasoning through Model Merging', 'desc': 'This paper discusses the transition from quick, intuitive reasoning (System 1) to more deliberate, analytical reasoning (System 2) in large language models (LLMs). It highlights the inefficiencies that arise when models overthink, leading to unnecessary complexity without significant gains in output quality. The authors propose a method called Long-to-Short (L2S) reasoning, which aims to optimize the balance between deep reasoning and efficiency. They introduce model merging as a solution, which combines the strengths of both reasoning systems, demonstrating that this approach can significantly reduce response length while maintaining or enhancing performance.'}, 'zh': {'title': 'æ¨¡å‹åˆå¹¶ï¼šé«˜æ•ˆçš„é•¿åˆ°çŸ­æ¨ç†è§£å†³æ–¹æ¡ˆ', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸­ä»ç³»ç»Ÿ1æ¨ç†åˆ°ç³»ç»Ÿ2æ¨ç†çš„è½¬å˜ã€‚å°½ç®¡è¿™ç§è¿›æ­¥æé«˜äº†æ¨ç†çš„æ·±åº¦ï¼Œä½†å¾€å¾€å¯¼è‡´æ•ˆç‡ä¸‹é™ï¼Œæ¨¡å‹å¯èƒ½ä¼šäº§ç”Ÿå†—ä½™çš„æ¨ç†æ­¥éª¤ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé•¿åˆ°çŸ­ï¼ˆL2Sï¼‰æ¨ç†æå‡ºäº†ä¸€ç§å¹³è¡¡æ¨ç†æ·±åº¦ä¸æ•ˆç‡çš„æ–¹æ¡ˆã€‚é€šè¿‡æ¨¡å‹åˆå¹¶ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå°†ç³»ç»Ÿ1æ¨¡å‹çš„å¿«é€Ÿæ€ç»´ä¸ç³»ç»Ÿ2æ¨¡å‹çš„ç³»ç»Ÿæ€§æ¨ç†ç»“åˆï¼Œä»è€Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—å‡å°‘å“åº”é•¿åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19953', 'title': 'Self-Supervised Learning of Motion Concepts by Optimizing\n  Counterfactuals', 'url': 'https://huggingface.co/papers/2503.19953', 'abstract': "Estimating motion in videos is an essential computer vision problem with many downstream applications, including controllable video generation and robotics. Current solutions are primarily trained using synthetic data or require tuning of situation-specific heuristics, which inherently limits these models' capabilities in real-world contexts. Despite recent developments in large-scale self-supervised learning from videos, leveraging such representations for motion estimation remains relatively underexplored. In this work, we develop Opt-CWM, a self-supervised technique for flow and occlusion estimation from a pre-trained next-frame prediction model. Opt-CWM works by learning to optimize counterfactual probes that extract motion information from a base video model, avoiding the need for fixed heuristics while training on unrestricted video inputs. We achieve state-of-the-art performance for motion estimation on real-world videos while requiring no labeled data.", 'score': 2, 'issue_id': 2919, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': '01093e8b98f32607', 'authors': ['Stefan Stojanov', 'David Wendt', 'Seungwoo Kim', 'Rahul Venkatesh', 'Kevin Feigelis', 'Jiajun Wu', 'Daniel LK Yamins'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2503.19953.jpg', 'data': {'categories': ['#cv', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Opt-CWM - ÑÑ‚Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ¸ Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°. ĞĞ½ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ñ… Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ· Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½ĞµĞ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ²Ñ…Ğ¾Ğ´Ğ°Ñ…. Opt-CWM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Revolutionizing Motion Estimation with Self-Supervised Learning', 'desc': 'This paper presents Opt-CWM, a self-supervised method for estimating motion in videos, which is crucial for applications like video generation and robotics. Unlike traditional approaches that rely on synthetic data or specific heuristics, Opt-CWM utilizes a pre-trained next-frame prediction model to learn motion information directly from real-world video inputs. The technique optimizes counterfactual probes, allowing it to adaptively extract flow and occlusion data without needing fixed rules. As a result, Opt-CWM achieves state-of-the-art performance in motion estimation while eliminating the requirement for labeled datasets.'}, 'zh': {'title': 'è‡ªç›‘ç£è¿åŠ¨ä¼°è®¡çš„æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†è§†é¢‘ä¸­çš„è¿åŠ¨ä¼°è®¡é—®é¢˜ï¼Œè¿™æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ä¸ªé‡è¦è¯¾é¢˜ï¼Œå¹¿æ³›åº”ç”¨äºå¯æ§è§†é¢‘ç”Ÿæˆå’Œæœºå™¨äººæŠ€æœ¯ã€‚ç°æœ‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–åˆæˆæ•°æ®è®­ç»ƒæˆ–ç‰¹å®šæƒ…å¢ƒçš„å¯å‘å¼è°ƒæ•´ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨çœŸå®åœºæ™¯ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºOpt-CWMçš„è‡ªç›‘ç£æŠ€æœ¯ï¼Œé€šè¿‡é¢„è®­ç»ƒçš„ä¸‹ä¸€å¸§é¢„æµ‹æ¨¡å‹è¿›è¡ŒæµåŠ¨å’Œé®æŒ¡ä¼°è®¡ã€‚Opt-CWMé€šè¿‡ä¼˜åŒ–åäº‹å®æ¢é’ˆæ¥æå–è¿åŠ¨ä¿¡æ¯ï¼Œé¿å…äº†å›ºå®šå¯å‘å¼çš„éœ€æ±‚ï¼Œå¹¶åœ¨æ— éœ€æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œåœ¨çœŸå®è§†é¢‘ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è¿åŠ¨ä¼°è®¡æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16870', 'title': 'Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs', 'url': 'https://huggingface.co/papers/2503.16870', 'abstract': "Knowledge distillation can be a cost-effective technique to distill knowledge in Large Language Models, if the teacher output logits can be pre-computed and cached. However, successfully applying this to pre-training remains largely unexplored. In this work, we prove that naive approaches for sparse knowledge distillation such as caching Top-K probabilities, while intuitive, provide biased estimates of teacher probability distribution to the student, resulting in suboptimal performance and calibration. We propose an importance-sampling-based method `Random Sampling Knowledge Distillation', which provides unbiased estimates, preserves the gradient in expectation, and requires storing significantly sparser logits. Our method enables faster training of student models with marginal overhead (<10%) compared to cross-entropy based training, while maintaining competitive performance compared to full distillation, across a range of model sizes from 300M to 3B.", 'score': 2, 'issue_id': 2921, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': 'e8b397bd8ee5118a', 'authors': ['Anshumann', 'Mohd Abbas Zaidi', 'Akhil Kedia', 'Jinwoo Ahn', 'Taehwak Kwon', 'Kangwook Lee', 'Haejun Lee', 'Joohyung Lee'], 'affiliations': ['Samsung Research, Seoul'], 'pdf_title_img': 'assets/pdf/title_img/2503.16870.jpg', 'data': {'categories': ['#optimization', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Top-K Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ´Ğ°ÑÑ‚ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ 'Random Sampling Knowledge Distillation', Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞµ Ğ¿Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµÑĞ¼ĞµÑ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚ Ğ² Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºÑ€Ğ¾ÑÑ-ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ."}, 'en': {'title': 'Unbiased Knowledge Distillation for Efficient Model Training', 'desc': "This paper discusses a new method for knowledge distillation in Large Language Models, focusing on the challenges of pre-training. The authors highlight that traditional methods, like caching Top-K probabilities, can lead to biased teacher probability distributions, which negatively affect the student's performance. They introduce 'Random Sampling Knowledge Distillation', an importance-sampling approach that provides unbiased estimates and maintains gradient preservation. This method allows for faster training with minimal overhead while achieving competitive results compared to full distillation across various model sizes."}, 'zh': {'title': 'é«˜æ•ˆçš„çŸ¥è¯†è’¸é¦æ–¹æ³•æå‡å­¦ç”Ÿæ¨¡å‹è®­ç»ƒ', 'desc': 'çŸ¥è¯†è’¸é¦æ˜¯ä¸€ç§æœ‰æ•ˆçš„æŠ€æœ¯ï¼Œå¯ä»¥ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æå–çŸ¥è¯†ã€‚æœ¬æ–‡æ¢è®¨äº†åœ¨é¢„è®­ç»ƒé˜¶æ®µåº”ç”¨çŸ¥è¯†è’¸é¦çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ç®€å•çš„ç¨€ç–çŸ¥è¯†è’¸é¦æ–¹æ³•å¯èƒ½å¯¼è‡´å­¦ç”Ÿæ¨¡å‹è·å¾—åå·®çš„æ•™å¸ˆæ¦‚ç‡åˆ†å¸ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé‡è¦æ€§é‡‡æ ·çš„éšæœºé‡‡æ ·çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œèƒ½å¤Ÿæä¾›æ— åä¼°è®¡ï¼Œå¹¶åœ¨æœŸæœ›ä¸­ä¿ç•™æ¢¯åº¦ã€‚è¯¥æ–¹æ³•åœ¨å­˜å‚¨ç¨€ç–logitsçš„åŒæ—¶ï¼Œèƒ½åŠ å¿«å­¦ç”Ÿæ¨¡å‹çš„è®­ç»ƒé€Ÿåº¦ï¼Œå¹¶åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ä¸‹ä¿æŒç«äº‰åŠ›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18929', 'title': 'Trajectory Balance with Asynchrony: Decoupling Exploration and Learning\n  for Fast, Scalable LLM Post-Training', 'url': 'https://huggingface.co/papers/2503.18929', 'abstract': 'Reinforcement learning (RL) is a critical component of large language model (LLM) post-training. However, existing on-policy algorithms used for post-training are inherently incompatible with the use of experience replay buffers, which can be populated scalably by distributed off-policy actors to enhance exploration as compute increases. We propose efficiently obtaining this benefit of replay buffers via Trajectory Balance with Asynchrony (TBA), a massively scalable LLM RL system. In contrast to existing approaches, TBA uses a larger fraction of compute on search, constantly generating off-policy data for a central replay buffer. A training node simultaneously samples data from this buffer based on reward or recency to update the policy using Trajectory Balance (TB), a diversity-seeking RL objective introduced for GFlowNets. TBA offers three key advantages: (1) decoupled training and search, speeding up training wall-clock time by 4x or more; (2) improved diversity through large-scale off-policy sampling; and (3) scalable search for sparse reward settings. On mathematical reasoning, preference-tuning, and automated red-teaming (diverse and representative post-training tasks), TBA produces speed and performance improvements over strong baselines.', 'score': 1, 'issue_id': 2932, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': 'ae8d68c1a89d88a1', 'authors': ['Brian R. Bartoldson', 'Siddarth Venkatraman', 'James Diffenderfer', 'Moksh Jain', 'Tal Ben-Nun', 'Seanie Lee', 'Minsu Kim', 'Johan Obando-Ceron', 'Yoshua Bengio', 'Bhavya Kailkhura'], 'affiliations': ['CIFAR Fellow', 'KAIST', 'Lawrence Livermore National Laboratory', 'Mila Quebec AI Institute', 'Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2503.18929.jpg', 'data': {'categories': ['#optimization', '#rl', '#reasoning', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'TBA: ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Trajectory Balance with Asynchrony (TBA). TBA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±ÑƒÑ„ĞµÑ€ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±Ñ‰ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. TBA Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Boosting LLM Training with Efficient Replay and Exploration', 'desc': 'This paper introduces a new method called Trajectory Balance with Asynchrony (TBA) for improving reinforcement learning in large language models (LLMs). TBA allows the use of experience replay buffers, which help in better exploration by storing past experiences and using them for training. The method separates the training and search processes, leading to faster training times and enhanced diversity in the data sampled. Overall, TBA shows significant improvements in performance and efficiency on various post-training tasks compared to existing methods.'}, 'zh': {'title': 'æå‡å¼ºåŒ–å­¦ä¹ æ•ˆç‡çš„è½¨è¿¹å¹³è¡¡ä¸å¼‚æ­¥æ–¹æ³•', 'desc': 'å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åè®­ç»ƒçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚ç°æœ‰çš„åœ¨çº¿ç®—æ³•ä¸ç»éªŒé‡æ”¾ç¼“å†²åŒºä¸å…¼å®¹ï¼Œè€Œæˆ‘ä»¬æå‡ºçš„è½¨è¿¹å¹³è¡¡ä¸å¼‚æ­¥ï¼ˆTBAï¼‰æ–¹æ³•å¯ä»¥æœ‰æ•ˆåˆ©ç”¨é‡æ”¾ç¼“å†²åŒºçš„ä¼˜åŠ¿ã€‚TBAé€šè¿‡å°†è®¡ç®—èµ„æºæ›´å¤šåœ°ç”¨äºæœç´¢ï¼ŒæŒç»­ç”Ÿæˆç¦»çº¿æ•°æ®æ¥æ›´æ–°ç­–ç•¥ï¼Œä»è€ŒåŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚è¯¥æ–¹æ³•åœ¨æ•°å­¦æ¨ç†ã€åå¥½è°ƒä¼˜å’Œè‡ªåŠ¨çº¢é˜Ÿç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„é€Ÿåº¦å’Œæ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15893', 'title': 'UniHDSA: A Unified Relation Prediction Approach for Hierarchical\n  Document Structure Analysis', 'url': 'https://huggingface.co/papers/2503.15893', 'abstract': "Document structure analysis, aka document layout analysis, is crucial for understanding both the physical layout and logical structure of documents, serving information retrieval, document summarization, knowledge extraction, etc. Hierarchical Document Structure Analysis (HDSA) specifically aims to restore the hierarchical structure of documents created using authoring software with hierarchical schemas. Previous research has primarily followed two approaches: one focuses on tackling specific subtasks of HDSA in isolation, such as table detection or reading order prediction, while the other adopts a unified framework that uses multiple branches or modules, each designed to address a distinct task. In this work, we propose a unified relation prediction approach for HDSA, called UniHDSA, which treats various HDSA sub-tasks as relation prediction problems and consolidates relation prediction labels into a unified label space. This allows a single relation prediction module to handle multiple tasks simultaneously, whether at a page-level or document-level structure analysis. To validate the effectiveness of UniHDSA, we develop a multimodal end-to-end system based on Transformer architectures. Extensive experimental results demonstrate that our approach achieves state-of-the-art performance on a hierarchical document structure analysis benchmark, Comp-HRDoc, and competitive results on a large-scale document layout analysis dataset, DocLayNet, effectively illustrating the superiority of our method across all sub-tasks. The Comp-HRDoc benchmark and UniHDSA's configurations are publicly available at https://github.com/microsoft/CompHRDoc.", 'score': 1, 'issue_id': 2932, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '459fe3a2bedb5ed6', 'authors': ['Jiawei Wang', 'Kai Hu', 'Qiang Huo'], 'affiliations': ['Department of EEIS, University of Science and Technology of China, Hefei, 230026, China', 'Microsoft Research Asia, Beijing, 100080, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.15893.jpg', 'data': {'categories': ['#dataset', '#architecture', '#multimodal', '#benchmark'], 'emoji': 'ğŸ“„', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² (HDSA) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ UniHDSA. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ HDSA ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğº. UniHDSA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Transformer Ğ´Ğ»Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ UniHDSA.'}, 'en': {'title': 'Unified Relation Prediction for Enhanced Document Structure Analysis', 'desc': 'This paper introduces a new method called UniHDSA for Hierarchical Document Structure Analysis (HDSA), which focuses on understanding the layout and structure of documents. Unlike previous methods that tackled individual tasks separately, UniHDSA treats various HDSA tasks as relation prediction problems, allowing for a more integrated approach. The method uses a single relation prediction module to analyze both page-level and document-level structures simultaneously. Experimental results show that UniHDSA outperforms existing methods on benchmark datasets, demonstrating its effectiveness in document layout analysis.'}, 'zh': {'title': 'ç»Ÿä¸€å…³ç³»é¢„æµ‹ï¼Œæå‡æ–‡æ¡£ç»“æ„åˆ†æ', 'desc': 'æ–‡æ¡£ç»“æ„åˆ†æå¯¹äºç†è§£æ–‡æ¡£çš„ç‰©ç†å¸ƒå±€å’Œé€»è¾‘ç»“æ„è‡³å…³é‡è¦ï¼Œæ¶‰åŠä¿¡æ¯æ£€ç´¢ã€æ–‡æ¡£æ‘˜è¦å’ŒçŸ¥è¯†æå–ç­‰åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å…³ç³»é¢„æµ‹æ–¹æ³•UniHDSAï¼Œæ—¨åœ¨å°†æ–‡æ¡£ç»“æ„åˆ†æçš„å„ä¸ªå­ä»»åŠ¡è§†ä¸ºå…³ç³»é¢„æµ‹é—®é¢˜ï¼Œå¹¶å°†å…³ç³»é¢„æµ‹æ ‡ç­¾æ•´åˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ ‡ç­¾ç©ºé—´ä¸­ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œå•ä¸€çš„å…³ç³»é¢„æµ‹æ¨¡å—èƒ½å¤ŸåŒæ—¶å¤„ç†å¤šä¸ªä»»åŠ¡ï¼Œæ— è®ºæ˜¯åœ¨é¡µé¢çº§åˆ«è¿˜æ˜¯æ–‡æ¡£çº§åˆ«çš„ç»“æ„åˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniHDSAåœ¨å±‚æ¬¡æ–‡æ¡£ç»“æ„åˆ†æåŸºå‡†Comp-HRDocä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨å¤§è§„æ¨¡æ–‡æ¡£å¸ƒå±€åˆ†ææ•°æ®é›†DocLayNetä¸Šä¹Ÿå–å¾—äº†ç«äº‰åŠ›çš„ç»“æœï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨å„ä¸ªå­ä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10997', 'title': 'RONA: Pragmatically Diverse Image Captioning with Coherence Relations', 'url': 'https://huggingface.co/papers/2503.10997', 'abstract': 'Writing Assistants (e.g., Grammarly, Microsoft Copilot) traditionally generate diverse image captions by employing syntactic and semantic variations to describe image components. However, human-written captions prioritize conveying a central message alongside visual descriptions using pragmatic cues. To enhance pragmatic diversity, it is essential to explore alternative ways of communicating these messages in conjunction with visual content. To address this challenge, we propose RONA, a novel prompting strategy for Multi-modal Large Language Models (MLLM) that leverages Coherence Relations as an axis for variation. We demonstrate that RONA generates captions with better overall diversity and ground-truth alignment, compared to MLLM baselines across multiple domains. Our code is available at: https://github.com/aashish2000/RONA', 'score': 1, 'issue_id': 2932, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 14', 'zh': '3æœˆ14æ—¥'}, 'hash': 'd3b2354bbfc88139', 'authors': ['Aashish Anantha Ramakrishnan', 'Aadarsh Anantha Ramakrishnan', 'Dongwon Lee'], 'affiliations': ['National Institute of Technology, Tiruchirappalli', 'The Pennsylvania State University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10997.jpg', 'data': {'categories': ['#open_source', '#story_generation', '#multimodal', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'RONA: Ğ¿Ñ€Ğ°Ğ³Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ RONA Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², RONA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ³Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RONA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ MLLM Ğ¿Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑĞ¼ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ ÑĞ²Ğ¾ĞµĞ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'RONA: Enhancing Caption Diversity with Pragmatic Cues', 'desc': 'This paper introduces RONA, a new prompting strategy designed for Multi-modal Large Language Models (MLLM) to improve the diversity of image captions. Unlike traditional methods that focus on syntactic and semantic variations, RONA emphasizes pragmatic cues to convey a central message alongside visual descriptions. By utilizing Coherence Relations, RONA enhances the way messages are communicated in relation to images. The results show that RONA outperforms existing MLLM baselines in generating captions that are more diverse and closely aligned with ground-truth descriptions across various domains.'}, 'zh': {'title': 'RONAï¼šæå‡å›¾åƒæ ‡é¢˜å¤šæ ·æ€§çš„åˆ›æ–°ç­–ç•¥', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æç¤ºç­–ç•¥RONAï¼Œç”¨äºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œæ—¨åœ¨æé«˜å›¾åƒæ ‡é¢˜çš„å¤šæ ·æ€§ã€‚ä¼ ç»Ÿçš„å†™ä½œåŠ©æ‰‹ç”Ÿæˆçš„æ ‡é¢˜å¾€å¾€ä¾§é‡äºè¯­æ³•å’Œè¯­ä¹‰çš„å˜åŒ–ï¼Œè€Œäººç±»æ’°å†™çš„æ ‡é¢˜åˆ™æ›´æ³¨é‡ä¼ è¾¾ä¸­å¿ƒä¿¡æ¯ã€‚RONAé€šè¿‡åˆ©ç”¨ä¸€è‡´æ€§å…³ç³»ä½œä¸ºå˜åŒ–çš„è½´å¿ƒï¼Œæ¢ç´¢äº†ä¸è§†è§‰å†…å®¹ç»“åˆçš„æ›¿ä»£æ²Ÿé€šæ–¹å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRONAç”Ÿæˆçš„æ ‡é¢˜åœ¨å¤šæ ·æ€§å’ŒçœŸå®å¯¹é½åº¦ä¸Šä¼˜äºç°æœ‰çš„MLLMåŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20731', 'title': 'RecTable: Fast Modeling Tabular Data with Rectified Flow', 'url': 'https://huggingface.co/papers/2503.20731', 'abstract': 'Score-based or diffusion models generate high-quality tabular data, surpassing GAN-based and VAE-based models. However, these methods require substantial training time. In this paper, we introduce RecTable, which uses the rectified flow modeling, applied in such as text-to-image generation and text-to-video generation. RecTable features a simple architecture consisting of a few stacked gated linear unit blocks. Additionally, our training strategies are also simple, incorporating a mixed-type noise distribution and a logit-normal timestep distribution. Our experiments demonstrate that RecTable achieves competitive performance compared to the several state-of-the-art diffusion and score-based models while reducing the required training time. Our code is available at https://github.com/fmp453/rectable.', 'score': 0, 'issue_id': 2935, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': 'dfe7dcfbd0067c32', 'authors': ['Masane Fuchi', 'Tomohiro Takagi'], 'affiliations': ['Meiji University'], 'pdf_title_img': 'assets/pdf/title_img/2503.20731.jpg', 'data': {'categories': ['#open_source', '#architecture', '#dataset', '#optimization', '#diffusion', '#training'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'RecTable: Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ RecTable - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ²Ñ‹Ğ¿Ñ€ÑĞ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° (rectified flow). RecTable Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ½Ğ¸Ñ‚Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‚-Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RecTable Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'RecTable: Fast and Efficient Tabular Data Generation', 'desc': 'This paper presents RecTable, a novel approach for generating high-quality tabular data using rectified flow modeling. Unlike traditional methods like GANs and VAEs, RecTable significantly reduces training time while maintaining competitive performance. The architecture is straightforward, utilizing stacked gated linear unit blocks, which simplifies the model design. Additionally, the training strategies involve a mixed-type noise distribution and a logit-normal timestep distribution, enhancing efficiency and effectiveness in data generation.'}, 'zh': {'title': 'RecTableï¼šé«˜æ•ˆç”Ÿæˆè¡¨æ ¼æ•°æ®çš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¨¡å‹RecTableï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡çš„è¡¨æ ¼æ•°æ®ï¼Œè¶…è¶Šäº†åŸºäºGANå’ŒVAEçš„æ¨¡å‹ã€‚RecTableé‡‡ç”¨äº†ä¿®æ­£æµå»ºæ¨¡ï¼Œå…·æœ‰ç®€å•çš„æ¶æ„ï¼Œç”±å‡ ä¸ªå †å çš„é—¨æ§çº¿æ€§å•å…ƒå—ç»„æˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨è®­ç»ƒç­–ç•¥ä¸Šä¹Ÿå¾ˆç®€å•ï¼Œç»“åˆäº†æ··åˆç±»å‹çš„å™ªå£°åˆ†å¸ƒå’Œå¯¹æ•°æ­£æ€æ—¶é—´æ­¥åˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRecTableåœ¨æ€§èƒ½ä¸Šä¸å¤šç§æœ€å…ˆè¿›çš„æ‰©æ•£å’Œè¯„åˆ†æ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶å‡å°‘äº†è®­ç»ƒæ—¶é—´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.17970', 'title': 'PathoHR: Breast Cancer Survival Prediction on High-Resolution\n  Pathological Images', 'url': 'https://huggingface.co/papers/2503.17970', 'abstract': "Breast cancer survival prediction in computational pathology presents a remarkable challenge due to tumor heterogeneity. For instance, different regions of the same tumor in the pathology image can show distinct morphological and molecular characteristics. This makes it difficult to extract representative features from whole slide images (WSIs) that truly reflect the tumor's aggressive potential and likely survival outcomes. In this paper, we present PathoHR, a novel pipeline for accurate breast cancer survival prediction that enhances any size of pathological images to enable more effective feature learning. Our approach entails (1) the incorporation of a plug-and-play high-resolution Vision Transformer (ViT) to enhance patch-wise WSI representation, enabling more detailed and comprehensive feature extraction, (2) the systematic evaluation of multiple advanced similarity metrics for comparing WSI-extracted features, optimizing the representation learning process to better capture tumor characteristics, (3) the demonstration that smaller image patches enhanced follow the proposed pipeline can achieve equivalent or superior prediction accuracy compared to raw larger patches, while significantly reducing computational overhead. Experimental findings valid that PathoHR provides the potential way of integrating enhanced image resolution with optimized feature learning to advance computational pathology, offering a promising direction for more accurate and efficient breast cancer survival prediction. Code will be available at https://github.com/AIGeeksGroup/PathoHR.", 'score': 0, 'issue_id': 2928, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 23', 'zh': '3æœˆ23æ—¥'}, 'hash': '7982f77e78076ec7', 'authors': ['Yang Luo', 'Shiru Wang', 'Jun Liu', 'Jiaxuan Xiao', 'Rundong Xue', 'Zeyu Zhang', 'Hao Zhang', 'Yu Lu', 'Yang Zhao', 'Yutong Xie'], 'affiliations': ['ANU', 'DLMU', 'Dartmouth', 'La Trobe', 'MBZUAI', 'NUP', 'SZTU', 'UCAS', 'XJLTU', 'XJTU'], 'pdf_title_img': 'assets/pdf/title_img/2503.17970.jpg', 'data': {'categories': ['#cv', '#healthcare', '#training'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ° Ñ€Ğ°ĞºĞ° Ğ³Ñ€ÑƒĞ´Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'PathoHR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°ĞºĞµ Ğ¼Ğ¾Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¶ĞµĞ»ĞµĞ·Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»ÑĞ±Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Vision Transformer Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ‚Ñ‡-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‡Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ½ĞµĞ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‡Ğ¸. PathoHR Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing Breast Cancer Survival Prediction with PathoHR', 'desc': 'This paper addresses the challenge of predicting breast cancer survival by focusing on the variability within tumors, which can complicate feature extraction from whole slide images (WSIs). The authors introduce PathoHR, a new pipeline that utilizes a high-resolution Vision Transformer (ViT) to improve the representation of tumor features at a patch level. They evaluate various similarity metrics to optimize the learning process, allowing for better capture of tumor characteristics. The results show that smaller, enhanced image patches can achieve similar or better prediction accuracy compared to larger patches, while also being more computationally efficient.'}, 'zh': {'title': 'æå‡ä¹³è…ºç™Œç”Ÿå­˜é¢„æµ‹çš„PathoHRç®¡é“', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºPathoHRçš„æ–°å‹ç®¡é“ï¼Œç”¨äºæé«˜ä¹³è…ºç™Œç”Ÿå­˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚ç”±äºè‚¿ç˜¤çš„å¼‚è´¨æ€§ï¼Œç—…ç†å›¾åƒä¸­çš„ä¸åŒåŒºåŸŸå¯èƒ½è¡¨ç°å‡ºä¸åŒçš„å½¢æ€å’Œåˆ†å­ç‰¹å¾ï¼Œè¿™ä½¿å¾—ä»å…¨åˆ‡ç‰‡å›¾åƒä¸­æå–ä»£è¡¨æ€§ç‰¹å¾å˜å¾—å›°éš¾ã€‚PathoHRé€šè¿‡å¼•å…¥é«˜åˆ†è¾¨ç‡çš„è§†è§‰å˜æ¢å™¨ï¼ˆViTï¼‰æ¥å¢å¼ºå›¾åƒè¡¥ä¸çš„è¡¨ç¤ºï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„ç‰¹å¾å­¦ä¹ ã€‚æ­¤å¤–ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡è¯¥ç®¡é“å¢å¼ºçš„å°å›¾åƒè¡¥ä¸åœ¨é¢„æµ‹å‡†ç¡®æ€§ä¸Šå¯ä»¥ä¸åŸå§‹çš„å¤§å›¾åƒè¡¥ä¸ç›¸åª²ç¾ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—å¼€é”€ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (4)', '#agi (2)', '#alignment', '#architecture (8)', '#audio (1)', '#benchmark (10)', '#cv (10)', '#data', '#dataset (9)', '#diffusion (5)', '#ethics (2)', '#games (2)', '#graphs', '#hallucinations (1)', '#healthcare (1)', '#inference (2)', '#interpretability (1)', '#leakage', '#long_context (3)', '#low_resource', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (13)', '#open_source (8)', '#optimization (10)', '#plp', '#rag (2)', '#reasoning (7)', '#rl (1)', '#rlhf', '#robotics (4)', '#science', '#security', '#small_models (1)', '#story_generation (1)', '#survey', '#synthetic (2)', '#training (10)', '#transfer_learning (1)', '#video (5)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-03-27 21:10',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-27 21:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-27 21:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    