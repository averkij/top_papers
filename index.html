
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 12 papers. September 23.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">23 сентября</span> | <span id="title-articles-count">12 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-09-22.html">⬅️ <span id="prev-date">22.09</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-09-24.html">➡️ <span id="next-date">24.09</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-09.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '23 сентября', 'en': 'September 23', 'zh': '9月23日'};
        let feedDateNext = {'ru': '24.09', 'en': '09/24', 'zh': '9月24日'};
        let feedDatePrev = {'ru': '22.09', 'en': '09/22', 'zh': '9月22日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2509.17627', 'title': 'OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion\n  Transformer Models', 'url': 'https://huggingface.co/papers/2509.17627', 'abstract': 'OmniInsert addresses challenges in mask-free video insertion using a novel data pipeline, feature injection, progressive training, and context-aware rephrasing, outperforming commercial solutions.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in video insertion based on diffusion models are impressive. However, existing methods rely on complex control signals but struggle with subject consistency, limiting their practical applicability. In this paper, we focus on the task of Mask-free Video Insertion and aim to resolve three key challenges: data scarcity, subject-scene equilibrium, and insertion harmonization. To address the data scarcity, we propose a new data pipeline InsertPipe, constructing diverse cross-pair data automatically. Building upon our data pipeline, we develop OmniInsert, a novel unified framework for mask-free video insertion from both single and multiple subject references. Specifically, to maintain subject-scene equilibrium, we introduce a simple yet effective Condition-Specific Feature Injection mechanism to distinctly inject multi-source conditions and propose a novel Progressive Training strategy that enables the model to balance feature injection from subjects and source video. Meanwhile, we design the Subject-Focused Loss to improve the detailed appearance of the subjects. To further enhance insertion harmonization, we propose an Insertive Preference Optimization methodology to optimize the model by simulating human preferences, and incorporate a Context-Aware Rephraser module during reference to seamlessly integrate the subject into the original scenes. To address the lack of a benchmark for the field, we introduce InsertBench, a comprehensive benchmark comprising diverse scenes with meticulously selected subjects. Evaluation on InsertBench indicates OmniInsert outperforms state-of-the-art closed-source commercial solutions. The code will be released.', 'score': 14, 'issue_id': 6030, 'pub_date': '2025-09-22', 'pub_date_card': {'ru': '22 сентября', 'en': 'September 22', 'zh': '9月22日'}, 'hash': '15e2a15d37cb464c', 'authors': ['Jinshu Chen', 'Xinghui Li', 'Xu Bai', 'Tianxiang Ma', 'Pengze Zhang', 'Zhuowei Chen', 'Gen Li', 'Lijie Liu', 'Songtao Zhao', 'Bingchuan Li', 'Qian He'], 'affiliations': ['Intelligent Creation Lab, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2509.17627.jpg', 'data': {'categories': ['#data', '#dataset', '#benchmark', '#optimization', '#video', '#open_source', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Умная вставка в видео без масок', 'desc': 'OmniInsert - это новая система для вставки объектов в видео без использования масок. Она решает проблемы нехватки данных, баланса между объектом и сценой, а также гармоничной интеграции. Система использует инновационный конвейер данных, прогрессивное обучение и контекстно-зависимое перефразирование. OmniInsert превосходит коммерческие решения на новом бенчмарке InsertBench.'}, 'en': {'title': 'Seamless Video Insertion with OmniInsert', 'desc': "OmniInsert is a novel framework designed for mask-free video insertion that tackles key challenges such as data scarcity and subject-scene equilibrium. It introduces a new data pipeline called InsertPipe to automatically create diverse training data, enhancing the model's learning capabilities. The framework employs Condition-Specific Feature Injection and Progressive Training to ensure that the inserted subjects harmonize well with the original video scenes. Additionally, it features a Context-Aware Rephraser and a Subject-Focused Loss to improve the visual quality and integration of subjects, outperforming existing commercial solutions in evaluations."}, 'zh': {'title': '无掩码视频插入的新突破', 'desc': 'OmniInsert是一种新颖的无掩码视频插入方法，旨在解决数据稀缺、主体场景平衡和插入和谐性等关键挑战。我们提出了InsertPipe数据管道，自动构建多样化的交叉配对数据，以应对数据稀缺问题。通过条件特定特征注入机制和渐进训练策略，OmniInsert能够有效地平衡来自不同来源的特征注入。最终，我们设计了插入偏好优化方法和上下文感知重述模块，以提高插入的和谐性，使主体更自然地融入原始场景。'}}}, {'id': 'https://huggingface.co/papers/2509.18091', 'title': 'OnePiece: Bringing Context Engineering and Reasoning to Industrial\n  Cascade Ranking System', 'url': 'https://huggingface.co/papers/2509.18091', 'abstract': 'OnePiece integrates LLM-style context engineering and reasoning into industrial search and recommendation systems, achieving significant improvements in key business metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the growing interest in replicating the scaled success of large language models (LLMs) in industrial search and recommender systems, most existing industrial efforts remain limited to transplanting Transformer architectures, which bring only incremental improvements over strong Deep Learning Recommendation Models (DLRMs). From a first principle perspective, the breakthroughs of LLMs stem not only from their architectures but also from two complementary mechanisms: context engineering, which enriches raw input queries with contextual cues to better elicit model capabilities, and multi-step reasoning, which iteratively refines model outputs through intermediate reasoning paths. However, these two mechanisms and their potential to unlock substantial improvements remain largely underexplored in industrial ranking systems.   In this paper, we propose OnePiece, a unified framework that seamlessly integrates LLM-style context engineering and reasoning into both retrieval and ranking models of industrial cascaded pipelines. OnePiece is built on a pure Transformer backbone and further introduces three key innovations: (1) structured context engineering, which augments interaction history with preference and scenario signals and unifies them into a structured tokenized input sequence for both retrieval and ranking; (2) block-wise latent reasoning, which equips the model with multi-step refinement of representations and scales reasoning bandwidth via block size; (3) progressive multi-task training, which leverages user feedback chains to effectively supervise reasoning steps during training. OnePiece has been deployed in the main personalized search scenario of Shopee and achieves consistent online gains across different key business metrics, including over +2% GMV/UU and a +2.90% increase in advertising revenue.', 'score': 12, 'issue_id': 6030, 'pub_date': '2025-09-22', 'pub_date_card': {'ru': '22 сентября', 'en': 'September 22', 'zh': '9月22日'}, 'hash': 'a4d1025bbd0ac828', 'authors': ['Sunhao Dai', 'Jiakai Tang', 'Jiahua Wu', 'Kun Wang', 'Yuxuan Zhu', 'Bingjun Chen', 'Bangyang Hong', 'Yu Zhao', 'Cong Fu', 'Kangle Wu', 'Yabo Ni', 'Anxiang Zeng', 'Wenjie Wang', 'Xu Chen', 'Jun Xu', 'See-Kiong Ng'], 'affiliations': ['National University of Singapore', 'Renmin University of China', 'Shopee', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2509.18091.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#optimization', '#reasoning', '#training'], 'emoji': '🧩', 'ru': {'title': 'Объединяя мощь LLM и промышленных рекомендательных систем', 'desc': 'OnePiece - это унифицированная система, интегрирующая методы контекстной инженерии и рассуждений, характерные для больших языковых моделей, в промышленные системы поиска и рекомендаций. Она использует структурированную контекстную инженерию, блочное латентное рассуждение и прогрессивное многозадачное обучение. Система построена на чистой архитектуре трансформера и была успешно внедрена в персонализированный поиск Shopee. OnePiece показала значительное улучшение ключевых бизнес-метрик, включая рост GMV/UU и доходов от рекламы.'}, 'en': {'title': 'Unlocking Search Potential with LLM-inspired Innovations', 'desc': 'OnePiece is a novel framework that enhances industrial search and recommendation systems by incorporating techniques from large language models (LLMs). It focuses on two main mechanisms: context engineering, which enriches input queries with relevant contextual information, and multi-step reasoning, which refines outputs through iterative processes. The framework introduces structured context engineering, block-wise latent reasoning, and progressive multi-task training to improve model performance. As a result, OnePiece has shown significant improvements in key business metrics, such as increased gross merchandise value and advertising revenue.'}, 'zh': {'title': 'OnePiece：提升搜索与推荐的智能框架', 'desc': 'OnePiece 是一个将大语言模型（LLM）风格的上下文工程和推理机制整合到工业搜索和推荐系统中的框架。它通过结构化的上下文工程增强用户的交互历史，并将其转化为统一的输入序列，从而提高检索和排序的效果。此外，OnePiece 采用块级潜在推理，允许模型通过多步推理逐步优化输出。该框架在 Shopee 的个性化搜索场景中应用，显著提升了多个关键业务指标。'}}}, {'id': 'https://huggingface.co/papers/2509.18084', 'title': 'ByteWrist: A Parallel Robotic Wrist Enabling Flexible and\n  Anthropomorphic Motion for Confined Spaces', 'url': 'https://huggingface.co/papers/2509.18084', 'abstract': 'This paper introduces ByteWrist, a novel highly-flexible and anthropomorphic parallel wrist for robotic manipulation. ByteWrist addresses the critical limitations of existing serial and parallel wrists in narrow-space operations through a compact three-stage parallel drive mechanism integrated with arc-shaped end linkages. The design achieves precise RPY (Roll-Pitch-Yaw) motion while maintaining exceptional compactness, making it particularly suitable for complex unstructured environments such as home services, medical assistance, and precision assembly. The key innovations include: (1) a nested three-stage motor-driven linkages that minimize volume while enabling independent multi-DOF control, (2) arc-shaped end linkages that optimize force transmission and expand motion range, and (3) a central supporting ball functioning as a spherical joint that enhances structural stiffness without compromising flexibility. Meanwhile, we present comprehensive kinematic modeling including forward / inverse kinematics and a numerical Jacobian solution for precise control. Empirically, we observe ByteWrist demonstrates strong performance in narrow-space maneuverability and dual-arm cooperative manipulation tasks, outperforming Kinova-based systems. Results indicate significant improvements in compactness, efficiency, and stiffness compared to traditional designs, establishing ByteWrist as a promising solution for next-generation robotic manipulation in constrained environments.', 'score': 9, 'issue_id': 6030, 'pub_date': '2025-09-22', 'pub_date_card': {'ru': '22 сентября', 'en': 'September 22', 'zh': '9月22日'}, 'hash': '5aa65074f4ca8fcb', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#robotics'], 'emoji': '🦾', 'ru': {'title': 'ByteWrist: Революция в роботизированных запястьях для узких пространств', 'desc': 'Статья представляет ByteWrist - новое высокогибкое и антропоморфное параллельное запястье для роботизированных манипуляций. ByteWrist решает критические ограничения существующих последовательных и параллельных запястий в операциях в узких пространствах с помощью компактного трехступенчатого параллельного приводного механизма, интегрированного с дугообразными концевыми звеньями. Ключевые инновации включают вложенные трехступенчатые моторизованные звенья, дугообразные концевые звенья и центральный опорный шар, функционирующий как сферический шарнир. Эмпирические результаты показывают, что ByteWrist демонстрирует высокую производительность в задачах маневрирования в узких пространствах и кооперативной манипуляции двумя руками, превосходя системы на базе Kinova.'}, 'en': {'title': 'ByteWrist: Revolutionizing Robotic Manipulation in Tight Spaces', 'desc': "This paper presents ByteWrist, an innovative robotic wrist designed for flexible and efficient manipulation in tight spaces. It features a compact three-stage parallel drive mechanism that allows for precise Roll-Pitch-Yaw (RPY) motion, making it ideal for complex tasks in unstructured environments. Key advancements include multi-degree-of-freedom control through nested linkages and arc-shaped end linkages that enhance force transmission. The paper also details kinematic modeling techniques for accurate control and demonstrates ByteWrist's superior performance in narrow-space tasks compared to existing systems."}, 'zh': {'title': 'ByteWrist：狭小空间中的灵活机器人腕关节', 'desc': '本文介绍了一种新型的高灵活性和类人并行腕关节，名为ByteWrist，旨在解决现有串行和并行腕关节在狭小空间操作中的关键限制。ByteWrist采用紧凑的三阶段并行驱动机制，结合弧形末端连杆，实现了精确的滚转-俯仰-偏航（RPY）运动，同时保持了卓越的紧凑性，特别适合复杂的非结构化环境，如家庭服务、医疗辅助和精密组装。其主要创新包括：嵌套的三阶段电机驱动连杆，最小化体积并实现独立的多自由度控制；优化力传输和扩展运动范围的弧形末端连杆；以及作为球形关节的中央支撑球，增强结构刚度而不影响灵活性。此外，本文还提供了全面的运动学建模，包括正/逆运动学和数值雅可比解，以实现精确控制。'}}}, {'id': 'https://huggingface.co/papers/2509.16596', 'title': 'Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from\n  Token and Parameter Levels', 'url': 'https://huggingface.co/papers/2509.16596', 'abstract': "Supervised fine-tuning of large language models can negatively impact closed-book question answering performance, with up to 90% of parameter updates not contributing to knowledge enhancement.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) acquire substantial world knowledge during pre-training, which is further shaped by post-training techniques such as supervised fine-tuning (SFT). However, the impact of SFT on a model's knowledge remains underexplored, limiting our ability to control knowledge change behavior in fine-tuned models. To address this gap, we evaluate closed-book question answering (CBQA) performance across five LLMs from the LLaMA-2 and LLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up to 14% worse than those fine-tuned on only 240 samples. Furthermore, varying the level of knowledge mastery in the fine-tuning data leads to performance fluctuations of over 12%. To investigate these effects, we analyze model behavior at both the token and parameter levels. Our analysis reveals that up to 90% of parameter updates during SFT do not contribute to knowledge enhancement. Restoring these updates can improve performance on the CBQA task, depending on the characteristics of the fine-tuning data. These insights offer practical guidance for developing fine-tuning strategies that more effectively strengthen model knowledge.", 'score': 4, 'issue_id': 6030, 'pub_date': '2025-09-20', 'pub_date_card': {'ru': '20 сентября', 'en': 'September 20', 'zh': '9月20日'}, 'hash': 'ad73526f5b38ef1d', 'authors': ['Junjie Ye', 'Yuming Yang', 'Yang Nan', 'Shuo Li', 'Qi Zhang', 'Tao Gui', 'Xuanjing Huang', 'Peng Wang', 'Zhongchao Shi', 'Jianping Fan'], 'affiliations': ['Fudan University', 'Lenovo Research, Beijing, China', 'Shanghai Innovation Institute', 'Shanghai Key Lab of Intelligent Information Processing'], 'pdf_title_img': 'assets/pdf/title_img/2509.16596.jpg', 'data': {'categories': ['#interpretability', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Осторожно с дообучением: больше не всегда лучше для языковых моделей', 'desc': 'Исследование показывает, что контролируемая дообучение больших языковых моделей может негативно влиять на их способность отвечать на вопросы без доступа к внешней информации. Анализ моделей семейств LLaMA-2 и LLaMA-3 выявил, что увеличение объема данных для дообучения может ухудшить производительность на 14%. Обнаружено, что до 90% обновлений параметров во время дообучения не способствуют улучшению знаний модели. Результаты исследования предлагают практические рекомендации по разработке стратегий дообучения для более эффективного усиления знаний модели.'}, 'en': {'title': 'Optimize Fine-Tuning to Preserve Knowledge in Language Models', 'desc': "This paper investigates how supervised fine-tuning (SFT) affects the knowledge retention of large language models (LLMs) during closed-book question answering (CBQA). The authors find that a significant portion of parameter updates during SFT, up to 90%, do not enhance the model's knowledge, leading to performance drops. They demonstrate that fine-tuning on fewer samples can sometimes yield better results than on larger datasets, indicating that the quality of fine-tuning data is crucial. Their findings provide valuable insights for optimizing fine-tuning strategies to improve knowledge retention in LLMs."}, 'zh': {'title': '优化微调策略，提升模型知识', 'desc': '这篇论文探讨了大型语言模型在监督微调（SFT）过程中对闭卷问答（CBQA）性能的影响。研究发现，微调过程中高达90%的参数更新并未提升模型的知识水平，甚至在某些情况下，微调样本数量的增加反而导致性能下降。通过分析模型在标记和参数层面的行为，作者揭示了微调数据的知识掌握程度对模型性能的显著影响。该研究为优化微调策略以增强模型知识提供了实用指导。'}}}, {'id': 'https://huggingface.co/papers/2509.16117', 'title': 'DiffusionNFT: Online Diffusion Reinforcement with Forward Process', 'url': 'https://huggingface.co/papers/2509.16117', 'abstract': 'Diffusion Negative-aware FineTuning (DiffusionNFT) optimizes diffusion models directly on the forward process via flow matching, improving efficiency and performance compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Online reinforcement learning (RL) has been central to post-training language models, but its extension to diffusion models remains challenging due to intractable likelihoods. Recent works discretize the reverse sampling process to enable GRPO-style training, yet they inherit fundamental drawbacks, including solver restrictions, forward-reverse inconsistency, and complicated integration with classifier-free guidance (CFG). We introduce Diffusion Negative-aware FineTuning (DiffusionNFT), a new online RL paradigm that optimizes diffusion models directly on the forward process via flow matching. DiffusionNFT contrasts positive and negative generations to define an implicit policy improvement direction, naturally incorporating reinforcement signals into the supervised learning objective. This formulation enables training with arbitrary black-box solvers, eliminates the need for likelihood estimation, and requires only clean images rather than sampling trajectories for policy optimization. DiffusionNFT is up to 25times more efficient than FlowGRPO in head-to-head comparisons, while being CFG-free. For instance, DiffusionNFT improves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO achieves 0.95 with over 5k steps and additional CFG employment. By leveraging multiple reward models, DiffusionNFT significantly boosts the performance of SD3.5-Medium in every benchmark tested.', 'score': 4, 'issue_id': 6030, 'pub_date': '2025-09-19', 'pub_date_card': {'ru': '19 сентября', 'en': 'September 19', 'zh': '9月19日'}, 'hash': '1d4f4b6ec61af4cd', 'authors': ['Kaiwen Zheng', 'Huayu Chen', 'Haotian Ye', 'Haoxiang Wang', 'Qinsheng Zhang', 'Kai Jiang', 'Hang Su', 'Stefano Ermon', 'Jun Zhu', 'Ming-Yu Liu'], 'affiliations': ['NVIDIA', 'Stanford University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.16117.jpg', 'data': {'categories': ['#rl', '#benchmark', '#optimization', '#diffusion', '#training'], 'emoji': '🔄', 'ru': {'title': 'DiffusionNFT: Эффективная оптимизация диффузионных моделей через прямой процесс', 'desc': 'Статья представляет новый метод оптимизации диффузионных моделей под названием DiffusionNFT. Этот подход основан на прямом процессе и использует сопоставление потоков, что позволяет избежать проблем, связанных с оценкой вероятности. DiffusionNFT сравнивает положительные и отрицательные генерации для определения направления улучшения политики, естественно интегрируя сигналы подкрепления в цель обучения с учителем. Метод демонстрирует значительное повышение эффективности и производительности по сравнению с существующими методами, такими как FlowGRPO.'}, 'en': {'title': 'Revolutionizing Diffusion Models with Efficient FineTuning', 'desc': 'Diffusion Negative-aware FineTuning (DiffusionNFT) is a novel approach that enhances diffusion models by optimizing them directly during the forward process using flow matching. This method addresses challenges in online reinforcement learning for diffusion models, such as intractable likelihoods and inconsistencies between forward and reverse processes. By contrasting positive and negative outputs, DiffusionNFT effectively integrates reinforcement signals into the training process without needing likelihood estimation or complex sampling. The results show that DiffusionNFT is significantly more efficient than previous methods, achieving higher performance scores in fewer training steps.'}, 'zh': {'title': '扩散模型的新优化：负向微调的力量', 'desc': '扩散负向微调（DiffusionNFT）通过流匹配直接优化扩散模型的前向过程，从而提高了效率和性能。与现有方法相比，DiffusionNFT克服了许多挑战，如求解器限制和前向-反向不一致性。该方法通过对比正向和负向生成，定义了隐式策略改进方向，自然地将强化信号融入监督学习目标中。DiffusionNFT在效率上比FlowGRPO高出25倍，并且不需要分类器引导，显著提升了模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2509.16941', 'title': 'SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering\n  Tasks?', 'url': 'https://huggingface.co/papers/2509.16941', 'abstract': 'SWE-Bench Pro is a challenging benchmark for coding models, featuring complex, enterprise-level problems that require substantial code modifications, with performance evaluations showing significant limitations in current models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SWE-Bench Pro, a substantially more challenging benchmark that builds upon the best practices of SWE-BENCH [25], but is explicitly designed to capture realistic, complex, enterprise-level problems beyond the scope of SWE-BENCH. SWE-BENCH PRO contains 1,865 problems sourced from a diverse set of 41 actively maintained repositories spanning business applications, B2B services, and developer tools. The benchmark is partitioned into a public set with open access to problems sourced from 11 repositories, a held-out set of 12 repositories and a commercial set of 18 proprietary repositories where we have formal partnership agreements with early-stage startups. Problems in the held-out and the commercial set are not publicly accessible, but we release results on the commercial set. Our benchmark features long-horizon tasks that may require hours to days for a professional software engineer to complete, often involving patches across multiple files and substantial code modifications. All tasks are human-verified and augmented with sufficient context to ensure resolvability. In our evaluation of widely used coding models, under a unified scaffold, we observe that their performance on SWE-Bench PRO remains below 25% (Pass@1), with GPT-5 achieving the highest score to date at 23.3%. To better understand these limitations, we cluster the failure modes observed in the collected agent trajectories for a clearer characterization of the error patterns exhibited by current models. Overall, SWE-BENCH PRO provides a contamination-resistant testbed that more faithfully captures the complexity and diversity of real-world software development, advancing the pursuit of truly autonomous software engineering agents at a professional level.', 'score': 2, 'issue_id': 6030, 'pub_date': '2025-09-21', 'pub_date_card': {'ru': '21 сентября', 'en': 'September 21', 'zh': '9月21日'}, 'hash': 'e407b0b4d298f1ec', 'authors': ['Xiang Deng', 'Jeff Da', 'Edwin Pan', 'Yannis Yiming He', 'Charles Ide', 'Kanak Garg', 'Niklas Lauffer', 'Andrew Park', 'Nitin Pasari', 'Chetan Rane', 'Karmini Sampath', 'Maya Krishnan', 'Srivatsa Kundurthy', 'Sean Hendryx', 'Zifan Wang', 'Chen Bo Calvin Zhang', 'Noah Jacobson', 'Bing Liu', 'Brad Kenstler'], 'affiliations': ['Scale AI'], 'pdf_title_img': 'assets/pdf/title_img/2509.16941.jpg', 'data': {'categories': ['#optimization', '#interpretability', '#agents', '#benchmark'], 'emoji': '🧑\u200d💻', 'ru': {'title': 'SWE-Bench Pro: Вызов для AI в реальной разработке ПО', 'desc': 'SWE-Bench Pro - это сложный бенчмарк для моделей кодирования, содержащий комплексные задачи корпоративного уровня. Он включает 1865 проблем из 41 активно поддерживаемого репозитория, охватывающих бизнес-приложения, B2B-сервисы и инструменты разработчиков. Задачи требуют значительных модификаций кода и могут занимать у профессиональных разработчиков часы или дни. Оценка производительности показывает, что современные модели кодирования достигают менее 25% успешности (Pass@1) на этом бенчмарке.'}, 'en': {'title': 'SWE-Bench Pro: Elevating the Challenge for Coding Models', 'desc': 'SWE-Bench Pro is a new benchmark designed to evaluate coding models on complex, real-world software engineering tasks. It includes 1,865 problems from various business applications and developer tools, emphasizing long-horizon tasks that require significant code changes. The benchmark reveals that current coding models, including GPT-5, struggle to achieve high performance, with a maximum score of only 23.3%. By analyzing the failure modes of these models, SWE-Bench Pro aims to enhance our understanding of their limitations and improve the development of autonomous software engineering agents.'}, 'zh': {'title': 'SWE-Bench Pro：挑战编码模型的极限', 'desc': 'SWE-Bench Pro 是一个具有挑战性的基准测试，专为编码模型设计，涵盖复杂的企业级问题。这些问题需要进行大量的代码修改，且当前模型的表现显示出显著的局限性。基准测试包含来自41个活跃维护的代码库的1865个问题，分为公共集、保留集和商业集。通过对现有编码模型的评估，我们发现它们在SWE-Bench Pro上的表现低于25%，这表明在真实软件开发中，当前模型仍面临许多挑战。'}}}, {'id': 'https://huggingface.co/papers/2509.17818', 'title': 'ContextFlow: Training-Free Video Object Editing via Adaptive Context\n  Enrichment', 'url': 'https://huggingface.co/papers/2509.17818', 'abstract': 'ContextFlow, a training-free framework for Diffusion Transformers, enhances video object editing by using a high-order Rectified Flow solver and Adaptive Context Enrichment to achieve precise, temporally consistent, and high-fidelity object manipulation.  \t\t\t\t\tAI-generated summary \t\t\t\t Training-free video object editing aims to achieve precise object-level manipulation, including object insertion, swapping, and deletion. However, it faces significant challenges in maintaining fidelity and temporal consistency. Existing methods, often designed for U-Net architectures, suffer from two primary limitations: inaccurate inversion due to first-order solvers, and contextual conflicts caused by crude "hard" feature replacement. These issues are more challenging in Diffusion Transformers (DiTs), where the unsuitability of prior layer-selection heuristics makes effective guidance challenging. To address these limitations, we introduce ContextFlow, a novel training-free framework for DiT-based video object editing. In detail, we first employ a high-order Rectified Flow solver to establish a robust editing foundation. The core of our framework is Adaptive Context Enrichment (for specifying what to edit), a mechanism that addresses contextual conflicts. Instead of replacing features, it enriches the self-attention context by concatenating Key-Value pairs from parallel reconstruction and editing paths, empowering the model to dynamically fuse information. Additionally, to determine where to apply this enrichment (for specifying where to edit), we propose a systematic, data-driven analysis to identify task-specific vital layers. Based on a novel Guidance Responsiveness Metric, our method pinpoints the most influential DiT blocks for different tasks (e.g., insertion, swapping), enabling targeted and highly effective guidance. Extensive experiments show that ContextFlow significantly outperforms existing training-free methods and even surpasses several state-of-the-art training-based approaches, delivering temporally coherent, high-fidelity results.', 'score': 1, 'issue_id': 6030, 'pub_date': '2025-09-22', 'pub_date_card': {'ru': '22 сентября', 'en': 'September 22', 'zh': '9月22日'}, 'hash': 'e6039499f6d1d2dc', 'authors': ['Yiyang Chen', 'Xuanhua He', 'Xiujun Ma', 'Yue Ma'], 'affiliations': ['State Key Laboratory of General Artificial Intelligence, Peking University, Beijing, China', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2509.17818.jpg', 'data': {'categories': ['#architecture', '#video', '#diffusion', '#optimization'], 'emoji': '🎬', 'ru': {'title': 'ContextFlow: Прорыв в редактировании видео без обучения', 'desc': 'ContextFlow - это новая система для редактирования объектов в видео без дополнительного обучения, основанная на диффузионных трансформерах. Она использует высокоточный решатель Rectified Flow и механизм адаптивного обогащения контекста для точного и согласованного манипулирования объектами. ContextFlow решает проблемы предыдущих методов, такие как неточная инверсия и конфликты контекста. Система превосходит существующие подходы без обучения и даже некоторые современные методы с обучением, обеспечивая высококачественные результаты.'}, 'en': {'title': 'Revolutionizing Video Editing with ContextFlow!', 'desc': 'ContextFlow is a novel framework designed for video object editing using Diffusion Transformers without the need for training. It addresses key challenges in maintaining high fidelity and temporal consistency during object manipulation tasks like insertion and swapping. By utilizing a high-order Rectified Flow solver and Adaptive Context Enrichment, it enhances the editing process by dynamically fusing information from different paths instead of simply replacing features. The framework also employs a data-driven approach to identify the most effective layers for specific editing tasks, leading to superior performance compared to existing methods.'}, 'zh': {'title': '无训练视频对象编辑的新突破', 'desc': 'ContextFlow 是一个无训练的框架，专为扩散变换器（Diffusion Transformers）设计，旨在提升视频对象编辑的精确性和一致性。它通过高阶修正流求解器和自适应上下文丰富机制，解决了对象插入、交换和删除中的时间一致性和保真度问题。与传统方法相比，ContextFlow 通过动态融合信息，避免了特征替换带来的上下文冲突。实验结果表明，ContextFlow 在无训练方法中表现优异，甚至超越了一些基于训练的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2509.17765', 'title': 'Qwen3-Omni Technical Report', 'url': 'https://huggingface.co/papers/2509.17765', 'abstract': 'Qwen3-Omni, a multimodal model, achieves state-of-the-art performance across text, image, audio, and video, using a Thinker-Talker MoE architecture and a lightweight causal ConvNet for efficient streaming synthesis.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Qwen3-Omni, a single multimodal model that, for the first time, maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages, and speech generation in 10 languages. To reduce first-packet latency in streaming synthesis, Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms. To further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.', 'score': 1, 'issue_id': 6030, 'pub_date': '2025-09-22', 'pub_date_card': {'ru': '22 сентября', 'en': 'September 22', 'zh': '9月22日'}, 'hash': '464827796e5c676c', 'authors': ['Jin Xu', 'Zhifang Guo', 'Hangrui Hu', 'Yunfei Chu', 'Xiong Wang', 'Jinzheng He', 'Yuxuan Wang', 'Xian Shi', 'Ting He', 'Xinfa Zhu', 'Yuanjun Lv', 'Yongqi Wang', 'Dake Guo', 'He Wang', 'Linhan Ma', 'Pei Zhang', 'Xinyu Zhang', 'Hongkun Hao', 'Zishan Guo', 'Baosong Yang', 'Bin Zhang', 'Ziyang Ma', 'Xipin Wei', 'Shuai Bai', 'Keqin Chen', 'Xuejing Liu', 'Peng Wang', 'Mingkun Yang', 'Dayiheng Liu', 'Xingzhang Ren', 'Bo Zheng', 'Rui Men', 'Fan Zhou', 'Bowen Yu', 'Jianxin Yang', 'Le Yu', 'Jingren Zhou', 'Junyang Lin'], 'affiliations': ['Qwen Team'], 'pdf_title_img': 'assets/pdf/title_img/2509.17765.jpg', 'data': {'categories': ['#hallucinations', '#audio', '#multimodal', '#architecture', '#open_source', '#long_context', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Qwen3-Omni: Единая мультимодальная модель для ИИ нового поколения', 'desc': 'Qwen3-Omni - это мультимодальная модель, достигающая передовых результатов в обработке текста, изображений, аудио и видео. Она использует архитектуру Thinker-Talker MoE для унификации восприятия и генерации контента разных модальностей. Модель поддерживает текстовое взаимодействие на 119 языках, распознавание речи на 19 языках и генерацию речи на 10 языках. Для снижения задержки при потоковом синтезе речи используется легковесная каузальная сверточная нейронная сеть.'}, 'en': {'title': 'Unifying Multimodal Mastery with Qwen3-Omni', 'desc': 'Qwen3-Omni is a cutting-edge multimodal model that excels in processing text, images, audio, and video simultaneously without losing performance compared to single-modal models. It utilizes a Thinker-Talker MoE architecture to integrate perception and generation, achieving state-of-the-art results in various audio and audio-visual tasks. The model is designed for efficient streaming synthesis, significantly reducing latency by employing a lightweight causal ConvNet and a multi-codebook scheme for speech codecs. Additionally, it introduces a Thinking model for enhanced multimodal reasoning and provides a specialized audio captioning capability, making it a versatile tool for diverse applications.'}, 'zh': {'title': '多模态模型的全能之选', 'desc': 'Qwen3-Omni是一种多模态模型，首次在文本、图像、音频和视频上实现了最先进的性能，而没有相对于单模态模型的性能下降。该模型采用Thinker-Talker MoE架构，统一了文本、图像、音频和视频的感知与生成，特别在音频任务上表现优异。Qwen3-Omni在36个音频和音频-视觉基准测试中，取得了32个基准的开源最优性能，并在22个基准上达到了整体最优，超越了许多强大的闭源模型。为了提高流媒体合成的效率，Qwen3-Omni使用轻量级因果卷积网络，显著降低了首次数据包的延迟。'}}}, {'id': 'https://huggingface.co/papers/2509.17396', 'title': 'EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering', 'url': 'https://huggingface.co/papers/2509.17396', 'abstract': "EpiCache is a KV cache management framework for long conversational question answering that reduces memory usage and improves accuracy through block-wise prefill, episodic KV compression, and adaptive layer-wise budget allocation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have extended context lengths, enabling assistants to sustain long histories for coherent, personalized responses. This ability, however, hinges on Key-Value (KV) caching, whose memory grows linearly with dialogue length and quickly dominates under strict resource constraints. An active line of research for reducing this overhead is KV cache compression, which seeks to limit cache size while preserving accuracy. Yet existing methods face two major limitations: (i) evicting entries after full-context prefill causes unbounded peak memory, and (ii) query-dependent eviction narrows the cache to a single query, leading to degraded accuracy in multi-turn conversations. We introduce EpiCache, a training-free KV cache management framework for long conversational question answering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth through block-wise prefill and preserves topic-relevant context via episodic KV compression, which clusters conversation history into coherent episodes and applies episode-specific KV cache eviction. We further design an adaptive layer-wise budget allocation strategy that measures each layer's sensitivity to eviction and distributes the memory budget across layers accordingly. Across three LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent baselines, sustains near-full KV accuracy under 4-6x compression, and reduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient multi-turn interaction under strict resource constraints.", 'score': 1, 'issue_id': 6030, 'pub_date': '2025-09-22', 'pub_date_card': {'ru': '22 сентября', 'en': 'September 22', 'zh': '9月22日'}, 'hash': '10727268d6361b72', 'authors': ['Minsoo Kim', 'Arnav Kundu', 'Han-Byul Kim', 'Richa Dixit', 'Minsik Cho'], 'affiliations': ['Apple', 'Hanyang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.17396.jpg', 'data': {'categories': ['#data', '#inference', '#optimization', '#long_context', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эффективное кэширование для длительных диалогов с ИИ', 'desc': 'EpiCache - это фреймворк управления KV-кэшем для длительных диалоговых систем вопросов и ответов. Он использует блочное предзаполнение, эпизодическое KV-сжатие и адаптивное распределение бюджета по слоям для снижения использования памяти и повышения точности. EpiCache позволяет ограничить рост кэша и сохранить релевантный контекст темы. В сравнении с существующими методами, EpiCache улучшает точность до 40% и обеспечивает эффективное многоходовое взаимодействие при строгих ресурсных ограничениях.'}, 'en': {'title': 'EpiCache: Efficient Memory Management for Long Conversations', 'desc': "EpiCache is a framework designed to manage Key-Value (KV) caches for long conversational question answering, aiming to reduce memory usage while enhancing accuracy. It employs block-wise prefill and episodic KV compression to maintain relevant context without excessive memory growth. The framework also features an adaptive layer-wise budget allocation that optimizes memory distribution based on each layer's sensitivity to eviction. Overall, EpiCache significantly improves performance in multi-turn conversations, achieving higher accuracy and lower latency under strict resource limitations."}, 'zh': {'title': 'EpiCache：高效的长对话问答缓存管理', 'desc': 'EpiCache是一个用于长对话问答的键值缓存管理框架，旨在减少内存使用并提高准确性。它通过块级预填充、情节键值压缩和自适应层级预算分配来实现这些目标。EpiCache能够在固定内存预算下控制缓存增长，并通过将对话历史聚类为一致的情节来保留与主题相关的上下文。实验结果表明，EpiCache在多个基准测试中提高了准确性，并显著降低了延迟和内存使用。'}}}, {'id': 'https://huggingface.co/papers/2509.17336', 'title': 'Mano Report', 'url': 'https://huggingface.co/papers/2509.17336', 'abstract': 'A robust GUI agent, Mano, integrates reinforcement learning with vision-language models for high-fidelity data generation and improved performance on GUI benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphical user interfaces (GUIs) are the primary medium for human-computer interaction, yet automating GUI interactions remains challenging due to the complexity of visual elements, dynamic environments, and the need for multi-step reasoning. Existing methods based on vision-language models (VLMs) often suffer from limited resolution, domain mismatch, and insufficient sequential decisionmaking capability. To address these issues, we propose Mano, a robust GUI agent built upon a multi-modal foundation model pre-trained on extensive web and computer system data. Our approach integrates a novel simulated environment for high-fidelity data generation, a three-stage training pipeline (supervised fine-tuning, offline reinforcement learning, and online reinforcement learning), and a verification module for error recovery. Mano demonstrates state-of-the-art performance on multiple GUI benchmarks, including Mind2Web and OSWorld, achieving significant improvements in success rate and operational accuracy. Our work provides new insights into the effective integration of reinforcement learning with VLMs for practical GUI agent deployment, highlighting the importance of domain-specific data, iterative training, and holistic reward design.', 'score': 1, 'issue_id': 6030, 'pub_date': '2025-09-22', 'pub_date_card': {'ru': '22 сентября', 'en': 'September 22', 'zh': '9月22日'}, 'hash': '379006ad6024a25b', 'authors': ['Tianyu Fu', 'Anyang Su', 'Chenxu Zhao', 'Hanning Wang', 'Minghui Wu', 'Zhe Yu', 'Fei Hu', 'Mingjia Shi', 'Wei Dong', 'Jiayao Wang', 'Yuyang Chen', 'Ruiyang Yu', 'Siran Peng', 'Menglin Li', 'Nan Huang', 'Haitian Wei', 'Jiawei Yu', 'Yi Xin', 'Xilin Zhao', 'Kai Gu', 'Ping Jiang', 'Sifan Zhou', 'Shuo Wang'], 'affiliations': ['Mininglamp Technology'], 'pdf_title_img': 'assets/pdf/title_img/2509.17336.jpg', 'data': {'categories': ['#cv', '#agents', '#rl', '#games', '#multimodal', '#rlhf', '#benchmark', '#optimization', '#training'], 'emoji': '🖥️', 'ru': {'title': 'Mano: ИИ-агент нового поколения для автоматизации графических интерфейсов', 'desc': 'Статья представляет Mano - надежного GUI-агента, интегрирующего обучение с подкреплением и визуально-языковые модели. Mano использует симулированную среду для генерации высококачественных данных и трехэтапный процесс обучения. Агент демонстрирует улучшенные результаты на нескольких эталонных тестах для GUI, включая Mind2Web и OSWorld. Исследование показывает эффективность интеграции обучения с подкреплением и визуально-языковых моделей для практического применения GUI-агентов.'}, 'en': {'title': 'Mano: Revolutionizing GUI Automation with Reinforcement Learning and Vision-Language Models', 'desc': 'This paper presents Mano, a GUI agent that combines reinforcement learning with vision-language models to enhance data generation and performance on GUI tasks. The authors identify challenges in automating GUI interactions, such as visual complexity and the need for multi-step reasoning, which existing methods struggle to address. Mano utilizes a multi-modal foundation model and a three-stage training process that includes supervised fine-tuning and both offline and online reinforcement learning. The results show that Mano achieves state-of-the-art performance on various benchmarks, emphasizing the importance of tailored data and comprehensive training strategies in developing effective GUI agents.'}, 'zh': {'title': 'Mano：强化学习与视觉语言模型的完美结合', 'desc': '本文介绍了一种名为Mano的强大图形用户界面（GUI）代理，它将强化学习与视觉语言模型结合，以生成高保真数据并提高GUI基准测试的性能。现有的视觉语言模型在处理复杂的视觉元素和动态环境时常常面临分辨率有限和决策能力不足的问题。为了解决这些问题，Mano采用了多模态基础模型，并通过一个新颖的模拟环境进行高保真数据生成，结合三阶段的训练流程。Mano在多个GUI基准测试中表现出色，显著提高了成功率和操作准确性，展示了强化学习与视觉语言模型有效结合的潜力。'}}}, {'id': 'https://huggingface.co/papers/2509.15248', 'title': 'Synthetic bootstrapped pretraining', 'url': 'https://huggingface.co/papers/2509.15248', 'abstract': 'Synthetic Bootstrapped Pretraining (SBP) enhances language model performance by learning inter-document correlations and synthesizing new training data, leading to significant improvements over standard pretraining methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Synthetic Bootstrapped Pretraining (SBP), a language model (LM) pretraining procedure that first learns a model of relations between documents from the pretraining dataset and then leverages it to synthesize a vast new corpus for joint training. While the standard pretraining teaches LMs to learn causal correlations among tokens within a single document, it is not designed to efficiently model the rich, learnable inter-document correlations that can potentially lead to better performance. We validate SBP by designing a compute-matched pretraining setup and pretrain a 3B-parameter model on up to 1T tokens from scratch. We find SBP consistently improves upon a strong repetition baseline and delivers a significant fraction of performance improvement attainable by an oracle upper bound with access to 20x more unique data. Qualitative analysis reveals that the synthesized documents go beyond mere paraphrases -- SBP first abstracts a core concept from the seed material and then crafts a new narration on top of it. Besides strong empirical performance, SBP admits a natural Bayesian interpretation: the synthesizer implicitly learns to abstract the latent concepts shared between related documents.', 'score': 1, 'issue_id': 6030, 'pub_date': '2025-09-17', 'pub_date_card': {'ru': '17 сентября', 'en': 'September 17', 'zh': '9月17日'}, 'hash': '99a93d8361bcbcf6', 'authors': ['Zitong Yang', 'Aonan Zhang', 'Hong Liu', 'Tatsunori Hashimoto', 'Emmanuel Candès', 'Chong Wang', 'Ruoming Pang'], 'affiliations': ['Apple', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2509.15248.jpg', 'data': {'categories': ['#data', '#dataset', '#synthetic', '#architecture', '#optimization', '#training'], 'emoji': '🔄', 'ru': {'title': 'Синтетическое предобучение: новый подход к улучшению языковых моделей', 'desc': 'Статья представляет новый метод предобучения языковых моделей под названием Synthetic Bootstrapped Pretraining (SBP). SBP сначала обучается моделировать отношения между документами из набора данных для предобучения, а затем использует эту модель для синтеза нового обширного корпуса. Этот подход позволяет языковым моделям эффективнее учитывать междокументные корреляции, что потенциально ведет к улучшению производительности. Эксперименты показали, что SBP превосходит стандартные методы предобучения и обеспечивает значительную долю улучшения производительности, достижимого при использовании в 20 раз большего объема уникальных данных.'}, 'en': {'title': 'Unlocking Language Models with Inter-Document Insights', 'desc': "Synthetic Bootstrapped Pretraining (SBP) is a novel approach that enhances language model performance by focusing on the relationships between different documents rather than just within a single document. It first learns inter-document correlations from the pretraining dataset and then uses this knowledge to create a large amount of new training data. This method allows the model to capture richer contextual information, leading to significant performance improvements compared to traditional pretraining techniques. Additionally, SBP's ability to synthesize documents that abstract core concepts demonstrates its effectiveness in generating diverse and informative training examples."}, 'zh': {'title': '合成自举预训练：提升语言模型的新方法', 'desc': '合成自举预训练（SBP）通过学习文档之间的关系并合成新的训练数据，提升了语言模型的性能。与传统的预训练方法不同，SBP能够有效建模文档间的丰富相关性，从而实现更好的表现。我们通过设计计算匹配的预训练设置，验证了SBP的有效性，并在从零开始的情况下对一个3B参数的模型进行了预训练。实验结果表明，SBP在性能上显著超越了强基线，并接近于理想情况下的性能上限。'}}}, {'id': 'https://huggingface.co/papers/2509.09873', 'title': 'From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI\n  Ecosystem', 'url': 'https://huggingface.co/papers/2509.09873', 'abstract': 'The study audits licenses in the Hugging Face ecosystem, revealing systemic non-compliance and proposing a rule engine to detect and resolve license conflicts in open-source AI.  \t\t\t\t\tAI-generated summary \t\t\t\t Hidden license conflicts in the open-source AI ecosystem pose serious legal and ethical risks, exposing organizations to potential litigation and users to undisclosed risk. However, the field lacks a data-driven understanding of how frequently these conflicts occur, where they originate, and which communities are most affected. We present the first end-to-end audit of licenses for datasets and models on Hugging Face, as well as their downstream integration into open-source software applications, covering 364 thousand datasets, 1.6 million models, and 140 thousand GitHub projects. Our empirical analysis reveals systemic non-compliance in which 35.5% of model-to-application transitions eliminate restrictive license clauses by relicensing under permissive terms. In addition, we prototype an extensible rule engine that encodes almost 200 SPDX and model-specific clauses for detecting license conflicts, which can solve 86.4% of license conflicts in software applications. To support future research, we release our dataset and the prototype engine. Our study highlights license compliance as a critical governance challenge in open-source AI and provides both the data and tools necessary to enable automated, AI-aware compliance at scale.', 'score': 0, 'issue_id': 6030, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': 'ccff3e22dddfc1aa', 'authors': ['James Jewitt', 'Hao Li', 'Bram Adams', 'Gopi Krishnan Rajbahadur', 'Ahmed E. Hassan'], 'affiliations': ['School of Computing, Queens University, Kingston, ON, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2509.09873.jpg', 'data': {'categories': ['#data', '#open_source', '#dataset', '#ethics'], 'emoji': '⚖️', 'ru': {'title': 'Скрытые конфликты лицензий в открытом ИИ: выявление и решение', 'desc': 'Исследование проводит аудит лицензий в экосистеме Hugging Face, выявляя системное несоблюдение правил. Анализ охватывает 364 тысячи датасетов, 1,6 миллиона моделей и 140 тысяч проектов на GitHub. Результаты показывают, что 35,5% переходов от модели к приложению устраняют ограничительные пункты лицензий путем релицензирования на более свободных условиях. Авторы разработали прототип движка правил для обнаружения конфликтов лицензий, способный решить 86,4% таких конфликтов в программных приложениях.'}, 'en': {'title': 'Ensuring License Compliance in Open-Source AI', 'desc': 'This paper examines the licensing issues within the Hugging Face ecosystem, identifying significant non-compliance with open-source licenses. It highlights that 35.5% of transitions from models to applications ignore restrictive license terms, which can lead to legal and ethical problems. The authors introduce a rule engine that can detect and resolve these license conflicts, successfully addressing 86.4% of issues identified. By providing a comprehensive dataset and tools, the study aims to enhance license compliance in the open-source AI community.'}, 'zh': {'title': '开源AI许可证合规性：挑战与解决方案', 'desc': '本研究审计了Hugging Face生态系统中的许可证，揭示了系统性的合规性问题，并提出了一种规则引擎来检测和解决开源AI中的许可证冲突。研究表明，35.5%的模型到应用程序的转移通过重新许可在宽松条款下消除了限制性许可证条款。我们对364,000个数据集、1.6百万个模型和140,000个GitHub项目进行了首次端到端的许可证审计，发现了潜在的法律和伦理风险。我们的规则引擎能够检测近200个SPDX和特定模型条款的许可证冲突，解决了86.4%的软件应用中的许可证冲突。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (2)', '#agi', '#alignment', '#architecture (4)', '#audio (1)', '#benchmark (4)', '#cv (1)', '#data (4)', '#dataset (3)', '#diffusion (3)', '#ethics (1)', '#games (1)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (1)', '#interpretability (2)', '#leakage', '#long_context (2)', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (3)', '#open_source (3)', '#optimization (9)', '#plp', '#rag', '#reasoning (2)', '#rl (2)', '#rlhf (1)', '#robotics (1)', '#science', '#security', '#small_models', '#story_generation', '#survey', '#synthetic (1)', '#training (6)', '#transfer_learning', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-09-23 03:25',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-09-23 03:25')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-09-23 03:25')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    