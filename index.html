
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 19 papers. April 8.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">8 апреля</span> | <span id="title-articles-count">19 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-04-07.html">⬅️ <span id="prev-date">07.04</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-04-09.html">➡️ <span id="next-date">09.04</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-04.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '8 апреля', 'en': 'April 8', 'zh': '4月8日'};
        let feedDateNext = {'ru': '09.04', 'en': '04/09', 'zh': '4月9日'};
        let feedDatePrev = {'ru': '07.04', 'en': '04/07', 'zh': '4月7日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2504.05299', 'title': 'SmolVLM: Redefining small and efficient multimodal models', 'url': 'https://huggingface.co/papers/2504.05299', 'abstract': 'Large Vision-Language Models (VLMs) deliver exceptional performance but require significant computational resources, limiting their deployment on mobile and edge devices. Smaller VLMs typically mirror design choices of larger models, such as extensive image tokenization, leading to inefficient GPU memory usage and constrained practicality for on-device applications.   We introduce SmolVLM, a series of compact multimodal models specifically engineered for resource-efficient inference. We systematically explore architectural configurations, tokenization strategies, and data curation optimized for low computational overhead. Through this, we identify key design choices that yield substantial performance gains on image and video tasks with minimal memory footprints.   Our smallest model, SmolVLM-256M, uses less than 1GB GPU memory during inference and outperforms the 300-times larger Idefics-80B model, despite an 18-month development gap. Our largest model, at 2.2B parameters, rivals state-of-the-art VLMs consuming twice the GPU memory. SmolVLM models extend beyond static images, demonstrating robust video comprehension capabilities.   Our results emphasize that strategic architectural optimizations, aggressive yet efficient tokenization, and carefully curated training data significantly enhance multimodal performance, facilitating practical, energy-efficient deployments at significantly smaller scales.', 'score': 61, 'issue_id': 3120, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': '325b8841a555743d', 'authors': ['Andrés Marafioti', 'Orr Zohar', 'Miquel Farré', 'Merve Noyan', 'Elie Bakouch', 'Pedro Cuenca', 'Cyril Zakka', 'Loubna Ben Allal', 'Anton Lozhkov', 'Nouamane Tazi', 'Vaibhav Srivastav', 'Joshua Lochner', 'Hugo Larcher', 'Mathieu Morlon', 'Lewis Tunstall', 'Leandro von Werra', 'Thomas Wolf'], 'affiliations': ['Hugging Face', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2504.05299.jpg', 'data': {'categories': ['#data', '#multimodal', '#video', '#optimization', '#low_resource', '#architecture', '#inference', '#small_models'], 'emoji': '🤏', 'ru': {'title': 'Маленькие модели - большие возможности: SmolVLM revolutionizes эффективность VLM', 'desc': 'SmolVLM - это серия компактных мультимодальных моделей, разработанных для эффективного использования ресурсов при выводе. Исследователи систематически изучили архитектурные конфигурации, стратегии токенизации и подготовку данных, оптимизированные для низких вычислительных затрат. Самая маленькая модель, SmolVLM-256M, использует менее 1 ГБ видеопамяти при выводе и превосходит в 300 раз большую модель Idefics-80B. Результаты показывают, что стратегические архитектурные оптимизации и тщательно подобранные данные для обучения значительно улучшают мультимодальную производительность при меньших масштабах.'}, 'en': {'title': 'SmolVLM: Compact Models for Efficient Vision-Language Tasks', 'desc': 'This paper presents SmolVLM, a series of compact vision-language models designed to operate efficiently on mobile and edge devices. Unlike larger models that require extensive computational resources, SmolVLM employs optimized architectural configurations and tokenization strategies to minimize GPU memory usage. The smallest model, SmolVLM-256M, achieves superior performance on image and video tasks while using less than 1GB of GPU memory, outperforming much larger models. The findings highlight the importance of strategic design choices in enhancing multimodal capabilities while ensuring practical deployment in resource-constrained environments.'}, 'zh': {'title': 'SmolVLM：高效的多模态模型', 'desc': '大型视觉语言模型（VLMs）表现优异，但需要大量计算资源，限制了它们在移动和边缘设备上的应用。较小的VLM通常模仿大型模型的设计选择，导致GPU内存使用效率低下。我们提出了SmolVLM，这是一系列专为资源高效推理而设计的紧凑型多模态模型。我们的研究表明，通过优化架构配置、标记策略和数据整理，可以在保持较小内存占用的同时显著提升图像和视频任务的性能。'}}}, {'id': 'https://huggingface.co/papers/2504.05298', 'title': 'One-Minute Video Generation with Test-Time Training', 'url': 'https://huggingface.co/papers/2504.05298', 'abstract': 'Transformers today still struggle to generate one-minute videos because self-attention layers are inefficient for long context. Alternatives such as Mamba layers struggle with complex multi-scene stories because their hidden states are less expressive. We experiment with Test-Time Training (TTT) layers, whose hidden states themselves can be neural networks, therefore more expressive. Adding TTT layers into a pre-trained Transformer enables it to generate one-minute videos from text storyboards. For proof of concept, we curate a dataset based on Tom and Jerry cartoons. Compared to baselines such as Mamba~2, Gated DeltaNet, and sliding-window attention layers, TTT layers generate much more coherent videos that tell complex stories, leading by 34 Elo points in a human evaluation of 100 videos per method. Although promising, results still contain artifacts, likely due to the limited capability of the pre-trained 5B model. The efficiency of our implementation can also be improved. We have only experimented with one-minute videos due to resource constraints, but the approach can be extended to longer videos and more complex stories. Sample videos, code and annotations are available at: https://test-time-training.github.io/video-dit', 'score': 48, 'issue_id': 3116, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': 'c8edb1a98923d77d', 'authors': ['Karan Dalal', 'Daniel Koceja', 'Gashon Hussein', 'Jiarui Xu', 'Yue Zhao', 'Youjin Song', 'Shihao Han', 'Ka Chun Cheung', 'Jan Kautz', 'Carlos Guestrin', 'Tatsunori Hashimoto', 'Sanmi Koyejo', 'Yejin Choi', 'Yu Sun', 'Xiaolong Wang'], 'affiliations': ['NVIDIA', 'Stanford University', 'UC Berkeley', 'UCSD', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2504.05298.jpg', 'data': {'categories': ['#training', '#video', '#story_generation', '#long_context', '#dataset'], 'emoji': '🎬', 'ru': {'title': 'TTT слои: прорыв в генерации длинных видео трансформерами', 'desc': "Эта статья представляет новый подход к генерации длинных видео с использованием слоев Test-Time Training (TTT) в трансформерах. TTT слои позволяют создавать более выразительные скрытые состояния, что улучшает генерацию сложных многосценных историй по сравнению с альтернативами вроде Mamba. Авторы провели эксперименты на наборе данных мультфильмов 'Том и Джерри', показав преимущество TTT слоев в создании связных минутных видео по текстовым раскадровкам. Хотя результаты многообещающие, все еще присутствуют артефакты, вероятно из-за ограничений предобученной модели."}, 'en': {'title': 'Enhancing Video Generation with Test-Time Training Layers', 'desc': 'This paper addresses the challenge of generating one-minute videos from text using Transformers, which struggle with long contexts due to inefficient self-attention layers. The authors introduce Test-Time Training (TTT) layers, which enhance the expressiveness of hidden states by allowing them to be neural networks. By integrating TTT layers into a pre-trained Transformer, the model significantly improves video coherence and storytelling ability compared to existing methods like Mamba and Gated DeltaNet. The results show a notable increase in human evaluation scores, although the authors acknowledge the presence of artifacts and the need for further efficiency improvements.'}, 'zh': {'title': '提升视频生成的表达能力', 'desc': '本文探讨了在生成一分钟视频时，变换器模型面临的挑战，尤其是自注意力层在处理长上下文时的低效。我们提出了测试时训练（TTT）层，这些层的隐藏状态可以是神经网络，从而提高了表达能力。通过将TTT层添加到预训练的变换器中，我们能够从文本故事板生成更连贯的一分钟视频。尽管结果显示出良好的潜力，但仍存在一些伪影，表明预训练的5B模型能力有限。'}}}, {'id': 'https://huggingface.co/papers/2504.04022', 'title': 'Rethinking Reflection in Pre-Training', 'url': 'https://huggingface.co/papers/2504.04022', 'abstract': "A language model's ability to reflect on its own reasoning provides a key advantage for solving complex problems. While most recent research has focused on how this ability develops during reinforcement learning, we show that it actually begins to emerge much earlier - during the model's pre-training. To study this, we introduce deliberate errors into chains-of-thought and test whether the model can still arrive at the correct answer by recognizing and correcting these mistakes. By tracking performance across different stages of pre-training, we observe that this self-correcting ability appears early and improves steadily over time. For instance, an OLMo2-7B model pre-trained on 4 trillion tokens displays self-correction on our six self-reflection tasks.", 'score': 44, 'issue_id': 3128, 'pub_date': '2025-04-05', 'pub_date_card': {'ru': '5 апреля', 'en': 'April 5', 'zh': '4月5日'}, 'hash': 'c3bb14b88112a3ea', 'authors': ['Essential AI', ':', 'Darsh J Shah', 'Peter Rushton', 'Somanshu Singla', 'Mohit Parmar', 'Kurt Smith', 'Yash Vanjani', 'Ashish Vaswani', 'Adarsh Chaluvaraju', 'Andrew Hojel', 'Andrew Ma', 'Anil Thomas', 'Anthony Polloreno', 'Ashish Tanwer', 'Burhan Drak Sibai', 'Divya S Mansingka', 'Divya Shivaprasad', 'Ishaan Shah', 'Karl Stratos', 'Khoi Nguyen', 'Michael Callahan', 'Michael Pust', 'Mrinal Iyer', 'Philip Monk', 'Platon Mazarakis', 'Ritvik Kapila', 'Saurabh Srivastava', 'Tim Romanski'], 'affiliations': ['DeepSeek-AI', 'Essential AI'], 'pdf_title_img': 'assets/pdf/title_img/2504.04022.jpg', 'data': {'categories': ['#interpretability', '#training', '#reasoning', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Самоанализ языковых моделей начинается с предобучения', 'desc': 'Исследование показывает, что способность языковых моделей к самоанализу начинает формироваться уже на этапе предварительного обучения, а не только при обучении с подкреплением. Авторы вводили намеренные ошибки в цепочки рассуждений и проверяли, может ли модель прийти к правильному ответу, распознав и исправив эти ошибки. Отслеживая производительность на разных этапах предварительного обучения, они обнаружили, что способность к самокоррекции появляется рано и постепенно улучшается. Например, модель OLMo2-7B, предобученная на 4 триллионах токенов, демонстрирует самокоррекцию на шести задачах по самоанализу.'}, 'en': {'title': 'Self-Reflection: A Key to Early Problem Solving in Language Models', 'desc': 'This paper explores how language models can reflect on their own reasoning, which helps them solve complex problems. The authors reveal that this self-reflective ability starts to develop during the pre-training phase, rather than only during reinforcement learning. They introduce intentional errors in reasoning tasks to see if the model can identify and correct these mistakes. Their findings show that the self-correcting capability improves as the model undergoes more pre-training, with the OLMo2-7B model demonstrating this ability effectively after being trained on a large dataset.'}, 'zh': {'title': '语言模型的自我反思能力：早期显现与持续提升', 'desc': '本文探讨了语言模型在解决复杂问题时自我反思能力的重要性。研究表明，这种能力不仅在强化学习阶段发展，实际上在模型的预训练阶段就开始显现。我们通过在思维链中引入故意错误，测试模型是否能够识别并纠正这些错误，从而得出正确答案。结果显示，OLMo2-7B模型在预训练的早期阶段就展现出自我纠正能力，并随着时间的推移不断提高。'}}}, {'id': 'https://huggingface.co/papers/2504.05305', 'title': 'URECA: Unique Region Caption Anything', 'url': 'https://huggingface.co/papers/2504.05305', 'abstract': 'Region-level captioning aims to generate natural language descriptions for specific image regions while highlighting their distinguishing features. However, existing methods struggle to produce unique captions across multi-granularity, limiting their real-world applicability. To address the need for detailed region-level understanding, we introduce URECA dataset, a large-scale dataset tailored for multi-granularity region captioning. Unlike prior datasets that focus primarily on salient objects, URECA dataset ensures a unique and consistent mapping between regions and captions by incorporating a diverse set of objects, parts, and background elements. Central to this is a stage-wise data curation pipeline, where each stage incrementally refines region selection and caption generation. By leveraging Multimodal Large Language Models (MLLMs) at each stage, our pipeline produces distinctive and contextually grounded captions with improved accuracy and semantic diversity. Building upon this dataset, we present URECA, a novel captioning model designed to effectively encode multi-granularity regions. URECA maintains essential spatial properties such as position and shape through simple yet impactful modifications to existing MLLMs, enabling fine-grained and semantically rich region descriptions. Our approach introduces dynamic mask modeling and a high-resolution mask encoder to enhance caption uniqueness. Experiments show that URECA achieves state-of-the-art performance on URECA dataset and generalizes well to existing region-level captioning benchmarks.', 'score': 26, 'issue_id': 3115, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': '6eec948e6319fc99', 'authors': ['Sangbeom Lim', 'Junwan Kim', 'Heeji Yoon', 'Jaewoo Jung', 'Seungryong Kim'], 'affiliations': ['KAIST AI', 'Korea University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2504.05305.jpg', 'data': {'categories': ['#data', '#games', '#cv', '#interpretability', '#dataset', '#benchmark', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'Точное описание регионов изображений с помощью многоуровневого подхода', 'desc': 'Статья представляет новый набор данных URECA для многоуровневого описания регионов изображений. Авторы разработали поэтапный процесс создания данных с использованием мультимодальных больших языковых моделей для генерации уникальных и контекстуально обоснованных описаний. На основе этого набора данных предложена модель URECA, которая эффективно кодирует регионы разной детализации, сохраняя их пространственные свойства. Эксперименты показывают, что URECA достигает наилучших результатов на созданном наборе данных и хорошо обобщается на существующие эталонные тесты.'}, 'en': {'title': 'Enhancing Region-Level Captioning with URECA Dataset and Model', 'desc': 'This paper presents a new approach to region-level captioning, which generates detailed descriptions for specific parts of images. The authors introduce the URECA dataset, designed to improve the uniqueness of captions by including a variety of objects and backgrounds. They propose a novel captioning model, URECA, that uses advanced techniques like dynamic mask modeling to maintain spatial properties and enhance the quality of generated captions. The results demonstrate that URECA outperforms existing methods, providing more accurate and diverse descriptions across different image regions.'}, 'zh': {'title': '多粒度区域描述的新突破', 'desc': '区域级描述旨在为特定图像区域生成自然语言描述，并突出其独特特征。然而，现有方法在多粒度生成独特描述方面存在困难，限制了其在实际应用中的有效性。为了解决这一问题，我们引入了URECA数据集，这是一个针对多粒度区域描述的大规模数据集，确保区域与描述之间的独特和一致的映射。基于此数据集，我们提出了URECA模型，能够有效编码多粒度区域，生成细致且语义丰富的描述。'}}}, {'id': 'https://huggingface.co/papers/2504.04718', 'title': 'T1: Tool-integrated Self-verification for Test-time Compute Scaling in\n  Small Language Models', 'url': 'https://huggingface.co/papers/2504.04718', 'abstract': 'Recent studies have demonstrated that test-time compute scaling effectively improves the performance of small language models (sLMs). However, prior research has mainly examined test-time compute scaling with an additional larger model as a verifier, leaving self-verification by sLMs underexplored. In this work, we investigate whether sLMs can reliably self-verify their outputs under test-time scaling. We find that even with knowledge distillation from larger verifiers, sLMs struggle with verification tasks requiring memorization, such as numerical calculations and fact-checking. To address this limitation, we propose Tool-integrated self-verification (T1), which delegates memorization-heavy verification steps to external tools, such as a code interpreter. Our theoretical analysis shows that tool integration reduces memorization demands and improves test-time scaling performance. Experiments on the MATH benchmark demonstrate that, with T1, a Llama-3.2 1B model under test-time scaling outperforms the significantly larger Llama-3.1 8B model. Moreover, T1 generalizes effectively to both mathematical (MATH500) and multi-domain knowledge-intensive tasks (MMLU-Pro). Our findings highlight the potential of tool integration to substantially improve the self-verification abilities of sLMs.', 'score': 25, 'issue_id': 3120, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': '51f832049b5599a6', 'authors': ['Minki Kang', 'Jongwon Jeong', 'Jaewoong Cho'], 'affiliations': ['KAIST', 'KRAFTON'], 'pdf_title_img': 'assets/pdf/title_img/2504.04718.jpg', 'data': {'categories': ['#optimization', '#training', '#math', '#small_models', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Малые языковые модели становятся умнее с помощью инструментов', 'desc': 'Исследователи изучили возможность самопроверки малых языковых моделей (sLMs) при масштабировании во время тестирования. Обнаружено, что sLMs испытывают трудности с задачами, требующими запоминания, даже после дистилляции знаний от больших моделей. Предложен метод Tool-integrated self-verification (T1), который делегирует задачи, требующие запоминания, внешним инструментам. Эксперименты показали, что T1 позволяет небольшой модели Llama-3.2 1B превзойти значительно большую Llama-3.1 8B на различных задачах.'}, 'en': {'title': 'Empowering Small Models with Tool Integration for Better Self-Verification', 'desc': 'This paper explores how small language models (sLMs) can improve their performance through test-time compute scaling, particularly focusing on their ability to self-verify outputs. The authors find that sLMs face challenges in verification tasks that require memorization, such as numerical calculations and fact-checking, even when they learn from larger models. To overcome this, they introduce Tool-integrated self-verification (T1), which allows sLMs to use external tools for tasks that require heavy memorization. Their experiments show that T1 significantly enhances the performance of sLMs, enabling them to outperform larger models in various knowledge-intensive tasks.'}, 'zh': {'title': '工具集成提升小型语言模型自我验证能力', 'desc': '最近的研究表明，测试时计算扩展可以有效提高小型语言模型（sLMs）的性能。然而，之前的研究主要关注于使用更大模型作为验证者的测试时计算扩展，而对sLMs的自我验证研究较少。我们发现，即使通过知识蒸馏从更大的验证者那里获得知识，sLMs在需要记忆的验证任务（如数字计算和事实核查）中仍然面临困难。为了解决这个问题，我们提出了工具集成自我验证（T1），将重记忆的验证步骤委托给外部工具，如代码解释器。'}}}, {'id': 'https://huggingface.co/papers/2504.02828', 'title': 'Concept Lancet: Image Editing with Compositional Representation\n  Transplant', 'url': 'https://huggingface.co/papers/2504.02828', 'abstract': 'Diffusion models are widely used for image editing tasks. Existing editing methods often design a representation manipulation procedure by curating an edit direction in the text embedding or score space. However, such a procedure faces a key challenge: overestimating the edit strength harms visual consistency while underestimating it fails the editing task. Notably, each source image may require a different editing strength, and it is costly to search for an appropriate strength via trial-and-error. To address this challenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play framework for principled representation manipulation in diffusion-based image editing. At inference time, we decompose the source input in the latent (text embedding or diffusion score) space as a sparse linear combination of the representations of the collected visual concepts. This allows us to accurately estimate the presence of concepts in each image, which informs the edit. Based on the editing task (replace/add/remove), we perform a customized concept transplant process to impose the corresponding editing direction. To sufficiently model the concept space, we curate a conceptual representation dataset, CoLan-150K, which contains diverse descriptions and scenarios of visual terms and phrases for the latent dictionary. Experiments on multiple diffusion-based image editing baselines show that methods equipped with CoLan achieve state-of-the-art performance in editing effectiveness and consistency preservation.', 'score': 15, 'issue_id': 3117, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': '1c289ffc8ceda51e', 'authors': ['Jinqi Luo', 'Tianjiao Ding', 'Kwan Ho Ryan Chan', 'Hancheng Min', 'Chris Callison-Burch', 'René Vidal'], 'affiliations': ['University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2504.02828.jpg', 'data': {'categories': ['#diffusion', '#dataset', '#cv', '#inference'], 'emoji': '✂️', 'ru': {'title': 'Точное редактирование изображений с помощью концептуального скальпеля', 'desc': 'Статья представляет новый подход к редактированию изображений с помощью диффузионных моделей, называемый Concept Lancet (CoLan). CoLan решает проблему определения оптимальной силы редактирования для каждого изображения, разлагая входное изображение в латентном пространстве как линейную комбинацию визуальных концептов. Этот метод позволяет точно оценить присутствие концептов в изображении и выполнить соответствующее редактирование. Авторы также создали датасет CoLan-150K с разнообразными описаниями визуальных терминов для латентного словаря.'}, 'en': {'title': 'Precision Editing with Concept Lancet', 'desc': 'This paper introduces Concept Lancet (CoLan), a novel framework for improving image editing using diffusion models. It addresses the challenge of determining the right strength of edits needed for different images, which can vary significantly. CoLan utilizes a sparse linear combination of visual concept representations to accurately assess and manipulate the presence of these concepts in images. The framework is supported by a comprehensive dataset, CoLan-150K, which enhances the editing process by providing diverse visual descriptions and scenarios.'}, 'zh': {'title': '精准编辑，概念移植！', 'desc': '扩散模型在图像编辑任务中被广泛应用。现有的编辑方法通常通过在文本嵌入或评分空间中设计编辑方向来操控表示。然而，这种方法面临一个关键挑战：过高的编辑强度会损害视觉一致性，而过低的编辑强度则无法完成编辑任务。为了解决这个问题，我们提出了Concept Lancet（CoLan），这是一个零-shot的即插即用框架，能够在扩散基础的图像编辑中进行原则性的表示操控。'}}}, {'id': 'https://huggingface.co/papers/2504.04823', 'title': 'Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models', 'url': 'https://huggingface.co/papers/2504.04823', 'abstract': 'Recent advancements in reasoning language models have demonstrated remarkable performance in complex tasks, but their extended chain-of-thought reasoning process increases inference overhead. While quantization has been widely adopted to reduce the inference cost of large language models, its impact on reasoning models remains understudied. In this study, we conduct the first systematic study on quantized reasoning models, evaluating the open-sourced DeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B parameters, and QwQ-32B. Our investigation covers weight, KV cache, and activation quantization using state-of-the-art algorithms at varying bit-widths, with extensive evaluation across mathematical (AIME, MATH-500), scientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our findings reveal that while lossless quantization can be achieved with W8A8 or W4A16 quantization, lower bit-widths introduce significant accuracy risks. We further identify model size, model origin, and task difficulty as critical determinants of performance. Contrary to expectations, quantized models do not exhibit increased output lengths. In addition, strategically scaling the model sizes or reasoning steps can effectively enhance the performance. All quantized models and codes will be open-sourced in https://github.com/ruikangliu/Quantized-Reasoning-Models.', 'score': 14, 'issue_id': 3117, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': '92bd3deed21195f2', 'authors': ['Ruikang Liu', 'Yuxuan Sun', 'Manyi Zhang', 'Haoli Bai', 'Xianzhi Yu', 'Tiezheng Yu', 'Chun Yuan', 'Lu Hou'], 'affiliations': ['Huawei Noahs Ark Lab', 'Shenzhen International Graduate School, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.04823.jpg', 'data': {'categories': ['#open_source', '#optimization', '#inference', '#reasoning', '#math', '#training', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Квантование моделей рассуждений: баланс между эффективностью и точностью', 'desc': 'Это исследование посвящено изучению влияния квантования на языковые модели, специализирующиеся на рассуждениях. Авторы провели систематический анализ квантованных моделей рассуждений, оценивая различные семейства моделей с параметрами от 1,5B до 70B. Исследование охватывает квантование весов, KV-кэша и активаций с использованием современных алгоритмов при различных битовых ширинах. Результаты показывают, что хотя безлосстное квантование возможно при W8A8 или W4A16, более низкие битовые ширины значительно снижают точность.'}, 'en': {'title': 'Optimizing Reasoning Models with Quantization', 'desc': 'This paper investigates the effects of quantization on reasoning language models, which are known for their complex task performance but high inference costs. The authors systematically evaluate various quantization techniques on models like DeepSeek-R1-Distilled Qwen and LLaMA, focusing on different parameter sizes and quantization methods. They find that while lossless quantization is possible with certain configurations, lower bit-widths can lead to significant accuracy drops. Additionally, the study highlights that model size, origin, and task difficulty are crucial factors influencing performance, and suggests that adjusting model sizes or reasoning steps can improve outcomes.'}, 'zh': {'title': '量化推理模型的系统研究', 'desc': '最近，推理语言模型在复杂任务中表现出色，但其链式推理过程增加了推理开销。虽然量化技术已被广泛应用于降低大型语言模型的推理成本，但其对推理模型的影响仍未得到充分研究。我们首次系统性地研究了量化推理模型，评估了多个开源模型，并在不同的位宽下进行权重、KV缓存和激活量化的实验。研究发现，尽管可以实现无损量化，但较低的位宽会显著影响准确性，同时模型大小、来源和任务难度是性能的关键因素。'}}}, {'id': 'https://huggingface.co/papers/2504.05288', 'title': 'LiveVQA: Live Visual Knowledge Seeking', 'url': 'https://huggingface.co/papers/2504.05288', 'abstract': 'We introduce LiveVQA, an automatically collected dataset of latest visual knowledge from the Internet with synthesized VQA problems. LiveVQA consists of 3,602 single- and multi-hop visual questions from 6 news websites across 14 news categories, featuring high-quality image-text coherence and authentic information. Our evaluation across 15 MLLMs (e.g., GPT-4o, Gemma-3, and Qwen-2.5-VL family) demonstrates that stronger models perform better overall, with advanced visual reasoning capabilities proving crucial for complex multi-hop questions. Despite excellent performance on textual problems, models with tools like search engines still show significant gaps when addressing visual questions requiring latest visual knowledge, highlighting important areas for future research.', 'score': 10, 'issue_id': 3117, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': '8302679426ac3ec4', 'authors': ['Mingyang Fu', 'Yuyang Peng', 'Benlin Liu', 'Yao Wan', 'Dongping Chen'], 'affiliations': ['Huazhong University of Science and Technology', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2504.05288.jpg', 'data': {'categories': ['#cv', '#reasoning', '#survey', '#dataset', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'LiveVQA: Новый рубеж в визуальном вопросно-ответном анализе', 'desc': 'LiveVQA - это автоматически собранный датасет с последними визуальными знаниями из интернета и синтезированными задачами визуальных вопросов и ответов (VQA). Он содержит 3602 одношаговых и многошаговых визуальных вопроса с 6 новостных сайтов по 14 категориям новостей, отличаясь высоким качеством согласованности изображений и текста. Оценка 15 мультимодальных языковых моделей (MLLM) показала, что более мощные модели работают лучше в целом, а продвинутые возможности визуального рассуждения критически важны для сложных многошаговых вопросов. Несмотря на отличные результаты в текстовых задачах, модели с инструментами вроде поисковых систем все еще демонстрируют значительные пробелы при ответах на визуальные вопросы, требующие актуальных визуальных знаний.'}, 'en': {'title': 'Empowering Visual Question Answering with LiveVQA', 'desc': 'LiveVQA is a new dataset designed to enhance visual question answering (VQA) by providing up-to-date visual knowledge sourced from the Internet. It includes 3,602 questions that require reasoning over images and text, covering various news topics. Our tests on 15 advanced machine learning language models (MLLMs) show that models with better visual reasoning skills excel at answering complex questions. However, even the best models struggle with visual questions that need the latest information, indicating a need for further research in this area.'}, 'zh': {'title': '最新视觉知识的问答挑战', 'desc': '我们介绍了LiveVQA，这是一个自动收集的最新视觉知识数据集，包含合成的视觉问答（VQA）问题。LiveVQA包含来自6个新闻网站的3,602个单跳和多跳视觉问题，涵盖14个新闻类别，具有高质量的图像-文本一致性和真实信息。我们的评估显示，15个大型语言模型（如GPT-4o、Gemma-3和Qwen-2.5-VL系列）中，性能更强的模型在整体表现上更好，尤其在复杂的多跳问题上，先进的视觉推理能力至关重要。尽管在文本问题上表现出色，但使用搜索引擎等工具的模型在处理需要最新视觉知识的视觉问题时仍存在显著差距，这突显了未来研究的重要领域。'}}}, {'id': 'https://huggingface.co/papers/2504.05304', 'title': 'Gaussian Mixture Flow Matching Models', 'url': 'https://huggingface.co/papers/2504.05304', 'abstract': 'Diffusion models approximate the denoising distribution as a Gaussian and predict its mean, whereas flow matching models reparameterize the Gaussian mean as flow velocity. However, they underperform in few-step sampling due to discretization error and tend to produce over-saturated colors under classifier-free guidance (CFG). To address these limitations, we propose a novel Gaussian mixture flow matching (GMFlow) model: instead of predicting the mean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a multi-modal flow velocity distribution, which can be learned with a KL divergence loss. We demonstrate that GMFlow generalizes previous diffusion and flow matching models where a single Gaussian is learned with an L_2 denoising loss. For inference, we derive GM-SDE/ODE solvers that leverage analytic denoising distributions and velocity fields for precise few-step sampling. Furthermore, we introduce a novel probabilistic guidance scheme that mitigates the over-saturation issues of CFG and improves image generation quality. Extensive experiments demonstrate that GMFlow consistently outperforms flow matching baselines in generation quality, achieving a Precision of 0.942 with only 6 sampling steps on ImageNet 256times256.', 'score': 6, 'issue_id': 3115, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': 'b0223808c61a3545', 'authors': ['Hansheng Chen', 'Kai Zhang', 'Hao Tan', 'Zexiang Xu', 'Fujun Luan', 'Leonidas Guibas', 'Gordon Wetzstein', 'Sai Bi'], 'affiliations': ['Adobe Research, CA 95110, USA', 'Hillbot', 'Stanford University, CA 94305, USA'], 'pdf_title_img': 'assets/pdf/title_img/2504.05304.jpg', 'data': {'categories': ['#training', '#cv', '#diffusion', '#inference'], 'emoji': '🌊', 'ru': {'title': 'GMFlow: мощная генерация изображений с гауссовыми смесями', 'desc': 'Статья представляет новую модель Gaussian mixture flow matching (GMFlow) для генерации изображений. GMFlow предсказывает параметры динамической гауссовой смеси для захвата мультимодального распределения скорости потока, что можно обучить с помощью потери KL-дивергенции. Авторы разработали специальные решатели GM-SDE/ODE для точного сэмплирования за небольшое число шагов. Также предложена новая схема вероятностного управления, улучшающая качество генерации изображений и решающая проблему пересыщенности цветов при классификационно-свободном управлении.'}, 'en': {'title': 'GMFlow: Enhancing Image Generation with Dynamic Gaussian Mixtures', 'desc': 'This paper introduces a new model called Gaussian Mixture Flow Matching (GMFlow) that improves upon traditional diffusion models and flow matching models. Instead of just predicting a single Gaussian mean, GMFlow predicts parameters for a dynamic Gaussian mixture, allowing it to better capture complex distributions in the data. The model addresses issues like discretization error and color saturation in generated images by using a novel probabilistic guidance scheme. Experimental results show that GMFlow achieves higher image generation quality with fewer sampling steps compared to existing methods.'}, 'zh': {'title': '高斯混合流匹配：提升图像生成质量的新方法', 'desc': '扩散模型通过高斯分布来近似去噪分布并预测其均值，而流匹配模型则将高斯均值重新参数化为流速。然而，它们在少步采样时表现不佳，主要是由于离散化误差，并且在无分类器引导下容易产生过饱和的颜色。为了解决这些问题，我们提出了一种新颖的高斯混合流匹配（GMFlow）模型：GMFlow预测动态高斯混合参数，以捕捉多模态流速分布，并通过KL散度损失进行学习。实验表明，GMFlow在生成质量上始终优于流匹配基线，在ImageNet 256x256上仅用6个采样步骤就达到了0.942的精度。'}}}, {'id': 'https://huggingface.co/papers/2504.05118', 'title': 'VAPO: Efficient and Reliable Reinforcement Learning for Advanced\n  Reasoning Tasks', 'url': 'https://huggingface.co/papers/2504.05118', 'abstract': 'We present VAPO, Value-based Augmented Proximal Policy Optimization framework for reasoning models., a novel framework tailored for reasoning models within the value-based paradigm. Benchmarked the AIME 2024 dataset, VAPO, built on the Qwen 32B pre-trained model, attains a state-of-the-art score of 60.4. In direct comparison under identical experimental settings, VAPO outperforms the previously reported results of DeepSeek-R1-Zero-Qwen-32B and DAPO by more than 10 points. The training process of VAPO stands out for its stability and efficiency. It reaches state-of-the-art performance within a mere 5,000 steps. Moreover, across multiple independent runs, no training crashes occur, underscoring its reliability. This research delves into long chain-of-thought (long-CoT) reasoning using a value-based reinforcement learning framework. We pinpoint three key challenges that plague value-based methods: value model bias, the presence of heterogeneous sequence lengths, and the sparsity of reward signals. Through systematic design, VAPO offers an integrated solution that effectively alleviates these challenges, enabling enhanced performance in long-CoT reasoning tasks.', 'score': 6, 'issue_id': 3123, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': '33841bfe919ea1d5', 'authors': ['YuYue', 'Yufeng Yuan', 'Qiying Yu', 'Xiaochen Zuo', 'Ruofei Zhu', 'Wenyuan Xu', 'Jiaze Chen', 'Chengyi Wang', 'TianTian Fan', 'Zhengyin Du', 'Xiangpeng Wei', 'Gaohong Liu', 'Juncai Liu', 'Lingjun Liu', 'Haibin Lin', 'Zhiqi Lin', 'Bole Ma', 'Chi Zhang', 'Mofan Zhang', 'Wang Zhang', 'Hang Zhu', 'Ru Zhang', 'Xin Liu', 'Mingxuan Wang', 'Yonghui Wu', 'Lin Yan'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.05118.jpg', 'data': {'categories': ['#long_context', '#rl', '#benchmark', '#optimization', '#reasoning', '#training', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'VAPO: прорыв в моделировании рассуждений с помощью ценностно-ориентированного обучения с подкреплением', 'desc': 'VAPO - это новая система для моделей рассуждений, основанная на ценностно-ориентированном подходе. Используя предобученную модель Qwen 32B, VAPO достигает лучшего результата в 60.4 балла на датасете AIME 2024. Система превосходит предыдущие результаты более чем на 10 пунктов и достигает высокой производительности всего за 5000 шагов обучения. VAPO решает ключевые проблемы ценностно-ориентированных методов в задачах рассуждений с длинной цепочкой мыслей.'}, 'en': {'title': 'VAPO: Revolutionizing Reasoning with Value-Based Reinforcement Learning', 'desc': 'VAPO is a new framework designed for reasoning models that uses value-based reinforcement learning. It achieves a remarkable score of 60.4 on the AIME 2024 dataset, surpassing previous models by over 10 points. The training of VAPO is both stable and efficient, completing in just 5,000 steps without any crashes during multiple runs. This research addresses key issues in value-based methods, such as model bias and reward sparsity, providing solutions that improve long chain-of-thought reasoning.'}, 'zh': {'title': 'VAPO：提升推理模型的价值基础框架', 'desc': '本文介绍了一种名为VAPO的框架，旨在为基于价值的推理模型提供支持。VAPO在AIME 2024数据集上表现出色，达到了60.4的最新成绩，超越了之前的DeepSeek-R1-Zero-Qwen-32B和DAPO模型。该框架的训练过程稳定高效，仅需5000步即可达到最佳性能，并且在多次独立运行中没有发生训练崩溃，显示出其可靠性。VAPO通过系统设计有效解决了基于价值的方法面临的三个主要挑战，提升了长链推理任务的表现。'}}}, {'id': 'https://huggingface.co/papers/2504.04715', 'title': 'Are You Getting What You Pay For? Auditing Model Substitution in LLM\n  APIs', 'url': 'https://huggingface.co/papers/2504.04715', 'abstract': 'The proliferation of Large Language Models (LLMs) accessed via black-box APIs introduces a significant trust challenge: users pay for services based on advertised model capabilities (e.g., size, performance), but providers may covertly substitute the specified model with a cheaper, lower-quality alternative to reduce operational costs. This lack of transparency undermines fairness, erodes trust, and complicates reliable benchmarking. Detecting such substitutions is difficult due to the black-box nature, typically limiting interaction to input-output queries. This paper formalizes the problem of model substitution detection in LLM APIs. We systematically evaluate existing verification techniques, including output-based statistical tests, benchmark evaluations, and log probability analysis, under various realistic attack scenarios like model quantization, randomized substitution, and benchmark evasion. Our findings reveal the limitations of methods relying solely on text outputs, especially against subtle or adaptive attacks. While log probability analysis offers stronger guarantees when available, its accessibility is often limited. We conclude by discussing the potential of hardware-based solutions like Trusted Execution Environments (TEEs) as a pathway towards provable model integrity, highlighting the trade-offs between security, performance, and provider adoption. Code is available at https://github.com/sunblaze-ucb/llm-api-audit', 'score': 6, 'issue_id': 3117, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': '28e9dfa4b4a0421a', 'authors': ['Will Cai', 'Tianneng Shi', 'Xuandong Zhao', 'Dawn Song'], 'affiliations': ['University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2504.04715.jpg', 'data': {'categories': ['#security', '#inference', '#benchmark', '#ethics'], 'emoji': '🕵️', 'ru': {'title': 'Защита от подмены: обеспечение честности в API языковых моделей', 'desc': 'Статья рассматривает проблему доверия к API больших языковых моделей (LLM), когда провайдеры могут тайно подменять заявленные модели на более дешевые аналоги. Авторы формализуют задачу обнаружения таких подмен и оценивают эффективность существующих методов верификации. Исследование показывает ограниченность методов, основанных только на анализе текстовых выходов, особенно против адаптивных атак. В качестве потенциального решения предлагается использование доверенных сред исполнения (TEE) для обеспечения целостности модели.'}, 'en': {'title': 'Ensuring Trust in Large Language Models: Detecting Substitutions in Black-Box APIs', 'desc': 'This paper addresses the issue of trust in Large Language Models (LLMs) accessed through APIs, where users may unknowingly receive lower-quality models instead of the advertised ones. It formalizes the challenge of detecting these model substitutions, which is complicated by the black-box nature of LLMs that limits user interactions to simple input-output queries. The authors evaluate various existing verification techniques, revealing their limitations, particularly against sophisticated attacks that can evade detection. They propose hardware-based solutions like Trusted Execution Environments (TEEs) as a potential way to ensure model integrity, while also considering the trade-offs involved.'}, 'zh': {'title': '确保大型语言模型的透明性与信任', 'desc': '本文探讨了大型语言模型（LLMs）在黑箱API中使用所带来的信任挑战。用户支付服务费用时，依赖于模型的能力（如规模和性能），但提供者可能会偷偷用更便宜、质量更低的替代模型来降低成本。这种缺乏透明度的问题影响了公平性和信任度，并使得可靠的基准测试变得复杂。我们系统评估了现有的验证技术，并提出了基于硬件的解决方案，以提高模型的完整性。'}}}, {'id': 'https://huggingface.co/papers/2504.02882', 'title': 'DiaTool-DPO: Multi-Turn Direct Preference Optimization for\n  Tool-Augmented Large Language Models', 'url': 'https://huggingface.co/papers/2504.02882', 'abstract': "Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in real-world applications, but face challenges in handling incomplete queries and out-of-scope requests. While existing approaches rely mainly on Supervised Fine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method that enhances TA-LLM's dialogue capabilities through Direct Preference Optimization. We model TA-LLM interactions as a Markov Decision Process with 5 distinct dialogue states and categorize user queries into 3 types based on their state transition trajectories. We automatically construct paired trajectory datasets of correct and incorrect dialogue flows and introduce a specialized objective loss for dialogue control. Our comprehensive evaluation demonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in information gathering, 91% in tool call rejection) with substantial improvements over baseline (44% and 9.6% respectively) while maintaining core functionality. Our approach opens new possibilities for developing TA-LLMs that can handle diverse real-world scenarios without requiring additional expert demonstrations or human labeling.", 'score': 4, 'issue_id': 3120, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': 'c42de57890092432', 'authors': ['Sunghee Jung', 'Donghun Lee', 'Shinbok Lee', 'Gaeun Seo', 'Daniel Lee', 'Byeongil Ko', 'Junrae Cho', 'Kihyun Kim', 'Eunggyun Kim', 'Myeongcheol Shin'], 'affiliations': ['Kakao Corp. Seongnam-si, Gyeonggi-do, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2504.02882.jpg', 'data': {'categories': ['#rlhf', '#optimization', '#training', '#alignment'], 'emoji': '🤖', 'ru': {'title': 'DiaTool-DPO: Умный диалог без экспертных примеров', 'desc': 'DiaTool-DPO - это новый метод, улучшающий диалоговые возможности инструментально-расширенных больших языковых моделей (TA-LLM) с помощью прямой оптимизации предпочтений. Авторы моделируют взаимодействие TA-LLM как марковский процесс принятия решений с 5 состояниями диалога и 3 типами запросов пользователей. Метод автоматически создает наборы данных правильных и неправильных траекторий диалога и вводит специальную функцию потерь для управления диалогом. Оценка показывает, что DiaTool-DPO приближается к производительности GPT-4, значительно превосходя базовый уровень, при сохранении основной функциональности.'}, 'en': {'title': 'Enhancing Dialogue with DiaTool-DPO for TA-LLMs', 'desc': 'This paper introduces DiaTool-DPO, a new method to improve Tool-Augmented Large Language Models (TA-LLMs) in handling incomplete and out-of-scope queries. It treats TA-LLM interactions as a Markov Decision Process, identifying five dialogue states and categorizing user queries into three types based on their transitions. The method involves creating paired datasets of correct and incorrect dialogue flows and using a specialized loss function for better dialogue control. The results show that DiaTool-DPO significantly enhances performance, approaching that of GPT-4o, while reducing the need for expert input and human labeling.'}, 'zh': {'title': '提升对话能力的新方法：DiaTool-DPO', 'desc': '本文提出了一种新方法DiaTool-DPO，旨在提升工具增强大型语言模型（TA-LLM）的对话能力。我们将TA-LLM的交互建模为马尔可夫决策过程，并根据对话状态的转移轨迹将用户查询分为三类。通过自动构建正确和错误对话流的配对轨迹数据集，并引入专门的目标损失函数，我们的评估显示DiaTool-DPO在信息获取和工具调用拒绝方面的表现接近GPT-4o。该方法为开发能够处理多样化现实场景的TA-LLM开辟了新可能，无需额外的专家演示或人工标注。'}}}, {'id': 'https://huggingface.co/papers/2504.03964', 'title': 'Clinical ModernBERT: An efficient and long context encoder for\n  biomedical text', 'url': 'https://huggingface.co/papers/2504.03964', 'abstract': 'We introduce Clinical ModernBERT, a transformer based encoder pretrained on large scale biomedical literature, clinical notes, and medical ontologies, incorporating PubMed abstracts, MIMIC IV clinical data, and medical codes with their textual descriptions. Building on ModernBERT the current state of the art natural language text encoder featuring architectural upgrades such as rotary positional embeddings (RoPE), Flash Attention, and extended context length up to 8,192 tokens our model adapts these innovations specifically for biomedical and clinical domains. Clinical ModernBERT excels at producing semantically rich representations tailored for long context tasks. We validate this both by analyzing its pretrained weights and through empirical evaluation on a comprehensive suite of clinical NLP benchmarks.', 'score': 3, 'issue_id': 3119, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 апреля', 'en': 'April 4', 'zh': '4月4日'}, 'hash': '6602bd6699f6f653', 'authors': ['Simon A. Lee', 'Anthony Wu', 'Jeffrey N. Chiang'], 'affiliations': ['Department of Computational Medicine & Neurosurgery UCLA', 'Department of Computational Medicine UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2504.03964.jpg', 'data': {'categories': ['#science', '#long_context', '#benchmark', '#architecture', '#healthcare', '#dataset'], 'emoji': '🏥', 'ru': {'title': 'Прорыв в обработке медицинских текстов: Clinical ModernBERT', 'desc': 'Эта статья представляет Clinical ModernBERT - трансформерную модель, предобученную на биомедицинской литературе, клинических заметках и медицинских онтологиях. Модель основана на архитектуре ModernBERT и включает улучшения, такие как ротационные позиционные эмбеддинги и расширенный контекст до 8192 токенов. Clinical ModernBERT специализируется на создании семантически богатых представлений для задач с длинным контекстом в медицинской сфере. Эффективность модели подтверждена анализом предобученных весов и оценкой на наборе клинических NLP-бенчмарков.'}, 'en': {'title': 'Empowering Clinical NLP with Advanced Transformer Technology', 'desc': 'Clinical ModernBERT is a specialized transformer model designed for the biomedical and clinical fields. It is pretrained on a vast array of data, including biomedical literature and clinical notes, to enhance its understanding of medical language. The model incorporates advanced features like rotary positional embeddings and Flash Attention, allowing it to handle longer text inputs effectively. Its performance is validated through rigorous testing on various clinical natural language processing benchmarks, demonstrating its ability to generate meaningful representations for complex medical tasks.'}, 'zh': {'title': '生物医学领域的强大文本编码器', 'desc': '我们介绍了Clinical ModernBERT，这是一种基于变换器的编码器，经过大规模生物医学文献、临床笔记和医学本体的预训练。该模型结合了PubMed摘要、MIMIC IV临床数据和医学代码及其文本描述，采用了现代BERT的架构升级，如旋转位置嵌入（RoPE）和闪存注意力（Flash Attention）。Clinical ModernBERT在处理长上下文任务时，能够生成语义丰富的表示。我们通过分析其预训练权重和在临床自然语言处理基准上的实证评估来验证其有效性。'}}}, {'id': 'https://huggingface.co/papers/2504.03193', 'title': 'Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language\n  Models for Domain-Generalized Semantic Segmentation', 'url': 'https://huggingface.co/papers/2504.03193', 'abstract': 'Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gained traction in Domain Generalized Semantic Segmentation (DGSS) due to their strong generalization capabilities. However, existing DGSS methods often rely exclusively on either VFMs or VLMs, overlooking their complementary strengths. VFMs (e.g., DINOv2) excel at capturing fine-grained features, while VLMs (e.g., CLIP) provide robust text alignment but struggle with coarse granularity. Despite their complementary strengths, effectively integrating VFMs and VLMs with attention mechanisms is challenging, as the increased patch tokens complicate long-sequence modeling. To address this, we propose MFuser, a novel Mamba-based fusion framework that efficiently combines the strengths of VFMs and VLMs while maintaining linear scalability in sequence length. MFuser consists of two key components: MVFuser, which acts as a co-adapter to jointly fine-tune the two models by capturing both sequential and spatial dynamics; and MTEnhancer, a hybrid attention-Mamba module that refines text embeddings by incorporating image priors. Our approach achieves precise feature locality and strong text alignment without incurring significant computational overhead. Extensive experiments demonstrate that MFuser significantly outperforms state-of-the-art DGSS methods, achieving 68.20 mIoU on synthetic-to-real and 71.87 mIoU on real-to-real benchmarks. The code is available at https://github.com/devinxzhang/MFuser.', 'score': 3, 'issue_id': 3120, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 апреля', 'en': 'April 4', 'zh': '4月4日'}, 'hash': '6eca2c0b1acce1f3', 'authors': ['Xin Zhang', 'Robby T. Tan'], 'affiliations': ['ASUS Intelligent Cloud Services', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2504.03193.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#architecture', '#cv', '#benchmark', '#synthetic'], 'emoji': '🔍', 'ru': {'title': 'MFuser: Эффективное слияние VFM и VLM для улучшенной семантической сегментации', 'desc': 'Эта статья представляет новый подход к обобщенной семантической сегментации доменов (DGSS), называемый MFuser. Он объединяет сильные стороны моделей визуального основания (VFM) и визуально-языковых моделей (VLM), используя архитектуру на основе Mamba. MFuser включает в себя MVFuser для совместной доводки моделей и MTEnhancer для улучшения текстовых эмбеддингов с учетом визуальных признаков. Экспериментальные результаты показывают значительное превосходство MFuser над современными методами DGSS на различных бенчмарках.'}, 'en': {'title': 'Harnessing the Power of Vision Models for Better Segmentation', 'desc': 'This paper introduces MFuser, a new framework that combines Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) for Domain Generalized Semantic Segmentation (DGSS). VFMs are good at capturing detailed visual features, while VLMs excel in aligning text with images, but they have limitations when used separately. MFuser uses a co-adapter and a hybrid attention mechanism to effectively merge these models, allowing for better feature extraction and text alignment without heavy computational costs. The results show that MFuser outperforms existing DGSS methods, achieving high accuracy on various benchmarks.'}, 'zh': {'title': '融合视觉与语言，提升语义分割能力', 'desc': '视觉基础模型（VFM）和视觉语言模型（VLM）在领域泛化语义分割（DGSS）中因其强大的泛化能力而受到关注。现有的DGSS方法通常只依赖于VFM或VLM，忽视了它们的互补优势。我们提出了MFuser，一个新颖的融合框架，能够高效结合VFM和VLM的优点，同时保持序列长度的线性可扩展性。通过联合微调和混合注意力机制，MFuser在特征局部性和文本对齐方面表现出色，显著超越了现有的DGSS方法。'}}}, {'id': 'https://huggingface.co/papers/2504.02812', 'title': 'BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose\n  Estimation', 'url': 'https://huggingface.co/papers/2504.02812', 'abstract': 'We present the evaluation methodology, datasets and results of the BOP Challenge 2024, the sixth in a series of public competitions organized to capture the state of the art in 6D object pose estimation and related tasks. In 2024, our goal was to transition BOP from lab-like setups to real-world scenarios. First, we introduced new model-free tasks, where no 3D object models are available and methods need to onboard objects just from provided reference videos. Second, we defined a new, more practical 6D object detection task where identities of objects visible in a test image are not provided as input. Third, we introduced new BOP-H3 datasets recorded with high-resolution sensors and AR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D models and onboarding videos to support both model-based and model-free tasks. Participants competed on seven challenge tracks, each defined by a task, object onboarding setup, and dataset group. Notably, the best 2024 method for model-based 6D localization of unseen objects (FreeZeV2.1) achieves 22% higher accuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is only 4% behind the best 2023 method for seen objects (GPose2023) although being significantly slower (24.9 vs 2.7s per image). A more practical 2024 method for this task is Co-op which takes only 0.8s per image and is 25X faster and 13% more accurate than GenFlow. Methods have a similar ranking on 6D detection as on 6D localization but higher run time. On model-based 2D detection of unseen objects, the best 2024 method (MUSE) achieves 21% relative improvement compared to the best 2023 method (CNOS). However, the 2D detection accuracy for unseen objects is still noticealy (-53%) behind the accuracy for seen objects (GDet2023). The online evaluation system stays open and is available at http://bop.felk.cvut.cz/', 'score': 3, 'issue_id': 3118, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': '692278d7307c7950', 'authors': ['Van Nguyen Nguyen', 'Stephen Tyree', 'Andrew Guo', 'Mederic Fourmy', 'Anas Gouda', 'Taeyeop Lee', 'Sungphill Moon', 'Hyeontae Son', 'Lukas Ranftl', 'Jonathan Tremblay', 'Eric Brachmann', 'Bertram Drost', 'Vincent Lepetit', 'Carsten Rother', 'Stan Birchfield', 'Jiri Matas', 'Yann Labbe', 'Martin Sundermeyer', 'Tomas Hodan'], 'affiliations': ['CTU Prague', 'ENPC', 'Google', 'Heidelberg University', 'KAIST', 'MVTec', 'Meta', 'NAVER LABS', 'NVIDIA', 'Niantic', 'TU Dortmund', 'TU Munich', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2504.02812.jpg', 'data': {'categories': ['#dataset', '#cv', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Прорыв в компьютерном зрении: новые горизонты оценки 6D позы объектов', 'desc': 'Статья представляет результаты соревнования BOP Challenge 2024 по оценке 6D позы объектов. Введены новые задачи без использования 3D моделей и более практичные сценарии обнаружения объектов. Представлены новые наборы данных BOP-H3, записанные с помощью высокоточных сенсоров и AR/VR гарнитур. Лучшие методы 2024 года показали значительное улучшение точности по сравнению с методами 2023 года для различных задач, включая локализацию и обнаружение объектов.'}, 'en': {'title': 'Advancing 6D Object Pose Estimation in Real-World Scenarios', 'desc': 'The BOP Challenge 2024 focuses on advancing 6D object pose estimation by transitioning from controlled lab environments to real-world applications. This year, new model-free tasks were introduced, requiring methods to learn from reference videos without 3D models. The challenge also featured a practical 6D object detection task where object identities were not provided, alongside the release of the BOP-H3 datasets that simulate real-world conditions. Notably, the top-performing methods demonstrated significant improvements in accuracy and speed compared to previous years, highlighting the ongoing evolution in this field of machine learning.'}, 'zh': {'title': 'BOP挑战赛：从实验室到真实世界的6D物体姿态估计', 'desc': '本文介绍了2024年BOP挑战赛的评估方法、数据集和结果，这是一个旨在捕捉6D物体姿态估计最新技术的公开竞赛。2024年的目标是将BOP从实验室环境转向真实世界场景，推出了新的无模型任务，要求方法仅通过参考视频进行物体识别。我们还定义了一个更实用的6D物体检测任务，测试图像中物体的身份不再作为输入提供。此外，BOP-H3数据集使用高分辨率传感器和AR/VR头显录制，支持模型基础和无模型任务。'}}}, {'id': 'https://huggingface.co/papers/2504.03770', 'title': 'JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language\n  Model', 'url': 'https://huggingface.co/papers/2504.03770', 'abstract': 'Multimodal large language models (MLLMs) excel in vision-language tasks but also pose significant risks of generating harmful content, particularly through jailbreak attacks. Jailbreak attacks refer to intentional manipulations that bypass safety mechanisms in models, leading to the generation of inappropriate or unsafe content. Detecting such attacks is critical to ensuring the responsible deployment of MLLMs. Existing jailbreak detection methods face three primary challenges: (1) Many rely on model hidden states or gradients, limiting their applicability to white-box models, where the internal workings of the model are accessible; (2) They involve high computational overhead from uncertainty-based analysis, which limits real-time detection, and (3) They require fully labeled harmful datasets, which are often scarce in real-world settings. To address these issues, we introduce a test-time adaptive framework called JAILDAM. Our method leverages a memory-based approach guided by policy-driven unsafe knowledge representations, eliminating the need for explicit exposure to harmful data. By dynamically updating unsafe knowledge during test-time, our framework improves generalization to unseen jailbreak strategies while maintaining efficiency. Experiments on multiple VLM jailbreak benchmarks demonstrate that JAILDAM delivers state-of-the-art performance in harmful content detection, improving both accuracy and speed.', 'score': 2, 'issue_id': 3117, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': '01c2f4c752f3e71d', 'authors': ['Yi Nian', 'Shenzhe Zhu', 'Yuehan Qin', 'Li Li', 'Ziyi Wang', 'Chaowei Xiao', 'Yue Zhao'], 'affiliations': ['University of Maryland', 'University of Southern California', 'University of Toronto', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2504.03770.jpg', 'data': {'categories': ['#alignment', '#multimodal', '#security', '#dataset', '#benchmark'], 'emoji': '🛡️', 'ru': {'title': "Адаптивная защита MLLM от атак 'jailbreak' без доступа к вредоносным данным", 'desc': "В статье представлен метод JAILDAM для обнаружения атак типа 'jailbreak' на мультимодальные большие языковые модели (MLLM). JAILDAM использует подход на основе памяти, управляемый представлениями небезопасных знаний, что устраняет необходимость в явном воздействии на вредоносные данные. Метод динамически обновляет небезопасные знания во время тестирования, улучшая обобщение на новые стратегии атак при сохранении эффективности. Эксперименты показывают, что JAILDAM обеспечивает современную производительность в обнаружении вредоносного контента, улучшая как точность, так и скорость."}, 'en': {'title': 'JAILDAM: Enhancing Safety in MLLMs Against Jailbreak Attacks', 'desc': 'This paper discusses the challenges of detecting jailbreak attacks in multimodal large language models (MLLMs), which can generate harmful content. Jailbreak attacks manipulate models to bypass safety features, making detection crucial for responsible use. The authors present JAILDAM, a novel framework that adapts during testing to identify these attacks without needing extensive harmful datasets. By using a memory-based approach, JAILDAM enhances detection efficiency and accuracy against various jailbreak strategies.'}, 'zh': {'title': '提升多模态模型安全性的关键', 'desc': '多模态大型语言模型（MLLMs）在视觉语言任务中表现出色，但也存在生成有害内容的重大风险，尤其是通过越狱攻击。越狱攻击是指故意操控以绕过模型的安全机制，导致生成不当或不安全的内容。检测此类攻击对于确保MLLMs的负责任部署至关重要。我们提出了一种名为JAILDAM的测试时自适应框架，通过动态更新不安全知识来提高对未见越狱策略的泛化能力，同时保持高效性。'}}}, {'id': 'https://huggingface.co/papers/2504.04155', 'title': 'GlotEval: A Test Suite for Massively Multilingual Evaluation of Large\n  Language Models', 'url': 'https://huggingface.co/papers/2504.04155', 'abstract': "Large language models (LLMs) are advancing at an unprecedented pace globally, with regions increasingly adopting these models for applications in their primary language. Evaluation of these models in diverse linguistic environments, especially in low-resource languages, has become a major challenge for academia and industry. Existing evaluation frameworks are disproportionately focused on English and a handful of high-resource languages, thereby overlooking the realistic performance of LLMs in multilingual and lower-resource scenarios. To address this gap, we introduce GlotEval, a lightweight framework designed for massively multilingual evaluation. Supporting seven key tasks (machine translation, text classification, summarization, open-ended generation, reading comprehension, sequence labeling, and intrinsic evaluation), spanning over dozens to hundreds of languages, GlotEval highlights consistent multilingual benchmarking, language-specific prompt templates, and non-English-centric machine translation. This enables a precise diagnosis of model strengths and weaknesses in diverse linguistic contexts. A multilingual translation case study demonstrates GlotEval's applicability for multilingual and language-specific evaluations.", 'score': 0, 'issue_id': 3124, 'pub_date': '2025-04-05', 'pub_date_card': {'ru': '5 апреля', 'en': 'April 5', 'zh': '4月5日'}, 'hash': '8105e367a29eaa88', 'authors': ['Hengyu Luo', 'Zihao Li', 'Joseph Attieh', 'Sawal Devkota', 'Ona de Gibert', 'Shaoxiong Ji', 'Peiqin Lin', 'Bhavani Sai Praneeth Varma Mantina', 'Ananda Sreenidhi', 'Raúl Vázquez', 'Mengjie Wang', 'Samea Yusofi', 'Jörg Tiedemann'], 'affiliations': ['Technical University of Darmstadt, Germany', 'University of Helsinki, Finland', 'University of Munich, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2504.04155.jpg', 'data': {'categories': ['#low_resource', '#multilingual', '#benchmark', '#machine_translation', '#open_source'], 'emoji': '🌐', 'ru': {'title': 'GlotEval: Многоязычная оценка языковых моделей без англоцентризма', 'desc': 'GlotEval - это новая система оценки больших языковых моделей (LLM), разработанная для многоязычной среды. Она поддерживает семь ключевых задач, включая машинный перевод, классификацию текста и генерацию текста, охватывая сотни языков. GlotEval использует специфичные для каждого языка шаблоны промптов и нацелена на оценку, не сфокусированную исключительно на английском языке. Эта система позволяет точно диагностировать сильные и слабые стороны моделей в различных языковых контекстах.'}, 'en': {'title': 'GlotEval: Bridging the Evaluation Gap for Multilingual LLMs', 'desc': 'This paper presents GlotEval, a new framework aimed at evaluating large language models (LLMs) across multiple languages, particularly focusing on low-resource languages. Current evaluation methods are biased towards English and a few high-resource languages, which limits our understanding of LLM performance in diverse linguistic settings. GlotEval supports various tasks such as machine translation and text classification, allowing for comprehensive benchmarking across dozens of languages. By providing language-specific templates and non-English-centric evaluations, GlotEval helps identify the strengths and weaknesses of LLMs in multilingual contexts.'}, 'zh': {'title': '多语言评估的新框架：GlotEval', 'desc': '大型语言模型（LLMs）在全球范围内迅速发展，各地区越来越多地采用这些模型进行本国语言的应用。评估这些模型在不同语言环境中的表现，尤其是在资源匮乏的语言中，已成为学术界和工业界的一大挑战。现有的评估框架主要集中在英语和少数高资源语言上，忽视了LLMs在多语言和低资源场景中的实际表现。为了解决这一问题，我们推出了GlotEval，这是一个轻量级框架，旨在进行大规模多语言评估，支持七项关键任务，帮助准确诊断模型在不同语言环境中的优缺点。'}}}, {'id': 'https://huggingface.co/papers/2504.04152', 'title': 'Rethinking Multilingual Continual Pretraining: Data Mixing for Adapting\n  LLMs Across Languages and Resources', 'url': 'https://huggingface.co/papers/2504.04152', 'abstract': 'Large Language Models (LLMs) exhibit significant disparities in performance across languages, primarily benefiting high-resource languages while marginalizing underrepresented ones. Continual Pretraining (CPT) has emerged as a promising approach to address this imbalance, although the relative effectiveness of monolingual, bilingual, and code-augmented data strategies remains unclear. This study systematically evaluates 36 CPT configurations involving three multilingual base models, across 30+ languages categorized as altruistic, selfish, and stagnant, spanning various resource levels. Our findings reveal three major insights: (1) Bilingual CPT improves multilingual classification but often causes language mixing issues during generation. (2) Including programming code data during CPT consistently enhances multilingual classification accuracy, particularly benefiting low-resource languages, but introduces a trade-off by slightly degrading generation quality. (3) Contrary to prior work, we observe substantial deviations from language classifications according to their impact on cross-lingual transfer: Languages classified as altruistic often negatively affect related languages, selfish languages show conditional and configuration-dependent behavior, and stagnant languages demonstrate surprising adaptability under certain CPT conditions. These nuanced interactions emphasize the complexity of multilingual representation learning, underscoring the importance of systematic studies on generalizable language classification to inform future multilingual CPT strategies.', 'score': 0, 'issue_id': 3124, 'pub_date': '2025-04-05', 'pub_date_card': {'ru': '5 апреля', 'en': 'April 5', 'zh': '4月5日'}, 'hash': '3c22dafb05a10aab', 'authors': ['Zihao Li', 'Shaoxiong Ji', 'Hengyu Luo', 'Jörg Tiedemann'], 'affiliations': ['Technical University of Darmstadt', 'University of Helsinki'], 'pdf_title_img': 'assets/pdf/title_img/2504.04152.jpg', 'data': {'categories': ['#training', '#transfer_learning', '#low_resource', '#multilingual'], 'emoji': '🌐', 'ru': {'title': 'Улучшение многоязычности LLM: сложности и компромиссы непрерывного предобучения', 'desc': 'Исследование посвящено проблеме неравномерной эффективности больших языковых моделей (LLM) для разных языков. Авторы оценивают различные стратегии непрерывного предобучения (CPT) для улучшения многоязычности моделей. Результаты показывают, что двуязычное CPT улучшает классификацию, но вызывает проблемы смешивания языков при генерации. Включение программного кода в данные для CPT повышает точность многоязычной классификации, особенно для малоресурсных языков. Исследование выявляет сложные взаимодействия между языками при многоязычном обучении представлений.'}, 'en': {'title': 'Bridging the Gap: Enhancing Low-Resource Languages with Continual Pretraining', 'desc': 'This paper investigates how Large Language Models (LLMs) perform differently across various languages, especially highlighting the advantages for high-resource languages and the challenges faced by low-resource ones. It explores the effectiveness of Continual Pretraining (CPT) using different data strategies, including monolingual, bilingual, and code-augmented data. The study evaluates 36 configurations across multiple languages and reveals that bilingual CPT can enhance classification but may lead to language mixing, while code data improves accuracy for low-resource languages at the cost of generation quality. The findings also challenge previous assumptions about language classifications, showing that altruistic languages can harm related ones, and stagnant languages can adapt under certain conditions, highlighting the complexity of multilingual learning.'}, 'zh': {'title': '解决语言不平衡的持续预训练策略', 'desc': '大型语言模型（LLMs）在不同语言上的表现差异显著，主要使高资源语言受益，而边缘化了低资源语言。持续预训练（CPT）被认为是解决这一不平衡的有效方法，但单语、双语和代码增强数据策略的相对有效性尚不明确。研究评估了36种CPT配置，涵盖三种多语言基础模型，涉及30多种语言，揭示了多语言表示学习的复杂性。研究结果表明，双语CPT提高了多语言分类，但在生成时常导致语言混合问题，而编程代码数据的加入则提高了低资源语言的分类准确性，但略微降低了生成质量。'}}}, {'id': 'https://huggingface.co/papers/2504.03790', 'title': "Sample, Don't Search: Rethinking Test-Time Alignment for Language Models", 'url': 'https://huggingface.co/papers/2504.03790', 'abstract': 'Increasing test-time computation has emerged as a promising direction for improving language model performance, particularly in scenarios where model finetuning is impractical or impossible due to computational constraints or private model weights. However, existing test-time search methods using a reward model (RM) often degrade in quality as compute scales, due to the over-optimization of what are inherently imperfect reward proxies. We introduce QAlign, a new test-time alignment approach. As we scale test-time compute, QAlign converges to sampling from the optimal aligned distribution for each individual prompt. By adopting recent advances in Markov chain Monte Carlo for text generation, our method enables better-aligned outputs without modifying the underlying model or even requiring logit access. We demonstrate the effectiveness of QAlign on mathematical reasoning benchmarks (GSM8K and GSM-Symbolic) using a task-specific RM, showing consistent improvements over existing test-time compute methods like best-of-n and majority voting. Furthermore, when applied with more realistic RMs trained on the Tulu 3 preference dataset, QAlign outperforms direct preference optimization (DPO), best-of-n, majority voting, and weighted majority voting on a diverse range of datasets (GSM8K, MATH500, IFEval, MMLU-Redux, and TruthfulQA). A practical solution to aligning language models at test time using additional computation without degradation, our approach expands the limits of the capability that can be obtained from off-the-shelf language models without further training.', 'score': 0, 'issue_id': 3128, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 апреля', 'en': 'April 4', 'zh': '4月4日'}, 'hash': 'bbc79821089111ea', 'authors': ['Gonçalo Faria', 'Noah A. Smith'], 'affiliations': ['Allen Institute for AI', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2504.03790.jpg', 'data': {'categories': ['#alignment', '#math', '#training', '#optimization', '#benchmark', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'QAlign: Улучшение языковых моделей без переобучения', 'desc': 'Статья представляет новый подход к выравниванию языковых моделей во время тестирования, называемый QAlign. Этот метод позволяет улучшить качество выходных данных без изменения базовой модели или доступа к логитам. QAlign использует достижения в области цепей Маркова Монте-Карло для генерации текста и сходится к оптимальному выровненному распределению для каждого отдельного запроса. Эффективность QAlign продемонстрирована на различных наборах данных, включая задачи математических рассуждений и общие языковые задачи.'}, 'en': {'title': 'QAlign: Optimizing Language Model Outputs at Test Time', 'desc': 'This paper presents QAlign, a novel approach for enhancing language model performance during test time by optimizing the alignment of outputs without requiring model retraining. Traditional methods using reward models often suffer from quality degradation as computational resources increase, due to reliance on imperfect reward proxies. QAlign leverages recent advancements in Markov chain Monte Carlo techniques to sample from the optimal distribution for each prompt, leading to better-aligned text generation. The method shows significant improvements over existing test-time strategies on various benchmarks, demonstrating its effectiveness in maximizing the utility of pre-trained language models.'}, 'zh': {'title': 'QAlign：提升语言模型性能的新方法', 'desc': '本文提出了一种新的测试时间对齐方法QAlign，旨在提高语言模型的性能，尤其是在模型微调不可行的情况下。QAlign通过在测试时间计算中采样最优对齐分布，克服了现有奖励模型在计算规模扩大时质量下降的问题。该方法利用马尔可夫链蒙特卡洛技术生成文本，能够在不修改基础模型的情况下，生成更好对齐的输出。实验结果表明，QAlign在多个数学推理基准上表现优于现有的测试时间计算方法，展示了其在实际应用中的有效性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents', '#agi', '#alignment (3)', '#architecture (3)', '#audio', '#benchmark (11)', '#cv (6)', '#data (2)', '#dataset (7)', '#diffusion (2)', '#ethics (1)', '#games (1)', '#graphs', '#hallucinations', '#healthcare (1)', '#inference (5)', '#interpretability (2)', '#leakage', '#long_context (3)', '#low_resource (3)', '#machine_translation (1)', '#math (3)', '#multilingual (2)', '#multimodal (4)', '#open_source (2)', '#optimization (7)', '#plp', '#rag', '#reasoning (6)', '#rl (2)', '#rlhf (2)', '#robotics', '#science (1)', '#security (2)', '#small_models (2)', '#story_generation (1)', '#survey (1)', '#synthetic (1)', '#training (9)', '#transfer_learning (1)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-04-08 16:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-04-08 16:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-04-08 16:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    