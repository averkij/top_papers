
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 40 papers. February 9.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["–º–∏–Ω—É—Ç—É", "–º–∏–Ω—É—Ç—ã", "–º–∏–Ω—É—Ç"],
                hour: ["—á–∞—Å", "—á–∞—Å–∞", "—á–∞—Å–æ–≤"],
                day: ["–¥–µ–Ω—å", "–¥–Ω—è", "–¥–Ω–µ–π"],
                justNow: "—Ç–æ–ª—å–∫–æ —á—Ç–æ",
                ago: "–Ω–∞–∑–∞–¥"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["ÂàÜÈíü", "ÂàÜÈíü", "ÂàÜÈíü"],
                hour: ["Â∞èÊó∂", "Â∞èÊó∂", "Â∞èÊó∂"],
                day: ["Â§©", "Â§©", "Â§©"],
                justNow: "ÂàöÂàö",
                ago: "Ââç"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "—Å—Ç–∞—Ç–µ–π";
            } else if (lastDigit === 1) {
                word = "—Å—Ç–∞—Ç—å—è";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "—Å—Ç–∞—Ç—å–∏";
            } else {
                word = "—Å—Ç–∞—Ç–µ–π";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ÁØáËÆ∫Êñá"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">üî∫</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">9 —Ñ–µ–≤—Ä–∞–ª—è</span> | <span id="title-articles-count">40 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2026-02-06.html">‚¨ÖÔ∏è <span id="prev-date">06.02</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2026-02-10.html">‚û°Ô∏è <span id="next-date">10.02</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2026-02.html">üìà <span id='top-month-label'>–ú–µ—Å—è—Ü</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">üîÄ <span id="sort-label-text">–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">—Ä–µ–π—Ç–∏–Ω–≥—É</option>
                    <option value="pub_date">–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏</option>
                    <option value="issue_id">–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">üè∑Ô∏è –§–∏–ª—å—Ç—Ä</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A‚à™B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A‚à©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">üßπ</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ‚úñÔ∏è <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '9 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 9', 'zh': '2Êúà9Êó•'};
        let feedDateNext = {'ru': '10.02', 'en': '02/10', 'zh': '2Êúà10Êó•'};
        let feedDatePrev = {'ru': '06.02', 'en': '02/06', 'zh': '2Êúà6Êó•'};
        let filterLabel = {'ru': '–§–∏–ª—å—Ç—Ä', 'en': 'Topics', 'zh': '‰∏ªÈ¢òÁ≠õÈÄâ'}
        let publishedLabel = {'ru': '—Å—Ç–∞—Ç—å—è –æ—Ç ', 'en': 'published on ', 'zh': 'ÂèëË°®‰∫é'}
        let sortLabel = {'ru': '–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ', 'en': 'Sort by', 'zh': 'ÊéíÂ∫èÊñπÂºè'}
        let paperLabel = {'ru': '–°—Ç–∞—Ç—å—è', 'en': 'Paper', 'zh': 'ËÆ∫Êñá'}
        let topMonthLabel = {'ru': '–ú–µ—Å—è—Ü', 'en': 'Month', 'zh': 'ÊúàÂ∫¶ËÆ∫Êñá'}
        let topDayLabel = {'ru': '–î–µ–Ω—å', 'en': 'Day', 'zh': 'Êó•Â∫¶ËÆ∫Êñá'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2602.06570', 'title': 'Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making', 'url': 'https://huggingface.co/papers/2602.06570', 'abstract': 'Baichuan-M3 is a medical-enhanced large language model designed for clinical decision support with capabilities in proactive information gathering, long-horizon reasoning, and hallucination suppression.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.', 'score': 54, 'issue_id': 963, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': 'cb1f6a45692326d3', 'authors': ['Baichuan-M3 Team', ':', 'Chengfeng Dou', 'Fan Yang', 'Fei Li', 'Jiyuan Jia', 'Qiang Ju', 'Shuai Wang', 'Tianpeng Li', 'Xiangrong Zeng', 'Yijie Zhou', 'Hongda Zhang', 'Jinyang Tai', 'Linzhuang Sun', 'Peidong Guo', 'Yichuan Mo', 'Xiaochuan Wang', 'Hengfu Cui', 'Zhishou Zhang'], 'affiliations': ['Baichuan Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2602.06570.jpg', 'data': {'categories': ['#open_source', '#science', '#hallucinations', '#reasoning'], 'emoji': '‚öïÔ∏è', 'ru': {'title': '–û—Ç –ø–∞—Å—Å–∏–≤–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –∫ –∞–∫—Ç–∏–≤–Ω–æ–π –≤—Ä–∞—á–µ–±–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–µ', 'desc': 'Baichuan-M3 ‚Äî —ç—Ç–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è LLM, –∫–æ—Ç–æ—Ä–∞—è –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –ø–∞—Ä–∞–¥–∏–≥–º—É –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –ø–∞—Ü–∏–µ–Ω—Ç–∞–º–∏ –æ—Ç –ø–∞—Å—Å–∏–≤–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∫ –∞–∫—Ç–∏–≤–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–µ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –∫–æ–Ω–≤–µ–π–µ—Ä–µ, –º–æ–¥–µ–ª–∏—Ä—É—é—â–µ–º —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–∞–±–æ—á–∏–π –ø—Ä–æ—Ü–µ—Å—Å –≤—Ä–∞—á–∞, –∏ –≤–∫–ª—é—á–∞–µ—Ç —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏: –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω—ã–π —Å–±–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ—Å—Ç–∏, –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –≤ —Å–≤—è–∑–Ω—ã–π –¥–∏–∞–≥–Ω–æ–∑ –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é —Ñ–∞–∫—Ç-–æ—Ç—á—ë—Ç–Ω–æ—Å—Ç—å –∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö HealthBench, HealthBench-Hallu –∏ ScanBench, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ—à–µ–Ω–∏—è –≤ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è—Ö –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –ú–æ–¥–µ–ª—å –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞ –≤ –æ—Ç–∫—Ä—ã—Ç–æ–º –¥–æ—Å—Ç—É–ø–µ –∏ –¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö.'}, 'en': {'title': 'Revolutionizing Clinical Decision Support with Baichuan-M3', 'desc': 'Baichuan-M3 is a large language model specifically designed for medical applications, focusing on enhancing clinical decision-making. It moves beyond traditional question-answering by actively supporting healthcare professionals with proactive information gathering and long-term reasoning. The model effectively integrates various pieces of evidence to form coherent diagnoses while minimizing inaccuracies through adaptive hallucination suppression. Evaluations show that Baichuan-M3 outperforms existing models like GPT-5.2 in clinical tasks, making it a significant advancement in medical AI.'}, 'zh': {'title': '‰∏ªÂä®ÂÜ≥Á≠ñÊîØÊåÅÔºåË∂ÖË∂ä‰º†ÁªüÈóÆÁ≠î', 'desc': 'Baichuan-M3ÊòØ‰∏ÄÁßçÂåªÂ≠¶Â¢ûÂº∫ÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÊó®Âú®‰∏∫‰∏¥Â∫äÂÜ≥Á≠ñÊèê‰æõÊîØÊåÅ„ÄÇÂÆÉÁöÑ‰∏ªË¶ÅÁâπÁÇπÂåÖÊã¨‰∏ªÂä®‰ø°ÊÅØËé∑Âèñ„ÄÅÈïøÊó∂Èó¥Êé®ÁêÜÂíåÂπªËßâÊäëÂà∂ÔºåËÉΩÂ§üÊúâÊïàËß£ÂÜ≥Áé∞ÊúâÁ≥ªÁªüÂú®ÂºÄÊîæÂºèÂí®ËØ¢‰∏≠ÁöÑÂ±ÄÈôêÊÄß„ÄÇËØ•Ê®°ÂûãÈÄöËøá‰∏ìÈó®ÁöÑËÆ≠ÁªÉÊµÅÁ®ãÊ®°ÊãüÂåªÁîüÁöÑÁ≥ªÁªüÂ∑•‰ΩúÊµÅÁ®ãÔºåËÉΩÂ§üÂ∞ÜÂàÜÊï£ÁöÑËØÅÊçÆÊï¥Âêà‰∏∫ËøûË¥ØÁöÑËØäÊñ≠„ÄÇÂÆûËØÅËØÑ‰º∞Ë°®ÊòéÔºåBaichuan-M3Âú®HealthBenchÁ≠âÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÊòæËëóË∂ÖË∂ä‰∫ÜGPT-5.2„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.06717', 'title': "F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare", 'url': 'https://huggingface.co/papers/2602.06717', 'abstract': 'RLVR methods using group sampling suffer from bias toward likely trajectories and missed rare-correct ones; a difficulty-aware advantage scaling technique improves performance on benchmarks without increasing computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 rightarrow 70.3 (GRPO), 69.3 rightarrow 72.5 (DAPO), and 73.2 rightarrow 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.', 'score': 53, 'issue_id': 964, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': 'a897454d6b9b2de1', 'authors': ['Daniil Plyusov', 'Alexey Gorbatovski', 'Boris Shaposhnikov', 'Viacheslav Sinii', 'Alexey Malakhov', 'Daniil Gavrilov'], 'affiliations': ['Saint Petersburg Electrotechnical University LETI', 'T-Tech'], 'pdf_title_img': 'assets/pdf/title_img/2602.06717.jpg', 'data': {'categories': [], 'emoji': 'üéØ', 'ru': {'title': '–°–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ: –∫–∞–∫ –Ω–∞–π—Ç–∏ —Ä–µ–¥–∫–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è', 'desc': '–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —Å–º–µ—â–µ–Ω–∏—è –≤ –º–µ—Ç–æ–¥–∞—Ö –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è–º–∏ (RLVR), –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –≥—Ä—É–ø–ø–æ–≤—É—é –≤—ã–±–æ—Ä–∫—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–∏ –º–∞–ª—ã—Ö —Ä–∞–∑–º–µ—Ä–∞—Ö –≥—Ä—É–ø–ø –∞–ª–≥–æ—Ä–∏—Ç–º –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç —Ä–µ–¥–∫–∏–µ, –Ω–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω–∏—é –Ω–∞ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏—Ö—Å—è —Ä–µ—à–µ–Ω–∏—è—Ö. –û–Ω–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ç–µ—Ö–Ω–∏–∫–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤, –æ—Å–≤–µ–¥–æ–º–ª—ë–Ω–Ω—ã–µ –æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏ –∏ –≤–¥–æ—Ö–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ Focal loss, –∫–æ—Ç–æ—Ä—ã–µ —Å–Ω–∏–∂–∞—é—Ç –≤–µ—Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –Ω–∞ –ª–µ–≥–∫–æ —Ä–µ—à–∞–µ–º—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö. –ú–µ—Ç–æ–¥ –ª–µ–≥–∫–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±–µ–∑ —É–≤–µ–ª–∏—á–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç.'}, 'en': {'title': 'Enhancing RLVR Performance with Difficulty-Aware Scaling', 'desc': 'This paper addresses the limitations of Reinforcement Learning with Verifiable Rewards (RLVR) that arise from group sampling methods. These methods tend to favor common trajectories, leading to a bias that overlooks rare but correct solutions. The authors introduce a difficulty-aware advantage scaling technique that adjusts the learning process to focus on less frequent but valuable outcomes. Their approach enhances performance on various benchmarks without increasing computational demands, demonstrating significant improvements in success rates across different RLVR algorithms.'}, 'zh': {'title': '‰ºòÂåñÂº∫ÂåñÂ≠¶‰π†ÔºåÂÖãÊúçÂÅèÂ∑ÆÊåëÊàò', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂü∫‰∫éÁæ§‰ΩìÈááÊ†∑ÁöÑÂèØÈ™åËØÅÂ•ñÂä±Âº∫ÂåñÂ≠¶‰π†ÔºàRLVRÔºâÊñπÊ≥ïÁöÑÂÅèÂ∑ÆÈóÆÈ¢òÔºåÂ∞§ÂÖ∂ÊòØÂØπÂ∏∏ËßÅËΩ®ËøπÁöÑÂÅèÂêëÂíåÂØπÁ®ÄÊúâÊ≠£Á°ÆËΩ®ËøπÁöÑÈÅóÊºè„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåËæÉÂ§ßÁöÑÁæ§‰ΩìËßÑÊ®°‰ºöÂØºËá¥Â≠¶‰π†ÂÅèÂêë‰∫éÂ∑≤ÁªèÂèØËÉΩÁöÑËΩ®ËøπÔºåËÄåËæÉÂ∞èÁöÑÁæ§‰ΩìÂàôÂèØËÉΩÈîôËøáÁ®ÄÊúâÁöÑÊ≠£Á°ÆËΩ®Ëøπ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂõ∞ÈöæÊÑüÁü•ÁöÑ‰ºòÂäøÁº©ÊîæÊäÄÊúØÔºåÂèØ‰ª•ÊúâÊïàÊîπÂñÑÁÆóÊ≥ïÊÄßËÉΩÔºåËÄå‰∏çÂ¢ûÂä†ËÆ°ÁÆóÊàêÊú¨„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåËØ•ÊñπÊ≥ïÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊÄßËÉΩÔºåÂêåÊó∂‰øùÊåÅÊàñÊîπÂñÑ‰∫ÜÂÖ∂‰ªñÊåáÊ†á„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05843', 'title': 'OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions', 'url': 'https://huggingface.co/papers/2602.05843', 'abstract': "OdysseyArena presents a new framework for evaluating large language models on long-horizon, inductive agent tasks that emphasize autonomous discovery of environmental transition laws.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena", 'score': 51, 'issue_id': 963, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '22d2e90196a24045', 'authors': ['Fangzhi Xu', 'Hang Yan', 'Qiushi Sun', 'Jinyang Wu', 'Zixian Huang', 'Muye Huang', 'Jingyang Gong', 'Zichen Ding', 'Kanzhi Cheng', 'Yian Wang', 'Xinyu Che', 'Zeyi Sun', 'Jian Zhang', 'Zhangyue Yin', 'Haoran Luo', 'Xuanjing Huang', 'Ben Kao', 'Jun Liu', 'Qika Lin'], 'affiliations': ['Fudan University', 'Nanjing University', 'Nanyang Technological University', 'National University of Singapore', 'Shanghai AI Laboratory', 'The University of Hong Kong', 'Tsinghua University', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2602.05843.jpg', 'data': {'categories': ['#agents', '#benchmark', '#reasoning', '#long_context', '#open_source', '#dataset'], 'emoji': 'üß≠', 'ru': {'title': '–û—Ü–µ–Ω–∫–∞ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è –∑–∞–∫–æ–Ω–æ–≤ –ø—Ä–∏—Ä–æ–¥—ã –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è—Ö –∞–≥–µ–Ω—Ç–æ–≤', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω OdysseyArena ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Å –¥–ª–∏–Ω–Ω—ã–º –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–º –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è, –≥–¥–µ –∞–≥–µ–Ω—Ç—ã –¥–æ–ª–∂–Ω—ã —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –æ—Ç–∫—Ä—ã–≤–∞—Ç—å –∑–∞–∫–æ–Ω—ã –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã –∏–∑ –æ–ø—ã—Ç–∞. –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –æ—Ü–µ–Ω–∫–µ –∞–≥–µ–Ω—Ç–æ–≤ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω—ã –Ω–∞ –¥–µ–¥—É–∫—Ç–∏–≤–Ω–æ–π –ø–∞—Ä–∞–¥–∏–≥–º–µ, –∫–æ–≥–¥–∞ –∞–≥–µ–Ω—Ç—ã –≤—ã–ø–æ–ª–Ω—è—é—Ç —è–≤–Ω–æ –∑–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏, –æ–¥–Ω–∞–∫–æ –ø—Ä–µ–Ω–µ–±—Ä–µ–≥–∞—é—Ç –∏–Ω–¥—É–∫—Ç–∏–≤–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å—é –∫ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–º—É –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—é —Å–∫—Ä—ã—Ç—ã—Ö –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–µ–π. –ê–≤—Ç–æ—Ä—ã —Ñ–æ—Ä–º–∞–ª–∏–∑—É—é—Ç —á–µ—Ç—ã—Ä–µ –ø—Ä–∏–º–∏—Ç–∏–≤–∞ –ø–µ—Ä–µ—Ö–æ–¥–Ω—ã—Ö –¥–∏–Ω–∞–º–∏–∫ –∏ —Å–æ–∑–¥–∞—é—Ç –¥–≤–µ –≤–µ—Ä—Å–∏–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞: OdysseyArena-Lite —Å 120 –∑–∞–¥–∞—á–∞–º–∏ –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏–Ω–¥—É–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ OdysseyArena-Challenge –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è—Ö (–±–æ–ª–µ–µ 200 —à–∞–≥–æ–≤). –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ 15+ –ø–µ—Ä–µ–¥–æ–≤—ã—Ö LLM –ø–æ–∫–∞–∑–∞–ª–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ –≤ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∫ –∏–Ω–¥—É–∫—Ç–∏–≤–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –≤ —Å–ª–æ–∂–Ω—ã—Ö –æ–∫—Ä—É–∂–∞—é—â–∏—Ö —Å—Ä–µ–¥–∞—Ö.'}, 'en': {'title': 'Empowering Agents to Learn and Adapt Autonomously', 'desc': 'OdysseyArena introduces a new framework for assessing large language models (LLMs) in tasks that require long-term planning and the ability to learn from their environment. Unlike traditional evaluations that rely on fixed rules, this framework emphasizes the importance of inductive reasoning, allowing agents to discover underlying transition laws through experience. The framework includes OdysseyArena-Lite for standardized benchmarking with 120 tasks, and OdysseyArena-Challenge to evaluate agent performance over extended interactions. Experiments show that even advanced LLMs struggle with these inductive tasks, highlighting a significant challenge in developing autonomous agents capable of effective long-term decision-making.'}, 'zh': {'title': 'Ëá™‰∏ªÂèëÁé∞ÁöÑÊú™Êù•ÔºöOdysseyArenaÊ°ÜÊû∂', 'desc': 'OdysseyArenaÊòØ‰∏Ä‰∏™Êñ∞ÁöÑÊ°ÜÊû∂ÔºåÁî®‰∫éËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÈïøÊó∂Èó¥ËåÉÂõ¥ÂÜÖÁöÑËá™‰∏ªÂèëÁé∞ÁéØÂ¢ÉËΩ¨ÂèòËßÑÂæãÁöÑËÉΩÂäõ„ÄÇÁé∞ÊúâÁöÑËØÑ‰º∞ÊñπÊ≥ï‰∏ªË¶ÅÂü∫‰∫éÊºîÁªéÊé®ÁêÜÔºåÂøΩËßÜ‰∫Ü‰ª£ÁêÜ‰ªéÁªèÈ™å‰∏≠Ëá™‰∏ªÂèëÁé∞ÊΩúÂú®ËΩ¨ÂèòËßÑÂæãÁöÑÂøÖË¶ÅÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜOdysseyArenaÔºåÈáçÊñ∞ËÅöÁÑ¶‰∫éÈïøÊó∂Èó¥ËåÉÂõ¥ÂÜÖÁöÑ‰∏ªÂä®ÂíåÂΩíÁ∫≥‰∫§‰∫íÔºåÂª∫Á´ã‰∫ÜÂõõ‰∏™Âü∫Êú¨ÂéüÁêÜÔºåÂ∞ÜÊäΩË±°ÁöÑËΩ¨ÂèòÂä®ÊÄÅËΩ¨Âåñ‰∏∫ÂÖ∑‰ΩìÁöÑ‰∫§‰∫íÁéØÂ¢É„ÄÇÈÄöËøáÊ†áÂáÜÂåñÂü∫ÂáÜÊµãËØïOdysseyArena-LiteÔºåÊàë‰ª¨Êèê‰æõ‰∫Ü120‰∏™‰ªªÂä°Êù•Ë°°Èáè‰ª£ÁêÜÁöÑÂΩíÁ∫≥ÊïàÁéáÂíåÈïøÊó∂Èó¥ÂèëÁé∞ËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05027', 'title': 'AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders', 'url': 'https://huggingface.co/papers/2602.05027', 'abstract': "Sparse Autoencoders trained on Whisper and HuBERT models demonstrate stable feature extraction and effective disentanglement of acoustic and semantic information, showing practical applications in audio processing and correlation with human neural activity.  \t\t\t\t\tAI-generated summary \t\t\t\t Sparse Autoencoders (SAEs) are powerful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT, provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, including environmental noises and paralinguistic sounds (e.g. laughter, whispering) and disentangle them effectively, requiring removal of only 19-27% of features to erase a concept. Feature steering reduces Whisper's false speech detections by 70% with negligible WER increase, demonstrating real-world applicability. Finally, we find SAE features correlated with human EEG activity during speech perception, indicating alignment with human neural processing. The code and checkpoints are available at https://github.com/audiosae/audiosae_demo.", 'score': 49, 'issue_id': 965, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': 'ad429b923319d2b1', 'authors': ['Georgii Aparin', 'Tasnima Sadekova', 'Alexey Rukhovich', 'Assel Yermekova', 'Laida Kushnareva', 'Vadim Popov', 'Kristian Kuznetsov', 'Irina Piontkovskaya'], 'affiliations': ['Huawei Noahs Ark Lab'], 'pdf_title_img': 'assets/pdf/title_img/2602.05027.jpg', 'data': {'categories': ['#open_source', '#interpretability'], 'emoji': 'üëÇ', 'ru': {'title': '–†–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∏ —Ä–∞—Å–∫—Ä—ã–≤–∞—é—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–≤—É–∫–∞', 'desc': '–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∏ (SAE) –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –≤ –∞—É–¥–∏–æ–º–æ–¥–µ–ª—è—Ö Whisper –∏ HuBERT. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –±–æ–ª–µ–µ 50% –∏–∑—É—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ—Å—Ç–∞—é—Ç—Å—è —Å—Ç–∞–±–∏–ª—å–Ω—ã–º–∏ –ø—Ä–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–∏, –∏ SAE —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞–∑–¥–µ–ª—è—é—Ç –∞–∫—É—Å—Ç–∏—á–µ—Å–∫—É—é –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç—Å—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ –ø—É—Ç—ë–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏, —á—Ç–æ —Å–Ω–∏–∂–∞–µ—Ç –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è –Ω–∞ 70%. –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ SAE –∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –º–æ–∑–≥–∞ –≤–æ –≤—Ä–µ–º—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è —Ä–µ—á–∏, —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–æ–¥–µ–ª–∏ –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º –ø—Ä–æ—Ü–µ—Å—Å–∞–º.'}, 'en': {'title': 'Unlocking Audio Insights with Sparse Autoencoders', 'desc': 'This paper explores the use of Sparse Autoencoders (SAEs) for extracting features from audio data, specifically using the Whisper and HuBERT models. The authors demonstrate that SAEs can effectively disentangle acoustic and semantic information, maintaining stability and interpretability across different training conditions. They show that a significant portion of features remains consistent, allowing for the identification of specific audio events while minimizing the loss of important information. Additionally, the study reveals a strong correlation between SAE features and human neural activity, suggesting that these models align well with how humans process speech.'}, 'zh': {'title': 'Á®ÄÁñèËá™ÁºñÁ†ÅÂô®ÔºöÈü≥È¢ëÂ§ÑÁêÜÁöÑÊñ∞Â∑•ÂÖ∑', 'desc': 'Á®ÄÁñèËá™ÁºñÁ†ÅÂô®ÔºàSAEÔºâÊòØ‰∏ÄÁßçÂº∫Â§ßÁöÑÂ∑•ÂÖ∑ÔºåÁî®‰∫éËß£ÈáäÁ•ûÁªèË°®Á§∫Ôºå‰ΩÜÂú®Èü≥È¢ëÈ¢ÜÂüüÁöÑÂ∫îÁî®‰ªçÁÑ∂ËæÉÂ∞ë„ÄÇÊàë‰ª¨Âú®WhisperÂíåHuBERTÁöÑÊâÄÊúâÁºñÁ†ÅÂô®Â±Ç‰∏äËÆ≠ÁªÉSAEÔºåÂπ∂ÂØπÂÖ∂Á®≥ÂÆöÊÄßÂíåÂèØËß£ÈáäÊÄßËøõË°å‰∫ÜÂπøÊ≥õËØÑ‰º∞„ÄÇÁªìÊûúÊòæÁ§∫ÔºåË∂ÖËøá50%ÁöÑÁâπÂæÅÂú®ÈöèÊú∫ÁßçÂ≠ê‰πãÈó¥‰øùÊåÅ‰∏ÄËá¥ÔºåÈáçÂª∫Ë¥®ÈáèÂæó‰ª•‰øùÊåÅ„ÄÇSAEÁâπÂæÅÊúâÊïàÂú∞ÂàÜÁ¶ª‰∫ÜÂ£∞Â≠¶ÂíåËØ≠‰πâ‰ø°ÊÅØÔºåÂπ∂‰∏é‰∫∫Á±ªÂú®ËØ≠Èü≥ÊÑüÁü•ËøáÁ®ã‰∏≠ÁöÑËÑëÁîµÊ¥ªÂä®Áõ∏ÂÖ≥ËÅîÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Èü≥È¢ëÂ§ÑÁêÜ‰∏≠ÁöÑÂÆûÈôÖÂ∫îÁî®„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03392', 'title': 'On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models', 'url': 'https://huggingface.co/papers/2602.03392', 'abstract': 'The paper establishes a theoretical framework for analyzing entropy dynamics in reinforcement fine-tuning of large language models, deriving expressions for entropy change and proposing entropy control methods based on discriminant analysis.  \t\t\t\t\tAI-generated summary \t\t\t\t Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning (RFT), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update. This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.', 'score': 45, 'issue_id': 962, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': 'ddcd17135a2a2ca9', 'authors': ['Shumin Wang', 'Yuexiang Xie', 'Wenhao Zhang', 'Yuchang Sun', 'Yanxi Chen', 'Yaliang Li', 'Yanyong Zhang'], 'affiliations': ['Tongyi Lab, Alibaba Group', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2602.03392.jpg', 'data': {'categories': ['#rlhf', '#math', '#training', '#rl'], 'emoji': '‚öñÔ∏è', 'ru': {'title': '–¢–µ–æ—Ä–∏—è —ç–Ω—Ç—Ä–æ–ø–∏–∏ –≤ —É—Å–∏–ª–µ–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∏–Ω–∞–º–∏–∫–∏ —ç–Ω—Ç—Ä–æ–ø–∏–∏ –ø—Ä–∏ —É—Å–∏–ª–µ–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –≤—ã–≤–æ–¥—è—Ç –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è —ç–Ω—Ç—Ä–æ–ø–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞–Ω—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥—ã –∫–æ–Ω—Ç—Ä–æ–ª—è —ç–Ω—Ç—Ä–æ–ø–∏–∏. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∑–≤–æ–ª—è—é—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —ç–Ω—Ç—Ä–æ–ø–∏–π–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≤ –µ–¥–∏–Ω–æ–π –ø–∞—Ä–∞–¥–∏–≥–º–µ –∏ –≤–¥–æ—Ö–Ω–æ–≤–ª—è—é—Ç —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É –Ω–æ–≤—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏ –ø—Ä–∏ —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥–µ LLM.'}, 'en': {'title': 'Optimizing Exploration-Exploitation in Language Models through Entropy Dynamics', 'desc': 'This paper presents a theoretical framework for understanding how entropy changes during the reinforcement fine-tuning (RFT) of large language models (LLMs). It introduces a discriminant expression to quantify entropy change with each logit update, leading to a first-order expression that can be applied to Group Relative Policy Optimization (GRPO). The authors propose new entropy control methods based on their analysis, which help balance exploration and exploitation in model training. Empirical results validate their findings and demonstrate the effectiveness of the proposed entropy-discriminator clipping techniques.'}, 'zh': {'title': '‰ºòÂåñÊé¢Á¥¢‰∏éÂà©Áî®ÁöÑÁÜµÂä®ÊÄÅÂàÜÊûê', 'desc': 'Êú¨ÊñáÂª∫Á´ã‰∫Ü‰∏Ä‰∏™ÁêÜËÆ∫Ê°ÜÊû∂ÔºåÁî®‰∫éÂàÜÊûêÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Âº∫ÂåñÂæÆË∞ÉËøáÁ®ã‰∏≠ÁöÑÁÜµÂä®ÊÄÅ„ÄÇÁÜµÊòØË°°ÈáèÊ®°ÂûãËæìÂá∫Â§öÊ†∑ÊÄßÁöÑÈáçË¶ÅÊåáÊ†áÔºåËÉΩÂ§üÊèê‰æõÊ®°ÂûãÊé¢Á¥¢ËÉΩÂäõÁöÑÊ∑±ÂàªËßÅËß£„ÄÇÊàë‰ª¨Êé®ÂØº‰∫ÜÁÜµÂèòÂåñÁöÑË°®ËææÂºèÔºåÂπ∂ÊèêÂá∫‰∫ÜÂü∫‰∫éÂà§Âà´ÂàÜÊûêÁöÑÁÜµÊéßÂà∂ÊñπÊ≥ïÔºå‰ª•‰ºòÂåñÊé¢Á¥¢‰∏éÂà©Áî®ÁöÑÂπ≥Ë°°„ÄÇÈÄöËøáÂÆûËØÅÁ†îÁ©∂ÔºåÊàë‰ª¨È™åËØÅ‰∫ÜËøô‰∫õÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåÂπ∂‰∏∫Áé∞ÊúâÁ†îÁ©∂‰∏≠ÁöÑÁÜµÁõ∏ÂÖ≥ÊñπÊ≥ïÊèê‰æõ‰∫ÜÁªü‰∏ÄÁöÑËß£ÈáäËßÜËßí„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01734', 'title': 'MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration', 'url': 'https://huggingface.co/papers/2602.01734', 'abstract': 'Training instability in large language models is linked to weight matrix stable rank decline and Jacobian alignment, which MSign addresses through matrix sign operations to prevent gradient explosions.  \t\t\t\t\tAI-generated summary \t\t\t\t Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via ŒºP, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.', 'score': 29, 'issue_id': 963, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '3da9fd3fb00930a1', 'authors': ['Lianhai Ren', 'Yucheng Ding', 'Xiao Liu', 'Qianxiao Li', 'Peng Cheng', 'Yeyun Gong'], 'affiliations': ['Department of Mathematics, National University of Singapore', 'Microsoft SIGMA Team, Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2602.01734.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization'], 'emoji': '‚ö°', 'ru': {'title': 'MSign: —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–∞–Ω–≥–∞ –º–∞—Ç—Ä–∏—Ü', 'desc': '–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–æ—è–≤–ª—è–µ—Ç—Å—è –≤ –≤–∏–¥–µ –≤–∑—Ä—ã–≤–Ω–æ–≥–æ —Ä–æ—Å—Ç–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ –¥–≤–∞ –∫–ª—é—á–µ–≤—ã—Ö —è–≤–ª–µ–Ω–∏—è, –ø—Ä–µ–¥—à–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–ª–ª–∞–ø—Å—É: —Å–Ω–∏–∂–µ–Ω–∏–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ —Ä–∞–Ω–≥–∞ –º–∞—Ç—Ä–∏—Ü –≤–µ—Å–æ–≤ –∏ –≤–æ–∑—Ä–∞—Å—Ç–∞—é—â–µ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —è–∫–æ–±–∏–∞–Ω–æ–≤ —Å–æ—Å–µ–¥–Ω–∏—Ö —Å–ª–æ—ë–≤. –û–Ω–∏ –¥–æ–∫–∞–∑–∞–ª–∏ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏, —á—Ç–æ —ç—Ç–∏ —É—Å–ª–æ–≤–∏—è –≤–º–µ—Å—Ç–µ –≤—ã–∑—ã–≤–∞—é—Ç —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π —Ä–æ—Å—Ç –Ω–æ—Ä–º—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ —Å –≥–ª—É–±–∏–Ω–æ–π —Å–µ—Ç–∏. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä MSign, –∫–æ—Ç–æ—Ä—ã–π –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –æ–ø–µ—Ä–∞—Ü–∏–∏ –º–∞—Ç—Ä–∏—á–Ω–æ–≥–æ –∑–Ω–∞–∫–∞ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ —Ä–∞–Ω–≥–∞.'}, 'en': {'title': 'Stabilizing Training in Large Language Models with MSign', 'desc': 'This paper addresses the problem of training instability in large language models, which often leads to gradient explosions during pretraining. The authors identify two critical factors that contribute to this instability: a rapid decline in the stable rank of weight matrices and increased alignment of Jacobians between layers. They introduce a new optimizer called MSign, which uses matrix sign operations to maintain stable rank and prevent these issues. Experimental results show that MSign successfully mitigates training failures with minimal additional computational cost.'}, 'zh': {'title': 'Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÊÄßÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Âú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËÆ≠ÁªÉ‰∏≠ÔºåËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÊÄßÊòØ‰∏Ä‰∏™ÈáçË¶ÅÊåëÊàòÔºåÂ∏∏Â∏∏Ë°®Áé∞‰∏∫Á™ÅÂèëÁöÑÊ¢ØÂ∫¶ÁàÜÁÇ∏ÔºåÊµ™Ë¥πÂ§ßÈáèËÆ°ÁÆóËµÑÊ∫ê„ÄÇÊàë‰ª¨Á†îÁ©∂‰∫Ü‰∏Ä‰∏™5MÂèÇÊï∞ÁöÑNanoGPTÊ®°ÂûãÁöÑËÆ≠ÁªÉÂ§±Ë¥•ÔºåÂèëÁé∞‰∫ÜÂØºËá¥Â¥©Ê∫ÉÁöÑ‰∏§‰∏™ÂÖ≥ÈîÆÁé∞Ë±°ÔºöÊùÉÈáçÁü©ÈòµÁ®≥ÂÆöÁß©ÁöÑÂø´ÈÄü‰∏ãÈôçÂíåÁõ∏ÈÇªÂ±ÇÈõÖÂèØÊØîÁü©Èòµ‰πãÈó¥ÁöÑÂØπÈΩêÂ¢ûÂä†„ÄÇÊàë‰ª¨ÁêÜËÆ∫ËØÅÊòéËøô‰∏§‰∏™Êù°‰ª∂ÂÖ±ÂêåÂØºËá¥‰∫ÜÈöèÁùÄÁΩëÁªúÊ∑±Â∫¶ÁöÑÂ¢ûÂä†ÔºåÊ¢ØÂ∫¶ËåÉÊï∞ÁöÑÊåáÊï∞Â¢ûÈïø„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏Ä‰∏çÁ®≥ÂÆöÊú∫Âà∂ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜMSignÔºå‰∏ÄÁßçÊñ∞ÁöÑ‰ºòÂåñÂô®ÔºåÈÄöËøáÂë®ÊúüÊÄßÂú∞Â∫îÁî®Áü©ÈòµÁ¨¶Âè∑Êìç‰ΩúÊù•ÊÅ¢Â§çÁ®≥ÂÆöÁß©„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.18415', 'title': 'Pisets: A Robust Speech Recognition System for Lectures and Interviews', 'url': 'https://huggingface.co/papers/2601.18415', 'abstract': 'A three-component speech-to-text system combines Wav2Vec2, AST, and Whisper models with curriculum learning and uncertainty modeling to improve transcription accuracy and reduce hallucinations in Russian-language speech.  \t\t\t\t\tAI-generated summary \t\t\t\t This work presents a speech-to-text system "Pisets" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system\'s effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of "Pisets" system is publicly available at GitHub: https://github.com/bond005/pisets.', 'score': 29, 'issue_id': 970, 'pub_date': '2026-01-26', 'pub_date_card': {'ru': '26 —è–Ω–≤–∞—Ä—è', 'en': 'January 26', 'zh': '1Êúà26Êó•'}, 'hash': '7d9a13b8554dfbda', 'authors': ['Ivan Bondarenko', 'Daniil Grebenkin', 'Oleg Sedukhin', 'Mikhail Klementev', 'Roman Derunets', 'Lyudmila Budneva'], 'affiliations': ['Novosibirsk State University', 'Siberian Neuronets LLC'], 'pdf_title_img': 'assets/pdf/title_img/2601.18415.jpg', 'data': {'categories': ['#multilingual', '#low_resource', '#architecture', '#open_source', '#training', '#audio', '#hallucinations'], 'emoji': 'üé§', 'ru': {'title': '–ê–Ω—Å–∞–º–±–ª—å –º–æ–¥–µ–ª–µ–π –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä—É—Å—Å–∫–æ–π —Ä–µ—á–∏ –±–µ–∑ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ ¬´Pisets¬ª, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ —Ç—Ä—ë—Ö–∫–æ–º–ø–æ–Ω–µ–Ω—Ç–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –º–æ–¥–µ–ª–∏ Wav2Vec2, AST –∏ Whisper –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–π —Ä–µ—á–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏ –º–µ—Ç–æ–¥—ã curriculum learning –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∫–æ—Ä–ø—É—Å—ã —Ä—É—Å—Å–∫–æ–π —Ä–µ—á–∏ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –ø–µ—Ä–≤–∏—á–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ Wav2Vec2, —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ª–æ–∂–Ω–æ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é Audio Spectrogram Transformer –∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ Whisper. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏–∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏ –ø–æ–∑–≤–æ–ª–∏–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∑–∏—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –∏ —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∞—É–¥–∏–æ–∑–∞–ø–∏—Å—è—Ö.'}, 'en': {'title': "Enhancing Russian Speech Recognition with 'Pisets' System", 'desc': "This paper introduces a speech-to-text system called 'Pisets' that enhances transcription accuracy for Russian-language audio. It employs a three-component architecture consisting of Wav2Vec2 for initial recognition, AST for filtering false positives, and Whisper for final transcription. The system incorporates curriculum learning and diverse speech corpora to improve performance and reduce errors, particularly hallucinations from the Whisper model. Additionally, advanced uncertainty modeling techniques are applied to ensure high-quality transcriptions across various acoustic conditions."}, 'zh': {'title': 'ÊèêÂçá‰øÑËØ≠ËØ≠Èü≥ËØÜÂà´ÁöÑÂáÜÁ°ÆÊÄß‰∏éÂèØÈù†ÊÄß', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫"Pisets"ÁöÑ‰∏âÁªÑ‰ª∂ËØ≠Èü≥ËΩ¨ÊñáÊú¨Á≥ªÁªüÔºåÊó®Âú®ÊèêÈ´ò‰øÑËØ≠ËØ≠Èü≥ËØÜÂà´ÁöÑÂáÜÁ°ÆÊÄßÂπ∂ÂáèÂ∞ëÈîôËØØÂíåÂπªËßâ„ÄÇËØ•Á≥ªÁªüÁªìÂêà‰∫ÜWav2Vec2ËøõË°åÂàùÊ≠•ËØÜÂà´Ôºå‰ΩøÁî®Èü≥È¢ëË∞±ÂõæÂèòÊç¢Âô®ÔºàASTÔºâËøõË°åÂÅáÈò≥ÊÄßËøáÊª§ÔºåÊúÄÂêéÈÄöËøáWhisperËøõË°åËØ≠Èü≥ËØÜÂà´„ÄÇÈÄöËøáÂÆûÊñΩËØæÁ®ãÂ≠¶‰π†ÊñπÊ≥ïÂíåÂà©Áî®Â§öÊ†∑ÁöÑ‰øÑËØ≠ËØ≠Èü≥ËØ≠ÊñôÂ∫ìÔºåÊòæËëóÊèêÂçá‰∫ÜÁ≥ªÁªüÁöÑÊïàÊûú„ÄÇÊ≠§Â§ñÔºåÂºïÂÖ•ÁöÑÂÖàËøõ‰∏çÁ°ÆÂÆöÊÄßÂª∫Ê®°ÊäÄÊúØËøõ‰∏ÄÊ≠•ÊîπÂñÑ‰∫ÜËΩ¨ÂΩïË¥®Èáè„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.06949', 'title': 'DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos', 'url': 'https://huggingface.co/papers/2602.06949', 'abstract': 'DreamDojo is a foundation world model trained on 44k hours of egocentric human videos that enables efficient simulation of dexterous robotic tasks through continuous latent actions and real-time distillation.  \t\t\t\t\tAI-generated summary \t\t\t\t Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.', 'score': 21, 'issue_id': 962, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '4b7549558bf19284', 'authors': ['Shenyuan Gao', 'William Liang', 'Kaiyuan Zheng', 'Ayaan Malik', 'Seonghyeon Ye', 'Sihyun Yu', 'Wei-Cheng Tseng', 'Yuzhu Dong', 'Kaichun Mo', 'Chen-Hsuan Lin', 'Qianli Ma', 'Seungjun Nah', 'Loic Magne', 'Jiannan Xiang', 'Yuqi Xie', 'Ruijie Zheng', 'Dantong Niu', 'You Liang Tan', 'K. R. Zentner', 'George Kurian', 'Suneel Indupuru', 'Pooya Jannaty', 'Jinwei Gu', 'Jun Zhang', 'Jitendra Malik', 'Pieter Abbeel', 'Ming-Yu Liu', 'Yuke Zhu', 'Joel Jang', 'Linxi "Jim" Fan'], 'affiliations': ['HKUST', 'KAIST', 'NVIDIA', 'Stanford', 'UC Berkeley', 'UCSD', 'UT Austin', 'UW'], 'pdf_title_img': 'assets/pdf/title_img/2602.06949.jpg', 'data': {'categories': ['#inference', '#open_source', '#synthetic', '#transfer_learning', '#video', '#training', '#robotics', '#dataset'], 'emoji': 'ü§ñ', 'ru': {'title': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è –ª–æ–≤–∫–æ–≥–æ —Ä–æ–±–æ—É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–∑ –≤–∏–¥–µ–æ —á–µ–ª–æ–≤–µ–∫–∞', 'desc': 'DreamDojo ‚Äî —ç—Ç–æ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ 44 —Ç—ã—Å—è—á–∞—Ö —á–∞—Å–æ–≤ –≤–∏–¥–µ–æ –æ—Ç –ø–µ—Ä–≤–æ–≥–æ –ª–∏—Ü–∞, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ —Å–∫—Ä—ã—Ç—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –µ–¥–∏–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –∑–Ω–∞–Ω–∏–π –∏–∑ –Ω–µ–º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ, —Ä–µ—à–∞—è –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Ö–≤–∞—Ç–∫–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –¥–µ–π—Å—Ç–≤–∏–π. –ü–æ—Å–ª–µ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –º–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (10.81 FPS) –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ç–æ—á–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ñ–∏–∑–∏–∫–∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏. –°–∏—Å—Ç–µ–º–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞ –¥–ª—è —Ç–µ–ª–µ–æ–ø–µ—Ä–∞—Ü–∏–∏, –æ—Ü–µ–Ω–∫–∏ –ø–æ–ª–∏—Ç–∏–∫ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è, –ø–æ–∫–∞–∑—ã–≤–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö out-of-distribution –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö.'}, 'en': {'title': 'Revolutionizing Robotics with DreamDojo: A New Era of Simulation and Control', 'desc': 'DreamDojo is a foundational world model that leverages 44,000 hours of egocentric human videos to simulate complex robotic tasks effectively. It addresses the challenges of limited data and scarce action labels by introducing continuous latent actions, which serve as unified proxy actions for better knowledge transfer. After fine-tuning on specific robot data, DreamDojo exhibits strong physics understanding and precise control capabilities. This model not only accelerates real-time performance but also enhances context consistency, making it suitable for various applications like teleoperation and model-based planning.'}, 'zh': {'title': 'DreamDojoÔºöÁÅµÂ∑ßÊú∫Âô®‰∫∫‰ªªÂä°ÁöÑÊú™Êù•Ê®°ÊãüÂô®', 'desc': 'DreamDojoÊòØ‰∏Ä‰∏™Âü∫Á°Ä‰∏ñÁïåÊ®°ÂûãÔºåÂà©Áî®44000Â∞èÊó∂ÁöÑËá™Êàë‰∏≠ÂøÉ‰∫∫Á±ªËßÜÈ¢ëËøõË°åËÆ≠ÁªÉÔºåËÉΩÂ§üÈ´òÊïàÊ®°ÊãüÁÅµÂ∑ßÊú∫Âô®‰∫∫‰ªªÂä°„ÄÇËØ•Ê®°ÂûãÈÄöËøáÂºïÂÖ•ËøûÁª≠ÊΩúÂú®Âä®‰Ωú‰Ωú‰∏∫Áªü‰∏ÄÁöÑ‰ª£ÁêÜÂä®‰ΩúÔºåËß£ÂÜ≥‰∫ÜÂä®‰ΩúÊ†áÁ≠æÁ®ÄÁº∫ÁöÑÈóÆÈ¢òÔºå‰ªéËÄåÂ¢ûÂº∫‰∫Ü‰ªéÊú™Ê†áËÆ∞ËßÜÈ¢ë‰∏≠ËΩ¨Áßª‰∫§‰∫íÁü•ËØÜÁöÑËÉΩÂäõ„ÄÇÁªèËøáÂ∞èËßÑÊ®°ÁõÆÊ†áÊú∫Âô®‰∫∫Êï∞ÊçÆÁöÑÂêéËÆ≠ÁªÉÔºåDreamDojoÂ±ïÁé∞Âá∫ÂØπÁâ©ÁêÜÁöÑÂº∫ÁêÜËß£ÂíåÁ≤æÁ°ÆÁöÑÂä®‰ΩúÊéßÂà∂ËÉΩÂäõ„ÄÇÊàë‰ª¨ÁöÑÂ∑•‰Ωú‰∏∫Âü∫‰∫éÁîüÊàê‰∏ñÁïåÊ®°ÂûãÁöÑÈáçË¶ÅÂ∫îÁî®Â•†ÂÆö‰∫ÜÂü∫Á°ÄÔºåÂåÖÊã¨ÂÆûÊó∂ÈÅ•Êìç‰Ωú„ÄÅÁ≠ñÁï•ËØÑ‰º∞ÂíåÂü∫‰∫éÊ®°ÂûãÁöÑËßÑÂàí„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.06130', 'title': 'Self-Improving World Modelling with Latent Actions', 'url': 'https://huggingface.co/papers/2602.06130', 'abstract': "SWIRL is a self-improvement framework that learns world models from state-only sequences by alternating between forward and inverse dynamics modeling with variational information maximization and ELBO maximization, achieving improved performance on various reasoning and planning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Internal modelling of the world -- predicting transitions between previous states X and next states Y under actions Z -- is essential to reasoning and planning for LLMs and VLMs. Learning such models typically requires costly action-labelled trajectories. We propose SWIRL, a self-improvement framework that learns from state-only sequences by treating actions as a latent variable and alternating between Forward World Modelling (FWM) P_Œ∏(Y|X,Z) and an Inverse Dynamics Modelling (IDM) Q_œÜ(Z|X,Y). SWIRL iterates two phases: (1) Variational Information Maximisation, which updates the FWM to generate next states that maximise conditional mutual information with latent actions given prior states, encouraging identifiable consistency; and (2) ELBO Maximisation, which updates the IDM to explain observed transitions, effectively performing coordinate ascent. Both models are trained with reinforcement learning (specifically, GRPO) with the opposite frozen model's log-probability as a reward signal. We provide theoretical learnability guarantees for both updates, and evaluate SWIRL on LLMs and VLMs across multiple environments: single-turn and multi-turn open-world visual dynamics and synthetic textual environments for physics, web, and tool calling. SWIRL achieves gains of 16% on AURORABench, 28% on ByteMorph, 16% on WorldPredictionBench, and 14% on StableToolBench.", 'score': 19, 'issue_id': 969, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '296eebd2c8944113', 'authors': ['Yifu Qiu', 'Zheng Zhao', 'Waylon Li', 'Yftah Ziser', 'Anna Korhonen', 'Shay B. Cohen', 'Edoardo M. Ponti'], 'affiliations': ['Nvidia Research', 'University of Cambridge', 'University of Edinburgh', 'University of Groningen'], 'pdf_title_img': 'assets/pdf/title_img/2602.06130.jpg', 'data': {'categories': ['#reasoning', '#rl', '#benchmark', '#multimodal', '#optimization', '#training'], 'emoji': 'üîÑ', 'ru': {'title': '–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –º–∏—Ä–∞ —á–µ—Ä–µ–∑ —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä—è–º–æ–π –∏ –æ–±—Ä–∞—Ç–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–∏', 'desc': 'SWIRL ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –∏–∑—É—á–∞–µ—Ç –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞ –∏–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Ç–æ–ª—å–∫–æ —Å–æ—Å—Ç–æ—è–Ω–∏–π, —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—è –¥–µ–π—Å—Ç–≤–∏—è –∫–∞–∫ —Å–∫—Ä—ã—Ç—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é. –ú–µ—Ç–æ–¥ —á–µ—Ä–µ–¥—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä—è–º–æ–π –¥–∏–Ω–∞–º–∏–∫–∏ (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è) –∏ –º–æ–¥–µ–ª–∏ –æ–±—Ä–∞—Ç–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–∏ (–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–æ–π –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ ELBO. –û–±–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –æ–±—É—á–∞—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é reinforcement learning, –≥–¥–µ –∫–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–æ–≥–∞—Ä–∏—Ñ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥—Ä—É–≥–æ–π –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∫–∞–∫ —Å–∏–≥–Ω–∞–ª –Ω–∞–≥—Ä–∞–¥—ã. –§—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ LLM –∏ VLM, –Ω–∞–±–∏—Ä–∞—è –æ—Ç 14% –¥–æ 28% –ø—Ä–∏—Ä–æ—Å—Ç–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö.'}, 'en': {'title': 'SWIRL: Learning World Models Without Action Labels', 'desc': 'SWIRL is a novel self-improvement framework designed to learn world models using only state sequences, without the need for action-labeled data. It alternates between two key processes: Forward World Modeling (FWM) and Inverse Dynamics Modeling (IDM), treating actions as latent variables to enhance learning efficiency. The framework employs Variational Information Maximization and ELBO Maximization to iteratively refine its models, ensuring that the generated states are consistent with the latent actions. SWIRL demonstrates significant performance improvements on various reasoning and planning benchmarks, showcasing its effectiveness in both language and vision models.'}, 'zh': {'title': 'SWIRLÔºöÈÄöËøáÁä∂ÊÄÅÂ∫èÂàóËá™ÊàëÊîπËøõÁöÑ‰∏ñÁïåÊ®°ÂûãÂ≠¶‰π†', 'desc': 'SWIRLÊòØ‰∏Ä‰∏™Ëá™ÊàëÊîπËøõÊ°ÜÊû∂ÔºåÈÄöËøá‰ªÖ‰ΩøÁî®Áä∂ÊÄÅÂ∫èÂàóÂ≠¶‰π†‰∏ñÁïåÊ®°Âûã„ÄÇÂÆÉ‰∫§ÊõøËøõË°åÂâçÂêë‰∏ñÁïåÂª∫Ê®°ÂíåÈÄÜÂêëÂä®ÊÄÅÂª∫Ê®°ÔºåÂà©Áî®ÂèòÂàÜ‰ø°ÊÅØÊúÄÂ§ßÂåñÂíåELBOÊúÄÂ§ßÂåñÊù•ÊèêÈ´òÊÄßËÉΩ„ÄÇSWIRLÂú®Â§ö‰∏™Êé®ÁêÜÂíåËßÑÂàíÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ÑÁêÜÂºÄÊîæ‰∏ñÁïåËßÜËßâÂä®ÊÄÅÂíåÂêàÊàêÊñáÊú¨ÁéØÂ¢ÉÊó∂„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ËøõË°åËÆ≠ÁªÉÔºåÁ°Æ‰øù‰∫ÜÊ®°ÂûãÁöÑÂèØÂ≠¶‰π†ÊÄßÂíåÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.06291', 'title': 'Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math', 'url': 'https://huggingface.co/papers/2602.06291', 'abstract': 'Consequence-Based Utility evaluates mathematical solutions by testing their effectiveness as exemplars for related problems, outperforming reward models and LLM judges in ranking quality and correct-wrong separation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose Consequence-Based Utility, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models, generative reward models, and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap, maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.', 'score': 16, 'issue_id': 963, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '35fbd37e3ab0ae57', 'authors': ['Guijin Son', 'Donghun Yang', 'Hitesh Laxmichand Patel', 'Hyunwoo Ko', 'Amit Agarwal', 'Sunghee Ahn', 'Kyong-Ha Lee', 'Youngjae Yu'], 'affiliations': ['OnelineAI', 'Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2602.06291.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#science', '#math', '#dataset'], 'emoji': 'üßÆ', 'ru': {'title': '–û—Ü–µ–Ω–∫–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∏—Ö –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫—É—é –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å –Ω–∞ —Å–æ—Å–µ–¥–Ω–∏—Ö –∑–∞–¥–∞—á–∞—Ö', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Consequence-Based Utility –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π –±–µ–∑ –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤. –í–º–µ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö reward –º–æ–¥–µ–ª–µ–π –∏–ª–∏ —Å—É–¥–µ–π—Å—Ç–≤–∞ —á–µ—Ä–µ–∑ LLM, –º–µ—Ç–æ–¥ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –¥–∞–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –ø—Ä–∏–º–µ—Ä –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (in-context exemplar) –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–º —É—Ä–æ–≤–Ω–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –æ—Ü–µ–Ω–∫–∏ –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—é –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π. –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–º–æ–≥–∞–µ—Ç —Ä–µ—à–∞—Ç—å –ø–æ—Ö–æ–∂–∏–µ –∑–∞–¥–∞—á–∏ –ª—É—á—à–µ, —á–µ–º –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è.'}, 'en': {'title': 'Evaluating Math Solutions with Consequence-Based Utility', 'desc': 'This paper introduces Consequence-Based Utility, a new method for evaluating mathematical solutions by assessing their effectiveness as examples for similar problems. Unlike traditional reward models and LLM judges, this approach does not rely on pre-defined rewards but instead tests how well a solution can help solve related questions. The authors demonstrate that their method significantly improves the ranking quality of solutions, achieving higher accuracy and area under the curve (AUC) scores compared to existing models. This innovation addresses the challenge of verifying complex mathematical solutions while reducing the burden on expert evaluators.'}, 'zh': {'title': 'Âü∫‰∫éÂêéÊûúÁöÑÊïàÁî®ÔºöÊèêÂçáÊï∞Â≠¶Ëß£Á≠îËØÑ‰º∞ÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËØÑ‰º∞ÊñπÊ≥ïÔºåÁß∞‰∏∫Âü∫‰∫éÂêéÊûúÁöÑÊïàÁî®ÔºàConsequence-Based UtilityÔºâÔºåÁî®‰∫éËØÑ‰º∞Êï∞Â≠¶ÈóÆÈ¢òÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÊµãËØïÂÄôÈÄâËß£Âú®Ëß£ÂÜ≥Áõ∏ÂÖ≥ÂèØÈ™åËØÅÈóÆÈ¢ò‰∏≠ÁöÑË°®Áé∞ÔºåÊù•Âà§Êñ≠ÂÖ∂‰Ωú‰∏∫Á§∫‰æãÁöÑ‰ª∑ÂÄº„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåËøôÁßçÊñπÊ≥ïÂú®ÊéíÂêçË¥®Èáè‰∏ä‰ºò‰∫é‰º†ÁªüÁöÑÂ•ñÂä±Ê®°ÂûãÂíåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËØÑ‰º∞ËÄÖ„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÂü∫‰∫éÂêéÊûúÁöÑÊïàÁî®Âú®Â§ö‰∏™Á†îÁ©∂Á∫ßÊï∞Â≠¶ÈóÆÈ¢ò‰∏äÊòæÁ§∫Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê≠£Á°Æ‰∏éÈîôËØØÁöÑÂå∫ÂàÜ‰∏äË°®Áé∞Êõ¥‰∏∫Âá∫Ëâ≤„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05940', 'title': 'Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training', 'url': 'https://huggingface.co/papers/2602.05940', 'abstract': 'TRIT framework improves multilingual reasoning by jointly training translation and reasoning components, enhancing question understanding and response generation across languages.  \t\t\t\t\tAI-generated summary \t\t\t\t Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200.', 'score': 16, 'issue_id': 962, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '370ca1f731ec277b', 'authors': ['Junxiao Liu', 'Zhijun Wang', 'Yixiao Li', 'Zhejian Lai', 'Liqian Huang', 'Xin Huang', 'Xue Han', 'Junlan Feng', 'Shujian Huang'], 'affiliations': ['China Mobile Communications Company Limited Research Institute', 'National Key Laboratory for Novel Software Technology, Nanjing University', 'University of T√ºbingen'], 'pdf_title_img': 'assets/pdf/title_img/2602.05940.jpg', 'data': {'categories': ['#machine_translation', '#transfer_learning', '#multilingual', '#training', '#reasoning'], 'emoji': 'üåç', 'ru': {'title': '–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ TRIT, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –ø—É—Ç—ë–º —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –ø–µ—Ä–µ–≤–æ–¥–∞ –∏ –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–≤–æ–¥–∞. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏, —á—Ç–æ –¥–ª–∏–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–∞—Å—Ç–æ –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞—é—Ç —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ –¥–∞–∂–µ –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–∞ –¥—Ä—É–≥–∏—Ö —è–∑—ã–∫–∞—Ö, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Å–Ω–∏–∂–µ–Ω–∏—é —Ç–æ—á–Ω–æ—Å—Ç–∏. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Å–∞–º–æ—É–ª—É—á—à–∞—é—â–µ–≥–æ—Å—è –æ–±—É—á–µ–Ω–∏—è –ø–æ–≤—ã—à–∞–µ—Ç –∫–∞–∫ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤, —Ç–∞–∫ –∏ –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–Ω–µ—à–Ω–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –∏–ª–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ ‚Äî –≤ —Å—Ä–µ–¥–Ω–µ–º –Ω–∞ 7 –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã—Ö –ø—É–Ω–∫—Ç–∞ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ MMATH —Å –∑–∞–º–µ—Ç–Ω—ã–º –ø–æ–≤—ã—à–µ–Ω–∏–µ–º –∫—Ä–æ—Å—Å-–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤.'}, 'en': {'title': 'Enhancing Multilingual Reasoning with Integrated Translation Training', 'desc': 'The TRIT framework enhances multilingual reasoning by combining translation and reasoning training. This approach addresses the challenges of understanding and responding to questions in different languages, which often leads to inaccuracies. By integrating translation into the reasoning process, TRIT improves both the accuracy of answers and the consistency of language used. The framework shows significant performance improvements on multilingual tasks, demonstrating its effectiveness without needing extra data or feedback.'}, 'zh': {'title': 'TRITÊ°ÜÊû∂ÔºöÊèêÂçáÂ§öËØ≠Ë®ÄÊé®ÁêÜÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'TRITÊ°ÜÊû∂ÈÄöËøáËÅîÂêàËÆ≠ÁªÉÁøªËØëÂíåÊé®ÁêÜÁªÑ‰ª∂ÔºåÊèêÂçá‰∫ÜÂ§öËØ≠Ë®ÄÊé®ÁêÜËÉΩÂäõ„ÄÇËØ•ÊñπÊ≥ïËß£ÂÜ≥‰∫ÜÂ§öËØ≠Ë®ÄÈóÆÈ¢òÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÈÅøÂÖç‰∫ÜÂú®ÈùûËã±ËØ≠ÈóÆÈ¢ò‰∏äÊé®ÁêÜÊó∂ÁöÑÂáÜÁ°ÆÊÄß‰∏ãÈôç„ÄÇTRITÊ°ÜÊû∂Êó†ÈúÄÂ§ñÈÉ®ÂèçÈ¶àÊàñÈ¢ùÂ§ñÁöÑÂ§öËØ≠Ë®ÄÊï∞ÊçÆÔºåËÉΩÂ§üËá™ÊàëÊèêÂçáÂ§öËØ≠Ë®ÄÈóÆÈ¢òÁêÜËß£ÂíåÂõûÁ≠îÁîüÊàêÁöÑÊïàÊûú„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåTRITÂú®MMATHÊï∞ÊçÆÈõÜ‰∏äÂπ≥ÂùáÊèêÈ´ò‰∫Ü7‰∏™ÁôæÂàÜÁÇπÔºåÊòæËëóÊîπÂñÑ‰∫ÜÁ≠îÊ°àÁöÑÊ≠£Á°ÆÊÄßÂíåËØ≠Ë®Ä‰∏ÄËá¥ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.06391', 'title': 'POINTS-GUI-G: GUI-Grounding Journey', 'url': 'https://huggingface.co/papers/2602.06391', 'abstract': "GUI agents for automated digital tasks rely on vision-language models with enhanced grounding capabilities, achieved through refined data engineering, improved training strategies, and reinforcement learning with verifiable rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of vision-language models has catalyzed the emergence of GUI agents, which hold immense potential for automating complex tasks, from online shopping to flight booking, thereby alleviating the burden of repetitive digital workflows. As a foundational capability, GUI grounding is typically established as a prerequisite for end-to-end task execution. It enables models to precisely locate interface elements, such as text and icons, to perform accurate operations like clicking and typing. Unlike prior works that fine-tune models already possessing strong spatial awareness (e.g., Qwen3-VL), we aim to master the full technical pipeline by starting from a base model with minimal grounding ability, such as POINTS-1.5. We introduce POINTS-GUI-G-8B, which achieves state-of-the-art performance with scores of 59.9 on ScreenSpot-Pro, 66.0 on OSWorld-G, 95.7 on ScreenSpot-v2, and 49.9 on UI-Vision. Our model's success is driven by three key factors: (1) Refined Data Engineering, involving the unification of diverse open-source datasets format alongside sophisticated strategies for augmentation, filtering, and difficulty grading; (2) Improved Training Strategies, including continuous fine-tuning of the vision encoder to enhance perceptual accuracy and maintaining resolution consistency between training and inference; and (3) Reinforcement Learning (RL) with Verifiable Rewards. While RL is traditionally used to bolster reasoning, we demonstrate that it significantly improves precision in the perception-intensive GUI grounding task. Furthermore, GUI grounding provides a natural advantage for RL, as rewards are easily verifiable and highly accurate.", 'score': 14, 'issue_id': 962, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '1ce7e27df005ead5', 'authors': ['Zhongyin Zhao', 'Yuan Liu', 'Yikun Liu', 'Haicheng Wang', 'Le Tian', 'Xiao Zhou', 'Yangxiu You', 'Zilin Yu', 'Yang Yu', 'Jie Zhou'], 'affiliations': ['WeChat AI'], 'pdf_title_img': 'assets/pdf/title_img/2602.06391.jpg', 'data': {'categories': ['#multimodal', '#rl', '#benchmark', '#agents', '#training', '#cv', '#data'], 'emoji': 'üñ±Ô∏è', 'ru': {'title': '–¢–æ—á–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ GUI —á–µ—Ä–µ–∑ –∏–Ω–∂–µ–Ω–µ—Ä–∏—é –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å POINTS-GUI-G-8B –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∑–∞–¥–∞—á —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º, –∫–æ—Ç–æ—Ä–∞—è –æ–±—É—á–µ–Ω–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞. –ö–ª—é—á–µ–≤—ã–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤–∫–ª—é—á–∞—é—Ç —Ç—Ä–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: –∏–Ω–∂–µ–Ω–µ—Ä–∏—è –¥–∞–Ω–Ω—ã—Ö —Å —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–±—É—á–µ–Ω–∏—è —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∞ —Ç–∞–∫–∂–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏—Å–∫—É—Å—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –≤–∫–ª—é—á–∞—è ScreenSpot-Pro (59.9) –∏ OSWorld-G (66.0), –ø–æ–∫–∞–∑—ã–≤–∞—è, —á—Ç–æ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –≤ –∑–∞–¥–∞—á–∞—Ö –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞. –ü–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≥–µ–Ω—Ç–∞–º –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ —Ü–∏—Ñ—Ä–æ–≤—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ –æ–Ω–ª–∞–π–Ω-–ø–æ–∫—É–ø–∫–∏ –∏ –±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∏–ª–µ—Ç–æ–≤, –Ω–∞—á–∏–Ω–∞—è —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞–≤—ã–∫–∞–º–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤.'}, 'en': {'title': 'Empowering GUI Agents with Enhanced Grounding for Automation', 'desc': 'This paper discusses the development of GUI agents that automate digital tasks using advanced vision-language models. The authors focus on enhancing grounding capabilities, which allow models to accurately identify and interact with graphical user interface elements. They introduce a new model, POINTS-GUI-G-8B, which outperforms existing models by utilizing refined data engineering, improved training strategies, and reinforcement learning with verifiable rewards. The results show significant improvements in task execution accuracy, demonstrating the effectiveness of their approach in automating complex workflows.'}, 'zh': {'title': 'ÊèêÂçáGUI‰ª£ÁêÜÁöÑËá™Âä®ÂåñËÉΩÂäõ', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÂûãÁöÑÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢ÔºàGUIÔºâ‰ª£ÁêÜÔºåÂà©Áî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÊù•Ëá™Âä®ÂåñÊï∞Â≠ó‰ªªÂä°„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑÊ®°ÂûãPOINTS-GUI-G-8BÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÊòæÁ§∫‰∫ÜÂÖ∂Âú®ÁïåÈù¢ÂÖÉÁ¥†ÂÆö‰ΩçÂíåÊìç‰ΩúÊâßË°åÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÈÄöËøáÁ≤æÁªÜÁöÑÊï∞ÊçÆÂ∑•Á®ã„ÄÅÊîπËøõÁöÑËÆ≠ÁªÉÁ≠ñÁï•ÂíåÂèØÈ™åËØÅÂ•ñÂä±ÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºåÊàë‰ª¨ÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÁ≤æÁ°ÆÂ∫¶„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåGUIÂÆö‰Ωç‰ªªÂä°ÁöÑÊàêÂäü‰æùËµñ‰∫éËøô‰∫õÂÖ≥ÈîÆÂõ†Á¥†ÁöÑÁªìÂêà„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05281', 'title': 'Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities', 'url': 'https://huggingface.co/papers/2602.05281', 'abstract': 'A novel reinforcement learning approach called ARM addresses entropy collapse in LLM reasoning by equilibrating confidence levels across correct responses through dynamic reward shaping.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation, our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks. Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse. Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32, highlighting its superior capability in generating diverse correct reasoning paths.', 'score': 13, 'issue_id': 962, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '2267e7fff36f239f', 'authors': ['Pengyi Li', 'Elizaveta Goncharova', 'Andrey Kuznetsov', 'Ivan Oseledets'], 'affiliations': ['FusionBrain Lab, Russia Mathematics, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2602.05281.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#rl'], 'emoji': '‚öñÔ∏è', 'ru': {'title': '–£—Ä–∞–≤–Ω–æ–≤–µ—à–∏–≤–∞–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ ARM (Advantage Re-weighting Mechanism) –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –∫–æ–ª–ª–∞–ø—Å–∞ —ç–Ω—Ç—Ä–æ–ø–∏–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–ª–∏—Ç–∏–∫–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ GRPO, –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –Ω–∏–∑–∫–æ—ç–Ω—Ç—Ä–æ–ø–∏–π–Ω—ã–º –º–æ–¥–µ–ª—è–º –∏ –ø–æ—Ç–µ—Ä–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –æ—Ç–≤–µ—Ç–æ–≤, —Ç–∞–∫ –∫–∞–∫ —á—Ä–µ–∑–º–µ—Ä–Ω–æ —É—Å–∏–ª–∏–≤–∞—é—Ç –ø—É—Ç–∏ —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Å–∏–≥–Ω–∞–ª –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –æ—Å–ª–∞–±–ª—è—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–ª—è –ø–µ—Ä–µ—É–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –ø–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤ —Å—Ç–æ—Ä–æ–Ω—É –º–∞–ª–æ–∏–∑—É—á–µ–Ω–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –º–æ–¥–µ–ª—è—Ö Qwen2.5 –∏ DeepSeek –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è.'}, 'en': {'title': 'Balancing Confidence for Diverse Reasoning in LLMs', 'desc': 'The paper introduces a new reinforcement learning method called Advantage Re-weighting Mechanism (ARM) to tackle the problem of entropy collapse in Large Language Models (LLMs). Traditional methods like Group Relative Policy Optimization (GRPO) often lead to low diversity in outputs due to over-reliance on high-likelihood responses. ARM addresses this by dynamically adjusting the reward structure to balance confidence levels across all valid responses, promoting exploration of alternative reasoning paths. Experimental results show that ARM improves generative diversity and response entropy while maintaining accuracy, outperforming GRPO in various benchmarks.'}, 'zh': {'title': 'Âπ≥Ë°°ÁΩÆ‰ø°Â∫¶ÔºåÊèêÂçáÊé®ÁêÜÂ§öÊ†∑ÊÄß', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÁß∞‰∏∫‰ºòÂäøÈáçÂä†ÊùÉÊú∫Âà∂ÔºàARMÔºâÔºåÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊé®ÁêÜ‰∏≠ÁöÑÁÜµÂ¥©Ê∫ÉÈóÆÈ¢ò„ÄÇÈÄöËøáÂä®ÊÄÅÂ•ñÂä±Â°ëÂΩ¢ÔºåARMËÉΩÂ§üÂπ≥Ë°°ÊâÄÊúâÊ≠£Á°ÆÂìçÂ∫îÁöÑÁΩÆ‰ø°Ê∞¥Âπ≥ÔºåÈÅøÂÖç‰∫Ü‰º†ÁªüÁ≠ñÁï•‰ºòÂåñÊñπÊ≥ïÂØºËá¥ÁöÑ‰ΩéÁÜµÁ≠ñÁï•ÂíåÊ®°ÂºèÂ¥©Ê∫É„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÈÄöËøáÁªìÂêàÊèêÁ§∫Âõ∞ÊÉëÂ∫¶ÂíåÁ≠îÊ°àÁΩÆ‰ø°Â∫¶Êù•ÈáçÊñ∞Â°ëÈÄ†Â•ñÂä±‰ø°Âè∑Ôºå‰ªéËÄåÊäëÂà∂Ëøá‰∫éËá™‰ø°ÁöÑÊé®ÁêÜË∑ØÂæÑÁöÑÊ¢ØÂ∫¶Êõ¥Êñ∞ÔºåÂπ∂Â∞ÜÊ¶ÇÁéáË¥®ÈáèÈáçÊñ∞ÂàÜÈÖçÁªôÊú™ÂÖÖÂàÜÊé¢Á¥¢ÁöÑÊ≠£Á°ÆËß£ÂÜ≥ÊñπÊ°à„ÄÇÂÆûÈ™åËØÅÊòéÔºåARMÊòæËëóÊèêÈ´ò‰∫ÜÁîüÊàêÁöÑÂ§öÊ†∑ÊÄßÂíåÂìçÂ∫îÁÜµÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÁ´û‰∫âÂäõÁöÑÂáÜÁ°ÆÊÄßÔºåÊàêÂäüÂÆûÁé∞‰∫ÜÊé®ÁêÜ‰ªªÂä°‰∏≠Êé¢Á¥¢‰∏éÂà©Áî®ÁöÑ‰ºòË∂äÂπ≥Ë°°„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.06075', 'title': 'MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments', 'url': 'https://huggingface.co/papers/2602.06075', 'abstract': 'A comprehensive memory-focused benchmark for mobile GUI agents reveals significant memory capability gaps and provides systematic evaluation methods and design insights.  \t\t\t\t\tAI-generated summary \t\t\t\t Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5.2-11.8% memory-related tasks and no cross-session learning evaluation. We introduce MemGUI-Bench, a comprehensive memory-centric benchmark with pass@k and staged LLM-as-judge evaluation. Our contributions include: (1) a systematic memory taxonomy analyzing 11 agents across 5 architectures; (2) 128 tasks across 26 applications where 89.8% challenge memory through cross-temporal and cross-spatial retention; (3) MemGUI-Eval, an automated pipeline with Progressive Scrutiny and 7 hierarchical metrics; and (4) RQ-driven assessment of 11 state-of-the-art agents. Our experiments reveal significant memory deficits across all evaluated systems, identify 5 distinct failure modes, and synthesize 5 actionable design implications. All resources including code, benchmark, and evaluation results will be \\textit{fully open-sourced and continuously maintained} at https://lgy0404.github.io/MemGUI-Bench/.', 'score': 13, 'issue_id': 962, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '85791424d234c269', 'authors': ['Guangyi Liu', 'Pengxiang Zhao', 'Yaozhen Liang', 'Qinyi Luo', 'Shunye Tang', 'Yuxiang Chai', 'Weifeng Lin', 'Han Xiao', 'WenHao Wang', 'Siheng Chen', 'Zhengxi Lu', 'Gao Wu', 'Hao Wang', 'Liang Liu', 'Yong Liu'], 'affiliations': ['Alibaba Group', 'ByteDance', 'Huawei', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2602.06075.jpg', 'data': {'categories': [], 'emoji': 'üß†', 'ru': {'title': '–ü–∞–º—è—Ç—å –∫–∞–∫ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ –º–æ–±–∏–ª—å–Ω—ã—Ö GUI-–∞–≥–µ–Ω—Ç–æ–≤: –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∏ —Ä–µ—à–µ–Ω–∏—è', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ MemGUI-Bench ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–±–∏–ª—å–Ω—ã—Ö GUI-–∞–≥–µ–Ω—Ç–æ–≤ –∫ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –æ–±—É—á–µ–Ω–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –æ–ø—ã—Ç–∞. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–∏–∫–∞—Ö –æ—Ü–µ–Ω–∫–∏, –≥–¥–µ —Ç–æ–ª—å–∫–æ 5.2-11.8% –∑–∞–¥–∞—á –ø—Ä–æ–≤–µ—Ä—è—é—Ç –ø–∞–º—è—Ç—å, –∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é —Ç–∞–∫—Å–æ–Ω–æ–º–∏—é –ø–∞–º—è—Ç–∏ —Å –∞–Ω–∞–ª–∏–∑–æ–º 11 –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ 5 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö. –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç 128 –∑–∞–¥–∞—á —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM-as-judge –ø–æ–¥—Ö–æ–¥–∞ –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, –≤—ã—è–≤–∏–≤ 5 —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã—Ö —Ç–∏–ø–æ–≤ –æ—à–∏–±–æ–∫. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –≤ –ø–∞–º—è—Ç–∏ –≤—Å–µ—Ö –æ—Ü–µ–Ω—ë–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏–ª–∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –±–æ–ª–µ–µ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤.'}, 'en': {'title': 'Enhancing Memory Evaluation for Mobile GUI Agents', 'desc': 'This paper addresses the shortcomings of current benchmarks for mobile GUI agents, particularly in evaluating their memory capabilities. It introduces MemGUI-Bench, a new benchmark that focuses on memory tasks and includes a systematic analysis of various agents. The benchmark features 128 tasks designed to challenge memory retention across different contexts and provides a structured evaluation method using advanced metrics. The findings highlight significant memory deficits in existing agents and propose actionable design insights to improve their performance.'}, 'zh': {'title': 'ÊèêÂçáÁßªÂä®GUI‰ª£ÁêÜÁöÑËÆ∞ÂøÜËÉΩÂäõ', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑ‰ª•ËÆ∞ÂøÜ‰∏∫‰∏≠ÂøÉÁöÑÂü∫ÂáÜÊµãËØïMemGUI-BenchÔºåÁî®‰∫éËØÑ‰º∞ÁßªÂä®GUI‰ª£ÁêÜÁöÑËÆ∞ÂøÜËÉΩÂäõ„ÄÇÁé∞ÊúâÁöÑÂü∫ÂáÜÊµãËØïÂú®ËØÑ‰º∞ËÆ∞ÂøÜËÉΩÂäõÊñπÈù¢Â≠òÂú®ÊòæËëó‰∏çË∂≥Ôºå‰ªÖÊúâ5.2%Âà∞11.8%ÁöÑ‰ªªÂä°‰∏éËÆ∞ÂøÜÁõ∏ÂÖ≥„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü128‰∏™‰ªªÂä°ÔºåÊ∂µÁõñ26‰∏™Â∫îÁî®Á®ãÂ∫èÔºåÂÖ∂‰∏≠89.8%ÁöÑ‰ªªÂä°ÊåëÊàòË∑®Êó∂Èó¥ÂíåË∑®Á©∫Èó¥ÁöÑËÆ∞ÂøÜ‰øùÊåÅËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÊâÄÊúâËØÑ‰º∞Á≥ªÁªüÂú®ËÆ∞ÂøÜËÉΩÂäõ‰∏äÂ≠òÂú®ÊòæËëóÁº∫Èô∑ÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∫îÁßçÂèØË°åÁöÑËÆæËÆ°Âª∫ËÆÆ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.06079', 'title': 'Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers', 'url': 'https://huggingface.co/papers/2602.06079', 'abstract': 'Canzona presents a unified asynchronous framework that addresses the conflict between matrix-based optimizers and distributed tensor fragmentation in LLM training, improving efficiency and reducing latency.  \t\t\t\t\tAI-generated summary \t\t\t\t The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, a Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an alpha-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving a 1.57x speedup in end-to-end iteration time and reducing optimizer step latency by 5.8x compared to the baseline.', 'score': 12, 'issue_id': 965, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': 'a71d4a1e946c02a3', 'authors': ['Liangyu Wang', 'Siqi Zhang', 'Junjie Wang', 'Yiming Dong', 'Bo Zheng', 'Zihan Qiu', 'Shengkun Tang', 'Di Wang', 'Rui Men', 'Dayiheng Liu'], 'affiliations': ['Alibaba Group', 'KAUST', 'MBZUAI', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2602.06079.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training'], 'emoji': '‚ö°', 'ru': {'title': '–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–∞—Ä–º–æ–Ω–∏—è: –º–∞—Ç—Ä–∏—á–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã –≤—Å—Ç—Ä–µ—á–∞—é—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã', 'desc': 'Canzona –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç –º–µ–∂–¥—É –º–∞—Ç—Ä–∏—á–Ω—ã–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞–º–∏ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π —Ç–µ–Ω–∑–æ—Ä–æ–≤ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ú–∞—Ç—Ä–∏—á–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ Shampoo –∏ Muon, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç —Ö–æ—Ä–æ—à—É—é —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å, –Ω–æ —Ç—Ä–µ–±—É—é—Ç –ø–æ–ª–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º, —á—Ç–æ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–º —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞–º —Ç–∏–ø–∞ Megatron, –∫–æ—Ç–æ—Ä—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏—Ä—É—é—Ç —Ç–µ–Ω–∑–æ—Ä—ã. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏ –∏ –º–∏–∫—Ä–æ-–≥—Ä—É–ø–ø–æ–≤—ã–º –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º, –æ—Ç–¥–µ–ª—è—è –ª–æ–≥–∏—á–µ—Å–∫–æ–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –æ—Ç —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤ 1.57 —Ä–∞–∑–∞ –¥–ª—è –∏—Ç–µ—Ä–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è –∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –∑–∞–¥–µ—Ä–∂–∫–∏ —à–∞–≥–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –≤ 5.8 —Ä–∞–∑ –Ω–∞ –º–æ–¥–µ–ª–∏ Qwen3 —Ä–∞–∑–º–µ—Ä–æ–º –¥–æ 32B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.'}, 'en': {'title': 'Canzona: Bridging the Gap in LLM Training Efficiency', 'desc': 'Canzona introduces a new framework that improves the training of Large Language Models (LLMs) by resolving issues between matrix-based optimizers and distributed tensor fragmentation. It offers a unified, asynchronous approach that allows for efficient updates without the need for holistic parameter updates, which are often problematic in distributed systems. The framework includes a novel alpha-Balanced Static Partitioning strategy for Data Parallelism and an Asynchronous Compute pipeline for Tensor Parallelism, which together enhance load balancing and reduce latency. Evaluations show that Canzona significantly speeds up training times and optimizes performance on large models across multiple GPUs.'}, 'zh': {'title': 'CanzonaÔºöÊèêÂçáÂ§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãËÆ≠ÁªÉÊïàÁéáÁöÑÂºÇÊ≠•Ê°ÜÊû∂', 'desc': 'CanzonaÊèêÂá∫‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂºÇÊ≠•Ê°ÜÊû∂ÔºåËß£ÂÜ≥‰∫ÜÁü©Èòµ‰ºòÂåñÂô®‰∏éÂàÜÂ∏ÉÂºèÂº†ÈáèÁ¢éÁâáÂåñ‰πãÈó¥ÁöÑÂÜ≤Á™ÅÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÊïàÁéáÂπ∂ÂáèÂ∞ë‰∫ÜÂª∂Ëøü„ÄÇÂ§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊâ©Â±ïÊé®Âä®‰∫ÜÂØπÁü©Èòµ‰ºòÂåñÂô®ÁöÑÂÖ≥Ê≥®Ôºå‰ΩÜÂÆÉ‰ª¨ÂØπÊï¥‰ΩìÊõ¥Êñ∞ÁöÑÈúÄÊ±Ç‰∏éÂàÜÂ∏ÉÂºèÊ°ÜÊû∂‰∏≠ÁöÑÂº†ÈáèÁ¢éÁâáÂåñÁõ∏ÁüõÁõæ„ÄÇÁé∞ÊúâÁöÑËß£ÂÜ≥ÊñπÊ°àÊïàÁéá‰∏çÈ´òÔºåÂØºËá¥ËÆ°ÁÆóÂÜó‰ΩôÊàñÊó†Ê≥ïÊúâÊïàÊ≤üÈÄö„ÄÇCanzonaÈÄöËøáËß£ËÄ¶ÈÄªËæë‰ºòÂåñÂô®ÂàÜÈÖç‰∏éÁâ©ÁêÜÂèÇÊï∞ÂàÜÂ∏ÉÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊï∞ÊçÆÂπ∂Ë°åÂíåÂº†ÈáèÂπ∂Ë°åÁ≠ñÁï•ÔºåÊòæËëóÊèêÂçá‰∫ÜËÆ≠ÁªÉÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.06139', 'title': 'EgoAVU: Egocentric Audio-Visual Understanding', 'url': 'https://huggingface.co/papers/2602.06139', 'abstract': 'Multi-modal large language models struggle to jointly understand audio and visual signals in egocentric videos, but a new scalable data engine and dataset significantly improve their performance through targeted fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding egocentric videos plays a vital role for embodied intelligence. Recent multi-modal large language models (MLLMs) can accept both visual and audio inputs. However, due to the challenge of obtaining text labels with coherent joint-modality information, whether MLLMs can jointly understand both modalities in egocentric videos remains under-explored. To address this problem, we introduce EgoAVU, a scalable data engine to automatically generate egocentric audio-visual narrations, questions, and answers. EgoAVU enriches human narrations with multimodal context and generates audio-visual narrations through cross-modal correlation modeling. Token-based video filtering and modular, graph-based curation ensure both data diversity and quality. Leveraging EgoAVU, we construct EgoAVU-Instruct, a large-scale training dataset of 3M samples, and EgoAVU-Bench, a manually verified evaluation split covering diverse tasks. EgoAVU-Bench clearly reveals the limitations of existing MLLMs: they bias heavily toward visual signals, often neglecting audio cues or failing to correspond audio with the visual source. Finetuning MLLMs on EgoAVU-Instruct effectively addresses this issue, enabling up to 113% performance improvement on EgoAVU-Bench. Such benefits also transfer to other benchmarks such as EgoTempo and EgoIllusion, achieving up to 28% relative performance gain. Code will be released to the community.', 'score': 9, 'issue_id': 962, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '5b2f07457e4c1967', 'authors': ['Ashish Seth', 'Xinhao Mei', 'Changsheng Zhao', 'Varun Nagaraja', 'Ernie Chang', 'Gregory P. Meyer', 'Gael Le Lan', 'Yunyang Xiong', 'Vikas Chandra', 'Yangyang Shi', 'Dinesh Manocha', 'Zhipeng Cai'], 'affiliations': ['Meta', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2602.06139.jpg', 'data': {'categories': ['#audio', '#multimodal', '#benchmark', '#video', '#training', '#robotics', '#dataset'], 'emoji': 'üëÅÔ∏è\u200düó®Ô∏è', 'ru': {'title': '–ì–∞—Ä–º–æ–Ω–∏—á–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –∑–≤—É–∫–∞ –∏ –≤–∏–¥–µ–Ω–∏—è –≤ —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ö', 'desc': '–†–∞–±–æ—Ç–∞ –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∞—É–¥–∏–æ –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –≤–∏–¥–µ–æ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç EgoAVU ‚Äî –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è, –≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω –±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ 3 –º–ª–Ω –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –∏ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –Ω–∞ —ç—Ç–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç —Å–º–µ—â–µ–Ω–∏–µ –≤ —Å—Ç–æ—Ä–æ–Ω—É –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–æ 113%.'}, 'en': {'title': 'Enhancing Multi-Modal Understanding in Egocentric Videos', 'desc': 'This paper addresses the challenges faced by multi-modal large language models (MLLMs) in understanding both audio and visual signals in egocentric videos. The authors introduce EgoAVU, a scalable data engine that generates rich audio-visual narrations and questions, enhancing the training data for MLLMs. By fine-tuning these models on the newly created EgoAVU-Instruct dataset, significant performance improvements are achieved, particularly in tasks that require joint modality understanding. The study highlights the limitations of existing MLLMs and demonstrates how targeted training can enhance their ability to process and integrate audio and visual information effectively.'}, 'zh': {'title': 'ÊèêÂçáÂ§öÊ®°ÊÄÅÁêÜËß£ÔºåÁ™ÅÁ†¥Ëá™Êàë‰∏≠ÂøÉËßÜÈ¢ëÁöÑÁì∂È¢à', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÁêÜËß£Ëá™Êàë‰∏≠ÂøÉËßÜÈ¢ë‰∏≠ÁöÑÈü≥È¢ëÂíåËßÜËßâ‰ø°Âè∑ÊñπÈù¢ÁöÑÊåëÊàò„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜEgoAVUÔºå‰∏Ä‰∏™ÂèØÊâ©Â±ïÁöÑÊï∞ÊçÆÂºïÊìéÔºåËÉΩÂ§üËá™Âä®ÁîüÊàêËá™Êàë‰∏≠ÂøÉÁöÑÈü≥È¢ë-ËßÜËßâÂèôËø∞„ÄÅÈóÆÈ¢òÂíåÁ≠îÊ°à„ÄÇÈÄöËøá‰∫§ÂèâÊ®°ÊÄÅÂÖ≥ËÅîÂª∫Ê®°ÔºåEgoAVU‰∏∞ÂØå‰∫Ü‰∫∫Á±ªÂèôËø∞ÁöÑÂ§öÊ®°ÊÄÅ‰∏ä‰∏ãÊñáÔºåÂπ∂Á°Æ‰øùÊï∞ÊçÆÁöÑÂ§öÊ†∑ÊÄßÂíåË¥®Èáè„ÄÇÂØπÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËøõË°åÈíàÂØπÊÄßÁöÑÂæÆË∞ÉÔºåËÉΩÂ§üÊòæËëóÊèêÈ´òÂÖ∂Âú®Ëá™Êàë‰∏≠ÂøÉËßÜÈ¢ëÁêÜËß£‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05367', 'title': 'RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs', 'url': 'https://huggingface.co/papers/2602.05367', 'abstract': 'Residual binarization framework RaBiT addresses feature co-adaptation in quantized LLMs through hierarchical path derivation and robust initialization, achieving superior accuracy-efficiency trade-offs.  \t\t\t\t\tAI-generated summary \t\t\t\t Efficient deployment of large language models (LLMs) requires extreme quantization, forcing a critical trade-off between low-bit efficiency and performance. Residual binarization enables hardware-friendly, matmul-free inference by stacking binary (pm1) layers, but is plagued by pathological feature co-adaptation. We identify a key failure mode, which we term inter-path adaptation: during quantization-aware training (QAT), parallel residual binary paths learn redundant features, degrading the error-compensation structure and limiting the expressive capacity of the model. While prior work relies on heuristic workarounds (e.g., path freezing) that constrain the solution space, we propose RaBiT, a novel quantization framework that resolves co-adaptation by algorithmically enforcing a residual hierarchy. Its core mechanism sequentially derives each binary path from a single shared full-precision weight, which ensures that every path corrects the error of the preceding one. This process is stabilized by a robust initialization that prioritizes functional preservation over mere weight approximation. RaBiT redefines the 2-bit accuracy-efficiency frontier: it achieves state-of-the-art performance, rivals even hardware-intensive Vector Quantization (VQ) methods, and delivers a 4.49times inference speed-up over full-precision models on an RTX 4090.', 'score': 7, 'issue_id': 963, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '042ce2bd26c0169f', 'authors': ['Youngcheon You', 'Banseok Lee', 'Minseop Choi', 'Seonyoung Kim', 'Hyochan Chong', 'Changdong Kim', 'Youngmin Kim', 'Dongkyu Kim'], 'affiliations': ['Samsung Research, Seoul, Korea'], 'pdf_title_img': 'assets/pdf/title_img/2602.05367.jpg', 'data': {'categories': ['#inference', '#architecture', '#training', '#optimization'], 'emoji': '‚ö°', 'ru': {'title': '–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ LLM', 'desc': 'RaBiT ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—É—Ç—ë–º –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –æ—Å—Ç–∞—Ç–æ—á–Ω—ã—Ö –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—É—Ç–µ–π. –ö–ª—é—á–µ–≤–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –∫–∞–∂–¥—ã–π –±–∏–Ω–∞—Ä–Ω—ã–π –ø—É—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –≤—ã–≤–æ–¥–∏—Ç—Å—è –∏–∑ –æ–¥–Ω–æ–≥–æ –æ–±—â–µ–≥–æ –ø–æ–ª–Ω–æ—Ç–æ—á–Ω–æ–≥–æ –≤–µ—Å–∞, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ü–∏—é –æ—à–∏–±–æ–∫ –ø—Ä–µ–¥—ã–¥—É—â–∏–º —Å–ª–æ–µ–º. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –Ω–∞–¥—ë–∂–Ω—É—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –Ω–∞–¥ –ø—Ä–æ—Å—Ç—ã–º –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ–º –≤–µ—Å–æ–≤. RaBiT –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–µ–≥–æ –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é, –∫–æ–Ω–∫—É—Ä–∏—Ä—É—è —Å –º–µ—Ç–æ–¥–∞–º–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è 4,49-–∫—Ä–∞—Ç–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –Ω–∞ RTX 4090.'}, 'en': {'title': 'RaBiT: Revolutionizing Quantization for Efficient LLMs', 'desc': "The paper introduces the Residual Binarization framework (RaBiT) to tackle the issue of feature co-adaptation in quantized large language models (LLMs). It highlights a problem called inter-path adaptation, where multiple binary paths learn similar features, which reduces the model's performance. RaBiT addresses this by creating a hierarchical structure for binary paths that ensures each path builds on the previous one, improving error correction. This method not only enhances accuracy but also significantly speeds up inference, outperforming traditional quantization techniques."}, 'zh': {'title': 'ÊÆãÂ∑Æ‰∫åÂÄºÂåñÔºöÊèêÂçáÈáèÂåñÊ®°ÂûãÊÄßËÉΩÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫RaBiTÁöÑÊÆãÂ∑Æ‰∫åÂÄºÂåñÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥ÈáèÂåñÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠ÁöÑÁâπÂæÅÂÖ±ÈÄÇÂ∫îÈóÆÈ¢ò„ÄÇÈÄöËøáÂàÜÂ±ÇË∑ØÂæÑÊé®ÂØºÂíåÁ®≥ÂÅ•ÂàùÂßãÂåñÔºåRaBiTËÉΩÂ§üÂú®‰ΩéÊØîÁâπÊïàÁéáÂíåÊ®°ÂûãÊÄßËÉΩ‰πãÈó¥ÂÆûÁé∞‰ºòË∂äÁöÑÊùÉË°°„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÁÆóÊ≥ïÊÄßÂú∞Âº∫Âà∂ÊâßË°åÊÆãÂ∑ÆÂ±ÇÊ¨°ÁªìÊûÑÔºåÁ°Æ‰øùÊØè‰∏™‰∫åËøõÂà∂Ë∑ØÂæÑÈÉΩËÉΩÁ∫†Ê≠£Ââç‰∏Ä‰∏™Ë∑ØÂæÑÁöÑÈîôËØØÔºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÁöÑË°®ËææËÉΩÂäõ„ÄÇÊúÄÁªàÔºåRaBiTÂú®2ÊØîÁâπÁ≤æÂ∫¶ÊïàÁéáÁöÑÂâçÊ≤ø‰∏äÈáçÊñ∞ÂÆö‰πâ‰∫ÜÊÄßËÉΩÊ†áÂáÜÔºåÊòæËëóÊèêÂçá‰∫ÜÊé®ÁêÜÈÄüÂ∫¶„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.06960', 'title': 'InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2602.06960', 'abstract': 'InftyThink+ uses reinforcement learning to optimize iterative reasoning processes, improving accuracy and efficiency in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization. InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning, enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.', 'score': 6, 'issue_id': 962, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '0d9d6ab163765d9f', 'authors': ['Yuchen Yan', 'Liang Jiang', 'Jin Jiang', 'Shuaicheng Li', 'Zujie Wen', 'Zhiqiang Zhang', 'Jun Zhou', 'Jian Shao', 'Yueting Zhuang', 'Yongliang Shen'], 'affiliations': ['Ant Group', 'Peking University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2602.06960.jpg', 'data': {'categories': ['#small_models', '#optimization', '#rl', '#training', '#reasoning', '#long_context'], 'emoji': 'üß†', 'ru': {'title': '–£–º–Ω–æ–µ —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏–µ –º—ã—Å–ª–µ–π —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ InftyThink+, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ —Ü–µ–ø–æ—á–∫–∏ –º—ã—Å–ª–µ–π –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏ —ç—Ñ—Ñ–µ–∫—Ç–µ –ø–æ—Ç–µ—Ä–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∞ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —Å –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–º —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏–µ–º –ø–æ–º–æ–≥–∞–µ—Ç —ç—Ç–æ –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—É—é —Å—Ö–µ–º—É –æ–±—É—á–µ–Ω–∏—è: —Å–Ω–∞—á–∞–ª–∞ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –∑–∞—Ç–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —É—á–∏—Ç—å—Å—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º —Ä–µ—à–µ–Ω–∏—è–º –æ —Ç–æ–º, –∫–æ–≥–¥–∞ –∏ –∫–∞–∫ —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ 21% –∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏, –ø—Ä–∏ —ç—Ç–æ–º –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –ª—É—á—à—É—é –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç—å –Ω–∞ –≤–Ω–µ—à–Ω–∏—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö.'}, 'en': {'title': 'Optimizing Reasoning with Reinforcement Learning', 'desc': 'InftyThink+ is a novel framework that enhances large language models by using reinforcement learning to optimize iterative reasoning processes. It addresses the limitations of traditional reasoning methods, such as high computational costs and context length restrictions, by implementing strategic summarization of intermediate thoughts. The framework employs a two-stage training approach, starting with supervised learning and transitioning to reinforcement learning, allowing the model to effectively decide when to summarize and how to continue reasoning. Experimental results show that InftyThink+ significantly boosts accuracy and efficiency, outperforming existing methods in both reasoning performance and inference speed.'}, 'zh': {'title': '‰ºòÂåñÊé®ÁêÜÔºåÊèêÂçáÊïàÁéá‚Äî‚ÄîInftyThink+', 'desc': 'InftyThink+ ÊòØ‰∏Ä‰∏™‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†‰ºòÂåñËø≠‰ª£Êé®ÁêÜËøáÁ®ãÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂÆöÊúüÊÄªÁªì‰∏≠Èó¥ÊÄùË∑ØÊù•ÁºìËß£Êé®ÁêÜËøáÁ®ã‰∏≠ÁöÑÈóÆÈ¢òÔºåÂ¶Ç‰∫åÊ¨°ÊàêÊú¨Âíå‰∏ä‰∏ãÊñáÈïøÂ∫¶ÈôêÂà∂„ÄÇ‰∏é‰º†ÁªüÁöÑÁõëÁù£Â≠¶‰π†ÊñπÊ≥ï‰∏çÂêåÔºåInftyThink+ ÈááÁî®Á´ØÂà∞Á´ØÁöÑÂº∫ÂåñÂ≠¶‰π†Ôºå‰ºòÂåñÊï¥‰∏™Ëø≠‰ª£Êé®ÁêÜËΩ®Ëøπ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåInftyThink+ Âú®ÂáÜÁ°ÆÊÄßÂíåÊé®ÁêÜÊïàÁéá‰∏äÂùáÊúâÊòæËëóÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.06869', 'title': 'Uncovering Cross-Objective Interference in Multi-Objective Alignment', 'url': 'https://huggingface.co/papers/2602.06869', 'abstract': 'Multi-objective alignment in LLMs suffers from cross-objective interference where improving performance on some objectives degrades others, with a covariance-based analysis and a proposed method to maintain positive correlations between rewards and training signals.  \t\t\t\t\tAI-generated summary \t\t\t\t We study a persistent failure mode in multi-objective alignment for large language models (LLMs): training improves performance on only a subset of objectives while causing others to degrade. We formalize this phenomenon as cross-objective interference and conduct the first systematic study across classic scalarization algorithms, showing that interference is pervasive and exhibits strong model dependence.   To explain this phenomenon, we derive a local covariance law showing that an objective improves at first order when its reward exhibits positive covariance with the scalarized score. We extend this analysis to clipped surrogate objectives used in modern alignment, demonstrating that the covariance law remains valid under mild conditions despite clipping. Building on this analysis, we propose Covariance Targeted Weight Adaptation (CTWA), a plug-and-play method that maintains positive covariance between objective rewards and the training signal to effectively mitigate cross-objective interference. Finally, we complement these local improvement conditions with a global convergence analysis under the Polyak--≈Åojasiewicz condition, establishing when non-convex scalarized optimization achieves global convergence and how cross-objective interference depends on specific model geometric properties.', 'score': 5, 'issue_id': 974, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '642333999180c56e', 'authors': ['Yining Lu', 'Meng Jiang'], 'affiliations': ['Department of Computer Science and Engineering, University of Notre Dame, USA'], 'pdf_title_img': 'assets/pdf/title_img/2602.06869.jpg', 'data': {'categories': ['#training', '#rlhf', '#optimization', '#architecture', '#alignment'], 'emoji': '‚öñÔ∏è', 'ru': {'title': '–ë–∞–ª–∞–Ω—Å —Ü–µ–ª–µ–π: –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –ø—Ä–∏ –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–º –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–í —Ä–∞–±–æ—Ç–µ –∏–∑—É—á–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –∫—Ä–æ—Å—Å-–æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ –ø—Ä–∏ –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ–≥–¥–∞ —É–ª—É—á—à–µ–Ω–∏–µ –ø–æ –æ–¥–Ω–∏–º —Ü–µ–ª—è–º –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –ø–æ –¥—Ä—É–≥–∏–º. –ê–≤—Ç–æ—Ä—ã —Ñ–æ—Ä–º–∞–ª–∏–∑—É—é—Ç —ç—Ç–æ —è–≤–ª–µ–Ω–∏–µ –∏ –≤—ã–≤–æ–¥—è—Ç –ª–æ–∫–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–∏, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∏–π —á—Ç–æ —Ü–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è —É–ª—É—á—à–∞–µ—Ç—Å—è, –∫–æ–≥–¥–∞ –µ—ë –Ω–∞–≥—Ä–∞–¥–∞ –∏–º–µ–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—É—é –∫–æ–≤–∞—Ä–∏–∞—Ü–∏—é —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º—ã–º —Å–∏–≥–Ω–∞–ª–æ–º. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Covariance Targeted Weight Adaptation (CTWA), –∫–æ—Ç–æ—Ä—ã–π –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–µ–∂–¥—É –Ω–∞–≥—Ä–∞–¥–∞–º–∏ –∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–º —Å–∏–≥–Ω–∞–ª–æ–º –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–Ω–∏–∂–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ—Ä–µ–Ω—Ü–∏–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–æ –≥–ª–æ–±–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–æ–¥ —É—Å–ª–æ–≤–∏–µ–º –ü–æ–ª–∏–∞–∫–∞-–õ–æ–π–∞—Å–∏–µ–≤–∏—á–∞, –∫–æ—Ç–æ—Ä—ã–π —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —É—Å–ª–æ–≤–∏—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç–∏ –≤ –Ω–µ–≤—ã–ø—É–∫–ª–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.'}, 'en': {'title': 'Mitigating Cross-Objective Interference in LLMs', 'desc': 'This paper addresses a challenge in training large language models (LLMs) where improving one objective can negatively impact others, a phenomenon known as cross-objective interference. The authors conduct a systematic analysis of this issue across various scalarization algorithms, revealing that this interference is common and varies with different models. They introduce a local covariance law that explains how objectives can improve when their rewards are positively correlated with the overall score. To combat this interference, they propose a new method called Covariance Targeted Weight Adaptation (CTWA), which helps maintain positive correlations between objectives during training, thus enhancing overall performance.'}, 'zh': {'title': 'Ëß£ÂÜ≥Â§öÁõÆÊ†áÂØπÈΩê‰∏≠ÁöÑË∑®ÁõÆÊ†áÂπ≤Êâ∞', 'desc': 'Âú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂ§öÁõÆÊ†áÂØπÈΩê‰∏≠ÔºåÂ≠òÂú®‰∏ÄÁßçÊåÅÁª≠ÁöÑÂ§±Ë¥•Ê®°ÂºèÔºåÂç≥Âú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÊüê‰∫õÁõÆÊ†áÁöÑÊÄßËÉΩÊèêÂçá‰ºöÂØºËá¥ÂÖ∂‰ªñÁõÆÊ†áÁöÑÊÄßËÉΩ‰∏ãÈôç„ÄÇËøôÁßçÁé∞Ë±°Ë¢´Áß∞‰∏∫Ë∑®ÁõÆÊ†áÂπ≤Êâ∞ÔºåÊàë‰ª¨ÈÄöËøáÁ≥ªÁªüÁ†îÁ©∂ÁªèÂÖ∏ÁöÑÊ†áÈáèÂåñÁÆóÊ≥ïÔºåÂèëÁé∞ËøôÁßçÂπ≤Êâ∞ÊôÆÈÅçÂ≠òÂú®‰∏î‰∏éÊ®°ÂûãÂØÜÂàáÁõ∏ÂÖ≥„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ±ÄÈÉ®ÂçèÊñπÂ∑ÆÊ≥ïÂàôÔºåË°®ÊòéÂΩìÁõÆÊ†áÁöÑÂ•ñÂä±‰∏éÊ†áÈáèÂåñÂæóÂàÜÂëàÊ≠£ÂçèÊñπÂ∑ÆÊó∂ÔºåÁõÆÊ†áÁöÑÊÄßËÉΩ‰ºöÂæóÂà∞ÊèêÂçá„ÄÇÂü∫‰∫éÊ≠§ÂàÜÊûêÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ï‚Äî‚ÄîÂçèÊñπÂ∑ÆÁõÆÊ†áÊùÉÈáçÈÄÇÂ∫îÔºàCTWAÔºâÔºåÊó®Âú®‰øùÊåÅÁõÆÊ†áÂ•ñÂä±‰∏éËÆ≠ÁªÉ‰ø°Âè∑‰πãÈó¥ÁöÑÊ≠£ÂçèÊñπÂ∑ÆÔºå‰ªéËÄåÊúâÊïàÂáèËΩªË∑®ÁõÆÊ†áÂπ≤Êâ∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.06669', 'title': "compar:IA: The French Government's LLM arena to collect French-language human prompts and preference data", 'url': 'https://huggingface.co/papers/2602.06669', 'abstract': 'Compar:IA is an open-source platform that collects large-scale human preference data for multilingual language model training and evaluation, featuring a blind pairwise comparison interface and releasing three datasets under open licenses.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) often show reduced performance, cultural alignment, and safety robustness in non-English languages, partly because English dominates both pre-training data and human preference alignment datasets. Training methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) require human preference data, which remains scarce and largely non-public for many languages beyond English. To address this gap, we introduce compar:IA, an open-source digital public service developed inside the French government and designed to collect large-scale human preference data from a predominantly French-speaking general audience. The platform uses a blind pairwise comparison interface to capture unconstrained, real-world prompts and user judgments across a diverse set of language models, while maintaining low participation friction and privacy-preserving automated filtering. As of 2026-02-07, compar:IA has collected over 600,000 free-form prompts and 250,000 preference votes, with approximately 89% of the data in French. We release three complementary datasets -- conversations, votes, and reactions -- under open licenses, and present initial analyses, including a French-language model leaderboard and user interaction patterns. Beyond the French context, compar:IA is evolving toward an international digital public good, offering reusable infrastructure for multilingual model training, evaluation, and the study of human-AI interaction.', 'score': 5, 'issue_id': 970, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '82096bfef4c600c2', 'authors': ['Lucie Termignon', 'Simonas Zilinskas', 'Hadrien P√©lissier', 'Aur√©lien Barrot', 'Nicolas Chesnais', 'Elie Gavoty'], 'affiliations': ['French government'], 'pdf_title_img': 'assets/pdf/title_img/2602.06669.jpg', 'data': {'categories': ['#multilingual', '#data', '#low_resource', '#rlhf', '#alignment', '#dataset', '#open_source', '#benchmark'], 'emoji': 'üåç', 'ru': {'title': '–û—Ç–∫—Ä—ã—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö —á–µ–ª–æ–≤–µ–∫–∞ –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'Compar:IA ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö —á–µ–ª–æ–≤–µ–∫–∞ –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–º –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ–º. –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ª–µ–ø–æ–µ –ø–æ–ø–∞—Ä–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–±–æ—Ä–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ü–µ–Ω–æ–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, —á—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –º–µ—Ç–æ–¥–∞–º–∏ RLHF –∏ DPO. –ù–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç —Å–æ–±—Ä–∞–Ω–æ –±–æ–ª–µ–µ 600 000 –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ 250 000 –≥–æ–ª–æ—Å–æ–≤ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π, –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –Ω–∞ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–º —è–∑—ã–∫–µ. –ü—Ä–æ–µ–∫—Ç –≤—ã–ø—É—Å–∫–∞–µ—Ç –æ—Ç–∫—Ä—ã—Ç—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –∏ —Ä–∞–∑–≤–∏–≤–∞–µ—Ç—Å—è –∫–∞–∫ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–µ —Ü–∏—Ñ—Ä–æ–≤–æ–µ –±–ª–∞–≥–æ –¥–ª—è —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –∏–∑—É—á–µ–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —á–µ–ª–æ–≤–µ–∫–∞ —Å AI.'}, 'en': {'title': 'Empowering Multilingual AI with Human Preferences', 'desc': 'Compar:IA is an open-source platform designed to gather extensive human preference data for training and evaluating multilingual language models. It addresses the performance gap of Large Language Models (LLMs) in non-English languages by providing a blind pairwise comparison interface for users to express their preferences. The platform has successfully collected over 600,000 prompts and 250,000 preference votes, primarily in French, and releases three datasets under open licenses. Ultimately, compar:IA aims to serve as a reusable resource for multilingual model development and enhance understanding of human-AI interactions globally.'}, 'zh': {'title': 'ÊûÑÂª∫Â§öËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÅèÂ•ΩÊï∞ÊçÆÂπ≥Âè∞', 'desc': 'compar:IAÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÂπ≥Âè∞ÔºåÊó®Âú®Êî∂ÈõÜÂ§ßËßÑÊ®°ÁöÑ‰∫∫Á±ªÂÅèÂ•ΩÊï∞ÊçÆÔºå‰ª•ÊîØÊåÅÂ§öËØ≠Ë®ÄÊ®°ÂûãÁöÑËÆ≠ÁªÉÂíåËØÑ‰º∞„ÄÇËØ•Âπ≥Âè∞ÈááÁî®Áõ≤ÂØπÊØîÁïåÈù¢ÔºåËÉΩÂ§üÊçïÊçâÁúüÂÆû‰∏ñÁïåÁöÑÊèêÁ§∫ÂíåÁî®Êà∑Âà§Êñ≠ÔºåÁâπÂà´ÂÖ≥Ê≥®Ê≥ïËØ≠Áî®Êà∑ÁöÑÂèçÈ¶à„ÄÇÂà∞2026Âπ¥2Êúà7Êó•Ôºåcompar:IAÂ∑≤Êî∂ÈõÜË∂ÖËøá60‰∏á‰∏™Ëá™Áî±ÂΩ¢ÂºèÁöÑÊèêÁ§∫Âíå25‰∏á‰∏™ÂÅèÂ•ΩÊäïÁ•®ÔºåÊï∞ÊçÆ‰∏ªË¶ÅÈõÜ‰∏≠Âú®Ê≥ïËØ≠„ÄÇÊàë‰ª¨ËøòÂèëÂ∏É‰∫Ü‰∏â‰∏™‰∫íË°•ÁöÑÊï∞ÊçÆÈõÜÔºåÂπ∂ËøõË°åÂàùÊ≠•ÂàÜÊûêÔºå‰ª•Êé®Âä®Â§öËØ≠Ë®ÄÊ®°ÂûãÁöÑÁ†îÁ©∂Âíå‰∫∫Êú∫‰∫§‰∫íÁöÑÁêÜËß£„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.06663', 'title': 'PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks', 'url': 'https://huggingface.co/papers/2602.06663', 'abstract': "PlanViz benchmark evaluates unified multimodal models' capabilities in computer-use planning tasks through route planning, work diagramming, and web&UI displaying sub-tasks with a task-adaptive scoring system.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal models (UMMs) have shown impressive capabilities in generating natural images and supporting multimodal reasoning. However, their potential in supporting computer-use planning tasks, which are closely related to our lives, remain underexplored. Image generation and editing in computer-use tasks require capabilities like spatial reasoning and procedural understanding, and it is still unknown whether UMMs have these capabilities to finish these tasks or not. Therefore, we propose PlanViz, a new benchmark designed to evaluate image generation and editing for computer-use tasks. To achieve the goal of our evaluation, we focus on sub-tasks which frequently involve in daily life and require planning steps. Specifically, three new sub-tasks are designed: route planning, work diagramming, and web&UI displaying. We address challenges in data quality ensuring by curating human-annotated questions and reference images, and a quality control process. For challenges of comprehensive and exact evaluation, a task-adaptive score, PlanScore, is proposed. The score helps understanding the correctness, visual quality and efficiency of generated images. Through experiments, we highlight key limitations and opportunities for future research on this topic.", 'score': 5, 'issue_id': 962, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '83fea21ac7f4026f', 'authors': ['Junxian Li', 'Kai Liu', 'Leyang Chen', 'Weida Wang', 'Zhixin Wang', 'Jiaqi Xu', 'Fan Li', 'Renjing Pei', 'Linghe Kong', 'Yulun Zhang'], 'affiliations': ['Alibaba Group', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2602.06663.jpg', 'data': {'categories': ['#survey', '#multimodal', '#benchmark', '#cv', '#dataset'], 'emoji': 'üìã', 'ru': {'title': '–û—Ü–µ–Ω–∫–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–∞', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –±–µ–Ω—á–º–∞—Ä–∫ PlanViz –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–∞, –≤–∫–ª—é—á–∞—é—â–∏–µ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—é, –¥–∏–∞–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ä–∞–±–æ—Ç—É —Å –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º–∏. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏, —á—Ç–æ —Ç–µ–∫—É—â–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–∑—É—á–µ–Ω—ã —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∏—Ö —É–º–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Ç—Ä–µ–±—É—é—â–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–æ—Ü–µ–¥—É—Ä. –î–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∫ –∑–∞–¥–∞—á–∞–º —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏—è PlanScore, –∫–æ—Ç–æ—Ä–∞—è –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å, –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –≤—ã—è–≤–ª—è—é—Ç –∫–ª—é—á–µ–≤—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π –∏ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –±—É–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è.'}, 'en': {'title': 'Evaluating Multimodal Models for Everyday Computer Planning Tasks', 'desc': 'The paper introduces PlanViz, a benchmark designed to assess the capabilities of unified multimodal models (UMMs) in performing computer-use planning tasks. It focuses on three specific sub-tasks: route planning, work diagramming, and web & UI displaying, which are relevant to everyday life. The benchmark employs a task-adaptive scoring system called PlanScore to evaluate the correctness, visual quality, and efficiency of the generated images. This research aims to uncover the strengths and weaknesses of UMMs in spatial reasoning and procedural understanding within the context of computer-use tasks.'}, 'zh': {'title': 'ËØÑ‰º∞Â§öÊ®°ÊÄÅÊ®°ÂûãÂú®ËÆ°ÁÆóÊú∫‰ΩøÁî®ËßÑÂàí‰∏≠ÁöÑËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫ÜPlanVizÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞Áªü‰∏ÄÂ§öÊ®°ÊÄÅÊ®°ÂûãÂú®ËÆ°ÁÆóÊú∫‰ΩøÁî®ËßÑÂàí‰ªªÂä°‰∏≠ÁöÑËÉΩÂäõ„ÄÇËØ•Âü∫ÂáÜÈÄöËøáË∑ØÁ∫øËßÑÂàí„ÄÅÂ∑•‰ΩúÂõæÁ§∫ÂíåÁΩëÈ°µ‰∏éÁî®Êà∑ÁïåÈù¢Â±ïÁ§∫Á≠âÂ≠ê‰ªªÂä°ÔºåÈááÁî®‰ªªÂä°Ëá™ÈÄÇÂ∫îËØÑÂàÜÁ≥ªÁªüËøõË°åËØÑ‰º∞„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂ∞ΩÁÆ°Áªü‰∏ÄÂ§öÊ®°ÊÄÅÊ®°ÂûãÂú®ÁîüÊàêËá™ÁÑ∂ÂõæÂÉèÂíåÊîØÊåÅÂ§öÊ®°ÊÄÅÊé®ÁêÜÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®ËÆ°ÁÆóÊú∫‰ΩøÁî®ËßÑÂàí‰ªªÂä°‰∏≠ÁöÑÊΩúÂäõ‰ªçÊú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Á¥¢„ÄÇÈÄöËøáÂÆûÈ™åÔºåÊàë‰ª¨Êè≠Á§∫‰∫ÜÂΩìÂâçÊ®°ÂûãÁöÑÂ±ÄÈôêÊÄßÔºåÂπ∂‰∏∫Êú™Êù•ÁöÑÁ†îÁ©∂Êèê‰æõ‰∫ÜÊú∫‰ºö„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02581', 'title': 'QuantLRM: Quantization of Large Reasoning Models via Fine-Tuning Signals', 'url': 'https://huggingface.co/papers/2602.02581', 'abstract': 'QuantLRM uses weight update magnitude signals from fine-tuning to improve quantization of Large Reasoning Models, achieving better performance than traditional methods through channel importance estimation.  \t\t\t\t\tAI-generated summary \t\t\t\t Weight-only quantization is important for compressing Large Language Models (LLMs). Inspired by the spirit of classical magnitude pruning, we study whether the magnitude of weight updates during reasoning-incentivized fine-tuning can provide valuable signals for quantizing Large Reasoning Models (LRMs). We hypothesize that the smallest and largest weight updates during fine-tuning are more important than those of intermediate magnitude, a phenomenon we term "protecting both ends". Upon hypothesis validation, we introduce QuantLRM, which stands for weight quantization of LRMs via fine-tuning signals. We fit simple restricted quadratic functions on weight updates to protect both ends. By multiplying the average quadratic values with the count of zero weight updates of channels, we compute channel importance that is more effective than using activation or second-order information. We run QuantLRM to quantize various fine-tuned models (including supervised, direct preference optimization, and reinforcement learning fine-tuning) over four reasoning benchmarks (AIME-120, FOLIO, temporal sequences, and GPQA-Diamond) and empirically find that QuantLRM delivers a consistent improvement for LRMs quantization, with an average improvement of 6.55% on a reinforcement learning fine-tuned model. Also supporting non-fine-tuned LRMs, QuantLRM gathers effective signals via pseudo-fine-tuning, which greatly enhances its applicability.', 'score': 5, 'issue_id': 965, 'pub_date': '2026-01-31', 'pub_date_card': {'ru': '31 —è–Ω–≤–∞—Ä—è', 'en': 'January 31', 'zh': '1Êúà31Êó•'}, 'hash': 'a21d4b9f7f84c154', 'authors': ['Nan Zhang', 'Eugene Kwek', 'Yusen Zhang', 'Muyu Pan', 'Suhang Wang', 'Prasenjit Mitra', 'Rui Zhang'], 'affiliations': ['IBM Research', 'Pennsylvania State University'], 'pdf_title_img': 'assets/pdf/title_img/2602.02581.jpg', 'data': {'categories': ['#optimization', '#inference', '#benchmark', '#training', '#reasoning'], 'emoji': '‚öñÔ∏è', 'ru': {'title': '–ó–∞—â–∏—Ç–∞ –æ–±–æ–∏—Ö –∫–æ–Ω—Ü–æ–≤: –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ —Å–∏–≥–Ω–∞–ª—ã fine-tuning', 'desc': 'QuantLRM ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–µ–ª–∏—á–∏–Ω–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –≤–µ—Å–æ–≤ –≤–æ –≤—Ä–µ–º—è fine-tuning –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –∫–∞–Ω–∞–ª–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏–ª–∏, —á—Ç–æ –Ω–∞–∏–±–æ–ª–µ–µ –∏ –Ω–∞–∏–º–µ–Ω–µ–µ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤–µ—Å–æ–≤ –±–æ–ª–µ–µ –≤–∞–∂–Ω—ã –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏, —á–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –ø–æ–¥–±–∏—Ä–∞–µ—Ç –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –≤–µ—Å–æ–≤ –∏ –≤—ã—á–∏—Å–ª—è–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –∫–∞–Ω–∞–ª–∞ —á–µ—Ä–µ–∑ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω—É–ª–µ–≤—ã—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π, —á—Ç–æ –æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ, —á–µ–º —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–∫—Ç–∏–≤–∞—Ü–∏–π –∏–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤—Ç–æ—Ä–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞. QuantLRM –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞ 6.55% –ø—Ä–∏ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π —Å reinforcement learning fine-tuning –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–±–æ—Ç—É —Å –Ω–µ–º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ pseudo-fine-tuning.'}, 'en': {'title': 'Enhancing LRM Quantization with Fine-Tuning Signals', 'desc': 'QuantLRM is a novel approach that enhances the quantization of Large Reasoning Models (LRMs) by utilizing weight update signals from fine-tuning. It operates on the principle that the magnitude of weight updates during fine-tuning can indicate the importance of different channels, particularly focusing on the extremes of these updates. By applying a quadratic function to these weight updates, QuantLRM effectively estimates channel importance, leading to better performance than traditional quantization methods. The method has been validated across various reasoning benchmarks, showing an average performance improvement of 6.55% for reinforcement learning fine-tuned models.'}, 'zh': {'title': 'ÈáèÂåñÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'QuantLRMÊòØ‰∏ÄÁßçÈÄöËøáÂæÆË∞É‰ø°Âè∑Êù•ÊîπËøõÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÈáèÂåñÁöÑÊñπÊ≥ï„ÄÇÂÆÉÂà©Áî®ÊùÉÈáçÊõ¥Êñ∞ÁöÑÂπÖÂ∫¶‰ø°Âè∑Ôºå‰º∞ËÆ°ÈÄöÈÅìÁöÑÈáçË¶ÅÊÄßÔºå‰ªéËÄåÂÆûÁé∞ÊØî‰º†ÁªüÊñπÊ≥ïÊõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‚Äú‰øùÊä§‰∏§Á´Ø‚ÄùÁöÑÂÅáËÆæÔºåËÆ§‰∏∫ÂæÆË∞ÉËøáÁ®ã‰∏≠ÊúÄÂ∞èÂíåÊúÄÂ§ßÊùÉÈáçÊõ¥Êñ∞ÊØî‰∏≠Èó¥ÂÄºÊõ¥ÈáçË¶Å„ÄÇÈÄöËøáËøôÁßçÊñπÊ≥ïÔºåQuantLRMÂú®Â§ö‰∏™Êé®ÁêÜÂü∫ÂáÜ‰∏äÂÆûÁé∞‰∫ÜÈáèÂåñÁöÑ‰∏ÄËá¥ÊÄßÊèêÂçáÔºåÂπ≥ÂùáÊèêÈ´ò‰∫Ü6.55%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.06176', 'title': 'Large Language Model Reasoning Failures', 'url': 'https://huggingface.co/papers/2602.06176', 'abstract': 'Large language models exhibit significant reasoning failures that can be categorized into embodied and non-embodied types, with fundamental, application-specific, and robustness-related subtypes, requiring systematic analysis and mitigation strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities. We additionally release a comprehensive collection of research works on LLM reasoning failures, as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures, to provide an easy entry point to this area.', 'score': 4, 'issue_id': 976, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '4e9f3eb111d4a9c1', 'authors': ['Peiyang Song', 'Pengrui Han', 'Noah Goodman'], 'affiliations': ['California Institute of Technology', 'Carleton College', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2602.06176.jpg', 'data': {'categories': ['#survey', '#reasoning', '#open_source'], 'emoji': 'üß†', 'ru': {'title': '–°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –æ—à–∏–±–æ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∞–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –ø—Ä–µ–¥–ª–æ–∂–∏–≤ –Ω–æ–≤—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–ª–∏—á–∞–µ—Ç –≤–æ–ø–ª–æ—â—ë–Ω–Ω—ã–µ –∏ –Ω–µ–≤–æ–ø–ª–æ—â—ë–Ω–Ω—ã–µ —Ç–∏–ø—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –û—à–∏–±–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Ä–∞–∑–¥–µ–ª—è—é—Ç—Å—è –Ω–∞ —Ç—Ä–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π –º–æ–¥–µ–ª–∏; —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π; –∏ –ø—Ä–æ–±–ª–µ–º—ã —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –º–∞–ª—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –æ—à–∏–±–∫–∏ –∞–≤—Ç–æ—Ä—ã –¥–∞—é—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –ø—Ä–∏—á–∏–Ω—ã –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏—Ö —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–±–æ—Ç—ã –ø–æ–º–æ–≥–∞—é—Ç –ª—É—á—à–µ –ø–æ–Ω—è—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Å–ª–∞–±–æ—Å—Ç–∏ –≤ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è—Ö LLM –∫ –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É –º—ã—à–ª–µ–Ω–∏—é –∏ –æ—Ç–∫—Ä—ã–≤–∞—é—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Understanding and Mitigating Reasoning Failures in LLMs', 'desc': 'This paper investigates the reasoning failures of Large Language Models (LLMs), categorizing them into embodied and non-embodied types. Non-embodied reasoning is further divided into informal and formal reasoning, while failures are classified into fundamental, application-specific, and robustness-related issues. The authors provide definitions, analyze existing research, and suggest strategies to mitigate these failures. By organizing this information, the paper aims to enhance understanding and improve the reliability of LLMs in reasoning tasks.'}, 'zh': {'title': 'Á≥ªÁªüÂàÜÊûêÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜÂ§±Ë¥•', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Êé®ÁêÜÊñπÈù¢Ë°®Áé∞Âá∫ÊòæËëóÁöÑÂ§±Ë¥•ÔºåËøô‰∫õÂ§±Ë¥•ÂèØ‰ª•ÂàÜ‰∏∫ÂÖ∑Ë∫´ÂíåÈùûÂÖ∑Ë∫´Á±ªÂûã„ÄÇÈùûÂÖ∑Ë∫´Êé®ÁêÜÂèàÂèØ‰ª•ÁªÜÂàÜ‰∏∫Áõ¥ËßÇÊé®ÁêÜÂíåÈÄªËæëÊé®ÁêÜ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂàÜÁ±ªÊ°ÜÊû∂ÔºåÁ≥ªÁªüÂàÜÊûêËøô‰∫õÊé®ÁêÜÂ§±Ë¥•ÁöÑÊ†πÊú¨ÂéüÂõ†ÔºåÂπ∂Êèê‰æõÁõ∏Â∫îÁöÑÁºìËß£Á≠ñÁï•„ÄÇÈÄöËøáÊï¥ÂêàÁé∞ÊúâÁ†îÁ©∂ÔºåÊàë‰ª¨‰∏∫ÁêÜËß£LLMsÁöÑÊé®ÁêÜÂº±ÁÇπÊèê‰æõ‰∫ÜÁªìÊûÑÂåñÁöÑËßÜËßíÔºåÊé®Âä®Êú™Êù•Á†îÁ©∂ÁöÑËøõÂ±ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05847', 'title': 'OmniVideo-R1: Reinforcing Audio-visual Reasoning with Query Intention and Modality Attention', 'url': 'https://huggingface.co/papers/2602.05847', 'abstract': 'OmniVideo-R1 enhances audio-visual understanding through reinforced frameworks that integrate self-supervised and contrastive learning for multimodal reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t While humans perceive the world through diverse modalities that operate synergistically to support a holistic understanding of their surroundings, existing omnivideo models still face substantial challenges on audio-visual understanding tasks. In this paper, we propose OmniVideo-R1, a novel reinforced framework that improves mixed-modality reasoning. OmniVideo-R1 empowers models to "think with omnimodal cues" by two key strategies: (1) query-intensive grounding based on self-supervised learning paradigms; and (2) modality-attentive fusion built upon contrastive learning paradigms. Extensive experiments on multiple benchmarks demonstrate that OmniVideo-R1 consistently outperforms strong baselines, highlighting its effectiveness and robust generalization capabilities.', 'score': 4, 'issue_id': 968, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '112491ce815401a9', 'authors': ['Zhangquan Chen', 'Jiale Tao', 'Ruihuang Li', 'Yihao Hu', 'Ruitao Chen', 'Zhantao Yang', 'Xinlei Yu', 'Haodong Jing', 'Manyuan Zhang', 'Shuai Shao', 'Biao Wang', 'Qinglin Lu', 'Ruqi Huang'], 'affiliations': ['CUHK', 'HNU', 'NUS', 'THU', 'Tencent HY', 'XJTU'], 'pdf_title_img': 'assets/pdf/title_img/2602.05847.jpg', 'data': {'categories': ['#training', '#video', '#rl', '#multimodal', '#audio'], 'emoji': 'üé¨', 'ru': {'title': '–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∑–≤—É–∫–∞ –∏ –≤–∏–¥–µ–æ', 'desc': 'OmniVideo-R1 ‚Äî —ç—Ç–æ —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–≤—É–∫–æ–≤—ã–µ –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–∞ –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–¥—Ö–æ–¥–∞: —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π. –§—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å, –∏—Å–ø–æ–ª—å–∑—É—è –ø–æ–¥—Å–∫–∞–∑–∫–∏ –∏–∑ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –≤–∏–¥–µ–æ. –ù–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ö–æ—Ä–æ—à—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –æ–±–æ–±—â–∞—Ç—å—Å—è –Ω–∞ –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ.'}, 'en': {'title': 'Empowering Multimodal Reasoning with OmniVideo-R1', 'desc': 'OmniVideo-R1 is a new framework designed to enhance how machines understand audio and visual information together. It uses self-supervised learning to help models focus on important cues in the data, and contrastive learning to effectively combine different types of information. This approach allows the model to reason better across multiple modalities, similar to how humans process sensory information. Experiments show that OmniVideo-R1 performs better than existing models on various tasks, proving its strength and adaptability.'}, 'zh': {'title': 'OmniVideo-R1ÔºöÊèêÂçáÈü≥ËßÜÈ¢ëÁêÜËß£ÁöÑÊñ∞Ê°ÜÊû∂', 'desc': 'OmniVideo-R1 ÊòØ‰∏ÄÁßçÂ¢ûÂº∫Èü≥ËßÜÈ¢ëÁêÜËß£ÁöÑÊ°ÜÊû∂ÔºåÁªìÂêà‰∫ÜËá™ÁõëÁù£Â≠¶‰π†ÂíåÂØπÊØîÂ≠¶‰π†ÁöÑÊñπÊ≥ï„ÄÇËØ•Ê®°ÂûãÈÄöËøáÊü•ËØ¢ÂØÜÈõÜÁöÑËá™ÁõëÁù£Â≠¶‰π†ÂíåÂü∫‰∫éÂØπÊØîÂ≠¶‰π†ÁöÑÊ®°ÊÄÅÊ≥®ÊÑèÂäõËûçÂêàÔºåÊèêÂçá‰∫ÜÂ§öÊ®°ÊÄÅÊé®ÁêÜËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåOmniVideo-R1 Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÂº∫Âü∫Á∫øÔºåÊòæÁ§∫Âá∫ÂÖ∂ÊúâÊïàÊÄßÂíåÂº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÊ≠§Á†îÁ©∂‰∏∫Èü≥ËßÜÈ¢ëÁêÜËß£‰ªªÂä°Êèê‰æõ‰∫ÜÊñ∞ÁöÑÊÄùË∑ØÂíåÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05711', 'title': 'OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale', 'url': 'https://huggingface.co/papers/2602.05711', 'abstract': 'OmniMoE presents a system-algorithm co-designed framework that achieves fine-grained expert specialization in Mixture-of-Experts architectures through vector-level atomic experts and optimized routing and scheduling mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Mixture-of-Experts (MoE) architectures are evolving towards finer granularity to improve parameter efficiency. However, existing MoE designs face an inherent trade-off between the granularity of expert specialization and hardware execution efficiency. We propose OmniMoE, a system-algorithm co-designed framework that pushes expert granularity to its logical extreme. OmniMoE introduces vector-level Atomic Experts, enabling scalable routing and execution within a single MoE layer, while retaining a shared dense MLP branch for general-purpose processing. Although this atomic design maximizes capacity, it poses severe challenges for routing complexity and memory access. To address these, OmniMoE adopts a system-algorithm co-design: (i) a Cartesian Product Router that decomposes the massive index space to reduce routing complexity from O(N) to O(sqrt(N)); and (ii) Expert-Centric Scheduling that inverts the execution order to turn scattered, memory-bound lookups into efficient dense matrix operations. Validated on seven benchmarks, OmniMoE (with 1.7B active parameters) achieves 50.9% zero-shot accuracy across seven benchmarks, outperforming coarse-grained (e.g., DeepSeekMoE) and fine-grained (e.g., PEER) baselines. Crucially, OmniMoE reduces inference latency from 73ms to 6.7ms (a 10.9-fold speedup) compared to PEER, demonstrating that massive-scale fine-grained MoE can be fast and accurate. Our code is open-sourced at https://github.com/flash-algo/omni-moe.', 'score': 4, 'issue_id': 962, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '688adfd08d5f60d2', 'authors': ['Jingze Shi', 'Zhangyang Peng', 'Yizhang Zhu', 'Yifan Wu', 'Guang Liu', 'Yuyu Luo'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2602.05711.jpg', 'data': {'categories': ['#inference', '#training', '#architecture'], 'emoji': '‚ö°', 'ru': {'title': '–ê—Ç–æ–º–∞—Ä–Ω—ã–µ —ç–∫—Å–ø–µ—Ä—Ç—ã: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏', 'desc': 'OmniMoE –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º—É, –∫–æ—Ç–æ—Ä–∞—è –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Mixture-of-Experts –±–ª–∞–≥–æ–¥–∞—Ä—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –∞—Ç–æ–º–∞—Ä–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –Ω–∞ —É—Ä–æ–≤–Ω–µ –≤–µ–∫—Ç–æ—Ä–æ–≤. –î–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ –∏ –¥–æ—Å—Ç—É–ø–∞ –≤ –ø–∞–º—è—Ç–∏ –∞–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –∏ —Å–∏—Å—Ç–µ–º—ã: –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä –¥–µ–∫–∞—Ä—Ç–æ–≤–∞ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è —Å–Ω–∏–∂–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ —Å O(N) –¥–æ O(‚àöN), –∞ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ä–∞—Å—Å–µ—è–Ω–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –º–∞—Ç—Ä–∏—á–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è. –ù–∞ —Å–µ–º–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö OmniMoE —Å 1,7B –∞–∫—Ç–∏–≤–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç 50,9% —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ zero-shot —Ä–µ–∂–∏–º–µ –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∞–Ω–∞–ª–æ–≥–∏ –∫–∞–∫ —Å –∫—Ä—É–ø–Ω–æ–π, —Ç–∞–∫ –∏ —Å –º–µ–ª–∫–æ–π –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π. –°–∏—Å—Ç–µ–º–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç 10,9-–∫—Ä–∞—Ç–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤—ã–≤–æ–¥–∞ (—Å 73ms –¥–æ 6,7ms) –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å PEER, –¥–æ–∫–∞–∑—ã–≤–∞—è, —á—Ç–æ –º–∞—Å—à—Ç–∞–±–Ω–∞—è fine-grained MoE –º–æ–∂–µ—Ç –±—ã—Ç—å –±—ã—Å—Ç—Ä–æ–π –∏ —Ç–æ—á–Ω–æ–π.'}, 'en': {'title': 'Unlocking Speed and Precision in Mixture-of-Experts with OmniMoE', 'desc': 'OmniMoE is a new framework designed to enhance Mixture-of-Experts (MoE) architectures by achieving fine-grained expert specialization. It introduces vector-level Atomic Experts, which allow for more efficient routing and execution within a single MoE layer while maintaining a shared dense MLP for general tasks. The framework addresses challenges in routing complexity and memory access through innovative techniques like the Cartesian Product Router and Expert-Centric Scheduling. As a result, OmniMoE significantly improves accuracy and reduces inference latency, demonstrating that fine-grained MoE can be both fast and effective.'}, 'zh': {'title': 'OmniMoEÔºöÁªÜÁ≤íÂ∫¶‰∏ìÂÆ∂ÁöÑÈ´òÊïàÂÆûÁé∞', 'desc': 'OmniMoEÊòØ‰∏ÄÁßçÁ≥ªÁªü‰∏éÁÆóÊ≥ïÂÖ±ÂêåËÆæËÆ°ÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂêëÈáèÁ∫ßÂéüÂ≠ê‰∏ìÂÆ∂ÂÆûÁé∞Ê∑∑Âêà‰∏ìÂÆ∂Êû∂ÊûÑ‰∏≠ÁöÑÁªÜÁ≤íÂ∫¶‰∏ìÂÆ∂‰∏ì‰∏öÂåñ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøá‰ºòÂåñË∑ØÁî±ÂíåË∞ÉÂ∫¶Êú∫Âà∂ÔºåÂÖãÊúç‰∫Ü‰∏ìÂÆ∂‰∏ì‰∏öÂåñÁ≤íÂ∫¶‰∏éÁ°¨‰ª∂ÊâßË°åÊïàÁéá‰πãÈó¥ÁöÑÊùÉË°°„ÄÇOmniMoEÂºïÂÖ•‰∫ÜÂéüÂ≠ê‰∏ìÂÆ∂Ôºå‰ΩøÂæóÂú®Âçï‰∏™MoEÂ±ÇÂÜÖÂÆûÁé∞ÂèØÊâ©Â±ïÁöÑË∑ØÁî±ÂíåÊâßË°åÔºåÂêåÊó∂‰øùÁïô‰∫ÜÁî®‰∫éÈÄöÁî®Â§ÑÁêÜÁöÑÂÖ±‰∫´ÂØÜÈõÜMLPÂàÜÊîØ„ÄÇÁªèËøá‰∏É‰∏™Âü∫ÂáÜÊµãËØïÈ™åËØÅÔºåOmniMoEÂú®Êé®ÁêÜÂª∂ËøüÂíåÂáÜÁ°ÆÊÄß‰∏äÂùáË°®Áé∞‰ºòÂºÇÔºåÊòæÁ§∫Âá∫Â§ßËßÑÊ®°ÁªÜÁ≤íÂ∫¶MoEÁöÑÂø´ÈÄü‰∏éÂáÜÁ°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04837', 'title': 'Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing', 'url': 'https://huggingface.co/papers/2602.04837', 'abstract': 'Group-Evolving Agents enable open-ended self-improvement by treating groups of agents as evolutionary units, allowing efficient experience sharing and reuse to enhance coding performance and robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.', 'score': 4, 'issue_id': 965, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '5f9249cbfedb151c', 'authors': ['Zhaotian Weng', 'Antonis Antoniades', 'Deepak Nathani', 'Zhen Zhang', 'Xiao Pu', 'Xin Eric Wang'], 'affiliations': ['University of California, Santa Barbara'], 'pdf_title_img': 'assets/pdf/title_img/2602.04837.jpg', 'data': {'categories': ['#benchmark', '#plp', '#training', '#agents'], 'emoji': 'üß¨', 'ru': {'title': '–ì—Ä—É–ø–ø–æ–≤–∞—è —ç–≤–æ–ª—é—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ Group-Evolving Agents (GEA) –¥–ª—è –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –≥—Ä—É–ø–ø—É –∞–≥–µ–Ω—Ç–æ–≤ –∫–∞–∫ –µ–¥–∏–Ω—É—é —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—É—é –µ–¥–∏–Ω–∏—Ü—É. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –¥—Ä–µ–≤–æ–≤–∏–¥–Ω–æ–π —ç–≤–æ–ª—é—Ü–∏–∏, GEA –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –æ–±–º–µ–Ω –æ–ø—ã—Ç–æ–º –≤–Ω—É—Ç—Ä–∏ –≥—Ä—É–ø–ø—ã, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏ –∏–∑–±–µ–∂–∞—Ç—å –∏–∑–æ–ª—è—Ü–∏–∏ —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã—Ö –≤–µ—Ç–≤–µ–π. –ú–µ—Ç–æ–¥ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ —Å–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è (71.0% –Ω–∞ SWE-bench Verified –ø—Ä–æ—Ç–∏–≤ 56.7% —É –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤). GEA —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ö–æ—Ä–æ—à—É—é —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–∞–±–µ–ª—å–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏ –∏ –≤—ã—Å–æ–∫—É—é —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –æ—à–∏–±–∫–∞–º, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É—è —Ä–∞–Ω–Ω—é—é –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫—É—é –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å.'}, 'en': {'title': 'Empowering Agents Through Group Evolution', 'desc': 'Group-Evolving Agents (GEA) represent a novel approach in machine learning that allows groups of agents to evolve together, enhancing their ability to share and reuse experiences. This method enables agents to autonomously adapt their structures, leading to improved performance without heavy reliance on human input. GEA outperforms traditional self-evolving methods by effectively utilizing exploratory diversity, resulting in better coding capabilities and robustness. The framework demonstrates significant advancements in coding benchmarks, showcasing its ability to achieve long-term progress and adaptability across various models.'}, 'zh': {'title': 'Áæ§‰ΩìËøõÂåñ‰ª£ÁêÜÔºöÂºÄÂêØËá™ÊàëÊîπËøõÁöÑÊñ∞Á∫™ÂÖÉ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂºÄÊîæÂºèËá™ÊàëÊîπËøõ‰ª£ÁêÜÊ®°ÂûãÔºåÁß∞‰∏∫Áæ§‰ΩìËøõÂåñ‰ª£ÁêÜÔºàGEAÔºâ„ÄÇGEAÂ∞Ü‰ª£ÁêÜÁªÑËßÜ‰∏∫Âü∫Êú¨ÁöÑËøõÂåñÂçïÂÖÉÔºåÂÖÅËÆ∏Âú®ËøõÂåñËøáÁ®ã‰∏≠ÂÆûÁé∞ÁªèÈ™åÁöÑÊòæÂºèÂÖ±‰∫´ÂíåÈáçÁî®Ôºå‰ªéËÄåÊèêÈ´òÁºñÁ†ÅÊÄßËÉΩÂíåÈ≤ÅÊ£íÊÄß„ÄÇ‰∏éÁé∞ÊúâÁöÑÊ†ëÁä∂ËøõÂåñÊñπÊ≥ï‰∏çÂêåÔºåGEAÂÖãÊúç‰∫ÜÁî±‰∫éÂ≠§Á´ãËøõÂåñÂàÜÊîØÂØºËá¥ÁöÑÊé¢Á¥¢Â§öÊ†∑ÊÄßÂà©Áî®ÊïàÁéá‰Ωé‰∏ãÁöÑÈóÆÈ¢ò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGEAÂú®Â§ö‰∏™ÁºñÁ†ÅÂü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóË∂ÖË∂ä‰∫ÜÊúÄÂÖàËøõÁöÑËá™ÊàëËøõÂåñÊñπÊ≥ïÔºåÂπ∂Âú®‰∏çÂêåÁºñÁ†ÅÊ®°Âûã‰∏≠Ë°®Áé∞Âá∫‰∏ÄËá¥ÁöÑÂèØËøÅÁßªÊÄßÂíåÊõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.06854', 'title': 'SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks', 'url': 'https://huggingface.co/papers/2602.06854', 'abstract': 'A novel framework called SEMA is introduced that effectively trains multi-turn attackers for large language models without relying on existing strategies or external data, achieving state-of-the-art attack success rates while being compact, reproducible, and transferable across different models and datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-turn jailbreaks capture the real threat model for safety-aligned chatbots, where single-turn attacks are merely a special case. Yet existing approaches break under exploration complexity and intent drift. We propose SEMA, a simple yet effective framework that trains a multi-turn attacker without relying on any existing strategies or external data. SEMA comprises two stages. Prefilling self-tuning enables usable rollouts by fine-tuning on non-refusal, well-structured, multi-turn adversarial prompts that are self-generated with a minimal prefix, thereby stabilizing subsequent learning. Reinforcement learning with intent-drift-aware reward trains the attacker to elicit valid multi-turn adversarial prompts while maintaining the same harmful objective. We anchor harmful intent in multi-turn jailbreaks via an intent-drift-aware reward that combines intent alignment, compliance risk, and level of detail. Our open-loop attack regime avoids dependence on victim feedback, unifies single- and multi-turn settings, and reduces exploration complexity. Across multiple datasets, victim models, and jailbreak judges, our method achieves state-of-the-art (SOTA) attack success rates (ASR), outperforming all single-turn baselines, manually scripted and template-driven multi-turn baselines, as well as our SFT (Supervised Fine-Tuning) and DPO (Direct Preference Optimization) variants. For instance, SEMA performs an average 80.1% ASR@1 across three closed-source and open-source victim models on AdvBench, 33.9% over SOTA. The approach is compact, reproducible, and transfers across targets, providing a stronger and more realistic stress test for large language model (LLM) safety and enabling automatic redteaming to expose and localize failure modes. Our code is available at: https://github.com/fmmarkmq/SEMA.', 'score': 3, 'issue_id': 963, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '5545fb68273ad4d6', 'authors': ['Mingqian Feng', 'Xiaodong Liu', 'Weiwei Yang', 'Jialin Song', 'Xuekai Zhu', 'Chenliang Xu', 'Jianfeng Gao'], 'affiliations': ['Microsoft Research', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2602.06854.jpg', 'data': {'categories': ['#training', '#rl', '#alignment', '#security', '#open_source'], 'emoji': 'üéØ', 'ru': {'title': '–ú–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã–µ –∞—Ç–∞–∫–∏ –Ω–∞ LLM —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ SEMA –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –∞—Ç–∞–∫ –Ω–∞ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∏ –≤–Ω–µ—à–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö —ç—Ç–∞–ø–æ–≤: –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å —Å–∞–º–æ–Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —É—Å—Ç–æ–π—á–∏–≤—ã—Ö –Ω–µ—É–¥–∞—á–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å —É—á—ë—Ç–æ–º –¥—Ä–µ–π—Ñ–∞ –∏–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ–π —Ü–µ–ª–∏. –°–∏—Å—Ç–µ–º–∞ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏—Å–∫—É—Å—Å—Ç–≤–∞ –≤ —á–∞—Å—Ç–æ—Ç–µ —É—Å–ø–µ—à–Ω—ã—Ö –∞—Ç–∞–∫ (80.1% –Ω–∞ AdvBench), –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –æ–¥–Ω–æ–æ—Ö–æ–¥–æ–≤—ã–µ –∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã–µ –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã. –ü–æ–¥—Ö–æ–¥ –∫–æ–º–ø–∞–∫—Ç–µ–Ω, –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º –∏ —Ö–æ—Ä–æ—à–æ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—Å—è –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏-–∂–µ—Ä—Ç–≤–∞–º–∏.'}, 'en': {'title': 'SEMA: Revolutionizing Multi-Turn Attacks for Safer Language Models', 'desc': 'The paper introduces SEMA, a new framework designed to train multi-turn attackers for large language models (LLMs) without using existing strategies or external data. It addresses the limitations of previous methods that struggle with exploration complexity and intent drift by employing a two-stage process: prefilling self-tuning and reinforcement learning with intent-drift-aware rewards. SEMA achieves state-of-the-art attack success rates by effectively generating adversarial prompts that maintain harmful intent across multiple turns. This approach is compact, reproducible, and transferable, making it a robust tool for testing the safety of LLMs.'}, 'zh': {'title': 'SEMAÔºöÂ§öËΩÆÊîªÂáªÁöÑÊñ∞Ê°ÜÊû∂ÔºåÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÂÆâÂÖ®ÊÄß', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫SEMAÁöÑÊñ∞Ê°ÜÊû∂ÔºåÊó®Âú®ÊúâÊïàËÆ≠ÁªÉÂ§öËΩÆÊîªÂáªËÄÖÔºå‰ª•Â∫îÂØπÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄßÈóÆÈ¢ò„ÄÇSEMA‰∏ç‰æùËµñ‰∫éÁé∞ÊúâÁ≠ñÁï•ÊàñÂ§ñÈÉ®Êï∞ÊçÆÔºåËÉΩÂ§üÂú®‰∏çÂêåÊ®°ÂûãÂíåÊï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞ÊúÄÂÖàËøõÁöÑÊîªÂáªÊàêÂäüÁéá„ÄÇËØ•Ê°ÜÊû∂ÂåÖÊã¨‰∏§‰∏™Èò∂ÊÆµÔºöËá™ÊàëË∞É‰ºòÁöÑÈ¢ÑÂ°´ÂÖÖÂíåÂü∫‰∫éÊÑèÂõæÊºÇÁßªÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºåÁ°Æ‰øùÁîüÊàêÊúâÊïàÁöÑÂ§öËΩÆÂØπÊäóÊèêÁ§∫„ÄÇÈÄöËøáËøôÁßçÊñπÊ≥ïÔºåSEMAÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜÂíåÊ®°Âûã‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÊèê‰æõ‰∫ÜÊõ¥Âº∫Â§ßÂíåÁé∞ÂÆûÁöÑÂéãÂäõÊµãËØïÔºå‰øÉËøõ‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄßÁ†îÁ©∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.06471', 'title': 'Revisiting the Shape Convention of Transformer Language Models', 'url': 'https://huggingface.co/papers/2602.06471', 'abstract': 'Replacing conventional feed-forward networks with hourglass-shaped MLPs in Transformers improves model efficiency and performance by enabling better parameter utilization and competitive scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Dense Transformer language models have largely adhered to one consistent architectural shape: each layer consists of an attention module followed by a feed-forward network (FFN) with a narrow-wide-narrow MLP, allocating most parameters to the MLP at expansion ratios between 2 and 4. Motivated by recent results that residual wide-narrow-wide (hourglass) MLPs offer superior function approximation capabilities, we revisit the long-standing MLP shape convention in Transformer, challenging the necessity of the narrow-wide-narrow design. To study this, we develop a Transformer variant that replaces the conventional FFN with a deeper hourglass-shaped FFN, comprising a stack of hourglass sub-MLPs connected by residual pathways. We posit that a deeper but lighter hourglass FFN can serve as a competitive alternative to the conventional FFN, and that parameters saved by using a lighter hourglass FFN can be more effectively utilized, such as by enlarging model hidden dimensions under fixed budgets. We confirm these through empirical validations across model scales: hourglass FFNs outperform conventional FFNs up to 400M and achieve comparable performance at larger scales to 1B parameters; hourglass FFN variants with reduced FFN and increased attention parameters show consistent improvements over conventional configurations at matched budgets. Together, these findings shed new light on recent work and prompt a rethinking of the narrow-wide-narrow MLP convention and the balance between attention and FFN towards efficient and expressive modern language models.', 'score': 3, 'issue_id': 962, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '766c634e6c31c129', 'authors': ['Feng-Ting Liao', 'Meng-Hsi Chen', 'Guan-Ting Yi', 'Da-shan Shiu'], 'affiliations': ['MediaTek Research', 'National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2602.06471.jpg', 'data': {'categories': ['#optimization'], 'emoji': '‚è≥', 'ru': {'title': '–ü–µ—Å–æ—á–Ω—ã–µ —á–∞—Å—ã –≤–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–∏: –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∑–∞–º–µ–Ω–∏—Ç—å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–µ—Ç–∏ (FFN) –≤ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö –Ω–∞ –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏–µ —Å–µ—Ç–∏ –≤ —Ñ–æ—Ä–º–µ –ø–µ—Å–æ—á–Ω—ã—Ö —á–∞—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ-—É–∑–∫–æ-—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è (hourglass) –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ú–õ–ü –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ª—É—á—à—É—é –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—é —Ñ—É–Ω–∫—Ü–∏–π –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–π —É–∑–∫–æ-—à–∏—Ä–æ–∫–æ-—É–∑–∫–æ–π —Å—Ö–µ–º–æ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø–µ—Å–æ—á–Ω—ã—Ö —á–∞—Å–æ–≤ –Ω–∞ –º–æ–¥–µ–ª—è—Ö –¥–æ 400 –º–ª–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, —Å—ç–∫–æ–Ω–æ–º–ª–µ–Ω–Ω—ã–µ –Ω–∞ FFN, –º–æ–∂–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –ø–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≤ —Å–ª–æ–∏ –≤–Ω–∏–º–∞–Ω–∏—è. –≠—Ç–∏ –Ω–∞—Ö–æ–¥–∫–∏ –ø–µ—Ä–µ–æ—Ü–µ–Ω–∏–≤–∞—é—Ç –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –≤–Ω–∏–º–∞–Ω–∏—è –∏ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–º–∏ —Å–ª–æ—è–º–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –∏ –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Rethinking MLP Shapes for Efficient Transformers', 'desc': 'This paper explores the use of hourglass-shaped multi-layer perceptrons (MLPs) in Transformers, replacing the traditional narrow-wide-narrow feed-forward networks (FFNs). The authors argue that hourglass MLPs provide better parameter efficiency and performance by allowing for deeper architectures with effective residual connections. Empirical results show that hourglass FFNs outperform conventional designs in smaller models and maintain competitive performance in larger models. This research encourages a reevaluation of the standard MLP structure in Transformers, promoting a more efficient balance between attention mechanisms and feed-forward networks.'}, 'zh': {'title': 'Áî®Â∞èÊó∂glass MLPÊèêÂçáTransformerÊïàÁéá‰∏éÊÄßËÉΩ', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑTransformerÊû∂ÊûÑÔºå‰ΩøÁî®Â∞èÊó∂glassÂΩ¢Áä∂ÁöÑÂ§öÂ±ÇÊÑüÁü•Êú∫ÔºàMLPÔºâÊõø‰ª£‰º†ÁªüÁöÑÂâçÈ¶àÁΩëÁªúÔºàFFNÔºâÔºå‰ª•ÊèêÈ´òÊ®°ÂûãÁöÑÊïàÁéáÂíåÊÄßËÉΩ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂ∞èÊó∂glass MLPÂú®ÂèÇÊï∞Âà©Áî®ÁéáÂíåÂäüËÉΩÈÄºËøëËÉΩÂäõ‰∏ä‰ºò‰∫é‰º†ÁªüÁöÑÁ™Ñ-ÂÆΩ-Á™ÑËÆæËÆ°„ÄÇÈÄöËøáÂÆûÈ™åËØÅÊòéÔºåÂ∞èÊó∂glass FFNÂú®Ê®°ÂûãËßÑÊ®°ËææÂà∞4‰∫øÂèÇÊï∞Êó∂Ë°®Áé∞‰ºò‰∫é‰º†ÁªüFFNÔºåÂπ∂Âú®Êõ¥Â§ßËßÑÊ®°ÔºàÂ¶Ç10‰∫øÂèÇÊï∞ÔºâÊó∂ÊÄßËÉΩÁõ∏ÂΩì„ÄÇËØ•Á†îÁ©∂‰øÉ‰ΩøÊàë‰ª¨ÈáçÊñ∞ÊÄùËÄÉTransformer‰∏≠FFN‰∏éÊ≥®ÊÑèÂäõÊú∫Âà∂‰πãÈó¥ÁöÑÂπ≥Ë°°ÔºåÊé®Âä®Áé∞‰ª£ËØ≠Ë®ÄÊ®°ÂûãÁöÑÈ´òÊïà‰∏éË°®ËææËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03075', 'title': 'ReMiT: RL-Guided Mid-Training for Iterative LLM Evolution', 'url': 'https://huggingface.co/papers/2602.03075', 'abstract': 'ReMiT introduces a bidirectional training approach where reinforcement learning-guided mid-training token reweighting improves large language model pre-training and post-training performance through an iterative feedback loop.  \t\t\t\t\tAI-generated summary \t\t\t\t Standard training pipelines for large language models (LLMs) are typically unidirectional, progressing from pre-training to post-training. However, the potential for a bidirectional process--where insights from post-training retroactively improve the pre-trained foundation--remains unexplored. We aim to establish a self-reinforcing flywheel: a cycle in which reinforcement learning (RL)-tuned model strengthens the base model, which in turn enhances subsequent post-training performance, requiring no specially trained teacher or reference model. To realize this, we analyze training dynamics and identify the mid-training (annealing) phase as a critical turning point for model capabilities. This phase typically occurs at the end of pre-training, utilizing high-quality corpora under a rapidly decaying learning rate. Building upon this insight, we introduce ReMiT (Reinforcement Learning-Guided Mid-Training). Specifically, ReMiT leverages the reasoning priors of RL-tuned models to dynamically reweight tokens during the mid-training phase, prioritizing those pivotal for reasoning. Empirically, ReMiT achieves an average improvement of 3\\% on 10 pre-training benchmarks, spanning math, code, and general reasoning, and sustains these gains by over 2\\% throughout the post-training pipeline. These results validate an iterative feedback loop, enabling continuous and self-reinforcing evolution of LLMs.', 'score': 3, 'issue_id': 968, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': 'd165243c4a9cbfbc', 'authors': ['Junjie Huang', 'Jiarui Qin', 'Di Yin', 'Weiwen Liu', 'Yong Yu', 'Xing Sun', 'Weinan Zhang'], 'affiliations': ['Shanghai Jiao Tong University', 'Tencent Youtu Lab'], 'pdf_title_img': 'assets/pdf/title_img/2602.03075.jpg', 'data': {'categories': ['#optimization', '#training', '#rl', '#reasoning'], 'emoji': 'üîÑ', 'ru': {'title': '–û–±—É—á–µ–Ω–∏–µ –≤ –¥–≤–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è: –∫–∞–∫ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏–µ —É–ª—É—á—à–∞–µ—Ç —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç –º–æ–¥–µ–ª–∏', 'desc': 'ReMiT –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –≥–¥–µ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø–µ—Ä–µweighting —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π —Ñ–∞–∑–µ mid-training. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Ñ–∞–∑–∞ –æ—Ç–∂–∏–≥–∞ –≤ –∫–æ–Ω—Ü–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–≤–ª—è–µ—Ç—Å—è –∫–ª—é—á–µ–≤–æ–π —Ç–æ—á–∫–æ–π –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏–∑ RL-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µweighting —Ç–æ–∫–µ–Ω–æ–≤, –≤—ã–¥–µ–ª—è—è –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ –¥–ª—è –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–≤–æ–¥–∞. –ü–æ–¥—Ö–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞ 3% –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–∏–±—ã–ª—å –±–æ–ª–µ–µ —á–µ–º –Ω–∞ 2% –Ω–∞ —ç—Ç–∞–ø–µ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è, —Å–æ–∑–¥–∞–≤–∞—è —Å–∞–º–æ—É—Å–∏–ª–∏–≤–∞—é—â–∏–π—Å—è —Ü–∏–∫–ª —ç–≤–æ–ª—é—Ü–∏–∏ LLM.'}, 'en': {'title': 'ReMiT: Enhancing LLMs with Bidirectional Training and Reinforcement Learning', 'desc': 'ReMiT presents a novel bidirectional training method that enhances large language models (LLMs) by integrating reinforcement learning (RL) during the mid-training phase. This approach allows insights gained from post-training to retroactively improve the pre-trained model, creating a self-reinforcing feedback loop. By dynamically reweighting tokens based on their importance for reasoning, ReMiT optimizes the training process without needing a separate teacher model. The results show significant performance improvements across various benchmarks, demonstrating the effectiveness of this iterative training strategy.'}, 'zh': {'title': 'ÂèåÂêëËÆ≠ÁªÉÔºåÊåÅÁª≠ÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊÄßËÉΩ', 'desc': 'ReMiTÊèêÂá∫‰∫Ü‰∏ÄÁßçÂèåÂêëËÆ≠ÁªÉÊñπÊ≥ïÔºåÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÊåáÂØºÁöÑ‰∏≠ÊúüËÆ≠ÁªÉ‰ª§ÁâåÈáçÂä†ÊùÉÔºåÊèêÂçá‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÈ¢ÑËÆ≠ÁªÉÂíåÂêéËÆ≠ÁªÉÊÄßËÉΩ„ÄÇ‰º†ÁªüÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËÆ≠ÁªÉÈÄöÂ∏∏ÊòØÂçïÂêëÁöÑÔºå‰ªéÈ¢ÑËÆ≠ÁªÉÂà∞ÂêéËÆ≠ÁªÉÔºåËÄåReMiTÊé¢Á¥¢‰∫ÜÂèåÂêëËøáÁ®ãÁöÑÊΩúÂäõ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂàÜÊûêËÆ≠ÁªÉÂä®ÊÄÅÔºåËØÜÂà´‰∏≠ÊúüËÆ≠ÁªÉÈò∂ÊÆµ‰∏∫Ê®°ÂûãËÉΩÂäõÁöÑÂÖ≥ÈîÆËΩ¨ÊäòÁÇπÔºåÂà©Áî®Âº∫ÂåñÂ≠¶‰π†Ë∞ÉÊï¥Ê®°ÂûãÂú®Ê≠§Èò∂ÊÆµÂä®ÊÄÅÈáçÂä†ÊùÉ‰ª§Áâå„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåReMiTÂú®10‰∏™È¢ÑËÆ≠ÁªÉÂü∫ÂáÜ‰∏äÂπ≥ÂùáÊèêÈ´ò‰∫Ü3%ÔºåÂπ∂Âú®ÂêéËÆ≠ÁªÉËøáÁ®ã‰∏≠‰øùÊåÅ‰∫ÜË∂ÖËøá2%ÁöÑÂ¢ûÁõäÔºåÈ™åËØÅ‰∫ÜËø≠‰ª£ÂèçÈ¶àÂæ™ÁéØÁöÑÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.06883', 'title': 'Vision Transformer Finetuning Benefits from Non-Smooth Components', 'url': 'https://huggingface.co/papers/2602.06883', 'abstract': 'Vision transformer components exhibit varying plasticity levels that correlate with finetuning performance, challenging the assumption that smoothness is always beneficial.  \t\t\t\t\tAI-generated summary \t\t\t\t The smoothness of the transformer architecture has been extensively studied in the context of generalization, training stability, and adversarial robustness. However, its role in transfer learning remains poorly understood. In this paper, we analyze the ability of vision transformer components to adapt their outputs to changes in inputs, or, in other words, their plasticity. Defined as an average rate of change, it captures the sensitivity to input perturbation; in particular, a high plasticity implies low smoothness. We demonstrate through theoretical analysis and comprehensive experiments that this perspective provides principled guidance in choosing the components to prioritize during adaptation. A key takeaway for practitioners is that the high plasticity of the attention modules and feedforward layers consistently leads to better finetuning performance. Our findings depart from the prevailing assumption that smoothness is desirable, offering a novel perspective on the functional properties of transformers. The code is available at https://github.com/ambroiseodt/vit-plasticity.', 'score': 2, 'issue_id': 964, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': 'd40bc4a7cb7a82da', 'authors': ['Ambroise Odonnat', 'Laetitia Chapel', 'Romain Tavenard', 'Ievgen Redko'], 'affiliations': ["Noah's Ark"], 'pdf_title_img': 'assets/pdf/title_img/2602.06883.jpg', 'data': {'categories': ['#open_source', '#transfer_learning', '#interpretability'], 'emoji': 'üîÑ', 'ru': {'title': '–ü–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å –≤–∞–∂–Ω–µ–µ –≥–ª–∞–¥–∫–æ—Å—Ç–∏: –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –¥–æ–æ–±—É—á–µ–Ω–∏—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤', 'desc': '–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ Vision Transformer ‚Äî –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Ö–æ–¥—ã –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ü–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∫–∞–∫ —Å—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏ –æ—Ç—Ä–∞–∂–∞–µ—Ç —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –≤–æ–∑–º—É—â–µ–Ω–∏—è–º –≤—Ö–æ–¥–∞. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —á–µ—Ä–µ–∑ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã, —á—Ç–æ –≤—ã—Å–æ–∫–∞—è –ø–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å –º–æ–¥—É–ª–µ–π –≤–Ω–∏–º–∞–Ω–∏—è –∏ —Å–ª–æ—ë–≤ –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∞—Ç –æ–±—â–µ–ø—Ä–∏–Ω—è—Ç–æ–º—É –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏—é –æ –∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≥–ª–∞–¥–∫–æ—Å—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–π –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö, –ø—Ä–µ–¥–ª–∞–≥–∞—è –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –∏—Ö —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞.'}, 'en': {'title': 'High Plasticity, Better Performance: Rethinking Transformer Components', 'desc': 'This paper investigates how different parts of vision transformers respond to changes in input, a property known as plasticity. It challenges the common belief that smoother components always lead to better performance in machine learning tasks. The authors find that components with high plasticity, like attention modules and feedforward layers, actually improve finetuning results. This research provides new insights into how to select transformer components for better adaptation in transfer learning scenarios.'}, 'zh': {'title': 'È´òÂèØÂ°ëÊÄßÂä©ÂäõËßÜËßâÂèòÊç¢Âô®ÂæÆË∞ÉÊÄßËÉΩ', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜËßÜËßâÂèòÊç¢Âô®ÔºàVision TransformerÔºâÁªÑ‰ª∂ÁöÑÂèØÂ°ëÊÄß‰∏éÂæÆË∞ÉÊÄßËÉΩ‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁªÑ‰ª∂ÁöÑÈ´òÂèØÂ°ëÊÄßÈÄöÂ∏∏ÊÑèÂë≥ÁùÄ‰ΩéÂπ≥ÊªëÊÄßÔºåËøô‰∏é‰º†ÁªüËßÇÁÇπÁõ∏ÊÇñ„ÄÇÈÄöËøáÁêÜËÆ∫ÂàÜÊûêÂíåÂÆûÈ™åÔºåÊàë‰ª¨ÂèëÁé∞ÂÖ≥Ê≥®Ê®°ÂùóÂíåÂâçÈ¶àÂ±ÇÁöÑÈ´òÂèØÂ°ëÊÄßËÉΩÂ§üÊòæËëóÊèêÂçáÂæÆË∞ÉÊïàÊûú„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂‰∏∫ÈÄâÊã©ÈÄÇÂêàÁöÑÁªÑ‰ª∂Êèê‰æõ‰∫ÜÊñ∞ÁöÑÊåáÂØºÊÄùË∑ØÔºåÊåëÊàò‰∫ÜÂπ≥ÊªëÊÄßÊÄªÊòØÊúâÁõäÁöÑÂÅáËÆæ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.06554', 'title': 'SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees', 'url': 'https://huggingface.co/papers/2602.06554', 'abstract': "SeeUPO is a critic-free reinforcement learning method that ensures convergence guarantees in multi-turn agent interactions by modeling sequential decision-making as multi-agent bandit problems and using backward induction for policy updates.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies.   In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties in single/multi-turn scenarios. We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPO's original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios.   To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), a critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems. Through turn-by-turn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backward induction.   Experiments on AppWorld and BFCL v4 demonstrate SeeUPO's substantial improvements over existing backbone algorithms: relative gains of 43.3%-54.6% on Qwen3-14B and 24.1%-41.9% on Qwen2.5-14B (averaged across benchmarks), along with superior training stability.", 'score': 2, 'issue_id': 963, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': 'b47e2950662223f8', 'authors': ['Tianyi Hu', 'Qingxu Fu', 'Yanxi Chen', 'Zhaoyang Liu', 'Bolin Ding'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2602.06554.jpg', 'data': {'categories': ['#agents', '#training', '#rl', '#alignment', '#optimization', '#reasoning'], 'emoji': 'üéØ', 'ru': {'title': '–ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–µ–∑ –∫—Ä–∏—Ç–∏–∫–∞ —á–µ—Ä–µ–∑ –æ–±—Ä–∞—Ç–Ω—É—é –∏–Ω–¥—É–∫—Ü–∏—é', 'desc': 'SeeUPO ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–µ–∑ –∫—Ä–∏—Ç–∏–∫–∞, –∫–æ—Ç–æ—Ä—ã–π –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø—Ä–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ö–æ–¥–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã (PPO, REINFORCE) —Ç–µ—Ä—è—é—Ç –≥–∞—Ä–∞–Ω—Ç–∏–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ bepa—Äametr–∏—á–µ—Å–∫–∏—Ö –æ—Ü–µ–Ω–æ–∫ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –∫–∞–∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ–º—ã–µ –∑–∞–¥–∞—á–∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –±–∞–Ω–¥–∏—Ç–∞ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—Ä–∞—Ç–Ω—É—é –∏–Ω–¥—É–∫—Ü–∏—é –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏ –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ —Ö–æ–¥–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö AppWorld –∏ BFCL v4.'}, 'en': {'title': 'SeeUPO: Ensuring Convergence in Multi-Turn Reinforcement Learning', 'desc': 'The paper introduces SeeUPO, a novel reinforcement learning method designed for multi-turn interactions among agents. It addresses the lack of convergence guarantees in existing RL algorithms by modeling these interactions as multi-agent bandit problems. SeeUPO employs backward induction for policy updates, ensuring that the learning process leads to optimal solutions while maintaining stability. Experimental results show that SeeUPO significantly outperforms traditional RL methods, achieving higher performance and better training consistency.'}, 'zh': {'title': 'SeeUPOÔºöÂ§öËΩÆ‰∫§‰∫í‰∏≠ÁöÑÊó†ËØÑËÆ∫Âº∫ÂåñÂ≠¶‰π†Êñ∞ÊñπÊ≥ï', 'desc': 'SeeUPOÊòØ‰∏ÄÁßçÊó†ËØÑËÆ∫ÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÊó®Âú®Á°Æ‰øùÂ§öËΩÆ‰ª£ÁêÜ‰∫§‰∫í‰∏≠ÁöÑÊî∂ÊïõÊÄß„ÄÇÂÆÉÂ∞ÜÈ°∫Â∫èÂÜ≥Á≠ñÂª∫Ê®°‰∏∫Â§öÊô∫ËÉΩ‰ΩìËµåÂçöÈóÆÈ¢òÔºåÂπ∂ÈÄöËøáÂèçÂêëÊé®ÂØºËøõË°åÁ≠ñÁï•Êõ¥Êñ∞„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºå‰º†ÁªüÁöÑÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÂú®Â§öËΩÆÂú∫ÊôØ‰∏≠Êó†Ê≥ïÂêåÊó∂ÂÆûÁé∞Êó†ËØÑËÆ∫ÂíåÊî∂Êïõ‰øùËØÅ„ÄÇSeeUPOÈÄöËøáÈÄêËΩÆÁöÑÂèçÂêëÊâßË°åÈ°∫Â∫èÁ≠ñÁï•Êõ¥Êñ∞ÔºåÁ°Æ‰øù‰∫ÜÂçïË∞ÉÊîπËøõÂíåÂÖ®Â±ÄÊúÄ‰ºòËß£ÁöÑÊî∂Êïõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03548', 'title': 'SEAD: Self-Evolving Agent for Multi-Turn Service Dialogue', 'url': 'https://huggingface.co/papers/2602.03548', 'abstract': 'SEAD framework enables service dialogue agents to learn effective strategies through self-evolving user modeling components, achieving superior task completion and dialogue efficiency compared to existing foundation and commercial models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models have demonstrated remarkable capabilities in open-domain dialogues. However, current methods exhibit suboptimal performance in service dialogues, as they rely on noisy, low-quality human conversation data. This limitation arises from data scarcity and the difficulty of simulating authentic, goal-oriented user behaviors. To address these issues, we propose SEAD (Self-Evolving Agent for Service Dialogue), a framework that enables agents to learn effective strategies without large-scale human annotations. SEAD decouples user modeling into two components: a Profile Controller that generates diverse user states to manage training curriculum, and a User Role-play Model that focuses on realistic role-playing. This design ensures the environment provides adaptive training scenarios rather than acting as an unfair adversary. Experiments demonstrate that SEAD significantly outperforms Open-source Foundation Models and Closed-source Commercial Models, improving task completion rate by 17.6% and dialogue efficiency by 11.1%. Code is available at: https://github.com/Da1yuqin/SEAD.', 'score': 2, 'issue_id': 968, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '2156450b8d007196', 'authors': ['Yuqin Dai', 'Ning Gao', 'Wei Zhang', 'Jie Wang', 'Zichen Luo', 'Jinpeng Wang', 'Yujie Wang', 'Ruiyuan Wu', 'Chaozheng Wang'], 'affiliations': ['Meituan'], 'pdf_title_img': 'assets/pdf/title_img/2602.03548.jpg', 'data': {'categories': ['#multimodal', '#training', '#agents'], 'emoji': 'ü§ñ', 'ru': {'title': '–°–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–∏–π –∞–≥–µ–Ω—Ç –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è', 'desc': 'SEAD ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è –±–µ–∑ –±–æ–ª—å—à–∏—Ö –æ–±—ä—ë–º–æ–≤ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–µ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, —Ä–∞–∑–¥–µ–ª—è—è –µ–≥–æ –Ω–∞ –¥–≤–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: Profile Controller –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –∏ User Role-play Model –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∞–Ω—Ç–∞–≥–æ–Ω–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Å—Ä–µ–¥—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ: –Ω–∞ 17,6% –ø–æ–≤—ã—à–∞–µ—Ç—Å—è –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–¥–∞—á –∏ –Ω–∞ 11,1% —É–ª—É—á—à–∞–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–∏–∞–ª–æ–≥–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º–∏ –∏ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏.'}, 'en': {'title': 'Empowering Dialogue Agents with Self-Evolving User Models', 'desc': 'The SEAD framework enhances service dialogue agents by allowing them to develop effective strategies through self-evolving user modeling components. It addresses the limitations of existing models that rely on low-quality human conversation data, which often leads to poor performance in service dialogues. By decoupling user modeling into a Profile Controller and a User Role-play Model, SEAD creates adaptive training scenarios that simulate realistic user behaviors. Experimental results show that SEAD outperforms both open-source and commercial models, achieving a 17.6% increase in task completion and an 11.1% boost in dialogue efficiency.'}, 'zh': {'title': 'SEADÔºöÊúçÂä°ÂØπËØùÁöÑËá™ÊàëÊºîÂåñ‰ª£ÁêÜ', 'desc': 'SEADÊ°ÜÊû∂‰ΩøÊúçÂä°ÂØπËØù‰ª£ÁêÜËÉΩÂ§üÈÄöËøáËá™ÊàëÊºîÂåñÁöÑÁî®Êà∑Âª∫Ê®°ÁªÑ‰ª∂Â≠¶‰π†ÊúâÊïàÁ≠ñÁï•Ôºå‰ªéËÄåÂú®‰ªªÂä°ÂÆåÊàêÁéáÂíåÂØπËØùÊïàÁéá‰∏äË∂ÖË∂äÁé∞ÊúâÁöÑÂü∫Á°ÄÂíåÂïÜ‰∏öÊ®°Âûã„ÄÇËØ•Ê°ÜÊû∂Â∞ÜÁî®Êà∑Âª∫Ê®°ÂàÜ‰∏∫‰∏§‰∏™ÈÉ®ÂàÜÔºöÈÖçÁΩÆÊéßÂà∂Âô®ÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÁî®Êà∑Áä∂ÊÄÅ‰ª•ÁÆ°ÁêÜËÆ≠ÁªÉËØæÁ®ãÔºåËßíËâ≤ÊâÆÊºîÊ®°ÂûãÂàô‰∏ìÊ≥®‰∫éÁúüÂÆûÁöÑËßíËâ≤ÊâÆÊºî„ÄÇÈÄöËøáËøôÁßçËÆæËÆ°ÔºåSEADËÉΩÂ§üÊèê‰æõÈÄÇÂ∫îÊÄßËÆ≠ÁªÉÂú∫ÊôØÔºåËÄå‰∏çÊòØÂÖÖÂΩì‰∏çÂÖ¨Âπ≥ÁöÑÂØπÊâã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSEADÂú®‰ªªÂä°ÂÆåÊàêÁéá‰∏äÊèêÈ´ò‰∫Ü17.6%ÔºåÂØπËØùÊïàÁéáÊèêÈ´ò‰∫Ü11.1%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.06724', 'title': 'Table-as-Search: Formulate Long-Horizon Agentic Information Seeking as Table Completion', 'url': 'https://huggingface.co/papers/2602.06724', 'abstract': "Table-as-Search framework reformulates information seeking tasks as table completion problems, improving long-horizon search robustness through structured state management.  \t\t\t\t\tAI-generated summary \t\t\t\t Current Information Seeking (InfoSeeking) agents struggle to maintain focus and coherence during long-horizon exploration, as tracking search states, including planning procedure and massive search results, within one plain-text context is inherently fragile. To address this, we introduce Table-as-Search (TaS), a structured planning framework that reformulates the InfoSeeking task as a Table Completion task. TaS maps each query into a structured table schema maintained in an external database, where rows represent search candidates and columns denote constraints or required information. This table precisely manages the search states: filled cells strictly record the history and search results, while empty cells serve as an explicit search plan. Crucially, TaS unifies three distinct InfoSeeking tasks: Deep Search, Wide Search, and the challenging DeepWide Search. Extensive experiments demonstrate that TaS significantly outperforms numerous state-of-the-art baselines across three kinds of benchmarks, including multi-agent framework and commercial systems. Furthermore, our analysis validates the TaS's superior robustness in long-horizon InfoSeeking, alongside its efficiency, scalability and flexibility. Code and datasets are publicly released at https://github.com/AIDC-AI/Marco-Search-Agent.", 'score': 1, 'issue_id': 966, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '058e980c028aa3ed', 'authors': ['Tian Lan', 'Felix Henry', 'Bin Zhu', 'Qianghuai Jia', 'Junyang Ren', 'Qihang Pu', 'Haijun Li', 'Longyue Wang', 'Zhao Xu', 'Weihua Luo'], 'affiliations': ['alibabainc.com'], 'pdf_title_img': 'assets/pdf/title_img/2602.06724.jpg', 'data': {'categories': ['#benchmark', '#agents', '#dataset'], 'emoji': 'üóÇÔ∏è', 'ru': {'title': '–¢–∞–±–ª–∏—Ü–∞ –∫–∞–∫ –Ω–∞–≤–∏–≥–∞—Ç–æ—Ä: —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –¥–æ–ª–≥–∏—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ Table-as-Search (TaS) –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –ø–æ–∏—Å–∫–∞ –≤ –∑–∞–¥–∞—á—É –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ö–µ–º—É —Ç–∞–±–ª–∏—Ü—ã, —Ö—Ä–∞–Ω—è—â—É—é—Å—è –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö, –≥–¥–µ —Å—Ç—Ä–æ–∫–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞, –∞ —Å—Ç–æ–ª–±—Ü—ã –æ–±–æ–∑–Ω–∞—á–∞—é—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ —Ç—Ä–µ–±—É–µ–º—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –ó–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ —è—á–µ–π–∫–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç –∏—Å—Ç–æ—Ä–∏—é –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞, –∞ –ø—É—Å—Ç—ã–µ —è—á–µ–π–∫–∏ —Å–ª—É–∂–∞—Ç —è–≤–Ω—ã–º –ø–ª–∞–Ω–æ–º –ø–æ–∏—Å–∫–∞ –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞—Ö. –ü–æ–¥—Ö–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –Ω–∞ —Ç—Ä—ë—Ö —Ç–∏–ø–∞—Ö –∑–∞–¥–∞—á –ø–æ–∏—Å–∫–∞ –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å.'}, 'en': {'title': 'Revolutionizing Information Seeking with Table-as-Search', 'desc': 'The Table-as-Search (TaS) framework transforms information seeking tasks into table completion problems, enhancing the ability to manage long-term searches. By organizing search states in a structured table format, TaS allows for better tracking of queries and results, which improves coherence during exploration. Each table cell captures either search history or planned queries, providing a clear overview of the search process. Experimental results show that TaS outperforms existing methods in various benchmarks, demonstrating its robustness, efficiency, and scalability in handling complex information seeking tasks.'}, 'zh': {'title': 'Ë°®Ê†ºÊêúÁ¥¢ÔºöÊèêÂçá‰ø°ÊÅØÊ£ÄÁ¥¢ÁöÑÈ≤ÅÊ£íÊÄß', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Table-as-SearchÔºàTaSÔºâÁöÑÊ°ÜÊû∂ÔºåÂ∞Ü‰ø°ÊÅØÊ£ÄÁ¥¢‰ªªÂä°ÈáçÊñ∞ÂÆö‰πâ‰∏∫Ë°®Ê†ºË°•ÂÖ®ÈóÆÈ¢òÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÈïøÊó∂Èó¥ÊêúÁ¥¢ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇTaSÈÄöËøáÂ∞ÜÊØè‰∏™Êü•ËØ¢Êò†Â∞ÑÂà∞‰∏Ä‰∏™Â§ñÈÉ®Êï∞ÊçÆÂ∫ì‰∏≠ÁöÑÁªìÊûÑÂåñË°®Ê†ºÊ®°ÂºèÔºåÊù•ÊúâÊïàÁÆ°ÁêÜÊêúÁ¥¢Áä∂ÊÄÅ„ÄÇË°®Ê†º‰∏≠ÁöÑË°åË°®Á§∫ÊêúÁ¥¢ÂÄôÈÄâÈ°πÔºåÂàóÂàôË°®Á§∫Á∫¶ÊùüÊàñÊâÄÈúÄ‰ø°ÊÅØÔºåÂ°´ÂÖÖÁöÑÂçïÂÖÉÊ†ºËÆ∞ÂΩïÂéÜÂè≤ÂíåÊêúÁ¥¢ÁªìÊûúÔºåËÄåÁ©∫ÂçïÂÖÉÊ†ºÂàô‰Ωú‰∏∫ÊòéÁ°ÆÁöÑÊêúÁ¥¢ËÆ°Âàí„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTaSÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÔºåÂ±ïÁé∞‰∫ÜÂÖ∂Âú®ÈïøÊó∂Èó¥‰ø°ÊÅØÊ£ÄÁ¥¢‰∏≠ÁöÑ‰ºòË∂äÊÄß„ÄÅÊïàÁéá„ÄÅÂèØÊâ©Â±ïÊÄßÂíåÁÅµÊ¥ªÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.06129', 'title': 'Urban Spatio-Temporal Foundation Models for Climate-Resilient Housing: Scaling Diffusion Transformers for Disaster Risk Prediction', 'url': 'https://huggingface.co/papers/2602.06129', 'abstract': 'A diffusion-transformer framework integrates spatio-temporal urban data to predict building-level climate risks while incorporating transportation network structures for emergency response applications.  \t\t\t\t\tAI-generated summary \t\t\t\t Climate hazards increasingly disrupt urban transportation and emergency-response operations by damaging housing stock, degrading infrastructure, and reducing network accessibility. This paper presents Skjold-DiT, a diffusion-transformer framework that integrates heterogeneous spatio-temporal urban data to forecast building-level climate-risk indicators while explicitly incorporating transportation-network structure and accessibility signals relevant to intelligent vehicles (e.g., emergency reachability and evacuation-route constraints). Concretely, Skjold-DiT enables hazard-conditioned routing constraints by producing calibrated, uncertainty-aware accessibility layers (reachability, travel-time inflation, and route redundancy) that can be consumed by intelligent-vehicle routing and emergency dispatch systems. Skjold-DiT combines: (1) Fjell-Prompt, a prompt-based conditioning interface designed to support cross-city transfer; (2) Norrland-Fusion, a cross-modal attention mechanism unifying hazard maps/imagery, building attributes, demographics, and transportation infrastructure into a shared latent representation; and (3) Valkyrie-Forecast, a counterfactual simulator for generating probabilistic risk trajectories under intervention prompts. We introduce the Baltic-Caspian Urban Resilience (BCUR) dataset with 847,392 building-level observations across six cities, including multi-hazard annotations (e.g., flood and heat indicators) and transportation accessibility features. Experiments evaluate prediction quality, cross-city generalization, calibration, and downstream transportation-relevant outcomes, including reachability and hazard-conditioned travel times under counterfactual interventions.', 'score': 1, 'issue_id': 965, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': 'f2f3cb493d6c1620', 'authors': ['Olaf Yunus Laitinen Imanov', 'Derya Umut Kulali', 'Taner Yilmaz'], 'affiliations': ['Department of Applied Mathematics and Computer Science (DTU Compute), Technical University of Denmark, Kongens Lyngby, Denmark', 'Department of Computer Engineering, Afyon Kocatepe University, Afyonkarahisar, T√ºrkiye', 'Department of Engineering, Eskisehir Technical University, Eskisehir, T√ºrkiye'], 'pdf_title_img': 'assets/pdf/title_img/2602.06129.jpg', 'data': {'categories': ['#graphs', '#open_source', '#benchmark', '#diffusion', '#dataset', '#science', '#transfer_learning', '#architecture', '#multimodal'], 'emoji': 'üè¢', 'ru': {'title': '–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª–∏–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∏—Å–∫–æ–≤ –∏ –º–∞—Ä—à—Ä—É—Ç–æ–≤ —ç–≤–∞–∫—É–∞—Ü–∏–∏ –≤ –≥–æ—Ä–æ–¥–∞—Ö —Å –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–º', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ Skjold-DiT ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–ª–∏–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∏—Å–∫–æ–≤ –Ω–∞ —É—Ä–æ–≤–Ω–µ –∑–¥–∞–Ω–∏–π –≤ –≥–æ—Ä–æ–¥–∞—Ö. –ú–æ–¥–µ–ª—å –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç —Ä–∞–∑–Ω–æ—Ä–æ–¥–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –≤–∫–ª—é—á–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–æ–π —Å–µ—Ç–∏, –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –∫–∞—Ä—Ç—ã –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å –æ—Ü–µ–Ω–∫–∞–º–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è —Å–∏—Å—Ç–µ–º —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–≥–æ —Ä–µ–∞–≥–∏—Ä–æ–≤–∞–Ω–∏—è. –ö–ª—é—á–µ–≤—ã–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ —è–≤–ª—è—é—Ç—Å—è –º–µ—Ö–∞–Ω–∏–∑–º —É—Å–ª–æ–≤–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–∞ –º–µ–∂–¥—É –≥–æ—Ä–æ–¥–∞–º–∏, –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –æ–ø–∞—Å–Ω–æ—Å—Ç–µ–π –∏ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤ –æ–±—â–µ–µ —Å–∫—Ä—ã—Ç–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ, –∏ –∫–æ–Ω—Ç—Ä—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Å–∏–º—É–ª—è—Ç–æ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Ä–∏—Å–∫–æ–≤. –†–∞–±–æ—Ç–∞ –≤–∫–ª—é—á–∞–µ—Ç –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç BCUR —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ 847 —Ç—ã—Å—è—á–∞—Ö –∑–¥–∞–Ω–∏–π –≤ —à–µ—Å—Ç–∏ –≥–æ—Ä–æ–¥–∞—Ö —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–ø–∞—Å–Ω–æ—Å—Ç–µ–π –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–æ–π –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Predicting Climate Risks with Smart Urban Data Integration', 'desc': 'This paper introduces Skjold-DiT, a diffusion-transformer framework designed to predict climate risks at the building level by utilizing diverse urban data. It integrates transportation network structures to enhance emergency response capabilities, focusing on factors like accessibility and evacuation routes. The framework employs advanced techniques such as prompt-based conditioning and cross-modal attention to create a unified representation of various urban features. Additionally, it includes a counterfactual simulator to assess risk trajectories, supported by a comprehensive dataset of building observations across multiple cities.'}, 'zh': {'title': 'Êô∫ËÉΩÂ∫îÊÄ•ÂìçÂ∫îÁöÑÊ∞îÂÄôÈ£éÈô©È¢ÑÊµãÊñ∞Ê°ÜÊû∂', 'desc': 'ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Skjold-DiTÁöÑÊâ©Êï£-ÂèòÊç¢Âô®Ê°ÜÊû∂ÔºåÊó®Âú®Êï¥ÂêàÂüéÂ∏ÇÁöÑÊó∂Á©∫Êï∞ÊçÆÔºå‰ª•È¢ÑÊµãÂª∫Á≠ëÁ∫ßÂà´ÁöÑÊ∞îÂÄôÈ£éÈô©„ÄÇËØ•Ê°ÜÊû∂ÁâπÂà´ËÄÉËôë‰∫Ü‰∫§ÈÄöÁΩëÁªúÁªìÊûÑÔºå‰ª•‰æøÂú®Á¥ßÊÄ•ÂìçÂ∫îÂ∫îÁî®‰∏≠ÊèêÈ´òÊïàÁéá„ÄÇSkjold-DiTËÉΩÂ§üÁîüÊàêÁªèËøáÊ†°ÂáÜÁöÑÂèØËææÊÄßÂ±ÇÔºåÂ∏ÆÂä©Êô∫ËÉΩËΩ¶ËæÜËøõË°åÊõ¥Â•ΩÁöÑË∑ØÂæÑËßÑÂàíÂíåÂ∫îÊÄ•Ë∞ÉÂ∫¶„ÄÇÁ†îÁ©∂ËøòÂºïÂÖ•‰∫ÜBaltic-CaspianÂüéÂ∏ÇÈüßÊÄßÊï∞ÊçÆÈõÜÔºåÂåÖÂê´847,392‰∏™Âª∫Á≠ëËßÇÂØüÊï∞ÊçÆÔºåÊîØÊåÅÂ§öÁßçÊ∞îÂÄôÈ£éÈô©ÁöÑËØÑ‰º∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04454', 'title': 'Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search', 'url': 'https://huggingface.co/papers/2602.04454', 'abstract': 'Seg-ReSearch introduces a novel segmentation approach that combines interleaved reasoning with external search to overcome limitations of frozen MLLM knowledge, using hierarchical reward design for training and demonstrating superior performance on video object segmentation benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Segmentation based on language has been a popular topic in computer vision. While recent advances in multimodal large language models (MLLMs) have endowed segmentation systems with reasoning capabilities, these efforts remain confined by the frozen internal knowledge of MLLMs, which limits their potential for real-world scenarios that involve up-to-date information or domain-specific concepts. In this work, we propose Seg-ReSearch, a novel segmentation paradigm that overcomes the knowledge bottleneck of existing approaches. By enabling interleaved reasoning and external search, Seg-ReSearch empowers segmentation systems to handle dynamic, open-world queries that extend beyond the frozen knowledge of MLLMs. To effectively train this capability, we introduce a hierarchical reward design that harmonizes initial guidance with progressive incentives, mitigating the dilemma between sparse outcome signals and rigid step-wise supervision. For evaluation, we construct OK-VOS, a challenging benchmark that explicitly requires outside knowledge for video object segmentation. Experiments on OK-VOS and two existing reasoning segmentation benchmarks demonstrate that our Seg-ReSearch improves state-of-the-art approaches by a substantial margin. Code and data will be released at https://github.com/iSEE-Laboratory/Seg-ReSearch.', 'score': 1, 'issue_id': 964, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': 'f339569c416d4d74', 'authors': ['Tianming Liang', 'Qirui Du', 'Jian-Fang Hu', 'Haichao Jiang', 'Zicheng Lin', 'Wei-Shi Zheng'], 'affiliations': ['ISEE Lab, Sun Yat-sen University'], 'pdf_title_img': 'assets/pdf/title_img/2602.04454.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#dataset', '#training', '#multimodal', '#rag', '#cv', '#benchmark', '#video'], 'emoji': 'üîç', 'ru': {'title': '–í—ã—Ö–æ–¥ –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π: —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å –ø–æ–∏—Å–∫–æ–º –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º', 'desc': 'Seg-ReSearch –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç—ë–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –≤–Ω–µ—à–Ω–µ–≥–æ –ø–æ–∏—Å–∫–∞. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –±–∞–ª–∞–Ω—Å–∏—Ä—É—è –º–µ–∂–¥—É —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–º–∏ —Å–∏–≥–Ω–∞–ª–∞–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –ø–æ—à–∞–≥–æ–≤—ã–º —Å—É–ø–µ—Ä–≤–∏–∑–æ—Ä–æ–º. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ OK-VOS –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∏–¥–µ–æ—Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤, —Ç—Ä–µ–±—É—é—â–µ–π –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏—è –≤–Ω–µ—à–Ω–∏—Ö –∑–Ω–∞–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ OK-VOS –∏ –¥—Ä—É–≥–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º.'}, 'en': {'title': 'Empowering Video Segmentation with Dynamic Reasoning and External Knowledge', 'desc': "Seg-ReSearch presents a new method for video object segmentation that integrates interleaved reasoning with external search capabilities. This approach addresses the limitations of traditional multimodal large language models (MLLMs) that rely on outdated, frozen knowledge. By implementing a hierarchical reward design during training, Seg-ReSearch effectively balances initial guidance with ongoing incentives, enhancing the model's adaptability to dynamic queries. The method shows significant improvements on video segmentation benchmarks, demonstrating its effectiveness in real-world applications that require current and specific information."}, 'zh': {'title': 'Seg-ReSearchÔºöÁ™ÅÁ†¥Áü•ËØÜÁì∂È¢àÁöÑÂàÜÂâ≤Êñ∞ÊñπÊ≥ï', 'desc': 'Seg-ReSearchÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂàÜÂâ≤ÊñπÊ≥ïÔºåÁªìÂêà‰∫Ü‰∫§ÈîôÊé®ÁêÜÂíåÂ§ñÈÉ®ÊêúÁ¥¢Ôºå‰ª•ÂÖãÊúçÂÜªÁªìÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÁü•ËØÜÁöÑÂ±ÄÈôêÊÄß„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂ±ÇÊ¨°Â•ñÂä±ËÆæËÆ°ËøõË°åËÆ≠ÁªÉÔºåËÉΩÂ§üÂ§ÑÁêÜÂä®ÊÄÅÁöÑÂºÄÊîæ‰∏ñÁïåÊü•ËØ¢ÔºåË∂ÖË∂ä‰∫ÜMLLMÁöÑÂõ∫ÂÆöÁü•ËØÜ„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫ÜOK-VOSÂü∫ÂáÜÔºåÊòéÁ°ÆË¶ÅÊ±ÇÂ§ñÈÉ®Áü•ËØÜÁî®‰∫éËßÜÈ¢ëÁâ©‰ΩìÂàÜÂâ≤„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSeg-ReSearchÂú®Â§ö‰∏™Âü∫ÂáÜ‰∏äÊòæËëóÊèêÂçá‰∫ÜÁé∞ÊúâÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03998', 'title': 'AtlasPatch: An Efficient and Scalable Tool for Whole Slide Image Preprocessing in Computational Pathology', 'url': 'https://huggingface.co/papers/2602.03998', 'abstract': "AtlasPatch is an efficient and scalable whole-slide image preprocessing framework that uses fine-tuned Segment-Anything model for accurate tissue detection and high-throughput patch extraction with reduced computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Whole-slide image (WSI) preprocessing, typically comprising tissue detection followed by patch extraction, is foundational to AI-driven computational pathology workflows. This remains a major computational bottleneck as existing tools either rely on inaccurate heuristic thresholding for tissue detection, or adopt AI-based approaches trained on limited-diversity data that operate at the patch level, incurring substantial computational complexity. We present AtlasPatch, an efficient and scalable slide preprocessing framework for accurate tissue detection and high-throughput patch extraction with minimal computational overhead. AtlasPatch's tissue detection module is trained on a heterogeneous and semi-manually annotated dataset of ~30,000 WSI thumbnails, using efficient fine-tuning of the Segment-Anything model. The tool extrapolates tissue masks from thumbnails to full-resolution slides to extract patch coordinates at user-specified magnifications, with options to stream patches directly into common image encoders for embedding or store patch images, all efficiently parallelized across CPUs and GPUs. We assess AtlasPatch across segmentation precision, computational complexity, and downstream multiple-instance learning, matching state-of-the-art performance while operating at a fraction of their computational cost. AtlasPatch is open-source and available at https://github.com/AtlasAnalyticsLab/AtlasPatch.", 'score': 1, 'issue_id': 972, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': 'fa50ee2c2bbbbb50', 'authors': ['Ahmed Alagha', 'Christopher Leclerc', 'Yousef Kotp', 'Omar Metwally', 'Calvin Moras', 'Peter Rentopoulos', 'Ghodsiyeh Rostami', 'Bich Ngoc Nguyen', 'Jumanah Baig', 'Abdelhakim Khellaf', 'Vincent Quoc-Huy Trinh', 'Rabeb Mizouni', 'Hadi Otrok', 'Jamal Bentahar', 'Mahdi S. Hosseini'], 'affiliations': ['Concordia Institute for Information Systems Engineering (CIISE), Concordia University, Montreal, QC, Canada', 'Department of Building, Civil, and Environmental Engineering, Concordia University, Montreal, QC, Canada', 'Department of Computer Science and Software Engineering (CSSE), Concordia University, Montreal, QC, Canada', 'Department of Computer Science, Khalifa University, Abu Dhabi, UAE', 'Department of Pathology, McGill University, Montreal, QC, Canada', 'Institute for Research in Immunology and Cancer, University of Montreal, Montreal, QC, Canada', 'MilaQuebec AI Institute, Montreal, QC, Canada', 'University of Montreal Hospital Center (CHUM), Montreal, QC, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2602.03998.jpg', 'data': {'categories': ['#healthcare', '#cv', '#dataset', '#open_source', '#data', '#inference', '#science'], 'emoji': 'üî¨', 'ru': {'title': '–ë—ã—Å—Ç—Ä–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Å–Ω–∏–º–∫–æ–≤ —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Segment-Anything', 'desc': 'AtlasPatch ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–∏–∫—Ä–æ—Å–Ω–∏–º–∫–æ–≤ —Ü–µ–ª—ã—Ö —Å—Ä–µ–∑–æ–≤ —Ç–∫–∞–Ω–∏ (WSI), –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å Segment-Anything –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Ç–∫–∞–Ω–∏. –°–∏—Å—Ç–µ–º–∞ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —É–∑–∫–æ–≥–æ –º–µ—Å—Ç–∞ –≤ —Ü–∏—Ñ—Ä–æ–≤–æ–π –ø–∞—Ç–æ–ª–æ–≥–∏–∏, –∑–∞–º–µ–Ω—è—è –Ω–µ—Ç–æ—á–Ω—ã–µ —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –Ω–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π –ø–æ–¥—Ö–æ–¥, –æ–±—É—á–µ–Ω–Ω—ã–π –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ 30 000 –º–∏–∫—Ä–æ—Å–Ω–∏–º–∫–æ–≤. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –ø–∞—Ç—á–µ–π –∏–∑ –ø–æ–ª–Ω–æ—Ä–∞–∑—Ä–µ—à–∞—é—â–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø—É—Ç—ë–º —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏ –º–∞—Å–æ–∫ —Ç–∫–∞–Ω–∏ —Å —ç—Å–∫–∏–∑–æ–≤ –∏ –º–æ–∂–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è —Å —ç–Ω–∫–æ–¥–µ—Ä–∞–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö. AtlasPatch –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞, —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–≥–æ —Å –ª—É—á—à–∏–º–∏ —Ä–µ—à–µ–Ω–∏—è–º–∏, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –±–ª–∞–≥–æ–¥–∞—Ä—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏–∏ –Ω–∞ CPU –∏ GPU.'}, 'en': {'title': 'Efficient Whole-Slide Image Preprocessing with AtlasPatch', 'desc': 'AtlasPatch is a new framework designed to preprocess whole-slide images (WSIs) efficiently, focusing on accurate tissue detection and fast patch extraction. It utilizes a fine-tuned Segment-Anything model trained on a diverse dataset of around 30,000 WSI thumbnails, which helps in reducing computational overhead. By extrapolating tissue masks from thumbnails to full-resolution slides, AtlasPatch allows for high-throughput extraction of image patches at various magnifications. The framework is open-source and demonstrates state-of-the-art performance in segmentation precision while significantly lowering computational costs compared to existing methods.'}, 'zh': {'title': 'AtlasPatchÔºöÈ´òÊïàÁöÑÂÖ®ÂàáÁâáÂõæÂÉèÈ¢ÑÂ§ÑÁêÜÊ°ÜÊû∂', 'desc': 'AtlasPatchÊòØ‰∏Ä‰∏™È´òÊïà‰∏îÂèØÊâ©Â±ïÁöÑÂÖ®ÂàáÁâáÂõæÂÉèÈ¢ÑÂ§ÑÁêÜÊ°ÜÊû∂ÔºåÊó®Âú®ÂÆûÁé∞ÂáÜÁ°ÆÁöÑÁªÑÁªáÊ£ÄÊµãÂíåÈ´òÈÄöÈáèÁöÑË°•‰∏ÅÊèêÂèñÔºåÂêåÊó∂ÂáèÂ∞ëËÆ°ÁÆóÂºÄÈîÄ„ÄÇËØ•Ê°ÜÊû∂‰ΩøÁî®ÁªèËøáÂæÆË∞ÉÁöÑSegment-AnythingÊ®°ÂûãÔºåËÆ≠ÁªÉ‰∫é‰∏Ä‰∏™ÂåÖÂê´Á∫¶30,000‰∏™ÂÖ®ÂàáÁâáÂõæÂÉèÁº©Áï•ÂõæÁöÑÂºÇË¥®ÂçäÊâãÂä®Ê†áÊ≥®Êï∞ÊçÆÈõÜ„ÄÇAtlasPatchËÉΩÂ§ü‰ªéÁº©Áï•Âõæ‰∏≠Êé®Êñ≠Âá∫ÁªÑÁªáÊé©ËÜúÔºåÂπ∂Âú®Áî®Êà∑ÊåáÂÆöÁöÑÊîæÂ§ßÂÄçÊï∞‰∏ãÊèêÂèñË°•‰∏ÅÂùêÊ†áÔºåÊîØÊåÅÂ∞ÜË°•‰∏ÅÁõ¥Êé•ÊµÅÂºè‰º†ËæìÂà∞Â∏∏ËßÅÁöÑÂõæÂÉèÁºñÁ†ÅÂô®‰∏≠„ÄÇÈÄöËøáÂú®ÂàÜÂâ≤Á≤æÂ∫¶„ÄÅËÆ°ÁÆóÂ§çÊùÇÊÄßÂíå‰∏ãÊ∏∏Â§öÂÆû‰æãÂ≠¶‰π†ÊñπÈù¢ÁöÑËØÑ‰º∞ÔºåAtlasPatchÂú®ÊÄßËÉΩ‰∏ä‰∏éÊúÄÂÖàËøõÁöÑÂ∑•ÂÖ∑Áõ∏ÂåπÈÖçÔºå‰ΩÜËÆ°ÁÆóÊàêÊú¨Âç¥Â§ßÂ§ßÈôç‰Ωé„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01064', 'title': 'Exploring Knowledge Purification in Multi-Teacher Knowledge Distillation for LLMs', 'url': 'https://huggingface.co/papers/2602.01064', 'abstract': 'Knowledge purification techniques consolidate rationales from multiple teacher LLMs to reduce conflicts and improve efficiency in distillation processes.  \t\t\t\t\tAI-generated summary \t\t\t\t Knowledge distillation has emerged as a pivotal technique for transferring knowledge from stronger large language models (LLMs) to smaller, more efficient models. However, traditional distillation approaches face challenges related to knowledge conflicts and high resource demands, particularly when leveraging multiple teacher models. In this paper, we introduce the concept of Knowledge Purification, which consolidates the rationales from multiple teacher LLMs into a single rationale, thereby mitigating conflicts and enhancing efficiency. To investigate the effectiveness of knowledge purification, we further propose five purification methods from various perspectives. Our experiments demonstrate that these methods not only improve the performance of the distilled model but also effectively alleviate knowledge conflicts. Moreover, router-based methods exhibit robust generalization capabilities, underscoring the potential of innovative purification techniques in optimizing multi-teacher distillation and facilitating the practical deployment of powerful yet lightweight models.', 'score': 1, 'issue_id': 962, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 1', 'zh': '2Êúà1Êó•'}, 'hash': 'cb1b1f412f81bede', 'authors': ['Ruihan Jin', 'Pengpeng Shao', 'Zhengqi Wen', 'Jinyang Wu', 'Mingkuan Feng', 'Shuo Yang', 'Chu Yuan Zhang', 'Jianhua Tao'], 'affiliations': ['Beijing National Research Center for Information Science and Technology', 'Department of Automation, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2602.01064.jpg', 'data': {'categories': ['#transfer_learning', '#small_models', '#training'], 'emoji': 'üßπ', 'ru': {'title': '–û—á–∏—Å—Ç–∫–∞ –∑–Ω–∞–Ω–∏–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∏–∑ –º–Ω–æ–≥–∏—Ö —É—á–∏—Ç–µ–ª–µ–π', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –æ—á–∏—Å—Ç–∫–∏ –∑–Ω–∞–Ω–∏–π –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–µ–∂–¥—É –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ –±–æ–ª–µ–µ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —É—á–∏—Ç–µ–ª—å—Å–∫–∏—Ö LLM –≤ –µ–¥–∏–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –∑–Ω–∞–Ω–∏–π. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω—ã –ø—è—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –æ—á–∏—Å—Ç–∫–∏ –∑–Ω–∞–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–∞—Ö –∫ –∞–Ω–∞–ª–∏–∑—É —Ä–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∏—Ä—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤.'}, 'en': {'title': 'Streamlining Knowledge Transfer with Purification Techniques', 'desc': 'This paper presents Knowledge Purification, a novel approach to enhance knowledge distillation from multiple teacher large language models (LLMs). By consolidating rationales from these teachers, the method reduces conflicts that typically arise during the distillation process. The authors propose five distinct purification methods to evaluate their effectiveness in improving the performance of smaller models. Results show that these techniques not only boost model efficiency but also enhance generalization, making them valuable for deploying lightweight models in practical applications.'}, 'zh': {'title': 'Áü•ËØÜÂáÄÂåñÔºöÊèêÂçáËí∏È¶èÊïàÁéáÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Áü•ËØÜÂáÄÂåñÊäÄÊúØÈÄöËøáÊï¥ÂêàÂ§ö‰∏™ÊïôÂ∏àÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊé®ÁêÜÔºåÂáèÂ∞ë‰∫ÜÁü•ËØÜÂÜ≤Á™ÅÂπ∂ÊèêÈ´ò‰∫ÜËí∏È¶èËøáÁ®ãÁöÑÊïàÁéá„ÄÇ‰º†ÁªüÁöÑÁü•ËØÜËí∏È¶èÊñπÊ≥ïÂú®‰ΩøÁî®Â§ö‰∏™ÊïôÂ∏àÊ®°ÂûãÊó∂Èù¢‰∏¥Áü•ËØÜÂÜ≤Á™ÅÂíåËµÑÊ∫êÈúÄÊ±ÇÈ´òÁöÑÈóÆÈ¢ò„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜÁü•ËØÜÂáÄÂåñÁöÑÊ¶ÇÂøµÔºåÂ∞ÜÂ§ö‰∏™ÊïôÂ∏àLLMsÁöÑÊé®ÁêÜÊï¥Âêà‰∏∫Âçï‰∏ÄÊé®ÁêÜÔºå‰ªéËÄåÁºìËß£ÂÜ≤Á™ÅÂπ∂ÊèêÂçáÊïàÁéá„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåËøô‰∫õÂáÄÂåñÊñπÊ≥ï‰∏ç‰ªÖÊèêÈ´ò‰∫ÜËí∏È¶èÊ®°ÂûãÁöÑÊÄßËÉΩÔºåËøòÊúâÊïàÂáèËΩª‰∫ÜÁü•ËØÜÂÜ≤Á™Å„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.23039', 'title': 'Avoiding Premature Collapse: Adaptive Annealing for Entropy-Regularized Structural Inference', 'url': 'https://huggingface.co/papers/2601.23039', 'abstract': 'Researchers identify and address premature mode collapse in optimal transport-based structural prediction models through an adaptive stability control algorithm that prevents gradient explosions during large-scale training.  \t\t\t\t\tAI-generated summary \t\t\t\t Differentiable matching layers and residual connection paradigms, often implemented via entropy-regularized Optimal Transport (OT), serve as critical mechanisms in structural prediction and architectural scaling. However, recovering discrete permutations or maintaining identity mappings via annealing Œµto 0 is notoriously unstable. In this work, we identify a fundamental mechanism for this failure: Premature Mode Collapse. By analyzing the non-normal dynamics of the Sinkhorn fixed-point map, we reveal a theoretical thermodynamic speed limit: standard exponential cooling outpaces the contraction rate of the inference operator, which degrades as O(1/Œµ). To address this, we propose Efficient Piecewise Hybrid Adaptive Stability Control (EPH-ASC), an adaptive scheduling algorithm that monitors the stability of the inference process. We demonstrate that EPH-ASC is essential for stabilizing Manifold-Constrained Hyper-Connections (mHC) during large-scale training on the FineWeb-Edu dataset, effectively preventing late-stage gradient explosions by enforcing a linear stability law.', 'score': 1, 'issue_id': 963, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': '0710cf59390fcf1d', 'authors': ['Yizhi Liu'], 'affiliations': ['Department of Computer Science, Stony Brook University'], 'pdf_title_img': 'assets/pdf/title_img/2601.23039.jpg', 'data': {'categories': ['#optimization'], 'emoji': '‚ö°', 'ru': {'title': '–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∂–∏–º–æ–≤ –≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–º —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–µ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä', 'desc': '–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –≤—ã—è–≤–∏–ª–∏ –ø—Ä–æ–±–ª–µ–º—É –ø—Ä–µ–∂–¥–µ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∫–æ–ª–ª–∞–ø—Å–∞ –º–æ–¥ –≤ –º–æ–¥–µ–ª—è—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–º —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–µ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –∞–Ω–∞–ª–∏–∑ –¥–∏–Ω–∞–º–∏–∫–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –°–∏–Ω—Ö–æ—Ä–Ω–∞ –∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –æ—Ö–ª–∞–∂–¥–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º —Å–∂–∞—Ç–∏–µ –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞ –≤—ã–≤–æ–¥–∞. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º EPH-ASC, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –ø—Ä–æ—Ü–µ—Å—Å–∞ –≤—ã–≤–æ–¥–∞ –∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–µ –≤–∑—Ä—ã–≤—ã. –ú–µ—Ç–æ–¥ —É—Å–ø–µ—à–Ω–æ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö –º–∞—Å—à—Ç–∞–±–∞ FineWeb-Edu, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å —á–µ—Ä–µ–∑ –ø—Ä–∏–Ω—Ü–∏–ø –ª–∏–Ω–µ–π–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Stabilizing Structural Prediction with Adaptive Control', 'desc': 'This paper tackles the issue of premature mode collapse in optimal transport-based structural prediction models, which can lead to instability during training. The authors introduce an adaptive stability control algorithm called Efficient Piecewise Hybrid Adaptive Stability Control (EPH-ASC) to prevent gradient explosions. By analyzing the dynamics of the Sinkhorn fixed-point map, they uncover a speed limit that contributes to the instability. The proposed method is shown to effectively stabilize the training process, particularly in large-scale applications like the FineWeb-Edu dataset.'}, 'zh': {'title': 'Èò≤Ê≠¢Êó©ÊúüÊ®°ÂºèÂ¥©Ê∫ÉÁöÑËá™ÈÄÇÂ∫îÁ®≥ÂÆöÊÄßÊéßÂà∂', 'desc': 'Êú¨Á†îÁ©∂ÈíàÂØπÂü∫‰∫éÊúÄ‰ºò‰º†ËæìÁöÑÁªìÊûÑÈ¢ÑÊµãÊ®°Âûã‰∏≠ÁöÑÊó©ÊúüÊ®°ÂºèÂ¥©Ê∫ÉÈóÆÈ¢òÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçËá™ÈÄÇÂ∫îÁ®≥ÂÆöÊÄßÊéßÂà∂ÁÆóÊ≥ï„ÄÇËØ•ÁÆóÊ≥ïËÉΩÂ§üÂú®Â§ßËßÑÊ®°ËÆ≠ÁªÉËøáÁ®ã‰∏≠Èò≤Ê≠¢Ê¢ØÂ∫¶ÁàÜÁÇ∏ÔºåÁ°Æ‰øùÊ®°ÂûãÁöÑÁ®≥ÂÆöÊÄß„ÄÇÊàë‰ª¨ÂàÜÊûê‰∫ÜSinkhornÂõ∫ÂÆöÁÇπÊò†Â∞ÑÁöÑÈùûÊ≠£Â∏∏Âä®ÊÄÅÔºåÊè≠Á§∫‰∫Ü‰∏Ä‰∏™ÁêÜËÆ∫ÁÉ≠ÂäõÂ≠¶ÈÄüÂ∫¶ÊûÅÈôêÔºåÊåáÂá∫Ê†áÂáÜÁöÑÊåáÊï∞ÂÜ∑Âç¥ÈÄüÂ∫¶Ë∂ÖËøá‰∫ÜÊé®ÁêÜÁÆóÂ≠êÁöÑÊî∂Áº©ÈÄüÁéá„ÄÇÈÄöËøáÂºïÂÖ•È´òÊïàÂàÜÊÆµÊ∑∑ÂêàËá™ÈÄÇÂ∫îÁ®≥ÂÆöÊÄßÊéßÂà∂ÔºàEPH-ASCÔºâÔºåÊàë‰ª¨ÊàêÂäüÂú∞Âú®FineWeb-EduÊï∞ÊçÆÈõÜ‰∏äÁ®≥ÂÆö‰∫ÜÊµÅÂΩ¢Á∫¶ÊùüË∂ÖËøûÊé•ÔºàmHCÔºâÔºåÊúâÊïàÈÅøÂÖç‰∫ÜÂêéÊúüÁöÑÊ¢ØÂ∫¶ÁàÜÁÇ∏„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.06964', 'title': 'Learning a Generative Meta-Model of LLM Activations', 'url': 'https://huggingface.co/papers/2602.06964', 'abstract': 'Training diffusion models on neural network activations creates meta-models that learn internal state distributions and improve intervention fidelity without restrictive structural assumptions.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing approaches for analyzing neural network activations, such as PCA and sparse autoencoders, rely on strong structural assumptions. Generative models offer an alternative: they can uncover structure without such assumptions and act as priors that improve intervention fidelity. We explore this direction by training diffusion models on one billion residual stream activations, creating "meta-models" that learn the distribution of a network\'s internal states. We find that diffusion loss decreases smoothly with compute and reliably predicts downstream utility. In particular, applying the meta-model\'s learned prior to steering interventions improves fluency, with larger gains as loss decreases. Moreover, the meta-model\'s neurons increasingly isolate concepts into individual units, with sparse probing scores that scale as loss decreases. These results suggest generative meta-models offer a scalable path toward interpretability without restrictive structural assumptions. Project page: https://generative-latent-prior.github.io.', 'score': 0, 'issue_id': 973, 'pub_date': '2026-02-06', 'pub_date_card': {'ru': '6 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 6', 'zh': '2Êúà6Êó•'}, 'hash': '1c21ecde75829e97', 'authors': ['Grace Luo', 'Jiahai Feng', 'Trevor Darrell', 'Alec Radford', 'Jacob Steinhardt'], 'affiliations': ['OpenAI', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2602.06964.jpg', 'data': {'categories': ['#training', '#interpretability', '#diffusion', '#architecture'], 'emoji': 'üß†', 'ru': {'title': '–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –∫–∞–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏—è—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π, —Å–æ–∑–¥–∞–≤–∞—è –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏–∑—É—á–∞—é—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π —Å–µ—Ç–∏. –ê–≤—Ç–æ—Ä—ã –æ–±—É—á–∏–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ –º–∏–ª–ª–∏–∞—Ä–¥–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–π –æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞ –∏ –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø–æ—Ç–µ—Ä–∏ –º–æ–¥–µ–ª–∏ —É–º–µ–Ω—å—à–∞—é—Ç—Å—è –ø–ª–∞–≤–Ω–æ —Å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞, –ø–æ–ª—É—á–µ–Ω–Ω–æ–≥–æ –º–µ—Ç–∞-–º–æ–¥–µ–ª—å—é, –∫ interventions —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞, –∞ –Ω–µ–π—Ä–æ–Ω—ã –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –≤—Å—ë –ª—É—á—à–µ –∏–∑–æ–ª–∏—Ä—É—é—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –µ–¥–∏–Ω–∏—Ü—ã. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –ø—É—Ç—å –∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—â–∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–π.'}, 'en': {'title': 'Unlocking Neural Insights with Diffusion Models', 'desc': "This paper discusses the use of diffusion models to analyze neural network activations, which helps in understanding the internal states of the network without needing strict structural assumptions. By training these models on a vast dataset of activations, the authors create 'meta-models' that learn the distribution of these internal states. The results show that as the model's loss decreases, the quality of interventions improves, leading to better performance in tasks. Additionally, the meta-models help in isolating concepts within the network, enhancing interpretability and scalability in machine learning applications."}, 'zh': {'title': 'Êó†ÁªìÊûÑÂÅáËÆæÁöÑÂèØËß£ÈáäÊÄßÊñ∞Ë∑ØÂæÑ', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂú®Á•ûÁªèÁΩëÁªúÊøÄÊ¥ª‰∏äËÆ≠ÁªÉÊâ©Êï£Ê®°ÂûãÁöÑÊñπÊ≥ïÔºåÂàõÂª∫‰∫ÜËÉΩÂ§üÂ≠¶‰π†ÂÜÖÈÉ®Áä∂ÊÄÅÂàÜÂ∏ÉÁöÑÂÖÉÊ®°Âûã„ÄÇËøô‰∫õÂÖÉÊ®°ÂûãÂú®Ê≤°Êúâ‰∏•Ê†ºÁªìÊûÑÂÅáËÆæÁöÑÊÉÖÂÜµ‰∏ãÔºåÊèêÈ´ò‰∫ÜÂπ≤È¢ÑÁöÑÂáÜÁ°ÆÊÄß„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊâ©Êï£ÊçüÂ§±ÈöèÁùÄËÆ°ÁÆóÈáèÁöÑÂ¢ûÂä†ËÄåÂπ≥Êªë‰∏ãÈôçÔºåÂπ∂ËÉΩÂèØÈù†Âú∞È¢ÑÊµã‰∏ãÊ∏∏ÊïàÁî®„ÄÇÁªìÊûúÊòæÁ§∫ÔºåÂÖÉÊ®°ÂûãÁöÑÁ•ûÁªèÂÖÉËÉΩÂ§üÂ∞ÜÊ¶ÇÂøµÈÄêÊ∏êÈöîÁ¶ª‰∏∫Áã¨Á´ãÂçïÂÖÉÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÂèØÊâ©Â±ïÁöÑÂèØËß£ÈáäÊÄßË∑ØÂæÑ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04811', 'title': 'SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization', 'url': 'https://huggingface.co/papers/2602.04811', 'abstract': 'SE-Bench presents a diagnostic environment that obscures NumPy\'s API to evaluate agents\' ability to internally store and utilize novel knowledge without external documentation, revealing challenges in knowledge retention and internalization through different training approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new\'\' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring "Closed-Book Training" to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https://github.com/thunlp/SE-Bench.', 'score': 0, 'issue_id': 974, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '7e37f52f85d1aa49', 'authors': ['Jiarui Yuan', 'Tailin Jin', 'Weize Chen', 'Zeyuan Liu', 'Zhiyuan Liu', 'Maosong Sun'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2602.04811.jpg', 'data': {'categories': ['#plp', '#benchmark', '#training', '#agents', '#rlhf', '#optimization', '#rl', '#open_source', '#reasoning', '#dataset'], 'emoji': 'üß†', 'ru': {'title': '–ö–∞–∫ –∑–∞—Å—Ç–∞–≤–∏—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ —É—á–∏—Ç—å—Å—è –Ω–∞ –≤—Å—é –∂–∏–∑–Ω—å', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SE-Bench ‚Äî –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫—É—é —Å—Ä–µ–¥—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ —É—Å–≤–∞–∏–≤–∞—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –Ω–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è –±–µ–∑ –¥–æ—Å—Ç—É–ø–∞ –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö –ø—Ä–æ–±–ª–µ–º—ã: –ø–∞—Ä–∞–¥–æ–∫—Å –æ—Ç–∫—Ä—ã—Ç–æ–π –∫–Ω–∏–≥–∏, –∫–æ–≥–¥–∞ –æ–±—É—á–µ–Ω–∏–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π –ø–æ–¥–∞–≤–ª—è–µ—Ç –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ, —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É –æ–±—É—á–µ–Ω–∏–µ–º —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –∏–Ω—Ç–µ—Ä–Ω–∞–ª–∏–∑–∞—Ü–∏–µ–π –∑–Ω–∞–Ω–∏–π –∏–∑-–∑–∞ —Ç–µ—Ö–Ω–∏–∫ –≤—Ä–æ–¥–µ PPO clipping, –∞ —Ç–∞–∫–∂–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —Å–∞–º–æ–∏–≥—Ä—ã –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —É—Å–≤–æ–µ–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –∏–Ω—Ç–µ—Ä–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è –≤ –≤–µ—Å–∞ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–∞–º–æ–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –∑–∞–¥–∞—á–∞—Ö –≤ —Å–æ—á–µ—Ç–∞–Ω–∏–∏ —Å fine-tuning, –Ω–æ –Ω–µ —á–µ—Ä–µ–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º.'}, 'en': {'title': 'Unlocking Lifelong Learning: The SE-Bench Approach', 'desc': 'SE-Bench is a diagnostic tool designed to evaluate how well agents can store and use new knowledge without relying on external documentation. It addresses challenges in knowledge retention by creating a pseudo-novel environment that obscures the familiar NumPy API, forcing agents to learn and adapt. The study uncovers key insights, such as the Open-Book Paradox, which shows that using reference materials can hinder knowledge retention, and the RL Gap, where traditional reinforcement learning struggles to fully internalize new information. Additionally, it highlights the effectiveness of Self-Play combined with supervised fine-tuning for knowledge internalization, establishing a robust framework for assessing lifelong learning in AI.'}, 'zh': {'title': 'Ëá™ÊàëËøõÂåñÔºöÁü•ËØÜÂÜÖÂåñÁöÑÊñ∞ÊåëÊàò', 'desc': 'SE-BenchÊòØ‰∏Ä‰∏™ËØäÊñ≠ÁéØÂ¢ÉÔºåÊó®Âú®ËØÑ‰º∞Êô∫ËÉΩ‰ΩìÂú®Ê≤°ÊúâÂ§ñÈÉ®ÊñáÊ°£ÁöÑÊÉÖÂÜµ‰∏ãÔºåÂÜÖÈÉ®Â≠òÂÇ®ÂíåÂà©Áî®Êñ∞Áü•ËØÜÁöÑËÉΩÂäõ„ÄÇËØ•Á†îÁ©∂Êè≠Á§∫‰∫ÜÁü•ËØÜ‰øùÁïôÂíåÂÜÖÂåñÁöÑÊåëÊàòÔºåÁâπÂà´ÊòØÂú®‰∏çÂêåÁöÑËÆ≠ÁªÉÊñπÊ≥ï‰∏ã„ÄÇÈÄöËøáÊ®°Á≥äNumPyÁöÑAPIÔºåÊô∫ËÉΩ‰ΩìË¢´ËÆ≠ÁªÉ‰ª•ÂÜÖÈÉ®Âåñ‰∏Ä‰∏™‰º™Êñ∞ÂåÖÔºåÂπ∂Âú®Ê≤°ÊúâÊñáÊ°£ÁöÑÊÉÖÂÜµ‰∏ãÂÆåÊàêÁÆÄÂçïÁöÑÁºñÁ†Å‰ªªÂä°„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂèÇËÄÉÊñáÊ°£ÁöÑËÆ≠ÁªÉ‰ºöÊäëÂà∂Áü•ËØÜÁöÑ‰øùÁïôÔºåËÄåËá™ÊàëÂ≠¶‰π†ÂíåËá™ÊàëÁîüÊàêÁöÑ‰ªªÂä°ÁªìÂêàÂæÆË∞ÉÂèØ‰ª•ÊúâÊïà‰øÉËøõÁü•ËØÜÁöÑÂÜÖÂåñ„ÄÇ'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (7)', '#agi', '#alignment (4)', '#architecture (8)', '#audio (3)', '#benchmark (13)', '#cv (4)', '#data (3)', '#dataset (11)', '#diffusion (2)', '#ethics', '#games', '#graphs (1)', '#hallucinations (2)', '#healthcare (1)', '#inference (5)', '#interpretability (3)', '#leakage', '#long_context (2)', '#low_resource (2)', '#machine_translation (1)', '#math (2)', '#multilingual (3)', '#multimodal (8)', '#open_source (13)', '#optimization (13)', '#plp (2)', '#rag (1)', '#reasoning (13)', '#rl (10)', '#rlhf (4)', '#robotics (2)', '#science (4)', '#security (1)', '#small_models (2)', '#story_generation', '#survey (2)', '#synthetic (1)', '#training (25)', '#transfer_learning (5)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            üî∫ ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'üîÑ ' + getTimeDiff('2026-02-09 21:33',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "—Ä–µ–π—Ç–∏–Ω–≥—É",
                    pub_date: "–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
                    issue_id: "–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "ËØÑÂàÜ",
                    pub_date: "ÂèëÂ∏ÉÊó•Êúü",
                    issue_id: "HF‰∏ä‰º†Êó•Êúü"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2026-02-09 21:33')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2026-02-09 21:33')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    