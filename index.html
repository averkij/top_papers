
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 15 papers. December 9.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">9 декабря</span> | <span id="title-articles-count">15 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2024-12-06.html">⬅️ <span id="prev-date">06.12</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-12-10.html">➡️ <span id="next-date">10.12</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-12.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '9 декабря', 'en': 'December 9', 'zh': '12月9日'};
        let feedDateNext = {'ru': '10.12', 'en': '12/10', 'zh': '12月10日'};
        let feedDatePrev = {'ru': '06.12', 'en': '12/06', 'zh': '12月6日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2412.05271', 'title': 'Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling', 'url': 'https://huggingface.co/papers/2412.05271', 'abstract': 'We introduce InternVL 2.5, an advanced multimodal large language model (MLLM) series that builds upon InternVL 2.0, maintaining its core model architecture while introducing significant enhancements in training and testing strategies as well as data quality. In this work, we delve into the relationship between model scaling and performance, systematically exploring the performance trends in vision encoders, language models, dataset sizes, and test-time configurations. Through extensive evaluations on a wide range of benchmarks, including multi-discipline reasoning, document understanding, multi-image / video understanding, real-world comprehension, multimodal hallucination detection, visual grounding, multilingual capabilities, and pure language processing, InternVL 2.5 exhibits competitive performance, rivaling leading commercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is the first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a 3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing strong potential for test-time scaling. We hope this model contributes to the open-source community by setting new standards for developing and applying multimodal AI systems. HuggingFace demo see https://huggingface.co/spaces/OpenGVLab/InternVL', 'score': 56, 'issue_id': 1013, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 декабря', 'en': 'December 6', 'zh': '12月6日'}, 'hash': '81590cc90bda9173', 'authors': ['Zhe Chen', 'Weiyun Wang', 'Yue Cao', 'Yangzhou Liu', 'Zhangwei Gao', 'Erfei Cui', 'Jinguo Zhu', 'Shenglong Ye', 'Hao Tian', 'Zhaoyang Liu', 'Lixin Gu', 'Xuehui Wang', 'Qingyun Li', 'Yimin Ren', 'Zixuan Chen', 'Jiapeng Luo', 'Jiahao Wang', 'Tan Jiang', 'Bo Wang', 'Conghui He', 'Botian Shi', 'Xingcheng Zhang', 'Han Lv', 'Yi Wang', 'Wenqi Shao', 'Pei Chu', 'Zhongying Tu', 'Tong He', 'Zhiyong Wu', 'Huipeng Deng', 'Jiaye Ge', 'Kai Chen', 'Min Dou', 'Lewei Lu', 'Xizhou Zhu', 'Tong Lu', 'Dahua Lin', 'Yu Qiao', 'Jifeng Dai', 'Wenhai Wang'], 'affiliations': ['Fudan University', 'Nanjing University', 'SenseTime Research', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.05271.jpg', 'data': {'categories': ['#open_source', '#hallucinations', '#reasoning', '#training', '#multimodal', '#benchmark', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Прорыв в мультимодальном ИИ: InternVL 2.5 устанавливает новые стандарты', 'desc': 'InternVL 2.5 - это усовершенствованная мультимодальная большая языковая модель (MLLM), развивающая архитектуру InternVL 2.0. Исследователи изучили связь между масштабированием модели и её производительностью, анализируя тренды в визуальных энкодерах, языковых моделях и размерах датасетов. Модель продемонстрировала конкурентоспособную производительность на различных бенчмарках, включая мультидисциплинарные рассуждения и понимание документов. InternVL 2.5 стала первой открытой MLLM, преодолевшей порог в 70% на бенчмарке MMMU, показав потенциал для масштабирования во время тестирования.'}, 'en': {'title': 'InternVL 2.5: Setting New Standards in Multimodal AI', 'desc': 'InternVL 2.5 is a state-of-the-art multimodal large language model that enhances its predecessor, InternVL 2.0, by improving training methods and data quality. The paper investigates how increasing the model size affects its performance across various tasks, including reasoning, document understanding, and multimodal comprehension. Extensive testing shows that InternVL 2.5 competes effectively with top commercial models, achieving significant benchmarks like surpassing 70% on the MMMU benchmark. This model aims to advance the open-source community by establishing new benchmarks for multimodal AI applications.'}, 'zh': {'title': '开创多模态AI新标准的InternVL 2.5', 'desc': '我们介绍了InternVL 2.5，这是一个先进的多模态大型语言模型系列，基于InternVL 2.0进行改进。该模型在训练和测试策略以及数据质量上进行了显著增强，并系统地探讨了模型规模与性能之间的关系。通过在多个基准测试上的广泛评估，InternVL 2.5展现了与领先商业模型如GPT-4o和Claude-3.5-Sonnet相媲美的竞争性能。我们的模型首次在MMMU基准上超过70%，并通过链式思维推理实现了3.7点的提升，展示了在测试时扩展的强大潜力。'}}}, {'id': 'https://huggingface.co/papers/2412.04814', 'title': 'LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment', 'url': 'https://huggingface.co/papers/2412.04814', 'abstract': 'Recent advancements in text-to-video (T2V) generative models have shown impressive capabilities. However, these models are still inadequate in aligning synthesized videos with human preferences (e.g., accurately reflecting text descriptions), which is particularly difficult to address, as human preferences are inherently subjective and challenging to formalize as objective functions. Therefore, this paper proposes LiFT, a novel fine-tuning method leveraging human feedback for T2V model alignment. Specifically, we first construct a Human Rating Annotation dataset, LiFT-HRA, consisting of approximately 10k human annotations, each including a score and its corresponding rationale. Based on this, we train a reward model LiFT-Critic to learn reward function effectively, which serves as a proxy for human judgment, measuring the alignment between given videos and human expectations. Lastly, we leverage the learned reward function to align the T2V model by maximizing the reward-weighted likelihood. As a case study, we apply our pipeline to CogVideoX-2B, showing that the fine-tuned model outperforms the CogVideoX-5B across all 16 metrics, highlighting the potential of human feedback in improving the alignment and quality of synthesized videos.', 'score': 32, 'issue_id': 1013, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 декабря', 'en': 'December 6', 'zh': '12月6日'}, 'hash': '3ac10cfceba4368e', 'authors': ['Yibin Wang', 'Zhiyu Tan', 'Junyan Wang', 'Xiaomeng Yang', 'Cheng Jin', 'Hao Li'], 'affiliations': ['Australian Institute for Machine Learning, The University of Adelaide', 'Fudan University', 'Shanghai Academy of Artificial Intelligence for Science'], 'pdf_title_img': 'assets/pdf/title_img/2412.04814.jpg', 'data': {'categories': ['#rlhf', '#dataset', '#training', '#alignment', '#video'], 'emoji': '🎥', 'ru': {'title': 'Улучшение генерации видео с помощью человеческих оценок', 'desc': 'Статья представляет LiFT - новый метод дообучения моделей генерации видео по тексту с использованием обратной связи от людей. Авторы создали датасет LiFT-HRA с 10 тысячами человеческих оценок видео и их обоснованиями. На основе этих данных была обучена модель-критик LiFT-Critic, оценивающая соответствие видео ожиданиям людей. Метод был применен к модели CogVideoX-2B, что позволило превзойти более крупную CogVideoX-5B по 16 метрикам.'}, 'en': {'title': 'Aligning Videos with Human Preferences Using Feedback', 'desc': 'This paper introduces LiFT, a new method for improving text-to-video (T2V) generative models by incorporating human feedback. The authors create a dataset called LiFT-HRA, which contains around 10,000 human annotations that provide scores and rationales for video quality. They develop a reward model, LiFT-Critic, to quantify how well generated videos align with human expectations, effectively serving as a stand-in for human judgment. By maximizing the reward-weighted likelihood using this model, they demonstrate that their approach significantly enhances the performance of T2V models, as shown in their case study with CogVideoX-2B.'}, 'zh': {'title': '利用人类反馈提升文本到视频生成模型的对齐性', 'desc': '最近，文本到视频生成模型（T2V）取得了显著进展，但在将生成的视频与人类偏好对齐方面仍然存在不足。由于人类偏好具有主观性，难以形式化为客观函数，因此本文提出了一种新颖的微调方法LiFT，利用人类反馈来改善T2V模型的对齐。我们构建了一个包含约1万条人类评分及其理由的注释数据集LiFT-HRA，并基于此训练了一个奖励模型LiFT-Critic，以有效学习奖励函数。通过最大化奖励加权的似然性，我们成功地将T2V模型与人类期望对齐，实验结果表明，微调后的模型在各项指标上均优于原模型，展示了人类反馈在提升生成视频质量方面的潜力。'}}}, {'id': 'https://huggingface.co/papers/2412.05237', 'title': 'MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale', 'url': 'https://huggingface.co/papers/2412.05237', 'abstract': 'Open-source multimodal large language models (MLLMs) have shown significant potential in a broad range of multimodal tasks. However, their reasoning capabilities remain constrained by existing instruction-tuning datasets, which were predominately repurposed from academic datasets such as VQA, AI2D, and ChartQA. These datasets target simplistic tasks, and only provide phrase-level answers without any intermediate rationales. To address these challenges, we introduce a scalable and cost-effective method to construct a large-scale multimodal instruction-tuning dataset with rich intermediate rationales designed to elicit CoT reasoning. Using only open models, we create a dataset containing 12M instruction-response pairs to cover diverse, reasoning-intensive tasks with detailed and faithful rationales. Experiments demonstrate that training MLLMs on this dataset significantly improves reasoning capabilities, achieving state-of-the-art performance on benchmarks such as MathVerse (+8.1%), MMMU-Pro (+7%), and MuirBench (+13.3%). Additionally, the model demonstrates notable improvements of up to 4% on non-reasoning-based benchmarks. Ablation studies further highlight the importance of key components, such as rewriting and self-filtering, in the dataset construction process.', 'score': 30, 'issue_id': 1012, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 декабря', 'en': 'December 6', 'zh': '12月6日'}, 'hash': '1e3d5645afd61cf2', 'authors': ['Jarvis Guo', 'Tuney Zheng', 'Yuelin Bai', 'Bo Li', 'Yubo Wang', 'King Zhu', 'Yizhi Li', 'Graham Neubig', 'Wenhu Chen', 'Xiang Yue'], 'affiliations': ['Carnegie Mellon University', 'Nanyang Technological University', 'The University of Manchester', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2412.05237.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#reasoning', '#dataset', '#training'], 'emoji': '🧠', 'ru': {'title': 'Улучшение рассуждений мультимодальных ИИ через обучение на масштабном наборе данных с промежуточными выводами', 'desc': 'Статья представляет новый метод создания масштабного набора данных для обучения мультимодальных языковых моделей с открытым исходным кодом. В отличие от существующих наборов данных, новый подход включает подробные промежуточные рассуждения, что способствует развитию навыков рассуждения у моделей. Авторы создали набор данных из 12 миллионов пар инструкций и ответов, охватывающих разнообразные задачи, требующие интенсивных рассуждений. Эксперименты показали значительное улучшение способностей моделей к рассуждению, достигая лучших результатов на нескольких бенчмарках.'}, 'en': {'title': 'Enhancing Reasoning in MLLMs with Rich Rationales', 'desc': "This paper presents a new method for creating a large-scale multimodal instruction-tuning dataset aimed at improving the reasoning abilities of multimodal large language models (MLLMs). The authors identify that existing datasets are limited as they mainly provide simple answers without detailed reasoning. To overcome this, they developed a dataset with 12 million instruction-response pairs that include rich intermediate rationales, promoting chain-of-thought (CoT) reasoning. Experiments show that training on this dataset enhances MLLMs' reasoning performance significantly, achieving state-of-the-art results on various benchmarks."}, 'zh': {'title': '提升多模态模型推理能力的新方法', 'desc': '这篇论文介绍了一种构建大规模多模态指令调优数据集的方法，以提高多模态大语言模型（MLLMs）的推理能力。现有的数据集主要来自学术研究，任务简单，缺乏中间推理过程的详细解释。我们的方法创建了一个包含1200万对指令和响应的数据集，涵盖了多样化且需要推理的任务，并提供了丰富的推理依据。实验结果表明，使用该数据集训练的MLLMs在多个基准测试中显著提高了推理能力，达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2412.04862', 'title': 'EXAONE 3.5: Series of Large Language Models for Real-world Use Cases', 'url': 'https://huggingface.co/papers/2412.04862', 'abstract': 'This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8B, and 2.4B. These models feature several standout capabilities: 1) exceptional instruction following capabilities in real-world scenarios, achieving the highest scores across seven benchmarks, 2) outstanding long-context comprehension, attaining the top performance in four benchmarks, and 3) competitive results compared to state-of-the-art open models of similar sizes across nine general benchmarks. The EXAONE 3.5 language models are open to anyone for research purposes and can be downloaded from https://huggingface.co/LGAI-EXAONE. For commercial use, please reach out to the official contact point of LG AI Research: contact_us@lgresearch.ai.', 'score': 29, 'issue_id': 1011, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 декабря', 'en': 'December 6', 'zh': '12月6日'}, 'hash': '83e6f957e42ebb5e', 'authors': ['LG AI Research', 'Soyoung An', 'Kyunghoon Bae', 'Eunbi Choi', 'Kibong Choi', 'Stanley Jungkyu Choi', 'Seokhee Hong', 'Junwon Hwang', 'Hyojin Jeon', 'Gerrard Jeongwon Jo', 'Hyunjik Jo', 'Jiyeon Jung', 'Yountae Jung', 'Hyosang Kim', 'Joonkee Kim', 'Seonghwan Kim', 'Soyeon Kim', 'Sunkyoung Kim', 'Yireun Kim', 'Yongil Kim', 'Youchul Kim', 'Edward Hwayoung Lee', 'Haeju Lee', 'Honglak Lee', 'Jinsik Lee', 'Kyungmin Lee', 'Woohyung Lim', 'Sangha Park', 'Sooyoun Park', 'Yongmin Park', 'Sihoon Yang', 'Heuiyeen Yeen', 'Hyeongu Yun'], 'affiliations': ['LG AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.04862.jpg', 'data': {'categories': ['#architecture', '#training', '#open_source', '#small_models', '#long_context', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'EXAONE 3.5: Новое слово в языковых моделях от LG AI Research', 'desc': 'Компания LG AI Research представила языковые модели EXAONE 3.5, обученные выполнению инструкций. Модели доступны в трех конфигурациях: 32B, 7.8B и 2.4B параметров. Они демонстрируют исключительные способности в следовании инструкциям, понимании длинного контекста и показывают конкурентоспособные результаты по сравнению с современными открытыми моделями аналогичного размера. Модели EXAONE 3.5 доступны для исследовательских целей и могут быть загружены с платформы Hugging Face.'}, 'en': {'title': 'EXAONE 3.5: Leading the Way in Instruction-Tuned Language Models', 'desc': 'The EXAONE 3.5 language models, created by LG AI Research, are advanced instruction-tuned models available in three sizes: 32B, 7.8B, and 2.4B. They excel in following instructions in real-world applications, achieving top scores on seven different benchmarks. Additionally, these models demonstrate superior long-context understanding, ranking first in four benchmarks. They are accessible for research purposes and show competitive performance against leading open models of similar sizes across nine general benchmarks.'}, 'zh': {'title': 'EXAONE 3.5：指令跟随与长上下文理解的先锋', 'desc': 'EXAONE 3.5 是由 LG AI 研究所开发的指令调优语言模型，提供三种配置：32B、7.8B 和 2.4B。这些模型在实际场景中具有卓越的指令跟随能力，在七个基准测试中取得了最高分。它们在长上下文理解方面表现出色，在四个基准测试中也达到了最佳性能。此外，与同类开源模型相比，EXAONE 3.5 在九个通用基准测试中也展现了竞争力的结果。'}}}, {'id': 'https://huggingface.co/papers/2412.05270', 'title': 'APOLLO: SGD-like Memory, AdamW-level Performance', 'url': 'https://huggingface.co/papers/2412.05270', 'abstract': "Large language models (LLMs) are notoriously memory-intensive during training, particularly with the popular AdamW optimizer. This memory burden necessitates using more or higher-end GPUs or reducing batch sizes, limiting training scalability and throughput. To address this, various memory-efficient optimizers have been proposed to reduce optimizer memory usage. However, they face critical challenges: (i) reliance on costly SVD operations; (ii) significant performance trade-offs compared to AdamW; and (iii) still substantial optimizer memory overhead to maintain competitive performance.   In this work, we identify that AdamW's learning rate adaptation rule can be effectively coarsened as a structured learning rate update. Based on this insight, we propose Approximated Gradient Scaling for Memory-Efficient LLM Optimization (APOLLO), which approximates learning rate scaling using an auxiliary low-rank optimizer state based on pure random projection. This structured learning rate update rule makes APOLLO highly tolerant to further memory reductions while delivering comparable pre-training performance. Even its rank-1 variant, APOLLO-Mini, achieves superior pre-training performance compared to AdamW with SGD-level memory costs.   Extensive experiments demonstrate that the APOLLO series performs on-par with or better than AdamW, while achieving greater memory savings by nearly eliminating the optimization states of AdamW. These savings provide significant system-level benefits: (1) Enhanced Throughput: 3x throughput on an 8xA100-80GB setup compared to AdamW by supporting 4x larger batch sizes. (2) Improved Model Scalability: Pre-training LLaMA-13B with naive DDP on A100-80GB GPUs without system-level optimizations. (3) Low-End GPU Friendly Pre-training: Pre-training LLaMA-7B on a single GPU using less than 12 GB of memory with weight quantization.", 'score': 23, 'issue_id': 1012, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 декабря', 'en': 'December 6', 'zh': '12月6日'}, 'hash': '14bb480f5fe29bae', 'authors': ['Hanqing Zhu', 'Zhenyu Zhang', 'Wenyan Cong', 'Xi Liu', 'Sem Park', 'Vikas Chandra', 'Bo Long', 'David Z. Pan', 'Zhangyang Wang', 'Jinwon Lee'], 'affiliations': ['Department of Electrical and Computer Engineering, The University of Texas at Austin', 'Meta'], 'pdf_title_img': 'assets/pdf/title_img/2412.05270.jpg', 'data': {'categories': ['#inference', '#training', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'APOLLO: эффективное обучение LLM с минимальными затратами памяти', 'desc': 'Статья представляет новый оптимизатор APOLLO для обучения больших языковых моделей (LLM). APOLLO использует структурированное обновление скорости обучения, основанное на случайном проецировании, что позволяет значительно снизить потребление памяти по сравнению с популярным оптимизатором AdamW. Эксперименты показывают, что APOLLO достигает сопоставимой или лучшей производительности, чем AdamW, при существенной экономии памяти. Это позволяет увеличить пропускную способность, масштабируемость модели и делает возможным обучение LLM даже на GPU среднего уровня.'}, 'en': {'title': 'APOLLO: Memory-Efficient Optimization for Large Language Models', 'desc': 'This paper addresses the high memory requirements of training large language models (LLMs) using the AdamW optimizer. It introduces a new method called APOLLO, which simplifies the learning rate adaptation process to reduce memory usage while maintaining performance. APOLLO utilizes a low-rank optimizer state based on random projection, allowing for significant memory savings without sacrificing training efficiency. The results show that APOLLO can achieve comparable or better performance than AdamW, enabling larger batch sizes and making LLM training feasible on lower-end GPUs.'}, 'zh': {'title': 'APOLLO：高效内存优化的未来', 'desc': '大型语言模型（LLMs）在训练过程中对内存的需求非常高，尤其是使用流行的AdamW优化器时。为了解决这个问题，研究者们提出了多种内存高效的优化器，但它们面临着依赖昂贵的SVD操作和性能折衷等挑战。本文提出了一种名为APOLLO的优化方法，通过近似学习率缩放来减少内存使用，同时保持与AdamW相当的预训练性能。实验结果表明，APOLLO系列在内存节省方面表现优异，能够在较低的内存成本下实现更高的训练吞吐量和模型可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2412.04301', 'title': 'SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step Diffusion', 'url': 'https://huggingface.co/papers/2412.04301', 'abstract': 'Recent advances in text-guided image editing enable users to perform image edits through simple text inputs, leveraging the extensive priors of multi-step diffusion-based text-to-image models. However, these methods often fall short of the speed demands required for real-world and on-device applications due to the costly multi-step inversion and sampling process involved. In response to this, we introduce SwiftEdit, a simple yet highly efficient editing tool that achieve instant text-guided image editing (in 0.23s). The advancement of SwiftEdit lies in its two novel contributions: a one-step inversion framework that enables one-step image reconstruction via inversion and a mask-guided editing technique with our proposed attention rescaling mechanism to perform localized image editing. Extensive experiments are provided to demonstrate the effectiveness and efficiency of SwiftEdit. In particular, SwiftEdit enables instant text-guided image editing, which is extremely faster than previous multi-step methods (at least 50 times faster) while maintain a competitive performance in editing results. Our project page is at: https://swift-edit.github.io/', 'score': 18, 'issue_id': 1011, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 декабря', 'en': 'December 5', 'zh': '12月5日'}, 'hash': 'a4cea89a59a9a3c0', 'authors': ['Trong-Tung Nguyen', 'Quang Nguyen', 'Khoi Nguyen', 'Anh Tran', 'Cuong Pham'], 'affiliations': ['Posts & Telecom. Inst. of Tech., Vietnam', 'VinAI Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.04301.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#cv'], 'emoji': '🚀', 'ru': {'title': 'Мгновенное редактирование изображений текстом', 'desc': 'SwiftEdit - это новый инструмент для быстрого редактирования изображений с помощью текстовых запросов. В отличие от существующих многошаговых методов, SwiftEdit использует одношаговую инверсию и механизм масштабирования внимания для локального редактирования. Это позволяет выполнять изменения изображений практически мгновенно - за 0.23 секунды. SwiftEdit работает как минимум в 50 раз быстрее аналогов, сохраняя при этом высокое качество результатов.'}, 'en': {'title': 'SwiftEdit: Instant Text-Guided Image Editing at Lightning Speed!', 'desc': 'This paper presents SwiftEdit, a new tool for fast text-guided image editing that significantly improves the speed of image modifications. Traditional methods rely on multi-step diffusion processes, which can be slow and inefficient for real-time applications. SwiftEdit introduces a one-step inversion framework that allows for quick image reconstruction and a mask-guided editing technique that uses attention rescaling for precise edits. The results show that SwiftEdit is at least 50 times faster than previous methods while still delivering high-quality editing outcomes.'}, 'zh': {'title': 'SwiftEdit：瞬时文本引导图像编辑的革命', 'desc': '最近的文本引导图像编辑技术使用户能够通过简单的文本输入进行图像编辑，利用了多步扩散模型的丰富先验知识。然而，这些方法在实际应用中往往速度较慢，无法满足实时和设备端的需求。为此，我们提出了SwiftEdit，这是一种简单而高效的编辑工具，可以实现瞬时的文本引导图像编辑，速度达到0.23秒。SwiftEdit的创新在于其一体化的反演框架和基于掩码的编辑技术，能够快速进行局部图像编辑，同时保持与传统多步方法相当的编辑效果。'}}}, {'id': 'https://huggingface.co/papers/2412.04445', 'title': 'Moto: Latent Motion Token as the Bridging Language for Robot Manipulation', 'url': 'https://huggingface.co/papers/2412.04445', 'abstract': 'Recent developments in Large Language Models pre-trained on extensive corpora have shown significant success in various natural language processing tasks with minimal fine-tuning. This success offers new promise for robotics, which has long been constrained by the high cost of action-labeled data. We ask: given the abundant video data containing interaction-related knowledge available as a rich "corpus", can a similar generative pre-training approach be effectively applied to enhance robot learning? The key challenge is to identify an effective representation for autoregressive pre-training that benefits robot manipulation tasks. Inspired by the way humans learn new skills through observing dynamic environments, we propose that effective robotic learning should emphasize motion-related knowledge, which is closely tied to low-level actions and is hardware-agnostic, facilitating the transfer of learned motions to actual robot actions. To this end, we introduce Moto, which converts video content into latent Motion Token sequences by a Latent Motion Tokenizer, learning a bridging "language" of motion from videos in an unsupervised manner. We pre-train Moto-GPT through motion token autoregression, enabling it to capture diverse visual motion knowledge. After pre-training, Moto-GPT demonstrates the promising ability to produce semantically interpretable motion tokens, predict plausible motion trajectories, and assess trajectory rationality through output likelihood. To transfer learned motion priors to real robot actions, we implement a co-fine-tuning strategy that seamlessly bridges latent motion token prediction and real robot control. Extensive experiments show that the fine-tuned Moto-GPT exhibits superior robustness and efficiency on robot manipulation benchmarks, underscoring its effectiveness in transferring knowledge from video data to downstream visual manipulation tasks.', 'score': 18, 'issue_id': 1011, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 декабря', 'en': 'December 5', 'zh': '12月5日'}, 'hash': 'a5ac6d786500ef9f', 'authors': ['Yi Chen', 'Yuying Ge', 'Yizhuo Li', 'Yixiao Ge', 'Mingyu Ding', 'Ying Shan', 'Xihui Liu'], 'affiliations': ['ARC Lab, Tencent PCG', 'The University of Hong Kong', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2412.04445.jpg', 'data': {'categories': ['#training', '#transfer_learning', '#games', '#robotics', '#multimodal', '#video'], 'emoji': '🤖', 'ru': {'title': 'Обучение роботов движению через предобучение на видеоданных', 'desc': 'В статье представлен новый подход к обучению роботов на основе предобученных языковых моделей. Авторы предлагают метод Moto, который преобразует видеоконтент в последовательности латентных токенов движения с помощью Latent Motion Tokenizer. Moto-GPT обучается на основе авторегрессии токенов движения, что позволяет ей захватывать разнообразные знания о визуальном движении. После предобучения и дообучения Moto-GPT демонстрирует высокую эффективность в задачах манипуляции роботов.'}, 'en': {'title': 'Bridging Video Knowledge to Robot Actions with Moto-GPT', 'desc': 'This paper explores the application of Large Language Models to enhance robot learning by leveraging abundant video data. It introduces Moto, a system that converts video content into Motion Token sequences, allowing robots to learn motion-related knowledge in an unsupervised manner. The approach emphasizes the importance of motion understanding for effective robotic manipulation, enabling the transfer of learned skills to real-world actions. Through extensive experiments, Moto-GPT demonstrates improved performance in robot manipulation tasks, showcasing its ability to bridge the gap between video knowledge and practical robot control.'}, 'zh': {'title': '利用视频数据提升机器人学习能力', 'desc': '这篇论文探讨了如何利用大规模视频数据来提升机器人学习能力。研究者提出了一种名为Moto的模型，通过将视频内容转换为潜在的运动标记序列，来学习运动知识。Moto-GPT经过预训练后，能够生成可解释的运动标记，并预测合理的运动轨迹。最终，研究表明，经过微调的Moto-GPT在机器人操作基准测试中表现出色，证明了从视频数据转移知识的有效性。'}}}, {'id': 'https://huggingface.co/papers/2412.04440', 'title': 'GenMAC: Compositional Text-to-Video Generation with Multi-Agent Collaboration', 'url': 'https://huggingface.co/papers/2412.04440', 'abstract': 'Text-to-video generation models have shown significant progress in the recent years. However, they still struggle with generating complex dynamic scenes based on compositional text prompts, such as attribute binding for multiple objects, temporal dynamics associated with different objects, and interactions between objects. Our key motivation is that complex tasks can be decomposed into simpler ones, each handled by a role-specialized MLLM agent. Multiple agents can collaborate together to achieve collective intelligence for complex goals. We propose GenMAC, an iterative, multi-agent framework that enables compositional text-to-video generation. The collaborative workflow includes three stages: Design, Generation, and Redesign, with an iterative loop between the Generation and Redesign stages to progressively verify and refine the generated videos. The Redesign stage is the most challenging stage that aims to verify the generated videos, suggest corrections, and redesign the text prompts, frame-wise layouts, and guidance scales for the next iteration of generation. To avoid hallucination of a single MLLM agent, we decompose this stage to four sequentially-executed MLLM-based agents: verification agent, suggestion agent, correction agent, and output structuring agent. Furthermore, to tackle diverse scenarios of compositional text-to-video generation, we design a self-routing mechanism to adaptively select the proper correction agent from a collection of correction agents each specialized for one scenario. Extensive experiments demonstrate the effectiveness of GenMAC, achieving state-of-the art performance in compositional text-to-video generation.', 'score': 13, 'issue_id': 1011, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 декабря', 'en': 'December 5', 'zh': '12月5日'}, 'hash': '28dc2191ba71c4ea', 'authors': ['Kaiyi Huang', 'Yukun Huang', 'Xuefei Ning', 'Zinan Lin', 'Yu Wang', 'Xihui Liu'], 'affiliations': ['Microsoft Research', 'The University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.04440.jpg', 'data': {'categories': ['#hallucinations', '#agents', '#games', '#multimodal', '#video'], 'emoji': '🎬', 'ru': {'title': 'Коллективный интеллект агентов для создания сложных видео по тексту', 'desc': 'Статья представляет GenMAC - итеративную мультиагентную систему для генерации видео по текстовому описанию. Система использует несколько специализированных агентов на основе мультимодальных языковых моделей (MLLM) для декомпозиции сложных задач. Процесс включает этапы проектирования, генерации и перепроектирования, с итеративным циклом между генерацией и перепроектированием для постепенного улучшения результата. GenMAC демонстрирует передовые результаты в генерации композиционных видео по текстовому описанию.'}, 'en': {'title': 'Collaborative Intelligence for Text-to-Video Mastery', 'desc': 'This paper introduces GenMAC, a multi-agent framework designed to improve text-to-video generation by breaking down complex tasks into simpler components. It utilizes specialized machine learning agents that collaborate through three main stages: Design, Generation, and Redesign, with an iterative process to refine video outputs. The Redesign stage is particularly crucial as it involves multiple agents that verify, suggest corrections, and restructure prompts to enhance the generated videos. The framework also includes a self-routing mechanism to select the most suitable correction agent for various scenarios, leading to significant advancements in generating dynamic scenes from text prompts.'}, 'zh': {'title': 'GenMAC：协作生成复杂视频的智能框架', 'desc': '文本到视频生成模型近年来取得了显著进展，但在生成复杂动态场景时仍面临挑战。我们提出了一种名为GenMAC的多代理框架，通过将复杂任务分解为简单任务来实现协作生成。该框架包括设计、生成和重新设计三个阶段，生成和重新设计之间存在迭代循环，以逐步验证和优化生成的视频。通过自适应选择专门化的修正代理，GenMAC能够有效应对多样化的文本到视频生成场景。'}}}, {'id': 'https://huggingface.co/papers/2412.04887', 'title': 'Momentum-GS: Momentum Gaussian Self-Distillation for High-Quality Large Scene Reconstruction', 'url': 'https://huggingface.co/papers/2412.04887', 'abstract': "3D Gaussian Splatting has demonstrated notable success in large-scale scene reconstruction, but challenges persist due to high training memory consumption and storage overhead. Hybrid representations that integrate implicit and explicit features offer a way to mitigate these limitations. However, when applied in parallelized block-wise training, two critical issues arise since reconstruction accuracy deteriorates due to reduced data diversity when training each block independently, and parallel training restricts the number of divided blocks to the available number of GPUs. To address these issues, we propose Momentum-GS, a novel approach that leverages momentum-based self-distillation to promote consistency and accuracy across the blocks while decoupling the number of blocks from the physical GPU count. Our method maintains a teacher Gaussian decoder updated with momentum, ensuring a stable reference during training. This teacher provides each block with global guidance in a self-distillation manner, promoting spatial consistency in reconstruction. To further ensure consistency across the blocks, we incorporate block weighting, dynamically adjusting each block's weight according to its reconstruction accuracy. Extensive experiments on large-scale scenes show that our method consistently outperforms existing techniques, achieving a 12.8% improvement in LPIPS over CityGaussian with much fewer divided blocks and establishing a new state of the art. Project page: https://jixuan-fan.github.io/Momentum-GS_Page/", 'score': 10, 'issue_id': 1025, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 декабря', 'en': 'December 6', 'zh': '12月6日'}, 'hash': 'a52f45b90e5831fe', 'authors': ['Jixuan Fan', 'Wanhua Li', 'Yifei Han', 'Yansong Tang'], 'affiliations': ['Harvard University', 'Tsinghua Shenzhen International Graduate School, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.04887.jpg', 'data': {'categories': ['#training', '#optimization', '#3d'], 'emoji': '🏙️', 'ru': {'title': 'Революция в 3D реконструкции: Momentum-GS повышает точность и эффективность', 'desc': 'Статья представляет новый метод Momentum-GS для улучшения 3D реконструкции больших сцен с использованием гауссовского сплаттинга. Авторы предлагают использовать самодистилляцию на основе момента для повышения согласованности и точности между блоками при параллельном обучении. Метод включает учитель-декодер гауссиан, обновляемый с помощью момента, который обеспечивает глобальное руководство для каждого блока. Эксперименты показывают, что Momentum-GS превосходит существующие методы, достигая улучшения на 12.8% по метрике LPIPS по сравнению с CityGaussian при использовании меньшего количества блоков.'}, 'en': {'title': 'Enhancing 3D Reconstruction with Momentum-GS', 'desc': 'This paper introduces Momentum-GS, a new method for improving 3D scene reconstruction using Gaussian splatting. It addresses the challenges of high memory usage and limited data diversity in parallelized training by employing momentum-based self-distillation. This technique allows for consistent and accurate reconstruction across multiple blocks, independent of the number of available GPUs. The results show significant improvements in reconstruction quality, outperforming existing methods and setting a new benchmark in the field.'}, 'zh': {'title': '动量自蒸馏，提升3D重建一致性', 'desc': '3D高斯点云技术在大规模场景重建中取得了显著成功，但仍面临高训练内存消耗和存储开销的问题。混合表示法结合了隐式和显式特征，能够缓解这些限制。然而，在并行块训练中，由于每个块独立训练导致数据多样性降低，重建精度下降，同时并行训练限制了可用GPU数量。为了解决这些问题，我们提出了Momentum-GS方法，通过动量自蒸馏促进块间的一致性和准确性，同时将块的数量与物理GPU数量解耦。'}}}, {'id': 'https://huggingface.co/papers/2412.05243', 'title': 'CompCap: Improving Multimodal Large Language Models with Composite Captions', 'url': 'https://huggingface.co/papers/2412.05243', 'abstract': "How well can Multimodal Large Language Models (MLLMs) understand composite images? Composite images (CIs) are synthetic visuals created by merging multiple visual elements, such as charts, posters, or screenshots, rather than being captured directly by a camera. While CIs are prevalent in real-world applications, recent MLLM developments have primarily focused on interpreting natural images (NIs). Our research reveals that current MLLMs face significant challenges in accurately understanding CIs, often struggling to extract information or perform complex reasoning based on these images. We find that existing training data for CIs are mostly formatted for question-answer tasks (e.g., in datasets like ChartQA and ScienceQA), while high-quality image-caption datasets, critical for robust vision-language alignment, are only available for NIs. To bridge this gap, we introduce Composite Captions (CompCap), a flexible framework that leverages Large Language Models (LLMs) and automation tools to synthesize CIs with accurate and detailed captions. Using CompCap, we curate CompCap-118K, a dataset containing 118K image-caption pairs across six CI types. We validate the effectiveness of CompCap-118K by supervised fine-tuning MLLMs of three sizes: xGen-MM-inst.-4B and LLaVA-NeXT-Vicuna-7B/13B. Empirical results show that CompCap-118K significantly enhances MLLMs' understanding of CIs, yielding average gains of 1.7%, 2.0%, and 2.9% across eleven benchmarks, respectively.", 'score': 7, 'issue_id': 1027, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 декабря', 'en': 'December 6', 'zh': '12月6日'}, 'hash': '718a80687afd1e64', 'authors': ['Xiaohui Chen', 'Satya Narayan Shukla', 'Mahmoud Azab', 'Aashu Singh', 'Qifan Wang', 'David Yang', 'ShengYun Peng', 'Hanchao Yu', 'Shen Yan', 'Xuewen Zhang', 'Baosheng He'], 'affiliations': ['Georgia Tech', 'Meta', 'Tufts University'], 'pdf_title_img': 'assets/pdf/title_img/2412.05243.jpg', 'data': {'categories': ['#data', '#dataset', '#optimization', '#synthetic', '#multimodal', '#training'], 'emoji': '🖼️', 'ru': {'title': 'Улучшение понимания композитных изображений мультимодальными ИИ-моделями', 'desc': 'Исследование показывает, что мультимодальные большие языковые модели (MLLM) испытывают трудности в понимании композитных изображений, созданных из нескольких визуальных элементов. Авторы представляют CompCap - фреймворк для создания датасета с композитными изображениями и подробными описаниями. Используя CompCap, они создали датасет CompCap-118K из 118 тысяч пар изображение-описание. Дообучение MLLM на этом датасете значительно улучшило понимание моделями композитных изображений, повысив производительность на 1.7-2.9% на различных бенчмарках.'}, 'en': {'title': 'Enhancing MLLM Understanding of Composite Images with CompCap', 'desc': "This paper investigates how well Multimodal Large Language Models (MLLMs) can interpret composite images, which are created by combining various visual elements. The authors highlight that MLLMs have primarily been trained on natural images, leading to difficulties in understanding composite images due to a lack of suitable training data. To address this issue, they propose a new framework called Composite Captions (CompCap) that generates detailed captions for composite images, thus improving the training process. The study introduces a dataset, CompCap-118K, which significantly enhances MLLMs' performance on composite image understanding tasks, demonstrating measurable improvements across multiple benchmarks."}, 'zh': {'title': '提升多模态模型对复合图像的理解能力', 'desc': '本研究探讨了多模态大型语言模型（MLLMs）对复合图像（CIs）的理解能力。复合图像是通过合成多个视觉元素而创建的图像，当前的MLLMs在理解这些图像时面临重大挑战。我们提出了复合标题（CompCap）框架，利用大型语言模型和自动化工具生成准确的图像标题，并创建了包含118K图像-标题对的数据集CompCap-118K。实验结果表明，CompCap-118K显著提升了MLLMs对复合图像的理解能力。'}}}, {'id': 'https://huggingface.co/papers/2412.05263', 'title': 'Mind the Time: Temporally-Controlled Multi-Event Video Generation', 'url': 'https://huggingface.co/papers/2412.05263', 'abstract': 'Real-world videos consist of sequences of events. Generating such sequences with precise temporal control is infeasible with existing video generators that rely on a single paragraph of text as input. When tasked with generating multiple events described using a single prompt, such methods often ignore some of the events or fail to arrange them in the correct order. To address this limitation, we present MinT, a multi-event video generator with temporal control. Our key insight is to bind each event to a specific period in the generated video, which allows the model to focus on one event at a time. To enable time-aware interactions between event captions and video tokens, we design a time-based positional encoding method, dubbed ReRoPE. This encoding helps to guide the cross-attention operation. By fine-tuning a pre-trained video diffusion transformer on temporally grounded data, our approach produces coherent videos with smoothly connected events. For the first time in the literature, our model offers control over the timing of events in generated videos. Extensive experiments demonstrate that MinT outperforms existing open-source models by a large margin.', 'score': 6, 'issue_id': 1014, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 декабря', 'en': 'December 6', 'zh': '12月6日'}, 'hash': '826bb588770d5c27', 'authors': ['Ziyi Wu', 'Aliaksandr Siarohin', 'Willi Menapace', 'Ivan Skorokhodov', 'Yuwei Fang', 'Varnith Chordia', 'Igor Gilitschenski', 'Sergey Tulyakov'], 'affiliations': ['Snap Research', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2412.05263.jpg', 'data': {'categories': ['#open_source', '#training', '#architecture', '#diffusion', '#video', '#multimodal'], 'emoji': '⏱️', 'ru': {'title': 'Точный контроль времени в генерации видео с несколькими событиями', 'desc': 'Статья представляет MinT - генератор видео с множеством событий и временным контролем. Метод привязывает каждое событие к определенному периоду в генерируемом видео, что позволяет модели фокусироваться на одном событии за раз. Авторы разработали метод временного позиционного кодирования ReRoPE для управления взаимодействием между описаниями событий и видеотокенами. Модель, дообученная на темпорально размеченных данных, создает согласованные видео с плавно соединенными событиями, превосходя существующие открытые модели.'}, 'en': {'title': 'MinT: Mastering Multi-Event Video Generation with Temporal Precision', 'desc': "This paper introduces MinT, a novel multi-event video generator that allows for precise temporal control over the events depicted in generated videos. Unlike traditional models that struggle with sequencing multiple events from a single text prompt, MinT binds each event to a specific time period, ensuring that all events are accurately represented and ordered. The authors implement a time-based positional encoding method called ReRoPE, which enhances the model's ability to manage interactions between event descriptions and video frames. By fine-tuning a pre-trained video diffusion transformer on data with temporal grounding, MinT achieves superior performance in generating coherent videos with well-timed events."}, 'zh': {'title': 'MinT：精准控制视频事件时序的生成器', 'desc': '本论文提出了一种名为MinT的多事件视频生成器，旨在解决现有视频生成器在生成多个事件时的时间控制问题。通过将每个事件绑定到生成视频的特定时间段，MinT能够逐个关注事件，从而提高生成视频的连贯性。我们设计了一种基于时间的位置信息编码方法ReRoPE，以增强事件描述与视频帧之间的交互。实验结果表明，MinT在生成视频的时间控制方面优于现有的开源模型。'}}}, {'id': 'https://huggingface.co/papers/2412.03428', 'title': '2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains for High-Fidelity Indoor Scene Reconstruction', 'url': 'https://huggingface.co/papers/2412.03428', 'abstract': 'The reconstruction of indoor scenes remains challenging due to the inherent complexity of spatial structures and the prevalence of textureless regions. Recent advancements in 3D Gaussian Splatting have improved novel view synthesis with accelerated processing but have yet to deliver comparable performance in surface reconstruction. In this paper, we introduce 2DGS-Room, a novel method leveraging 2D Gaussian Splatting for high-fidelity indoor scene reconstruction. Specifically, we employ a seed-guided mechanism to control the distribution of 2D Gaussians, with the density of seed points dynamically optimized through adaptive growth and pruning mechanisms. To further improve geometric accuracy, we incorporate monocular depth and normal priors to provide constraints for details and textureless regions respectively. Additionally, multi-view consistency constraints are employed to mitigate artifacts and further enhance reconstruction quality. Extensive experiments on ScanNet and ScanNet++ datasets demonstrate that our method achieves state-of-the-art performance in indoor scene reconstruction.', 'score': 5, 'issue_id': 1013, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 декабря', 'en': 'December 4', 'zh': '12月4日'}, 'hash': 'ead3f67b9be4d52b', 'authors': ['Wanting Zhang', 'Haodong Xiang', 'Zhichao Liao', 'Xiansong Lai', 'Xinghui Li', 'Long Zeng'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.03428.jpg', 'data': {'categories': ['#3d'], 'emoji': '🏠', 'ru': {'title': '2DGS-Room: Революция в реконструкции интерьеров с помощью 2D гауссовского сплаттинга', 'desc': 'Эта статья представляет новый метод 2DGS-Room для реконструкции интерьеров с использованием 2D гауссовского сплаттинга. Авторы применяют механизм управления распределением 2D гауссианов с помощью семян, оптимизируя их плотность через адаптивный рост и отсечение. Для улучшения геометрической точности используются монокулярные глубинные и нормальные приоры, а также ограничения многоракурсной согласованности. Эксперименты на наборах данных ScanNet и ScanNet++ показывают, что метод достигает наилучших результатов в реконструкции интерьеров.'}, 'en': {'title': 'Revolutionizing Indoor Scene Reconstruction with 2D Gaussian Splatting', 'desc': 'This paper presents 2DGS-Room, a new approach for reconstructing indoor scenes using 2D Gaussian Splatting. The method introduces a seed-guided mechanism that optimizes the distribution of 2D Gaussians, enhancing the reconstruction process. By incorporating monocular depth and normal priors, the approach improves geometric accuracy, especially in areas lacking texture. The use of multi-view consistency constraints helps reduce artifacts, leading to high-fidelity results in indoor scene reconstruction, as demonstrated by experiments on ScanNet and ScanNet++ datasets.'}, 'zh': {'title': '高保真室内场景重建的新方法', 'desc': '室内场景的重建因空间结构复杂和无纹理区域的普遍存在而具有挑战性。本文提出了一种新方法2DGS-Room，利用2D高斯点云实现高保真度的室内场景重建。我们采用种子引导机制来控制2D高斯的分布，并通过自适应生长和修剪机制动态优化种子点的密度。通过结合单目深度和法线先验，我们进一步提高了几何精度，并使用多视图一致性约束来减少伪影，提升重建质量。'}}}, {'id': 'https://huggingface.co/papers/2412.04827', 'title': 'PanoDreamer: 3D Panorama Synthesis from a Single Image', 'url': 'https://huggingface.co/papers/2412.04827', 'abstract': 'In this paper, we present PanoDreamer, a novel method for producing a coherent 360^circ 3D scene from a single input image. Unlike existing methods that generate the scene sequentially, we frame the problem as single-image panorama and depth estimation. Once the coherent panoramic image and its corresponding depth are obtained, the scene can be reconstructed by inpainting the small occluded regions and projecting them into 3D space. Our key contribution is formulating single-image panorama and depth estimation as two optimization tasks and introducing alternating minimization strategies to effectively solve their objectives. We demonstrate that our approach outperforms existing techniques in single-image 360^circ scene reconstruction in terms of consistency and overall quality.', 'score': 3, 'issue_id': 1026, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 декабря', 'en': 'December 6', 'zh': '12月6日'}, 'hash': 'db76a410fcf3c53c', 'authors': ['Avinash Paliwal', 'Xilong Zhou', 'Andrii Tsarov', 'Nima Khademi Kalantari'], 'affiliations': ['Leia Inc.', 'Max Planck Institute for Informatics', 'Texas A&M University'], 'pdf_title_img': 'assets/pdf/title_img/2412.04827.jpg', 'data': {'categories': ['#cv', '#optimization', '#3d'], 'emoji': '🌐', 'ru': {'title': 'PanoDreamer: Целостная 360° 3D-сцена из одного изображения', 'desc': 'PanoDreamer - это новый метод для создания целостной 360-градусной 3D-сцены из одного входного изображения. В отличие от существующих подходов, генерирующих сцену последовательно, авторы формулируют задачу как оценку панорамы и глубины по одному изображению. Ключевой вклад заключается в формулировке этой задачи как двух оптимизационных подзадач и введении стратегий поочередной минимизации для их эффективного решения. Эксперименты показывают, что предложенный подход превосходит существующие методы реконструкции 360-градусных сцен по одному изображению с точки зрения согласованности и общего качества.'}, 'en': {'title': 'Revolutionizing 360-Degree Scene Reconstruction from a Single Image', 'desc': 'PanoDreamer is a new method that creates a complete 360-degree 3D scene from just one image. Instead of building the scene step by step, it treats the task as estimating a panoramic view and its depth simultaneously. The method fills in missing parts of the image and converts them into 3D space. By using optimization techniques, PanoDreamer achieves better consistency and quality compared to previous methods for 360-degree scene reconstruction.'}, 'zh': {'title': '单图生成360度3D场景的新方法', 'desc': '本文提出了一种新方法PanoDreamer，可以从单张输入图像生成一致的360度3D场景。与现有方法按顺序生成场景不同，我们将问题框定为单图全景和深度估计。通过获取一致的全景图像及其对应的深度，我们可以通过修复小的遮挡区域并将其投影到3D空间中来重建场景。我们的主要贡献是将单图全景和深度估计形式化为两个优化任务，并引入交替最小化策略来有效解决这些目标。'}}}, {'id': 'https://huggingface.co/papers/2412.04905', 'title': 'DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling', 'url': 'https://huggingface.co/papers/2412.04905', 'abstract': 'Large language models (LLMs) have made dialogue one of the central modes of human-machine interaction, leading to the accumulation of vast amounts of conversation logs and increasing demand for dialogue generation. A conversational life-cycle spans from the Prelude through the Interlocution to the Epilogue, encompassing various elements. Despite the existence of numerous dialogue-related studies, there is a lack of benchmarks that encompass comprehensive dialogue elements, hindering precise modeling and systematic evaluation. To bridge this gap, we introduce an innovative research task Dialogue Element MOdeling, including Element Awareness and Dialogue Agent Interaction, and propose a novel benchmark, DEMO, designed for a comprehensive dialogue modeling and assessment. Inspired by imitation learning, we further build the agent which possesses the adept ability to model dialogue elements based on the DEMO benchmark. Extensive experiments indicate that existing LLMs still exhibit considerable potential for enhancement, and our DEMO agent has superior performance in both in-domain and out-of-domain tasks.', 'score': 3, 'issue_id': 1017, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 декабря', 'en': 'December 6', 'zh': '12月6日'}, 'hash': '4b12d357252b671f', 'authors': ['Minzheng Wang', 'Xinghua Zhang', 'Kun Chen', 'Nan Xu', 'Haiyang Yu', 'Fei Huang', 'Wenji Mao', 'Yongbin Li'], 'affiliations': ['MAIS, Institute of Automation, Chinese Academy of Sciences', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2412.04905.jpg', 'data': {'categories': ['#benchmark', '#dialogue_generation', '#agents'], 'emoji': '🗣️', 'ru': {'title': 'DEMO: Новый стандарт для комплексной оценки диалоговых систем', 'desc': "Эта статья представляет новую задачу исследования под названием 'Моделирование элементов диалога' и соответствующий бенчмарк DEMO. Авторы отмечают нехватку комплексных бенчмарков для оценки диалоговых систем, охватывающих все элементы разговора. Используя принципы имитационного обучения, исследователи создали агента на основе DEMO, способного эффективно моделировать элементы диалога. Эксперименты показали, что существующие языковые модели (LLM) все еще имеют значительный потенциал для улучшения в этой области."}, 'en': {'title': 'Enhancing Dialogue Generation with Comprehensive Element Modeling', 'desc': 'This paper addresses the growing need for effective dialogue generation in human-machine interactions, particularly with large language models (LLMs). It identifies a gap in existing research due to the lack of comprehensive benchmarks that cover all aspects of dialogue, which hampers accurate modeling and evaluation. To tackle this issue, the authors introduce a new task called Dialogue Element MOdeling (DEMO), which focuses on understanding dialogue elements and how agents interact within conversations. Their experiments show that while current LLMs have room for improvement, the proposed DEMO agent outperforms them in various tasks, demonstrating its effectiveness in dialogue modeling.'}, 'zh': {'title': '全面对话建模的新基准DEMO', 'desc': '大型语言模型（LLMs）使对话成为人机交互的主要方式，导致大量对话日志的积累和对对话生成的需求增加。对话的生命周期包括前奏、对话和结尾，涵盖了多个元素。尽管已有许多与对话相关的研究，但缺乏全面的基准，限制了精确建模和系统评估。为了解决这个问题，我们提出了对话元素建模的新任务，并设计了一个新的基准DEMO，以便进行全面的对话建模和评估。'}}}, {'id': 'https://huggingface.co/papers/2412.04626', 'title': 'BigDocs: An Open and Permissively-Licensed Dataset for Training Multimodal Models on Document and Code Tasks', 'url': 'https://huggingface.co/papers/2412.04626', 'abstract': 'Multimodal AI has the potential to significantly enhance document-understanding tasks, such as processing receipts, understanding workflows, extracting data from documents, and summarizing reports. Code generation tasks that require long-structured outputs can also be enhanced by multimodality. Despite this, their use in commercial applications is often limited due to limited access to training data and restrictive licensing, which hinders open access. To address these limitations, we introduce BigDocs-7.5M, a high-quality, open-access dataset comprising 7.5 million multimodal documents across 30 tasks. We use an efficient data curation process to ensure our data is high-quality and license-permissive. Our process emphasizes accountability, responsibility, and transparency through filtering rules, traceable metadata, and careful content analysis. Additionally, we introduce BigDocs-Bench, a benchmark suite with 10 novel tasks where we create datasets that reflect real-world use cases involving reasoning over Graphical User Interfaces (GUI) and code generation from images. Our experiments show that training with BigDocs-Bench improves average performance up to 25.8% over closed-source GPT-4o in document reasoning and structured output tasks such as Screenshot2HTML or Image2Latex generation. Finally, human evaluations showed a preference for outputs from models trained on BigDocs over GPT-4o. This suggests that BigDocs can help both academics and the open-source community utilize and improve AI tools to enhance multimodal capabilities and document reasoning. The project is hosted at https://bigdocs.github.io .', 'score': 1, 'issue_id': 1030, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 декабря', 'en': 'December 5', 'zh': '12月5日'}, 'hash': '32747c6004865ffe', 'authors': ['Juan Rodriguez', 'Xiangru Jian', 'Siba Smarak Panigrahi', 'Tianyu Zhang', 'Aarash Feizi', 'Abhay Puri', 'Akshay Kalkunte', 'François Savard', 'Ahmed Masry', 'Shravan Nayak', 'Rabiul Awal', 'Mahsa Massoud', 'Amirhossein Abaskohi', 'Zichao Li', 'Suyuchen Wang', 'Pierre-André Noël', 'Mats Leon Richter', 'Saverio Vadacchino', 'Shubbam Agarwal', 'Sanket Biswas', 'Sara Shanian', 'Ying Zhang', 'Noah Bolger', 'Kurt MacDonald', 'Simon Fauvel', 'Sathwik Tejaswi', 'Srinivas Sunkara', 'Joao Monteiro', 'Krishnamurthy DJ Dvijotham', 'Torsten Scholak', 'Nicolas Chapados', 'Sepideh Kharagani', 'Sean Hughes', 'M. Özsu', 'Siva Reddy', 'Marco Pedersoli', 'Yoshua Bengio', 'Christopher Pal', 'Issam Laradji', 'Spandanna Gella', 'Perouz Taslakian', 'David Vazquez', 'Sai Rajeswar'], 'affiliations': ['CIFAR AI Chair', 'McGill University', 'Mila', 'Polytechnique Montréal', 'ServiceNow', 'Universitat Autònoma de Barcelona', 'University of British Columbia', 'University of Waterloo', 'Université de Montréal', 'York University', 'École de Technologie Supérieure'], 'pdf_title_img': 'assets/pdf/title_img/2412.04626.jpg', 'data': {'categories': ['#data', '#open_source', '#reasoning', '#multimodal', '#games', '#graphs', '#dataset', '#benchmark'], 'emoji': '📄', 'ru': {'title': 'BigDocs: Открытый путь к улучшению мультимодального ИИ', 'desc': 'Статья представляет BigDocs-7.5M - крупный открытый датасет из 7,5 миллионов мультимодальных документов для 30 задач. Авторы также вводят BigDocs-Bench - набор тестов с 10 новыми задачами для рассуждений по GUI и генерации кода из изображений. Эксперименты показывают, что обучение на BigDocs-Bench улучшает среднюю производительность до 25,8% по сравнению с закрытым GPT-4o в задачах рассуждения по документам и структурированного вывода. Человеческая оценка показала предпочтение выходных данных моделей, обученных на BigDocs, по сравнению с GPT-4o.'}, 'en': {'title': 'Unlocking Multimodal AI with BigDocs-7.5M', 'desc': 'This paper presents BigDocs-7.5M, a large open-access dataset designed to improve multimodal AI applications in document understanding and code generation. The dataset includes 7.5 million multimodal documents across 30 tasks, ensuring high quality and permissive licensing through a careful curation process. Additionally, the authors introduce BigDocs-Bench, a benchmark suite with 10 new tasks that simulate real-world scenarios involving reasoning over graphical user interfaces and generating code from images. Experiments demonstrate that models trained on this dataset outperform existing closed-source models, indicating its potential to advance both academic research and open-source AI development.'}, 'zh': {'title': '多模态AI助力文档理解的未来', 'desc': '多模态人工智能可以显著提升文档理解任务的效果，例如处理收据、理解工作流程、从文档中提取数据和总结报告。我们提出的BigDocs-7.5M是一个高质量、开放访问的数据集，包含750万份多模态文档，涵盖30个任务。通过高效的数据整理过程，我们确保数据的高质量和许可友好性，并强调责任和透明度。实验结果表明，使用BigDocs-Bench进行训练可以在文档推理和结构化输出任务上提高平均性能，帮助学术界和开源社区更好地利用和改进AI工具。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (2)', '#agi', '#alignment (1)', '#architecture (2)', '#audio', '#benchmark (5)', '#cv (2)', '#data (2)', '#dataset (5)', '#diffusion (2)', '#ethics', '#games (3)', '#graphs (1)', '#hallucinations (2)', '#healthcare', '#inference (1)', '#interpretability', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (8)', '#open_source (5)', '#optimization (4)', '#plp', '#rag', '#reasoning (3)', '#rl', '#rlhf (1)', '#robotics (1)', '#science', '#security', '#small_models (1)', '#story_generation', '#survey', '#synthetic (1)', '#training (9)', '#transfer_learning (1)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-12-09 21:10',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-12-09 21:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-12-09 21:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    