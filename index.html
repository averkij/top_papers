
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 29 papers. December 4.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">4 декабря</span> | <span id="title-articles-count">29 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2024-12-03.html">⬅️ <span id="prev-date">03.12</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-12-05.html">➡️ <span id="next-date">05.12</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-12.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '4 декабря', 'en': 'December 4', 'zh': '12月4日'};
        let feedDateNext = {'ru': '05.12', 'en': '12/05', 'zh': '12月5日'};
        let feedDatePrev = {'ru': '03.12', 'en': '12/03', 'zh': '12月3日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2412.01824', 'title': 'X-Prompt: Towards Universal In-Context Image Generation in Auto-Regressive Vision Language Foundation Models', 'url': 'https://huggingface.co/papers/2412.01824', 'abstract': "In-context generation is a key component of large language models' (LLMs) open-task generalization capability. By leveraging a few examples as context, LLMs can perform both in-domain and out-of-domain tasks. Recent advancements in auto-regressive vision-language models (VLMs) built upon LLMs have showcased impressive performance in text-to-image generation. However, the potential of in-context learning for general image generation tasks remains largely unexplored. To address this, we introduce X-Prompt, a purely auto-regressive large-vision language model designed to deliver competitive performance across a wide range of both seen and unseen image generation tasks, all within a unified in-context learning framework. X-Prompt incorporates a specialized design that efficiently compresses valuable features from in-context examples, supporting longer in-context token sequences and improving its ability to generalize to unseen tasks. A unified training task for both text and image prediction enables X-Prompt to handle general image generation with enhanced task awareness from in-context examples. Extensive experiments validate the model's performance across diverse seen image generation tasks and its capacity to generalize to previously unseen tasks.", 'score': 51, 'issue_id': 911, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '6b7c2d6555d8df8d', 'authors': ['Zeyi Sun', 'Ziyang Chu', 'Pan Zhang', 'Tong Wu', 'Xiaoyi Dong', 'Yuhang Zang', 'Yuanjun Xiong', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['CPII under InnoHK', 'MThreads AI', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.01824.jpg', 'data': {'categories': ['#agi', '#cv', '#games', '#optimization', '#training', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'X-Prompt: универсальная модель для генерации изображений с контекстным обучением', 'desc': 'Статья представляет X-Prompt - авторегрессивную мультимодальную модель для генерации изображений. Модель использует контекстное обучение, что позволяет ей решать как знакомые, так и новые задачи генерации изображений. X-Prompt эффективно сжимает признаки из контекстных примеров и поддерживает длинные последовательности токенов контекста. Эксперименты подтверждают способность модели решать разнообразные задачи генерации изображений, включая ранее не встречавшиеся.'}, 'en': {'title': 'X-Prompt: Unlocking Image Generation with In-Context Learning', 'desc': 'This paper presents X-Prompt, an advanced auto-regressive vision-language model that enhances image generation through in-context learning. By utilizing a few examples as context, X-Prompt can effectively generate images for both familiar and novel tasks. The model is designed to compress important features from these examples, allowing it to manage longer sequences of context and improve generalization. Extensive testing shows that X-Prompt performs well on a variety of image generation tasks, demonstrating its ability to adapt to new challenges.'}, 'zh': {'title': 'X-Prompt：提升图像生成的上下文学习能力', 'desc': '本文介绍了一种名为X-Prompt的自回归大规模视觉语言模型，旨在提升图像生成任务的表现。X-Prompt通过利用上下文中的示例，能够在已知和未知的图像生成任务中实现竞争力的性能。该模型采用了专门的设计，能够有效压缩上下文示例中的重要特征，从而支持更长的上下文序列并提高对未知任务的泛化能力。通过统一的训练任务，X-Prompt在文本和图像预测方面表现出色，验证了其在多样化图像生成任务中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2412.00927', 'title': 'VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation', 'url': 'https://huggingface.co/papers/2412.00927', 'abstract': 'Current large multimodal models (LMMs) face significant challenges in processing and comprehending long-duration or high-resolution videos, which is mainly due to the lack of high-quality datasets. To address this issue from a data-centric perspective, we propose VISTA, a simple yet effective Video Spatiotemporal Augmentation framework that synthesizes long-duration and high-resolution video instruction-following pairs from existing video-caption datasets. VISTA spatially and temporally combines videos to create new synthetic videos with extended durations and enhanced resolutions, and subsequently produces question-answer pairs pertaining to these newly synthesized videos. Based on this paradigm, we develop seven video augmentation methods and curate VISTA-400K, a video instruction-following dataset aimed at enhancing long-duration and high-resolution video understanding. Finetuning various video LMMs on our data resulted in an average improvement of 3.3% across four challenging benchmarks for long-video understanding. Furthermore, we introduce the first comprehensive high-resolution video understanding benchmark HRVideoBench, on which our finetuned models achieve a 6.5% performance gain. These results highlight the effectiveness of our framework.', 'score': 19, 'issue_id': 912, 'pub_date': '2024-12-01', 'pub_date_card': {'ru': '1 декабря', 'en': 'December 1', 'zh': '12月1日'}, 'hash': 'b4065e6e554dea31', 'authors': ['Weiming Ren', 'Huan Yang', 'Jie Min', 'Cong Wei', 'Wenhu Chen'], 'affiliations': ['University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2412.00927.jpg', 'data': {'categories': ['#multimodal', '#video', '#dataset', '#data', '#long_context', '#benchmark', '#synthetic'], 'emoji': '🎥', 'ru': {'title': 'Синтез данных для обучения ИИ пониманию сложных видео', 'desc': 'VISTA - это система для улучшения понимания длинных и высокого разрешения видео большими мультимодальными моделями. Она синтезирует новые видео, комбинируя существующие пространственно и временно, и создает вопросно-ответные пары к ним. На основе VISTA создан датасет VISTA-400K, который позволил улучшить результаты моделей на 3.3% в задачах понимания длинных видео. Также авторы представили первый бенчмарк HRVideoBench для оценки понимания видео высокого разрешения.'}, 'en': {'title': 'Enhancing Video Understanding with VISTA: A Data-Centric Approach', 'desc': "This paper introduces VISTA, a framework designed to improve the understanding of long-duration and high-resolution videos by creating synthetic video instruction-following pairs. VISTA utilizes existing video-caption datasets to spatially and temporally augment videos, generating new high-quality video data. The authors developed seven augmentation methods and created the VISTA-400K dataset, which significantly enhances the training of large multimodal models (LMMs). The results show that finetuning LMMs on this dataset leads to notable performance improvements on various benchmarks, demonstrating the framework's effectiveness in video comprehension tasks."}, 'zh': {'title': 'VISTA：提升视频理解的新方法', 'desc': '当前的大型多模态模型在处理长时长或高分辨率视频时面临重大挑战，主要是由于缺乏高质量的数据集。为了解决这个问题，我们提出了VISTA，一个简单而有效的视频时空增强框架，能够从现有的视频-字幕数据集中合成长时长和高分辨率的视频指令对。VISTA通过空间和时间的组合，创建新的合成视频，并生成与这些新合成视频相关的问题-答案对。通过在我们的数据上微调各种视频多模态模型，平均提高了3.3%的性能，进一步验证了我们框架的有效性。'}}}, {'id': 'https://huggingface.co/papers/2412.00131', 'title': 'Open-Sora Plan: Open-Source Large Video Generation Model', 'url': 'https://huggingface.co/papers/2412.00131', 'abstract': 'We introduce Open-Sora Plan, an open-source project that aims to contribute a large generation model for generating desired high-resolution videos with long durations based on various user inputs. Our project comprises multiple components for the entire video generation process, including a Wavelet-Flow Variational Autoencoder, a Joint Image-Video Skiparse Denoiser, and various condition controllers. Moreover, many assistant strategies for efficient training and inference are designed, and a multi-dimensional data curation pipeline is proposed for obtaining desired high-quality data. Benefiting from efficient thoughts, our Open-Sora Plan achieves impressive video generation results in both qualitative and quantitative evaluations. We hope our careful design and practical experience can inspire the video generation research community. All our codes and model weights are publicly available at https://github.com/PKU-YuanGroup/Open-Sora-Plan.', 'score': 19, 'issue_id': 911, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 ноября', 'en': 'November 28', 'zh': '11月28日'}, 'hash': 'fa9b0009de797ca2', 'authors': ['Bin Lin', 'Yunyang Ge', 'Xinhua Cheng', 'Zongjian Li', 'Bin Zhu', 'Shaodong Wang', 'Xianyi He', 'Yang Ye', 'Shenghai Yuan', 'Liuhan Chen', 'Tanghui Jia', 'Junwu Zhang', 'Zhenyu Tang', 'Yatian Pang', 'Bin She', 'Cen Yan', 'Zhiheng Hu', 'Xiaoyi Dong', 'Lin Chen', 'Zhang Pan', 'Xing Zhou', 'Shaoling Dong', 'Yonghong Tian', 'Li Yuan'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2412.00131.jpg', 'data': {'categories': ['#open_source', '#data', '#inference', '#training', '#video'], 'emoji': '🎬', 'ru': {'title': 'Открытая модель для генерации высококачественного видео', 'desc': 'Проект Open-Sora Plan представляет собой открытую модель генерации видео высокого разрешения на основе различных пользовательских входных данных. Модель включает в себя вейвлет-поточный вариационный автоэнкодер, совместный денойзер изображений и видео, а также различные контроллеры условий. Разработаны стратегии для эффективного обучения и вывода, а также предложен многомерный конвейер курирования данных. Проект достиг впечатляющих результатов в генерации видео как в качественных, так и в количественных оценках.'}, 'en': {'title': 'Empowering High-Resolution Video Generation with Open-Sora Plan', 'desc': 'The Open-Sora Plan is an open-source initiative focused on creating a large-scale generative model for producing high-resolution videos that can last for extended periods, tailored to user specifications. It integrates several advanced components, including a Wavelet-Flow Variational Autoencoder and a Joint Image-Video Skiparse Denoiser, to enhance the video generation process. The project also introduces various condition controllers and efficient training strategies, along with a multi-dimensional data curation pipeline to ensure high-quality output. Overall, the Open-Sora Plan demonstrates significant advancements in video generation, aiming to inspire further research in the field.'}, 'zh': {'title': '开放源代码，生成高质量视频的未来', 'desc': 'Open-Sora计划是一个开源项目，旨在基于用户输入生成高分辨率的长时视频。该项目包含多个组件，如小波流变分自编码器和联合图像-视频去噪器，支持整个视频生成过程。我们还设计了多种高效的训练和推理策略，并提出了多维数据策划管道，以获取高质量数据。通过这些高效的设计，Open-Sora计划在视频生成的定性和定量评估中取得了令人印象深刻的结果。'}}}, {'id': 'https://huggingface.co/papers/2411.18499', 'title': 'GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation', 'url': 'https://huggingface.co/papers/2411.18499', 'abstract': 'Multimodal Large Language Models (MLLMs) have made significant strides in visual understanding and generation tasks. However, generating interleaved image-text content remains a challenge, which requires integrated multimodal understanding and generation abilities. While the progress in unified models offers new solutions, existing benchmarks are insufficient for evaluating these methods due to data size and diversity limitations. To bridge this gap, we introduce GATE OpenING (OpenING), a comprehensive benchmark comprising 5,400 high-quality human-annotated instances across 56 real-world tasks. OpenING covers diverse daily scenarios such as travel guide, design, and brainstorming, offering a robust platform for challenging interleaved generation methods. In addition, we present IntJudge, a judge model for evaluating open-ended multimodal generation methods. Trained with a novel data pipeline, our IntJudge achieves an agreement rate of 82. 42% with human judgments, outperforming GPT-based evaluators by 11.34%. Extensive experiments on OpenING reveal that current interleaved generation methods still have substantial room for improvement. Key findings on interleaved image-text generation are further presented to guide the development of next-generation models. The OpenING is open-sourced at https://opening.github.io.', 'score': 16, 'issue_id': 914, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 ноября', 'en': 'November 27', 'zh': '11月27日'}, 'hash': 'bec1bf0079934886', 'authors': ['Pengfei Zhou', 'Xiaopeng Peng', 'Jiajun Song', 'Chuanhao Li', 'Zhaopan Xu', 'Yue Yang', 'Ziyao Guo', 'Hao Zhang', 'Yuqi Lin', 'Yefei He', 'Lirui Zhao', 'Shuo Liu', 'Tianhua Li', 'Yuxuan Xie', 'Xiaojun Chang', 'Yu Qiao', 'Wenqi Shao', 'Kaipeng Zhang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2411.18499.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#dataset', '#games'], 'emoji': '🔀', 'ru': {'title': 'Новый бенчмарк для оценки мультимодальной генерации', 'desc': 'Статья представляет новый набор данных GATE OpenING для оценки мультимодальных языковых моделей в задачах генерации чередующегося текстово-визуального контента. OpenING включает 5400 аннотированных примеров из 56 реальных задач. Авторы также предлагают модель IntJudge для оценки генерации открытого типа, которая превосходит GPT-based оценщиков на 11.34%. Эксперименты показывают, что существующие методы генерации чередующегося контента имеют значительный потенциал для улучшения.'}, 'en': {'title': 'Bridging the Gap in Multimodal Generation with OpenING!', 'desc': 'This paper discusses the advancements in Multimodal Large Language Models (MLLMs) for tasks that involve both images and text. It highlights the challenges of generating content that combines these two modalities effectively, which requires a deep understanding of both. To address the limitations of existing benchmarks, the authors introduce GATE OpenING, a new dataset with 5,400 annotated examples across 56 tasks that reflect real-world scenarios. Additionally, they present IntJudge, a model designed to evaluate multimodal generation methods, which shows improved agreement with human judgments compared to previous evaluators.'}, 'zh': {'title': '推动多模态生成的基准与评估', 'desc': '多模态大型语言模型（MLLMs）在视觉理解和生成任务上取得了显著进展。然而，生成交错的图像-文本内容仍然是一个挑战，这需要综合的多模态理解和生成能力。为了解决这一问题，我们引入了GATE OpenING（OpenING），这是一个包含5400个高质量人类标注实例的综合基准，涵盖56个真实世界任务。我们的研究还提出了IntJudge，一个用于评估开放式多模态生成方法的评估模型，显示出与人类判断的高一致性。'}}}, {'id': 'https://huggingface.co/papers/2412.00154', 'title': 'o1-Coder: an o1 Replication for Coding', 'url': 'https://huggingface.co/papers/2412.00154', 'abstract': "The technical report introduces O1-CODER, an attempt to replicate OpenAI's o1 model with a focus on coding tasks. It integrates reinforcement learning (RL) and Monte Carlo Tree Search (MCTS) to enhance the model's System-2 thinking capabilities. The framework includes training a Test Case Generator (TCG) for standardized code testing, using MCTS to generate code data with reasoning processes, and iteratively fine-tuning the policy model to initially produce pseudocode, followed by the generation of the full code. The report also addresses the opportunities and challenges in deploying o1-like models in real-world applications, suggesting transitioning to the System-2 paradigm and highlighting the imperative for environment state updates. Updated model progress and experimental results will be reported in subsequent versions. All source code, curated datasets, as well as the derived models will be disclosed at https://github.com/ADaM-BJTU/O1-CODER .", 'score': 16, 'issue_id': 912, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '4a9b72e0b9a6ba64', 'authors': ['Yuxiang Zhang', 'Shangxi Wu', 'Yuqi Yang', 'Jiangming Shu', 'Jinlin Xiao', 'Chao Kong', 'Jitao Sang'], 'affiliations': ['School of Computer Science and Technology, Beijing Jiaotong University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.00154.jpg', 'data': {'categories': ['#dataset', '#training', '#open_source', '#optimization', '#reasoning', '#rl'], 'emoji': '🧠', 'ru': {'title': 'O1-CODER: Усиление ИИ для программирования через Систему-2 мышления', 'desc': 'Технический отчет представляет O1-CODER - попытку воспроизвести модель o1 от OpenAI для задач программирования. Модель интегрирует обучение с подкреплением и метод Монте-Карло для улучшения способностей мышления Системы-2. Фреймворк включает обучение генератора тестовых случаев, использование MCTS для генерации кода с процессами рассуждения, и итеративную доводку модели политики. Отчет также рассматривает возможности и проблемы развертывания подобных o1 моделей в реальных приложениях.'}, 'en': {'title': 'Enhancing Coding with O1-CODER: A New Approach to System-2 Thinking', 'desc': "The paper presents O1-CODER, a model designed to replicate OpenAI's o1 specifically for coding tasks. It combines reinforcement learning (RL) with Monte Carlo Tree Search (MCTS) to improve the model's ability to think critically and reason through problems. The framework includes a Test Case Generator (TCG) for consistent code testing and uses MCTS to create code data that incorporates reasoning steps. The report discusses the potential and challenges of applying o1-like models in practical scenarios, emphasizing the need for updates in the environment state during the learning process."}, 'zh': {'title': 'O1-CODER：提升编码任务的智能模型', 'desc': '本文介绍了O1-CODER，这是一个旨在复制OpenAI的o1模型，专注于编码任务的技术报告。该模型结合了强化学习（RL）和蒙特卡洛树搜索（MCTS），以增强其系统2思维能力。框架中包括训练测试用例生成器（TCG）以进行标准化代码测试，利用MCTS生成带有推理过程的代码数据，并迭代微调策略模型，初步生成伪代码，随后生成完整代码。报告还讨论了在实际应用中部署类似o1模型的机遇和挑战，建议转向系统2范式，并强调环境状态更新的重要性。'}}}, {'id': 'https://huggingface.co/papers/2412.01819', 'title': 'Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis', 'url': 'https://huggingface.co/papers/2412.01819', 'abstract': 'This work presents Switti, a scale-wise transformer for text-to-image generation. Starting from existing next-scale prediction AR models, we first explore them for T2I generation and propose architectural modifications to improve their convergence and overall performance. We then observe that self-attention maps of our pretrained scale-wise AR model exhibit weak dependence on preceding scales. Based on this insight, we propose a non-AR counterpart facilitating {sim}11% faster sampling and lower memory usage while also achieving slightly better generation quality.Furthermore, we reveal that classifier-free guidance at high-resolution scales is often unnecessary and can even degrade performance. %may be not only unnecessary but potentially detrimental. By disabling guidance at these scales, we achieve an additional sampling acceleration of {sim}20% and improve the generation of fine-grained details. Extensive human preference studies and automated evaluations show that Switti outperforms existing T2I AR models and competes with state-of-the-art T2I diffusion models while being up to 7{times} faster.', 'score': 15, 'issue_id': 914, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '25cf4512f592aea4', 'authors': ['Anton Voronov', 'Denis Kuznedelev', 'Mikhail Khoroshikh', 'Valentin Khrulkov', 'Dmitry Baranchuk'], 'affiliations': ['HSE University', 'MIPT', 'Skoltech', 'Yandex Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.01819.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#video', '#cv', '#optimization'], 'emoji': '🖼️', 'ru': {'title': 'Switti: быстрый и эффективный генератор изображений по тексту', 'desc': 'Статья представляет Switti - трансформер для генерации изображений по текстовому описанию. Авторы предлагают архитектурные модификации для улучшения сходимости и производительности авторегрессионных моделей. На основе наблюдения о слабой зависимости карт внимания от предыдущих масштабов, разработана неавторегрессионная версия, обеспечивающая ускорение и снижение потребления памяти. Исследование также показывает, что отключение guidance на высоких разрешениях дополнительно ускоряет генерацию и улучшает детализацию.'}, 'en': {'title': 'Switti: Accelerating Text-to-Image Generation with Scale-Wise Transformers', 'desc': 'This paper introduces Switti, a new transformer model designed for generating images from text. It builds on existing autoregressive (AR) models by making architectural changes that enhance performance and speed. The authors find that the self-attention mechanisms in their model do not rely heavily on previous scales, allowing for a non-autoregressive approach that speeds up sampling and reduces memory usage. Additionally, they demonstrate that removing classifier-free guidance at higher resolutions can improve detail generation and further accelerate the process, leading to a model that is significantly faster than current state-of-the-art methods.'}, 'zh': {'title': 'Switti：加速文本到图像生成的变换器', 'desc': '本文介绍了Switti，一种用于文本到图像生成的尺度变换器。我们从现有的下一尺度预测自回归模型出发，探索其在T2I生成中的应用，并提出架构修改以提高收敛性和整体性能。研究发现，我们的预训练尺度自回归模型的自注意力图对前一尺度的依赖性较弱，因此我们提出了一种非自回归的替代方案，能够实现约11%的采样加速和更低的内存使用，同时生成质量略有提升。此外，我们发现高分辨率尺度下的无分类器引导通常是不必要的，甚至可能会降低性能。'}}}, {'id': 'https://huggingface.co/papers/2412.01064', 'title': 'FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait', 'url': 'https://huggingface.co/papers/2412.01064', 'abstract': 'With the rapid advancement of diffusion-based generative models, portrait image animation has achieved remarkable results. However, it still faces challenges in temporally consistent video generation and fast sampling due to its iterative sampling nature. This paper presents FLOAT, an audio-driven talking portrait video generation method based on flow matching generative model. We shift the generative modeling from the pixel-based latent space to a learned motion latent space, enabling efficient design of temporally consistent motion. To achieve this, we introduce a transformer-based vector field predictor with a simple yet effective frame-wise conditioning mechanism. Additionally, our method supports speech-driven emotion enhancement, enabling a natural incorporation of expressive motions. Extensive experiments demonstrate that our method outperforms state-of-the-art audio-driven talking portrait methods in terms of visual quality, motion fidelity, and efficiency.', 'score': 14, 'issue_id': 912, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '1978e3ecf42cac0c', 'authors': ['Taekyung Ki', 'Dongchan Min', 'Gyoungsu Chae'], 'affiliations': ['DeepBrain AI Inc.', 'Graduate School of AI, KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2412.01064.jpg', 'data': {'categories': ['#audio', '#diffusion', '#multimodal', '#video', '#architecture'], 'emoji': '🗣️', 'ru': {'title': 'Генерация реалистичных видео с говорящим портретом на основе аудио и потоковых моделей', 'desc': 'Статья представляет FLOAT - метод генерации видео с говорящим портретом на основе аудио, используя генеративную модель сопоставления потоков. Авторы переносят генеративное моделирование из пиксельного латентного пространства в пространство движения, что позволяет эффективно создавать согласованные во времени движения. Метод включает предиктор векторного поля на основе трансформера с покадровым механизмом обусловливания. FLOAT также поддерживает усиление эмоций на основе речи, позволяя естественно добавлять выразительные движения.'}, 'en': {'title': 'FLOAT: Revolutionizing Audio-Driven Talking Portraits with Motion Consistency', 'desc': 'This paper introduces FLOAT, a novel method for generating talking portrait videos driven by audio. It addresses the challenges of creating temporally consistent animations and speeding up the sampling process by utilizing a flow matching generative model. By transitioning from pixel-based latent spaces to learned motion latent spaces, FLOAT enhances the design of motion consistency. The method also incorporates a transformer-based vector field predictor that allows for effective frame-wise conditioning and supports emotion enhancement based on speech, leading to improved visual quality and motion fidelity.'}, 'zh': {'title': 'FLOAT：高效音频驱动的人像视频生成', 'desc': '本论文提出了一种名为FLOAT的音频驱动人像视频生成方法，基于流匹配生成模型。我们将生成建模从基于像素的潜在空间转移到学习的运动潜在空间，从而实现时间一致的运动设计。该方法引入了基于变换器的向量场预测器，并采用简单有效的逐帧条件机制。实验结果表明，我们的方法在视觉质量、运动保真度和效率方面优于现有的音频驱动人像生成方法。'}}}, {'id': 'https://huggingface.co/papers/2411.18671', 'title': 'TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any Point in Long Video', 'url': 'https://huggingface.co/papers/2411.18671', 'abstract': 'In this paper, we present TAPTRv3, which is built upon TAPTRv2 to improve its point tracking robustness in long videos. TAPTRv2 is a simple DETR-like framework that can accurately track any point in real-world videos without requiring cost-volume. TAPTRv3 improves TAPTRv2 by addressing its shortage in querying high quality features from long videos, where the target tracking points normally undergo increasing variation over time. In TAPTRv3, we propose to utilize both spatial and temporal context to bring better feature querying along the spatial and temporal dimensions for more robust tracking in long videos. For better spatial feature querying, we present Context-aware Cross-Attention (CCA), which leverages surrounding spatial context to enhance the quality of attention scores when querying image features. For better temporal feature querying, we introduce Visibility-aware Long-Temporal Attention (VLTA) to conduct temporal attention to all past frames while considering their corresponding visibilities, which effectively addresses the feature drifting problem in TAPTRv2 brought by its RNN-like long-temporal modeling. TAPTRv3 surpasses TAPTRv2 by a large margin on most of the challenging datasets and obtains state-of-the-art performance. Even when compared with methods trained with large-scale extra internal data, TAPTRv3 is still competitive.', 'score': 14, 'issue_id': 909, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 ноября', 'en': 'November 27', 'zh': '11月27日'}, 'hash': 'f99e1015e9222dc6', 'authors': ['Jinyuan Qu', 'Hongyang Li', 'Shilong Liu', 'Tianhe Ren', 'Zhaoyang Zeng', 'Lei Zhang'], 'affiliations': ['International Digital Economy Academy (IDEA)', 'South China University of Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.18671.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#video', '#cv', '#optimization'], 'emoji': '👁️', 'ru': {'title': 'Контекстное внимание для надежного отслеживания точек в видео', 'desc': 'TAPTRv3 - это улучшенная версия TAPTRv2 для более надежного отслеживания точек в длинных видео. Система использует пространственный и временной контекст для повышения качества запроса признаков. Введены два новых механизма: Context-aware Cross-Attention (CCA) для улучшения пространственного запроса и Visibility-aware Long-Temporal Attention (VLTA) для временного запроса. TAPTRv3 значительно превосходит предыдущую версию и достигает наилучших результатов на сложных наборах данных.'}, 'en': {'title': 'Enhancing Video Point Tracking with TAPTRv3', 'desc': 'TAPTRv3 is an advanced framework designed to enhance point tracking in lengthy videos, building on the previous version, TAPTRv2. It improves the ability to query high-quality features by incorporating both spatial and temporal contexts, which helps in maintaining robust tracking despite variations over time. The paper introduces Context-aware Cross-Attention (CCA) for better spatial feature querying and Visibility-aware Long-Temporal Attention (VLTA) for improved temporal feature querying, effectively addressing issues like feature drifting. TAPTRv3 demonstrates significant performance improvements over TAPTRv2 and competes well against other state-of-the-art methods, even those trained on larger datasets.'}, 'zh': {'title': 'TAPTRv3：长视频点跟踪的新突破', 'desc': '本文介绍了TAPTRv3，这是在TAPTRv2基础上开发的，旨在提高长视频中的点跟踪鲁棒性。TAPTRv2是一个简单的类似DETR的框架，可以准确跟踪现实视频中的任意点，而无需成本体积。TAPTRv3通过利用空间和时间上下文来改善特征查询，从而在长视频中实现更稳健的跟踪。我们提出了上下文感知交叉注意力（CCA）和可见性感知长时间注意力（VLTA），显著提升了特征查询的质量，超越了TAPTRv2，达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2412.00174', 'title': 'SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters', 'url': 'https://huggingface.co/papers/2412.00174', 'abstract': "Human beings are social animals. How to equip 3D autonomous characters with similar social intelligence that can perceive, understand and interact with humans remains an open yet foundamental problem. In this paper, we introduce SOLAMI, the first end-to-end Social vision-Language-Action (VLA) Modeling framework for Immersive interaction with 3D autonomous characters. Specifically, SOLAMI builds 3D autonomous characters from three aspects: (1) Social VLA Architecture: We propose a unified social VLA framework to generate multimodal response (speech and motion) based on the user's multimodal input to drive the character for social interaction. (2) Interactive Multimodal Data: We present SynMSI, a synthetic multimodal social interaction dataset generated by an automatic pipeline using only existing motion datasets to address the issue of data scarcity. (3) Immersive VR Interface: We develop a VR interface that enables users to immersively interact with these characters driven by various architectures. Extensive quantitative experiments and user studies demonstrate that our framework leads to more precise and natural character responses (in both speech and motion) that align with user expectations with lower latency.", 'score': 13, 'issue_id': 913, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '5a30d9c535229ab0', 'authors': ['Jianping Jiang', 'Weiye Xiao', 'Zhengyu Lin', 'Huaizhong Zhang', 'Tianxiang Ren', 'Yang Gao', 'Zhiqian Lin', 'Zhongang Cai', 'Lei Yang', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University', 'SenseTime Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.00174.jpg', 'data': {'categories': ['#synthetic', '#games', '#dataset', '#agents', '#3d', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Создание социально умных 3D персонажей для виртуальной реальности', 'desc': 'В статье представлен SOLAMI - первый сквозной фреймворк для социального моделирования зрения-языка-действия (VLA) для иммерсивного взаимодействия с 3D автономными персонажами. Фреймворк включает в себя унифицированную архитектуру социального VLA для генерации мультимодальных ответов на основе пользовательского ввода. Для решения проблемы нехватки данных авторы создали синтетический набор данных SynMSI, используя существующие наборы данных о движении. Также разработан VR-интерфейс для иммерсивного взаимодействия пользователей с персонажами.'}, 'en': {'title': 'Empowering 3D Characters with Social Intelligence through SOLAMI', 'desc': 'This paper presents SOLAMI, a novel framework designed to enhance the social intelligence of 3D autonomous characters. It integrates a Social Vision-Language-Action (VLA) architecture that allows characters to respond to user inputs with both speech and motion, facilitating more natural interactions. To support this, the authors introduce SynMSI, a synthetic dataset that helps overcome the challenge of limited training data for social interactions. Additionally, a virtual reality interface is developed to provide users with an immersive experience when interacting with these characters, resulting in improved response accuracy and reduced latency.'}, 'zh': {'title': '赋予3D角色社交智能的创新框架', 'desc': '本文介绍了SOLAMI，这是第一个端到端的社会视觉-语言-动作（VLA）建模框架，旨在与3D自主角色进行沉浸式互动。SOLAMI从三个方面构建3D自主角色：首先，提出了统一的社会VLA架构，根据用户的多模态输入生成多模态响应（语音和动作），以驱动角色进行社交互动。其次，介绍了SynMSI，这是一个合成的多模态社交互动数据集，通过自动化流程生成，解决了数据稀缺的问题。最后，开发了一个虚拟现实接口，使用户能够与这些角色进行沉浸式互动，实验结果表明，该框架能够提供更精确和自然的角色响应。'}}}, {'id': 'https://huggingface.co/papers/2412.00100', 'title': 'Steering Rectified Flow Models in the Vector Field for Controlled Image Generation', 'url': 'https://huggingface.co/papers/2412.00100', 'abstract': 'Diffusion models (DMs) excel in photorealism, image editing, and solving inverse problems, aided by classifier-free guidance and image inversion techniques. However, rectified flow models (RFMs) remain underexplored for these tasks. Existing DM-based methods often require additional training, lack generalization to pretrained latent models, underperform, and demand significant computational resources due to extensive backpropagation through ODE solvers and inversion processes. In this work, we first develop a theoretical and empirical understanding of the vector field dynamics of RFMs in efficiently guiding the denoising trajectory. Our findings reveal that we can navigate the vector field in a deterministic and gradient-free manner. Utilizing this property, we propose FlowChef, which leverages the vector field to steer the denoising trajectory for controlled image generation tasks, facilitated by gradient skipping. FlowChef is a unified framework for controlled image generation that, for the first time, simultaneously addresses classifier guidance, linear inverse problems, and image editing without the need for extra training, inversion, or intensive backpropagation. Finally, we perform extensive evaluations and show that FlowChef significantly outperforms baselines in terms of performance, memory, and time requirements, achieving new state-of-the-art results. Project Page: https://flowchef.github.io.', 'score': 13, 'issue_id': 911, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 ноября', 'en': 'November 27', 'zh': '11月27日'}, 'hash': '5fceae277a0e3d85', 'authors': ['Maitreya Patel', 'Song Wen', 'Dimitris N. Metaxas', 'Yezhou Yang'], 'affiliations': ['Arizona State University', 'Rutgers University'], 'pdf_title_img': 'assets/pdf/title_img/2412.00100.jpg', 'data': {'categories': ['#dataset', '#cv', '#optimization', '#diffusion', '#benchmark'], 'emoji': '🌊', 'ru': {'title': 'FlowChef: Управление векторным полем для эффективной генерации изображений', 'desc': 'Статья представляет новый подход к управляемой генерации изображений под названием FlowChef. Он основан на использовании динамики векторного поля в ректифицированных потоковых моделях (RFM) для эффективного управления траекторией шумоподавления. FlowChef позволяет решать задачи управляемой генерации изображений, линейных обратных задач и редактирования изображений без дополнительного обучения или инверсии. Результаты показывают, что FlowChef значительно превосходит существующие методы по производительности, требованиям к памяти и времени вычислений.'}, 'en': {'title': 'FlowChef: Revolutionizing Controlled Image Generation with Efficiency', 'desc': 'This paper introduces FlowChef, a new framework that enhances controlled image generation using rectified flow models (RFMs). Unlike traditional diffusion models, FlowChef efficiently guides the denoising process without requiring additional training or extensive computational resources. It utilizes a unique property of RFMs to navigate the vector field in a deterministic way, allowing for effective classifier guidance and image editing. The results demonstrate that FlowChef outperforms existing methods in performance, memory usage, and processing time, setting new benchmarks in the field.'}, 'zh': {'title': 'FlowChef：高效的受控图像生成新方法', 'desc': '扩散模型在生成真实感图像、图像编辑和解决逆问题方面表现出色，但修正流模型在这些任务中仍未得到充分探索。现有的基于扩散模型的方法通常需要额外的训练，缺乏对预训练潜在模型的泛化能力，并且在性能上表现不佳，计算资源消耗大。本文提出FlowChef，通过有效引导去噪轨迹，利用向量场的特性，实现了受控图像生成任务，且无需额外训练或复杂的反向传播。实验结果表明，FlowChef在性能、内存和时间需求上显著优于基线方法，达到了新的最先进结果。'}}}, {'id': 'https://huggingface.co/papers/2412.00568', 'title': 'The Well: a Large-Scale Collection of Diverse Physics Simulations for Machine Learning', 'url': 'https://huggingface.co/papers/2412.00568', 'abstract': 'Machine learning based surrogate models offer researchers powerful tools for accelerating simulation-based workflows. However, as standard datasets in this space often cover small classes of physical behavior, it can be difficult to evaluate the efficacy of new approaches. To address this gap, we introduce the Well: a large-scale collection of datasets containing numerical simulations of a wide variety of spatiotemporal physical systems. The Well draws from domain experts and numerical software developers to provide 15TB of data across 16 datasets covering diverse domains such as biological systems, fluid dynamics, acoustic scattering, as well as magneto-hydrodynamic simulations of extra-galactic fluids or supernova explosions. These datasets can be used individually or as part of a broader benchmark suite. To facilitate usage of the Well, we provide a unified PyTorch interface for training and evaluating models. We demonstrate the function of this library by introducing example baselines that highlight the new challenges posed by the complex dynamics of the Well. The code and data is available at https://github.com/PolymathicAI/the_well.', 'score': 11, 'issue_id': 923, 'pub_date': '2024-11-30', 'pub_date_card': {'ru': '30 ноября', 'en': 'November 30', 'zh': '11月30日'}, 'hash': '1352e219e54ac56e', 'authors': ['Ruben Ohana', 'Michael McCabe', 'Lucas Meyer', 'Rudy Morel', 'Fruzsina J. Agocs', 'Miguel Beneitez', 'Marsha Berger', 'Blakesley Burkhart', 'Stuart B. Dalziel', 'Drummond B. Fielding', 'Daniel Fortunato', 'Jared A. Goldberg', 'Keiya Hirashima', 'Yan-Fei Jiang', 'Rich R. Kerswell', 'Suryanarayana Maddu', 'Jonah Miller', 'Payel Mukhopadhyay', 'Stefan S. Nixon', 'Jeff Shen', 'Romain Watteaux', 'Bruno Régaldo-Saint Blancard', 'François Rozet', 'Liam H. Parker', 'Miles Cranmer', 'Shirley Ho'], 'affiliations': ['CEA DAM', 'Cornell University', 'Flatiron Institute', 'Los Alamos National Laboratory', 'New York University', 'Polymathic AI', 'Princeton University', 'Rutgers University', 'University of California, Berkeley', 'University of Cambridge', 'University of Colorado, Boulder', 'University of Liège', 'University of Tokyo'], 'pdf_title_img': 'assets/pdf/title_img/2412.00568.jpg', 'data': {'categories': ['#data', '#dataset', '#benchmark'], 'emoji': '🌊', 'ru': {'title': 'Well: масштабный бенчмарк для суррогатных моделей в физическом моделировании', 'desc': "Статья представляет новый набор данных под названием 'Well' для оценки суррогатных моделей машинного обучения в области численного моделирования. 'Well' содержит 15 ТБ данных из 16 наборов, охватывающих различные области физики, включая биологические системы, гидродинамику и астрофизику. Авторы предоставляют унифицированный интерфейс PyTorch для обучения и оценки моделей на этих данных. Набор данных призван помочь исследователям в разработке более эффективных методов машинного обучения для ускорения рабочих процессов, основанных на симуляциях."}, 'en': {'title': 'Unlocking Complex Simulations with the Well: A New Dataset for Machine Learning', 'desc': 'This paper presents a large-scale collection of datasets called the Well, designed to enhance machine learning surrogate models for simulation-based workflows. The Well contains 15TB of data across 16 diverse datasets, covering various physical systems such as fluid dynamics and biological systems. By providing a unified PyTorch interface, the Well facilitates the training and evaluation of machine learning models on these complex datasets. The authors also introduce example baselines to demonstrate the unique challenges posed by the intricate dynamics represented in the Well.'}, 'zh': {'title': '机器学习加速仿真：探索"Well"数据集', 'desc': '本文介绍了一种基于机器学习的替代模型，旨在加速基于仿真的工作流程。我们提出了一个名为"Well"的大规模数据集，包含多种时空物理系统的数值仿真数据，总计15TB，涵盖生物系统、流体动力学、声散射等多个领域。该数据集为研究人员提供了丰富的资源，以评估新方法的有效性，并可单独使用或作为更广泛基准套件的一部分。为了方便使用，我们提供了统一的PyTorch接口，帮助训练和评估模型，并展示了新的挑战。'}}}, {'id': 'https://huggingface.co/papers/2412.01199', 'title': 'TinyFusion: Diffusion Transformers Learned Shallow', 'url': 'https://huggingface.co/papers/2412.01199', 'abstract': 'Diffusion Transformers have demonstrated remarkable capabilities in image generation but often come with excessive parameterization, resulting in considerable inference overhead in real-world applications. In this work, we present TinyFusion, a depth pruning method designed to remove redundant layers from diffusion transformers via end-to-end learning. The core principle of our approach is to create a pruned model with high recoverability, allowing it to regain strong performance after fine-tuning. To accomplish this, we introduce a differentiable sampling technique to make pruning learnable, paired with a co-optimized parameter to simulate future fine-tuning. While prior works focus on minimizing loss or error after pruning, our method explicitly models and optimizes the post-fine-tuning performance of pruned models. Experimental results indicate that this learnable paradigm offers substantial benefits for layer pruning of diffusion transformers, surpassing existing importance-based and error-based methods. Additionally, TinyFusion exhibits strong generalization across diverse architectures, such as DiTs, MARs, and SiTs. Experiments with DiT-XL show that TinyFusion can craft a shallow diffusion transformer at less than 7% of the pre-training cost, achieving a 2times speedup with an FID score of 2.86, outperforming competitors with comparable efficiency. Code is available at https://github.com/VainF/TinyFusion.', 'score': 11, 'issue_id': 912, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '7d1de7010c3fabd7', 'authors': ['Gongfan Fang', 'Kunjun Li', 'Xinyin Ma', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2412.01199.jpg', 'data': {'categories': ['#inference', '#diffusion', '#training', '#optimization', '#architecture'], 'emoji': '✂️', 'ru': {'title': 'TinyFusion: Эффективная обрезка диффузионных трансформеров без потери качества', 'desc': 'TinyFusion - это метод обрезки глубины для уменьшения количества параметров в диффузионных трансформерах. Он использует дифференцируемую технику сэмплирования для обучаемой обрезки и оптимизирует производительность модели после дообучения. TinyFusion превосходит существующие методы обрезки и хорошо обобщается на различные архитектуры. Эксперименты показывают, что метод позволяет создать компактный диффузионный трансформер с двукратным ускорением при сохранении высокого качества генерации изображений.'}, 'en': {'title': 'TinyFusion: Efficient Layer Pruning for Fast and Effective Diffusion Transformers', 'desc': 'This paper introduces TinyFusion, a method for optimizing diffusion transformers by removing unnecessary layers through a process called depth pruning. The approach focuses on maintaining high performance after the model is pruned, using a learnable technique that allows the model to adapt during fine-tuning. Unlike previous methods that only aim to reduce loss or error, TinyFusion specifically targets the performance of the pruned model post-fine-tuning. Experimental results show that TinyFusion significantly improves layer pruning efficiency and generalization across various architectures, achieving faster inference times and better performance metrics.'}, 'zh': {'title': 'TinyFusion：高效剪枝，提升扩散变换器性能', 'desc': '本论文提出了一种名为TinyFusion的深度剪枝方法，旨在减少扩散变换器中的冗余层，从而降低推理开销。我们的方法通过端到端学习实现剪枝，并确保剪枝后的模型在微调后能够恢复强大的性能。TinyFusion引入了一种可微分采样技术，使得剪枝过程可学习，并与共同优化的参数结合，以模拟未来的微调效果。实验结果表明，TinyFusion在扩散变换器的层剪枝方面优于现有的方法，展现出强大的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2412.01822', 'title': 'VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models', 'url': 'https://huggingface.co/papers/2412.01822', 'abstract': 'The recent surge in high-quality visual instruction tuning samples from closed-source vision-language models (VLMs) such as GPT-4V has accelerated the release of open-source VLMs across various model sizes. However, scaling VLMs to improve performance using larger models brings significant computational challenges, especially for deployment on resource-constrained devices like mobile platforms and robots. To address this, we propose VLsI: Verbalized Layers-to-Interactions, a new VLM family in 2B and 7B model sizes, which prioritizes efficiency without compromising accuracy. VLsI leverages a unique, layer-wise distillation process, introducing intermediate "verbalizers" that map features from each layer to natural language space, allowing smaller VLMs to flexibly align with the reasoning processes of larger VLMs. This approach mitigates the training instability often encountered in output imitation and goes beyond typical final-layer tuning by aligning the small VLMs\' layer-wise progression with that of the large ones. We validate VLsI across ten challenging vision-language benchmarks, achieving notable performance gains (11.0% for 2B and 17.4% for 7B) over GPT-4V without the need for model scaling, merging, or architectural changes.', 'score': 9, 'issue_id': 916, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '0c3ac9e6ce3f4cd2', 'authors': ['Byung-Kwan Lee', 'Ryo Hachiuma', 'Yu-Chiang Frank Wang', 'Yong Man Ro', 'Yueh-Hua Wu'], 'affiliations': ['KAIST', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2412.01822.jpg', 'data': {'categories': ['#dataset', '#small_models', '#architecture', '#transfer_learning', '#optimization', '#training', '#benchmark', '#open_source'], 'emoji': '🔬', 'ru': {'title': 'Эффективное визуально-языковое обучение через послойную вербализацию', 'desc': "Исследователи представили новое семейство моделей визуально-языкового обучения под названием VLsI, которое фокусируется на эффективности без ущерба для точности. VLsI использует уникальный процесс послойной дистилляции, вводя промежуточные 'вербализаторы', которые отображают признаки из каждого слоя в пространство естественного языка. Этот подход позволяет меньшим моделям гибко согласовываться с процессами рассуждений более крупных моделей. Авторы продемонстрировали значительное улучшение производительности VLsI по сравнению с GPT-4V на десяти сложных визуально-языковых тестах без необходимости увеличения размера модели или изменения архитектуры."}, 'en': {'title': 'Efficient Vision-Language Models with Layer-wise Distillation', 'desc': "This paper introduces VLsI, a new family of vision-language models designed to enhance efficiency while maintaining accuracy. It employs a layer-wise distillation technique that uses intermediate 'verbalizers' to translate features from each layer into natural language, enabling smaller models to better mimic the reasoning of larger models. This method addresses the common issue of training instability seen in traditional output imitation approaches. The results show significant performance improvements on various benchmarks, demonstrating that smaller models can achieve competitive results without needing to increase their size or alter their architecture."}, 'zh': {'title': '高效视觉语言模型的创新之路', 'desc': '本文提出了一种新的视觉语言模型（VLM）家族，称为VLsI，旨在提高模型的效率而不牺牲准确性。VLsI采用了一种独特的层级蒸馏过程，通过引入中间的“语言化器”，将每一层的特征映射到自然语言空间，使得较小的VLM能够灵活地与较大VLM的推理过程对齐。该方法有效缓解了输出模仿中常见的训练不稳定性，并通过对齐小型VLM的层级进展与大型VLM的层级进展，超越了典型的最终层调优。我们在十个具有挑战性的视觉语言基准上验证了VLsI，取得了显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2411.18933', 'title': 'Efficient Track Anything', 'url': 'https://huggingface.co/papers/2411.18933', 'abstract': 'Segment Anything Model 2 (SAM 2) has emerged as a powerful tool for video object segmentation and tracking anything. Key components of SAM 2 that drive the impressive video object segmentation performance include a large multistage image encoder for frame feature extraction and a memory mechanism that stores memory contexts from past frames to help current frame segmentation. The high computation complexity of multistage image encoder and memory module has limited its applications in real-world tasks, e.g., video object segmentation on mobile devices. To address this limitation, we propose EfficientTAMs, lightweight track anything models that produce high-quality results with low latency and model size. Our idea is based on revisiting the plain, nonhierarchical Vision Transformer (ViT) as an image encoder for video object segmentation, and introducing an efficient memory module, which reduces the complexity for both frame feature extraction and memory computation for current frame segmentation. We take vanilla lightweight ViTs and efficient memory module to build EfficientTAMs, and train the models on SA-1B and SA-V datasets for video object segmentation and track anything tasks. We evaluate on multiple video segmentation benchmarks including semi-supervised VOS and promptable video segmentation, and find that our proposed EfficientTAM with vanilla ViT perform comparably to SAM 2 model (HieraB+SAM 2) with ~2x speedup on A100 and ~2.4x parameter reduction. On segment anything image tasks, our EfficientTAMs also perform favorably over original SAM with ~20x speedup on A100 and ~20x parameter reduction. On mobile devices such as iPhone 15 Pro Max, our EfficientTAMs can run at ~10 FPS for performing video object segmentation with reasonable quality, highlighting the capability of small models for on-device video object segmentation applications.', 'score': 9, 'issue_id': 912, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 ноября', 'en': 'November 28', 'zh': '11月28日'}, 'hash': '86ff7f63f69fa104', 'authors': ['Yunyang Xiong', 'Chong Zhou', 'Xiaoyu Xiang', 'Lemeng Wu', 'Chenchen Zhu', 'Zechun Liu', 'Saksham Suri', 'Balakrishnan Varadarajan', 'Ramya Akula', 'Forrest Iandola', 'Raghuraman Krishnamoorthi', 'Bilge Soran', 'Vikas Chandra'], 'affiliations': ['Meta AI', 'Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2411.18933.jpg', 'data': {'categories': ['#video', '#small_models', '#benchmark', '#optimization', '#architecture'], 'emoji': '🎥', 'ru': {'title': 'Эффективная сегментация видео на мобильных устройствах', 'desc': 'EfficientTAMs - это облегченные модели для сегментации и отслеживания объектов в видео. Они основаны на использовании простого Vision Transformer (ViT) в качестве энкодера изображений и эффективного модуля памяти. EfficientTAMs показывают результаты, сравнимые с SAM 2, но работают в 2 раза быстрее и имеют в 2,4 раза меньше параметров. На мобильных устройствах, таких как iPhone 15 Pro Max, EfficientTAMs могут выполнять сегментацию объектов в видео со скоростью около 10 кадров в секунду.'}, 'en': {'title': 'EfficientTAMs: Lightweight Video Segmentation for Mobile Devices', 'desc': 'The Segment Anything Model 2 (SAM 2) is a sophisticated tool for video object segmentation, but its complexity limits its use on mobile devices. To overcome this, the authors introduce EfficientTAMs, which are lightweight models designed to maintain high-quality segmentation while reducing latency and model size. They utilize a simple Vision Transformer (ViT) for feature extraction and an efficient memory module to streamline the segmentation process. The results show that EfficientTAMs can achieve comparable performance to SAM 2 with significant improvements in speed and resource efficiency, making them suitable for real-time applications on mobile platforms.'}, 'zh': {'title': '高效视频物体分割，轻量化模型新选择', 'desc': 'Segment Anything Model 2（SAM 2）是一种强大的视频物体分割和跟踪工具。为了提高性能，SAM 2使用了多阶段图像编码器和记忆机制，但其计算复杂性限制了在移动设备上的应用。为了解决这个问题，我们提出了高效的跟踪模型EfficientTAMs，它使用轻量级的视觉变换器（ViT）作为图像编码器，并引入高效的记忆模块，从而降低了计算复杂性。我们的EfficientTAMs在多个视频分割基准测试中表现出色，能够在移动设备上以合理的质量进行视频物体分割。'}}}, {'id': 'https://huggingface.co/papers/2411.17459', 'title': 'WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model', 'url': 'https://huggingface.co/papers/2411.17459', 'abstract': 'Video Variational Autoencoder (VAE) encodes videos into a low-dimensional latent space, becoming a key component of most Latent Video Diffusion Models (LVDMs) to reduce model training costs. However, as the resolution and duration of generated videos increase, the encoding cost of Video VAEs becomes a limiting bottleneck in training LVDMs. Moreover, the block-wise inference method adopted by most LVDMs can lead to discontinuities of latent space when processing long-duration videos. The key to addressing the computational bottleneck lies in decomposing videos into distinct components and efficiently encoding the critical information. Wavelet transform can decompose videos into multiple frequency-domain components and improve the efficiency significantly, we thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages multi-level wavelet transform to facilitate low-frequency energy flow into latent representation. Furthermore, we introduce a method called Causal Cache, which maintains the integrity of latent space during block-wise inference. Compared to state-of-the-art video VAEs, WF-VAE demonstrates superior performance in both PSNR and LPIPS metrics, achieving 2x higher throughput and 4x lower memory consumption while maintaining competitive reconstruction quality. Our code and models are available at https://github.com/PKU-YuanGroup/WF-VAE.', 'score': 8, 'issue_id': 909, 'pub_date': '2024-11-26', 'pub_date_card': {'ru': '26 ноября', 'en': 'November 26', 'zh': '11月26日'}, 'hash': '9b68162718865b81', 'authors': ['Zongjian Li', 'Bin Lin', 'Yang Ye', 'Liuhan Chen', 'Xinhua Cheng', 'Shenghai Yuan', 'Li Yuan'], 'affiliations': ['Peking University', 'Peng Cheng Laboratory', 'Rabbitpre Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2411.17459.jpg', 'data': {'categories': ['#diffusion', '#data', '#video', '#architecture', '#open_source', '#optimization', '#training'], 'emoji': '🌊', 'ru': {'title': 'Эффективное кодирование видео с помощью вейвлетов', 'desc': 'Статья представляет новый метод кодирования видео под названием Wavelet Flow VAE (WF-VAE). Этот автоэнкодер использует многоуровневое вейвлет-преобразование для эффективного кодирования низкочастотной информации в видео. WF-VAE решает проблему вычислительных ограничений при обработке видео высокого разрешения и большой длительности в латентных видео-диффузионных моделях (LVDM). Метод также включает технику Causal Cache для сохранения целостности латентного пространства при поблочной обработке.'}, 'en': {'title': 'Efficient Video Encoding with Wavelet Flow VAE', 'desc': 'The paper introduces Wavelet Flow VAE (WF-VAE), a novel approach to encoding videos into a low-dimensional latent space using wavelet transforms. This method addresses the computational bottleneck of traditional Video VAEs, especially when generating high-resolution and long-duration videos. By decomposing videos into frequency-domain components, WF-VAE enhances the efficiency of encoding critical information. Additionally, the proposed Causal Cache method ensures continuity in the latent space during block-wise inference, resulting in improved performance metrics compared to existing video VAEs.'}, 'zh': {'title': '小波流VAE：高效视频编码的新方法', 'desc': '视频变分自编码器（VAE）将视频编码为低维潜在空间，是大多数潜在视频扩散模型（LVDMs）的关键组成部分，能够降低模型训练成本。然而，随着生成视频的分辨率和时长增加，视频VAE的编码成本成为训练LVDMs的瓶颈。此外，大多数LVDMs采用的块状推理方法在处理长时长视频时可能导致潜在空间的不连续性。为了解决计算瓶颈，我们提出了小波流VAE（WF-VAE），通过多级小波变换有效编码视频的关键信息，并引入因果缓存方法以保持潜在空间的完整性。'}}}, {'id': 'https://huggingface.co/papers/2412.00947', 'title': 'VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information', 'url': 'https://huggingface.co/papers/2412.00947', 'abstract': 'Errors in understanding visual information in images (i.e., visual perception errors) remain a major source of mistakes in Large Vision Language Models (LVLMs). While further analysis is essential, there is a deficiency in datasets for evaluating the visual perception of LVLMs. In this work, we introduce VisOnlyQA, a new dataset designed to directly evaluate the visual perception capabilities of LVLMs on questions about geometric and numerical information in scientific figures. Our dataset enables us to analyze the visual perception of LVLMs for fine-grained visual information, independent of other capabilities such as reasoning. The evaluation set of VisOnlyQA includes 1,200 multiple-choice questions in 12 tasks on four categories of figures. We also provide synthetic training data consisting of 70k instances. Our experiments on VisOnlyQA highlight the following findings: (i) 20 LVLMs we evaluate, including GPT-4o and Gemini 1.5 Pro, work poorly on the visual perception tasks in VisOnlyQA, while human performance is nearly perfect. (ii) Fine-tuning on synthetic training data demonstrates the potential for enhancing the visual perception of LVLMs, but observed improvements are limited to certain tasks and specific models. (iii) Stronger language models improve the visual perception of LVLMs. In summary, our experiments suggest that both training data and model architectures should be improved to enhance the visual perception capabilities of LVLMs. The datasets, code, and model responses are provided at https://github.com/psunlpgroup/VisOnlyQA.', 'score': 6, 'issue_id': 911, 'pub_date': '2024-12-01', 'pub_date_card': {'ru': '1 декабря', 'en': 'December 1', 'zh': '12月1日'}, 'hash': '7135f8936b0d3ee8', 'authors': ['Ryo Kamoi', 'Yusen Zhang', 'Sarkar Snigdha Sarathi Das', 'Ranran Haoran Zhang', 'Rui Zhang'], 'affiliations': ['Penn State University'], 'pdf_title_img': 'assets/pdf/title_img/2412.00947.jpg', 'data': {'categories': ['#dataset', '#interpretability', '#cv', '#synthetic', '#training'], 'emoji': '👁️', 'ru': {'title': 'VisOnlyQA: новый путь к улучшению визуального восприятия AI', 'desc': 'Исследователи представили новый датасет VisOnlyQA для оценки визуального восприятия больших визуально-языковых моделей (LVLM). Датасет фокусируется на геометрической и числовой информации в научных изображениях, позволяя анализировать способности моделей к тонкому визуальному восприятию. Эксперименты показали, что даже передовые LVLM, такие как GPT-4o и Gemini 1.5 Pro, плохо справляются с задачами VisOnlyQA, в то время как люди демонстрируют почти идеальные результаты. Исследование подчеркивает необходимость улучшения как обучающих данных, так и архитектур моделей для повышения качества визуального восприятия LVLM.'}, 'en': {'title': 'Enhancing Visual Perception in LVLMs with VisOnlyQA', 'desc': 'This paper addresses the issue of visual perception errors in Large Vision Language Models (LVLMs), which can lead to mistakes when interpreting images. The authors introduce a new dataset called VisOnlyQA, specifically designed to evaluate LVLMs on geometric and numerical questions related to scientific figures. The dataset includes 1,200 multiple-choice questions across various tasks and provides synthetic training data to improve model performance. The findings reveal that while LVLMs struggle with visual perception tasks, fine-tuning with synthetic data can enhance their capabilities, particularly when using stronger language models.'}, 'zh': {'title': '提升视觉感知，助力大型视觉语言模型', 'desc': '本研究介绍了一个新的数据集VisOnlyQA，旨在直接评估大型视觉语言模型（LVLMs）在科学图形中几何和数值信息问题上的视觉感知能力。该数据集包含1200个多项选择题，涵盖四类图形的12个任务，帮助分析LVLMs对细粒度视觉信息的理解。实验结果显示，评估的20个LVLMs在视觉感知任务上的表现较差，而人类的表现几乎完美。通过合成训练数据进行微调可以提升LVLMs的视觉感知能力，但改进效果有限，且更强的语言模型能够提高视觉感知能力。'}}}, {'id': 'https://huggingface.co/papers/2412.01316', 'title': 'Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation', 'url': 'https://huggingface.co/papers/2412.01316', 'abstract': 'We introduce Presto, a novel video diffusion model designed to generate 15-second videos with long-range coherence and rich content. Extending video generation methods to maintain scenario diversity over long durations presents significant challenges. To address this, we propose a Segmented Cross-Attention (SCA) strategy, which splits hidden states into segments along the temporal dimension, allowing each segment to cross-attend to a corresponding sub-caption. SCA requires no additional parameters, enabling seamless incorporation into current DiT-based architectures. To facilitate high-quality long video generation, we build the LongTake-HD dataset, consisting of 261k content-rich videos with scenario coherence, annotated with an overall video caption and five progressive sub-captions. Experiments show that our Presto achieves 78.5% on the VBench Semantic Score and 100% on the Dynamic Degree, outperforming existing state-of-the-art video generation methods. This demonstrates that our proposed Presto significantly enhances content richness, maintains long-range coherence, and captures intricate textual details. More details are displayed on our project page: https://presto-video.github.io/.', 'score': 6, 'issue_id': 910, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '5bca597a08ec0a14', 'authors': ['Xin Yan', 'Yuxuan Cai', 'Qiuyue Wang', 'Yuan Zhou', 'Wenhao Huang', 'Huan Yang'], 'affiliations': ['01.AI'], 'pdf_title_img': 'assets/pdf/title_img/2412.01316.jpg', 'data': {'categories': ['#video', '#dataset', '#architecture', '#diffusion', '#long_context'], 'emoji': '🎬', 'ru': {'title': 'Presto: Революция в генерации длинных видео с помощью ИИ', 'desc': 'Presto - это новая модель диффузии видео, разработанная для генерации 15-секундных видеороликов с долгосрочной согласованностью и богатым содержанием. Модель использует стратегию Segmented Cross-Attention (SCA), которая разделяет скрытые состояния на сегменты вдоль временного измерения, позволяя каждому сегменту перекрестно обращаться к соответствующей подписи. Для обучения модели был создан датасет LongTake-HD, содержащий 261 тысячу видео с богатым содержанием и согласованностью сценариев. Эксперименты показывают, что Presto превосходит существующие методы генерации видео по показателям семантической оценки и степени динамичности.'}, 'en': {'title': 'Presto: Revolutionizing Video Generation with Long-Range Coherence', 'desc': 'Presto is a new video diffusion model that generates 15-second videos while ensuring long-range coherence and rich content. It introduces a Segmented Cross-Attention (SCA) strategy that divides hidden states into segments, allowing each segment to focus on specific sub-captions without needing extra parameters. To support this model, the LongTake-HD dataset was created, containing 261,000 videos with coherent scenarios and detailed captions. Experiments show that Presto outperforms existing methods, achieving high scores in semantic understanding and dynamic content generation.'}, 'zh': {'title': 'Presto：生成长时间一致性视频的新方法', 'desc': '我们介绍了一种新的视频扩散模型Presto，旨在生成具有长时间一致性和丰富内容的15秒视频。为了解决在长时间内保持场景多样性的挑战，我们提出了一种分段交叉注意力(SCA)策略，该策略将隐藏状态沿时间维度分段，使每个段能够与相应的子标题进行交叉关注。SCA不需要额外的参数，可以无缝集成到现有的基于DiT的架构中。我们的实验表明，Presto在视频生成方面显著优于现有的最先进方法，提升了内容丰富性和长距离一致性。'}}}, {'id': 'https://huggingface.co/papers/2411.19939', 'title': 'VLSBench: Unveiling Visual Leakage in Multimodal Safety', 'url': 'https://huggingface.co/papers/2411.19939', 'abstract': 'Safety concerns of Multimodal large language models (MLLMs) have gradually become an important problem in various applications. Surprisingly, previous works indicate a counter-intuitive phenomenon that using textual unlearning to align MLLMs achieves comparable safety performances with MLLMs trained with image-text pairs. To explain such a counter-intuitive phenomenon, we discover a visual safety information leakage (VSIL) problem in existing multimodal safety benchmarks, i.e., the potentially risky and sensitive content in the image has been revealed in the textual query. In this way, MLLMs can easily refuse these sensitive text-image queries according to textual queries. However, image-text pairs without VSIL are common in real-world scenarios and are overlooked by existing multimodal safety benchmarks. To this end, we construct multimodal visual leakless safety benchmark (VLSBench) preventing visual safety leakage from image to textual query with 2.4k image-text pairs. Experimental results indicate that VLSBench poses a significant challenge to both open-source and close-source MLLMs, including LLaVA, Qwen2-VL, Llama3.2-Vision, and GPT-4o. This study demonstrates that textual alignment is enough for multimodal safety scenarios with VSIL, while multimodal alignment is a more promising solution for multimodal safety scenarios without VSIL. Please see our code and data at: http://hxhcreate.github.io/VLSBench', 'score': 5, 'issue_id': 914, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '03d8c636cf8c9486', 'authors': ['Xuhao Hu', 'Dongrui Liu', 'Hao Li', 'Xuanjing Huang', 'Jing Shao'], 'affiliations': ['Beihang University', 'Fudan University', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2411.19939.jpg', 'data': {'categories': ['#ethics', '#multimodal', '#benchmark', '#leakage', '#open_source', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Новый взгляд на безопасность мультимодальных ИИ-моделей', 'desc': 'Статья рассматривает проблему безопасности мультимодальных больших языковых моделей (MLLM). Авторы обнаружили феномен утечки визуальной информации о безопасности (VSIL) в существующих мультимодальных тестах безопасности. Для решения этой проблемы они создали новый набор данных VLSBench, содержащий 2400 пар изображение-текст без VSIL. Эксперименты показали, что VLSBench представляет значительную сложность для современных MLLM и демонстрирует необходимость мультимодальной настройки для сценариев без VSIL.'}, 'en': {'title': 'Unveiling Safety in Multimodal Models: The VSIL Challenge', 'desc': 'This paper addresses safety issues in Multimodal Large Language Models (MLLMs) by highlighting a problem called Visual Safety Information Leakage (VSIL). It shows that using textual unlearning can achieve safety performance similar to training with image-text pairs, which is surprising. The authors create a new benchmark, VLSBench, to test MLLMs without the influence of VSIL, using 2.4k image-text pairs. The findings suggest that while textual alignment can ensure safety in scenarios with VSIL, multimodal alignment is necessary for scenarios without it.'}, 'zh': {'title': '多模态安全性的新挑战与解决方案', 'desc': '这篇论文探讨了多模态大型语言模型（MLLMs）在安全性方面的挑战。研究发现，文本去学习可以与使用图像-文本对训练的模型在安全性表现上相当。作者指出，现有的多模态安全基准存在视觉安全信息泄漏（VSIL）问题，这使得模型能够轻易拒绝敏感的文本-图像查询。为了解决这个问题，研究者构建了一个新的多模态视觉无泄漏安全基准（VLSBench），以更好地评估模型在没有VSIL情况下的安全性。'}}}, {'id': 'https://huggingface.co/papers/2412.00176', 'title': 'Art-Free Generative Models: Art Creation Without Graphic Art Knowledge', 'url': 'https://huggingface.co/papers/2412.00176', 'abstract': 'We explore the question: "How much prior art knowledge is needed to create art?" To investigate this, we propose a text-to-image generation model trained without access to art-related content. We then introduce a simple yet effective method to learn an art adapter using only a few examples of selected artistic styles. Our experiments show that art generated using our method is perceived by users as comparable to art produced by models trained on large, art-rich datasets. Finally, through data attribution techniques, we illustrate how examples from both artistic and non-artistic datasets contributed to the creation of new artistic styles.', 'score': 4, 'issue_id': 921, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '41f372dd1c030fbd', 'authors': ['Hui Ren', 'Joanna Materzynska', 'Rohit Gandikota', 'David Bau', 'Antonio Torralba'], 'affiliations': ['MIT', 'Northeastern University', 'ShanghaiTech University'], 'pdf_title_img': 'assets/pdf/title_img/2412.00176.jpg', 'data': {'categories': ['#cv', '#dataset', '#synthetic', '#games', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Искусственный интеллект творит шедевры с минимальным обучением', 'desc': 'Исследователи изучают вопрос о необходимом объеме предварительных знаний об искусстве для его создания. Они предлагают модель генерации изображений по тексту, обученную без доступа к контенту, связанному с искусством. Затем авторы представляют простой, но эффективный метод обучения адаптера для создания искусства, используя лишь несколько примеров выбранных художественных стилей. Эксперименты показывают, что искусство, созданное с помощью этого метода, воспринимается пользователями на уровне, сравнимом с произведениями моделей, обученных на больших наборах данных, богатых искусством.'}, 'en': {'title': 'Creating Art with Minimal Prior Knowledge', 'desc': "This paper investigates the necessity of prior art knowledge in generating art through a text-to-image model. The authors propose a novel approach that trains the model without any art-related content, relying instead on a small number of examples from specific artistic styles to create an 'art adapter'. Their experiments reveal that the art produced by this method is perceived similarly to that generated by models trained on extensive art datasets. Additionally, they employ data attribution techniques to demonstrate how both artistic and non-artistic examples influence the development of new artistic styles."}, 'zh': {'title': '创造艺术无需丰富的艺术知识', 'desc': '我们探讨了创造艺术需要多少先前的艺术知识。为此，我们提出了一种文本到图像生成模型，该模型在没有艺术相关内容的情况下进行训练。我们还引入了一种简单有效的方法，仅使用少量选定艺术风格的示例来学习艺术适配器。实验结果表明，使用我们的方法生成的艺术作品在用户眼中与在大型艺术丰富数据集上训练的模型生成的艺术作品相当。'}}}, {'id': 'https://huggingface.co/papers/2412.01800', 'title': 'PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos', 'url': 'https://huggingface.co/papers/2412.01800', 'abstract': 'Recent advancements in video-based large language models (Video LLMs) have witnessed the emergence of diverse capabilities to reason and interpret dynamic visual content. Among them, gameplay videos stand out as a distinctive data source, often containing glitches that defy physics commonsense. This characteristic renders them an effective benchmark for assessing the under-explored capability of physical commonsense understanding in video LLMs. In this paper, we propose PhysGame as a pioneering benchmark to evaluate physical commonsense violations in gameplay videos. PhysGame comprises 880 videos associated with glitches spanning four fundamental domains (i.e., mechanics, kinematics, optics, and material properties) and across 12 distinct physical commonsense. Through extensively evaluating various state-ofthe-art video LLMs, our findings reveal that the performance of current open-source video LLMs significantly lags behind that of proprietary counterparts. To bridge this gap, we curate an instruction tuning dataset PhysInstruct with 140,057 question-answering pairs to facilitate physical commonsense learning. In addition, we also propose a preference optimization dataset PhysDPO with 34,358 training pairs, where the dis-preferred responses are generated conditioned on misleading titles (i.e., meta information hacking), fewer frames (i.e., temporal hacking) and lower spatial resolutions (i.e., spatial hacking). Based on the suite of datasets, we propose PhysVLM as a physical knowledge-enhanced video LLM. Extensive experiments on both physical-oriented benchmark PhysGame and general video understanding benchmarks demonstrate the state-ofthe-art performance of PhysVLM.', 'score': 4, 'issue_id': 917, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '43cc035146493599', 'authors': ['Meng Cao', 'Haoran Tang', 'Haoze Zhao', 'Hangyu Guo', 'Jiaheng Liu', 'Ge Zhang', 'Ruyang Liu', 'Qiang Sun', 'Ian Reid', 'Xiaodan Liang'], 'affiliations': ['Alibaba Group', 'Mohamed bin Zayed University of Artificial Intelligence', 'Peking University', 'Sun Yat-sen University', 'University of Toronto', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2412.01800.jpg', 'data': {'categories': ['#dataset', '#video', '#open_source', '#optimization', '#benchmark', '#reasoning', '#multimodal', '#interpretability'], 'emoji': '🎮', 'ru': {'title': 'PhysVLM: Обучение видеоязыковых моделей физическому здравому смыслу через игровые глитчи', 'desc': 'Исследователи представили новый бенчмарк PhysGame для оценки понимания физических закономерностей в видеоязыковых моделях (Video LLM) на основе геймплейных видео с глитчами. Они создали набор данных PhysInstruct для обучения моделей физическому здравому смыслу и набор PhysDPO для оптимизации предпочтений. На базе этих наборов данных была разработана модель PhysVLM, показавшая лучшие результаты как на специализированном бенчмарке PhysGame, так и на общих тестах понимания видео.'}, 'en': {'title': 'Enhancing Video LLMs with Physical Commonsense Understanding', 'desc': 'This paper introduces PhysGame, a new benchmark designed to evaluate how well video-based large language models (Video LLMs) understand physical commonsense by analyzing glitches in gameplay videos. The benchmark includes 880 videos that highlight violations of physical laws across four domains: mechanics, kinematics, optics, and material properties. To improve the performance of Video LLMs, the authors created two additional datasets: PhysInstruct for instruction tuning and PhysDPO for preference optimization, which help models learn from common misconceptions and misleading information. The proposed PhysVLM model, enhanced with physical knowledge, shows superior performance on both the PhysGame benchmark and general video understanding tasks compared to existing models.'}, 'zh': {'title': '提升视频模型的物理常识理解能力', 'desc': '本文介绍了一种新颖的基准测试PhysGame，用于评估视频大语言模型在游戏视频中对物理常识的理解能力。游戏视频中常常出现违反物理常识的故障，这使得它们成为评估模型能力的有效数据源。我们还创建了PhysInstruct和PhysDPO两个数据集，以帮助模型学习物理常识并优化其偏好。通过这些数据集，我们提出了PhysVLM，一个增强物理知识的视频大语言模型，并在多个基准测试中展示了其优越的性能。'}}}, {'id': 'https://huggingface.co/papers/2411.19799', 'title': 'INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge', 'url': 'https://huggingface.co/papers/2411.19799', 'abstract': 'The performance differential of large language models (LLM) between languages hinders their effective deployment in many regions, inhibiting the potential economic and societal value of generative AI tools in many communities. However, the development of functional LLMs in many languages (\\ie, multilingual LLMs) is bottlenecked by the lack of high-quality evaluation resources in languages other than English. Moreover, current practices in multilingual benchmark construction often translate English resources, ignoring the regional and cultural knowledge of the environments in which multilingual systems would be used. In this work, we construct an evaluation suite of 197,243 QA pairs from local exam sources to measure the capabilities of multilingual LLMs in a variety of regional contexts. Our novel resource, INCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across 44 written languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed.', 'score': 4, 'issue_id': 914, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '7b0cbdd28adecf2c', 'authors': ['Angelika Romanou', 'Negar Foroutan', 'Anna Sotnikova', 'Zeming Chen', 'Sree Harsha Nelaturu', 'Shivalika Singh', 'Rishabh Maheshwary', 'Micol Altomare', 'Mohamed A. Haggag', 'Snegha A', 'Alfonso Amayuelas', 'Azril Hafizi Amirudin', 'Viraat Aryabumi', 'Danylo Boiko', 'Michael Chang', 'Jenny Chim', 'Gal Cohen', 'Aditya Kumar Dalmia', 'Abraham Diress', 'Sharad Duwal', 'Daniil Dzenhaliou', 'Daniel Fernando Erazo Florez', 'Fabian Farestam', 'Joseph Marvin Imperial', 'Shayekh Bin Islam', 'Perttu Isotalo', 'Maral Jabbarishiviari', 'Börje F. Karlsson', 'Eldar Khalilov', 'Christopher Klamm', 'Fajri Koto', 'Dominik Krzemiński', 'Gabriel Adriano de Melo', 'Syrielle Montariol', 'Yiyang Nan', 'Joel Niklaus', 'Jekaterina Novikova', 'Johan Samir Obando Ceron', 'Debjit Paul', 'Esther Ploeger', 'Jebish Purbey', 'Swati Rajwal', 'Selvan Sunitha Ravi', 'Sara Rydell', 'Roshan Santhosh', 'Drishti Sharma', 'Marjana Prifti Skenduli', 'Arshia Soltani Moakhar', 'Bardia Soltani Moakhar', 'Ran Tamir', 'Ayush Kumar Tarun', 'Azmine Toushik Wasi', 'Thenuka Ovin Weerasinghe', 'Serhan Yilmaz', 'Mike Zhang', 'Imanol Schlag', 'Marzieh Fadaee', 'Sara Hooker', 'Antoine Bosselut'], 'affiliations': ['Cohere For AI', 'Cohere For AI Community', 'EPFL', 'ETH Zurich', 'Swiss AI Initiative'], 'pdf_title_img': 'assets/pdf/title_img/2411.19799.jpg', 'data': {'categories': ['#multilingual', '#low_resource', '#benchmark', '#reasoning'], 'emoji': '🌍', 'ru': {'title': 'Глобальная оценка многоязычных ИИ-моделей в локальных контекстах', 'desc': 'Статья посвящена проблеме неравномерной эффективности больших языковых моделей (LLM) для разных языков. Авторы создали набор данных INCLUDE из 197,243 пар вопросов и ответов на 44 языках для оценки многоязычных LLM. Этот ресурс основан на местных экзаменационных материалах и охватывает различные региональные контексты. INCLUDE позволяет оценивать знания и способности к рассуждению многоязычных LLM в реальных языковых средах их применения.'}, 'en': {'title': 'Bridging Language Gaps with INCLUDE: A Multilingual Benchmark for LLMs', 'desc': 'This paper addresses the performance gap of large language models (LLMs) across different languages, which limits their use in various regions. It highlights the challenge of developing multilingual LLMs due to the scarcity of high-quality evaluation resources in non-English languages. The authors propose a new evaluation suite called INCLUDE, consisting of 197,243 question-and-answer pairs sourced from local exams. This benchmark is designed to assess the capabilities of multilingual LLMs in real-world contexts, taking into account regional and cultural knowledge.'}, 'zh': {'title': '提升多语言模型的实际应用能力', 'desc': '这篇论文讨论了大型语言模型（LLM）在不同语言之间的性能差异，这影响了它们在许多地区的有效应用。为了克服多语言LLM开发中的瓶颈，作者构建了一个包含197,243个问答对的评估套件，来源于当地考试材料。这个新资源INCLUDE是一个全面的知识和推理中心基准，涵盖44种书面语言，旨在评估多语言LLM在实际语言环境中的表现。通过这种方式，研究希望提升生成性人工智能工具在不同社区的经济和社会价值。'}}}, {'id': 'https://huggingface.co/papers/2412.01821', 'title': 'World-consistent Video Diffusion with Explicit 3D Modeling', 'url': 'https://huggingface.co/papers/2412.01821', 'abstract': 'Recent advancements in diffusion models have set new benchmarks in image and video generation, enabling realistic visual synthesis across single- and multi-frame contexts. However, these models still struggle with efficiently and explicitly generating 3D-consistent content. To address this, we propose World-consistent Video Diffusion (WVD), a novel framework that incorporates explicit 3D supervision using XYZ images, which encode global 3D coordinates for each image pixel. More specifically, we train a diffusion transformer to learn the joint distribution of RGB and XYZ frames. This approach supports multi-task adaptability via a flexible inpainting strategy. For example, WVD can estimate XYZ frames from ground-truth RGB or generate novel RGB frames using XYZ projections along a specified camera trajectory. In doing so, WVD unifies tasks like single-image-to-3D generation, multi-view stereo, and camera-controlled video generation. Our approach demonstrates competitive performance across multiple benchmarks, providing a scalable solution for 3D-consistent video and image generation with a single pretrained model.', 'score': 3, 'issue_id': 922, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '843891601e85de06', 'authors': ['Qihang Zhang', 'Shuangfei Zhai', 'Miguel Angel Bautista', 'Kevin Miao', 'Alexander Toshev', 'Joshua Susskind', 'Jiatao Gu'], 'affiliations': ['Apple', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.01821.jpg', 'data': {'categories': ['#3d', '#benchmark', '#diffusion', '#video'], 'emoji': '🎥', 'ru': {'title': '3D-согласованная генерация видео с помощью диффузионных моделей', 'desc': 'Статья представляет новую модель World-consistent Video Diffusion (WVD) для генерации 3D-согласованного контента. WVD использует XYZ-изображения для явного 3D-контроля в процессе диффузии. Модель обучается совместному распределению RGB и XYZ кадров, что позволяет решать различные задачи, включая генерацию 3D из одного изображения и создание видео с контролем камеры. WVD демонстрирует конкурентоспособные результаты на нескольких эталонных тестах.'}, 'en': {'title': 'Unifying 3D Consistency in Video Generation with WVD', 'desc': 'This paper introduces World-consistent Video Diffusion (WVD), a new framework that enhances video and image generation by incorporating 3D supervision. WVD uses XYZ images to provide global 3D coordinates for each pixel, allowing for more accurate and consistent 3D content generation. The framework employs a diffusion transformer to learn the relationship between RGB and XYZ frames, enabling tasks like generating new RGB frames from 3D data. Overall, WVD offers a versatile solution for various tasks, achieving strong performance across benchmarks with a single pretrained model.'}, 'zh': {'title': '统一3D一致性的视频生成新框架', 'desc': '最近，扩散模型在图像和视频生成方面取得了显著进展，能够在单帧和多帧上下文中实现逼真的视觉合成。然而，这些模型在高效且明确地生成3D一致内容方面仍然存在挑战。为了解决这个问题，我们提出了世界一致视频扩散（WVD），这是一个新颖的框架，利用XYZ图像进行明确的3D监督，编码每个图像像素的全局3D坐标。WVD通过灵活的修补策略支持多任务适应性，能够从真实的RGB估计XYZ帧，或沿指定的相机轨迹生成新的RGB帧。'}}}, {'id': 'https://huggingface.co/papers/2412.01250', 'title': 'Collaborative Instance Navigation: Leveraging Agent Self-Dialogue to Minimize User Input', 'url': 'https://huggingface.co/papers/2412.01250', 'abstract': 'Existing embodied instance goal navigation tasks, driven by natural language, assume human users to provide complete and nuanced instance descriptions prior to the navigation, which can be impractical in the real world as human instructions might be brief and ambiguous. To bridge this gap, we propose a new task, Collaborative Instance Navigation (CoIN), with dynamic agent-human interaction during navigation to actively resolve uncertainties about the target instance in natural, template-free, open-ended dialogues. To address CoIN, we propose a novel method, Agent-user Interaction with UncerTainty Awareness (AIUTA), leveraging the perception capability of Vision Language Models (VLMs) and the capability of Large Language Models (LLMs). First, upon object detection, a Self-Questioner model initiates a self-dialogue to obtain a complete and accurate observation description, while a novel uncertainty estimation technique mitigates inaccurate VLM perception. Then, an Interaction Trigger module determines whether to ask a question to the user, continue or halt navigation, minimizing user input. For evaluation, we introduce CoIN-Bench, a benchmark supporting both real and simulated humans. AIUTA achieves competitive performance in instance navigation against state-of-the-art methods, demonstrating great flexibility in handling user inputs.', 'score': 3, 'issue_id': 915, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': '8b768de35e4c5114', 'authors': ['Francesco Taioli', 'Edoardo Zorzi', 'Gianni Franchi', 'Alberto Castellini', 'Alessandro Farinelli', 'Marco Cristani', 'Yiming Wang'], 'affiliations': ['Fondazione Bruno Kessler', 'Polytechnic of Turin', 'U2IS, ENSTA Paris, Institut Polytechnique de Paris', 'University of Verona'], 'pdf_title_img': 'assets/pdf/title_img/2412.01250.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#multimodal', '#reasoning', '#agents'], 'emoji': '🗺️', 'ru': {'title': 'Интерактивная навигация робота с уточнением цели у человека', 'desc': 'Статья представляет новую задачу навигации по инстансам объектов - Collaborative Instance Navigation (CoIN), где агент взаимодействует с человеком во время навигации для уточнения целевого объекта. Предложен метод AIUTA, использующий Vision Language Models и Large Language Models для обработки визуальной информации и генерации вопросов. AIUTA включает модули Self-Questioner для самоанализа наблюдений и Interaction Trigger для определения необходимости задать вопрос пользователю. Авторы также представили бенчмарк CoIN-Bench для оценки подобных систем.'}, 'en': {'title': 'Navigating Together: Enhancing Instance Navigation with Human-AI Collaboration', 'desc': 'This paper introduces a new task called Collaborative Instance Navigation (CoIN), which allows agents to interact with humans during navigation to clarify ambiguous instructions. The proposed method, Agent-user Interaction with UncerTainty Awareness (AIUTA), uses Vision Language Models (VLMs) and Large Language Models (LLMs) to enhance the navigation process. It includes a Self-Questioner model that helps the agent gather detailed information about the target instance and an Interaction Trigger module that decides when to engage the user for input. The authors also present CoIN-Bench, a benchmark for evaluating the performance of navigation systems in both real and simulated environments, showing that AIUTA performs well compared to existing methods.'}, 'zh': {'title': '协作实例导航：让机器更懂人类指令', 'desc': '现有的实例目标导航任务通常需要用户提供详细的描述，但在现实中，这种要求往往不切实际。为了解决这个问题，我们提出了一种新的任务，称为协作实例导航（CoIN），通过动态的代理-用户互动来解决导航中的不确定性。我们的方法，代理-用户互动与不确定性意识（AIUTA），结合了视觉语言模型和大型语言模型的能力，能够在导航过程中主动与用户对话。通过引入CoIN-Bench基准，我们的AIUTA方法在实例导航中表现出色，展示了处理用户输入的灵活性。'}}}, {'id': 'https://huggingface.co/papers/2411.19477', 'title': 'A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models', 'url': 'https://huggingface.co/papers/2411.19477', 'abstract': 'We propose a general two-stage algorithm that enjoys a provable scaling law for the test-time compute of large language models (LLMs). Given an input problem, the proposed algorithm first generates N candidate solutions, and then chooses the best one via a multiple-round knockout tournament where each pair of candidates are compared for K times and only the winners move on to the next round. In a minimalistic implementation, both stages can be executed with a black-box LLM alone and nothing else (e.g., no external verifier or reward model), and a total of N times (K + 1) highly parallelizable LLM calls are needed for solving an input problem. Assuming that a generated candidate solution is correct with probability p_{gen} > 0 and a comparison between a pair of correct and incorrect solutions identifies the right winner with probability p_{comp} > 0.5 (i.e., better than a random guess), we prove theoretically that the failure probability of the proposed algorithm decays to zero exponentially with respect to N and K: $P(final output is incorrect) le (1 - p_{gen})^N + lceil log_2 N rceil e^{-2 K (p_{comp} - 0.5)^2}.$ Our empirical results with the challenging MMLU-Pro benchmark validate the technical assumptions, as well as the efficacy of the proposed algorithm and the gains from scaling up its test-time compute.', 'score': 3, 'issue_id': 912, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 ноября', 'en': 'November 29', 'zh': '11月29日'}, 'hash': '6057902d5fcbe3f4', 'authors': ['Yanxi Chen', 'Xuchen Pan', 'Yaliang Li', 'Bolin Ding', 'Jingren Zhou'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2411.19477.jpg', 'data': {'categories': ['#training', '#benchmark', '#architecture', '#optimization'], 'emoji': '🏆', 'ru': {'title': 'Масштабируемый алгоритм улучшения точности больших языковых моделей', 'desc': 'Авторы предлагают двухэтапный алгоритм, который демонстрирует масштабируемый закон для вычислений больших языковых моделей (LLM) во время тестирования. Алгоритм сначала генерирует N кандидатов решений, а затем выбирает лучшее с помощью многораундового турнира на выбывание. Теоретически доказано, что вероятность ошибки алгоритма экспоненциально уменьшается с увеличением N и K. Эмпирические результаты на сложном бенчмарке MMLU-Pro подтверждают эффективность предложенного метода.'}, 'en': {'title': 'Efficient Solution Selection for Large Language Models', 'desc': 'This paper introduces a two-stage algorithm designed to improve the efficiency of large language models (LLMs) during test time. The first stage generates multiple candidate solutions for a given problem, while the second stage employs a knockout tournament to select the best solution through repeated comparisons. The algorithm can operate solely with a black-box LLM, requiring a manageable number of parallel calls to generate and evaluate candidates. The authors provide theoretical proof that the likelihood of incorrect outputs decreases exponentially as the number of candidates and comparisons increases, supported by empirical results from the MMLU-Pro benchmark.'}, 'zh': {'title': '高效选择：两阶段算法优化大语言模型计算', 'desc': '我们提出了一种通用的两阶段算法，能够在大语言模型（LLMs）的测试时间计算中实现可证明的扩展规律。该算法首先生成N个候选解，然后通过多轮淘汰赛选择最佳解，每对候选解比较K次，只有胜者进入下一轮。该算法的最简实现仅需使用黑箱LLM，无需外部验证器或奖励模型，总共需要N次(K + 1)高度可并行的LLM调用来解决输入问题。理论证明表明，假设生成的候选解正确的概率为p_{gen} > 0，且正确与错误解的比较能以概率p_{comp} > 0.5识别出正确的胜者，则该算法的失败概率随着N和K的增加呈指数级下降。'}}}, {'id': 'https://huggingface.co/papers/2412.00869', 'title': 'Exploring the Abilities of Large Language Models to Solve Proportional Analogies via Knowledge-Enhanced Prompting', 'url': 'https://huggingface.co/papers/2412.00869', 'abstract': 'Making analogies is fundamental to cognition. Proportional analogies, which consist of four terms, are often used to assess linguistic and cognitive abilities. For instance, completing analogies like "Oxygen is to Gas as <blank> is to <blank>" requires identifying the semantic relationship (e.g., "type of") between the first pair of terms ("Oxygen" and "Gas") and finding a second pair that shares the same relationship (e.g., "Aluminum" and "Metal"). In this work, we introduce a 15K Multiple-Choice Question Answering (MCQA) dataset for proportional analogy completion and evaluate the performance of contemporary Large Language Models (LLMs) in various knowledge-enhanced prompt settings. Specifically, we augment prompts with three types of knowledge: exemplar, structured, and targeted. Our results show that despite extensive training data, solving proportional analogies remains challenging for current LLMs, with the best model achieving an accuracy of 55%. Notably, we find that providing targeted knowledge can better assist models in completing proportional analogies compared to providing exemplars or collections of structured knowledge.', 'score': 2, 'issue_id': 926, 'pub_date': '2024-12-01', 'pub_date_card': {'ru': '1 декабря', 'en': 'December 1', 'zh': '12月1日'}, 'hash': '6816796631155a8c', 'authors': ['Thilini Wijesiriwardene', 'Ruwan Wickramarachchi', 'Sreeram Vennam', 'Vinija Jain', 'Aman Chadha', 'Amitava Das', 'Ponnurangam Kumaraguru', 'Amit Sheth'], 'affiliations': ['AI Institute, University of South Carolina, USA', 'Amazon GenAI, USA', 'IIIT Hyderabad, India', 'Meta, USA', 'Stanford University, USA'], 'pdf_title_img': 'assets/pdf/title_img/2412.00869.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#reasoning', '#data'], 'emoji': '🧠', 'ru': {'title': 'Большие языковые модели все еще не мастера аналогий', 'desc': 'Эта статья посвящена исследованию способности больших языковых моделей (LLM) решать пропорциональные аналогии. Авторы создали набор данных из 15 000 вопросов с множественным выбором для завершения аналогий. Они оценили производительность современных LLM в различных настройках промптов с дополнительными знаниями. Результаты показывают, что даже лучшие модели достигают точности лишь 55%, и целевые знания помогают больше, чем примеры или структурированная информация.'}, 'en': {'title': 'Enhancing Analogy Completion in LLMs with Targeted Knowledge', 'desc': 'This paper focuses on the challenge of solving proportional analogies, which are important for assessing cognitive abilities. The authors present a new dataset containing 15,000 multiple-choice questions designed for this purpose. They evaluate how well current Large Language Models (LLMs) perform on these analogies, especially when given different types of knowledge prompts. The findings reveal that while LLMs struggle with these tasks, targeted knowledge prompts significantly improve their performance compared to other types of knowledge.'}, 'zh': {'title': '提升类比能力，知识是关键！', 'desc': '类比是认知的重要部分，比例类比由四个术语组成，常用于评估语言和认知能力。本文介绍了一个包含15,000个多项选择题的比例类比完成数据集，并评估了当前大型语言模型（LLMs）在不同知识增强提示设置下的表现。研究发现，尽管模型经过大量训练，解决比例类比仍然具有挑战性，最佳模型的准确率仅为55%。特别是，提供针对性的知识比提供示例或结构化知识更能帮助模型完成比例类比。'}}}, {'id': 'https://huggingface.co/papers/2412.00319', 'title': 'Improving speaker verification robustness with synthetic emotional utterances', 'url': 'https://huggingface.co/papers/2412.00319', 'abstract': 'A speaker verification (SV) system offers an authentication service designed to confirm whether a given speech sample originates from a specific speaker. This technology has paved the way for various personalized applications that cater to individual preferences. A noteworthy challenge faced by SV systems is their ability to perform consistently across a range of emotional spectra. Most existing models exhibit high error rates when dealing with emotional utterances compared to neutral ones. Consequently, this phenomenon often leads to missing out on speech of interest. This issue primarily stems from the limited availability of labeled emotional speech data, impeding the development of robust speaker representations that encompass diverse emotional states.   To address this concern, we propose a novel approach employing the CycleGAN framework to serve as a data augmentation method. This technique synthesizes emotional speech segments for each specific speaker while preserving the unique vocal identity. Our experimental findings underscore the effectiveness of incorporating synthetic emotional data into the training process. The models trained using this augmented dataset consistently outperform the baseline models on the task of verifying speakers in emotional speech scenarios, reducing equal error rate by as much as 3.64% relative.', 'score': 1, 'issue_id': 926, 'pub_date': '2024-11-30', 'pub_date_card': {'ru': '30 ноября', 'en': 'November 30', 'zh': '11月30日'}, 'hash': 'e9e71338ed876bc0', 'authors': ['Nikhil Kumar Koditala', 'Chelsea Jui-Ting Ju', 'Ruirui Li', 'Minho Jin', 'Aman Chadha', 'Andreas Stolcke'], 'affiliations': ['Amazon Alexa AI, USA', 'Amazon Business, USA', 'Amazon GenAI, USA', 'Amazon Search Experience Science, USA', 'Amazon Web Services, USA'], 'pdf_title_img': 'assets/pdf/title_img/2412.00319.jpg', 'data': {'categories': ['#training', '#dataset', '#synthetic', '#audio'], 'emoji': '🎭', 'ru': {'title': 'CycleGAN для аугментации эмоциональной речи улучшает верификацию говорящего', 'desc': 'Статья предлагает новый подход к верификации говорящего с использованием CycleGAN для аугментации данных эмоциональной речи. Авторы синтезируют эмоциональные речевые сегменты для каждого конкретного диктора, сохраняя при этом уникальную голосовую идентичность. Эксперименты показывают, что модели, обученные на расширенном наборе данных, превосходят базовые модели в задаче верификации говорящих в сценариях с эмоциональной речью. Предложенный метод снижает равную ошибку (EER) до 3.64% относительно базовой линии.'}, 'en': {'title': 'Enhancing Speaker Verification with Synthetic Emotional Data', 'desc': "This paper discusses a speaker verification (SV) system that authenticates speakers based on their voice. A major challenge for these systems is accurately verifying speakers when they express different emotions, as most existing models struggle with emotional speech. The authors propose using a CycleGAN framework to create synthetic emotional speech data, which helps improve the training of SV models. Their results show that incorporating this augmented data significantly enhances the models' performance, reducing error rates in emotional speech verification."}, 'zh': {'title': '情感语音验证的新突破', 'desc': '本研究提出了一种说话人验证系统，旨在确认特定语音样本是否来自特定说话者。该系统面临的主要挑战是如何在不同情感状态下保持一致的性能。为了克服这一问题，我们采用CycleGAN框架进行数据增强，合成每个说话者的情感语音片段，同时保留其独特的声音特征。实验结果表明，使用合成情感数据训练的模型在情感语音验证任务中表现优于基线模型，错误率降低了3.64%。'}}}, {'id': 'https://huggingface.co/papers/2412.01408', 'title': 'Towards Cross-Lingual Audio Abuse Detection in Low-Resource Settings with Few-Shot Learning', 'url': 'https://huggingface.co/papers/2412.01408', 'abstract': 'Online abusive content detection, particularly in low-resource settings and within the audio modality, remains underexplored. We investigate the potential of pre-trained audio representations for detecting abusive language in low-resource languages, in this case, in Indian languages using Few Shot Learning (FSL). Leveraging powerful representations from models such as Wav2Vec and Whisper, we explore cross-lingual abuse detection using the ADIMA dataset with FSL. Our approach integrates these representations within the Model-Agnostic Meta-Learning (MAML) framework to classify abusive language in 10 languages. We experiment with various shot sizes (50-200) evaluating the impact of limited data on performance. Additionally, a feature visualization study was conducted to better understand model behaviour. This study highlights the generalization ability of pre-trained models in low-resource scenarios and offers valuable insights into detecting abusive language in multilingual contexts.', 'score': 1, 'issue_id': 918, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': 'f4c5e5e0485d3969', 'authors': ['Aditya Narayan Sankaran', 'Reza Farahbaksh', 'Noel Crespi'], 'affiliations': ['SAMOVAR, Télécom SudParis Institut Polytechnique de Paris 91120 Palaiseau, France'], 'pdf_title_img': 'assets/pdf/title_img/2412.01408.jpg', 'data': {'categories': ['#low_resource', '#interpretability', '#dataset', '#multilingual', '#audio', '#training'], 'emoji': '🎙️', 'ru': {'title': 'Борьба с оскорблениями в аудио: мало данных, много языков', 'desc': 'Исследование посвящено обнаружению оскорбительного контента в аудио на языках с ограниченными ресурсами, в частности на индийских языках, с использованием Few Shot Learning (FSL). Авторы применяют предобученные аудио-представления из моделей Wav2Vec и Whisper в рамках Model-Agnostic Meta-Learning (MAML) для классификации оскорбительного языка в 10 языках. Эксперименты проводились с различными размерами выборок (50-200) для оценки влияния ограниченных данных на производительность. Исследование демонстрирует способность предобученных моделей к обобщению в условиях ограниченных ресурсов и предоставляет ценные insights для обнаружения оскорбительного языка в многоязычном контексте.'}, 'en': {'title': 'Empowering Low-Resource Languages: Detecting Abuse in Audio with Few Shot Learning', 'desc': 'This paper focuses on detecting abusive language in audio, especially in languages with limited resources, like many Indian languages. It uses Few Shot Learning (FSL) techniques with pre-trained audio models such as Wav2Vec and Whisper to improve detection accuracy. The authors apply the Model-Agnostic Meta-Learning (MAML) framework to classify abusive content across 10 different languages, even with minimal training data. Their findings demonstrate how well pre-trained models can generalize in low-resource settings, providing insights for better abuse detection in multilingual environments.'}, 'zh': {'title': '利用预训练模型提升低资源环境下的辱骂内容检测', 'desc': '本研究探讨了在低资源环境中，特别是音频模式下，检测在线辱骂内容的潜力。我们使用预训练的音频表示，结合少量学习（Few Shot Learning），在印度语言中进行辱骂语言的检测。通过利用Wav2Vec和Whisper等模型的强大表示，我们在ADIMA数据集上进行跨语言的辱骂检测。研究表明，预训练模型在低资源场景中的泛化能力，为多语言环境中的辱骂语言检测提供了有价值的见解。'}}}, {'id': 'https://huggingface.co/papers/2412.01718', 'title': 'HUGSIM: A Real-Time, Photo-Realistic and Closed-Loop Simulator for Autonomous Driving', 'url': 'https://huggingface.co/papers/2412.01718', 'abstract': 'In the past few decades, autonomous driving algorithms have made significant progress in perception, planning, and control. However, evaluating individual components does not fully reflect the performance of entire systems, highlighting the need for more holistic assessment methods. This motivates the development of HUGSIM, a closed-loop, photo-realistic, and real-time simulator for evaluating autonomous driving algorithms. We achieve this by lifting captured 2D RGB images into the 3D space via 3D Gaussian Splatting, improving the rendering quality for closed-loop scenarios, and building the closed-loop environment. In terms of rendering, We tackle challenges of novel view synthesis in closed-loop scenarios, including viewpoint extrapolation and 360-degree vehicle rendering. Beyond novel view synthesis, HUGSIM further enables the full closed simulation loop, dynamically updating the ego and actor states and observations based on control commands. Moreover, HUGSIM offers a comprehensive benchmark across more than 70 sequences from KITTI-360, Waymo, nuScenes, and PandaSet, along with over 400 varying scenarios, providing a fair and realistic evaluation platform for existing autonomous driving algorithms. HUGSIM not only serves as an intuitive evaluation benchmark but also unlocks the potential for fine-tuning autonomous driving algorithms in a photorealistic closed-loop setting.', 'score': 0, 'issue_id': 931, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 декабря', 'en': 'December 2', 'zh': '12月2日'}, 'hash': 'cbcbc2a1e36ef143', 'authors': ['Hongyu Zhou', 'Longzhong Lin', 'Jiabao Wang', 'Yichong Lu', 'Dongfeng Bai', 'Bingbing Liu', 'Yue Wang', 'Andreas Geiger', 'Yiyi Liao'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2412.01718.jpg', 'data': {'categories': ['#benchmark', '#3d', '#video'], 'emoji': '🚗', 'ru': {'title': 'HUGSIM: Фотореалистичная симуляция для прорыва в автономном вождении', 'desc': 'HUGSIM - это новый симулятор для оценки алгоритмов автономного вождения. Он использует 3D Gaussian Splatting для создания фотореалистичной среды на основе 2D изображений. HUGSIM решает проблемы синтеза новых ракурсов и обеспечивает полностью замкнутый цикл симуляции. Симулятор предоставляет обширный набор тестовых сценариев из различных датасетов для объективной оценки алгоритмов автономного вождения.'}, 'en': {'title': 'HUGSIM: A Holistic Simulator for Evaluating Autonomous Driving Algorithms', 'desc': 'This paper introduces HUGSIM, a new simulator designed to evaluate autonomous driving algorithms in a more comprehensive way. Unlike traditional methods that assess individual components, HUGSIM provides a closed-loop, photo-realistic environment that simulates real-time interactions between vehicles and their surroundings. It utilizes advanced techniques like 3D Gaussian Splatting to enhance rendering quality and supports dynamic updates of vehicle states based on control commands. With a benchmark that includes over 70 sequences and 400 scenarios, HUGSIM offers a robust platform for testing and fine-tuning autonomous driving systems.'}, 'zh': {'title': 'HUGSIM：自动驾驶算法的全面评估新工具', 'desc': '在过去的几十年中，自动驾驶算法在感知、规划和控制方面取得了显著进展。然而，仅仅评估单个组件并不能全面反映整个系统的性能，这突显了更全面评估方法的必要性。为此，本文开发了HUGSIM，这是一个闭环、照片级真实感、实时的模拟器，用于评估自动驾驶算法。HUGSIM通过将捕获的2D RGB图像提升到3D空间，改善了闭环场景的渲染质量，并提供了一个全面的基准测试平台。'}}}, {'id': 'https://huggingface.co/papers/2411.19415', 'title': 'AMO Sampler: Enhancing Text Rendering with Overshooting', 'url': 'https://huggingface.co/papers/2411.19415', 'abstract': 'Achieving precise alignment between textual instructions and generated images in text-to-image generation is a significant challenge, particularly in rendering written text within images. Sate-of-the-art models like Stable Diffusion 3 (SD3), Flux, and AuraFlow still struggle with accurate text depiction, resulting in misspelled or inconsistent text. We introduce a training-free method with minimal computational overhead that significantly enhances text rendering quality. Specifically, we introduce an overshooting sampler for pretrained rectified flow (RF) models, by alternating between over-simulating the learned ordinary differential equation (ODE) and reintroducing noise. Compared to the Euler sampler, the overshooting sampler effectively introduces an extra Langevin dynamics term that can help correct the compounding error from successive Euler steps and therefore improve the text rendering. However, when the overshooting strength is high, we observe over-smoothing artifacts on the generated images. To address this issue, we propose an Attention Modulated Overshooting sampler (AMO), which adaptively controls the strength of overshooting for each image patch according to their attention score with the text content. AMO demonstrates a 32.3% and 35.9% improvement in text rendering accuracy on SD3 and Flux without compromising overall image quality or increasing inference cost.', 'score': 0, 'issue_id': 931, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 ноября', 'en': 'November 28', 'zh': '11月28日'}, 'hash': '54efd1b20c6a4457', 'authors': ['Xixi Hu', 'Keyang Xu', 'Bo Liu', 'Qiang Liu', 'Hongliang Fei'], 'affiliations': ['Google', 'University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2411.19415.jpg', 'data': {'categories': ['#cv', '#optimization', '#training', '#alignment'], 'emoji': '🔤', 'ru': {'title': 'Точный текст на изображениях: новый метод улучшает генерацию без переобучения', 'desc': 'Исследователи представили метод улучшения генерации текста на изображениях в моделях text-to-image без дополнительного обучения. Они разработали сэмплер с перерегулированием для предобученных моделей выпрямленного потока, чередующий избыточное моделирование ОДУ и добавление шума. Для решения проблемы чрезмерного сглаживания при сильном перерегулировании был предложен сэмплер с модуляцией внимания (AMO). AMO продемонстрировал значительное улучшение точности рендеринга текста в моделях SD3 и Flux без ухудшения общего качества изображений.'}, 'en': {'title': 'Enhancing Text Rendering in Images with Adaptive Overshooting', 'desc': 'This paper addresses the challenge of accurately rendering text in images generated from textual instructions, a common issue in text-to-image generation models. The authors propose a novel training-free method that enhances text quality by using an overshooting sampler with pretrained rectified flow models. This method improves text rendering by correcting errors from previous steps through an additional Langevin dynamics term. To prevent over-smoothing artifacts, they introduce the Attention Modulated Overshooting sampler, which adjusts the overshooting strength based on the relevance of each image patch to the text, achieving significant improvements in text accuracy without degrading image quality.'}, 'zh': {'title': '提升文本渲染质量的新方法', 'desc': '在文本到图像生成中，实现文本指令与生成图像之间的精确对齐是一个重大挑战，尤其是在图像中渲染书面文本方面。当前的先进模型如Stable Diffusion 3（SD3）、Flux和AuraFlow在准确描绘文本时仍然存在困难，导致文本拼写错误或不一致。我们提出了一种无训练的方法，具有最小的计算开销，显著提高了文本渲染质量。具体来说，我们引入了一种超调采样器，通过在过度模拟学习的常微分方程（ODE）和重新引入噪声之间交替，来改善文本渲染效果。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (2)', '#agi (1)', '#alignment (1)', '#architecture (8)', '#audio (3)', '#benchmark (15)', '#cv (7)', '#data (5)', '#dataset (15)', '#diffusion (7)', '#ethics (1)', '#games (4)', '#graphs', '#hallucinations', '#healthcare', '#inference (2)', '#interpretability (4)', '#leakage (1)', '#long_context (3)', '#low_resource (2)', '#machine_translation', '#math', '#multilingual (2)', '#multimodal (9)', '#open_source (7)', '#optimization (12)', '#plp', '#rag', '#reasoning (5)', '#rl (1)', '#rlhf', '#robotics', '#science', '#security', '#small_models (2)', '#story_generation', '#survey', '#synthetic (5)', '#training (11)', '#transfer_learning (1)', '#video (11)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-12-04 03:29',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-12-04 03:29')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-12-04 03:29')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    