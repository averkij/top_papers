
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 14 papers. December 12.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">12 декабря</span> | <span id="title-articles-count">14 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2024-12-11.html">⬅️ <span id="prev-date">11.12</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-12-13.html">➡️ <span id="next-date">13.12</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-12.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '12 декабря', 'en': 'December 12', 'zh': '12月12日'};
        let feedDateNext = {'ru': '13.12', 'en': '12/13', 'zh': '12月13日'};
        let feedDatePrev = {'ru': '11.12', 'en': '12/11', 'zh': '12月11日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2412.07760', 'title': 'SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints', 'url': 'https://huggingface.co/papers/2412.07760', 'abstract': 'Recent advancements in video diffusion models have shown exceptional abilities in simulating real-world dynamics and maintaining 3D consistency. This progress inspires us to investigate the potential of these models to ensure dynamic consistency across various viewpoints, a highly desirable feature for applications such as virtual filming. Unlike existing methods focused on multi-view generation of single objects for 4D reconstruction, our interest lies in generating open-world videos from arbitrary viewpoints, incorporating 6 DoF camera poses. To achieve this, we propose a plug-and-play module that enhances a pre-trained text-to-video model for multi-camera video generation, ensuring consistent content across different viewpoints. Specifically, we introduce a multi-view synchronization module to maintain appearance and geometry consistency across these viewpoints. Given the scarcity of high-quality training data, we design a hybrid training scheme that leverages multi-camera images and monocular videos to supplement Unreal Engine-rendered multi-camera videos. Furthermore, our method enables intriguing extensions, such as re-rendering a video from novel viewpoints. We also release a multi-view synchronized video dataset, named SynCamVideo-Dataset. Project page: https://jianhongbai.github.io/SynCamMaster/.', 'score': 32, 'issue_id': 1081, 'pub_date': '2024-12-10', 'pub_date_card': {'ru': '10 декабря', 'en': 'December 10', 'zh': '12月10日'}, 'hash': '5ac69027d8ae0669', 'authors': ['Jianhong Bai', 'Menghan Xia', 'Xintao Wang', 'Ziyang Yuan', 'Xiao Fu', 'Zuozhu Liu', 'Haoji Hu', 'Pengfei Wan', 'Di Zhang'], 'affiliations': ['CUHK', 'Kuaishou Technology', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2412.07760.jpg', 'data': {'categories': ['#diffusion', '#dataset', '#3d', '#open_source', '#video'], 'emoji': '🎥', 'ru': {'title': 'Согласованная генерация видео с множества ракурсов', 'desc': 'Статья представляет новый подход к генерации мультиракурсных видео с использованием диффузионных моделей. Авторы разработали модуль, который улучшает предобученную модель text-to-video для создания согласованного контента с разных точек обзора. Они применяют гибридную схему обучения, используя мультиракурсные изображения и монокулярные видео в дополнение к рендерам из Unreal Engine. Метод также позволяет перерендерить видео с новых ракурсов и включает выпуск нового датасета SynCamVideo-Dataset.'}, 'en': {'title': 'Dynamic Consistency in Multi-View Video Generation', 'desc': "This paper explores the use of video diffusion models to create videos that maintain dynamic consistency from multiple viewpoints, which is important for applications like virtual filming. The authors propose a new module that enhances existing text-to-video models, allowing them to generate videos that are consistent in appearance and geometry across different camera angles. They introduce a multi-view synchronization module to ensure that the content remains coherent, even when viewed from various perspectives. Additionally, they present a hybrid training approach that combines different types of video data to improve the model's performance and release a new dataset for multi-view synchronized videos."}, 'zh': {'title': '实现多视角视频的一致性', 'desc': '最近视频扩散模型的进展显示出在模拟现实世界动态和保持三维一致性方面的卓越能力。我们研究这些模型在不同视角下确保动态一致性的潜力，这对于虚拟拍摄等应用非常重要。与现有方法不同，我们关注的是从任意视角生成开放世界视频，并引入六自由度相机姿态。为此，我们提出了一个可插拔模块，增强了预训练的文本到视频模型，以实现多相机视频生成，并确保不同视角下内容的一致性。'}}}, {'id': 'https://huggingface.co/papers/2412.08580', 'title': 'LAION-SG: An Enhanced Large-Scale Dataset for Training Complex Image-Text Models with Structural Annotations', 'url': 'https://huggingface.co/papers/2412.08580', 'abstract': 'Recent advances in text-to-image (T2I) generation have shown remarkable success in producing high-quality images from text. However, existing T2I models show decayed performance in compositional image generation involving multiple objects and intricate relationships. We attribute this problem to limitations in existing datasets of image-text pairs, which lack precise inter-object relationship annotations with prompts only. To address this problem, we construct LAION-SG, a large-scale dataset with high-quality structural annotations of scene graphs (SG), which precisely describe attributes and relationships of multiple objects, effectively representing the semantic structure in complex scenes. Based on LAION-SG, we train a new foundation model SDXL-SG to incorporate structural annotation information into the generation process. Extensive experiments show advanced models trained on our LAION-SG boast significant performance improvements in complex scene generation over models on existing datasets. We also introduce CompSG-Bench, a benchmark that evaluates models on compositional image generation, establishing a new standard for this domain.', 'score': 24, 'issue_id': 1081, 'pub_date': '2024-12-11', 'pub_date_card': {'ru': '11 декабря', 'en': 'December 11', 'zh': '12月11日'}, 'hash': '07b05e5ae44a52c7', 'authors': ['Zejian Li', 'Chenye Meng', 'Yize Li', 'Ling Yang', 'Shengyuan Zhang', 'Jiarui Ma', 'Jiayi Li', 'Guang Yang', 'Changyuan Yang', 'Zhiyuan Yang', 'Jinxiong Chang', 'Lingyun Sun'], 'affiliations': ['Alibaba Group', 'Ant Group', 'Jiangnan University', 'Peking University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2412.08580.jpg', 'data': {'categories': ['#dataset', '#cv', '#synthetic', '#benchmark', '#games'], 'emoji': '🖼️', 'ru': {'title': 'Новый уровень генерации сложных сцен с помощью графов', 'desc': 'Исследователи представили новый подход к генерации изображений по тексту, который улучшает композиционную генерацию сложных сцен с несколькими объектами. Они создали датасет LAION-SG с высококачественными структурными аннотациями в виде графов сцен. На основе этого датасета была обучена новая фундаментальная модель SDXL-SG, которая демонстрирует значительное улучшение в генерации сложных сцен по сравнению с существующими моделями. Также авторы представили бенчмарк CompSG-Bench для оценки моделей в задаче композиционной генерации изображений.'}, 'en': {'title': 'Enhancing Text-to-Image Generation with Structured Scene Graphs', 'desc': 'This paper addresses the challenges faced by text-to-image (T2I) models in generating complex images with multiple objects and their relationships. The authors identify that existing datasets lack detailed annotations for inter-object relationships, which hampers model performance. To overcome this, they introduce LAION-SG, a new dataset that includes comprehensive scene graph annotations, enhancing the understanding of object attributes and relationships. They also present a new model, SDXL-SG, trained on this dataset, which shows significant improvements in generating intricate scenes, along with a new benchmark, CompSG-Bench, for evaluating compositional image generation.'}, 'zh': {'title': '构建高质量数据集，提升图像生成能力', 'desc': '最近在文本到图像生成（T2I）方面取得了显著进展，能够从文本生成高质量图像。然而，现有的T2I模型在生成包含多个对象和复杂关系的图像时表现不佳。我们认为这个问题源于现有图像-文本对数据集的局限性，这些数据集缺乏精确的对象间关系注释。为了解决这个问题，我们构建了LAION-SG，这是一个具有高质量结构注释的大规模数据集，能够有效表示复杂场景中的语义结构，并基于此训练了新的基础模型SDXL-SG。'}}}, {'id': 'https://huggingface.co/papers/2412.08443', 'title': 'POINTS1.5: Building a Vision-Language Model towards Real World Applications', 'url': 'https://huggingface.co/papers/2412.08443', 'abstract': 'Vision-language models have made significant strides recently, demonstrating superior performance across a range of tasks, e.g. optical character recognition and complex diagram analysis. Building on this trend, we introduce a new vision-language model, POINTS1.5, designed to excel in various real-world applications. POINTS1.5 is an enhancement of POINTS1.0 and incorporates several key innovations: i) We replace the original CLIP vision encoder, which had a fixed image resolution, with a NaViT-style vision encoder that supports native dynamic high resolution. This allows POINTS1.5 to process images of any resolution without needing to split them into tiles. ii) We add bilingual support to POINTS1.5, significantly enhancing its capability in Chinese. Due to the scarcity of open-source Chinese datasets for vision-language models, we collect numerous images from the Internet and annotate them using a combination of manual and automatic methods. iii) We propose a set of rigorous filtering methods for visual instruction tuning datasets. We comprehensively evaluate all these filtering methods, and choose the most effective ones to obtain the final visual instruction tuning set. Thanks to these innovations, POINTS1.5 significantly outperforms POINTS1.0 and demonstrates strong performance across a range of real-world applications. Notably, POINTS1.5-7B is trained on fewer than 4 billion tokens and ranks first on the OpenCompass leaderboard among models with fewer than 10 billion parameters', 'score': 23, 'issue_id': 1083, 'pub_date': '2024-12-11', 'pub_date_card': {'ru': '11 декабря', 'en': 'December 11', 'zh': '12月11日'}, 'hash': '02dbe9638e613a10', 'authors': ['Yuan Liu', 'Le Tian', 'Xiao Zhou', 'Xinyu Gao', 'Kavio Yu', 'Yang Yu', 'Jie Zhou'], 'affiliations': ['Pattern Recognition Center, WeChat AI, Tencent Inc, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.08443.jpg', 'data': {'categories': ['#dataset', '#cv', '#low_resource', '#architecture', '#data', '#multilingual', '#training', '#open_source'], 'emoji': '🔍', 'ru': {'title': 'POINTS1.5: Новый уровень мультимодального искусственного интеллекта', 'desc': 'Статья представляет новую мультимодальную модель POINTS1.5, улучшенную версию POINTS1.0. Ключевые инновации включают замену энкодера изображений на NaViT для поддержки динамического высокого разрешения, добавление двуязычной поддержки (английский и китайский) и применение строгих методов фильтрации для наборов данных визуального обучения. Модель POINTS1.5 демонстрирует превосходную производительность в различных реальных приложениях, несмотря на обучение на менее чем 4 миллиардах токенов. POINTS1.5-7B занимает первое место в рейтинге OpenCompass среди моделей с менее чем 10 миллиардами параметров.'}, 'en': {'title': 'POINTS1.5: Elevating Vision-Language Models with Dynamic Resolution and Bilingual Support', 'desc': 'The paper presents POINTS1.5, an advanced vision-language model that improves upon its predecessor, POINTS1.0. It features a NaViT-style vision encoder that allows for dynamic high-resolution image processing, eliminating the need for image tiling. Additionally, POINTS1.5 introduces bilingual support, particularly enhancing its performance in Chinese by utilizing a newly curated dataset. The model also employs rigorous filtering methods for visual instruction tuning datasets, leading to superior performance in various real-world applications and achieving top rankings in benchmark evaluations.'}, 'zh': {'title': 'POINTS1.5：视觉语言模型的新突破', 'desc': '本文介绍了一种新的视觉语言模型POINTS1.5，该模型在多个实际应用中表现优异。与之前的版本POINTS1.0相比，POINTS1.5进行了多项重要改进，包括使用支持动态高分辨率的NaViT风格视觉编码器，能够处理任意分辨率的图像。该模型还增加了对中文的双语支持，通过收集和注释大量图像来提升其中文能力。此外，本文提出了一套严格的视觉指令调优数据集过滤方法，确保最终数据集的有效性。'}}}, {'id': 'https://huggingface.co/papers/2412.08486', 'title': 'Learning Flow Fields in Attention for Controllable Person Image Generation', 'url': 'https://huggingface.co/papers/2412.08486', 'abstract': "Controllable person image generation aims to generate a person image conditioned on reference images, allowing precise control over the person's appearance or pose. However, prior methods often distort fine-grained textural details from the reference image, despite achieving high overall image quality. We attribute these distortions to inadequate attention to corresponding regions in the reference image. To address this, we thereby propose learning flow fields in attention (Leffa), which explicitly guides the target query to attend to the correct reference key in the attention layer during training. Specifically, it is realized via a regularization loss on top of the attention map within a diffusion-based baseline. Our extensive experiments show that Leffa achieves state-of-the-art performance in controlling appearance (virtual try-on) and pose (pose transfer), significantly reducing fine-grained detail distortion while maintaining high image quality. Additionally, we show that our loss is model-agnostic and can be used to improve the performance of other diffusion models.", 'score': 13, 'issue_id': 1086, 'pub_date': '2024-12-11', 'pub_date_card': {'ru': '11 декабря', 'en': 'December 11', 'zh': '12月11日'}, 'hash': 'ff329acbd2056afe', 'authors': ['Zijian Zhou', 'Shikun Liu', 'Xiao Han', 'Haozhe Liu', 'Kam Woh Ng', 'Tian Xie', 'Yuren Cong', 'Hang Li', 'Mengmeng Xu', 'Juan-Manuel Pérez-Rúa', 'Aditya Patel', 'Tao Xiang', 'Miaojing Shi', 'Sen He'], 'affiliations': ['Kings College London', 'Meta AI', 'Tongji University'], 'pdf_title_img': 'assets/pdf/title_img/2412.08486.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#training', '#cv'], 'emoji': '👤', 'ru': {'title': 'Точный контроль деталей при генерации изображений людей', 'desc': 'Статья представляет новый метод под названием Leffa для улучшения контролируемой генерации изображений людей. Метод использует обучение полей потока в слоях внимания, чтобы точнее передавать детали из референсного изображения. Leffa реализуется через регуляризационную функцию потерь поверх карты внимания в базовой модели диффузии. Эксперименты показывают, что Leffa достигает современного уровня производительности в контроле внешнего вида и позы, значительно уменьшая искажения мелких деталей.'}, 'en': {'title': 'Enhancing Image Generation with Targeted Attention', 'desc': 'This paper presents a method called Learning Flow Fields in Attention (Leffa) for controllable person image generation. The goal is to create images of people that accurately reflect the appearance and pose of reference images without losing important details. Previous methods struggled with distorting fine textures, which this approach aims to fix by improving how the model focuses on specific areas of the reference image. The authors demonstrate that Leffa not only enhances the quality of generated images but is also adaptable to other diffusion models, making it a versatile solution in the field.'}, 'zh': {'title': '精确控制人物图像生成的关键', 'desc': '可控的人物图像生成旨在根据参考图像生成特定外观或姿势的人物图像。以往的方法虽然在整体图像质量上表现良好，但常常会扭曲参考图像中的细节纹理。我们认为这种扭曲是由于对参考图像中相应区域关注不足造成的。为了解决这个问题，我们提出了一种在注意力机制中学习流场的方法（Leffa），通过在训练过程中引导目标查询关注正确的参考关键点，从而显著减少细节扭曲，同时保持高图像质量。'}}}, {'id': 'https://huggingface.co/papers/2412.07744', 'title': 'StyleMaster: Stylize Your Video with Artistic Generation and Translation', 'url': 'https://huggingface.co/papers/2412.07744', 'abstract': 'Style control has been popular in video generation models. Existing methods often generate videos far from the given style, cause content leakage, and struggle to transfer one video to the desired style. Our first observation is that the style extraction stage matters, whereas existing methods emphasize global style but ignore local textures. In order to bring texture features while preventing content leakage, we filter content-related patches while retaining style ones based on prompt-patch similarity; for global style extraction, we generate a paired style dataset through model illusion to facilitate contrastive learning, which greatly enhances the absolute style consistency. Moreover, to fill in the image-to-video gap, we train a lightweight motion adapter on still videos, which implicitly enhances stylization extent, and enables our image-trained model to be seamlessly applied to videos. Benefited from these efforts, our approach, StyleMaster, not only achieves significant improvement in both style resemblance and temporal coherence, but also can easily generalize to video style transfer with a gray tile ControlNet. Extensive experiments and visualizations demonstrate that StyleMaster significantly outperforms competitors, effectively generating high-quality stylized videos that align with textual content and closely resemble the style of reference images. Our project page is at https://zixuan-ye.github.io/stylemaster', 'score': 12, 'issue_id': 1084, 'pub_date': '2024-12-10', 'pub_date_card': {'ru': '10 декабря', 'en': 'December 10', 'zh': '12月10日'}, 'hash': '1eac9c28026939fc', 'authors': ['Zixuan Ye', 'Huijuan Huang', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Wenhan Luo'], 'affiliations': ['Hong Kong University of Science and Technology', 'KuaiShou Technology'], 'pdf_title_img': 'assets/pdf/title_img/2412.07744.jpg', 'data': {'categories': ['#optimization', '#style_transfer', '#video', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'StyleMaster: Совершенствование стилевого контроля в генерации видео', 'desc': 'StyleMaster - это новый подход к стилевому контролю в генерации видео. Метод улучшает извлечение стиля, используя фильтрацию патчей на основе сходства с промптом и контрастное обучение на парных стилевых данных. Легковесный адаптер движения позволяет применять модель, обученную на изображениях, к видео. StyleMaster значительно превосходит конкурентов по соответствию стилю и временной согласованности генерируемых видео.'}, 'en': {'title': 'StyleMaster: Elevating Video Style Transfer with Texture and Consistency', 'desc': 'This paper presents StyleMaster, a novel approach to video style transfer that addresses common issues in existing methods, such as content leakage and poor style adherence. The authors emphasize the importance of local texture features in addition to global style, proposing a method that filters content-related patches while preserving style-related ones. They introduce a paired style dataset for contrastive learning to enhance style consistency and develop a lightweight motion adapter to bridge the gap between image and video stylization. Extensive experiments show that StyleMaster significantly improves style resemblance and temporal coherence, outperforming existing models in generating high-quality stylized videos.'}, 'zh': {'title': 'StyleMaster：提升视频风格转移的创新方法', 'desc': '本论文介绍了一种名为StyleMaster的视频生成模型，旨在改善视频的风格转移效果。我们发现，现有方法在风格提取阶段往往忽视局部纹理，导致生成的视频与目标风格不符。为了解决这个问题，我们通过过滤内容相关的图像块来保留风格特征，并利用对比学习增强全局风格的一致性。此外，我们还训练了一个轻量级的运动适配器，使得图像训练的模型能够无缝应用于视频生成。'}}}, {'id': 'https://huggingface.co/papers/2412.08646', 'title': 'StreamChat: Chatting with Streaming Video', 'url': 'https://huggingface.co/papers/2412.08646', 'abstract': 'This paper presents StreamChat, a novel approach that enhances the interaction capabilities of Large Multimodal Models (LMMs) with streaming video content. In streaming interaction scenarios, existing methods rely solely on visual information available at the moment a question is posed, resulting in significant delays as the model remains unaware of subsequent changes in the streaming video. StreamChat addresses this limitation by innovatively updating the visual context at each decoding step, ensuring that the model utilizes up-to-date video content throughout the decoding process. Additionally, we introduce a flexible and efficient crossattention-based architecture to process dynamic streaming inputs while maintaining inference efficiency for streaming interactions. Furthermore, we construct a new dense instruction dataset to facilitate the training of streaming interaction models, complemented by a parallel 3D-RoPE mechanism that encodes the relative temporal information of visual and text tokens. Experimental results demonstrate that StreamChat achieves competitive performance on established image and video benchmarks and exhibits superior capabilities in streaming interaction scenarios compared to state-of-the-art video LMM.', 'score': 9, 'issue_id': 1086, 'pub_date': '2024-12-11', 'pub_date_card': {'ru': '11 декабря', 'en': 'December 11', 'zh': '12月11日'}, 'hash': '6d48f15bab7c3545', 'authors': ['Jihao Liu', 'Zhiding Yu', 'Shiyi Lan', 'Shihao Wang', 'Rongyao Fang', 'Jan Kautz', 'Hongsheng Li', 'Jose M. Alvare'], 'affiliations': ['CPII under InnoHK', 'CUHK MMLab', 'NVIDIA', 'Shanghai AI Laboratory', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2412.08646.jpg', 'data': {'categories': ['#video', '#multimodal', '#dataset', '#architecture', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'StreamChat: Революция в потоковом видеовзаимодействии с ИИ', 'desc': 'Статья представляет StreamChat - новый подход к улучшению взаимодействия больших мультимодальных моделей (LMM) с потоковым видео. StreamChat обновляет визуальный контекст на каждом шаге декодирования, обеспечивая использование актуального содержания видео. Авторы вводят архитектуру на основе кросс-внимания для обработки динамических потоковых входных данных и создают новый набор данных для обучения моделей потокового взаимодействия. Экспериментальные результаты показывают превосходство StreamChat в сценариях потокового взаимодействия по сравнению с современными видео LMM.'}, 'en': {'title': 'StreamChat: Real-Time Interaction with Streaming Video', 'desc': 'This paper introduces StreamChat, a new method that improves how Large Multimodal Models (LMMs) interact with live video. Traditional approaches only use the visual information available at the time a question is asked, which can cause delays as they miss changes in the video. StreamChat solves this by updating the visual context continuously during the decoding process, allowing the model to respond with the most current video content. It also features a cross-attention architecture for efficient processing of dynamic inputs and a new dataset for training, leading to better performance in streaming interactions compared to existing models.'}, 'zh': {'title': 'StreamChat：实时视频交互的新突破', 'desc': '本文提出了一种新方法StreamChat，旨在增强大型多模态模型（LMM）与流媒体视频内容的交互能力。在流媒体交互场景中，现有方法仅依赖于提问时可用的视觉信息，导致模型无法及时获取视频中的后续变化，从而产生显著延迟。StreamChat通过在每个解码步骤中创新性地更新视觉上下文，确保模型在解码过程中利用最新的视频内容。此外，我们引入了一种灵活高效的基于交叉注意力的架构，以处理动态流媒体输入，同时保持流媒体交互的推理效率。'}}}, {'id': 'https://huggingface.co/papers/2412.06234', 'title': 'Generative Densification: Learning to Densify Gaussians for High-Fidelity Generalizable 3D Reconstruction', 'url': 'https://huggingface.co/papers/2412.06234', 'abstract': 'Generalized feed-forward Gaussian models have achieved significant progress in sparse-view 3D reconstruction by leveraging prior knowledge from large multi-view datasets. However, these models often struggle to represent high-frequency details due to the limited number of Gaussians. While the densification strategy used in per-scene 3D Gaussian splatting (3D-GS) optimization can be adapted to the feed-forward models, it may not be ideally suited for generalized scenarios. In this paper, we propose Generative Densification, an efficient and generalizable method to densify Gaussians generated by feed-forward models. Unlike the 3D-GS densification strategy, which iteratively splits and clones raw Gaussian parameters, our method up-samples feature representations from the feed-forward models and generates their corresponding fine Gaussians in a single forward pass, leveraging the embedded prior knowledge for enhanced generalization. Experimental results on both object-level and scene-level reconstruction tasks demonstrate that our method outperforms state-of-the-art approaches with comparable or smaller model sizes, achieving notable improvements in representing fine details.', 'score': 9, 'issue_id': 1083, 'pub_date': '2024-12-09', 'pub_date_card': {'ru': '9 декабря', 'en': 'December 9', 'zh': '12月9日'}, 'hash': '1da969b562c1f280', 'authors': ['Seungtae Nam', 'Xiangyu Sun', 'Gyeongjin Kang', 'Younggeun Lee', 'Seungjun Oh', 'Eunbyung Park'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2412.06234.jpg', 'data': {'categories': ['#training', '#3d', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Генеративное уплотнение: новый шаг в 3D-реконструкции с высоким разрешением', 'desc': "Статья представляет новый метод под названием 'Генеративное уплотнение' для улучшения реконструкции 3D-объектов при ограниченном количестве ракурсов. Этот подход позволяет эффективно уплотнять гауссовы распределения, генерируемые моделями прямого распространения, улучшая детализацию реконструкции. В отличие от существующих методов, 'Генеративное уплотнение' использует встроенные априорные знания для повышения обобщающей способности. Экспериментальные результаты показывают, что предложенный метод превосходит современные подходы как для объектов, так и для сцен."}, 'en': {'title': 'Enhancing 3D Reconstruction with Generative Densification', 'desc': 'This paper introduces Generative Densification, a novel method aimed at improving the representation of high-frequency details in sparse-view 3D reconstruction using feed-forward Gaussian models. Traditional methods struggle with detail due to a limited number of Gaussians, but our approach efficiently up-samples feature representations in a single forward pass. By leveraging prior knowledge from large datasets, this method enhances generalization and performance in both object-level and scene-level tasks. Experimental results show that Generative Densification outperforms existing techniques while maintaining comparable or smaller model sizes.'}, 'zh': {'title': '生成稠密化：提升3D重建细节的高效方法', 'desc': '本文提出了一种名为生成稠密化的高效方法，用于增强前馈高斯模型在稀疏视图3D重建中的表现。与传统的3D高斯稠密化策略不同，我们的方法通过单次前向传播从前馈模型中上采样特征表示，生成相应的细节高斯。该方法利用嵌入的先验知识，提升了模型的泛化能力，能够更好地表示高频细节。实验结果表明，我们的方法在物体级和场景级重建任务中均优于现有的最先进方法，且模型规模相对较小。'}}}, {'id': 'https://huggingface.co/papers/2412.07825', 'title': '3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark', 'url': 'https://huggingface.co/papers/2412.07825', 'abstract': '3D spatial reasoning is the ability to analyze and interpret the positions, orientations, and spatial relationships of objects within the 3D space. This allows models to develop a comprehensive understanding of the 3D scene, enabling their applicability to a broader range of areas, such as autonomous navigation, robotics, and AR/VR. While large multi-modal models (LMMs) have achieved remarkable progress in a wide range of image and video understanding tasks, their capabilities to perform 3D spatial reasoning on diverse natural images are less studied. In this work we present the first comprehensive 3D spatial reasoning benchmark, 3DSRBench, with 2,772 manually annotated visual question-answer pairs across 12 question types. We conduct robust and thorough evaluation of 3D spatial reasoning capabilities by balancing the data distribution and adopting a novel FlipEval strategy. To further study the robustness of 3D spatial reasoning w.r.t. camera 3D viewpoints, our 3DSRBench includes two subsets with 3D spatial reasoning questions on paired images with common and uncommon viewpoints. We benchmark a wide range of open-sourced and proprietary LMMs, uncovering their limitations in various aspects of 3D awareness, such as height, orientation, location, and multi-object reasoning, as well as their degraded performance on images with uncommon camera viewpoints. Our 3DSRBench provide valuable findings and insights about the future development of LMMs with strong 3D reasoning capabilities. Our project page and dataset is available https://3dsrbench.github.io.', 'score': 8, 'issue_id': 1082, 'pub_date': '2024-12-10', 'pub_date_card': {'ru': '10 декабря', 'en': 'December 10', 'zh': '12月10日'}, 'hash': '91db0c60d08d4efd', 'authors': ['Wufei Ma', 'Haoyu Chen', 'Guofeng Zhang', 'Celso M de Melo', 'Alan Yuille', 'Jieneng Chen'], 'affiliations': ['Carnegie Mellon University', 'DEVCOM Army Research Laboratory', 'Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2412.07825.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#3d', '#reasoning'], 'emoji': '🧠', 'ru': {'title': '3DSRBench: новый стандарт для оценки 3D пространственного мышления у LMM', 'desc': 'Эта статья представляет первый комплексный benchmark для оценки способностей больших мультимодальных моделей (LMM) к 3D пространственному мышлению на разнообразных естественных изображениях. Авторы создали датасет 3DSRBench, содержащий 2772 вручную размеченных пар вопрос-ответ по 12 типам вопросов, связанных с 3D пространственным мышлением. Исследование включает оценку робастности моделей к различным ракурсам камеры и использует стратегию FlipEval для тщательного тестирования. Результаты показывают ограничения существующих LMM в различных аспектах 3D восприятия, таких как высота, ориентация, расположение объектов и рассуждения о нескольких объектах.'}, 'en': {'title': 'Enhancing 3D Spatial Reasoning in AI Models', 'desc': 'This paper introduces 3DSRBench, a new benchmark designed to evaluate 3D spatial reasoning in large multi-modal models (LMMs). It includes 2,772 annotated visual question-answer pairs that cover various question types related to spatial relationships in 3D environments. The study highlights the limitations of current LMMs in understanding aspects like height, orientation, and location, especially when dealing with images taken from uncommon viewpoints. By providing a structured evaluation framework, this work aims to enhance the development of models with improved 3D reasoning capabilities.'}, 'zh': {'title': '推动3D空间推理的未来发展', 'desc': '3D空间推理是分析和理解三维空间中物体位置、方向和空间关系的能力。本文提出了第一个全面的3D空间推理基准，3DSRBench，包含2772个手动标注的视觉问答对，涵盖12种问题类型。我们通过平衡数据分布和采用新颖的FlipEval策略，对3D空间推理能力进行了全面评估。研究结果揭示了现有大型多模态模型在3D意识方面的局限性，并为未来的模型发展提供了重要见解。'}}}, {'id': 'https://huggingface.co/papers/2412.07797', 'title': 'Mogo: RQ Hierarchical Causal Transformer for High-Quality 3D Human Motion Generation', 'url': 'https://huggingface.co/papers/2412.07797', 'abstract': 'In the field of text-to-motion generation, Bert-type Masked Models (MoMask, MMM) currently produce higher-quality outputs compared to GPT-type autoregressive models (T2M-GPT). However, these Bert-type models often lack the streaming output capability required for applications in video game and multimedia environments, a feature inherent to GPT-type models. Additionally, they demonstrate weaker performance in out-of-distribution generation. To surpass the quality of BERT-type models while leveraging a GPT-type structure, without adding extra refinement models that complicate scaling data, we propose a novel architecture, Mogo (Motion Only Generate Once), which generates high-quality lifelike 3D human motions by training a single transformer model. Mogo consists of only two main components: 1) RVQ-VAE, a hierarchical residual vector quantization variational autoencoder, which discretizes continuous motion sequences with high precision; 2) Hierarchical Causal Transformer, responsible for generating the base motion sequences in an autoregressive manner while simultaneously inferring residuals across different layers. Experimental results demonstrate that Mogo can generate continuous and cyclic motion sequences up to 260 frames (13 seconds), surpassing the 196 frames (10 seconds) length limitation of existing datasets like HumanML3D. On the HumanML3D test set, Mogo achieves a FID score of 0.079, outperforming both the GPT-type model T2M-GPT (FID = 0.116), AttT2M (FID = 0.112) and the BERT-type model MMM (FID = 0.080). Furthermore, our model achieves the best quantitative performance in out-of-distribution generation.', 'score': 7, 'issue_id': 1080, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 декабря', 'en': 'December 5', 'zh': '12月5日'}, 'hash': 'ff9bb8b603f9d972', 'authors': ['Dongjie Fu'], 'affiliations': ['Mogo AI'], 'pdf_title_img': 'assets/pdf/title_img/2412.07797.jpg', 'data': {'categories': ['#optimization', '#3d', '#games', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Mogo: Революция в генерации движений из текста', 'desc': 'Статья представляет новую архитектуру Mogo для генерации высококачественных трехмерных движений человека на основе текста. Mogo использует RVQ-VAE для дискретизации непрерывных последовательностей движений и иерархический причинный трансформер для генерации базовых последовательностей движений. Эксперименты показывают, что Mogo превосходит существующие модели по качеству генерации, включая GPT-подобные и BERT-подобные модели. Модель также демонстрирует лучшие результаты при генерации вне распределения обучающих данных.'}, 'en': {'title': 'Mogo: Revolutionizing Text-to-Motion with High-Quality 3D Generation', 'desc': 'This paper introduces Mogo, a new architecture for generating high-quality 3D human motions from text. Mogo combines a hierarchical residual vector quantization variational autoencoder (RVQ-VAE) with a hierarchical causal transformer to produce continuous and cyclic motion sequences efficiently. Unlike existing Bert-type models, Mogo maintains the streaming output capability of GPT-type models while improving performance in out-of-distribution scenarios. Experimental results show that Mogo not only generates longer motion sequences but also achieves superior quality metrics compared to both GPT-type and Bert-type models.'}, 'zh': {'title': 'Mogo：高效生成高质量3D人类动作的创新架构', 'desc': '在文本到动作生成领域，Bert类型的模型（如MoMask, MMM）虽然输出质量较高，但缺乏流式输出能力，无法满足视频游戏和多媒体环境的需求。相比之下，GPT类型的自回归模型（如T2M-GPT）具备这一特性，但在生成质量上稍逊一筹。为了解决这一问题，我们提出了一种新架构Mogo（Motion Only Generate Once），它通过训练单一的变换器模型生成高质量的3D人类动作。Mogo结合了高精度的层次残差向量量化变分自编码器和层次因果变换器，能够生成连续且循环的动作序列，超越了现有数据集的限制。'}}}, {'id': 'https://huggingface.co/papers/2412.08629', 'title': 'FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow Models', 'url': 'https://huggingface.co/papers/2412.08629', 'abstract': "Editing real images using a pre-trained text-to-image (T2I) diffusion/flow model often involves inverting the image into its corresponding noise map. However, inversion by itself is typically insufficient for obtaining satisfactory results, and therefore many methods additionally intervene in the sampling process. Such methods achieve improved results but are not seamlessly transferable between model architectures. Here, we introduce FlowEdit, a text-based editing method for pre-trained T2I flow models, which is inversion-free, optimization-free and model agnostic. Our method constructs an ODE that directly maps between the source and target distributions (corresponding to the source and target text prompts) and achieves a lower transport cost than the inversion approach. This leads to state-of-the-art results, as we illustrate with Stable Diffusion 3 and FLUX. Code and examples are available on the project's webpage.", 'score': 6, 'issue_id': 1088, 'pub_date': '2024-12-11', 'pub_date_card': {'ru': '11 декабря', 'en': 'December 11', 'zh': '12月11日'}, 'hash': 'b08b9bb78c9561f4', 'authors': ['Vladimir Kulikov', 'Matan Kleiner', 'Inbar Huberman-Spiegelglas', 'Tomer Michaeli'], 'affiliations': ['Technion Israel Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2412.08629.jpg', 'data': {'categories': ['#optimization', '#architecture', '#cv', '#open_source', '#diffusion', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'FlowEdit: эффективное редактирование изображений без инверсии', 'desc': 'Статья представляет FlowEdit - новый метод редактирования изображений с помощью текстовых запросов для предобученных моделей text-to-image. В отличие от существующих подходов, FlowEdit не требует инверсии изображения и оптимизации, а также применим к различным архитектурам моделей. Метод основан на построении ODE, напрямую отображающего распределения исходного и целевого текстовых запросов. FlowEdit достигает лучших результатов по сравнению с методами, основанными на инверсии, что продемонстрировано на моделях Stable Diffusion 3 и FLUX.'}, 'en': {'title': 'Seamless Image Editing with FlowEdit: No Inversion Needed!', 'desc': 'This paper presents FlowEdit, a novel method for editing images using pre-trained text-to-image (T2I) flow models without the need for inversion or optimization. Traditional methods often require converting images into noise maps, which can be inefficient and model-specific. FlowEdit instead utilizes an ordinary differential equation (ODE) to directly connect the source and target distributions based on text prompts, resulting in a more efficient editing process. The approach demonstrates superior performance compared to existing methods, as shown with models like Stable Diffusion 3 and FLUX.'}, 'zh': {'title': '无反演的文本图像编辑新方法', 'desc': '本文介绍了一种名为FlowEdit的文本编辑方法，专为预训练的文本到图像（T2I）流模型设计。与传统的图像反演方法不同，FlowEdit不需要反演和优化，且对模型架构不敏感。该方法通过构建一个常微分方程（ODE），直接在源分布和目标分布之间进行映射，从而降低了传输成本。实验结果表明，FlowEdit在Stable Diffusion 3和FLUX上达到了最先进的效果。'}}}, {'id': 'https://huggingface.co/papers/2412.06071', 'title': 'KaSA: Knowledge-Aware Singular-Value Adaptation of Large Language Models', 'url': 'https://huggingface.co/papers/2412.06071', 'abstract': "The increasing sizes of large language models (LLMs) result in significant computational overhead and memory usage when adapting these models to specific tasks or domains. Various parameter-efficient fine-tuning (PEFT) methods have been devised to mitigate these challenges by training a small set of parameters for the task-specific updates of the model weights. Among PEFT methods, LoRA stands out for its simplicity and efficiency, inspiring the development of a series of variants. However, LoRA and its successors disregard the knowledge that is noisy or irrelevant to the targeted task, detrimentally impacting model performance and leading to suboptimality. To address this limitation, we introduce Knowledge-aware Singular-value Adaptation (KaSA), a PEFT method that leverages singular value decomposition (SVD) with knowledge-aware singular values to dynamically activate knowledge based on its relevance to the task at hand. We conduct extensive experiments across a range of LLMs on tasks spanning natural language understanding (NLU), generation (NLG), instruction following, and commonsense reasoning. The experimental results demonstrate that KaSA consistently outperforms FFT and 14 popular PEFT baselines across 16 benchmarks and 4 synthetic datasets, underscoring our method's efficacy and adaptability. The source code of our method is available at https://github.com/juyongjiang/KaSA.", 'score': 4, 'issue_id': 1089, 'pub_date': '2024-12-08', 'pub_date_card': {'ru': '8 декабря', 'en': 'December 8', 'zh': '12月8日'}, 'hash': 'eec0cadc18c298d0', 'authors': ['Fan Wang', 'Juyong Jiang', 'Chansung Park', 'Sunghun Kim', 'Jing Tang'], 'affiliations': ['Electronics and Telecommunications Research Institute', 'The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2412.06071.jpg', 'data': {'categories': ['#synthetic', '#transfer_learning', '#benchmark', '#optimization', '#dataset', '#training', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'KaSA: Умная настройка языковых моделей с учетом знаний', 'desc': 'В статье представлен новый метод эффективной настройки больших языковых моделей под названием KaSA (Knowledge-aware Singular-value Adaptation). Этот метод использует сингулярное разложение с учетом знаний для динамической активации релевантной информации в зависимости от задачи. KaSA преодолевает ограничения предыдущих методов, таких как LoRA, которые игнорируют шумные или нерелевантные знания. Эксперименты показали, что KaSA превосходит 14 популярных методов PEFT на 16 бенчмарках и 4 синтетических наборах данных в задачах понимания и генерации естественного языка, следования инструкциям и здравого смысла.'}, 'en': {'title': 'Enhancing Task Adaptation in LLMs with Knowledge-aware Singular-value Adaptation', 'desc': 'This paper addresses the challenges of adapting large language models (LLMs) to specific tasks while managing computational and memory costs. It introduces a new parameter-efficient fine-tuning (PEFT) method called Knowledge-aware Singular-value Adaptation (KaSA), which utilizes singular value decomposition (SVD) to focus on relevant knowledge for task performance. Unlike previous methods like LoRA, KaSA dynamically activates knowledge based on its importance, improving model efficiency and effectiveness. Experimental results show that KaSA outperforms existing PEFT techniques across various natural language tasks, demonstrating its superior adaptability and performance.'}, 'zh': {'title': '知识感知微调，提升模型性能！', 'desc': '随着大型语言模型（LLMs）规模的增加，适应特定任务时会导致显著的计算开销和内存使用。为了解决这个问题，提出了多种参数高效微调（PEFT）方法，通过训练少量参数来更新模型权重。本文介绍了一种新的PEFT方法——知识感知奇异值适应（KaSA），它利用奇异值分解（SVD）动态激活与任务相关的知识，从而提高模型性能。实验结果表明，KaSA在多个大型语言模型上表现优于14种流行的PEFT基线，证明了其有效性和适应性。'}}}, {'id': 'https://huggingface.co/papers/2412.08503', 'title': 'StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements', 'url': 'https://huggingface.co/papers/2412.08503', 'abstract': 'Text-driven style transfer aims to merge the style of a reference image with content described by a text prompt. Recent advancements in text-to-image models have improved the nuance of style transformations, yet significant challenges remain, particularly with overfitting to reference styles, limiting stylistic control, and misaligning with textual content. In this paper, we propose three complementary strategies to address these issues. First, we introduce a cross-modal Adaptive Instance Normalization (AdaIN) mechanism for better integration of style and text features, enhancing alignment. Second, we develop a Style-based Classifier-Free Guidance (SCFG) approach that enables selective control over stylistic elements, reducing irrelevant influences. Finally, we incorporate a teacher model during early generation stages to stabilize spatial layouts and mitigate artifacts. Our extensive evaluations demonstrate significant improvements in style transfer quality and alignment with textual prompts. Furthermore, our approach can be integrated into existing style transfer frameworks without fine-tuning.', 'score': 2, 'issue_id': 1088, 'pub_date': '2024-12-11', 'pub_date_card': {'ru': '11 декабря', 'en': 'December 11', 'zh': '12月11日'}, 'hash': '7b217a36b7d6db44', 'authors': ['Mingkun Lei', 'Xue Song', 'Beier Zhu', 'Hao Wang', 'Chi Zhang'], 'affiliations': ['AGI Lab, Westlake University', 'Fudan University', 'Nanyang Technological University', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2412.08503.jpg', 'data': {'categories': ['#cv', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Улучшенный перенос стиля изображений с помощью текстового контроля', 'desc': 'Статья предлагает новый подход к переносу стиля изображения на основе текстового описания. Авторы вводят три стратегии: кросс-модальный механизм AdaIN для лучшей интеграции стиля и текста, подход Style-based Classifier-Free Guidance для селективного контроля стилистических элементов, и использование учительской модели для стабилизации пространственной компоновки. Эксперименты показывают значительные улучшения в качестве переноса стиля и соответствии текстовым промптам. Предложенный метод может быть интегрирован в существующие фреймворки без дополнительного обучения.'}, 'en': {'title': 'Enhancing Text-Driven Style Transfer with Adaptive Techniques', 'desc': 'This paper focuses on improving text-driven style transfer, which combines the style of an image with content from a text description. The authors identify challenges such as overfitting to styles and misalignment with text, and propose three strategies to overcome these issues. They introduce a cross-modal Adaptive Instance Normalization (AdaIN) for better feature integration, a Style-based Classifier-Free Guidance (SCFG) for selective stylistic control, and a teacher model to stabilize outputs. The results show enhanced style transfer quality and better alignment with text, and the methods can be easily integrated into existing frameworks.'}, 'zh': {'title': '提升文本驱动风格迁移的质量与对齐性', 'desc': '本文研究了文本驱动的风格迁移，旨在将参考图像的风格与文本提示描述的内容相结合。尽管最近的文本到图像模型在风格转换的细微差别上取得了进展，但仍面临过拟合、风格控制有限和文本内容不对齐等挑战。为了解决这些问题，我们提出了三种互补策略，包括跨模态自适应实例归一化机制、基于风格的无分类器引导方法以及在生成早期阶段引入教师模型。我们的评估结果显示，所提方法在风格迁移质量和与文本提示的对齐性上有显著提升，并且可以无缝集成到现有的风格迁移框架中。'}}}, {'id': 'https://huggingface.co/papers/2412.07147', 'title': 'MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation', 'url': 'https://huggingface.co/papers/2412.07147', 'abstract': 'Image Translation (IT) holds immense potential across diverse domains, enabling the translation of textual content within images into various languages. However, existing datasets often suffer from limitations in scale, diversity, and quality, hindering the development and evaluation of IT models. To address this issue, we introduce MIT-10M, a large-scale parallel corpus of multilingual image translation with over 10M image-text pairs derived from real-world data, which has undergone extensive data cleaning and multilingual translation validation. It contains 840K images in three sizes, 28 categories, tasks with three levels of difficulty and 14 languages image-text pairs, which is a considerable improvement on existing datasets. We conduct extensive experiments to evaluate and train models on MIT-10M. The experimental results clearly indicate that our dataset has higher adaptability when it comes to evaluating the performance of the models in tackling challenging and complex image translation tasks in the real world. Moreover, the performance of the model fine-tuned with MIT-10M has tripled compared to the baseline model, further confirming its superiority.', 'score': 1, 'issue_id': 1087, 'pub_date': '2024-12-10', 'pub_date_card': {'ru': '10 декабря', 'en': 'December 10', 'zh': '12月10日'}, 'hash': 'cf26c855c674b8aa', 'authors': ['Bo Li', 'Shaolin Zhu', 'Lijie Wen'], 'affiliations': ['Baidu Inc., Beijing, China', 'College of Intelligence and Computing, Tianjin University, Tianjin, China', 'School of Software, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.07147.jpg', 'data': {'categories': ['#machine_translation', '#training', '#multilingual', '#benchmark', '#dataset', '#data', '#synthetic'], 'emoji': '🌐', 'ru': {'title': 'MIT-10M: новый стандарт для машинного перевода текста на изображениях', 'desc': 'Статья представляет новый набор данных MIT-10M для задачи перевода текста на изображениях. Этот датасет содержит более 10 миллионов пар изображение-текст на 14 языках, охватывающих 28 категорий и три уровня сложности. MIT-10M значительно превосходит существующие наборы данных по масштабу, разнообразию и качеству. Эксперименты показали, что модели, обученные на MIT-10M, демонстрируют трехкратное улучшение производительности по сравнению с базовыми моделями.'}, 'en': {'title': 'Unlocking Multilingual Image Translation with MIT-10M', 'desc': 'This paper presents MIT-10M, a new large-scale dataset designed for multilingual image translation (IT). It consists of over 10 million image-text pairs, which are carefully curated to enhance diversity and quality, addressing the limitations of existing datasets. The dataset includes images across 28 categories and supports 14 languages, making it suitable for various IT tasks of different complexities. Experimental results show that models trained on MIT-10M significantly outperform baseline models, demonstrating its effectiveness in real-world image translation scenarios.'}, 'zh': {'title': 'MIT-10M：提升图像翻译的多语言数据集', 'desc': '本文介绍了MIT-10M，这是一个大规模的多语言图像翻译平行语料库，包含超过1000万对图像和文本。该数据集经过严格的数据清理和多语言翻译验证，解决了现有数据集在规模、多样性和质量上的不足。MIT-10M包含840K张图像，涵盖28个类别和14种语言，提供了不同难度级别的任务。实验结果表明，使用MIT-10M微调的模型在处理复杂的图像翻译任务时，性能显著提升，达到了基线模型的三倍。'}}}, {'id': 'https://huggingface.co/papers/2412.08467', 'title': 'Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel', 'url': 'https://huggingface.co/papers/2412.08467', 'abstract': 'Creating high-quality data for training robust language-instructed agents is a long-lasting challenge in embodied AI. In this paper, we introduce a Self-Refining Data Flywheel (SRDF) that generates high-quality and large-scale navigational instruction-trajectory pairs by iteratively refining the data pool through the collaboration between two models, the instruction generator and the navigator, without any human-in-the-loop annotation. Specifically, SRDF starts with using a base generator to create an initial data pool for training a base navigator, followed by applying the trained navigator to filter the data pool. This leads to higher-fidelity data to train a better generator, which can, in turn, produce higher-quality data for training the next-round navigator. Such a flywheel establishes a data self-refining process, yielding a continuously improved and highly effective dataset for large-scale language-guided navigation learning. Our experiments demonstrate that after several flywheel rounds, the navigator elevates the performance boundary from 70% to 78% SPL on the classic R2R test set, surpassing human performance (76%) for the first time. Meanwhile, this process results in a superior generator, evidenced by a SPICE increase from 23.5 to 26.2, better than all previous VLN instruction generation methods. Finally, we demonstrate the scalability of our method through increasing environment and instruction diversity, and the generalization ability of our pre-trained navigator across various downstream navigation tasks, surpassing state-of-the-art methods by a large margin in all cases.', 'score': 0, 'issue_id': 1090, 'pub_date': '2024-12-11', 'pub_date_card': {'ru': '11 декабря', 'en': 'December 11', 'zh': '12月11日'}, 'hash': 'f49d4b05298275a6', 'authors': ['Zun Wang', 'Jialu Li', 'Yicong Hong', 'Songze Li', 'Kunchang Li', 'Shoubin Yu', 'Yi Wang', 'Yu Qiao', 'Yali Wang', 'Mohit Bansal', 'Limin Wang'], 'affiliations': ['Adobe Research', 'Nanjing University', 'Shanghai AI Laboratory', 'UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2412.08467.jpg', 'data': {'categories': ['#dataset', '#transfer_learning', '#optimization', '#agents', '#training'], 'emoji': '🧭', 'ru': {'title': 'Самосовершенствующийся цикл данных для обучения навигации по языковым инструкциям', 'desc': 'Эта статья представляет новый метод под названием Self-Refining Data Flywheel (SRDF) для создания высококачественных данных для обучения агентов с языковыми инструкциями в воплощенном ИИ. SRDF использует итеративный процесс, в котором два модуля - генератор инструкций и навигатор - сотрудничают для улучшения качества данных без участия человека. Эксперименты показывают, что после нескольких итераций навигатор превзошел человеческую производительность на тестовом наборе R2R, достигнув 78% SPL. Метод также демонстрирует масштабируемость и способность к обобщению на различные задачи навигации.'}, 'en': {'title': 'Self-Refining Data Flywheel: Elevating Language-Guided Navigation Performance', 'desc': "This paper presents a novel approach called the Self-Refining Data Flywheel (SRDF) to enhance the quality of data used for training language-instructed agents in embodied AI. The SRDF operates by iteratively refining a dataset of navigational instruction-trajectory pairs through the collaboration of an instruction generator and a navigator, eliminating the need for human annotation. By using a base generator to create initial data and then filtering it with a trained navigator, the process continuously improves the dataset, leading to better training outcomes. The results show significant performance improvements, with the navigator achieving a success rate of 78% on the R2R test set, surpassing human performance, and demonstrating the method's scalability and generalization across various tasks."}, 'zh': {'title': '自我精炼数据飞轮：提升导航性能的新方法', 'desc': '本文提出了一种自我精炼数据飞轮（SRDF），旨在为语言指导的导航代理生成高质量的大规模数据。SRDF通过指令生成器和导航器两个模型的协作，迭代地精炼数据池，从而无需人工标注。该方法从基础生成器开始，创建初始数据池，然后利用训练好的导航器过滤数据，形成高保真数据以训练更好的生成器。实验结果表明，经过多轮飞轮迭代，导航器的性能从70%提升至78%，首次超越人类表现，同时生成器的性能也显著提高。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (4)', '#agents (1)', '#agi', '#alignment', '#architecture (4)', '#audio', '#benchmark (5)', '#cv (5)', '#data (2)', '#dataset (8)', '#diffusion (3)', '#ethics', '#games (2)', '#graphs', '#hallucinations', '#healthcare', '#inference', '#interpretability', '#leakage', '#long_context', '#low_resource (1)', '#machine_translation (1)', '#math', '#multilingual (2)', '#multimodal (4)', '#open_source (5)', '#optimization (6)', '#plp', '#rag', '#reasoning (1)', '#rl', '#rlhf', '#robotics', '#science', '#security', '#small_models', '#story_generation', '#survey', '#synthetic (3)', '#training (6)', '#transfer_learning (2)', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-12-12 16:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-12-12 16:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-12-12 16:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    