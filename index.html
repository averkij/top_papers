
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF (18 статей)</title>
    <link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #03dac6;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        header {
            padding: 1.6em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.5em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .background-digit {
            position: absolute;
            bottom: -20px;
            right: -10px;
            font-size: 12em;
            font-weight: bold;
            color: rgba(0, 0, 0, 0.03);
            z-index: 0;
            line-height: 1;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: var(--secondary-color);
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: fixed;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
        }
        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .update-info-container {
            flex: 1;
        }
        .sort-container {
            flex: 2;
        }
        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .category-toggle {
            display: none;
            margin-bottom: 10px;
            margin-top: 15px;
            cursor: pointer;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }

        .svg-container span {
            position: relative;
            z-index: 1;
        }

        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .sort-container {
                margin-top: 0px;
                text-align: left;
                width: 100%;
            .sort-dropdown {
                float: right;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiffRu(dateString) {
        const timeUnits = {
            minute: ["минуту", "минуты", "минут"],
            hour: ["час", "часа", "часов"],
            day: ["день", "дня", "дней"]
        };

        function getRussianPlural(number, words) {
            if (number % 10 === 1 && number % 100 !== 11) {
                return words[0];
            } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                return words[1];
            } else {
                return words[2];
            }
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);

        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes == 0) {
            return 'только что';
        }
        else if (minutes < 60) {
            return `${minutes} ${getRussianPlural(minutes, timeUnits.minute)} назад`;
        } else if (hours < 24) {
            return `${hours} ${getRussianPlural(hours, timeUnits.hour)} назад`;
        } else {
            return `${days} ${getRussianPlural(days, timeUnits.day)} назад`;
        }
    }
    function formatArticlesTitle(number) {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;

        let word;

        if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
            word = "статей";
        } else if (lastDigit === 1) {
            word = "статья";
        } else if (lastDigit >= 2 && lastDigit <= 4) {
            word = "статьи";
        } else {
            word = "статей";
        }

        return `${number} ${word}`;
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">
            <h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">хф дэйли</h1>
            <p>16 октября | 18 статей</p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 Сортировка по</label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="category-toggle">
            <div class="svg-container">
                <span id="category-toggle">🔍 Фильтр</span>
                <svg height="3" width="200">
                    <line x1="0" y1="0" x2="200" y2="0" 
                        stroke="black" 
                        stroke-width="2" 
                        stroke-dasharray="3, 3" />
                </svg>
            </div>
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">градиент обреченный</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>    
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "хф найтли";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "хф дэйли";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.11779', 'title': 'MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation', 'url': 'https://huggingface.co/papers/2410.11779', 'abstract': 'Multimodal Large Language Models (MLLMs) frequently exhibit hallucination phenomena, but the underlying reasons remain poorly understood. In this paper, we present an empirical analysis and find that, although MLLMs incorrectly generate the objects in the final output, they are actually able to recognize visual objects in the preceding layers. We speculate that this may be due to the strong knowledge priors of the language model suppressing the visual information, leading to hallucinations. Motivated by this, we propose a novel dynamic correction decoding method for MLLMs (DeCo), which adaptively selects the appropriate preceding layers and proportionally integrates knowledge into the final layer to adjust the output logits. Note that DeCo is model agnostic and can be seamlessly incorporated with various classic decoding strategies and applied to different MLLMs. We evaluate DeCo on widely-used benchmarks, demonstrating that it can reduce hallucination rates by a large margin compared to baselines, highlighting its potential to mitigate hallucinations. Code is available at https://github.com/zjunlp/DeCo.', 'score': 16, 'issue_id': 121, 'pub_date': '2024-10-15', 'pub_date_ru': '15 октября', 'data': {'desc': 'Статья исследует проблему галлюцинаций в мультимодальных больших языковых моделях (MLLM). Авторы обнаружили, что MLLM способны распознавать визуальные объекты в промежуточных слоях, но в итоговом выводе могут их некорректно генерировать. Предполагается, что это связано с подавлением визуальной информации сильными языковыми приорами модели. Для решения проблемы предложен метод декодирования DeCo, адаптивно интегрирующий знания из предыдущих слоев в финальный слой.', 'categories': ['#multimodal', '#nlp', '#cv', '#code'], 'emoji': '🔍', 'title': 'Борьба с галлюцинациями в мультимодальных моделях', 'embedding': [0.03247479333420629, 0.04238699545387423, -0.04501564790079551, 0.00994643192840189, -0.03170810321628497, 0.07119262924505257, 0.05678981664780065, 0.04542637551724215, -0.10766515991710686, 0.09594575843424231, 0.00925504223169056, -0.04512517383240219, -0.06609961969612593, 0.04863004232885147, -0.042852488185292845, -0.07184979558053584, 0.05919940863483404, -0.03463795304970895, -0.03827973218441954, 0.04151078155351484, -0.017962448357841804, 0.02907945076936369, -0.012670919243103726, 0.027751436490913002, 0.05941846479638468, 0.018003521119486464, 0.011965838052732538, 0.09523383128212434, 0.06987830517076056, 0.07141168325743456, 0.03619871584303758, -0.04936934988928681, -0.11111526759692143, -0.024246567994463716, -0.08000957204865364, 0.06084231910062063, -0.006472368325561398, 0.025656729945372363, -0.07842142239950177, 0.03852616445595027, -0.011917920189008535, -0.06724965272383929, -0.047534772266941465, 0.027655598614296364, -0.00787910729559665, 0.13066586345474648, -0.0011757052768053859, 0.03370696973604034, -0.04030598126166089, 0.10782945096368553, -0.12343707030029728, 0.08471923009111373, -0.017182066961177488, 0.10120306117891624, -0.0727260094808952, -0.04036074637663287, 0.0824191618865184, 0.010391385914912685, 0.0027997870911612363, -0.03732136846243357, 0.019783335776028466, -0.026984744223823055, -0.017305286320695796, 0.001033662208944189, -0.026437108118283734, -0.06889256748796312, -0.09830059605214687, -0.0483562232014975, 0.04298939667438552, 0.06182806108175532, -0.021658988309187104, -0.01956428176364646, -0.007872261978600447, 0.014608180059061907, -0.09972445035638282, -0.022863787526456743, -0.021700061070831767, 0.056406467290502735, 0.008905924832295225, -0.018879737169014473, 0.008111852801638507, 0.030339014027020987, -0.020152990630830446, -0.07814760542131644, 0.027285943759494384, -0.047835971802612794, 0.025738873319493063, 0.020194064467059424, -0.07289030052747388, -0.06029468299508131, 0.055092145365379354, 0.007160335686149922, -0.06171853515014864, 0.04975269494824747, -0.01579928778267169, 0.04501564790079551, 0.04301677708270288, -0.08855268282988897, 0.05761126758235669, 0.023438805114897734, -0.06330667835179463, 0.030421159550310314, 0.05446236158738208, -0.11927504191587061, 0.030585448447720348, -0.0391833286422649, -0.01664812342388234, -0.03006519489966701, -0.01071996714840255, 0.04293463370858217, -0.013759345062601847, 0.031215231151133307, -0.04419419696623947, -0.021303025807712428, 0.04441324882945284, -0.18028164776473266, -0.056625521302884736, 0.07387604895611444, 0.01881128399905245, 0.009823213643467898, -0.06987830517076056, -0.007064499743785051, 0.044248959932042815, -0.07245219250270987, -0.013314390646257326, -0.014717707924920359, 0.05925417589897465, -0.08345966468428781, 0.02687521829221637, -0.021125044556975094, -0.043345359175860194, 0.03272122775490565, 0.06790681691015392, -0.07732615233759176, -0.12288943419475797, 0.005757019482071203, -0.08395253997319242, 0.011582493853439329, -0.06905684993786727, 0.02621805625507036, 0.04416681225958485, -0.03233788269594499, 0.06155424410356998, 0.07091881656520448, -0.10799374630860144, 0.06319715242018793, 0.08110483566305778, -0.01849639425922244, -0.0743689134991759, 0.07250695761768185, -0.07721662640598509, -0.062375699336463274, -0.013540291050219843, 0.002375369485472774, -0.13230876317468993, -0.02650556558658301, -0.017058849750827807, -0.01897557504563111, 0.11095098084868003, -0.08773123404450156, 0.04947888011923076, 0.09183848871728174, 0.14829972757026233, -0.02230246229134306, -0.027655598614296364, 0.11281293458100547, -0.11905598145598273, 0.14939499978134096, -0.04162030533595291, -0.1341707276528585, -0.07683327919785579, 0.043619178303214164, -0.03014734257212497, -0.07414986378513116, 0.028312760651442368, -0.01304057280840453, -0.03814282154615824, -0.21894472715728147, -0.029380650305035023, -0.09578146953683228, -0.03485700921125958, 0.07683327919785579, -0.06845445301569324, 0.05145036623066877, -0.08762169736705173, -0.08088578379984439, -0.06845445301569324, 0.09769819698080422, 0.009453559648333368, 0.034583190083905604, -0.06264951416547998, 0.037211842530826884, 0.10766515991710686, 0.062211406140715984, 0.007913334740245113, -0.06735917865544597, 0.14118045604908258, 0.06401860335474398, -0.017140995274117138, -0.059473227762188025, -0.02946279797749298, 0.09309805197493905, 0.0009344032857059229, -0.16943845588389023, 0.05260040355671938, 0.03455580752641962, 0.06467576754105861, 0.015676069282820834, 0.04112743649455419, 0.030339014027020987, 0.05295636605819405, -0.024506692619321754, 0.1424947908692177, 0.005031402125794547, -0.023479875727373766, -0.04622044604348083, -0.09112656371433239, -0.012390255873337863, 0.007749043908583316, -0.002043365400059631, -0.047233572731270136, 0.058925593805817335, -0.012465556294547852, 0.055092145365379354, -0.08920984486703498, -0.006951549541803792, 0.0806667233399565, -0.07404033570435585, 0.037704713521394234, 0.025259694682253024, 0.050711058670233435, -0.12168463820124126, -0.015210579775155157, -0.005674874173698737, 0.023945368458792384, -0.027997870911612363, 0.03129737559983833, -0.023548333195673045, 0.014060544598273177, -0.06467576754105861, 0.09605528651501762, 0.02383584037801707, 0.0650043474850473, -0.05739221142080606, -0.015279034019701494, 0.07666899030044574, -0.09134562417422028, -0.018085666642775795, 0.04400252336217483, -0.04167507045092488, 0.00946040518024643, 0.029380650305035023, -0.06149948113776664, -0.024465622006845714, -0.08313108903863638, 0.08838839393247894]}}, {'id': 'https://huggingface.co/papers/2410.11710', 'title': 'MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models', 'url': 'https://huggingface.co/papers/2410.11710', 'abstract': 'Large Language Models (LLMs) have displayed massive improvements in reasoning and decision-making skills and can hold natural conversations with users. Recently, many tool-use benchmark datasets have been proposed. However, existing datasets have the following limitations: (1). Insufficient evaluation scenarios (e.g., only cover limited tool-use scenes). (2). Extensive evaluation costs (e.g., GPT API costs). To address these limitations, in this work, we propose a multi-granularity tool-use benchmark for large language models called MTU-Bench. For the "multi-granularity" property, our MTU-Bench covers five tool usage scenes (i.e., single-turn and single-tool, single-turn and multiple-tool, multiple-turn and single-tool, multiple-turn and multiple-tool, and out-of-distribution tasks). Besides, all evaluation metrics of our MTU-Bench are based on the prediction results and the ground truth without using any GPT or human evaluation metrics. Moreover, our MTU-Bench is collected by transforming existing high-quality datasets to simulate real-world tool usage scenarios, and we also propose an instruction dataset called MTU-Instruct data to enhance the tool-use abilities of existing LLMs. Comprehensive experimental results demonstrate the effectiveness of our MTU-Bench. Code and data will be released at https: //github.com/MTU-Bench-Team/MTU-Bench.git.', 'score': 16, 'issue_id': 121, 'pub_date': '2024-10-15', 'pub_date_ru': '15 октября', 'data': {'desc': 'В этой статье представлен новый бенчмарк MTU-Bench для оценки навыков использования инструментов большими языковыми моделями (LLM). MTU-Bench охватывает пять сценариев использования инструментов, от простых до сложных и нестандартных задач. Оценка производится без использования API GPT или человеческой экспертизы, что снижает затраты. Авторы также предлагают набор данных MTU-Instruct для улучшения способностей LLM в использовании инструментов.', 'categories': ['#benchmark', '#nlp', '#dataset', '#rlhf'], 'emoji': '🛠️', 'title': 'Многогранная оценка навыков LLM в использовании инструментов', 'embedding': [-0.027825905545227552, -0.00598734934856303, 0.12383590192401733, -0.05639525478511897, -0.04678363350831473, -0.021904935453124316, 0.023683882397844493, 0.13233237287956254, -0.05528009634613364, 0.07567159741562754, 0.06462619288525875, -0.09951479019133007, -0.03552582258205096, -0.04086266138677993, 0.0331361932967907, 0.0001733103607738907, -0.021719076051532026, -0.05735110487567783, -0.08453977499776791, 0.09054040636709484, 0.06770615969998811, 0.03202103485780538, 0.003833363856504123, -0.026936433087583242, 0.04253540411883683, -0.017829288923940027, -0.007779571211036647, 0.03164931402518922, 0.029950022269398063, 0.00654492937982832, 0.045562266797802994, -0.038818199851538435, -0.10179821526392706, 0.035632028824145796, 0.042243336445718135, 0.04951843257302532, 0.09229279819978609, 0.06425448219980043, 0.07338817640189367, 0.033985839174686276, -0.005678688434138847, -0.059050396631468986, 0.005516061153330855, 0.012240212069988437, -0.05926280911565866, 0.029418993088355435, 0.06021866326508064, 0.03366722450726489, -0.07142336802614964, 0.023351990174408636, -0.049173262793574986, 0.09781549640610734, -0.07503436199249007, -0.08268117692298184, 0.0071157857494491454, -0.11406497229878661, -0.023723709992309005, -0.002918998797168974, 0.03042794731467749, -0.03316274637938808, -0.03042794731467749, -0.0526249504409205, 0.012837619898661392, 0.041738862376704454, -0.08682320209979645, -0.04842982315777424, -0.022794408925484407, 0.034994795430439894, 0.01785584200653741, -0.027241775272569067, -0.03576478611940646, -0.0436240135340879, 0.0026816951318905497, -0.04333194789040078, 0.06653790524296585, -0.011271085438130891, 0.0070958719522168905, 0.10572782389768887, 0.0007840971663037858, 0.0010189114293529162, -0.016780509132585025, -0.0601124570229858, -0.03849958518411705, -0.005340157940053641, -0.01611672265628174, 0.0018685575101916516, -0.012266764137870036, -0.011470221381021876, -0.08257497271031856, -0.11587047638496731, -0.03616305800518843, -0.04383642195941445, 0.022502343281797278, 0.01194150917036774, 0.15081216565021244, -0.03483548708201342, 0.04973084302778343, -0.011868492455031222, 0.005018222107876017, 0.08007913718296346, -0.05233288276780181, 0.012200385287296552, 0.08889421671763532, -0.05204081509468311, 0.09123073780826926, -0.03008277956465872, 0.030295190019416832, 0.003395265188030268, -0.010627212759059862, 0.024480424139976874, -0.2400250112412901, 0.011782199807225483, -0.11268430332814311, 0.02628592315257865, 0.004102197176463797, -0.06568825530620713, -0.013780196695012058, 0.03385308187942562, 0.023219231864432196, -0.06988338664821653, -0.0028841500071630517, 0.02681695334833706, 0.06913994295355265, -0.1142773847829763, 0.07466264115987391, -0.007441040716951443, 0.0027762847047637677, -0.019488755114698237, 0.010554196246666502, 0.06112140921987629, -0.059900044538796125, 0.039800605054126234, 0.007261818368349555, -0.05259839735832312, -0.0671220365303401, -0.03345481405250678, -0.1293586062186333, 0.08172532480299143, -0.06590066779039681, -0.04118128011306445, 0.003587763063215064, -0.04333194789040078, -0.025715067391787295, 0.08268117692298184, -0.14263431545038335, 0.01700619612864183, -0.018293941486783892, 0.0747688453725372, 0.014377603306026075, 0.09999271726604106, -0.05650146102721381, -0.07721158488185534, -0.053872866175166484, 0.0023464831643005475, -0.06478550326311679, -0.09404519815020357, -0.04001301753831591, 0.11438359305450269, 0.03687994861725491, -0.173965024955309, 0.10418783846089262, 0.027985215923085588, 0.016899991915978557, 0.07954810800192084, 0.015107767821130219, 0.16599958723966957, -0.03988026024305526, 0.014404154968021363, -0.11470220975135566, -0.07057372620711719, 0.011403842124562079, -0.009697912301064732, -0.026511611163351236, -0.06048417379673883, 0.021825282293626864, -0.10535610509450428, -0.11629529729448353, -0.0463057084630353, 0.03122449108624143, -0.0593690153577535, -0.05798834029881529, 0.10100166337463685, -0.13764264439567314, -0.012021163344580978, -0.10498438223245657, -0.06473240318621672, -0.008383616904472637, 0.053235634810892156, -0.03082621920045946, 0.005436406979117617, -0.034994795430439894, 0.10928572184599236, 0.15463558224790036, 0.1365805921218826, 0.10355060912604984, -0.030958976495720117, 0.10737401151771682, 0.055970429816739616, -0.047553624197281284, 0.04885464406729047, 0.08507080823767366, 0.03090587438938848, -0.025715067391787295, -0.11172844714928956, 0.04067680198518763, 0.036455125678307124, -0.015745002838381372, 0.026949708614166155, -0.07784881218726654, -0.061068307113544655, -0.010613937232476953, 0.1094981302713189, 0.05968763205460645, -0.00443409041461544, -0.0300562285114929, -0.02305992351600572, -0.07073303658497523, -0.05480216927142265, 0.01910375977021496, 0.09160245864088543, -0.003737115020383303, 0.032233443283131914, 0.07275094097875622, 0.040437838447832136, -0.029286237822526342, 0.02510438505124721, 0.0408095592804483, -0.09101832532407958, 0.10694919669649529, -0.06675031772715552, 0.0019565090153586804, -0.07009579507354306, -0.05429768911411428, -0.12064972683776157, 0.017630155010480607, -0.03576478611940646, 0.025807998107299224, -0.00988377271737281, -0.005323563278146059, -0.08570804163137957, 0.09685964834498005, 0.016793785673883718, 0.032658268251511274, -0.018307215998651018, 0.037145459148913096, -0.018904623827323976, -0.03918992068415459, -0.009173521796557763, 0.04550916266203979, 0.03557892468838259, 0.008834991302472558, -0.07423782430922081, -0.06037796755464398, -0.06356414467033132, -0.008848266829055467, 0.004161937959331092]}}, {'id': 'https://huggingface.co/papers/2410.09342', 'title': 'LLM$\\times$MapReduce: Simplified Long-Sequence Processing using Large Language Models', 'url': 'https://huggingface.co/papers/2410.09342', 'abstract': 'Enlarging the context window of large language models (LLMs) has become a crucial research area, particularly for applications involving extremely long texts. In this work, we propose a novel training-free framework for processing long texts, utilizing a divide-and-conquer strategy to achieve comprehensive document understanding. The proposed LLMtimesMapReduce framework splits the entire document into several chunks for LLMs to read and then aggregates the intermediate answers to produce the final output. The main challenge for divide-and-conquer long text processing frameworks lies in the risk of losing essential long-range information when splitting the document, which can lead the model to produce incomplete or incorrect answers based on the segmented texts. Disrupted long-range information can be classified into two categories: inter-chunk dependency and inter-chunk conflict. We design a structured information protocol to better cope with inter-chunk dependency and an in-context confidence calibration mechanism to resolve inter-chunk conflicts. Experimental results demonstrate that LLMtimesMapReduce can outperform representative open-source and commercial long-context LLMs, and is applicable to several different models.', 'score': 15, 'issue_id': 125, 'pub_date': '2024-10-12', 'pub_date_ru': '12 октября', 'data': {'desc': 'Статья представляет новый подход к обработке длинных текстов с помощью больших языковых моделей (LLM). Авторы предлагают фреймворк LLM×MapReduce, который разделяет документ на части для обработки, а затем объединяет промежуточные результаты. Для решения проблем потери информации при разделении текста разработаны протокол структурированной информации и механизм калибровки уверенности в контексте. Эксперименты показывают, что LLM×MapReduce превосходит существующие модели с длинным контекстом и применим к различным LLM.', 'categories': ['#nlp', '#rag', '#benchmark'], 'emoji': '📄', 'title': 'Эффективная обработка длинных текстов без переобучения LLM', 'embedding': [0.00915309708275282, 0.06845138927568167, 0.17312147522446034, -0.02031150997756016, -0.005490012879244443, -0.01400030128081017, 0.0704197976795912, 0.004711876569259529, -0.06638455646072372, 0.13385175151158302, 0.02161557904858036, 0.0052900966875649375, -0.010198813498587474, 0.061463539441802856, 0.036710808751206754, 0.012966887267842965, 0.06505588378122451, -0.015292069744846754, 0.03799027221832147, 0.2133754230935572, 0.0036107984425794394, 0.016916006478529467, 0.04551943336556217, -0.008119683069785614, -0.011699724806797164, -0.014787664642373981, -0.025859958921225352, 0.08134445833500964, 0.07002611400338281, -0.08149208796758961, -0.01395109127025508, -0.042837478411805296, -0.11170714699046845, 0.030707165114709178, -0.016202457933255643, 0.10471930713372203, -0.03348754248408801, 0.12095867047969619, -0.03609568262155489, 0.041262753684104155, -0.014283259938986503, -0.01787560507657874, -0.03936815810105814, 0.06200485075516474, -0.01365583000966865, 0.09384384970440865, 0.014799967444326725, 0.12460022004064938, -3.332741350004716e-05, 0.09674724910703549, -0.0017792562410539265, 0.011355253735198294, -0.02721323920005653, -0.08247629616039732, 0.016940612082434952, -0.07765369756624547, 0.04049999393101936, -0.03186360255772293, 0.02780376172122939, -0.010973874656826485, 0.06013486476687717, -0.10412877663084326, -0.022550574038150625, 0.03319227723264862, -0.08911966753959931, 0.0188967166866785, -0.07539002590632304, -0.025490882844348934, -0.06412088679622777, -0.011668968699857219, 0.0028495779614800976, -0.03501305799940464, 0.024838847311125595, 0.00014378604209722837, 0.048324414343420004, -0.0020283829421451414, -0.03437332227499434, 0.05103097490108236, -0.030731769720901428, 0.012284096026764975, 0.05772356347437475, -0.02563851447235539, 0.02829586382220677, -0.03508687281569464, 0.038014877822226956, 0.017432714183412336, -0.0024097620205169503, 0.12253339919825029, -0.02220610156975322, -0.10343983568490141, -0.015611935412082787, 0.04987453745806862, -0.02802520816552583, 0.026794953910795612, 0.0712071530594491, -0.06594166556755732, -0.027508502456069436, -0.10629402787057021, 0.042419193122544385, 0.06815612601966876, -0.02168939386487035, -0.026229036993528243, 0.08597021608877055, -0.1244033811956849, 0.07194531519033433, 0.0005147848121569337, -0.0033063103299567622, -0.08887361748682386, -0.08488759745289974, -0.01551351499188731, -0.09468042626920992, -0.005566903645450926, -0.057871195102381215, 0.008968558046601372, 0.08877519706662838, -0.15333897475602254, -0.10511298681907745, 0.03274938833490869, -0.005004062279014757, 0.03447174269518981, -0.025195621583762505, 0.010530982167318896, -0.013212938517874303, -0.0360218678052649, 0.019942433900683745, -0.0866591570347124, 0.053737539449597686, -0.0766202761702003, 0.04094288482418576, 0.01812165712478067, 0.025441671636537957, 0.005736063730190485, 0.061069855765594476, -0.016682256234567042, -0.16318100879386427, -0.009337635719818972, -0.1414301118940758, 0.03641554749062032, -0.06412088679622777, -0.02568772368473989, -0.024605099062589647, -0.024469771234249173, 0.007596824462782991, -0.008218103090895795, 0.034422533482805315, 0.021738605072681327, 0.0007381529519234246, 0.030830189143383663, -0.06126669860141191, -0.01750652899970232, -0.05442648039553954, -0.15973630406415498, -0.034619372327769786, 0.0345455575114798, -0.006846368708906822, -0.043501817744694624, 0.007049361050045471, -0.021197293759319447, 0.10678612598069463, -0.06968164552583836, 0.0428866876241898, 0.04303431925219625, -0.009190004490897814, 0.05427884876753308, -0.0010380276385279778, 0.05393437829456215, 0.005551525492209629, 0.003955269713720959, -0.1012745884360274, -0.09802671656500317, -0.10176669053700478, -0.1123960979135427, -0.04495351844372128, -0.059052240144726924, -0.016042524501009684, -0.06505588378122451, -0.07981894481511947, -0.030854794747289156, -0.04684810804048786, -0.0645637836756736, -0.07278188377342969, 0.14802426807461386, -0.08040946534086585, 0.057428304209214805, -0.08705284669720022, -0.05196596989065263, 0.009343786821481372, 0.09079281667834889, -0.010438713447413765, 0.04170564258184408, -0.0760789668522649, 0.15835841419056537, 0.0701245344235783, 0.016177854324776635, 0.06028249439945714, 0.031592948896468465, 0.16820045421468652, 0.037202910852184135, -0.010900059042365904, -0.020619073042386093, 0.028172839793532283, 0.10294773757477699, -0.048275205131035503, -0.10570350734482384, -0.0201023673329297, 0.0029618388592725945, 0.024678913878879636, 0.05703462053300642, -0.020459139610140138, -0.11623449031031333, 0.06554798388677541, 0.020176182149219693, 0.14211905284001766, 0.009817434420215666, -0.027016400355092052, -0.1168250108360597, -0.055705943862654254, 0.0029233933763980293, 0.08552732719103062, -0.019733289260626806, 0.0496530890183457, 0.008617935673797452, 0.07711238425745712, 0.11593922705430042, -0.018269286757360646, -0.005723761127780389, 0.009811283318553266, -0.03203583779420839, 0.075586866746714, -0.03014124819744181, 0.053196224145382845, -0.06874664654541515, -0.029058621579865093, -0.05211359952323261, 0.05629646837925359, -0.10816401784971075, -0.03282320315119868, -0.004681120262776934, 0.009602139875752216, -0.010272629113048054, 0.023153398363562987, 0.027508502456069436, 0.045716274205953124, -0.02374392088473585, -0.02632745741372372, 0.04896414807240384, -0.08710205590958472, -0.10255405788942155, 0.07824421609656537, -0.015119833311105405, -0.021590974442388115, -0.035357528472375575, -0.041804064997466035, -0.08685599787510336, -0.08705284669720022, 0.04805376068216555]}}, {'id': 'https://huggingface.co/papers/2406.15786', 'title': 'What Matters in Transformers? Not All Attention is Needed', 'url': 'https://huggingface.co/papers/2406.15786', 'abstract': 'While scaling Transformer-based large language models (LLMs) has demonstrated promising performance across various tasks, it also introduces redundant architectures, posing efficiency challenges for real-world deployment. Despite some recognition of redundancy in LLMs, the variability of redundancy across different architectures in transformers, such as MLP and Attention layers, is under-explored. In this work, we investigate redundancy across different modules within Transformers, including Blocks, MLP, and Attention layers, using a similarity-based metric. Surprisingly, despite the critical role of attention layers in distinguishing transformers from other architectures, we found that a large portion of these layers exhibit excessively high similarity and can be pruned without degrading performance. For instance, Llama-2-70B achieved a 48.4\\% speedup with only a 2.4\\% performance drop by pruning half of the attention layers. Furthermore, by tracing model checkpoints throughout the training process, we observed that attention layer redundancy is inherent and consistent across training stages. Additionally, we further propose a method that jointly drops Attention and MLP layers, allowing us to more aggressively drop additional layers. For instance, when dropping 31 layers (Attention + MLP), Llama-2-13B still retains 90\\% of the performance on the MMLU task. Our work provides valuable insights for future network architecture design. The code is released at: https://github.com/Shwai-He/LLM-Drop.', 'score': 14, 'issue_id': 121, 'pub_date': '2024-06-22', 'pub_date_ru': '22 июня', 'data': {'desc': 'Это исследование посвящено проблеме избыточности в архитектуре больших языковых моделей (LLM) на базе трансформеров. Авторы обнаружили, что значительная часть слоев внимания (attention layers) обладает высокой схожестью и может быть удалена без существенной потери производительности. Например, модель Llama-2-70B достигла ускорения на 48.4% при снижении производительности всего на 2.4% после удаления половины слоев внимания. Исследователи также предложили метод совместного удаления слоев внимания и MLP, что позволяет еще более агрессивно сокращать архитектуру модели.', 'categories': ['#nlp', '#benchmark', '#code'], 'emoji': '✂️', 'title': 'Оптимизация LLM: меньше слоев, та же мощность', 'embedding': [0.07973888359077871, 0.01813971157876565, 0.060676386777125876, -0.035748502926044506, 0.061889910692635525, 0.08929541357829962, 0.003909202489554159, 0.07999169738717994, -0.08191311561485319, 0.12640912990402067, -0.05673242198845783, -0.05339521714700099, -0.06815980101063218, 0.06492373455521495, 0.03610244706631088, -0.05334465599615564, 0.1036554941613627, -0.10931862453214812, 0.005609405629560923, 0.08039621072046613, 0.00690193888280499, 0.029276355941404767, 0.05334465599615564, 0.027582472945253667, 0.11002651080213727, -0.00976510579274786, 0.015978115821315295, -0.06598557300764493, 0.06315400179062138, -0.11629640514122114, 0.05329409283476667, -0.05208056288762618, -0.08166029980790834, 0.057844817570645926, -0.03511645637177978, 0.05931116734581848, 0.07185095803452984, 0.060170749131605375, -0.03372595636504971, 0.08343003056195833, 0.07660393742650859, -0.04611405153796332, -0.05971567665801745, 0.043914534917378956, -0.0031791897849665117, 0.08833470144864758, 0.05101872644573981, 0.05779425843034419, -0.027582472945253667, 0.09642488769262676, -0.07043517343128987, 0.05703580095679163, -0.059968492464962275, -0.022551389128732967, -0.04426847905764533, -0.10446450876467338, 0.018228197613832247, 0.02090807130451445, -0.0017634073933245145, 0.01910041968733047, -0.029301638527371054, -0.08120522733432042, 0.04836413735156752, -0.01588962777570508, -0.06800811152646527, -0.039793596048034105, -0.14531994125459385, 0.025357673738703006, 0.011629640112013392, 0.045608415902986435, -0.044420172562899464, -0.013032782014889688, 0.02327192171806429, -0.023828122524973763, 0.08393566217584798, 0.04778265395869175, 0.01528286461162409, 0.1559383056734575, 0.03051516445342432, 0.042397626001904665, 0.0836322852180578, 0.028897129215172096, 0.06371020661861808, -0.0644686580605398, 0.02877071829588426, 0.00540083022644269, 0.0018044904394571581, -0.019745106529306538, -0.024864676380893836, -0.07801971599811772, 0.026065564029779376, 0.02117353142025785, -0.061940475864568106, 0.034181030849181254, 0.007584548196349963, 0.02266515774976585, 0.03868119443421515, 0.029478610597504246, 0.07599716943712292, 0.12489222500963361, 0.058653840216131084, 0.060474130110482784, 0.03511645637177978, -0.05728862078482369, -0.01717900145965722, -0.09192472008650562, 0.07427800586554915, -0.10871185252167523, -0.061889910692635525, 0.027734164439964185, -0.050917598112418264, 0.013020140922960905, -0.05197943455430463, 0.03544512395771071, 0.030363474969257422, -0.06881713618249403, -0.10350380266665218, 0.0735701175850164, 0.012419697701681219, 0.027683599268031604, 0.01655959619837563, -0.0078879295773361, 0.07235658964841951, -0.017431819277145665, 0.0012949036841668703, -0.03385236728433754, 0.09248092491450233, -0.04975463735829758, 0.014132541330453684, -0.0009678199240745688, 0.03359954745630549, 0.06269893132757706, 0.08459299533716623, -0.05079119121421766, -0.0068892977908762065, -0.02990840048512588, -0.049653509024976034, 0.03223433003554171, -0.08853696213637789, 0.017659356519211435, 0.07822196864367358, -0.002711476018395441, 0.0033877647859760223, 0.04457186204706636, -0.0012253785631977498, 0.09025612168686444, 0.07822196864367358, 0.023562662409230363, -0.10982425614603779, 0.09824517959752209, -0.0013667987937391987, -0.06325513012394292, -0.08393566217584798, -0.029958963646514847, -0.1290384384227703, -0.07008122124884904, 0.02596443569645783, 0.06942389612970525, 0.1633205996158167, -0.1163975334745427, 0.02689986524014359, 0.05364803898557667, 0.021502192974557938, 0.01687562048077981, -0.00812178736536626, 0.09930701603940845, -0.12337532011524656, -0.023828122524973763, -0.15876987487993746, -0.15209546519702377, -0.0611314592507138, -0.047100044243038056, 0.015131173519022299, -0.10158238443897892, -0.010618367032570353, 0.00017005978097185087, -0.09369445486164282, -0.0461646146993523, -0.02023810187657209, -0.013475213396548832, -0.016155087891448483, 0.001189035896380246, -0.05339521714700099, 0.08019395003273579, -0.09237979457063718, -0.048288291604212254, -0.05688410946208111, 0.07169926251873211, 0.013626904690204984, 0.058047074237289026, -0.03587491384533235, 0.0960203763698842, 0.03984416121996669, 0.02408094034246221, 0.0772106973737198, 0.00046692376366818967, 0.06219329569260017, 0.03789746040632714, -0.029074101285305284, -0.08732343017869379, 0.0073317299767527945, 0.12772379421611357, 0.06694628111620976, -0.13773540672994047, 0.0828232686042035, -0.0349394863121902, -0.026444790756012047, 0.0032123719978528944, -0.00883599880557011, -0.049173153965421816, 0.06942389612970525, 0.005767416866018384, 0.0013217655521432223, -0.05991792930357331, 0.05208056288762618, -0.06815980101063218, -0.09875081925358621, 0.07230602447648694, 0.036734495631119225, 0.04664497176945013, 0.0946046018193623, -0.0014789869264358534, 0.04664497176945013, 0.013500495379352039, -0.08873921679247739, -0.0029737749564120166, 0.03913626690780308, -0.13884780030158497, 0.058754966538909006, -0.012640912990402068, -0.02978199157638166, 0.004180982347044504, -0.05223225438233669, -0.032866376589806436, 0.037214848680129836, -0.1347015768357302, 0.05541776370799579, -0.02319607798125265, -0.015952833235349, -0.0848458151651983, 0.0565807284832037, 0.010529880394340675, -0.024940524138792712, -0.04613933412392962, 0.04191727094235045, 0.06735078640732148, -0.03681033936793087, -0.047453988383304425, 0.04464770578387801, 0.06780586491254026, -0.005508277899402461, 0.02286741240586533, -0.09516079860518453, -0.07250828114313003, -0.09162134312871545, 0.0480101891902139]}}, {'id': 'https://huggingface.co/papers/2410.11096', 'title': 'SecCodePLT: A Unified Platform for Evaluating the Security of Code GenAI', 'url': 'https://huggingface.co/papers/2410.11096', 'abstract': "Existing works have established multiple benchmarks to highlight the security risks associated with Code GenAI. These risks are primarily reflected in two areas: a model potential to generate insecure code (insecure coding) and its utility in cyberattacks (cyberattack helpfulness). While these benchmarks have made significant strides, there remain opportunities for further improvement. For instance, many current benchmarks tend to focus more on a model ability to provide attack suggestions rather than its capacity to generate executable attacks. Additionally, most benchmarks rely heavily on static evaluation metrics, which may not be as precise as dynamic metrics such as passing test cases. Conversely, expert-verified benchmarks, while offering high-quality data, often operate at a smaller scale. To address these gaps, we develop SecCodePLT, a unified and comprehensive evaluation platform for code GenAIs' risks. For insecure code, we introduce a new methodology for data creation that combines experts with automatic generation. Our methodology ensures the data quality while enabling large-scale generation. We also associate samples with test cases to conduct code-related dynamic evaluation. For cyberattack helpfulness, we set up a real environment and construct samples to prompt a model to generate actual attacks, along with dynamic metrics in our environment. We conduct extensive experiments and show that SecCodePLT outperforms the state-of-the-art (SOTA) benchmark CyberSecEval in security relevance. Furthermore, it better identifies the security risks of SOTA models in insecure coding and cyberattack helpfulness. Finally, we apply SecCodePLT to the SOTA code agent, Cursor, and, for the first time, identify non-trivial security risks in this advanced coding agent.", 'score': 12, 'issue_id': 121, 'pub_date': '2024-10-14', 'pub_date_ru': '14 октября', 'data': {'desc': 'Статья представляет SecCodePLT - новую платформу для оценки рисков безопасности генеративных моделей AI для кода. Авторы разработали методологию создания данных, сочетающую экспертную оценку и автоматическую генерацию, а также внедрили динамические метрики оценки. SecCodePLT позволяет оценивать как потенциал моделей генерировать небезопасный код, так и их полезность для кибератак. Эксперименты показали превосходство SecCodePLT над существующими бенчмарками в выявлении рисков безопасности современных моделей кодогенерации.', 'categories': ['#code', '#benchmark', '#dataset', '#rlhf'], 'emoji': '🛡️', 'title': 'SecCodePLT: Новый стандарт оценки безопасности AI-кодогенераторов', 'embedding': [0.08386529588367087, 0.02522823610500497, 0.17837106110636208, -0.0017296143234627663, -8.184383661526759e-05, -0.05694945368100336, 0.04402069650169448, -0.014930997095150975, 0.040960130865253366, -0.008488071303535995, 0.006492981470993424, -0.09839584414627313, -0.040445266260447255, -0.1122971205256148, -0.01367244483482687, -0.1077777697187723, -0.0005179904516883597, -0.08026126276108465, -0.07791577765192109, -0.04219007812078776, 0.04384907921366058, 0.0584082282341511, 0.04158940487196469, 0.03644078430531233, -0.019822185359400283, -0.059151918013119816, 0.03418111421051791, 0.0021166548387192073, 0.08878508490580843, -0.0033966588353574055, 0.06601674543532297, -0.027330588228984244, -0.09513505197010692, -0.02894668287639786, 0.031950044873611735, 0.03761352792161948, 0.043706060560064194, -0.058208005941327474, 0.035153631287099966, 0.07185184789481536, 0.03686983601920003, 0.05869426341789314, -0.03106333750277199, 0.08815580877564637, -0.0690487081904251, 0.015045410743957646, 0.08443735775735202, -0.17402334032825786, -0.020222633130223627, 0.04530785164335758, -0.10903631773219494, 0.01915000490722975, 0.03918671400012315, -0.05068529950762328, -0.051800833114351005, 0.07105095235316866, -0.06206947136631674, 0.023082977535566485, -0.06899150030429639, -0.04147499122315802, 0.07053608987181327, -0.010168523920377832, 0.010876459035947458, 0.07025005681152198, -0.03386647614543688, -0.055519279885743886, -0.029776183422834265, -0.01613234146934638, -0.05134317639567359, 0.0035718547749073208, 0.014265966301589826, 0.10285797857318402, 0.05377447227230484, -0.07991802181466465, -0.04982719607984788, -0.019621960943125925, 0.05832241959013414, 0.11429935194765398, 0.006346389049817712, 0.008337903416020373, 0.017819944381832816, -0.0045050421464405245, -0.02830310530556631, 0.029432942476414252, -0.09267515533558741, -0.002399113706741047, -0.13947038866033318, -0.07402570722214209, -0.1288299044571577, -0.01623245367748356, -0.07871667107011705, 0.06624557485638703, 0.030548476083141974, 0.0950206340743988, 0.03850023210728313, 0.06590232966306557, 0.037184476207731784, -0.051486195049269975, 0.07820181071221238, 0.10566111827757425, -0.003511072484164076, -0.01963626450724615, 0.06681763885351893, 0.03615474912157029, 0.10480301697324959, -0.010068411712240655, 0.01459490686906571, -0.015560273225313028, -0.006400019983190992, -0.022696832267137996, -0.09444856158346399, -0.05054228085402689, -0.13878389827369023, 0.01679022048084742, -0.024098398934157026, -0.037413301381894394, -0.013794010265693652, -0.004755322454438397, 0.09965439215969576, 0.04150359622794774, -0.021080737619724385, 0.09055849540058646, 0.05720688385995568, -0.049026298414750466, 0.07682884692653234, -0.05251592001198074, 0.01097657060704942, -0.07127977752733126, 0.006139013913208611, -0.04219007812078776, -0.04725288579652171, 0.008745502544213687, 0.0732248116804954, 0.0542893326302095, -0.03961576571401085, 0.020122521983811814, -0.08455177352960944, 0.0035146479505039855, -0.0561485602628074, 0.03612614836368203, -0.05769314558342282, 0.06984960373207179, -0.06452936587738552, 0.12413894060918275, -0.11275476662703857, -0.10302959586121793, -0.04087431797433495, 0.07963198450747186, -0.03512502840576099, 0.20914837510437106, -0.017705531794751515, -0.011806070516450608, -0.02266822726234828, 0.008109076118407034, -0.04510762935053395, -0.04104594163272105, -0.08535267119470685, 0.1290587275078696, -0.022139065463774137, -0.05200105753062536, 0.05065669450283357, 0.14736493255144414, -0.07946035872563503, 0.05995281355476651, 0.025528572729416502, 0.07253832978765537, -0.06801898959806653, -0.052487315007191025, -0.11899031579562892, 0.009961149208458876, 0.04470717839453451, -0.0317212175759984, 0.01079779898881452, 0.012385290967234217, 0.031120546450626056, -0.07076491716942662, -0.12917314752702844, 0.002048721615296143, -0.026129244916514204, -0.11664483705681755, -0.06859105571864915, 0.03332301078274251, -0.05703526232502032, 0.0721950909646861, -0.029718977660156295, -0.10875029104225584, 0.0664743957836482, 0.008616787454737523, -0.052372903481835084, 0.010790648905514993, -0.03232188870137073, 0.02630086538972421, 0.12047769747701711, 0.05654900697190538, 0.09587873750217418, -0.10537508309383221, 0.020565875138369005, 0.05077111239854169, 0.004580126090198335, -0.12745694067147767, 0.05088552392389764, -0.06281315902183474, -0.001681346164909205, -0.005767169023724248, 0.024355831236560082, 0.01034014460593291, -0.02860344192997784, -0.06304198844289881, -0.05832241959013414, 0.05303078249333612, 0.05071390238896226, 0.012185066126269715, 0.07065049715026775, 0.005520464394411359, 0.01975067603260209, 0.010082713152910146, 0.03335161578753223, 0.07831622860792052, 0.07522705371978822, 0.04593712989697036, -0.07871667107011705, 0.10039808830901668, 0.00705074869904743, -0.015731893698523035, 0.011877578993868508, 0.08672563498038689, -0.003105261089140719, -0.05837962747626284, 0.12985962941986848, -0.12265155892779457, 0.021681408745096723, -0.006664601944203429, -0.07871667107011705, -0.09353325876336281, -0.0559197287182926, 0.02103783329771591, 0.0559197287182926, -0.007830192716725103, 0.013150432694862105, -0.009868187295966297, 0.023140185421695183, 0.03910090535610619, 0.004033085261164059, -0.030777305504206046, 0.02808857944862246, 0.049512560138217594, 0.003589731894261796, -0.03981599225373593, 0.018649443866543863, 0.03852883498862211, 0.025800306472489053, -0.0040759904325528284, 0.12860108140644583, -0.017834246884227675, -0.014673565642128211, -0.04493600781559858]}}, {'id': 'https://huggingface.co/papers/2410.10626', 'title': 'Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts', 'url': 'https://huggingface.co/papers/2410.10626', 'abstract': 'Adapting medical Large Language Models to local languages can reduce barriers to accessing healthcare services, but data scarcity remains a significant challenge, particularly for low-resource languages. To address this, we first construct a high-quality medical dataset and conduct analysis to ensure its quality. In order to leverage the generalization capability of multilingual LLMs to efficiently scale to more resource-constrained languages, we explore the internal information flow of LLMs from a multilingual perspective using Mixture of Experts (MoE) modularity. Technically, we propose a novel MoE routing method that employs language-specific experts and cross-lingual routing. Inspired by circuit theory, our routing analysis revealed a Spread Out in the End information flow mechanism: while earlier layers concentrate cross-lingual information flow, the later layers exhibit language-specific divergence. This insight directly led to the development of the Post-MoE architecture, which applies sparse routing only in the later layers while maintaining dense others. Experimental results demonstrate that this approach enhances the generalization of multilingual models to other languages while preserving interpretability. Finally, to efficiently scale the model to 50 languages, we introduce the concept of language family experts, drawing on linguistic priors, which enables scaling the number of languages without adding additional parameters.', 'score': 12, 'issue_id': 121, 'pub_date': '2024-10-14', 'pub_date_ru': '14 октября', 'data': {'desc': 'Статья посвящена адаптации больших языковых моделей (LLM) для медицинских целей в различных языках. Авторы создают качественный медицинский датасет и исследуют внутренние механизмы многоязычных LLM с использованием модульности Mixture of Experts (MoE). Они предлагают новый метод маршрутизации MoE с языково-специфичными экспертами и кросс-языковой маршрутизацией. На основе анализа информационного потока разработана архитектура Post-MoE, применяющая разреженную маршрутизацию только в поздних слоях модели.', 'categories': ['#nlp', '#dataset', '#multimodal'], 'emoji': '🏥', 'title': 'Многоязычные медицинские LLM: преодоление языковых барьеров в здравоохранении', 'embedding': [0.027926801633037828, 0.037439522828982434, 0.09742590695806506, 0.04262383315726325, -0.0643147909544701, 0.036754801643578285, -0.001386252783215639, 0.02526128521715111, -0.1666804626504423, 0.07199343841285567, 0.02188659184413999, -0.12853175477978285, -0.07013491496699086, 0.05805449134691886, 0.026166093189501416, -0.09781717505193133, 0.023292713830252353, -0.033233384756505474, 0.0010989148472907325, 0.08304677621254514, 0.07551485934220481, 0.015345069052049175, 0.04494699201215513, -0.0058048377403988796, 0.05252782244707413, -0.05986410729161947, 0.09145905842091352, 0.05184310328280815, -0.002925344640333566, -0.03726834253263139, 0.07786246790540248, -0.026875268125487663, -0.06720039819957929, 0.07717774671999832, -0.07629739957221368, 0.036608077118947514, 0.018597490640442544, 0.051060563052799275, 0.05546233517220937, 0.04961775943024468, -0.020492697743887754, -0.08739964487306734, -0.03986049870734088, 0.0711619906816826, -0.036241264796801495, 0.04340637136613396, 0.07844936600392557, 0.006810520314022237, 0.02870933984190854, 0.052625639470540705, -0.029002792933446406, 0.05365271922750876, -0.006296980435538207, -0.05917939014849165, -0.01812063138783167, -0.09004070652780288, 0.1042731046641394, 0.022644674281290442, 0.023133761419761447, -0.02483333447627352, 0.028146891451691226, -0.10505564489414826, 0.005728418304504644, -0.016164286876223965, 0.002041933649214145, 0.014000083059302594, -0.06905892164544411, -0.04000722323197166, 0.03037223126197838, 0.035165275517531076, -0.039738225912154056, -0.037048252713978, 0.08285114822902646, -0.026752997351438993, -0.030249960487929706, -0.0623584464428624, -0.13479206853530118, 0.07590612743607109, -0.009249836808487987, -0.04538716760718561, -0.009567741831583614, -0.015650748008309012, 0.06852092902694706, -0.041181029534708646, 0.017044642108561246, -0.034015922965376186, 0.03673034587185803, 0.02496783313618232, -0.09292631377291206, -0.134009524263016, 0.022241180322702194, -0.05590251480951618, -0.011132817036642147, 0.04428672659847125, 0.08011226550854805, 0.04876185996962766, -0.012728459935212847, -0.059472843240029515, 0.10124077491553758, 0.12647761143495198, 0.05243000542360757, -0.0421347480399304, 0.027315445741656307, -0.12882522404042598, 0.08930708188351084, -0.10212112610559855, -0.03548317831737473, -0.05076710996126141, -0.07287379364519296, 0.019111029508357493, -0.021972180981746428, 0.035776631408912594, 0.05252782244707413, 0.03543427081621052, 0.09072543175548334, -0.0421347480399304, -0.10055604968785713, 0.0020021952434206946, 0.015393977968010092, 0.015308387415606941, -0.012233260066494674, 0.03460282712731376, 0.06690694915031774, -0.010882160737159474, 0.11943476755511556, -0.06294534858366181, -0.004793041628073089, 0.004410942831691383, -0.003359408515685958, 0.002263550639860258, 0.010570368646424832, 0.036925981939929325, 0.09679009327382512, -0.09185032853591796, -0.13665058591775153, 0.016800098539325743, -0.061233549662427764, -0.015993108601011084, -0.030836864649867284, -0.048297228602876906, 0.021311915568062546, -0.026043821404883667, 0.004401772725751548, -0.028000166927060453, -0.049911212521782544, -0.01695905297095481, 0.0019318891441150764, 0.048712952468463445, -0.05438634993521527, 0.06543968369262841, -0.0648527855941053, -0.10593599608420923, 0.0021840740303871735, 0.10730543845501753, -0.09811062410119288, -0.02187436395827986, -0.025579191048702002, 0.037048252713978, 0.12667323739733247, -0.0313993130400846, -0.01311972782696531, 0.11200066366596549, 0.010172985801173227, -0.040447400848140296, 0.0643147909544701, 0.05546233517220937, -0.10896832987508735, -0.032719841846314206, -0.11082685736322849, -0.15083407655292383, -0.07615066494189211, -0.039933859959087194, -0.07893846122694921, -0.02741326276512288, -0.028782703114793008, -0.06294534858366181, 0.0041664004751387745, -0.15063845059054334, 0.02861152281844197, 0.009763376686972016, -0.03389365017018935, 0.11053340225055247, -0.0701838204470169, 0.08691055773459633, -0.026801905863172278, 0.0023139873341553353, -0.039102414249052274, 0.08876908724387562, 0.04122993905701102, -0.026215002711803784, -0.015968652829290823, 0.11601116769150932, 0.10319712144828345, 0.05629378492452061, 0.10172985801173226, 0.014953800351841448, 0.14936681917978714, 0.05570688076258303, -0.02314598930562158, -0.04037403555411767, 0.003747620244428651, 0.03445609856040667, 0.0014114713324769935, -0.1847766342242774, 0.04147448262624652, 0.024221976563753845, -0.050278024843928563, 0.07737338278806963, 0.06045101145355705, -0.035263092540997645, 0.03638798730029411, 0.07756901885614091, 0.13870474947396394, 0.046120798314892124, 0.01571188339533335, 0.039151321750216475, -0.11708715494964157, 0.026924175626651867, 0.09434466768716089, 0.0029314583811498153, 0.028440344543229093, -0.02858706704672171, 0.06514623868564318, -0.0005574814431923104, -0.03293993368610577, -0.0211774189292919, 0.03418710124058906, -0.09884425076662308, 0.06172263477976061, -0.04929985258812471, 0.06597768035340178, -0.037855246694568974, 0.02447874801884948, -0.08808436201619517, 0.11366355508603533, -0.022889217850525945, 0.07854718909080662, -0.0009063372463267051, 0.007110085933693458, -0.029149517458077184, 0.03487182242599321, 0.039567047636941174, 0.023720664571129944, -0.07874282515887791, -0.0023094021801285105, 0.058396853960759096, -0.030445594534862848, -0.056000331832982746, 0.06054882847702362, -0.004579066358691201, -0.04656097390992261, -0.08896472129080878, -0.0007668713564335565, -0.08691055773459633, -0.0565383244514798, 0.07571049338913795]}}, {'id': 'https://huggingface.co/papers/2410.09704', 'title': 'EchoPrime: A Multi-Video View-Informed Vision-Language Model for Comprehensive Echocardiography Interpretation', 'url': 'https://huggingface.co/papers/2410.09704', 'abstract': 'Echocardiography is the most widely used cardiac imaging modality, capturing ultrasound video data to assess cardiac structure and function. Artificial intelligence (AI) in echocardiography has the potential to streamline manual tasks and improve reproducibility and precision. However, most echocardiography AI models are single-view, single-task systems that do not synthesize complementary information from multiple views captured during a full exam, and thus lead to limited performance and scope of applications. To address this problem, we introduce EchoPrime, a multi-view, view-informed, video-based vision-language foundation model trained on over 12 million video-report pairs. EchoPrime uses contrastive learning to train a unified embedding model for all standard views in a comprehensive echocardiogram study with representation of both rare and common diseases and diagnoses. EchoPrime then utilizes view-classification and a view-informed anatomic attention model to weight video-specific interpretations that accurately maps the relationship between echocardiographic views and anatomical structures. With retrieval-augmented interpretation, EchoPrime integrates information from all echocardiogram videos in a comprehensive study and performs holistic comprehensive clinical echocardiography interpretation. In datasets from two independent healthcare systems, EchoPrime achieves state-of-the art performance on 23 diverse benchmarks of cardiac form and function, surpassing the performance of both task-specific approaches and prior foundation models. Following rigorous clinical evaluation, EchoPrime can assist physicians in the automated preliminary assessment of comprehensive echocardiography.', 'score': 10, 'issue_id': 127, 'pub_date': '2024-10-13', 'pub_date_ru': '13 октября', 'data': {'desc': 'EchoPrime - это многозадачная модель искусственного интеллекта для анализа эхокардиограмм, обученная на более чем 12 миллионах пар видео-отчетов. Она использует контрастное обучение для создания единой модели вложений для всех стандартных проекций в комплексном эхокардиографическом исследовании. EchoPrime применяет классификацию проекций и анатомическую модель внимания для точной интерпретации взаимосвязи между эхокардиографическими проекциями и анатомическими структурами. Модель превосходит существующие подходы в 23 различных задачах оценки формы и функции сердца, что было подтверждено в двух независимых медицинских системах.', 'categories': ['#cv', '#multimodal', '#dataset', '#benchmark', '#rag'], 'emoji': '❤️', 'title': 'EchoPrime: революция в автоматизированной интерпретации эхокардиограмм', 'embedding': [0.048974926672118446, -0.0587892518992495, 0.05390626515805745, 0.06633129271963487, 0.03961989672157691, 0.024185297139047494, -0.0008589044710247132, 0.10278449639259617, -0.008980348214280662, -0.012213515853519858, 0.08069018474727875, -0.0659445210342087, -0.022493173279908876, -0.04723445509418997, 0.012896408761375112, 0.002447537942582958, -0.06183507766386899, -0.07203616853749056, 0.05443807396091757, 0.04295579873404283, -0.012509637075948944, -0.055501695592594893, 0.09456560965191677, -0.04252068134280535, 0.09611269236766433, -0.09848166416007556, -0.04904745026328179, 0.036332340414922296, -0.0017767310787146094, -0.021175733170009682, 0.030216515026288405, -0.02094608855118275, -0.014721485827718034, -0.0332381689444915, -0.03732344046841483, -0.03099005839714074, -0.07527538719024804, 0.05898263874845187, -0.06347885581719628, 0.046460913736316174, 0.19618980105312717, 0.03340737790834183, 0.04288327715585803, 0.032851397259065336, -0.0745985291920827, -0.09214828410178001, -0.02779919350210876, -0.07421176153261362, -0.03464021454280513, 0.12241314684685828, -0.0998837137843463, 0.1288915700615234, -0.07687081359882839, -0.014733573160011209, 0.0125217236030507, -0.004106424506409775, -0.02003959398610466, 0.025333526272117806, -0.06425239516209154, 0.1318890460943745, 0.05361618287127537, -0.07029569897254064, 0.04421280721492252, 0.11535456912352172, -0.14629628584080825, -0.05685539548509721, -0.0423756401994143, 0.06961885305224656, 0.015821367241998483, -0.12976181088293404, 0.05173067417591294, 0.04145705971112802, 0.017984870287956256, 0.06517098571527055, 0.060529723477177985, -0.024185297139047494, -0.017972784364748064, 0.16592493025613472, 0.10355804378940561, 0.010243398851573023, 0.04559067895384121, 0.011192196966049752, -0.011820701206489596, -0.0072157037834465225, -0.02242065371470263, 0.017271758546123426, 0.06753995549470324, -0.08900576893851646, -0.036791631665554714, -0.020607664584546447, -0.07063412897811258, -0.044309498626545145, -0.03120761709275948, -0.032005332310028195, 0.027291556545665032, 0.15171108742379605, -0.020051679909312854, -0.04788713319402474, 0.01620813892742465, 0.10858606563505305, -0.053132719774226574, 0.03050659328711339, -0.03986162726361203, -0.02828265861213611, 0.08919915176176171, -0.09055285366724257, -0.049434217922750864, 0.028016752197727507, 0.06652468359479431, 0.07807947234793966, -0.04921665922713212, -0.040659342480880746, -0.10713567634390665, 0.017162979198314058, 0.02646966746900138, -0.0847512803988286, -0.07010232017525247, -0.0888607318210825, -0.1090695387969946, 0.0056232934206261465, -0.0891508060559504, 0.05559838700421752, 0.07604892049620764, -0.05289098721921289, 0.08320419969605958, -0.07208452028223752, -0.008382061600031267, 0.01651030512443638, -0.07474357033547374, -0.08354261561078169, 0.028234310893346246, 0.10462166139512584, 0.06154499940304399, -0.014818179252319213, -0.03374580590093523, 0.015265383170658451, 0.02738824997026621, 0.04762122879259468, -0.08286577170346618, -0.009602808486627135, 0.04679933770295248, 0.08160876724854359, -0.03930564258837843, -0.00502802788458309, -0.04520390726841504, 0.02526100670691156, -0.03993414682881828, -0.013138141316388784, -0.03609060785990862, 0.02891116285257595, -0.03872548807970701, 0.00917373385569589, -0.03398753644297036, -0.11777188662174427, -0.04660595085375012, -0.014322628219083672, -0.017078375722878167, 0.0368158035119711, -0.04994185689217315, 0.028886987986691745, -0.0293221063844185, 0.09369537486944178, 0.07237459854306251, 0.042641544600844364, -0.12105944695435597, 0.013669951327036028, -0.0546798045029527, 0.0029899241425266434, -0.07667743278856168, -0.07643569419461237, -0.1472632100219273, 0.003538353907967989, -0.04757288308678337, -0.05753224341836982, 0.03304478209528914, -0.05994956494254946, -0.045034694278607616, -0.022964550453749483, -0.06376893206504272, -0.08828056926049689, -0.05579177385341988, 0.04549398351626149, -0.042327294493602986, 0.08165711094137634, 0.01571258789418911, 0.04411611177734279, -0.079143093979617, -0.0519240610251153, -0.024281990563648676, -0.021103213604803436, -0.009820367383543731, 0.13498321555182674, 0.06691145528022048, 0.10249442618368537, 0.053036028362603946, 0.00724592020184984, 0.02726738469924865, 0.06101318657422678, 0.021671286216223773, -0.07358326333110943, 0.11583803222057053, 0.06115822771761783, -0.04203721824575654, -0.03609060785990862, 0.038314542534885906, 0.02598620336546331, 0.07082750777540074, 0.0217679776278464, -0.10326795948964498, -0.01379081740324501, 0.04254485318922173, -0.015108257513144202, 0.10423489373565677, 0.02646966746900138, -0.14881030481554616, -0.05975618010632565, -0.017936524582144942, -0.0067141097886068875, 0.024076517791238122, 0.04215808150379556, -0.01580928131879029, 0.0018568048290408283, 0.0995936355235213, 0.005741137613437371, -0.09998040720894746, 0.014564360774097347, 0.11051993010111955, -0.05472815020876401, 0.007650821778577571, -0.09751473997895652, 0.042327294493602986, 0.03297226051710435, -0.08518639980304464, -0.0007535243595718955, 0.0423756401994143, -0.05081209368762666, 0.08803884475739741, -0.043439261831091626, 0.028234310893346246, -0.036332340414922296, -0.022323962303079998, 0.023834787249202998, 0.05380956972047773, -0.029177067254006008, -0.11429094346588729, 0.07662908506977181, -0.019024318060238653, -0.10123741568982715, -0.043245876994867816, 0.08726529736058797, 0.04631587460590368, -0.0605780691829893, -0.011730051347386077, 0.005747181380232886, 0.04235146835299791, 0.004604997020447362]}}, {'id': 'https://huggingface.co/papers/2410.10814', 'title': 'Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free', 'url': 'https://huggingface.co/papers/2410.10814', 'abstract': 'While large language models (LLMs) excel on generation tasks, their decoder-only architecture often limits their potential as embedding models if no further representation finetuning is applied. Does this contradict their claim of generalists? To answer the question, we take a closer look at Mixture-of-Experts (MoE) LLMs. Our study shows that the expert routers in MoE LLMs can serve as an off-the-shelf embedding model with promising performance on a diverse class of embedding-focused tasks, without requiring any finetuning. Moreover, our extensive analysis shows that the MoE routing weights (RW) is complementary to the hidden state (HS) of LLMs, a widely-used embedding. Compared to HS, we find that RW is more robust to the choice of prompts and focuses on high-level semantics. Motivated by the analysis, we propose MoEE combining RW and HS, which achieves better performance than using either separately. Our exploration of their combination and prompting strategy shed several novel insights, e.g., a weighted sum of RW and HS similarities outperforms the similarity on their concatenation. Our experiments are conducted on 6 embedding tasks with 20 datasets from the Massive Text Embedding Benchmark (MTEB). The results demonstrate the significant improvement brought by MoEE to LLM-based embedding without further finetuning.', 'score': 10, 'issue_id': 124, 'pub_date': '2024-10-14', 'pub_date_ru': '14 октября', 'data': {'desc': 'Исследование показывает, что маршрутизаторы экспертов в моделях Mixture-of-Experts (MoE) LLM могут служить готовой моделью для создания эмбеддингов с многообещающей производительностью на разнообразных задачах, не требуя дополнительной настройки. Анализ демонстрирует, что веса маршрутизации MoE (RW) дополняют скрытое состояние (HS) LLM, широко используемое для эмбеддингов. Предложенный метод MoEE, объединяющий RW и HS, показывает лучшую производительность, чем использование каждого по отдельности. Эксперименты проводились на 6 задачах эмбеддинга с 20 наборами данных из Massive Text Embedding Benchmark (MTEB).', 'categories': ['#nlp', '#benchmark', '#dataset'], 'emoji': '🧠', 'title': 'MoE LLM: Скрытый потенциал эмбеддингов без дополнительной настройки', 'embedding': [0.08637327886249895, -0.029413892819731103, 0.07169302301517033, -0.020312133779083162, 0.0006956438757852448, 0.06630136326102971, 0.1161608540933941, 0.08968301036660775, -0.025743828857898946, 0.15598437362763748, 0.03472547361843745, -0.057333066377410924, -0.04900535960684655, 0.024395913400233542, -0.0022153839698459093, -0.0913912603585175, -0.0069397570264843, -0.019578120986716734, 0.03336421651341531, 0.06982463380108093, 0.04529526032033924, -0.015214082671974792, -0.014813711774794786, 0.06544724241811681, 0.0715328796399487, 0.012985352901730167, 0.05012639645773378, 0.09859793027819463, 0.04230582387275766, -0.03541944900960773, 0.09875807157689528, -0.03822204632812826, -0.04713696208667319, 0.02687821358570588, -0.06683519527697837, 0.07158626284154357, 0.02666468077932642, 0.020632430912131367, 0.0028359582894316466, 0.06101647821616278, 0.04457459125185054, -0.03226986918680452, -0.055571435260427286, 0.05135419971181074, -0.0013662646353880203, 0.03226986918680452, 0.06026911754687664, 0.0480978534858178, 0.03379127797313225, 0.10292193223000093, -0.030508238069820896, 0.08343722976955423, 0.04105133109440426, -0.04121147862266787, -0.007934010434800512, -0.024943087063538937, 0.02105949237184831, -0.03325745011022557, -0.05242185543762408, 0.014706946409865551, 0.07660423603147816, -0.09160478901185498, 0.0017733080174631868, -0.010676548952049541, 0.04940572946576606, -0.04163854008238481, -0.1011603011130173, 0.012551618334161766, -0.007346800200907368, -0.0024122327606841344, -0.016949021149900496, -0.058827785639462206, 0.07772528118844935, -0.027065051676506427, -0.03365781996914508, -0.08306355358795313, -0.011897678890622942, 0.023541790480799657, -0.011383870558922737, -0.027518805775281298, -0.010402962120396845, -0.0473771844173291, 0.022033727494870644, -0.04142500935252634, -0.003673400100453684, -0.08183574410431321, 0.023915468738921735, -0.025303421597783288, -0.10756622716181344, -0.08920256570529593, -0.0483380758164737, -0.09881145893153212, -0.023568480005076102, -0.030828535202869097, 0.06213751506705, 0.05925484709917914, -0.03325745011022557, -0.11178346478695098, 0.03307061098116454, 0.12149912649289789, -0.02003187404723111, -0.05711953564755245, -0.045402022570486984, -0.028106014763261845, 0.10585797924642468, -0.12139235801318718, -0.06897050257556309, -0.03387135069900356, -0.10046632156880504, -0.025129926192599975, -0.05535790453056881, 0.02422242007157122, -0.04137162615093147, 0.04908543440923885, 0.10516400800829637, -0.04462797652996639, -0.0712659615554534, 0.022367370428317563, -0.07788542248714998, 0.03843557705798674, -0.04572232178005618, -0.014640217823176167, 0.08882888952369485, -0.043800541058287956, 0.04849822334473731, 0.05952176103063248, -0.05306244970372049, -0.014146427984421938, 0.008167560280561256, -0.012378124174891047, 0.04601592315926497, 0.04214567634449405, 0.08455826454392046, -0.03827542537668115, -0.11391877623857768, -0.03469878201764002, -0.11690820853311727, 0.04887190367938038, -0.04759071514718756, -0.024195729509034283, 0.007373491386400603, -0.03742130245724727, -0.006118996946830411, -0.03635364880795491, -0.08562592442277578, -0.013132155460203453, -0.02312807274496044, 0.01637515588579767, -0.06865020751903587, 0.019578120986716734, -0.06187060113559667, -0.15235434499048048, -0.07729821557569043, 0.08274325022534196, -0.1459484230947263, -0.08914918458022206, -0.057599980308864256, 0.003303057357996839, 0.09203184631852994, -0.0639525293856285, 0.05888116468801509, 0.10745946698818667, -0.014600181044936314, 0.034218341509370184, 0.06304502326459974, 0.13078773089216986, -0.06646151701885629, 0.017883220948247673, -0.05589173446999648, -0.1363395298684901, -0.03168265812230298, 0.011377197658723378, -0.09107096530199028, -0.040997947892809396, 0.03485893162242461, -0.04665651950188236, -0.026144198716818463, -0.13420422256990538, 0.06913065425686866, -0.03960999503394785, -0.036754018666874416, 0.11199699344028846, -0.06870359072063074, 0.07596364799494473, -0.08370414162448657, 0.051861335973919974, 0.03216310382187529, 0.07292083457533126, 0.06614122196232908, -0.021486553831565253, -0.044601282852647965, 0.12053823509375328, 0.12939977180374532, 0.05695938811928884, 0.0443076773203972, 0.033417597638489185, 0.12705092754573916, 0.0513275081110133, 0.01088340781996915, -0.07425540215607696, -0.012204630223272428, 0.023221495424272442, 0.023782011773195066, -0.13591247256181516, 0.024809630097812268, 0.06790285100279171, -0.0025990723050493716, 0.09000330749965595, -0.0646465006237568, 0.06827652926091379, 0.0550376094740416, 0.036567181614334365, 0.2018935324602337, -0.018750689668080276, -0.06555400882130655, 0.03128228826338347, -0.0746824636157939, 0.004497496318729103, 0.057333066377410924, 0.08840182806397791, -0.07030507015630878, 0.009228542483218952, 0.0586676360346776, 0.005451712741153364, -0.11989764705721984, 0.01275847616382088, -0.0527421504941513, -0.10975491973851402, 0.02765226170274747, -0.02166004923674857, 0.020725850476661888, -0.045508791050197714, -0.08978978092283946, -0.09875807157689528, 0.026611297577731553, 0.03392473390059842, -0.032696932723042464, -0.028426311896310046, 0.009268579261458803, -0.014159773784820653, 0.018443738335430792, 0.012778494033810558, 0.023168110146156586, -0.10783314524630876, -0.07943352495079613, 0.06427282444215571, -0.022834469289230656, -0.07873954540658387, 0.000495875662557637, -0.018190170204376167, 0.012618345882590655, 0.034672094569884564, 0.016281737359527648, -0.06534048224449004, -0.01533419300669435, 0.09229876647954625]}}, {'id': 'https://huggingface.co/papers/2410.11795', 'title': 'Efficient Diffusion Models: A Comprehensive Survey from Principles to Practices', 'url': 'https://huggingface.co/papers/2410.11795', 'abstract': 'As one of the most popular and sought-after generative models in the recent years, diffusion models have sparked the interests of many researchers and steadily shown excellent advantage in various generative tasks such as image synthesis, video generation, molecule design, 3D scene rendering and multimodal generation, relying on their dense theoretical principles and reliable application practices. The remarkable success of these recent efforts on diffusion models comes largely from progressive design principles and efficient architecture, training, inference, and deployment methodologies. However, there has not been a comprehensive and in-depth review to summarize these principles and practices to help the rapid understanding and application of diffusion models. In this survey, we provide a new efficiency-oriented perspective on these existing efforts, which mainly focuses on the profound principles and efficient practices in architecture designs, model training, fast inference and reliable deployment, to guide further theoretical research, algorithm migration and model application for new scenarios in a reader-friendly way. https://github.com/ponyzym/Efficient-DMs-Survey', 'score': 10, 'issue_id': 124, 'pub_date': '2024-10-15', 'pub_date_ru': '15 октября', 'data': {'desc': 'Эта статья представляет собой обзор диффузионных моделей, одного из самых популярных и востребованных типов генеративных моделей последних лет. Авторы предоставляют комплексный анализ принципов работы и эффективных практик применения диффузионных моделей в различных задачах, таких как синтез изображений, генерация видео и молекулярный дизайн. Особое внимание уделяется архитектуре моделей, методам обучения, быстрому выводу и надежному развертыванию. Статья призвана помочь исследователям и практикам быстро понять и применить диффузионные модели в новых сценариях.', 'categories': ['#cv', '#video', '#multimodal', '#benchmark'], 'emoji': '🌀', 'title': 'Эффективные диффузионные модели: от теории к практике', 'embedding': [0.08395502400454966, 0.0881918005160918, 0.10073488611836341, 0.012570955791380631, -0.022396369979509813, 0.012947248007120677, -0.07637343723901242, 0.019344218990730657, -0.06416483121137756, 0.061823457608774446, -0.010766144934731122, -0.10736877661072673, -0.06271541143121205, -0.0146196589371693, 0.055496171320643224, -0.020877263845850955, -0.0010705171447942684, -0.02171346899999753, 0.09750155869332268, 0.06572575040064338, 0.042562859547962094, 0.009595458133429569, 0.008264497485885262, -0.018521950485527277, -0.011839275607933948, -0.053768012344059035, -0.02288415580129909, 0.1692479953229017, -0.011449046743250703, -0.06483380072324224, 0.05379588356942718, -0.02172740461268161, -0.05438122697007796, -0.036486430868290176, -0.02401303265577105, 0.07732113143966808, -0.055468295950238576, 0.06962804111999406, -0.07146769121560556, 0.09270730586146142, 0.053600770484222425, -0.037545626550564405, 0.0038744184577234105, 0.10625384469527094, -0.04089044923966894, -0.02675857529528162, 0.0601510473004813, 0.0014502937611097122, -0.018521950485527277, 0.13245495196030643, -0.0643878217395052, 0.068457350173656, 0.05128726810698746, -0.10781476181201852, -0.05636024770015795, -0.05056255821690471, -0.06957229659673951, -0.11517336219446458, -0.05680622461137675, 0.0004237645311190751, 0.0881918005160918, -0.05421398925527784, 0.005679228485365618, 0.0230513976611357, 0.008229655759901356, -0.04495998181885624, -0.07509125310112881, 0.06288264914601217, 0.01718402182438911, 0.02464018807576967, -0.023121081942110808, -0.029908282826663156, 0.07007401803121287, -0.02164378471902242, -0.04089044923966894, -0.062994144410076, 0.007853362715154012, 0.012452492720974769, -0.016738044913170307, 0.042730099335280464, 0.07252688897039804, -0.011811401688292079, -0.0860734153690981, -0.06784413347511883, -0.10162683786828278, 0.011198184989754906, 0.03796372601886032, 0.00979754047573914, -0.0775441198952775, -0.15107445587965873, 0.0017290290197437283, 0.09850499948975114, -0.013874042312031244, 0.0888607700279565, 0.054297609148937026, -0.030521499525200328, 0.012557019142437433, 0.047607963770727936, 0.11628830861754809, 0.09694408444552181, -0.005463208872357376, -0.006703580576389514, 0.04208900726633865, 0.04214475593462968, 0.09588489083576583, -0.008362054650243117, -0.009992655011706673, -0.10034465994795388, -0.04618641521593059, 0.03932953005040313, -0.06366310770438596, -0.04788669882211013, 0.038437576227965516, 0.05226283975056601, -0.059983803368126434, -0.07040850175088609, -0.067007934538527, 0.005222799864633757, 0.021671658016908818, 0.04167090572552448, -0.09265156548324335, 0.097947531459505, -0.03336460077983153, 0.09081191538763185, 0.06750965597300035, -0.0772653806988588, 0.021755277910568, -0.1882019705268177, -0.028486732199347557, -0.016515056457560906, 0.07347458524357195, 0.01772755631446942, 0.10798199745430039, -0.07520274214763788, 0.014208523959186224, -0.0893067427941388, -0.0912021436305596, 0.015846093163250943, -0.06879183803836572, 0.01602727063577163, 0.004055596344747747, -0.0227029783287784, -0.07241539370633422, 0.09298604920291659, 0.015414053937234457, 0.008410833439673868, -0.0309117277681281, 0.041141306848128244, -0.021183870122601298, 0.07007401803121287, -0.0634401213212948, -0.04936399397268027, 0.002895361460211072, -0.07832458052616952, -0.03765711766959173, -0.13947906862307927, 0.05047893832324552, 0.11829519642795971, 0.011950769628486823, -0.002431964204129876, -0.04479273788650138, 0.11550785005917429, -0.1007906285690997, -0.010431663494827966, -0.04699474499967252, 0.017769365225039892, -0.15018250620225762, 0.03361545838829083, -0.06488954110146031, 0.010508314960389639, -0.0038395767317395044, -0.03381057147349559, -0.03311373695381748, -0.018786749924225396, 0.09014295209332188, 0.0022769176634064242, -0.06622747598015322, -0.08707686445559953, -0.02561575971934821, -0.07648492214048502, -0.08061020338796335, 0.12844115768681894, -0.012515208159348716, -0.028333428024713267, 0.020542782198695975, -0.025671508387639246, -0.07520274214763788, 0.025769064930241628, -0.005745428241414235, 0.005870859118162134, -0.05020020119934509, 0.09733431268844958, 0.051538129860483256, -0.021323238684551514, 0.0060067425334303865, 0.014536038629006466, 0.10848372096129197, 0.060095298632190264, 0.043064578909917196, -0.02937868394926692, 0.1192986475869794, 0.028626100761297776, -0.041336422005851255, -0.17036292723835747, -0.0451829682019474, -0.01825715311934741, 0.04769158366438712, -0.04944761801137595, -0.05254157687446645, -0.050646176038045644, 0.046771758616581366, 0.011093659604551366, 0.0956061578569019, 0.0012595344149836881, -0.0735303297668265, -0.0054353355744709815, -0.05663898067902189, 0.10486016529332348, 0.0551338143030836, 0.15018250620225762, -0.08133491742308259, 0.009442153337039802, 0.11483887847479135, -0.0019372092495868877, -0.02727423441495729, 0.017407010279998514, 0.016724109300486234, -0.048778650572029486, 0.03551085922471162, 0.0038709345545523913, 0.11188428610113282, -0.05959357512519867, -0.057865416148614485, -0.12654575685039815, 0.03146919787089248, -0.10251879376323862, 0.05507806563479257, 0.031775808292679314, 0.05909185161820708, -0.048778650572029486, -0.0003601780962271887, 0.023943349411055064, -0.019901690129754162, 0.024654123688453742, 0.019190915852355488, 0.08579468239023415, -0.08005273743023546, -0.03116259159414214, 0.008598979754795715, 0.038437576227965516, 0.03676516591967237, 0.0017778076019226558, -0.004348268459576784, -0.0427579747056851, 0.039273781382112086, -0.05585852419316635]}}, {'id': 'https://huggingface.co/papers/2410.10816', 'title': 'LVD-2M: A Long-take Video Dataset with Temporally Dense Captions', 'url': 'https://huggingface.co/papers/2410.10816', 'abstract': 'The efficacy of video generation models heavily depends on the quality of their training datasets. Most previous video generation models are trained on short video clips, while recently there has been increasing interest in training long video generation models directly on longer videos. However, the lack of such high-quality long videos impedes the advancement of long video generation. To promote research in long video generation, we desire a new dataset with four key features essential for training long video generation models: (1) long videos covering at least 10 seconds, (2) long-take videos without cuts, (3) large motion and diverse contents, and (4) temporally dense captions. To achieve this, we introduce a new pipeline for selecting high-quality long-take videos and generating temporally dense captions. Specifically, we define a set of metrics to quantitatively assess video quality including scene cuts, dynamic degrees, and semantic-level quality, enabling us to filter high-quality long-take videos from a large amount of source videos. Subsequently, we develop a hierarchical video captioning pipeline to annotate long videos with temporally-dense captions. With this pipeline, we curate the first long-take video dataset, LVD-2M, comprising 2 million long-take videos, each covering more than 10 seconds and annotated with temporally dense captions. We further validate the effectiveness of LVD-2M by fine-tuning video generation models to generate long videos with dynamic motions. We believe our work will significantly contribute to future research in long video generation.', 'score': 10, 'issue_id': 123, 'pub_date': '2024-10-14', 'pub_date_ru': '14 октября', 'data': {'desc': 'Статья представляет новый датасет LVD-2M для обучения моделей генерации длинных видео. Датасет содержит 2 миллиона видео длительностью более 10 секунд, снятых одним кадром и сопровождаемых плотными временными подписями. Авторы разработали методику отбора качественных видео и создания аннотаций. Эффективность датасета подтверждена экспериментами по дообучению моделей генерации видео.', 'categories': ['#dataset', '#video', '#benchmark', '#multimodal'], 'emoji': '🎥', 'title': 'LVD-2M: Новый стандарт для обучения генерации длинных видео', 'embedding': [0.03350044880537811, 0.036083366872931004, 0.01366363713911428, -0.06483124652043631, -0.03685824488593277, -0.00040802036008987945, 0.05971706848740797, -0.06643265416667757, -0.036729094229205984, 0.09680776727054301, 0.08415147824623206, -0.09530968447090965, -0.10166365911441048, -0.11943413697481595, 0.02474435621067566, -0.03822718567129231, 0.06700089761075621, -0.046931622324530534, -0.02027590553392231, 0.052252434926481975, -0.05563605681715551, -0.03799472529225032, 0.030168482856168802, -0.04651835534729754, 0.052278262897214094, -0.046879964222453066, -0.030788384402324915, 0.09060877341511431, 0.09520636394552823, -0.12697625876916477, 0.03135662568579031, -0.06415968471158949, -0.16654657237937723, -0.02792134353242607, -0.03980276966802795, 0.08642443905886718, 0.032467280281989, 0.050728511192437056, -0.05263987393298286, -0.03701321703155193, 0.06632933580190939, -0.046105088370064544, 0.03311301087918384, 0.020857064043673632, 0.012204287869187449, -0.018222488954349886, 0.004487820142373162, 0.029161146624738295, -0.028773707618237415, 0.06612270555421276, -0.04662167371206572, 0.07945056719043674, -0.02298797209758875, -0.01181039338243281, 0.0698421040280832, -0.033552106907455574, -0.015859117280472918, -0.08849077178441891, -0.0304526034979015, -0.0663809960646001, -0.005821251681171876, -0.0810519705154515, -0.020159675344401315, 0.02446023556894296, 0.018041684516772124, -0.033629594060571776, -0.10827593075013837, 0.03084004250440238, 0.04721574512687648, 0.0002899729219806846, 0.00580510844324967, 0.02965189967478087, 0.05054770891547254, 0.01589786139718433, -0.07051366583693003, -0.00749691960464776, 0.011203407766194674, -0.008988555134357678, -0.059820384691562915, -0.020211333446478786, 0.05429293968130159, 0.09055711315242361, -0.07557618792849415, 0.022458472770221515, -0.12325685381345458, 0.011196950341388996, -0.019191082149375604, -0.02934195106231606, -0.049514542552696675, 0.022277668332643753, -0.06824070070306844, 0.11488820022525302, -0.026862349198918094, 0.04512357794875291, 0.019333141389935332, 0.0032060471377745657, 0.05147755907409346, -0.0928300782862852, 0.0876642464724059, 0.07304492580240546, -0.08286001489122913, -0.007645437782442435, 0.019901384834013974, -0.0009072499712279558, -0.04904961531277297, -0.004691225026617482, 0.0012123572543821719, 0.05413796537506919, -0.031382453656522424, 0.0049204591779618626, -0.00708365284347609, -0.06137013423572671, -0.07221838968732623, 0.012572353520964676, 0.01873907213573782, -0.15332202046546844, -0.06793074560876389, 0.024641037845907477, 0.07381980381540723, 0.044942775671788386, 0.0023197834622700067, 0.10734608059151744, 0.05687585774885449, -0.09660113702284637, 0.10579632888674041, -0.04972117280039331, 0.05191665402205857, -0.05723746446339677, 0.037219847279248555, -0.056307612144162605, -0.01703434396411513, 0.0055532740613000574, 0.06483124652043631, 0.026578228557185397, -0.112098647588777, 0.019513945827513094, -0.15249547786854947, 0.005843852343899759, -0.058787218328787055, 0.10026888171586584, 0.009046670661240822, -0.08626946259202153, 0.020133847373669207, 0.07857237297328003, 0.007438804077764614, -0.018777814091835987, -0.009033756243752118, 0.007690638891836876, -0.06359144342812408, 0.06648431226875504, -0.10899914850044942, -0.03796889732151821, 0.009104786836307941, 0.044942775671788386, -0.05408630727299171, -0.11592136658802889, -0.07283829123348236, 0.032751400923721696, 0.06090520915641624, -0.0876642464724059, 0.012759616031532089, 0.03381039957845616, 0.02996185044785893, 0.06369475963227902, -0.028489586976504715, 0.1184009706120401, -0.0961362162647624, 0.0029994138652193933, -0.09892576674062516, -0.08487469383592987, 0.02307837431637763, -0.0019775466454701883, -0.013011449981359053, -0.043496342331779535, 0.10879250961029982, -0.0669492373480655, -0.1638603251439899, 0.007031994525337298, -0.006244204687582723, -0.04285061065427807, -0.06116350182741684, 0.1096190435647658, -0.11003231918445178, -0.013076023365170525, -0.07666101239334748, 0.056307612144162605, 0.010409160503634421, 0.07743589040634923, 0.09851249544216568, -0.02957441360197129, -0.10693281145367121, 0.00471382547328404, 0.07686764480165735, 0.011784564331394074, 0.020766662905191375, -0.030478433629246855, 0.09675610700785228, 0.050806001586473123, 0.1494993122692828, -0.07423306647141373, 0.07976051148167505, 0.06715587191698862, -0.029471095237203107, -0.1551817251039368, -0.011261522428832521, -0.037607288446362686, -0.05269153203506033, 0.049282075691814954, -0.1361714398299926, -0.060388625975028316, 0.04217905316665773, 0.03585090433327577, 0.09530968447090965, 0.058115656519940224, 0.04026769474733841, -0.01356031985465272, -0.10037219575940753, 0.03029763135228234, 0.08564956104586542, -0.0011090405316800558, -0.011358382720611052, 0.0011009688479005557, 0.04158498391246022, 0.01642735964424494, -0.027895515561693958, -0.051555045146903046, -0.042514834071081135, -0.024963904764964833, 0.010544763399695093, -0.07893397752720907, 0.03512768658296472, -0.03740065603805281, -0.031098334095096346, -0.08084533594652839, -0.026009985113106752, -0.06545114590597917, 0.06715587191698862, 0.024421490371924925, 0.07635105945965617, -0.01522630174895075, 0.10527974786596572, 0.04525272644486645, -0.04832639756246192, 0.002674934480497206, -0.01972058039643621, 0.10083712515994449, -0.08482302709139944, -0.08601117640286068, 0.014076903036040651, -0.021464050524157067, -0.00564367584796629, -0.13865104601461706, -0.037478142110862395, -0.11664458433833994, -0.020572944482247554, -0.03267391485091212]}}, {'id': 'https://huggingface.co/papers/2410.11805', 'title': 'NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of Large Language Models', 'url': 'https://huggingface.co/papers/2410.11805', 'abstract': 'Large language models (LLMs) combined with tool learning have gained impressive results in real-world applications. During tool learning, LLMs may call multiple tools in nested orders, where the latter tool call may take the former response as its input parameters. However, current research on the nested tool learning capabilities is still under-explored, since the existing benchmarks lack of relevant data instances. To address this problem, we introduce NesTools to bridge the current gap in comprehensive nested tool learning evaluations. NesTools comprises a novel automatic data generation method to construct large-scale nested tool calls with different nesting structures. With manual review and refinement, the dataset is in high quality and closely aligned with real-world scenarios. Therefore, NesTools can serve as a new benchmark to evaluate the nested tool learning abilities of LLMs. We conduct extensive experiments on 22 LLMs, and provide in-depth analyses with NesTools, which shows that current LLMs still suffer from the complex nested tool learning task.', 'score': 8, 'issue_id': 127, 'pub_date': '2024-10-15', 'pub_date_ru': '15 октября', 'data': {'desc': 'Статья представляет новый датасет NesTools для оценки способностей больших языковых моделей (LLM) к обучению вложенному использованию инструментов. NesTools включает метод автоматической генерации данных для создания крупномасштабных вложенных вызовов инструментов с различными структурами вложенности. Датасет прошел ручную проверку и доработку, что обеспечивает его высокое качество и соответствие реальным сценариям. Эксперименты на 22 LLM с использованием NesTools показали, что современные модели все еще испытывают трудности со сложными задачами вложенного обучения инструментам.', 'categories': ['#benchmark', '#dataset', '#nlp', '#rlhf'], 'emoji': '🛠️', 'title': 'NesTools: новый стандарт для оценки вложенного обучения инструментам в LLM', 'embedding': [-0.0011571742166324825, 0.05000139322181262, 0.15168618066821116, -0.14108446730191207, 0.013150214059183324, -0.012959076306909158, 0.050434637057632736, 0.07742315868049222, -0.07538436333418881, 0.08761712339353857, 0.04933878688757335, -0.12151204720079554, -0.031040619135977388, -0.06177542027201026, 0.0534673399022166, -0.012691485056188091, 0.05841141426030675, -0.0466883555413809, -0.06590397929588887, 0.07563921300286154, 0.05372218756781088, -0.021636688596124352, 0.0689621653045694, -0.06789180030168514, 0.09459998107946142, -0.05316152190227209, 0.03784509066706666, 0.05127563956209784, 0.042177525019110866, -0.030199618634590735, 0.07100095864779436, -0.031448379807700835, -0.14353101530762513, 0.05876820059152305, 0.037615724162490594, 0.056066800914058815, 0.09990084978108169, 0.03787057583424178, -0.03040349596583478, 0.1230411472159104, 0.013570714309876646, -0.004937701163371834, -0.025727014861544155, 0.044114377693635354, -0.048574235878476694, 0.03483787298965791, -0.01673084299033611, 0.045031833696547355, -0.0363669669955374, 0.04908393521582217, 0.002919615044284751, 0.0372589378312743, -0.015864357321774342, -0.09194955373942587, -0.0379725144998638, -0.09893241743458407, 0.022146385930391366, 0.007537162175214047, 0.030428983136088352, 0.02400678510646895, -0.0690131376419981, -0.08980882373365734, 0.03562790315669432, 0.04949169588754561, -0.07202035131324994, -0.10056345611532094, -0.030021222464364904, 0.02068100384091044, 0.018170739912641915, -0.006059037101529873, -0.04880360037997431, 0.0033544511271682517, -0.0189862572499319, -0.010372357878654506, 0.06294772193743808, 0.011875967718898109, 0.003188799343300588, 0.0426362540221061, -0.018285422163390724, 0.04148943151461802, 0.054486738576907535, -0.009639666833914042, 0.01819622708289549, -0.0480900237113848, -0.022350265264713863, -0.07976776401442634, 0.04329886235942382, -0.07166356898819055, -0.10459006645818528, -0.07920710235504445, -0.051428548562070096, -0.08838167039647833, 0.06468070529303234, 0.05104627506060022, 0.11733252385180205, 0.03519465932087421, 0.03305392931510568, -0.11855579985773704, 0.07976776401442634, 0.09526258541062223, 0.012513090889040715, 0.02552313753030011, 0.035092720655252194, -0.09816787043164434, 0.0563216485796531, -0.007518048199678786, 0.028721492959983025, -0.05708619958874975, -0.0808891013547393, 0.029537010297273007, -0.18033122413590422, -0.06019535392870207, -0.099442118775008, -0.05061303122478011, 0.02823728079289113, -0.056372620917081784, -0.03394590015084257, 0.04013873367896435, 0.011990649969646914, -0.004077585684910697, -0.001922517923996676, -0.055251285579847285, 0.09500774375426332, -0.005262633739775149, 0.07930904102066647, 0.0030008482885395026, 0.016947465909785395, -0.026835608616730323, 0.04100521934752612, -0.048829087550227884, -0.04921136105169776, 0.03236583380753438, -0.0181834834977687, -0.026147517115315932, -0.10785213981350211, -0.014781247732224065, -0.14954545066244265, 0.08292789670104271, -0.04001130784308876, -0.01743167807687729, 0.016080978238145173, 0.005310417777228, -0.013252152724805344, 0.06595494362100374, -0.07966582534880433, 0.05040915189045761, 0.01670535782316099, 0.033079410476123894, -0.02570153069590826, 0.13578360060337027, -0.0894520353993626, -0.07023641364793307, -0.01892254533353333, 0.019827259754397003, -0.13068662726070016, -0.04332434352044203, -0.0582075329229058, 0.05723910858872201, 0.04360468035936834, -0.04024067234458638, 0.02737079312125091, 0.03618857082531157, -0.0031187156343386252, 0.15668121734833773, -0.057137167920021534, 0.14077863728349685, -0.043120466189197994, 0.04454762152945546, -0.14220580063606814, -0.02681012445109443, 0.010487040129403312, -0.0725300546567523, 0.026784640285458537, -0.002847938687643708, 0.012213642678581138, -0.02884891879585862, -0.12161398386333912, -0.08685256837828502, 0.02269431201696028, -0.04816647921291016, -0.061010875272148955, 0.11988101452929403, -0.1243663618874674, 0.011041337006996397, -0.128138122561659, -0.017979602160367755, 0.011162390048769372, -0.022605114933386596, -0.0016246630222320681, -0.03562790315669432, -0.07813673134292481, 0.06911508031377703, 0.11580342984592257, 0.10754632381663605, 0.019546926921627607, -0.0823162546919183, 0.08302983536666472, 0.09021658039922388, -0.0418717090222448, -0.006670675104497361, 0.08496668203195386, 0.004558613157567967, 0.027039489954131274, -0.07084804764474364, 0.08486474536941029, 0.017240540324603124, -0.025879925864594866, 0.04426728669360761, -0.0153673999670929, -0.09719944209130361, 0.029715404464420385, 0.09567035209158103, 0.0593288682601403, -0.027702096288370545, -0.05112272855904712, 0.010283161195696507, -0.06850343630157417, -0.02000565392154438, 0.02391758802289526, 0.14322519730768063, 0.0354495109926254, 0.010391472254805459, 0.047784205711440285, 0.026529790616785804, -0.03384396148522055, -0.024350830857176143, 0.004555427862209806, -0.1243663618874674, 0.08929912039015497, -0.05937983859449054, 0.02997025413309312, 0.029154736795803136, 0.02050260967376306, -0.08960494439933485, -0.04467504736533105, -0.012003392753542319, -0.05484352490812384, -0.06396712261520747, -0.019266592085779754, -0.019151909835030945, 0.08517057138166861, 0.033563626649372694, -0.007396995157905812, -0.038176391831107846, -0.017087633327709323, 0.028925372294305522, 0.017240540324603124, 0.02790597662423227, 0.09149082874258754, -0.07798383035526638, 0.05407897790518409, -0.05718813825437177, -0.00666430411316535, -0.05948177726011256, -0.012716970223363211, 0.06289675761232322]}}, {'id': 'https://huggingface.co/papers/2410.11419', 'title': 'GS^3: Efficient Relighting with Triple Gaussian Splatting', 'url': 'https://huggingface.co/papers/2410.11419', 'abstract': 'We present a spatial and angular Gaussian based representation and a triple splatting process, for real-time, high-quality novel lighting-and-view synthesis from multi-view point-lit input images. To describe complex appearance, we employ a Lambertian plus a mixture of angular Gaussians as an effective reflectance function for each spatial Gaussian. To generate self-shadow, we splat all spatial Gaussians towards the light source to obtain shadow values, which are further refined by a small multi-layer perceptron. To compensate for other effects like global illumination, another network is trained to compute and add a per-spatial-Gaussian RGB tuple. The effectiveness of our representation is demonstrated on 30 samples with a wide variation in geometry (from solid to fluffy) and appearance (from translucent to anisotropic), as well as using different forms of input data, including rendered images of synthetic/reconstructed objects, photographs captured with a handheld camera and a flash, or from a professional lightstage. We achieve a training time of 40-70 minutes and a rendering speed of 90 fps on a single commodity GPU. Our results compare favorably with state-of-the-art techniques in terms of quality/performance. Our code and data are publicly available at https://GSrelight.github.io/.', 'score': 7, 'issue_id': 121, 'pub_date': '2024-10-15', 'pub_date_ru': '15 октября', 'data': {'desc': 'Статья представляет новый метод синтеза изображений с изменением освещения и ракурса в реальном времени. Авторы используют представление на основе пространственных и угловых гауссианов, а также процесс тройного сплаттинга. Для описания сложных визуальных эффектов применяется функция отражения, состоящая из ламбертовской составляющей и смеси угловых гауссианов. Метод демонстрирует высокое качество результатов на разнообразных объектах при быстром обучении и рендеринге.', 'categories': ['#cv', '#rl', '#multimodal'], 'emoji': '🎨', 'title': 'Реалистичный рендеринг с динамическим освещением на основе гауссианов', 'embedding': [-0.0240430555213835, -0.02645137684060785, 0.08552210666143689, -0.03684728906669504, -0.03093352697206664, -0.10141701435194758, 0.05908410631040403, -0.0684497923751615, -0.05212673939547591, -0.005391959901418043, 0.03933588450913436, 0.022718481507553855, -0.03615155107356523, -0.07931399237551856, 0.04310892232728335, 0.051939422984130956, -0.02448458163892338, -0.06497111108709244, -0.07235662546059045, 0.05303654894849566, -0.051002856113171215, -0.11335157215175436, 0.02437754585897841, 0.058174296757384274, 0.0019149485051508572, -0.04642704886073745, 0.09156965751516025, 0.025073281682713222, 0.01830322900973505, -0.032003890195003847, -0.005682965150100714, -0.021193212423409272, -0.02790974829111246, -0.11720488409311833, -0.017473695749325276, 0.07728029954019411, 0.018410264789680025, 0.17479047646792273, -0.13379552760736382, 0.05977984213413884, 0.01875813378624493, 0.05913762094628401, 0.08969652160384575, 0.06877090622318141, 0.10329014592447208, 0.03872042497800954, -0.01594842753293869, 0.010957853650300038, -0.00492033039108892, 0.021099553133039295, -0.0835954452672674, 0.09884813610871328, 0.010068113170432779, -0.01542662425503083, -0.0403259725241591, -0.10382532916298694, -0.02253116726560391, 0.012202152673156701, 0.01564069733349727, -0.020724928987929398, -0.019025724320804856, -0.07995620705518838, -0.011479656060449896, 0.05078878238388627, 0.007311925457917524, -0.0573180040096395, -0.044393358194808, 0.06202760869874823, 0.04005838198959918, 0.012978166308077991, 0.03601775580628526, 0.05758559454419943, 0.07749437326947906, -0.08525451178808695, -0.06941311873345624, -0.0159082885188757, 0.012362707427771657, 0.019159519588084824, 2.6968161440120834e-05, 0.026826005324507747, -0.0020989174315398074, 0.08755579515797132, -0.024243748422303448, -0.05726448720436452, -0.05747855876425446, -0.08723469215692642, 0.02750836031987756, -0.0942990883436145, -0.162695370421686, 0.10066775955354278, 0.04872833006122683, 0.03965699618775928, -0.02188894824714408, -0.02911390569663213, 0.03700784382131, 0.06604147322533215, 0.03989782940437922, 0.026665448400497792, 0.16301647559212593, 0.057103934619144564, 0.007994283056561339, 0.0662020258105521, 0.029435015205862045, -0.09553000957525917, 0.16729793716145477, -0.05020008234009642, 0.055712460802279935, 0.07845769528898379, 0.07187496336614058, 0.07257068834290038, -0.06331204456627289, -0.02048409577130946, 0.0403259725241591, 0.03556285319917039, -0.013740801067089786, -0.07872528365414873, -0.058388370486669214, -0.0058702791751111625, -0.015185793207805895, -0.01867785640893745, -0.10666179794078116, 0.014142187085869177, 0.04787204165227206, -0.034867113036645574, 0.07669160166579927, -0.06390074461006272, 0.06684424482901193, -0.13507996781367848, 0.020577750722889435, 0.0037395850964179887, 0.0447679823399179, -0.0654527731815423, 0.007091163266905583, -0.06946663553873121, -0.09462019568344943, -0.004448701314638797, 0.02940825788792205, 0.03310101290527605, -0.040433010473499065, 0.06673721121846196, -0.02892659362407718, 0.10200571222634243, 0.024912726928098265, 0.04902268442191175, 0.02483245063548829, 0.04904944173985174, -0.11228120784411964, 0.05432098481602031, -0.03446572723480568, 0.0820969400092478, -0.04110198680989889, -0.016282915918078098, -0.13754181244636282, -0.05464209215585522, -0.062455753987923114, -0.1543465101653383, -0.036392382120790163, 0.09403149780905458, -0.05870947782650413, -0.06716535650763684, 0.04832694425938694, 0.008295323058759758, 0.02863224360218226, 0.02972936739715196, -0.1138867510514792, 0.012295809794131677, -0.1246974320771663, 0.036231829535570206, -0.11602748834432863, -0.13625735705428316, -0.05156479450023106, -0.05116340869839117, -0.03350240087651094, -0.07208903492603051, 0.07572827096871453, 0.02693303893505772, 0.016630784480764004, -0.0008157346065676232, -0.04185124160830368, -0.019146140929114822, -0.06791460696725163, 0.0876628287685213, -0.018249710035065068, 0.03427841516225073, -0.03136167226124152, 0.07530012351014465, 0.002664203628964905, -0.023347319697648687, 0.009579759794042412, 0.0769056667175042, -0.11859636007937793, 0.10462809859727171, -0.01815605399878759, 0.1013099720638176, 0.004672808929681487, -0.04998600861081149, 0.06909201139362132, -0.00011613033936131704, -0.014490055214676082, 0.002023657430990203, 0.08343489485144245, 0.08627135929044667, 0.005465547298422023, -0.1176330315516882, 0.03088000799739665, -0.0004137208516507975, 0.043724377519618184, -0.10468161757194169, -0.11217417423356968, 0.0454904776509877, 0.04990573014880651, 0.007666483567027927, 0.07246365907114041, -0.0020671410013259413, -0.06138538968028841, 0.005492306568817515, -0.0014324484168000754, 0.014262603260300145, 0.029301219938582077, 0.0414230941497338, -0.05303654894849566, 0.0551237585890951, 0.03631210799757518, -0.04096819154261892, 0.029622329447811995, 0.05389284169624043, 0.10612661253287131, -0.1298351798860549, 0.07246365907114041, -0.08541506220391192, 0.01927993619639479, 0.012583469401844098, -0.028899834136742186, -0.02115307210770928, 0.02283889811586383, -0.06572036588549723, 0.013232377749788924, -0.12116523398382226, -0.036552939044800115, -0.02964909110454199, 0.009265340698994996, 0.012529951077992613, 0.035696648466450355, -0.07706622581090916, 0.034385453111590705, 0.0917302144391702, -0.06534573740159734, -0.02452472086992587, 0.05070850609127629, 0.004054004625344404, -0.04059356305871902, -0.07540716362887961, -0.05343793475033555, -0.08921485734000088, -0.0377303413017748, -0.010529708361125153]}}, {'id': 'https://huggingface.co/papers/2410.09754', 'title': 'SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning', 'url': 'https://huggingface.co/papers/2410.09754', 'abstract': "Recent advances in CV and NLP have been largely driven by scaling up the number of network parameters, despite traditional theories suggesting that larger networks are prone to overfitting. These large networks avoid overfitting by integrating components that induce a simplicity bias, guiding models toward simple and generalizable solutions. However, in deep RL, designing and scaling up networks have been less explored. Motivated by this opportunity, we present SimBa, an architecture designed to scale up parameters in deep RL by injecting a simplicity bias. SimBa consists of three components: (i) an observation normalization layer that standardizes inputs with running statistics, (ii) a residual feedforward block to provide a linear pathway from the input to output, and (iii) a layer normalization to control feature magnitudes. By scaling up parameters with SimBa, the sample efficiency of various deep RL algorithms-including off-policy, on-policy, and unsupervised methods-is consistently improved. Moreover, solely by integrating SimBa architecture into SAC, it matches or surpasses state-of-the-art deep RL methods with high computational efficiency across DMC, MyoSuite, and HumanoidBench. These results demonstrate SimBa's broad applicability and effectiveness across diverse RL algorithms and environments.", 'score': 6, 'issue_id': 125, 'pub_date': '2024-10-13', 'pub_date_ru': '13 октября', 'data': {'desc': 'Статья представляет архитектуру SimBa для масштабирования параметров в глубоком обучении с подкреплением (RL). SimBa включает нормализацию наблюдений, резидуальный блок прямой связи и нормализацию слоев для внедрения смещения к простоте. Эксперименты показывают, что SimBa улучшает эффективность выборки для различных алгоритмов RL, включая off-policy, on-policy и unsupervised методы. Интеграция SimBa в алгоритм SAC позволяет достичь или превзойти современные методы RL на различных средах с высокой вычислительной эффективностью.', 'categories': ['#rl', '#deeplearning', '#architecture', '#scalability'], 'emoji': '🤖', 'title': 'SimBa: Простота и масштабируемость для глубокого обучения с подкреплением', 'embedding': [0.05562089720289916, -0.023483124144785164, 0.09150807997460408, 0.004806570104994543, 0.021918522634977233, 0.009317135016108546, -0.015082199762322538, 0.029995252624533602, -0.08259970862075855, 0.05925754092284945, -0.003233158399186959, -0.10182599280336581, -0.08868897194057779, 0.04090518169403498, 0.04648700431351979, -0.08851982504046631, -0.015927930082180936, 0.06709464160815733, 0.02779635337483187, -0.04473916067698268, -0.02160842346868868, -0.03196862406765302, 0.08620815979712394, 0.05937030273579111, 0.021030506635265712, -0.05477516688304076, 0.038367988644110305, 0.06410639921611651, -0.009634283467985547, -0.10086749439951728, 0.06726379477931727, -0.05429591663594174, -0.15538893048064337, -0.06997012845830497, 0.003992554358578413, 0.08288161942416118, -0.027274819538229227, -0.004024269350090577, -0.025639739804983262, 0.04011583228064742, 0.09094425627744933, -0.08767409263025842, -0.0661925303818281, 0.10921204414655755, -0.06957546002265966, 0.02004382195888075, -0.0023574746253324503, -0.05353476185648858, 0.0037282633200702397, 0.13001702088489148, -0.11406089616877613, 0.05206882902335409, -0.08406564145389304, -0.07589024278766322, 0.010092387112528911, 0.053675716213015146, 0.07464984403215952, -0.05040555883687271, 0.01694280730215081, 0.04879866955686217, -0.005345723915526593, -0.04338599174713934, -0.030840982944391995, 0.03684567281415549, -0.021974905631797555, 0.04586679970989421, -0.15065284654241493, 0.05254807299940463, 0.02004382195888075, 0.036648335460808604, 0.009613140419024037, -0.05863733422887437, 0.060497945949401624, -0.021622518695306386, -0.05483154987986108, -0.013214544400150465, -0.021918522634977233, 0.13869985189005773, 0.0006444292876136906, -0.019761911155478117, 0.011459652732234598, -0.08237418499487523, 0.026302227998112476, -0.1437742379899071, -0.030728219041100843, -0.07295838130209323, -0.08181036338806996, -0.05184330121677178, -0.036028135037882006, 0.0036472139902213527, 0.08970384707019817, -0.00833044824937409, -0.08682836022005053, 0.05387305565671153, 0.10193875670665697, 0.05863733422887437, 0.07808914412771445, 0.010092387112528911, 0.11310240612632556, 0.11129817531226914, -0.019888769240212233, -0.02586526761156557, 0.04192005682365536, -0.0019892293151384135, 0.0559309984595372, -0.08158483140078868, 0.029600577917839817, 0.0010139960540454431, -0.07538281044872676, -0.05254807299940463, -0.04775559979330739, -0.015223154118849109, -0.027472154801226625, 0.033857424151066205, 0.02414561651861336, -0.09573673157389608, -0.01854969449181186, 0.04268121578380752, -0.12674686350874903, 0.017633488038864928, -0.018070448425411834, -0.03774778404048472, 0.016618610818895055, -0.07893487862827182, 0.030305353881171643, -0.021509754792015234, 0.014814385021677413, -0.02633042054169738, -0.10278448284581639, 0.016407178238930457, -0.0396365841238979, 0.008351591507370549, 0.064275541935529, -0.05669215532933986, -0.11671085103164247, -0.04017221527746774, -0.16226755157524814, -0.02170709005501263, -0.018056353198794128, -0.0833326750373258, 0.0018729413752543891, 0.009895051013391715, -0.059144774929208804, -0.05494431169280274, -0.05717140348608938, 0.07132330365919676, 0.010134674882731527, 0.07904764253156298, -0.13757221285714621, 0.05125128706638162, -0.020706310152009962, 0.01491305286221106, -0.13655734817927329, -0.020001533143503388, -0.09669521952599715, -0.08987299187996016, -0.07081585668781387, 0.0721690235273077, 0.10120579238043921, -0.07278923440198176, 0.0475864549835454, 0.03690205581097581, 0.0017020333138897691, 0.034393053214286554, 0.05953945172625208, 0.05328103941597187, -0.08141568450067721, 0.01754891458880919, -0.1251681625916244, -0.0866028365941672, 0.0424274975239898, 0.006646034804896251, -0.026851952287950537, -0.04293493613397473, 0.0010351393956558824, -0.004013697407539923, -0.06416277594188835, -0.1540357761832465, -0.017985874975356093, -0.08085186498422144, 0.002628813277149477, -0.009140940711723165, -0.08705389220733183, 0.034280289310995395, -0.11242581852587966, -0.03856532808780668, -0.1268596211409917, 0.05612833581288409, 0.019705528158657795, 0.003742358755722897, 0.027204341314791197, 0.09043681348676541, 0.02022706199526044, 0.05615652626611951, 0.04679710557015784, 0.014370375976646907, 0.059032013116267136, 0.050602896190219605, -0.015533255375487153, -0.08152845258466734, -0.005204768513825279, -0.02139699088872408, 0.07769446524032168, -0.10323554682037896, 0.03326541418137501, -0.0866028365941672, -0.033462749444372414, 0.04237111243681998, -0.09663884280022529, -0.05858095750310253, -0.03374465815742555, -0.018338264002196753, 0.08761771381413709, -0.05178691821995146, -0.1294531971877367, 0.055057077686443384, -0.03960839367066248, 0.006822228482176784, -0.039100955060677546, 0.0932559215207917, 0.07769446524032168, -0.04411895816370658, 0.0571995918489753, -0.0424274975239898, -0.13204677114413224, -0.05111033480020454, 0.056720347872924765, -0.09043681348676541, 0.10013452798295004, -0.027415773894755795, 0.08801238852083086, 0.052717221989865594, -0.046571577763575525, 0.005617061940238772, -0.008478451264384259, -0.035718031690894464, -0.006054023371960424, 0.039185528510733286, 0.07487536765804285, -0.029544197011368987, -0.013588075012707992, 0.07295838130209323, 0.024300666101757636, 0.007787771363809934, 0.026626424481368226, 0.08688474948791934, 0.006903277812025671, -0.03938286586408018, -0.05858095750310253, 0.07549556808096944, 0.006290123121093385, -0.06004689033623701, -0.034223908404524564, -0.014588856378955309, -0.05068746754992585, 0.026908335284770858]}}, {'id': 'https://huggingface.co/papers/2410.10934', 'title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'url': 'https://huggingface.co/papers/2410.10934', 'abstract': 'Contemporary evaluation techniques are inadequate for agentic systems. These approaches either focus exclusively on final outcomes -- ignoring the step-by-step nature of agentic systems, or require excessive manual labour. To address this, we introduce the Agent-as-a-Judge framework, wherein agentic systems are used to evaluate agentic systems. This is an organic extension of the LLM-as-a-Judge framework, incorporating agentic features that enable intermediate feedback for the entire task-solving process. We apply the Agent-as-a-Judge to the task of code generation. To overcome issues with existing benchmarks and provide a proof-of-concept testbed for Agent-as-a-Judge, we present DevAI, a new benchmark of 55 realistic automated AI development tasks. It includes rich manual annotations, like a total of 365 hierarchical user requirements. We benchmark three of the popular agentic systems using Agent-as-a-Judge and find it dramatically outperforms LLM-as-a-Judge and is as reliable as our human evaluation baseline. Altogether, we believe that Agent-as-a-Judge marks a concrete step forward for modern agentic systems -- by providing rich and reliable reward signals necessary for dynamic and scalable self-improvement.', 'score': 5, 'issue_id': 127, 'pub_date': '2024-10-14', 'pub_date_ru': '14 октября', 'data': {'desc': 'Статья представляет новый подход к оценке агентных систем искусственного интеллекта - Agent-as-a-Judge. Этот метод использует агентные системы для оценки других агентных систем, что позволяет получать промежуточную обратную связь на протяжении всего процесса решения задачи. Авторы применяют Agent-as-a-Judge к задаче генерации кода и представляют новый бенчмарк DevAI, содержащий 55 реалистичных задач по автоматизированной разработке ИИ. Результаты показывают, что Agent-as-a-Judge значительно превосходит LLM-as-a-Judge и сопоставим с оценкой человека.', 'categories': ['#rl', '#code', '#benchmark', '#rlhf'], 'emoji': '🤖', 'title': 'Агент судит агента: новый подход к оценке ИИ-систем', 'embedding': [-0.012691625155356014, 0.042267129704319936, 0.05757173797887448, -0.025986247269249425, 0.003359547772439315, 0.03876401210190981, 0.026000602672148616, -0.04335826844070366, 0.0628838434675224, 0.0437315494861636, 0.09446933417714747, -0.14380012710368667, -0.026273387356244546, -0.08740567736073045, -0.012375770205554415, -0.021320207510157365, -0.016711595768464326, -0.12255170941962981, 0.016941309973327844, -0.020688497183500684, 0.04792380329736202, -0.016323957184837774, 0.11353548506477402, 0.0677078076053775, 0.024162901844845014, -0.12909851834999045, -0.02558424901218884, 0.01919536446059122, -3.092933157095056e-05, -0.05943815174724387, 0.09556047718406603, -0.021650418076384896, -0.0298339329760537, -0.03919472543530362, 0.04933079506180665, 0.10360041243153387, -0.014134508202896271, 0.013789938390288192, 0.012863909421079827, 0.07683888799503133, 0.018635439689500172, -0.026273387356244546, 0.019410720059654413, 0.06696123910865535, -0.047866375279962986, 0.01788887546331287, 0.04709109597744246, -0.005538228814282601, -0.07448433501113652, 0.013775581706228548, -0.14150299786665604, 0.05251805963615587, 0.11566032597907275, -0.08889880581310507, -0.008729081492013013, -0.026560527443239664, -0.08022715169791084, 0.030953782518235794, 0.04350183955183493, 0.05022093466966008, 0.07293377427082803, -0.04476525806988087, -0.038534297897046293, 0.054097331182262724, -0.10210728397915926, -0.05355176608460571, -0.051168495889110245, 0.0501347894406604, -0.007659481838726863, -0.034370759162181096, 0.03167163593862472, 0.061792709001673544, 0.0027978287361309258, -0.04453554386501735, -0.0841896955747806, -0.05208734843802947, 0.1232408456284181, 0.11347806131790986, -0.0352896095758329, 0.039682864650829035, -0.019539934700252788, 0.009705359923063836, -0.05872029832685495, 0.009497182666030463, 0.024607970581138016, -0.016596740160719763, -0.0475505243871695, -0.07838944660007238, -0.11991001421347246, -0.061333282727213924, -0.04054428704708181, 0.11795745735137081, -0.0012284243462063174, 0.09337819971129858, 0.060529290483627596, 0.021334562913056552, -0.04668909985564929, -0.03675402722240914, 0.12415970244787215, 0.07034950281153485, -0.017860160386979643, -0.05335076695607542, 0.06357297967631069, 0.08361541326552292, 0.08872651749037314, -0.06047185819569373, -0.016955665376227034, 0.01334487050810216, -0.06747810194158368, -0.027594233891689506, -0.14104357372746387, -0.03339448073113027, -0.18974266059788125, -0.01689823735882801, -0.03442818717958013, -0.14138815037292768, -0.05435576259872689, -0.006090975029817085, 0.030063645045649793, -0.07270406220123195, -0.022827697771233436, 0.06776523989331137, -0.015275892130587602, -0.027536805874290485, 0.1125592066337232, -0.07235949409683781, 0.058863869437986216, -0.06914351444615537, -0.07770031039128411, -0.020229073044308495, -0.028800224392336427, 0.04327212534697141, 0.0168551668795956, -0.06282641545012338, -0.0994655887730019, -0.06391754991597226, -0.014170401194205831, -0.019123578905025584, -0.07080892695072705, -0.03661045824654529, 0.03132706463132945, 0.0002103082325709843, -0.11192750057760135, 0.0925167751797784, -0.08556797226289199, 0.035433180686964175, -0.009023400988671663, 0.03014978813938204, -0.05567660699890443, 0.12657167917863113, -0.05521718285971224, -0.03560546260389382, -0.08281141461613432, 0.017228451127956675, -0.06506611453448757, 0.005566943250035597, -0.09699617334850677, 0.004242505835361783, 0.027350164283926802, -0.10090128707271007, 0.11732574275417927, 0.023588619535587354, 0.02205241633344548, 0.08775023905932232, -0.027637306506189344, 0.07925087326686002, -0.05085264286104933, 0.0049639473591319115, -0.0730486303056261, 0.06357297967631069, 0.014471897751733848, -0.11319091909564731, -0.005692566797878376, 0.025957532192916202, 0.07580517514077921, -0.07034950281153485, -0.08281141461613432, -0.03411232774571694, 0.0007200954084799747, -0.038620440990778535, -0.06977522263754463, 0.03592131990248958, 0.0003640856017042902, 0.007609232376884403, 0.06397498220390613, -0.07103863902032313, -0.012849552523493443, 0.04772280630409915, 0.021033066355528533, -0.011815845647990111, -0.01752994768548469, 0.056021177238566, 0.0711535035961909, 0.0886116614555751, 0.1356453187392814, -0.013682260483993222, 0.07293377427082803, 0.0651809684340182, 0.03167163593862472, -0.06592753693074035, 0.0501347894406604, -0.08091628790669912, 0.10199242794436121, -0.05254677471248909, 0.003905115432417235, 0.003696938175383862, 0.060644144383158224, -0.10124586158290648, -0.030264644174180087, -0.05292005575794903, 0.11709603709038545, -0.03655303022914627, 0.10716096232187787, 0.05892130172592009, -0.11370777338750594, -0.027034310188232175, -0.012483448111849383, 0.06736324590678563, -0.01597938694517621, 0.08097371805936557, -0.03612231903101988, 0.0569687405932836, -0.06242441932833022, -0.042697843037713755, -0.10428955077558957, 0.04970407824253401, -0.02818287373911378, -0.0950436143511377, 0.049187223950675374, -0.05030707135759005, -0.002144583169858037, 0.027852661037618826, -0.13989501337948337, -0.02400497319554713, 0.02716352269356312, 0.022526198010804282, 0.0005015991513245198, 0.008779331380908957, 0.048182232578558755, -0.00587203025973898, 0.008477832901640258, -0.03701245436833846, 0.04321469732957239, -0.11640689661106232, -0.04163542151293068, 0.09567533321886407, -0.04180770770039518, 0.004156363595736504, 0.08016971940997697, -0.02001371637761159, -0.030034932104583995, -0.059897580156970914, -0.00589356592640867, -0.04335826844070366, -0.03796001985832348, 0.0420948477873903]}}, {'id': 'https://huggingface.co/papers/2410.08001', 'title': 'Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation', 'url': 'https://huggingface.co/papers/2410.08001', 'abstract': 'The increasing demand for versatile robotic systems to operate in diverse and dynamic environments has emphasized the importance of a generalist policy, which leverages a large cross-embodiment data corpus to facilitate broad adaptability and high-level reasoning. However, the generalist would struggle with inefficient inference and cost-expensive training. The specialist policy, instead, is curated for specific domain data and excels at task-level precision with efficiency. Yet, it lacks the generalization capacity for a wide range of applications. Inspired by these observations, we introduce RoboDual, a synergistic dual-system that supplements the merits of both generalist and specialist policy. A diffusion transformer-based specialist is devised for multi-step action rollouts, exquisitely conditioned on the high-level task understanding and discretized action output of a vision-language-action (VLA) based generalist. Compared to OpenVLA, RoboDual achieves 26.7% improvement in real-world setting and 12% gain on CALVIN by introducing a specialist policy with merely 20M trainable parameters. It maintains strong performance with 5% of demonstration data only, and enables a 3.8 times higher control frequency in real-world deployment. Code would be made publicly available. Our project page is hosted at: https://opendrivelab.com/RoboDual/', 'score': 3, 'issue_id': 122, 'pub_date': '2024-10-10', 'pub_date_ru': '10 октября', 'data': {'desc': 'RoboDual - это синергетическая двойная система, объединяющая преимущества обобщенной и специализированной политик для роботов. Обобщенная политика на основе vision-language-action обеспечивает высокоуровневое понимание задач, а специализированная политика на основе диффузионного трансформера осуществляет точные многошаговые действия. RoboDual демонстрирует значительное улучшение производительности по сравнению с OpenVLA как в реальных, так и в симулированных средах. Система сохраняет высокую эффективность даже при использовании всего 5% демонстрационных данных и обеспечивает более высокую частоту управления при развертывании в реальном мире.', 'categories': ['#rl', '#multimodal', '#nlp', '#cv'], 'emoji': '🤖', 'title': 'RoboDual: Объединение сильных сторон обобщенной и специализированной политик для универсальных роботов', 'embedding': [0.05827261376776874, 0.05928278856167894, 0.08993436079824421, 0.011155207988757625, -0.005545134045120047, 0.03674147572931303, -0.004408688139728053, 0.03685692204886128, 0.04222527603201631, 0.11198501676011394, 0.05330833138847945, -0.050537564914517316, 0.0018065880872919116, -0.0376650610408386, 0.0976694033278644, -0.016855476727729756, 0.004650408943361429, 0.0341150208435711, 0.03685692204886128, 0.08572048898146543, 0.028530202639901373, -0.06072589390449559, 0.00899054934216952, 0.016076199631821053, 0.041590311789054056, -0.10771342494537664, 0.05838806430307115, 0.10182555093093053, 0.0027689593909452992, -0.06638287106409103, 0.08583593740889077, -0.030997913303087123, -0.1059239637808986, -0.034374780859216636, -0.0010273109913832101, 0.06753735533834435, 0.08999208079620273, 0.07550330104644645, -0.025514110892506763, 0.1232989626477965, 0.05651202630347096, -0.10551990482429531, -0.009329678939640653, 0.13842270664051504, -0.03284509024976952, -0.06072589390449559, 0.02515333402983333, 0.07827406330465442, 0.0007716106626700116, 0.03804027159178655, -0.10863700604114805, -0.0005767913886928363, -0.05628112944862029, 0.04814200899150314, -0.00464319283698115, 0.013442530747575926, 0.07348295145862606, -0.05506891885277723, 0.014041419728329473, -0.005891479748971459, -0.04349521084157205, -0.050479840700804655, 0.10632803749264141, 0.0057183066862580454, -0.04008947801677059, 0.019611808459478835, -0.12179813309126718, 0.14234797425174708, -0.04903673746586507, 0.0678836964048662, -0.04444766142176957, 0.0022728918107839885, 0.04078217279707673, -0.03570243777460796, 0.028587926853614044, -0.0600332033399436, 0.026553148847211827, 0.13715278447822177, -0.022829933900929245, 0.00444115787819911, -0.029352774266214673, -0.05555957572327343, 0.025701716167980733, 0.016177216689636658, -0.04124396650677806, -0.10257595938556395, 0.028198287884084277, -0.128609596632993, -0.0364239936078319, -0.11186956200905737, 0.09605312534390975, -0.034057298737735504, -0.07492605890931978, 0.013168340205471491, 0.09784257386112534, 0.05385671247268832, 0.037318717866439685, 0.05850351062261941, 0.04337976241414671, 0.1133126694597511, -0.10736707544734649, -0.026784045702062494, 0.0061331999202417125, -0.02636554567961493, 0.049787154352006806, -0.013933187618072126, 0.059225064347966275, 0.0735983977781743, 0.0595136854165296, -0.043726107696422704, -0.12168268466384184, 0.005465763198568204, -0.003412945558578415, 0.03232557127241698, 0.0770618569245655, -0.07105852815906284, 0.006353273247885081, -0.04277365500834811, 0.014171299314576823, -0.06320803720201731, -0.0825456572272688, -0.010015154240963197, 0.035442679866839494, -0.10523127743210076, -0.006880007066891649, 0.0012392671795116053, 0.02845804789972981, -0.09593767902436151, -0.0064326443052246315, -0.01975611793982196, -0.04643914574091802, 0.016913200941442424, 0.03815571791133481, -0.0344613661258471, -0.08387330992690596, -0.016480268284658886, -0.02938163531913247, -0.0862977311185921, -0.10026699505281146, 0.04750704474854089, -0.08543186580502501, 0.03365322713386977, -0.05174977551036039, 0.03402843768481771, -0.055270954654710096, -0.015628836448578622, -0.07100080394535017, -0.0059455961202816935, -0.06315031088042757, 0.11008011349184178, -0.12526158591402717, -0.04903673746586507, -0.10967604188797606, -0.049700564869622196, -0.0298434290288338, -0.06297714034716666, -0.05573274836441143, 0.10551990482429531, 0.0639584498724049, -0.06771052587160528, -0.051201396534028595, 0.12583882805115382, -0.07394474938408152, 0.08445055206403262, 0.06251534663746532, -0.020088034803516137, -0.0006845733407219375, 0.08537414369918943, -0.1938379855307078, -0.05010463647348794, -0.044591970902112706, 0.00864420427068123, -0.09680353801429734, 0.014654739235541916, -0.013940402881301574, -0.10003609398220666, -0.040580134887266785, -0.1268778638979818, 0.0037232130491932146, -0.07925537704564682, -0.04384155401597097, 0.016364820489596678, -0.011948916454276055, 0.009654378432228303, -0.10061333611933332, -0.06291941402557691, -0.0539432956314417, -0.005314237190269382, -0.004650408943361429, 0.07290570932149938, 0.03336460606530644, 0.13761456764853772, 0.060956790759346256, -0.0024568877683415753, 0.13899996985641247, -0.03451909244743683, 0.0250811803436003, 0.04029151381870346, 0.0775813748479795, -0.03200808809699732, -0.02946822269364001, 0.024431781358425, 0.04649686995463068, -0.14373334484146572, 0.00954614526803242, -0.04205210549875539, 0.02604806039631819, 0.10476947950664527, -0.047939977405324415, -0.07492605890931978, 0.017144097796293088, -0.056771786319116495, 0.07105852815906284, 0.05945596120281693, -0.047564766854376474, -0.052240432380856586, -0.03636626939411924, 0.06805687326175835, 0.02964139533477801, 0.04880584061101441, -0.02041994745145616, 0.0386752358347488, -0.03169060386763912, -0.05301971031991612, -0.05004691225977527, 0.043812692963053165, 0.019395344238964142, -0.10321092784428036, 0.03821344212504747, -0.08889532284353914, 0.06407390040770732, 0.0554441272958481, -0.0043004553971075846, -0.06442024358210623, 0.02026120533677706, -0.041590311789054056, -0.003645646123842588, -0.008074177396784017, 0.06245762031587558, -0.04707411209175735, -0.051663192351607, 0.06442024358210623, 0.07631143582266962, -0.07267480825089458, -0.03838661476618547, 0.0030972660935722593, -0.03694351153124589, -0.04456310774131783, -0.009344110098462671, 0.02382567816838054, 0.08929939023165073, -0.023594781313529878, 0.018500617872479277, -0.06903818700275074, -0.02173317384038859, 0.04363952242979225]}}, {'id': 'https://huggingface.co/papers/2410.09745', 'title': 'Empirical Study of Mutual Reinforcement Effect and Application in Few-shot Text Classification Tasks via Prompt', 'url': 'https://huggingface.co/papers/2410.09745', 'abstract': "The Mutual Reinforcement Effect (MRE) investigates the synergistic relationship between word-level and text-level classifications in text classification tasks. It posits that the performance of both classification levels can be mutually enhanced. However, this mechanism has not been adequately demonstrated or explained in prior research. To address this gap, we employ empirical experiment to observe and substantiate the MRE theory. Our experiments on 21 MRE mix datasets revealed the presence of MRE in the model and its impact. Specifically, we conducted compare experiments use fine-tune. The results of findings from comparison experiments corroborates the existence of MRE. Furthermore, we extended the application of MRE to prompt learning, utilizing word-level information as a verbalizer to bolster the model's prediction of text-level classification labels. In our final experiment, the F1-score significantly surpassed the baseline in 18 out of 21 MRE Mix datasets, further validating the notion that word-level information enhances the language model's comprehension of the text as a whole.", 'score': 2, 'issue_id': 125, 'pub_date': '2024-10-13', 'pub_date_ru': '13 октября', 'data': {'desc': 'Статья исследует взаимоусиливающий эффект (MRE) между классификацией на уровне слов и текста в задачах классификации текста. Авторы проводят эмпирические эксперименты на 21 наборе данных MRE Mix для подтверждения теории MRE. Результаты сравнительных экспериментов с использованием тонкой настройки подтверждают существование MRE. Применение MRE к обучению с подсказками, используя информацию на уровне слов в качестве вербализатора, значительно улучшило F1-меру на 18 из 21 набора данных.', 'categories': ['#nlp', '#classification', '#finetuning', '#promptlearning'], 'emoji': '🔄', 'title': 'Взаимное усиление классификации слов и текста улучшает понимание языка', 'embedding': [0.0482350874175379, 0.12184057477672086, 0.08602968396380986, 0.007347498374008819, -0.05058420096116788, 0.11484544991984637, 0.0987670861566691, 0.07162180098579161, -0.11672473572115825, 0.12267582194528975, -0.00045473252233926896, -0.02959880315858084, -0.06373922093032065, 0.05191536536709143, -0.0027177914463645853, -0.059980645455702974, 0.0778338925121157, -0.026205643238220017, -0.01142581729859366, 0.002370318518335801, -0.018062055944559507, 0.0261142893496558, -0.036567836584421795, 0.05181095705102413, -0.01652208265048918, 0.029102882049232243, -0.04048302066114653, 0.11641152626093201, 0.04646020606029941, 0.039230161524274984, 0.08274092267953155, -0.03672444131453492, -0.06838524579554392, -0.02931169093737902, -0.01932796757293719, 0.05606545944298129, 0.013533491886909697, -0.0010636255011894431, -0.012959264734110312, -0.016991906520813352, -0.07919116190105152, -0.014956010181397464, -0.06765440887903931, 0.0778338925121157, -0.038864744034021154, -0.03774238851624438, 0.03354009221831784, -0.05507361335229017, 0.049879465155681625, 0.1452272825369837, -0.13583084385043948, 0.04669511605946454, 0.019066954526756757, -0.014499238415380034, 0.05063640124720761, -0.0458337735878682, 0.023595518584406568, -0.007451903979680385, -0.04267552560266946, -0.06979470772653162, 0.08002639938963559, -0.05154994594084065, -0.008809167560625322, -0.0034323127484865213, -0.11035603946472104, -0.022760279159825536, -0.04951405153742173, 0.0182447656576849, 0.043380257536161794, 0.026022933525094626, 0.08247991737733897, -0.05888439298494151, -0.02158572335600903, -0.03434923246787748, -0.047086634660736704, -0.03307026834799366, 0.07611121144091694, 0.04820898630651956, -0.0652008928273329, -0.034427533864935564, 0.019693385063191012, -0.03032963813908848, -0.01465584546868951, -0.03860373292383769, -0.05862338187475804, -0.03340958472722914, -0.042075196177253554, -0.025226845767041112, -0.07391870649939401, -0.022773329715334707, -0.09944572085113701, -0.0012887486485210163, -0.028580855956871376, 0.015817350716996845, 0.008639510339006064, -0.08571646869559274, -0.11505425687199618, -0.09605255318678833, -0.08054842161000254, 0.06645375390020142, -0.07767728971799955, 0.0014820609070792293, 0.019419321461501406, -0.037794594610275005, 0.03489735386326581, -0.12257141362922244, 0.008848320001551627, -0.0773640705777885, -0.050270983756953797, -0.018910347860646673, -0.012117499838174998, -0.03387940472555939, 0.009168059773124558, 0.02015015644200905, 0.05322042479000272, -0.07062995102310658, -0.1109824641931644, 0.00025673013690507754, -0.07564138950658973, 0.06045046932602715, -0.0015367103647934914, 0.051419440385748916, 0.06624494501205463, -0.04105724897554409, 0.04306704420394163, -0.015543289051304197, 0.04116165148362051, -0.04027421177299975, 0.018114260102593163, 0.0170180076318317, -0.023882632741605347, 0.05836236689258065, 0.04103114786452575, -0.02248621652613441, -0.1820822371505982, 0.006525309698536656, -0.06723678723075183, 0.019093055637775105, -0.11338377802283411, 0.015243124338596243, -0.00707996101807228, -0.09072790330708198, -0.0726136432044888, 0.02873746359097994, -0.17435627344122215, -0.10033315862576081, 0.03860373292383769, 0.00406200543979883, 0.004845042642343167, -0.07762508555996589, -0.03432312942086217, -0.08780456725704532, -0.08112264798840313, 0.06332160121803014, -0.04301484198190493, -0.0652530950493696, -0.06232975706333599, 0.0014192547844118406, 0.03716816407384074, -0.07616341172695669, 0.04711273770775201, 0.07099536657736344, -0.056117661665017984, -0.028293743735669556, 0.057422724959923184, 0.09934130866307578, -0.11641152626093201, -0.022394861669571713, -0.08362837013414015, -0.025814124636947846, -0.030120829250941707, -0.011066925279694122, -0.044398204737871254, -0.08905742832991384, -0.04625139523615567, -0.01778799427886686, -0.16475101037855547, -0.03836882292467256, -0.003549768522467868, -0.0778338925121157, 0.0075628333143089675, 0.10826793315928061, -0.03933456596834837, 0.12288463857742438, -0.05851897549468769, -0.07423191983161417, 0.031686905592027345, 0.12110975528418892, -0.03346178694926583, 0.03742917518402422, -0.002897237455814354, 0.02881576789203346, 0.09088451578118294, 0.03742917518402422, -0.05549123306458068, -0.023739076631004438, 0.055595637508654074, 0.004375220127216868, -0.08044401910192611, -0.05987624294762655, -0.045651067746736725, 0.08294973543967225, 0.049200834333207634, 0.01710936152039591, 0.0026166491571692494, 0.017579185390720082, -0.05201976787516786, 0.043484663916232145, -0.057840344672213696, 0.03607191160307928, 0.10576221682153447, 0.026283947539273537, 0.026022933525094626, 0.043354154489146486, -0.10435275295454981, -0.08696932783246429, -0.10920758283592592, 0.047347645770920176, 0.07532817036637868, 0.05679629635948589, -0.0603460629459568, -0.036228518269189366, 0.11787319428595033, -0.016091414318686447, -0.08529884898330223, 0.13760572714367494, 0.09657458508714008, -0.10038536278379447, -0.018401374259791943, -0.08884861169777922, 0.031086576166611434, -0.0312692820077429, 0.006665603460659817, -0.04794797519633608, 0.014146871867834778, 0.009840166479442134, -0.06916828299808826, -0.030590649249271955, 0.05862338187475804, 0.025448707146694022, 0.014186022759963515, 0.14846384353522227, -0.030460143694180215, 0.020815738644970827, -0.00581078839641472, 0.13436917582542116, -0.05228077898535133, -0.040117603170892704, 0.031608598386978375, -0.034140421643733744, 0.12716523240041508, 0.05324652590102106, -0.022316560272513632, -0.05987624294762655, -0.036854948805623615, -0.026166491571692492]}}, {'id': 'https://huggingface.co/papers/2410.06593', 'title': 'Towards Natural Image Matting in the Wild via Real-Scenario Prior', 'url': 'https://huggingface.co/papers/2410.06593', 'abstract': 'Recent approaches attempt to adapt powerful interactive segmentation models, such as SAM, to interactive matting and fine-tune the models based on synthetic matting datasets. However, models trained on synthetic data fail to generalize to complex and occlusion scenes. We address this challenge by proposing a new matting dataset based on the COCO dataset, namely COCO-Matting. Specifically, the construction of our COCO-Matting includes accessory fusion and mask-to-matte, which selects real-world complex images from COCO and converts semantic segmentation masks to matting labels. The built COCO-Matting comprises an extensive collection of 38,251 human instance-level alpha mattes in complex natural scenarios. Furthermore, existing SAM-based matting methods extract intermediate features and masks from a frozen SAM and only train a lightweight matting decoder by end-to-end matting losses, which do not fully exploit the potential of the pre-trained SAM. Thus, we propose SEMat which revamps the network architecture and training objectives. For network architecture, the proposed feature-aligned transformer learns to extract fine-grained edge and transparency features. The proposed matte-aligned decoder aims to segment matting-specific objects and convert coarse masks into high-precision mattes. For training objectives, the proposed regularization and trimap loss aim to retain the prior from the pre-trained model and push the matting logits extracted from the mask decoder to contain trimap-based semantic information. Extensive experiments across seven diverse datasets demonstrate the superior performance of our method, proving its efficacy in interactive natural image matting. We open-source our code, models, and dataset at https://github.com/XiaRho/SEMat.', 'score': 1, 'issue_id': 126, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'В статье представлен новый датасет COCO-Matting для задачи интерактивного маттинга изображений, основанный на реальных сложных сценах из COCO. Авторы предлагают архитектуру SEMat, которая улучшает существующие методы на основе SAM, используя feature-aligned transformer и matte-aligned decoder. Введены новые функции потерь для обучения модели, включая регуляризацию и trimap loss. Эксперименты на семи различных датасетах показывают превосходство предложенного метода в задаче интерактивного маттинга естественных изображений.', 'categories': ['#cv', '#dataset', '#benchmark'], 'emoji': '✂️', 'title': 'Революция в интерактивном маттинге: от синтетики к реальности', 'embedding': [0.06896135535924419, 0.041528201695645424, 0.06922237360063092, 0.028999267311730074, -0.07089290084503316, 0.03134844198373785, 0.08686729071459144, 0.054918504675758664, 0.00813075769741888, 0.010603917504019145, 0.08979070971746088, -0.11745877559849692, -0.04108447068528794, -0.0070018483436671336, 0.03090470887347497, -0.03273184496280384, 0.03865698844095874, 0.030487077587350767, -0.03677764870335252, 0.05841616590958435, 0.029390796773715602, -0.046774693546383965, 0.019237137835994008, 0.004864751307771902, -0.04082345034399579, -0.011922065922738449, -0.037012565120600596, 0.03518542693136632, 0.034376270383067384, 0.00841787944286863, -0.0007618506773815627, -0.04272889400564609, -0.11829403397093452, -0.07261563173771274, 0.05121202884882357, 0.08853781795899367, -0.11067226982386034, 0.02127309061833768, -0.1573425518738788, 0.03257523401797179, 0.08086384176402053, -0.014277767100733597, 0.026754497836371596, 0.03369761875565103, 0.02543634941765229, 0.026062799496696702, 0.0519167802004732, -0.023439551471422033, 0.0705274648075647, 0.09260971932387022, -0.05272593884867754, 0.10247625714611368, -0.02222581034925743, -0.06394977992575372, -0.04766216186681512, -0.037482400055002155, -0.018884762160169193, 0.03273184496280384, -0.016496432652048, -0.019928839325527, -0.06394977992575372, -0.00565433484781547, -0.031165729214767124, 0.02785078074991216, -0.049358788835450626, 0.035394245724286524, -0.09850875887798104, 0.007660922553026785, 0.024196508571254417, -0.02991283535639451, 0.07068407575239674, -0.004877802009850699, -0.009116106388436294, -0.05946024097503677, -0.048001485580617904, -0.08634525423181794, -0.08091605381191948, 0.09715146402276992, -0.06039991504365068, 0.0007630742082627084, 0.05144694736597705, -0.007432530121879596, 0.09120021452066554, -0.07569565958550882, -0.09908300740865349, -0.04805369132880065, -0.07256342808943538, 0.013573014699131263, -0.16266734499722257, 0.04888895390104906, -0.04479094861212844, 0.13030093607188717, -0.04732283395320154, 0.040614637850791786, 0.036412221065505665, 0.022669545559425713, -0.11140312719945945, -0.09407143617497384, 0.16298057528650825, 0.11850285696366554, -0.06128737916427104, -0.06655997703895905, 0.019028322192931903, -0.033227783821249475, 0.05121202884882357, 0.012835633967402886, 0.06421079816714048, -0.02210835214063339, -0.11150753869582494, 0.0327840507109866, 0.06415859451886312, -0.12664665969341862, 0.023648367114484134, 0.038865805133973544, 0.06488944979455682, -0.030095550225270638, -0.08149028764325413, 0.014173359804178897, -0.023282940526589982, 0.03281015253512527, 0.03487220504170222, -0.002841849857478645, 0.06922237360063092, 0.023413449647283355, 0.059616858219585026, 0.03669934323093649, 0.04244177184021526, -0.058781593547431205, -0.01620931132657933, -0.02491431293487879, 0.03766511282397288, -0.04967201282502012, 0.07026645286589414, -0.014290818012802936, -0.09662941914037482, -0.07110171333823716, -0.1545235548669019, 0.052230002090137305, -0.08697169801114615, 0.03166166597330736, -0.03818715350655719, -0.022591237987104284, -0.008835509469049593, 0.025462453341696372, -0.06447181640852723, -0.04823640199786598, 0.0055792922084120485, 0.03510712145895029, -0.13875796909092597, 0.0662989545977615, -0.07533022564794575, -0.08932087478305932, 0.020020195710012366, 0.023034971097367164, -0.06494165344283419, -0.046748593822150686, -0.008515761493379202, 0.04439941915014291, -0.00592840568119588, -0.035994589779381456, 0.026206357429554013, 0.036464424713783014, 0.03369761875565103, 0.06896135535924419, -0.04301601617107692, 0.04181532386107625, -0.13124060594069029, 0.11380449922002837, -0.10216303525644957, -0.046800799570333444, 0.12800396504815675, -0.07355529740670504, 0.005644546558768197, -0.13447724893312923, -0.056014787589299224, -0.01075400404276923, -0.025788726143429808, -0.03333218901789878, -0.06739523541139666, -0.045469596039733995, -0.09010393370703039, 0.06410639297049117, -0.057841923678628095, 0.11818963507400143, -0.053508997772648594, -0.02046393092018065, 0.06494165344283419, 0.07120612273469726, 0.09662941914037482, 0.00946848248424219, -0.06645556554259356, 0.041136672233659896, 0.08916426383822727, -0.022800053630166385, 0.03706477086878335, -0.005569503919364776, 0.11338687633352577, 0.0999182699809019, 0.03615120282411891, -0.032418623073139745, 0.0001150117201359218, 0.05695445535819693, 0.04860183173561824, 0.027041621051755126, 0.04056243210260904, 0.03497661443816232, -0.024744648978071996, -0.0588860008439859, -0.057633106985613296, -0.04930658518717327, 0.11547502856433599, 0.06969220643512708, 0.10184981126688009, 0.0206205418650127, -0.03672544295516977, -0.03168776779744603, -0.05622360218240863, -0.07788822121277914, 0.011112905174628715, 0.11234279706826256, -0.020568336116829948, 0.007993722490719215, 0.10586951108338467, -0.026362970474291467, 0.04873234295621701, -0.012822583055333549, 0.006701675896138588, -0.07893230047804235, -0.00830042060427297, -0.0995006365948723, -0.05408324210351025, 0.021977843019940015, -0.06447181640852723, 0.011850284856404391, 0.026963314529386398, -0.040405819057871585, 0.0009869799815293912, -0.1244540980661483, 0.08081164021564857, 0.0621748474847022, 0.04129328527839735, 0.039387843716652444, -0.011830708698290927, 0.08008078284004946, -0.019419851654917435, 0.08937307633143128, 0.0037586807351556854, -0.08905985444176717, -0.012913939439818911, -0.05343069230023257, 0.16016156987990973, -0.0005615997504874798, 0.013357672550081792, -0.07715736383718003, -0.004682037488848475, 0.01325979049957122]}}, {'id': 'https://huggingface.co/papers/2410.11619', 'title': 'MultiVENT 2.0: A Massive Multilingual Benchmark for Event-Centric Video Retrieval', 'url': 'https://huggingface.co/papers/2410.11619', 'abstract': 'Efficiently retrieving and synthesizing information from large-scale multimodal collections has become a critical challenge. However, existing video retrieval datasets suffer from scope limitations, primarily focusing on matching descriptive but vague queries with small collections of professionally edited, English-centric videos. To address this gap, we introduce MultiVENT 2.0, a large-scale, multilingual event-centric video retrieval benchmark featuring a collection of more than 218,000 news videos and 3,906 queries targeting specific world events. These queries specifically target information found in the visual content, audio, embedded text, and text metadata of the videos, requiring systems leverage all these sources to succeed at the task. Preliminary results show that state-of-the-art vision-language models struggle significantly with this task, and while alternative approaches show promise, they are still insufficient to adequately address this problem. These findings underscore the need for more robust multimodal retrieval systems, as effective video retrieval is a crucial step towards multimodal content understanding and generation tasks.', 'score': 0, 'issue_id': 130, 'pub_date': '2024-10-15', 'pub_date_ru': '15 октября', 'data': {'desc': 'MultiVENT 2.0 - это новый крупномасштабный многоязычный бенчмарк для поиска видео, ориентированный на события. Он включает более 218 000 новостных видео и 3 906 запросов, нацеленных на конкретные мировые события. Бенчмарк требует от систем использования визуального контента, аудио, встроенного текста и текстовых метаданных видео. Предварительные результаты показывают, что современные модели компьютерного зрения и обработки естественного языка значительно уступают в этой задаче, подчеркивая необходимость более надежных мультимодальных систем поиска.', 'categories': ['#multimodal', '#benchmark', '#video', '#dataset'], 'emoji': '🎥', 'title': 'MultiVENT 2.0: Новый вызов в мире мультимодального поиска видео', 'embedding': [0.021813343868390643, 0.003003569748697959, 0.10596542238467484, -0.03859685057970808, -0.05420759201114238, 0.007798856619574405, 0.11873547757773831, 0.038492604587665145, -0.06093141801651371, 0.07323237263718364, 0.05248754166483351, -0.12071614077355461, -0.0308045084823964, -0.04112479671135019, 0.010463629024732718, 0.05212268708448294, -0.0010864323065227829, 0.012581113593685444, -0.043965487210920834, -0.0185817372904587, -0.030726323988364196, 0.004182845502704012, -0.08798310167906293, -0.060045335606548345, 0.057282831731609814, -0.07875738251766062, -0.022829735899009575, 0.09543664323693511, 0.14083549792424702, -0.015324069853695992, 0.06166113783021436, -0.030856630413117918, -0.09178805056023152, -0.035521612989840544, -0.06082717841627047, 0.107372732624255, -0.059628353768976496, 0.01872507552951774, -0.0489692628699833, 0.06765524828308483, 0.07005288905527318, -0.04545098407513308, 0.003883140193120479, 0.05420759201114238, 0.047092845666209936, -0.017604437506855863, -0.004062311926644326, 0.009531936010408712, -0.07662034820318, 0.10831094229144161, -0.09705244759120105, 0.0013918386753261594, -0.05058506722424922, -0.020575429104680465, 0.05738707772365275, -0.11008311137257214, -0.009434206884288388, -0.04075994213099962, -0.07073048374235248, 0.0027608735398929264, -0.04834379117892548, -0.1121680226910313, 0.024041589164709016, -0.061869627683700336, -0.04404366744375322, -0.0675510022910419, 0.06343330691134488, 0.09611423153221471, 0.015610743988154178, -0.04169815179818625, -0.03195120693776905, 0.07635973322307266, 0.013773419883636874, -0.016978965815798042, 0.014854965233162664, 0.05957622864235512, -0.07505666684493555, 0.038414420093632946, -0.12426048532761536, -0.04980322654512699, 0.026895306152085217, -0.09256989336995365, -0.10403688218488, 0.04975110567970543, -0.09512390313020641, 0.009988009455816689, -0.059367738788869154, -0.0056618267793535895, -0.03737196443440337, -0.018685983282501636, -0.09481116302347768, 0.08511634328968186, -0.021709097876347703, 0.08099865430178456, 0.07781916858927417, 0.021565761767888568, 0.03346276316939213, -0.0450861231029828, 0.11956944338348191, 0.09006800021392265, -0.11404444202540455, -0.06051443830954176, 0.03450521882862171, -0.018959626881014446, -0.034453095832600235, -0.0019236529445138535, 0.039352629760819574, 0.014346769430913188, 0.0235333942146995, -0.07073048374235248, -0.12884728341030577, -0.010939249019768728, -0.08136351527393483, 0.026895306152085217, 0.11195953070694542, -0.07291963253045494, -0.005205753547005601, -0.03338457867535993, -0.00564553834309688, 0.028563231371772704, -0.024745245349799043, 0.055093678682307544, 0.013851604164609089, -0.016496830233199363, 0.12050764665886883, -0.04920381848267981, 0.04109873734393936, 0.040655696138956685, 0.019272364857143256, -0.05994108961450541, 0.015363162100712093, 0.0148810267311734, 0.07667247119920147, 0.036772556371956186, -0.11727603795033699, -0.03666831037991325, -0.16012088926919585, 0.03468764931469685, -0.11883972143918133, 0.06510123426163228, -0.02603528097893078, -0.0498814131697591, 0.02591800423788248, 0.12259255797732797, -0.06285996034690844, -0.020262693259151553, -0.013604020998807063, 0.02335096372862436, 0.002029527013166497, 0.04016052980735263, -0.11977793749816769, -0.08193685970777138, -0.07271115332996847, 0.04928200084611211, -0.08542908126581068, -0.09366446776400487, -0.042349682856654906, 0.0460764557661909, 0.047144968662231404, -0.05008990302324507, 0.07490030211807096, -0.008841310787384043, 0.023259748485586793, 0.006652157525021779, 0.022060925968892718, 0.06426727271708849, -0.11550387526100617, 0.0332803348139169, -0.09856400169222422, -0.11946519739143896, -0.0004177958971826999, -0.009642697376954331, -0.02989236137852045, -0.020731798092744873, 0.016158032889659717, -0.0967918326110937, -0.0006254722450137952, -0.006495789602257324, 0.034739772310718314, -0.05900288207791868, -0.07172081427496066, 0.025983157982909316, -0.011356230644280586, 0.026764998662031542, -0.04542492257712234, 0.02529253361212461, -0.09512390313020641, 0.05436396099920678, -0.024641001488356008, -0.030048728235984954, -0.04612857876221237, 0.10622603736478219, 0.09126682060001684, 0.07771492259723124, 0.0774543076171239, 0.009597089755435547, 0.036954984727431424, 0.04492975411491839, 0.023012166385084718, -0.06504911126561082, 0.009851188082680266, 0.028615354367794173, 0.0016280195096285083, -0.1444840884703507, -0.008196292120578211, -0.054572452983292656, -0.02827655702425453, 0.10262956981469996, -0.08892130282384995, 0.03614708468089837, 0.05827316439481792, 0.0919965425443174, 0.11811000801728037, -0.010672119517398663, -0.0373980259324141, 0.03786713289660732, -0.12227982000119916, 0.040655696138956685, -0.09147531258410271, -0.0686976996811146, 0.04453883590595718, 0.009844672708177581, 0.12770058388963318, -0.003440097283657877, -0.0680722322512566, 0.01451616788962302, -0.03262880162484834, -0.05243542079941195, 0.004896274962608051, -0.1775298719327709, 0.052539666791454886, -0.008437359911877555, -0.047066788429399, -0.004922336460618786, -0.057543446711717156, -0.043261826764631, 0.06301632720437295, -0.016496830233199363, -0.05277422027355149, -0.017917173352384776, 0.07286751166503337, 0.011134708976489296, -0.0438612412188779, -0.011304107648259117, -0.04847409866897915, 0.03249849413479467, -0.05334756896858785, -0.06874982267713607, 0.1048708415988239, -0.04787468634533216, 0.07208567311651104, -0.07922648095945423, -0.06041019231749882, -0.0539991000270565, 0.02950144103895934, 0.028068067170768554]}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'default';        

        function loadSettings() {
            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            const themeToggle = document.getElementById('theme-toggle');
            let settingSortBy = localStorage.getItem('sort_by');
            const sortDropdown = document.getElementById('sort-dropdown');
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "хф найтли";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'default';
            }
            
            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            return Array.from(categories);
        }
        
        function createCategoryButtons() {
            const categories = getUniqueCategories(articlesData);
            categories.forEach(category => {
                const button = document.createElement('span');
                button.textContent = category;
                button.className = 'category-button';
                button.onclick = () => toggleCategory(category, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if (selectedArticles.length === articlesData.length) {
                categoryToggle.textContent = '🏷️ Фильтр';
            } else {
                categoryToggle.textContent = `🏷️ Фильтр (${formatArticlesTitle(selectedArticles.length)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const savedCategories = localStorage.getItem('selectedCategories');
            if (savedCategories) {
                if (savedCategories != '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);
                    updateCategoryButtonStates();
                }
            }
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles = selectedCategories.length === 0
                ? articlesData
                : articlesData.filter(article => 
                    article.data && article.data.categories && 
                    article.data.categories.some(cat => selectedCategories.includes(cat))
                );

            console.log('filteredArticles', filteredArticles)

            if (filteredArticles.length === 0) {
                selectedArticles = articlesData;
                selectedCategories = [];
                cleanCategorySelection();
            } else {
                selectedArticles = filteredArticles;
            }

            console.log('selectedArticles', selectedArticles)

            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
        }

        function renderArticles(articles) {
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                const explanation = item["data"]["desc"];
                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${item['data']['title']}</p>
                            <p class="pub-date">📅 Статья от ${item['pub_date_ru']}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">Статья</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            }
            if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
        });

        clearCategoriesButton.addEventListener('click', clearAllCategories);
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiffRu('2024-10-16 18:16');
        } 

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();  
    </script>
</body>
</html>
    