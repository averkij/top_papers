
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 17 papers. August 26.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">26 августа</span> | <span id="title-articles-count">17 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-08-25.html">⬅️ <span id="prev-date">25.08</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-08-27.html">➡️ <span id="next-date">27.08</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-08.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '26 августа', 'en': 'August 26', 'zh': '8月26日'};
        let feedDateNext = {'ru': '27.08', 'en': '08/27', 'zh': '8月27日'};
        let feedDatePrev = {'ru': '25.08', 'en': '08/25', 'zh': '8月25日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2508.18265', 'title': 'InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,\n  Reasoning, and Efficiency', 'url': 'https://huggingface.co/papers/2508.18265', 'abstract': 'InternVL 3.5 introduces Cascade RL, ViR, and DvD to enhance reasoning, efficiency, and performance in multimodal models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce InternVL 3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning performance and a 4.05times inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e., InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks -- narrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released.', 'score': 78, 'issue_id': 5540, 'pub_date': '2025-08-25', 'pub_date_card': {'ru': '25 августа', 'en': 'August 25', 'zh': '8月25日'}, 'hash': 'f9e73f2c4171c881', 'authors': ['Weiyun Wang', 'Zhangwei Gao', 'Lixin Gu', 'Hengjun Pu', 'Long Cui', 'Xingguang Wei', 'Zhaoyang Liu', 'Linglin Jing', 'Shenglong Ye', 'Jie Shao', 'Zhaokai Wang', 'Zhe Chen', 'Hongjie Zhang', 'Ganlin Yang', 'Haomin Wang', 'Qi Wei', 'Jinhui Yin', 'Wenhao Li', 'Erfei Cui', 'Guanzhou Chen', 'Zichen Ding', 'Changyao Tian', 'Zhenyu Wu', 'Jingjing Xie', 'Zehao Li', 'Bowen Yang', 'Yuchen Duan', 'Xuehui Wang', 'Songze Li', 'Xiangyu Zhao', 'Haodong Duan', 'Nianchen Deng', 'Bin Fu', 'Yinan He', 'Yi Wang', 'Conghui He', 'Botian Shi', 'Junjun He', 'Yingtong Xiong', 'Han Lv', 'Lijun Wu', 'Wenqi Shao', 'Kaipeng Zhang', 'Huipeng Deng', 'Biqing Qi', 'Jiaye Ge', 'Qipeng Guo', 'Wenwei Zhang', 'Wanli Ouyang', 'Limin Wang', 'Min Dou', 'Xizhou Zhu', 'Tong Lu', 'Dahua Lin', 'Jifeng Dai', 'Bowen Zhou', 'Weijie Su', 'Kai Chen', 'Yu Qiao', 'Wenhai Wang', 'Gen Luo'], 'affiliations': ['Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2508.18265.jpg', 'data': {'categories': ['#training', '#agents', '#reasoning', '#architecture', '#inference', '#open_source', '#multimodal', '#rl'], 'emoji': '🧠', 'ru': {'title': 'InternVL 3.5: Революция в мультимодальном ИИ через каскадное обучение с подкреплением', 'desc': 'InternVL 3.5 представляет собой новое семейство открытых мультимодальных моделей, значительно улучшающих универсальность, способность к рассуждениям и эффективность вывода. Ключевым нововведением является фреймворк Cascade Reinforcement Learning (Cascade RL), который улучшает рассуждения через двухэтапный процесс: офлайн-обучение с подкреплением для стабильной сходимости и онлайн-обучение для уточненного выравнивания. Для оптимизации эффективности предложен Visual Resolution Router (ViR), динамически регулирующий разрешение визуальных токенов. Модель достигает значительного улучшения производительности в задачах рассуждений и ускорения вывода по сравнению с предшественником.'}, 'en': {'title': 'Revolutionizing Multimodal Reasoning with InternVL 3.5', 'desc': 'InternVL 3.5 is a new set of open-source multimodal models that improve reasoning, efficiency, and performance. It introduces Cascade Reinforcement Learning (Cascade RL), which uses a two-stage training process to enhance reasoning capabilities. The Visual Resolution Router (ViR) optimizes the resolution of visual inputs dynamically, while the Decoupled Vision-Language Deployment (DvD) strategy balances the computational load across GPUs. These innovations lead to significant performance gains in reasoning tasks and faster inference speeds compared to previous models.'}, 'zh': {'title': '提升推理与效率的多模态模型', 'desc': 'InternVL 3.5 是一款新型的开源多模态模型，显著提升了其多样性、推理能力和推理效率。其核心创新是级联强化学习（Cascade RL）框架，通过离线和在线强化学习的两阶段过程来增强推理能力。为了优化效率，提出了视觉分辨率路由器（ViR），动态调整视觉标记的分辨率，同时不影响性能。结合解耦视觉-语言部署（DvD）策略，InternVL 3.5 在推理性能上提升了 16.0%，并实现了 4.05 倍的推理速度提升。'}}}, {'id': 'https://huggingface.co/papers/2508.18032', 'title': 'Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance\n  for Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2508.18032', 'abstract': 'The Visual-Chain of Guidance (Visual-CoG) paradigm enhances text-to-image generation by providing stage-aware rewards, improving performance across multiple benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the promising progress of recent autoregressive models in text-to-image (T2I) generation, their ability to handle multi-attribute and ambiguous prompts remains limited. To address these limitations, existing works have applied chain-of-thought (CoT) to enable stage-aware visual synthesis and employed reinforcement learning (RL) to improve reasoning capabilities. However, most models provide reward signals only at the end of the generation stage. This monolithic final-only guidance makes it difficult to identify which stages contribute positively to the final outcome and may lead to suboptimal policies. To tackle this issue, we propose a Visual-Chain of Guidance (Visual-CoG) paradigm consisting of three stages: semantic reasoning, process refining, and outcome evaluation, with stage-aware rewards providing immediate guidance throughout the image generation pipeline. We further construct a visual cognition benchmark, VisCog-Bench, which comprises four subtasks to evaluate the effectiveness of semantic reasoning. Comprehensive evaluations on GenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%, 5%, and 19%, respectively, demonstrating the superior performance of the proposed Visual-CoG. We will release all the resources soon.', 'score': 26, 'issue_id': 5540, 'pub_date': '2025-08-25', 'pub_date_card': {'ru': '25 августа', 'en': 'August 25', 'zh': '8月25日'}, 'hash': '19b061daf44e7f58', 'authors': ['Yaqi Li', 'Peng Chen', 'Mingyang Han', 'Bu Pi', 'Haoxiang Shi', 'Runzhou Zhao', 'Yang Yao', 'Xuan Zhang', 'Jun Song'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2508.18032.jpg', 'data': {'categories': ['#games', '#benchmark', '#reasoning', '#rl', '#optimization', '#cv'], 'emoji': '🎨', 'ru': {'title': 'Поэтапное обучение для улучшенной генерации изображений по тексту', 'desc': 'Статья представляет новую парадигму Visual-Chain of Guidance (Visual-CoG) для улучшения генерации изображений по текстовому описанию. Авторы предлагают трехэтапный процесс с поэтапным обучением с подкреплением, что позволяет лучше обрабатывать сложные и неоднозначные запросы. Метод включает семантические рассуждения, уточнение процесса и оценку результата, с вознаграждениями на каждом этапе. Эксперименты на нескольких бенчмарках показывают значительное улучшение производительности по сравнению с существующими подходами.'}, 'en': {'title': 'Stage-Aware Rewards for Enhanced Image Generation', 'desc': "The Visual-Chain of Guidance (Visual-CoG) paradigm improves text-to-image generation by introducing stage-aware rewards that enhance the model's performance. Traditional models often provide feedback only at the end of the generation process, making it hard to understand which parts of the process are effective. Visual-CoG breaks down the generation into three stages: semantic reasoning, process refining, and outcome evaluation, allowing for immediate feedback at each stage. This approach leads to significant performance improvements across various benchmarks, demonstrating its effectiveness in handling complex prompts."}, 'zh': {'title': '视觉引导链：提升文本到图像生成的革命性方法', 'desc': '本文提出了一种名为视觉引导链（Visual-CoG）的新范式，旨在提升文本到图像生成的效果。该方法通过提供阶段性奖励，改善了模型在处理多属性和模糊提示时的表现。与传统的仅在生成结束时提供奖励的方式不同，Visual-CoG在生成过程中每个阶段都给予即时指导，从而优化生成策略。通过在多个基准测试上的评估，Visual-CoG显示出显著的性能提升，证明了其有效性。'}}}, {'id': 'https://huggingface.co/papers/2508.16577', 'title': 'MV-RAG: Retrieval Augmented Multiview Diffusion', 'url': 'https://huggingface.co/papers/2508.16577', 'abstract': 'MV-RAG enhances text-to-3D generation by retrieving 2D images and conditioning a multiview diffusion model to improve consistency and accuracy, especially for out-of-domain concepts.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-3D generation approaches have advanced significantly by leveraging pretrained 2D diffusion priors, producing high-quality and 3D-consistent outputs. However, they often fail to produce out-of-domain (OOD) or rare concepts, yielding inconsistent or inaccurate results. To this end, we propose MV-RAG, a novel text-to-3D pipeline that first retrieves relevant 2D images from a large in-the-wild 2D database and then conditions a multiview diffusion model on these images to synthesize consistent and accurate multiview outputs. Training such a retrieval-conditioned model is achieved via a novel hybrid strategy bridging structured multiview data and diverse 2D image collections. This involves training on multiview data using augmented conditioning views that simulate retrieval variance for view-specific reconstruction, alongside training on sets of retrieved real-world 2D images using a distinctive held-out view prediction objective: the model predicts the held-out view from the other views to infer 3D consistency from 2D data. To facilitate a rigorous OOD evaluation, we introduce a new collection of challenging OOD prompts. Experiments against state-of-the-art text-to-3D, image-to-3D, and personalization baselines show that our approach significantly improves 3D consistency, photorealism, and text adherence for OOD/rare concepts, while maintaining competitive performance on standard benchmarks.', 'score': 19, 'issue_id': 5545, 'pub_date': '2025-08-22', 'pub_date_card': {'ru': '22 августа', 'en': 'August 22', 'zh': '8月22日'}, 'hash': '346f008a32848d96', 'authors': ['Yosef Dayani', 'Omer Benishu', 'Sagie Benaim'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2508.16577.jpg', 'data': {'categories': ['#synthetic', '#diffusion', '#3d', '#multimodal', '#benchmark', '#rag'], 'emoji': '🖼️', 'ru': {'title': 'Улучшение генерации 3D из текста с помощью извлечения 2D-изображений', 'desc': 'MV-RAG - это новый подход к генерации 3D-объектов из текста, который использует извлечение релевантных 2D-изображений для улучшения качества результатов. Метод применяет мультивидовую диффузионную модель, обученную на структурированных 3D-данных и разнообразных 2D-коллекциях изображений. MV-RAG значительно повышает 3D-согласованность, фотореалистичность и соответствие тексту для редких и нестандартных концепций. Авторы также представили новый набор сложных запросов для оценки генерации объектов вне обучающей выборки.'}, 'en': {'title': 'Enhancing 3D Generation with 2D Image Retrieval', 'desc': 'MV-RAG is a new method for generating 3D models from text descriptions by first retrieving relevant 2D images from a large database. It uses a multiview diffusion model that conditions on these images to create outputs that are more consistent and accurate, especially for rare or out-of-domain concepts. The training process combines multiview data with diverse 2D images, allowing the model to learn how to predict different views from a set of images. Experiments show that MV-RAG outperforms existing methods in producing high-quality, photorealistic 3D outputs that adhere closely to the input text, even for challenging prompts.'}, 'zh': {'title': 'MV-RAG：提升文本到3D生成的一致性与准确性', 'desc': 'MV-RAG是一种新颖的文本到3D生成管道，通过从大型2D数据库中检索相关的2D图像，并将其用于多视角扩散模型的条件化，从而提高生成的一致性和准确性，特别是在处理域外概念时。该方法通过一种新颖的混合策略进行训练，结合了结构化的多视角数据和多样化的2D图像集合。模型在多视角数据上进行训练，同时使用增强的条件视图来模拟检索变异，以实现视图特定的重建。实验结果表明，MV-RAG在处理域外或稀有概念时，显著提高了3D一致性、照片真实感和文本遵循性，同时在标准基准测试中保持了竞争力。'}}}, {'id': 'https://huggingface.co/papers/2508.17472', 'title': 'T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image\n  Generation', 'url': 'https://huggingface.co/papers/2508.17472', 'abstract': 'T2I-ReasonBench evaluates the reasoning capabilities of text-to-image models across four dimensions using a two-stage protocol, analyzing their performance comprehensively.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose T2I-ReasonBench, a benchmark evaluating reasoning capabilities of text-to-image (T2I) models. It consists of four dimensions: Idiom Interpretation, Textual Image Design, Entity-Reasoning and Scientific-Reasoning. We propose a two-stage evaluation protocol to assess the reasoning accuracy and image quality. We benchmark various T2I generation models, and provide comprehensive analysis on their performances.', 'score': 18, 'issue_id': 5541, 'pub_date': '2025-08-24', 'pub_date_card': {'ru': '24 августа', 'en': 'August 24', 'zh': '8月24日'}, 'hash': '26d1dcc01fb66261', 'authors': ['Kaiyue Sun', 'Rongyao Fang', 'Chengqi Duan', 'Xian Liu', 'Xihui Liu'], 'affiliations': ['The Chinese University of Hong Kong', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2508.17472.jpg', 'data': {'categories': ['#cv', '#benchmark', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Многомерная оценка разумности text-to-image моделей', 'desc': 'T2I-ReasonBench - это новый бенчмарк для оценки способностей моделей text-to-image к рассуждению. Он включает четыре измерения: интерпретацию идиом, текстовый дизайн изображений, рассуждения о сущностях и научные рассуждения. Авторы предлагают двухэтапный протокол оценки для анализа точности рассуждений и качества изображений. Бенчмарк был применен к различным моделям генерации T2I, что позволило провести комплексный анализ их производительности.'}, 'en': {'title': 'Evaluating Reasoning in Text-to-Image Models', 'desc': 'T2I-ReasonBench is a benchmark designed to assess the reasoning abilities of text-to-image (T2I) models. It evaluates these models across four key dimensions: Idiom Interpretation, Textual Image Design, Entity-Reasoning, and Scientific-Reasoning. The evaluation follows a two-stage protocol that measures both the accuracy of reasoning and the quality of the generated images. By benchmarking multiple T2I models, the study provides a detailed analysis of their performance in reasoning tasks.'}, 'zh': {'title': '评估文本到图像模型的推理能力', 'desc': 'T2I-ReasonBench是一个评估文本到图像（T2I）模型推理能力的基准。它涵盖了四个维度：成语解释、文本图像设计、实体推理和科学推理。我们提出了一个两阶段的评估协议，以评估推理准确性和图像质量。通过对多种T2I生成模型进行基准测试，我们提供了对其性能的全面分析。'}}}, {'id': 'https://huggingface.co/papers/2508.16745', 'title': 'Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory\n  and Test-Time Compute Scaling', 'url': 'https://huggingface.co/papers/2508.16745', 'abstract': 'Models trained on random Boolean functions in a cellular automata framework show that increasing depth, recurrence, memory, and test-time compute scaling enhances multi-step reasoning capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning is a core capability of large language models, yet understanding how they learn and perform multi-step reasoning remains an open problem. In this study, we explore how different architectures and training methods affect model multi-step reasoning capabilities within a cellular automata framework. By training on state sequences generated with random Boolean functions for random initial conditions to exclude memorization, we demonstrate that most neural architectures learn to abstract the underlying rules. While models achieve high accuracy in next-state prediction, their performance declines sharply if multi-step reasoning is required. We confirm that increasing model depth plays a crucial role for sequential computations. We demonstrate that an extension of the effective model depth with recurrence, memory, and test-time compute scaling substantially enhances reasoning capabilities.', 'score': 14, 'issue_id': 5546, 'pub_date': '2025-08-22', 'pub_date_card': {'ru': '22 августа', 'en': 'August 22', 'zh': '8月22日'}, 'hash': 'a385e4e24a0f85fb', 'authors': ['Ivan Rodkin', 'Daniil Orel', 'Konstantin Smirnov', 'Arman Bolatov', 'Bilal Elbouardi', 'Besher Hassan', 'Yuri Kuratov', 'Aydar Bulatov', 'Preslav Nakov', 'Timothy Baldwin', 'Artem Shelmanov', 'Mikhail Burtsev'], 'affiliations': ['AIRI, Moscow, Russia', 'London Institute for Mathematical Sciences, London, UK', 'MBZUAI, Abu Dhabi, UAE', 'Neural Networks and Deep Learning Lab, MIPT, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2508.16745.jpg', 'data': {'categories': ['#reasoning', '#architecture', '#training'], 'emoji': '🧠', 'ru': {'title': 'Глубина и рекуррентность - ключ к улучшению рассуждений нейросетей', 'desc': 'В статье исследуется, как различные архитектуры и методы обучения влияют на способности моделей к многоступенчатым рассуждениям в рамках клеточных автоматов. Авторы обучали нейронные сети на последовательностях состояний, генерируемых случайными булевыми функциями, чтобы исключить простое запоминание. Результаты показывают, что увеличение глубины модели, использование рекуррентных связей, памяти и масштабирование вычислений во время тестирования значительно улучшают способности к рассуждениям. Однако производительность моделей резко падает при необходимости многоступенчатых рассуждений.'}, 'en': {'title': 'Enhancing Multi-Step Reasoning in Neural Networks', 'desc': 'This paper investigates how different neural network architectures and training methods influence the ability of models to perform multi-step reasoning. Using a cellular automata framework, the authors train models on sequences generated by random Boolean functions to prevent memorization. They find that while models can predict the next state accurately, their performance drops significantly when multi-step reasoning is needed. The study highlights that increasing model depth, along with incorporating recurrence, memory, and scaling during testing, significantly improves the reasoning capabilities of these models.'}, 'zh': {'title': '深度与递归提升多步推理能力', 'desc': '本研究探讨了不同架构和训练方法如何影响模型的多步推理能力，采用了细胞自动机框架。通过在随机布尔函数生成的状态序列上进行训练，我们排除了记忆的影响，发现大多数神经网络架构能够抽象出基本规则。尽管模型在下一状态预测中表现出高准确率，但在需要多步推理时，其性能显著下降。我们确认增加模型深度对顺序计算至关重要，并且通过递归、记忆和测试时计算扩展有效深度，可以显著增强推理能力。'}}}, {'id': 'https://huggingface.co/papers/2508.16949', 'title': 'Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement\n  Learning for General LLM Reasoning', 'url': 'https://huggingface.co/papers/2508.16949', 'abstract': 'RuscaRL, a novel instructional scaffolding framework, enhances LLM reasoning by using rubrics for exploration and verifiable rewards, significantly improving performance on reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Models (LLMs) have underscored the potential of Reinforcement Learning (RL) to facilitate the emergence of reasoning capabilities. Despite the encouraging results, a fundamental dilemma persists as RL improvement relies on learning from high-quality samples, yet the exploration for such samples remains bounded by the inherent limitations of LLMs. This, in effect, creates an undesirable cycle in which what cannot be explored cannot be learned. In this work, we propose Rubric-Scaffolded Reinforcement Learning (RuscaRL), a novel instructional scaffolding framework designed to break the exploration bottleneck for general LLM reasoning. Specifically, RuscaRL introduces checklist-style rubrics as (1) explicit scaffolding for exploration during rollout generation, where different rubrics are provided as external guidance within task instructions to steer diverse high-quality responses. This guidance is gradually decayed over time, encouraging the model to internalize the underlying reasoning patterns; (2) verifiable rewards for exploitation during model training, where we can obtain robust LLM-as-a-Judge scores using rubrics as references, enabling effective RL on general reasoning tasks. Extensive experiments demonstrate the superiority of the proposed RuscaRL across various benchmarks, effectively expanding reasoning boundaries under the best-of-N evaluation. Notably, RuscaRL significantly boosts Qwen-2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500, surpassing GPT-4.1. Furthermore, our fine-tuned variant on Qwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading LLMs including OpenAI-o3.', 'score': 12, 'issue_id': 5541, 'pub_date': '2025-08-23', 'pub_date_card': {'ru': '23 августа', 'en': 'August 23', 'zh': '8月23日'}, 'hash': 'd0852bed475aa719', 'authors': ['Yang Zhou', 'Sunzhu Li', 'Shunyu Liu', 'Wenkai Fang', 'Jiale Zhao', 'Jingwen Yang', 'Jianwei Lv', 'Kongcheng Zhang', 'Yihe Zhou', 'Hengtong Lu', 'Wei Chen', 'Yan Xie', 'Mingli Song'], 'affiliations': ['Li Auto Inc.', 'Nanyang Technological University', 'The Chinese University of Hong Kong, Shenzhen', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.16949.jpg', 'data': {'categories': ['#training', '#rl', '#rlhf', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'RuscaRL: Усиление способностей ИИ к рассуждению через обучение с подкреплением', 'desc': 'RuscaRL - это новая система обучения больших языковых моделей (LLM) с использованием подкрепляющего обучения (RL). Она применяет рубрики в качестве инструкций для исследования и верифицируемых наград, что значительно улучшает способности LLM к рассуждению. RuscaRL решает проблему ограниченного исследования, предоставляя модели внешние указания в виде чек-листов. Эксперименты показывают превосходство RuscaRL на различных бенчмарках, значительно расширяя границы рассуждений LLM.'}, 'en': {'title': 'Unlocking Reasoning in LLMs with RuscaRL', 'desc': 'RuscaRL is a new framework that improves the reasoning abilities of Large Language Models (LLMs) by using structured rubrics to guide exploration and provide clear rewards. It addresses the challenge of limited exploration in LLMs, which hinders their learning from high-quality samples. By implementing checklist-style rubrics, RuscaRL helps models generate diverse and high-quality responses while gradually encouraging them to learn reasoning patterns independently. Experimental results show that RuscaRL significantly enhances performance on reasoning tasks, outperforming existing models like GPT-4.1.'}, 'zh': {'title': 'RuscaRL：打破推理瓶颈的创新框架', 'desc': 'RuscaRL是一种新颖的教学支架框架，通过使用评分标准来增强大型语言模型（LLM）的推理能力。该框架解决了探索样本的瓶颈问题，提供了清单式的评分标准作为外部指导，帮助模型生成多样化的高质量响应。RuscaRL还引入了可验证的奖励机制，使得模型在训练过程中能够有效利用这些评分标准进行强化学习。实验结果表明，RuscaRL在多个基准测试中表现优越，显著提升了推理能力。'}}}, {'id': 'https://huggingface.co/papers/2508.17188', 'title': 'PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent\n  LLMs', 'url': 'https://huggingface.co/papers/2508.17188', 'abstract': 'PosterGen, a multi-agent framework using large language models, automates paper-to-poster generation with high design quality and minimal manual refinement.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-agent systems built upon large language models (LLMs) have demonstrated remarkable capabilities in tackling complex compositional tasks. In this work, we apply this paradigm to the paper-to-poster generation problem, a practical yet time-consuming process faced by researchers preparing for conferences. While recent approaches have attempted to automate this task, most neglect core design and aesthetic principles, resulting in posters that require substantial manual refinement. To address these design limitations, we propose PosterGen, a multi-agent framework that mirrors the workflow of professional poster designers. It consists of four collaborative specialized agents: (1) Parser and Curator agents extract content from the paper and organize storyboard; (2) Layout agent maps the content into a coherent spatial layout; (3) Stylist agents apply visual design elements such as color and typography; and (4) Renderer composes the final poster. Together, these agents produce posters that are both semantically grounded and visually appealing. To evaluate design quality, we introduce a vision-language model (VLM)-based rubric that measures layout balance, readability, and aesthetic coherence. Experimental results show that PosterGen consistently matches in content fidelity, and significantly outperforms existing methods in visual designs, generating posters that are presentation-ready with minimal human refinements.', 'score': 7, 'issue_id': 5539, 'pub_date': '2025-08-24', 'pub_date_card': {'ru': '24 августа', 'en': 'August 24', 'zh': '8月24日'}, 'hash': 'fb29b368892da03a', 'authors': ['Zhilin Zhang', 'Xiang Zhang', 'Jiaqi Wei', 'Yiwei Xu', 'Chenyu You'], 'affiliations': ['New York University', 'Stony Brook University', 'University of British Columbia', 'University of California, Los Angeles', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.17188.jpg', 'data': {'categories': ['#multimodal', '#agents', '#optimization', '#agi', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'PosterGen: ИИ-дизайнер научных постеров', 'desc': 'PosterGen - это мультиагентная система на основе больших языковых моделей (LLM) для автоматического создания постеров из научных статей. Система состоит из четырех специализированных агентов, которые извлекают содержание, создают макет, применяют визуальный дизайн и формируют итоговый постер. PosterGen превосходит существующие методы по качеству визуального дизайна, создавая постеры, готовые к презентации с минимальными доработками. Для оценки качества дизайна авторы предложили рубрику на основе мультимодальной языковой модели (VLM).'}, 'en': {'title': 'Automating Academic Posters with PosterGen: Design Meets Efficiency', 'desc': 'PosterGen is a multi-agent framework that automates the process of creating academic posters from research papers using large language models. It consists of four specialized agents that work together: Parser and Curator agents extract and organize content, the Layout agent arranges this content spatially, the Stylist agents enhance the visual design, and the Renderer composes the final poster. This system not only ensures that the posters are semantically accurate but also visually appealing, addressing the shortcomings of previous automation methods. The framework is evaluated using a vision-language model to ensure high design quality, demonstrating superior performance in generating ready-to-present posters with minimal manual adjustments.'}, 'zh': {'title': 'PosterGen：自动生成高质量学术海报的智能框架', 'desc': 'PosterGen是一个基于大型语言模型的多智能体框架，旨在自动生成高质量的学术海报。该系统通过四个协作的专门代理，分别负责内容提取、布局设计、视觉风格应用和最终海报合成。与传统方法相比，PosterGen在设计美学和内容准确性上表现更佳，能够生成几乎无需人工修改的海报。通过引入视觉-语言模型评估设计质量，PosterGen确保了海报的可读性和视觉一致性。'}}}, {'id': 'https://huggingface.co/papers/2508.17580', 'title': 'UQ: Assessing Language Models on Unsolved Questions', 'url': 'https://huggingface.co/papers/2508.17580', 'abstract': 'UQ is a benchmark for evaluating AI models on unsolved questions, combining difficulty and realism to assess capabilities like reasoning, factuality, and browsing.  \t\t\t\t\tAI-generated summary \t\t\t\t Benchmarks shape progress in AI research. A useful benchmark should be both difficult and realistic: questions should challenge frontier models while also reflecting real-world usage. Yet, current paradigms face a difficulty-realism tension: exam-style benchmarks are often made artificially difficult with limited real-world value, while benchmarks based on real user interaction often skew toward easy, high-frequency problems. In this work, we explore a radically different paradigm: assessing models on unsolved questions. Rather than a static benchmark scored once, we curate unsolved questions and evaluate models asynchronously over time with validator-assisted screening and community verification. We introduce UQ, a testbed of 500 challenging, diverse questions sourced from Stack Exchange, spanning topics from CS theory and math to sci-fi and history, probing capabilities including reasoning, factuality, and browsing. UQ is difficult and realistic by construction: unsolved questions are often hard and naturally arise when humans seek answers, thus solving them yields direct real-world value. Our contributions are threefold: (1) UQ-Dataset and its collection pipeline combining rule-based filters, LLM judges, and human review to ensure question quality (e.g., well-defined and difficult); (2) UQ-Validators, compound validation strategies that leverage the generator-validator gap to provide evaluation signals and pre-screen candidate solutions for human review; and (3) UQ-Platform, an open platform where experts collectively verify questions and solutions. The top model passes UQ-validation on only 15% of questions, and preliminary human verification has already identified correct answers among those that passed. UQ charts a path for evaluating frontier models on real-world, open-ended challenges, where success pushes the frontier of human knowledge. We release UQ at https://uq.stanford.edu.', 'score': 5, 'issue_id': 5539, 'pub_date': '2025-08-25', 'pub_date_card': {'ru': '25 августа', 'en': 'August 25', 'zh': '8月25日'}, 'hash': '9f8a8ba45f2a8eca', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#dataset', '#benchmark', '#data', '#survey', '#reasoning', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'UQ: Оценка ИИ на грани человеческих знаний', 'desc': 'UQ - это новый бенчмарк для оценки моделей искусственного интеллекта на нерешенных вопросах, сочетающий сложность и реалистичность. Он включает 500 разнообразных вопросов из Stack Exchange, охватывающих темы от теории информатики до научной фантастики. UQ использует валидаторы для предварительной проверки решений и открытую платформу для экспертной верификации. Этот подход позволяет оценивать передовые возможности моделей ИИ в решении реальных открытых задач.'}, 'en': {'title': 'UQ: Evaluating AI on Real-World Unsolved Questions', 'desc': 'The paper introduces UQ, a new benchmark for evaluating AI models on unsolved questions, which combines difficulty and realism to better assess capabilities like reasoning and factuality. Unlike traditional benchmarks that often present artificially difficult questions, UQ focuses on real-world challenges that arise from genuine human inquiries. The UQ framework includes a dataset of 500 diverse questions, validation strategies, and a collaborative platform for community verification. This innovative approach aims to push the boundaries of AI performance by addressing open-ended challenges that reflect actual knowledge gaps.'}, 'zh': {'title': 'UQ：评估AI模型的新标准', 'desc': 'UQ是一个用于评估人工智能模型在未解决问题上的基准，结合了难度和现实性，以评估推理、事实性和浏览等能力。当前的基准往往面临难度与现实性之间的矛盾，而UQ通过评估未解决的问题，提供了一种新的评估范式。我们构建了一个包含500个来自Stack Exchange的挑战性问题的数据集，并引入了验证者辅助筛选和社区验证的方法。UQ为评估前沿模型在真实世界中的表现提供了新的路径，推动人类知识的前沿。'}}}, {'id': 'https://huggingface.co/papers/2508.17290', 'title': 'MEENA (PersianMMMU): Multimodal-Multilingual Educational Exams for\n  N-level Assessment', 'url': 'https://huggingface.co/papers/2508.17290', 'abstract': "MEENA, a Persian-English dataset, evaluates vision-language models across scientific, reasoning, and human-level understanding tasks, enhancing capabilities beyond English.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in large vision-language models (VLMs) have primarily focused on English, with limited attention given to other languages. To address this gap, we introduce MEENA (also known as PersianMMMU), the first dataset designed to evaluate Persian VLMs across scientific, reasoning, and human-level understanding tasks. Our dataset comprises approximately 7,500 Persian and 3,000 English questions, covering a wide range of topics such as reasoning, mathematics, physics, diagrams, charts, and Persian art and literature. Key features of MEENA include: (1) diverse subject coverage spanning various educational levels, from primary to upper secondary school, (2) rich metadata, including difficulty levels and descriptive answers, (3) original Persian data that preserves cultural nuances, (4) a bilingual structure to assess cross-linguistic performance, and (5) a series of diverse experiments assessing various capabilities, including overall performance, the model's ability to attend to images, and its tendency to generate hallucinations. We hope this benchmark contributes to enhancing VLM capabilities beyond English.", 'score': 5, 'issue_id': 5545, 'pub_date': '2025-08-24', 'pub_date_card': {'ru': '24 августа', 'en': 'August 24', 'zh': '8月24日'}, 'hash': 'f8b5adba597a583e', 'authors': ['Omid Ghahroodi', 'Arshia Hemmat', 'Marzia Nouri', 'Seyed Mohammad Hadi Hosseini', 'Doratossadat Dastgheib', 'Mohammad Vali Sanian', 'Alireza Sahebi', 'Reihaneh Zohrabi', 'Mohammad Hossein Rohban', 'Ehsaneddin Asgari', 'Mahdieh Soleymani Baghshah'], 'affiliations': ['Computer Engineering Department, Sharif University of Technology, Iran', 'Computer Engineering Department, University of Isfahan, Iran', 'Qatar Computing Research Institute, Qatar'], 'pdf_title_img': 'assets/pdf/title_img/2508.17290.jpg', 'data': {'categories': ['#low_resource', '#dataset', '#reasoning', '#hallucinations', '#multilingual', '#benchmark'], 'emoji': '🌐', 'ru': {'title': 'MEENA: Расширение возможностей мультимодальных моделей за пределы английского языка', 'desc': 'Представлен набор данных MEENA для оценки персидско-английских мультимодальных моделей в различных задачах. Датасет содержит около 7500 вопросов на персидском и 3000 на английском языках, охватывая широкий спектр тем. MEENA позволяет оценивать способности моделей к рассуждению, понимание научных концепций и общечеловеческих знаний. Особенности датасета включают разнообразие предметных областей, богатые метаданные и оригинальные персидские данные.'}, 'en': {'title': 'Enhancing Vision-Language Models Beyond English with MEENA', 'desc': 'The paper introduces MEENA, a dataset specifically designed to evaluate vision-language models (VLMs) in Persian and English. It addresses the lack of resources for non-English languages in the field of machine learning, particularly in tasks requiring scientific reasoning and human-level understanding. MEENA includes a diverse set of questions across various subjects and educational levels, along with rich metadata to aid in performance assessment. The dataset aims to improve the capabilities of VLMs by providing a bilingual framework that highlights cross-linguistic performance and cultural nuances.'}, 'zh': {'title': '提升视觉语言模型的波斯语能力', 'desc': 'MEENA是一个评估视觉语言模型的波斯语-英语数据集，旨在填补非英语语言的研究空白。该数据集包含约7500个波斯语和3000个英语问题，涵盖科学、推理和人类理解等任务。MEENA的特点包括多样的主题覆盖、丰富的元数据和保留文化细节的原始波斯语数据。通过这一基准，我们希望提升视觉语言模型在非英语语言上的能力。'}}}, {'id': 'https://huggingface.co/papers/2508.17298', 'title': 'Explain Before You Answer: A Survey on Compositional Visual Reasoning', 'url': 'https://huggingface.co/papers/2508.17298', 'abstract': 'A comprehensive survey of compositional visual reasoning from 2023 to 2025 reviews advancements in multimodal AI, highlighting architectural designs, benchmarks, and future directions.  \t\t\t\t\tAI-generated summary \t\t\t\t Compositional visual reasoning has emerged as a key research frontier in multimodal AI, aiming to endow machines with the human-like ability to decompose visual scenes, ground intermediate concepts, and perform multi-step logical inference. While early surveys focus on monolithic vision-language models or general multimodal reasoning, a dedicated synthesis of the rapidly expanding compositional visual reasoning literature is still missing. We fill this gap with a comprehensive survey spanning 2023 to 2025 that systematically reviews 260+ papers from top venues (CVPR, ICCV, NeurIPS, ICML, ACL, etc.). We first formalize core definitions and describe why compositional approaches offer advantages in cognitive alignment, semantic fidelity, robustness, interpretability, and data efficiency. Next, we trace a five-stage paradigm shift: from prompt-enhanced language-centric pipelines, through tool-enhanced LLMs and tool-enhanced VLMs, to recently minted chain-of-thought reasoning and unified agentic VLMs, highlighting their architectural designs, strengths, and limitations. We then catalog 60+ benchmarks and corresponding metrics that probe compositional visual reasoning along dimensions such as grounding accuracy, chain-of-thought faithfulness, and high-resolution perception. Drawing on these analyses, we distill key insights, identify open challenges (e.g., limitations of LLM-based reasoning, hallucination, a bias toward deductive reasoning, scalable supervision, tool integration, and benchmark limitations), and outline future directions, including world-model integration, human-AI collaborative reasoning, and richer evaluation protocols. By offering a unified taxonomy, historical roadmap, and critical outlook, this survey aims to serve as a foundational reference and inspire the next generation of compositional visual reasoning research.', 'score': 2, 'issue_id': 5542, 'pub_date': '2025-08-24', 'pub_date_card': {'ru': '24 августа', 'en': 'August 24', 'zh': '8月24日'}, 'hash': 'e2e4d94059c3bca1', 'authors': ['Fucai Ke', 'Joy Hsu', 'Zhixi Cai', 'Zixian Ma', 'Xin Zheng', 'Xindi Wu', 'Sukai Huang', 'Weiqing Wang', 'Pari Delir Haghighi', 'Gholamreza Haffari', 'Ranjay Krishna', 'Jiajun Wu', 'Hamid Rezatofighi'], 'affiliations': ['Allen Institute for Artificial Intelligence', 'Griffith University', 'Monash University', 'Princeton University', 'Stanford University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2508.17298.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#benchmark', '#hallucinations', '#survey', '#multimodal', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Композиционное визуальное рассуждение: от подсказок к агентам', 'desc': 'Эта статья представляет собой всесторонний обзор прогресса в области композиционного визуального рассуждения с 2023 по 2025 год. Авторы анализируют более 260 работ, описывая эволюцию подходов от языковых моделей с подсказками до агентных визуально-языковых моделей. В обзоре рассматриваются архитектурные решения, бенчмарки и метрики для оценки различных аспектов композиционного визуального рассуждения. Статья также выделяет ключевые проблемы и намечает перспективные направления будущих исследований в этой области.'}, 'en': {'title': 'Advancing AI: A Deep Dive into Compositional Visual Reasoning', 'desc': 'This paper presents a comprehensive survey of compositional visual reasoning in multimodal AI, covering advancements from 2023 to 2025. It reviews over 260 research papers, formalizes key definitions, and discusses the benefits of compositional approaches, such as improved cognitive alignment and interpretability. The authors outline a five-stage evolution in the field, highlighting various architectural designs and their respective strengths and weaknesses. Additionally, the survey identifies open challenges and proposes future research directions to enhance the capabilities of AI in visual reasoning tasks.'}, 'zh': {'title': '组合视觉推理的未来探索', 'desc': '这篇论文对2023年至2025年间的组合视觉推理进行了全面的调查，强调了多模态人工智能的进展。它系统地回顾了260多篇来自顶级会议的论文，探讨了组合方法在认知对齐、语义保真度、鲁棒性、可解释性和数据效率方面的优势。论文还追踪了从语言中心管道到工具增强的语言模型和视觉语言模型的五阶段范式转变，并列出了60多个基准和相应的指标。最后，作者总结了关键见解，识别了开放挑战，并提出了未来的研究方向。'}}}, {'id': 'https://huggingface.co/papers/2508.18190', 'title': 'ST-Raptor: LLM-Powered Semi-Structured Table Question Answering', 'url': 'https://huggingface.co/papers/2508.18190', 'abstract': 'ST-Raptor, a tree-based framework using large language models, addresses challenges in answering questions from semi-structured tables by introducing a Hierarchical Orthogonal Tree and a two-stage verification mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t Semi-structured tables, widely used in real-world applications (e.g., financial reports, medical records, transactional orders), often involve flexible and complex layouts (e.g., hierarchical headers and merged cells). These tables generally rely on human analysts to interpret table layouts and answer relevant natural language questions, which is costly and inefficient. To automate the procedure, existing methods face significant challenges. First, methods like NL2SQL require converting semi-structured tables into structured ones, which often causes substantial information loss. Second, methods like NL2Code and multi-modal LLM QA struggle to understand the complex layouts of semi-structured tables and cannot accurately answer corresponding questions. To this end, we propose ST-Raptor, a tree-based framework for semi-structured table question answering using large language models. First, we introduce the Hierarchical Orthogonal Tree (HO-Tree), a structural model that captures complex semi-structured table layouts, along with an effective algorithm for constructing the tree. Second, we define a set of basic tree operations to guide LLMs in executing common QA tasks. Given a user question, ST-Raptor decomposes it into simpler sub-questions, generates corresponding tree operation pipelines, and conducts operation-table alignment for accurate pipeline execution. Third, we incorporate a two-stage verification mechanism: forward validation checks the correctness of execution steps, while backward validation evaluates answer reliability by reconstructing queries from predicted answers. To benchmark the performance, we present SSTQA, a dataset of 764 questions over 102 real-world semi-structured tables. Experiments show that ST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code is available at https://github.com/weAIDB/ST-Raptor.', 'score': 1, 'issue_id': 5540, 'pub_date': '2025-08-25', 'pub_date_card': {'ru': '25 августа', 'en': 'August 25', 'zh': '8月25日'}, 'hash': 'e7ede69164051787', 'authors': ['Zirui Tang', 'Boyu Niu', 'Xuanhe Zhou', 'Boxiu Li', 'Wei Zhou', 'Jiannan Wang', 'Guoliang Li', 'Xinyi Zhang', 'Fan Wu'], 'affiliations': ['Renmin University of China', 'Shanghai Jiao Tong University', 'Simon Fraser University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.18190.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#reasoning', '#multimodal', '#data', '#interpretability'], 'emoji': '🌳', 'ru': {'title': 'ST-Raptor: Древовидный подход к анализу сложных таблиц с помощью ИИ', 'desc': 'ST-Raptor - это древовидная система для ответов на вопросы по полуструктурированным таблицам с использованием больших языковых моделей. Она вводит Иерархическое Ортогональное Дерево (HO-Tree) для захвата сложных макетов таблиц и набор базовых древесных операций для выполнения задач вопросно-ответной системы. ST-Raptor применяет двухэтапный механизм проверки: прямую валидацию для проверки правильности шагов выполнения и обратную валидацию для оценки надежности ответов. Эксперименты показывают, что ST-Raptor превосходит базовые методы до 20% по точности ответов.'}, 'en': {'title': 'ST-Raptor: Revolutionizing Table Question Answering with Hierarchical Trees', 'desc': 'ST-Raptor is a novel framework designed to enhance question answering from semi-structured tables using large language models (LLMs). It introduces a Hierarchical Orthogonal Tree (HO-Tree) to effectively represent complex table layouts, allowing for better interpretation of the data. The framework employs a two-stage verification mechanism to ensure the accuracy of answers by validating execution steps and reconstructing queries. Experimental results demonstrate that ST-Raptor significantly improves answer accuracy compared to existing methods, making it a valuable tool for automating table-based question answering.'}, 'zh': {'title': 'ST-Raptor：半结构化表格问答的新突破', 'desc': 'ST-Raptor是一个基于树的框架，旨在解决从半结构化表格中回答问题的挑战。它引入了层次正交树（HO-Tree）来捕捉复杂的表格布局，并通过基本树操作指导大型语言模型（LLM）执行常见的问答任务。该框架还采用了两阶段验证机制，确保执行步骤的正确性和答案的可靠性。实验结果表明，ST-Raptor在答案准确性上比九个基线方法提高了多达20%。'}}}, {'id': 'https://huggingface.co/papers/2508.18159', 'title': 'SpotEdit: Evaluating Visually-Guided Image Editing Methods', 'url': 'https://huggingface.co/papers/2508.18159', 'abstract': 'SpotEdit is a benchmark for evaluating visually-guided image editing methods, revealing performance disparities and hallucination issues across diffusion, autoregressive, and hybrid generative models.  \t\t\t\t\tAI-generated summary \t\t\t\t Visually-guided image editing, where edits are conditioned on both visual cues and textual prompts, has emerged as a powerful paradigm for fine-grained, controllable content generation. Although recent generative models have shown remarkable capabilities, existing evaluations remain simple and insufficiently representative of real-world editing challenges. We present SpotEdit, a comprehensive benchmark designed to systematically assess visually-guided image editing methods across diverse diffusion, autoregressive, and hybrid generative models, uncovering substantial performance disparities. To address a critical yet underexplored challenge, our benchmark includes a dedicated component on hallucination, highlighting how leading models, such as GPT-4o, often hallucinate the existence of a visual cue and erroneously perform the editing task. Our code and benchmark are publicly released at https://github.com/SaraGhazanfari/SpotEdit.', 'score': 1, 'issue_id': 5540, 'pub_date': '2025-08-25', 'pub_date_card': {'ru': '25 августа', 'en': 'August 25', 'zh': '8月25日'}, 'hash': '7e9e7873459fbab7', 'authors': ['Sara Ghazanfari', 'Wei-An Lin', 'Haitong Tian', 'Ersin Yumer'], 'affiliations': ['Adobe Inc.', 'New York University, US'], 'pdf_title_img': 'assets/pdf/title_img/2508.18159.jpg', 'data': {'categories': ['#benchmark', '#diffusion', '#open_source', '#hallucinations', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'SpotEdit: новый стандарт в редактировании изображений', 'desc': 'SpotEdit — это новый бенчмарк для оценки методов редактирования изображений, управляемых визуальными подсказками. Он выявляет различия в производительности и проблемы галлюцинаций в диффузионных, авторегрессионных и гибридных генеративных моделях. Исследование показывает, что современные модели, такие как GPT-4o, часто ошибочно воспринимают визуальные подсказки, что приводит к неверному редактированию. SpotEdit предоставляет более полное представление о реальных вызовах редактирования изображений.'}, 'en': {'title': 'SpotEdit: A New Standard for Evaluating Image Editing Models', 'desc': 'SpotEdit is a new benchmark created to evaluate how well different image editing methods work when guided by visual cues and text prompts. It highlights the differences in performance among various generative models, including diffusion, autoregressive, and hybrid types. The benchmark also focuses on a significant issue called hallucination, where models mistakenly believe a visual cue exists and make incorrect edits. By providing a more thorough evaluation framework, SpotEdit aims to improve the understanding and development of visually-guided image editing techniques.'}, 'zh': {'title': 'SpotEdit：评估视觉引导图像编辑的基准', 'desc': 'SpotEdit是一个用于评估视觉引导图像编辑方法的基准，揭示了扩散、自动回归和混合生成模型之间的性能差异和幻觉问题。视觉引导图像编辑结合了视觉线索和文本提示，成为一种强大的细粒度可控内容生成方式。尽管最近的生成模型表现出色，但现有的评估方法过于简单，无法充分代表现实世界的编辑挑战。SpotEdit基准系统地评估不同生成模型的表现，并特别关注幻觉问题，指出一些领先模型在编辑任务中常常错误地假设存在视觉线索。'}}}, {'id': 'https://huggingface.co/papers/2508.17973', 'title': 'German4All - A Dataset and Model for Readability-Controlled Paraphrasing\n  in German', 'url': 'https://huggingface.co/papers/2508.17973', 'abstract': 'A large-scale German dataset and model for readability-controlled paraphrasing are introduced, achieving state-of-the-art performance in text simplification.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability to paraphrase texts across different complexity levels is essential for creating accessible texts that can be tailored toward diverse reader groups. Thus, we introduce German4All, the first large-scale German dataset of aligned readability-controlled, paragraph-level paraphrases. It spans five readability levels and comprises over 25,000 samples. The dataset is automatically synthesized using GPT-4 and rigorously evaluated through both human and LLM-based judgments. Using German4All, we train an open-source, readability-controlled paraphrasing model that achieves state-of-the-art performance in German text simplification, enabling more nuanced and reader-specific adaptations. We opensource both the dataset and the model to encourage further research on multi-level paraphrasing', 'score': 1, 'issue_id': 5546, 'pub_date': '2025-08-25', 'pub_date_card': {'ru': '25 августа', 'en': 'August 25', 'zh': '8月25日'}, 'hash': 'b12e2be4bd1806ae', 'authors': ['Miriam Anschütz', 'Thanh Mai Pham', 'Eslam Nasrallah', 'Maximilian Müller', 'Cristian-George Craciun', 'Georg Groh'], 'affiliations': ['Technical University of Munich'], 'pdf_title_img': 'assets/pdf/title_img/2508.17973.jpg', 'data': {'categories': ['#open_source', '#dataset', '#data', '#training', '#synthetic'], 'emoji': '📚', 'ru': {'title': 'Революция в доступности текстов: German4All открывает новые горизонты парафразирования', 'desc': 'В статье представлен German4All - первый масштабный немецкий датасет парафразов с контролируемой читабельностью на уровне абзацев. Датасет охватывает пять уровней читабельности и содержит более 25 000 образцов, синтезированных с помощью GPT-4. На основе German4All обучена модель парафразирования с контролем читабельности, достигающая лучших результатов в упрощении немецких текстов. Авторы открыли доступ к датасету и модели для стимулирования дальнейших исследований в области многоуровневого парафразирования.'}, 'en': {'title': 'Empowering Text Accessibility with German4All', 'desc': 'This paper presents German4All, a comprehensive dataset designed for readability-controlled paraphrasing in the German language. It includes over 25,000 paragraph-level samples across five different readability levels, allowing for tailored text simplification. The dataset is generated using GPT-4 and is evaluated through both human assessments and large language model (LLM) judgments. An open-source model trained on this dataset achieves state-of-the-art results, promoting further research in the field of multi-level paraphrasing.'}, 'zh': {'title': '可读性控制的德语改写新突破', 'desc': '本文介绍了一个大型德语数据集和模型，旨在实现可读性控制的改写，达到文本简化的最先进水平。该数据集名为German4All，包含超过25,000个样本，涵盖五个可读性等级，能够为不同读者群体提供定制化的文本。数据集通过GPT-4自动合成，并经过人类和大型语言模型的严格评估。我们还开源了该数据集和模型，以促进多层次改写的进一步研究。'}}}, {'id': 'https://huggingface.co/papers/2508.17821', 'title': 'Limitations of Normalization in Attention Mechanism', 'url': 'https://huggingface.co/papers/2508.17821', 'abstract': "Theoretical and empirical analysis of softmax normalization in attention mechanisms reveals limitations in token selection and gradient sensitivity, highlighting the need for improved normalization strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper investigates the limitations of the normalization in attention mechanisms. We begin with a theoretical framework that enables the identification of the model's selective ability and the geometric separation involved in token selection. Our analysis includes explicit bounds on distances and separation criteria for token vectors under softmax scaling. Through experiments with pre-trained GPT-2 model, we empirically validate our theoretical results and analyze key behaviors of the attention mechanism. Notably, we demonstrate that as the number of selected tokens increases, the model's ability to distinguish informative tokens declines, often converging toward a uniform selection pattern. We also show that gradient sensitivity under softmax normalization presents challenges during training, especially at low temperature settings. These findings advance current understanding of softmax-based attention mechanism and motivate the need for more robust normalization and selection strategies in future attention architectures.", 'score': 1, 'issue_id': 5547, 'pub_date': '2025-08-25', 'pub_date_card': {'ru': '25 августа', 'en': 'August 25', 'zh': '8月25日'}, 'hash': '309afcc613e927f2', 'authors': ['Timur Mudarisov', 'Mikhail Burtsev', 'Tatiana Petrova', 'Radu State'], 'affiliations': ['London Institute for Mathematical Sciences', 'State University of Luxembourg', 'University of Luxembourg'], 'pdf_title_img': 'assets/pdf/title_img/2508.17821.jpg', 'data': {'categories': ['#architecture', '#interpretability', '#optimization', '#training', '#math'], 'emoji': '🔍', 'ru': {'title': 'Ограничения софтмакс-нормализации в механизмах внимания', 'desc': 'Статья анализирует ограничения нормализации в механизмах внимания нейронных сетей. Авторы разработали теоретическую основу для оценки способности модели выбирать токены и геометрического разделения при их отборе. Экспериментально на модели GPT-2 показано, что с увеличением числа выбираемых токенов способность модели различать информативные токены снижается. Также выявлены проблемы чувствительности градиентов при софтмакс-нормализации, особенно при низких значениях температуры.'}, 'en': {'title': 'Rethinking Softmax: Enhancing Token Selection in Attention Mechanisms', 'desc': 'This paper explores the limitations of softmax normalization in attention mechanisms used in machine learning models. It provides a theoretical framework to analyze how well models can select important tokens and the geometric relationships between them. The authors present empirical evidence from experiments with the GPT-2 model, showing that as more tokens are selected, the model struggles to differentiate between them, leading to a uniform selection pattern. Additionally, the paper highlights issues with gradient sensitivity during training, suggesting that better normalization strategies are needed for future attention models.'}, 'zh': {'title': '提升注意力机制的归一化策略', 'desc': '本文研究了注意力机制中softmax归一化的局限性。我们建立了一个理论框架，以识别模型的选择能力和令牌选择中的几何分离。通过对预训练的GPT-2模型进行实验，我们验证了理论结果，并分析了注意力机制的关键行为。研究表明，随着选择的令牌数量增加，模型区分有用令牌的能力下降，且softmax归一化下的梯度敏感性在训练中也面临挑战。'}}}, {'id': 'https://huggingface.co/papers/2508.17811', 'title': 'MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian\n  Splatting', 'url': 'https://huggingface.co/papers/2508.17811', 'abstract': 'MeshSplat uses Gaussian Splatting and a feed-forward network to reconstruct surfaces from sparse views, improving accuracy with a Weighted Chamfer Distance Loss and normal prediction.  \t\t\t\t\tAI-generated summary \t\t\t\t Surface reconstruction has been widely studied in computer vision and graphics. However, existing surface reconstruction works struggle to recover accurate scene geometry when the input views are extremely sparse. To address this issue, we propose MeshSplat, a generalizable sparse-view surface reconstruction framework via Gaussian Splatting. Our key idea is to leverage 2DGS as a bridge, which connects novel view synthesis to learned geometric priors and then transfers these priors to achieve surface reconstruction. Specifically, we incorporate a feed-forward network to predict per-view pixel-aligned 2DGS, which enables the network to synthesize novel view images and thus eliminates the need for direct 3D ground-truth supervision. To improve the accuracy of 2DGS position and orientation prediction, we propose a Weighted Chamfer Distance Loss to regularize the depth maps, especially in overlapping areas of input views, and also a normal prediction network to align the orientation of 2DGS with normal vectors predicted by a monocular normal estimator. Extensive experiments validate the effectiveness of our proposed improvement, demonstrating that our method achieves state-of-the-art performance in generalizable sparse-view mesh reconstruction tasks. Project Page: https://hanzhichang.github.io/meshsplat_web', 'score': 1, 'issue_id': 5550, 'pub_date': '2025-08-25', 'pub_date_card': {'ru': '25 августа', 'en': 'August 25', 'zh': '8月25日'}, 'hash': '0c22f3ea15ee787c', 'authors': ['Hanzhi Chang', 'Ruijie Zhu', 'Wenjie Chang', 'Mulin Yu', 'Yanzhe Liang', 'Jiahao Lu', 'Zhuoyuan Li', 'Tianzhu Zhang'], 'affiliations': ['Shanghai AI Laboratory', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2508.17811.jpg', 'data': {'categories': ['#cv', '#3d'], 'emoji': '🔬', 'ru': {'title': 'Точная реконструкция 3D-поверхностей по нескольким изображениям', 'desc': 'MeshSplat - это новый метод реконструкции поверхностей по малому числу изображений, использующий Gaussian Splatting и нейронную сеть прямого распространения. Для повышения точности в нем применяется Weighted Chamfer Distance Loss и предсказание нормалей поверхности. Метод позволяет восстанавливать геометрию сцены даже при очень малом количестве входных ракурсов. MeshSplat демонстрирует лучшие результаты по сравнению с существующими подходами в задаче обобщаемой реконструкции поверхностей по малому числу изображений.'}, 'en': {'title': 'Revolutionizing Surface Reconstruction from Sparse Views with MeshSplat', 'desc': 'MeshSplat is a novel framework designed for reconstructing surfaces from very few input views using Gaussian Splatting. It employs a feed-forward network to predict 2D Gaussian splats that help in synthesizing new view images without needing direct 3D ground-truth data. To enhance the accuracy of the reconstruction, it introduces a Weighted Chamfer Distance Loss that focuses on improving depth predictions in overlapping view areas, along with a normal prediction network for better alignment with surface normals. The results show that MeshSplat outperforms existing methods in sparse-view surface reconstruction tasks, making it a significant advancement in the field.'}, 'zh': {'title': 'MeshSplat：稀疏视图重建的新突破', 'desc': 'MeshSplat 是一个用于稀疏视图表面重建的框架，采用高斯点云和前馈网络来提高重建精度。它通过加权的 Chamfer 距离损失和法线预测来优化深度图的准确性，特别是在输入视图重叠的区域。该方法利用 2D 高斯点云作为桥梁，将新视图合成与学习的几何先验连接起来，从而实现表面重建。实验结果表明，MeshSplat 在稀疏视图网格重建任务中达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2508.16790', 'title': 'TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language\n  Modeling', 'url': 'https://huggingface.co/papers/2508.16790', 'abstract': 'TaDiCodec, a Text-aware Diffusion Transformer Speech Codec, achieves low frame rates and bitrates with superior speech generation performance using end-to-end optimization and text guidance.  \t\t\t\t\tAI-generated summary \t\t\t\t Speech tokenizers serve as foundational components for speech language models, yet current designs exhibit several limitations, including: 1) dependence on multi-layer residual vector quantization structures or high frame rates, 2) reliance on auxiliary pre-trained models for semantic distillation, and 3) requirements for complex two-stage training processes. In this work, we introduce the Text-aware Diffusion Transformer Speech Codec (TaDiCodec), a novel approach designed to overcome these challenges. TaDiCodec employs end-to-end optimization for quantization and reconstruction through a diffusion autoencoder, while integrating text guidance into the diffusion decoder to enhance reconstruction quality and achieve optimal compression. TaDiCodec achieves an extremely low frame rate of 6.25 Hz and a corresponding bitrate of 0.0875 kbps with a single-layer codebook for 24 kHz speech, while maintaining superior performance on critical speech generation evaluation metrics such as Word Error Rate (WER), speaker similarity (SIM), and speech quality (UTMOS). Notably, TaDiCodec employs a single-stage, end-to-end training paradigm, and obviating the need for auxiliary pre-trained models. We also validate the compatibility of TaDiCodec in language model based zero-shot text-to-speech with both autoregressive modeling and masked generative modeling, demonstrating its effectiveness and efficiency for speech language modeling, as well as a significantly small reconstruction-generation gap. We will open source our code and model checkpoints. Audio samples are are available at https:/tadicodec.github.io/. We release code and model checkpoints at https:/github.com/HeCheng0625/Diffusion-Speech-Tokenizer.', 'score': 1, 'issue_id': 5540, 'pub_date': '2025-08-22', 'pub_date_card': {'ru': '22 августа', 'en': 'August 22', 'zh': '8月22日'}, 'hash': '566e852623f03412', 'authors': ['Yuancheng Wang', 'Dekun Chen', 'Xueyao Zhang', 'Junan Zhang', 'Jiaqi Li', 'Zhizheng Wu'], 'affiliations': ['The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2508.16790.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#audio', '#open_source'], 'emoji': '🗣️', 'ru': {'title': 'TaDiCodec: эффективное кодирование речи с помощью диффузии и текста', 'desc': 'TaDiCodec - это новый подход к кодированию речи, использующий диффузионный трансформер и текстовое руководство. Он достигает низкой частоты кадров (6,25 Гц) и битрейта (0,0875 кбит/с) при сохранении высокого качества генерации речи. TaDiCodec применяет сквозную оптимизацию для квантования и реконструкции, интегрируя текстовое руководство в диффузионный декодер. Модель показывает превосходные результаты по таким метрикам, как Word Error Rate, сходство голоса и качество речи.'}, 'en': {'title': 'Revolutionizing Speech Generation with TaDiCodec', 'desc': 'TaDiCodec is a new speech codec that uses a Text-aware Diffusion Transformer to improve speech generation while keeping low frame rates and bitrates. It addresses limitations of existing speech tokenizers by using end-to-end optimization for quantization and reconstruction, eliminating the need for complex training processes and auxiliary models. The codec achieves a low frame rate of 6.25 Hz and a bitrate of 0.0875 kbps, while still performing well on important metrics like Word Error Rate and speech quality. Additionally, TaDiCodec supports zero-shot text-to-speech applications, showcasing its versatility in speech language modeling.'}, 'zh': {'title': '文本感知的高效语音编解码器', 'desc': 'TaDiCodec是一种文本感知的扩散变换器语音编解码器，旨在通过端到端优化和文本指导来实现低帧率和低比特率的优越语音生成性能。该模型克服了现有语音模型在多层残差向量量化、依赖预训练模型和复杂的两阶段训练过程等方面的局限性。TaDiCodec采用扩散自编码器进行量化和重建，并通过扩散解码器集成文本指导，以提高重建质量和压缩效率。最终，TaDiCodec在语音生成评估指标上表现出色，同时实现了极低的帧率和比特率。'}}}, {'id': 'https://huggingface.co/papers/2508.18076', 'title': 'Neither Valid nor Reliable? Investigating the Use of LLMs as Judges', 'url': 'https://huggingface.co/papers/2508.18076', 'abstract': 'The paper critiques the use of large language models as judges for evaluating natural language generation systems, questioning their reliability, capabilities, scalability, and cost-effectiveness.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating natural language generation (NLG) systems remains a core challenge of natural language processing (NLP), further complicated by the rise of large language models (LLMs) that aims to be general-purpose. Recently, large language models as judges (LLJs) have emerged as a promising alternative to traditional metrics, but their validity remains underexplored. This position paper argues that the current enthusiasm around LLJs may be premature, as their adoption has outpaced rigorous scrutiny of their reliability and validity as evaluators. Drawing on measurement theory from the social sciences, we identify and critically assess four core assumptions underlying the use of LLJs: their ability to act as proxies for human judgment, their capabilities as evaluators, their scalability, and their cost-effectiveness. We examine how each of these assumptions may be challenged by the inherent limitations of LLMs, LLJs, or current practices in NLG evaluation. To ground our analysis, we explore three applications of LLJs: text summarization, data annotation, and safety alignment. Finally, we highlight the need for more responsible evaluation practices in LLJs evaluation, to ensure that their growing role in the field supports, rather than undermines, progress in NLG.', 'score': 0, 'issue_id': 5540, 'pub_date': '2025-08-25', 'pub_date_card': {'ru': '25 августа', 'en': 'August 25', 'zh': '8月25日'}, 'hash': '0e751250907ca130', 'authors': ['Khaoula Chehbouni', 'Mohammed Haddou', 'Jackie Chi Kit Cheung', 'Golnoosh Farnadi'], 'affiliations': ['McGill University', 'Mila - Quebec AI Institute', 'Statistics Canada'], 'pdf_title_img': 'assets/pdf/title_img/2508.18076.jpg', 'data': {'categories': ['#ethics', '#benchmark', '#alignment', '#multimodal', '#interpretability'], 'emoji': '⚖️', 'ru': {'title': 'Большие языковые модели как судьи: преждевременный энтузиазм?', 'desc': 'Статья критикует использование больших языковых моделей (LLM) в качестве судей для оценки систем генерации естественного языка. Авторы ставят под сомнение надежность, возможности, масштабируемость и экономическую эффективность такого подхода. Они анализируют четыре ключевых предположения, лежащих в основе использования LLM как судей: способность заменять человеческие оценки, возможности в качестве оценщиков, масштабируемость и экономичность. Исследователи призывают к более ответственным практикам оценки с использованием LLM для обеспечения прогресса в области генерации естественного языка.'}, 'en': {'title': 'Rethinking Large Language Models as Evaluators in NLG', 'desc': 'This paper critiques the use of large language models (LLMs) as judges for evaluating natural language generation (NLG) systems. It questions their reliability, capabilities, scalability, and cost-effectiveness, suggesting that the excitement around using LLMs as evaluators may be premature. The authors analyze four key assumptions about LLMs acting as proxies for human judgment and their effectiveness in evaluation tasks. They call for more responsible evaluation practices to ensure that the integration of LLMs in NLG evaluation supports meaningful progress in the field.'}, 'zh': {'title': '审慎使用大型语言模型评估生成系统', 'desc': '这篇论文批评了将大型语言模型（LLMs）作为评估自然语言生成系统的评判者的做法，质疑其可靠性、能力、可扩展性和成本效益。作者认为，尽管大型语言模型作为评判者（LLJs）被视为传统评估指标的有希望替代方案，但其有效性尚未得到充分探讨。论文分析了LLJs使用的四个核心假设，包括其作为人类判断代理的能力、评估能力、可扩展性和成本效益，并指出这些假设可能受到LLMs的固有限制的挑战。最后，作者强调需要更负责任的评估实践，以确保LLJs在自然语言生成领域的作用能够促进而不是阻碍进展。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (2)', '#agi (1)', '#alignment (1)', '#architecture (4)', '#audio (1)', '#benchmark (10)', '#cv (5)', '#data (3)', '#dataset (4)', '#diffusion (3)', '#ethics (1)', '#games (1)', '#graphs', '#hallucinations (3)', '#healthcare', '#inference (1)', '#interpretability (4)', '#leakage', '#long_context', '#low_resource (1)', '#machine_translation', '#math (1)', '#multilingual (1)', '#multimodal (6)', '#open_source (5)', '#optimization (4)', '#plp', '#rag (1)', '#reasoning (9)', '#rl (3)', '#rlhf (1)', '#robotics', '#science', '#security', '#small_models', '#story_generation', '#survey (2)', '#synthetic (2)', '#training (5)', '#transfer_learning', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-08-26 13:27',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-08-26 13:27')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-08-26 13:27')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    