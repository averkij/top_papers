
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 11 papers. November 7.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }        
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 0;
            line-height: 1;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">7 ноября</span> | <span id="title-articles-count">11 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-11-06.html">⬅️ <span id="prev-date">06.11</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-11-08.html">➡️ <span id="next-date">08.11</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-11.html">📈 <span id='top-month-label'>Топ за месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'};
        let feedDateNext = {'ru': '08.11', 'en': '11/08', 'zh': '11月8日'};
        let feedDatePrev = {'ru': '06.11', 'en': '11/06', 'zh': '11月6日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'Статья от ', 'en': 'Published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Топ за месяц', 'en': 'Top by Month', 'zh': '月度热门论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2411.02959', 'title': 'HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems', 'url': 'https://huggingface.co/papers/2411.02959', 'abstract': 'Retrieval-Augmented Generation (RAG) has been shown to improve knowledge capabilities and alleviate the hallucination problem of LLMs. The Web is a major source of external knowledge used in RAG systems, and many commercial systems such as ChatGPT and Perplexity have used Web search engines as their major retrieval systems. Typically, such RAG systems retrieve search results, download HTML sources of the results, and then extract plain texts from the HTML sources. Plain text documents or chunks are fed into the LLMs to augment the generation. However, much of the structural and semantic information inherent in HTML, such as headings and table structures, is lost during this plain-text-based RAG process. To alleviate this problem, we propose HtmlRAG, which uses HTML instead of plain text as the format of retrieved knowledge in RAG. We believe HTML is better than plain text in modeling knowledge in external documents, and most LLMs possess robust capacities to understand HTML. However, utilizing HTML presents new challenges. HTML contains additional content such as tags, JavaScript, and CSS specifications, which bring extra input tokens and noise to the RAG system. To address this issue, we propose HTML cleaning, compression, and pruning strategies, to shorten the HTML while minimizing the loss of information. Specifically, we design a two-step block-tree-based pruning method that prunes useless HTML blocks and keeps only the relevant part of the HTML. Experiments on six QA datasets confirm the superiority of using HTML in RAG systems.', 'score': 40, 'issue_id': 437, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 ноября', 'en': 'November 5', 'zh': '11月5日'}, 'hash': '6fb8684374e5fdcb', 'data': {'categories': ['#rag', '#data', '#training'], 'emoji': '🌐', 'ru': {'title': 'HtmlRAG: Улучшение RAG-систем с помощью структурированной веб-информации', 'desc': 'Статья представляет новый подход к генерации с извлечением информации (RAG), названный HtmlRAG. В отличие от традиционных систем RAG, использующих простой текст, HtmlRAG сохраняет структурную и семантическую информацию HTML-документов. Авторы предлагают методы очистки, сжатия и обрезки HTML для уменьшения шума и сокращения входных токенов. Эксперименты на шести наборах данных для вопросно-ответных систем подтверждают превосходство использования HTML в системах RAG.'}, 'en': {'title': 'Harnessing HTML for Enhanced Knowledge Retrieval in RAG Systems', 'desc': 'This paper introduces HtmlRAG, a novel approach to Retrieval-Augmented Generation (RAG) that utilizes HTML instead of plain text for knowledge retrieval. By leveraging the structural and semantic information present in HTML, HtmlRAG aims to enhance the performance of large language models (LLMs) and reduce the hallucination problem. The authors address the challenges posed by HTML, such as excess tokens and noise, by implementing cleaning, compression, and pruning techniques to streamline the input. Experimental results demonstrate that HtmlRAG outperforms traditional plain-text-based RAG systems across multiple question-answering datasets.'}, 'zh': {'title': '用HTML提升检索增强生成的能力', 'desc': '本文提出了一种新的检索增强生成（RAG）方法，称为HtmlRAG，旨在改善大语言模型（LLMs）的知识能力并减少幻觉问题。HtmlRAG使用HTML格式而非纯文本来增强生成过程，从而保留更多的结构和语义信息。为了应对HTML中多余内容带来的挑战，本文提出了HTML清理、压缩和修剪策略，以减少输入的冗余信息。实验结果表明，HtmlRAG在六个问答数据集上的表现优于传统的纯文本RAG系统。'}}}, {'id': 'https://huggingface.co/papers/2411.00871', 'title': 'LLaMo: Large Language Model-based Molecular Graph Assistant', 'url': 'https://huggingface.co/papers/2411.00871', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable generalization and instruction-following capabilities with instruction tuning. The advancements in LLMs and instruction tuning have led to the development of Large Vision-Language Models (LVLMs). However, the competency of the LLMs and instruction tuning have been less explored in the molecular domain. Thus, we propose LLaMo: Large Language Model-based Molecular graph assistant, which is an end-to-end trained large molecular graph-language model. To bridge the discrepancy between the language and graph modalities, we present the multi-level graph projector that transforms graph representations into graph tokens by abstracting the output representations of each GNN layer and motif representations with the cross-attention mechanism. We also introduce machine-generated molecular graph instruction data to instruction-tune the large molecular graph-language model for general-purpose molecule and language understanding. Our extensive experiments demonstrate that LLaMo shows the best performance on diverse tasks, such as molecular description generation, property prediction, and IUPAC name prediction. The code of LLaMo is available at https://github.com/mlvlab/LLaMo.', 'score': 16, 'issue_id': 439, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': 'd1284691dab4e739', 'data': {'categories': ['#multimodal', '#dataset', '#agents', '#architecture', '#training'], 'emoji': '🧬', 'ru': {'title': 'LLaMo: Революция в понимании молекулярных структур с помощью ИИ', 'desc': 'Исследователи представили LLaMo - ассистента для работы с молекулярными графами, основанного на большой языковой модели. LLaMo использует многоуровневый графовый проектор для преобразования графовых представлений в токены, что позволяет объединить языковую и графовую модальности. Модель обучена на инструкциях, сгенерированных машинным способом, для понимания молекул и языка. Эксперименты показывают, что LLaMo превосходит существующие решения в задачах генерации описаний молекул, предсказания свойств и определения названий по IUPAC.'}, 'en': {'title': 'Bridging Language and Molecules with LLaMo', 'desc': 'This paper introduces LLaMo, a Large Language Model-based Molecular graph assistant designed to enhance understanding in the molecular domain. It utilizes a multi-level graph projector to convert molecular graph representations into tokens, facilitating better interaction between language and graph data. The model is instruction-tuned using machine-generated molecular graph instruction data, enabling it to perform various tasks like molecular description generation and property prediction. Experimental results show that LLaMo outperforms existing models in these tasks, highlighting its effectiveness in bridging language and molecular graph understanding.'}, 'zh': {'title': 'LLaMo：连接语言与分子的桥梁', 'desc': '大型语言模型（LLMs）在指令调优方面展现了出色的泛化能力和遵循指令的能力。本文提出了LLaMo，一个基于大型语言模型的分子图助手，旨在填补语言和图形模态之间的差距。我们引入了多层图投影器，通过跨注意力机制将图表示转换为图标记，并使用机器生成的分子图指令数据对模型进行指令调优。实验结果表明，LLaMo在分子描述生成、属性预测和IUPAC名称预测等多项任务中表现最佳。'}}}, {'id': 'https://huggingface.co/papers/2410.23054', 'title': 'Controlling Language and Diffusion Models by Transporting Activations', 'url': 'https://huggingface.co/papers/2410.23054', 'abstract': 'The increasing capabilities of large generative models and their ever more widespread deployment have raised concerns about their reliability, safety, and potential misuse. To address these issues, recent works have proposed to control model generation by steering model activations in order to effectively induce or prevent the emergence of concepts or behaviors in the generated output. In this paper we introduce Activation Transport (AcT), a general framework to steer activations guided by optimal transport theory that generalizes many previous activation-steering works. AcT is modality-agnostic and provides fine-grained control over the model behavior with negligible computational overhead, while minimally impacting model abilities. We experimentally show the effectiveness and versatility of our approach by addressing key challenges in large language models (LLMs) and text-to-image diffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate toxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is, we show how AcT enables fine-grained style control and concept negation.', 'score': 11, 'issue_id': 444, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '5238611006cc8b68', 'data': {'categories': ['#alignment', '#interpretability', '#cv', '#rlhf'], 'emoji': '🎛️', 'ru': {'title': 'AcT: точное управление генеративными моделями через активации нейронов', 'desc': 'Эта статья представляет AcT (Activation Transport) - новый фреймворк для управления генеративными моделями, основанный на теории оптимального транспорта. AcT позволяет контролировать поведение модели путем управления активациями нейронов, что обобщает предыдущие подходы в этой области. Фреймворк применим к различным модальностям и обеспечивает точный контроль с минимальными вычислительными затратами. Эксперименты показали эффективность AcT для снижения токсичности, индукции концепций и повышения правдивости языковых моделей, а также для управления стилем и отрицания концепций в моделях генерации изображений.'}, 'en': {'title': 'Steering Generative Models with Activation Transport', 'desc': 'This paper presents Activation Transport (AcT), a new framework designed to control the behavior of large generative models by steering their activations. Using principles from optimal transport theory, AcT allows for precise manipulation of model outputs without significantly increasing computational costs. The framework is applicable across different modalities, including large language models (LLMs) and text-to-image diffusion models (T2Is). Experimental results demonstrate that AcT can reduce toxicity in LLMs, induce specific concepts, and enhance truthfulness, while also providing style control and concept negation in T2Is.'}, 'zh': {'title': '激活传输：控制生成模型的新方法', 'desc': '本文提出了一种名为激活传输（AcT）的新框架，旨在通过最优传输理论来引导模型的激活，从而控制生成模型的输出。该方法具有通用性，可以在不同的模态中应用，且对模型的计算开销影响极小。实验结果表明，AcT在大型语言模型（LLMs）中能够有效减少有害内容、引入任意概念并提高生成内容的真实性。在文本到图像的扩散模型（T2Is）中，AcT则实现了对风格的精细控制和概念的否定。'}}}, {'id': 'https://huggingface.co/papers/2411.02359', 'title': 'DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution', 'url': 'https://huggingface.co/papers/2411.02359', 'abstract': 'MLLMs have demonstrated remarkable comprehension and reasoning capabilities with complex language and visual data. These advances have spurred the vision of establishing a generalist robotic MLLM proficient in understanding complex human instructions and accomplishing various embodied tasks. However, developing MLLMs for real-world robots is challenging due to the typically limited computation and memory capacities available on robotic platforms. In contrast, the inference of MLLMs involves storing billions of parameters and performing tremendous computation, imposing significant hardware demands. In our paper, we propose a Dynamic Early-Exit Framework for Robotic Vision-Language-Action Model (DeeR-VLA, or simply DeeR) that automatically adjusts the size of the activated MLLM based on each situation at hand. The approach leverages a multi-exit architecture in MLLMs, which allows the model to terminate processing once a proper size of the model has been activated for a specific situation, thus avoiding further redundant computation. Additionally, we develop novel algorithms that establish early-termination criteria for DeeR, conditioned on predefined demands such as average computational cost (i.e., power consumption), as well as peak computational consumption (i.e., latency) and GPU memory usage. These enhancements ensure that DeeR operates efficiently under varying resource constraints while maintaining competitive performance. On the CALVIN robot manipulation benchmark, DeeR demonstrates significant reductions in computational costs of LLM by 5.2-6.5x and GPU memory of LLM by 2-6x without compromising performance. Code and checkpoints are available at https://github.com/yueyang130/DeeR-VLA.', 'score': 11, 'issue_id': 437, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '08c45469caff5fa0', 'data': {'categories': ['#agents', '#inference', '#robots', '#optimization', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Эффективные языковые модели для роботов: меньше ресурсов, та же мощность', 'desc': 'Статья представляет Dynamic Early-Exit Framework для робототехнических моделей зрения, языка и действия (DeeR-VLA). Эта система автоматически регулирует размер активированной мультимодальной языковой модели (MLLM) в зависимости от ситуации, используя архитектуру с множественными выходами. DeeR разработан для эффективной работы в условиях ограниченных вычислительных ресурсов роботов. На бенчмарке CALVIN система показала значительное снижение вычислительных затрат и использования памяти GPU без ущерба для производительности.'}, 'en': {'title': 'Efficient Robotic Intelligence with Dynamic Early-Exit MLLMs', 'desc': 'This paper introduces a new framework called DeeR for improving the efficiency of robotic vision-language-action models (MLLMs). DeeR uses a Dynamic Early-Exit approach that allows the model to adjust its size based on the specific task, reducing unnecessary computations. By implementing a multi-exit architecture, the model can stop processing once it has enough information, which helps save power and memory. The results show that DeeR can significantly lower computational costs and memory usage while still performing well on tasks, making it suitable for real-world robotic applications.'}, 'zh': {'title': '动态调整，智能机器人更高效！', 'desc': '本论文提出了一种动态早期退出框架（DeeR-VLA），旨在解决机器人视觉-语言-动作模型（MLLM）在实际应用中的计算和内存限制问题。该框架通过多出口架构，能够根据具体情况自动调整激活的模型大小，从而避免冗余计算。我们还开发了新算法，设定早期终止标准，以满足预定义的计算需求，如功耗和延迟。实验结果表明，DeeR在CALVIN机器人操作基准上显著降低了计算成本和GPU内存使用，同时保持了竞争力的性能。'}}}, {'id': 'https://huggingface.co/papers/2411.01493', 'title': 'Sample-Efficient Alignment for LLMs', 'url': 'https://huggingface.co/papers/2411.01493', 'abstract': "We study methods for efficiently aligning large language models (LLMs) with human preferences given budgeted online feedback. We first formulate the LLM alignment problem in the frame of contextual dueling bandits. This formulation, subsuming recent paradigms such as online RLHF and online DPO, inherently quests for sample-efficient algorithms that incorporate online active exploration. Leveraging insights from bandit theory, we introduce a unified algorithm based on Thompson sampling and highlight its applications in two distinct LLM alignment scenarios. The practical agent that efficiently implements this algorithm, named SEA (Sample-Efficient Alignment), is empirically validated through extensive experiments across three model scales (1B, 2.8B, 6.9B) and three preference learning algorithms (DPO, IPO, SLiC). The results demonstrate that SEA achieves highly sample-efficient alignment with oracle's preferences, outperforming recent active exploration methods for LLMs. Additionally, we release the implementation of SEA together with an efficient codebase designed for online alignment of LLMs, aiming to accelerate future research in this field.", 'score': 9, 'issue_id': 440, 'pub_date': '2024-11-03', 'pub_date_card': {'ru': '3 ноября', 'en': 'November 3', 'zh': '11月3日'}, 'hash': 'c5bb9727a6ba6119', 'data': {'categories': ['#rlhf', '#alignment', '#agents'], 'emoji': '🎯', 'ru': {'title': 'Эффективное согласование языковых моделей с помощью контекстных дуэльных бандитов', 'desc': 'Исследователи изучают методы эффективного согласования больших языковых моделей (LLM) с человеческими предпочтениями при ограниченной онлайн-обратной связи. Они формулируют проблему согласования LLM в рамках контекстных дуэльных бандитов и предлагают унифицированный алгоритм на основе выборки Томпсона. Практический агент SEA (Sample-Efficient Alignment), реализующий этот алгоритм, показывает высокую эффективность выборки при согласовании с предпочтениями оракула. Исследователи также выпускают реализацию SEA вместе с эффективной кодовой базой для онлайн-согласования LLM.'}, 'en': {'title': 'Efficiently Aligning LLMs with Human Preferences Using SEA', 'desc': 'This paper explores how to align large language models (LLMs) with human preferences using limited online feedback. It frames the alignment challenge as a contextual dueling bandits problem, which seeks efficient algorithms that can learn from active exploration. The authors propose a new algorithm called SEA (Sample-Efficient Alignment) based on Thompson sampling, which is tested across various model sizes and preference learning methods. The results show that SEA is highly effective in aligning LLMs with human preferences while using fewer samples compared to existing methods.'}, 'zh': {'title': '高效对齐大型语言模型与人类偏好', 'desc': '本文研究了如何在有限的在线反馈下高效地将大型语言模型（LLMs）与人类偏好对齐。我们将LLM对齐问题框定为上下文对抗赌博者问题，提出了一种基于汤普森采样的统一算法，并在两个不同的LLM对齐场景中进行了应用。通过大量实验验证，名为SEA（样本高效对齐）的实用代理在不同规模的模型和偏好学习算法中表现出色，显示出其在对齐方面的高样本效率。我们还发布了SEA的实现和高效代码库，以促进该领域未来的研究。'}}}, {'id': 'https://huggingface.co/papers/2411.02393', 'title': 'Adaptive Length Image Tokenization via Recurrent Allocation', 'url': 'https://huggingface.co/papers/2411.02393', 'abstract': 'Current vision systems typically assign fixed-length representations to images, regardless of the information content. This contrasts with human intelligence - and even large language models - which allocate varying representational capacities based on entropy, context and familiarity. Inspired by this, we propose an approach to learn variable-length token representations for 2D images. Our encoder-decoder architecture recursively processes 2D image tokens, distilling them into 1D latent tokens over multiple iterations of recurrent rollouts. Each iteration refines the 2D tokens, updates the existing 1D latent tokens, and adaptively increases representational capacity by adding new tokens. This enables compression of images into a variable number of tokens, ranging from 32 to 256. We validate our tokenizer using reconstruction loss and FID metrics, demonstrating that token count aligns with image entropy, familiarity and downstream task requirements. Recurrent token processing with increasing representational capacity in each iteration shows signs of token specialization, revealing potential for object / part discovery.', 'score': 7, 'issue_id': 448, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '1c3553b38b491652', 'data': {'categories': ['#cv', '#architecture', '#training'], 'emoji': '🖼️', 'ru': {'title': 'Адаптивное токенизированное представление изображений', 'desc': 'Статья предлагает новый подход к представлению изображений с помощью токенов переменной длины. Авторы разработали архитектуру энкодер-декодер, которая рекурсивно обрабатывает 2D-токены изображения, преобразуя их в 1D-латентные токены. Процесс включает несколько итераций, на каждой из которых уточняются 2D-токены, обновляются существующие 1D-токены и адаптивно увеличивается емкость представления путем добавления новых токенов. Эксперименты показали, что количество токенов соответствует энтропии изображения и требованиям downstream-задач.'}, 'en': {'title': 'Adaptive Tokenization for Enhanced Image Representation', 'desc': "This paper introduces a novel method for creating variable-length token representations of images, which contrasts with traditional fixed-length approaches. The proposed encoder-decoder architecture processes 2D image tokens recursively, refining them into 1D latent tokens through multiple iterations. Each iteration not only updates the existing tokens but also allows for the addition of new tokens, enabling a flexible representation that adapts to the complexity of the image. The effectiveness of this method is validated through reconstruction loss and FID metrics, showing that the number of tokens correlates with the image's information content and can enhance downstream tasks."}, 'zh': {'title': '可变长度标记表示：提升图像理解能力', 'desc': '当前的视觉系统通常为图像分配固定长度的表示，这与人类智能和大型语言模型的动态表示能力形成对比。我们提出了一种方法，通过编码器-解码器架构学习二维图像的可变长度标记表示。该方法通过递归处理二维图像标记，将其提炼为一维潜在标记，并在多个迭代中逐步更新和增加表示能力。我们的实验表明，标记数量与图像的熵、熟悉度和下游任务需求相一致，显示出标记的专业化潜力。'}}}, {'id': 'https://huggingface.co/papers/2411.01602', 'title': 'DreamPolish: Domain Score Distillation With Progressive Geometry Generation', 'url': 'https://huggingface.co/papers/2411.01602', 'abstract': 'We introduce DreamPolish, a text-to-3D generation model that excels in producing refined geometry and high-quality textures. In the geometry construction phase, our approach leverages multiple neural representations to enhance the stability of the synthesis process. Instead of relying solely on a view-conditioned diffusion prior in the novel sampled views, which often leads to undesired artifacts in the geometric surface, we incorporate an additional normal estimator to polish the geometry details, conditioned on viewpoints with varying field-of-views. We propose to add a surface polishing stage with only a few training steps, which can effectively refine the artifacts attributed to limited guidance from previous stages and produce 3D objects with more desirable geometry. The key topic of texture generation using pretrained text-to-image models is to find a suitable domain in the vast latent distribution of these models that contains photorealistic and consistent renderings. In the texture generation phase, we introduce a novel score distillation objective, namely domain score distillation (DSD), to guide neural representations toward such a domain. We draw inspiration from the classifier-free guidance (CFG) in textconditioned image generation tasks and show that CFG and variational distribution guidance represent distinct aspects in gradient guidance and are both imperative domains for the enhancement of texture quality. Extensive experiments show our proposed model can produce 3D assets with polished surfaces and photorealistic textures, outperforming existing state-of-the-art methods.', 'score': 7, 'issue_id': 439, 'pub_date': '2024-11-03', 'pub_date_card': {'ru': '3 ноября', 'en': 'November 3', 'zh': '11月3日'}, 'hash': '403fd08e60540a0a', 'data': {'categories': ['#3d', '#cv'], 'emoji': '🎨', 'ru': {'title': 'DreamPolish: Революция в генерации 3D-объектов с идеальной геометрией и текстурами', 'desc': 'DreamPolish - это модель генерации 3D-объектов из текста, которая превосходит существующие методы в создании утонченной геометрии и высококачественных текстур. Модель использует множественные нейронные представления и дополнительный оценщик нормалей для улучшения стабильности синтеза геометрии. Для генерации текстур авторы предлагают новый метод дистилляции оценок в определенном домене (DSD), используя предобученные модели text-to-image. Эксперименты показывают, что DreamPolish создает 3D-объекты с отполированными поверхностями и фотореалистичными текстурами, превосходя современные методы.'}, 'en': {'title': 'DreamPolish: Elevating 3D Generation with Refined Geometry and Textures', 'desc': 'DreamPolish is a text-to-3D generation model that focuses on creating high-quality 3D objects with refined geometry and textures. It improves the geometry construction by using multiple neural representations and an additional normal estimator to reduce artifacts caused by limited guidance. The model also introduces a surface polishing stage that requires minimal training to enhance the geometric details further. For texture generation, it employs a novel domain score distillation (DSD) method, inspired by classifier-free guidance, to achieve photorealistic and consistent textures in the generated 3D assets.'}, 'zh': {'title': 'DreamPolish：生成高质量3D资产的创新模型', 'desc': '本文介绍了一种名为DreamPolish的文本到3D生成模型，能够生成精细的几何形状和高质量的纹理。在几何构建阶段，我们利用多种神经表示来增强合成过程的稳定性，并引入额外的法线估计器来改善几何细节。纹理生成阶段采用了一种新的评分蒸馏目标，称为领域评分蒸馏（DSD），以引导神经表示朝向包含真实感和一致性渲染的适当领域。实验表明，我们的模型能够生成表面光滑且具有真实感纹理的3D资产，超越了现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2411.03047', 'title': 'GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single In-the-Wild Image using a Dataset with Levels of Details', 'url': 'https://huggingface.co/papers/2411.03047', 'abstract': 'Neural implicit functions have brought impressive advances to the state-of-the-art of clothed human digitization from multiple or even single images. However, despite the progress, current arts still have difficulty generalizing to unseen images with complex cloth deformation and body poses. In this work, we present GarVerseLOD, a new dataset and framework that paves the way to achieving unprecedented robustness in high-fidelity 3D garment reconstruction from a single unconstrained image. Inspired by the recent success of large generative models, we believe that one key to addressing the generalization challenge lies in the quantity and quality of 3D garment data. Towards this end, GarVerseLOD collects 6,000 high-quality cloth models with fine-grained geometry details manually created by professional artists. In addition to the scale of training data, we observe that having disentangled granularities of geometry can play an important role in boosting the generalization capability and inference accuracy of the learned model. We hence craft GarVerseLOD as a hierarchical dataset with levels of details (LOD), spanning from detail-free stylized shape to pose-blended garment with pixel-aligned details. This allows us to make this highly under-constrained problem tractable by factorizing the inference into easier tasks, each narrowed down with smaller searching space. To ensure GarVerseLOD can generalize well to in-the-wild images, we propose a novel labeling paradigm based on conditional diffusion models to generate extensive paired images for each garment model with high photorealism. We evaluate our method on a massive amount of in-the-wild images. Experimental results demonstrate that GarVerseLOD can generate standalone garment pieces with significantly better quality than prior approaches. Project page: https://garverselod.github.io/', 'score': 5, 'issue_id': 444, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 ноября', 'en': 'November 5', 'zh': '11月5日'}, 'hash': 'eadbb8f8b93d4818', 'data': {'categories': ['#dataset', '#3d', '#cv'], 'emoji': '👚', 'ru': {'title': 'GarVerseLOD: Революция в 3D-реконструкции одежды по одному изображению', 'desc': 'GarVerseLOD - это новый набор данных и фреймворк для реконструкции 3D-одежды по одному изображению. Он включает 6000 высококачественных моделей одежды с детализированной геометрией, созданных профессиональными художниками. Набор данных имеет иерархическую структуру с уровнями детализации, от стилизованных форм до одежды с учетом позы и пиксельно-точными деталями. Для генерации реалистичных парных изображений для каждой модели одежды используется условная диффузионная модель.'}, 'en': {'title': 'Revolutionizing 3D Garment Reconstruction with GarVerseLOD', 'desc': "This paper introduces GarVerseLOD, a new dataset and framework aimed at improving 3D garment reconstruction from single images. The authors highlight the challenges faced by existing methods in generalizing to unseen images with complex clothing and poses. By collecting 6,000 high-quality cloth models and organizing them into a hierarchical dataset with varying levels of detail, they enhance the model's ability to learn and generalize. Additionally, they employ a novel labeling approach using conditional diffusion models to create realistic paired images, resulting in superior garment reconstruction quality compared to previous methods."}, 'zh': {'title': '高保真3D服装重建的新突破', 'desc': '神经隐式函数在从多张或单张图像中数字化穿衣人类方面取得了显著进展。然而，当前的方法在处理复杂的布料变形和身体姿势的未见图像时仍然面临挑战。为了解决这一问题，我们提出了GarVerseLOD，一个新的数据集和框架，旨在从单张无约束图像中实现高保真度的3D服装重建。通过收集6000个高质量的布料模型，并采用分层细节的方式，我们的研究显著提高了模型的泛化能力和推理准确性。'}}}, {'id': 'https://huggingface.co/papers/2411.03312', 'title': 'Inference Optimal VLMs Need Only One Visual Token but Larger Models', 'url': 'https://huggingface.co/papers/2411.03312', 'abstract': 'Vision Language Models (VLMs) have demonstrated strong capabilities across various visual understanding and reasoning tasks. However, their real-world deployment is often constrained by high latency during inference due to substantial compute required to process the large number of input tokens (predominantly from the image) by the LLM. To reduce inference costs, one can either downsize the LLM or reduce the number of input image-tokens, the latter of which has been the focus of many recent works around token compression. However, it is unclear what the optimal trade-off is, as both the factors directly affect the VLM performance. We first characterize this optimal trade-off between the number of visual tokens and LLM parameters by establishing scaling laws that capture variations in performance with these two factors. Our results reveal a surprising trend: for visual reasoning tasks, the inference-optimal behavior in VLMs, i.e., minimum downstream error at any given fixed inference compute, is achieved when using the largest LLM that fits within the inference budget while minimizing visual token count - often to a single token. While the token reduction literature has mainly focused on maintaining base model performance by modestly reducing the token count (e.g., 5-10times), our results indicate that the compute-optimal inference regime requires operating under even higher token compression ratios. Based on these insights, we take some initial steps towards building approaches tailored for high token compression settings. Code is available at https://github.com/locuslab/llava-token-compression.', 'score': 4, 'issue_id': 446, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 ноября', 'en': 'November 5', 'zh': '11月5日'}, 'hash': '60954fb0c9d4b5fb', 'data': {'categories': ['#inference', '#cv', '#training'], 'emoji': '🔬', 'ru': {'title': 'Меньше токенов, больше модель: новый подход к оптимизации VLM', 'desc': 'Эта статья исследует оптимальный баланс между количеством визуальных токенов и параметрами языковой модели (LLM) в vision-language моделях (VLM). Авторы установили закономерности масштабирования, которые показывают, как производительность VLM меняется в зависимости от этих двух факторов. Результаты демонстрируют, что для задач визуального рассуждения оптимальная производительность достигается при использовании самой большой LLM, которая помещается в бюджет вычислений, при минимизации количества визуальных токенов. Исследование предлагает новый взгляд на оптимизацию VLM для эффективного вывода.'}, 'en': {'title': 'Maximizing VLM Efficiency with Token Compression', 'desc': 'This paper explores the efficiency of Vision Language Models (VLMs) in visual understanding tasks, focusing on the trade-off between the number of visual tokens and the size of the language model (LLM). The authors establish scaling laws to determine how performance varies with these two factors, revealing that optimal performance is achieved with minimal visual tokens, often reducing to just one token. They highlight that traditional token reduction methods may not be sufficient for achieving the best inference performance, suggesting that higher compression ratios are necessary. The paper also proposes initial strategies for adapting VLMs to work effectively under these high token compression conditions.'}, 'zh': {'title': '优化推理：最大化LLM与最小化视觉标记的平衡', 'desc': '视觉语言模型（VLMs）在视觉理解和推理任务中表现出色，但在实际应用中，由于处理大量输入标记（主要来自图像）所需的计算量大，推理延迟较高。为了降低推理成本，可以选择缩小大型语言模型（LLM）或减少输入图像标记的数量，后者是许多近期研究的重点。我们通过建立缩放法则来描述视觉标记数量与LLM参数之间的最佳权衡，发现对于视觉推理任务，最佳推理行为是在推理预算内使用最大的LLM，同时将视觉标记数量减少到最小，通常只需一个标记。我们的研究表明，计算最优的推理模式需要在更高的标记压缩比下进行操作。'}}}, {'id': 'https://huggingface.co/papers/2411.02657', 'title': 'Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare Disease Knowledge', 'url': 'https://huggingface.co/papers/2411.02657', 'abstract': "Rare diseases present unique challenges in healthcare, often suffering from delayed diagnosis and fragmented information landscapes. The scarcity of reliable knowledge in these conditions poses a distinct challenge for Large Language Models (LLMs) in supporting clinical management and delivering precise patient information underscoring the need for focused training on these 'zebra' cases. We present Zebra-Llama, a specialized context-aware language model with high precision Retrieval Augmented Generation (RAG) capability, focusing on Ehlers-Danlos Syndrome (EDS) as our case study. EDS, affecting 1 in 5,000 individuals, exemplifies the complexities of rare diseases with its diverse symptoms, multiple subtypes, and evolving diagnostic criteria. By implementing a novel context-aware fine-tuning methodology trained on questions derived from medical literature, patient experiences, and clinical resources, along with expertly curated responses, Zebra-Llama demonstrates unprecedented capabilities in handling EDS-related queries. On a test set of real-world questions collected from EDS patients and clinicians, medical experts evaluated the responses generated by both models, revealing Zebra-Llama's substantial improvements over base model (Llama 3.1-8B-Instruct) in thoroughness (77.5% vs. 70.1%), accuracy (83.0% vs. 78.8%), clarity (74.7% vs. 72.0%) and citation reliability (70.6% vs. 52.3%). Released as an open-source resource, Zebra-Llama not only provides more accessible and reliable EDS information but also establishes a framework for developing specialized AI solutions for other rare conditions. This work represents a crucial step towards democratizing expert-level knowledge in rare disease management, potentially transforming how healthcare providers and patients navigate the complex landscape of rare diseases.", 'score': 4, 'issue_id': 439, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '26c0b7bc39488945', 'data': {'categories': ['#rag', '#medicine', '#training', '#alignment'], 'emoji': '🦓', 'ru': {'title': 'Zebra-Llama: ИИ-эксперт по редким заболеваниям', 'desc': 'Статья представляет Zebra-Llama - специализированную языковую модель для редких заболеваний, используя синдром Элерса-Данлоса (EDS) как пример. Модель использует контекстно-зависимую настройку и усиленную генерацию с извлечением (RAG) для обработки запросов, связанных с EDS. Zebra-Llama показала значительные улучшения по сравнению с базовой моделью в полноте, точности, ясности и надежности цитирования при ответе на реальные вопросы пациентов и врачей. Эта работа открывает путь к созданию специализированных ИИ-решений для других редких заболеваний, демократизируя экспертные знания в этой области.'}, 'en': {'title': 'Zebra-Llama: Transforming Rare Disease Management with AI', 'desc': 'This paper introduces Zebra-Llama, a specialized language model designed to improve the management of rare diseases, specifically Ehlers-Danlos Syndrome (EDS). It addresses the challenges posed by limited information and delayed diagnoses in rare conditions by utilizing a context-aware fine-tuning approach. The model employs Retrieval Augmented Generation (RAG) to enhance the precision of responses to EDS-related queries, outperforming the base model in various evaluation metrics. By making Zebra-Llama an open-source resource, the authors aim to democratize access to expert knowledge in rare disease management, paving the way for similar advancements in other rare conditions.'}, 'zh': {'title': 'Zebra-Llama：罕见疾病管理的新突破', 'desc': '罕见疾病在医疗保健中面临独特挑战，常常导致诊断延迟和信息碎片化。针对这些罕见病例，本文提出了一种名为Zebra-Llama的专门语言模型，具备高精度的检索增强生成能力，重点关注Ehlers-Danlos综合症（EDS）。通过一种新颖的上下文感知微调方法，Zebra-Llama在处理与EDS相关的问题时表现出前所未有的能力，显著提高了回答的全面性、准确性和清晰度。该模型作为开源资源发布，不仅提供了更易获取和可靠的EDS信息，还为其他罕见疾病开发专门的人工智能解决方案奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2411.02844', 'title': 'Correlation of Object Detection Performance with Visual Saliency and Depth Estimation', 'url': 'https://huggingface.co/papers/2411.02844', 'abstract': "As object detection techniques continue to evolve, understanding their relationships with complementary visual tasks becomes crucial for optimising model architectures and computational resources. This paper investigates the correlations between object detection accuracy and two fundamental visual tasks: depth prediction and visual saliency prediction. Through comprehensive experiments using state-of-the-art models (DeepGaze IIE, Depth Anything, DPT-Large, and Itti's model) on COCO and Pascal VOC datasets, we find that visual saliency shows consistently stronger correlations with object detection accuracy (mArho up to 0.459 on Pascal VOC) compared to depth prediction (mArho up to 0.283). Our analysis reveals significant variations in these correlations across object categories, with larger objects showing correlation values up to three times higher than smaller objects. These findings suggest incorporating visual saliency features into object detection architectures could be more beneficial than depth information, particularly for specific object categories. The observed category-specific variations also provide insights for targeted feature engineering and dataset design improvements, potentially leading to more efficient and accurate object detection systems.", 'score': 3, 'issue_id': 444, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 ноября', 'en': 'November 5', 'zh': '11月5日'}, 'hash': 'd17ec72bcba1cd05', 'data': {'categories': ['#cv', '#architecture', '#dataset'], 'emoji': '👁️', 'ru': {'title': 'Визуальная значимость превосходит глубину в улучшении обнаружения объектов', 'desc': 'Исследование изучает связь между точностью обнаружения объектов и задачами предсказания глубины и визуальной значимости. Эксперименты с современными моделями на наборах данных COCO и Pascal VOC показали, что визуальная значимость имеет более сильную корреляцию с точностью обнаружения объектов по сравнению с предсказанием глубины. Обнаружены значительные различия в корреляциях между категориями объектов, причем для более крупных объектов корреляция до трех раз выше. Результаты предполагают, что включение признаков визуальной значимости в архитектуры обнаружения объектов может быть более полезным, чем информация о глубине.'}, 'en': {'title': 'Enhancing Object Detection with Visual Saliency Insights', 'desc': 'This paper explores how object detection accuracy relates to two other visual tasks: depth prediction and visual saliency prediction. The authors conducted experiments with advanced models on popular datasets and found that visual saliency has a stronger correlation with object detection accuracy than depth prediction. They noted that larger objects tend to show much higher correlation values compared to smaller ones. The results suggest that integrating visual saliency features into object detection models could enhance performance, especially for certain object categories.'}, 'zh': {'title': '视觉显著性助力物体检测精度提升', 'desc': '本文研究了物体检测精度与深度预测和视觉显著性预测这两种基本视觉任务之间的关系。通过在COCO和Pascal VOC数据集上使用先进模型进行全面实验，发现视觉显著性与物体检测精度的相关性明显高于深度预测。研究还表明，不同物体类别之间的相关性存在显著差异，大型物体的相关性值可高达小型物体的三倍。结果表明，将视觉显著性特征融入物体检测架构可能比深度信息更有利，尤其是在特定物体类别中。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (3)', '#agi', '#alignment (3)', '#architecture (3)', '#audio', '#benchmark (1)', '#cv (6)', '#data (1)', '#dataset (3)', '#diffusion', '#edge_computing', '#ethics', '#games', '#graphs', '#hallucinations', '#inference (2)', '#interpretability (1)', '#long_context', '#math', '#medicine (1)', '#multilingual', '#multimodal (1)', '#optimization (1)', '#plp', '#rag (2)', '#reasoning', '#rl', '#rlhf (2)', '#robotics', '#robots', '#security', '#story_generation', '#survey', '#synthetic', '#training (5)', '#transfer_learning', '#translation', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">📝 ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-11-07 08:15',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-11-07 08:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-11-07 08:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    