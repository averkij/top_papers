
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 40 papers. June 4.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">4 июня</span> | <span id="title-articles-count">40 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-06-03.html">⬅️ <span id="prev-date">03.06</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-06-05.html">➡️ <span id="next-date">05.06</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-06.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'};
        let feedDateNext = {'ru': '05.06', 'en': '06/05', 'zh': '6月5日'};
        let feedDatePrev = {'ru': '03.06', 'en': '06/03', 'zh': '6月3日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2505.24726', 'title': 'Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.24726', 'abstract': "A method using self-reflection and reinforcement learning improves the performance of large language models, especially with limited feedback, by rewarding self-reflections that lead to better task performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We explore a method for improving the performance of large language models through self-reflection and reinforcement learning. By incentivizing the model to generate better self-reflections when it answers incorrectly, we demonstrate that a model's ability to solve complex, verifiable tasks can be enhanced even when generating synthetic data is infeasible and only binary feedback is available. Our framework operates in two stages: first, upon failing a given task, the model generates a self-reflective commentary analyzing its previous attempt; second, the model is given another attempt at the task with the self-reflection in context. If the subsequent attempt succeeds, the tokens generated during the self-reflection phase are rewarded. Our experimental results show substantial performance gains across a variety of model architectures, as high as 34.7% improvement at math equation writing and 18.1% improvement at function calling. Notably, smaller fine-tuned models (1.5 billion to 7 billion parameters) outperform models in the same family that are 10 times larger. Our novel paradigm is thus an exciting pathway to more useful and reliable language models that can self-improve on challenging tasks with limited external feedback.", 'score': 91, 'issue_id': 4116, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '05f75b6123a35a65', 'authors': ['Shelly Bensal', 'Umar Jamil', 'Christopher Bryant', 'Melisa Russak', 'Kiran Kamble', 'Dmytro Mozolevskyi', 'Muayad Ali', 'Waseem AlShikh'], 'affiliations': ['Writer, Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.24726.jpg', 'data': {'categories': ['#optimization', '#training', '#rlhf', '#small_models', '#reasoning', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Самоанализ и обучение с подкреплением повышают эффективность языковых моделей', 'desc': 'Исследователи предложили метод улучшения работы больших языковых моделей с помощью самоанализа и обучения с подкреплением. Модель генерирует самоанализ после неудачной попытки решения задачи, а затем повторно пытается её решить с учетом этого анализа. Если вторая попытка успешна, токены самоанализа получают положительное подкрепление. Эксперименты показали значительное улучшение производительности на различных задачах, особенно для небольших моделей.'}, 'en': {'title': 'Empowering Language Models Through Self-Reflection and Reinforcement Learning', 'desc': 'This paper presents a novel approach to enhance large language models using self-reflection and reinforcement learning. The method encourages models to analyze their mistakes and generate self-reflective commentary, which is then used to improve subsequent task attempts. By rewarding successful outcomes that follow self-reflection, the model learns to perform better even with minimal feedback. Experimental results indicate significant performance improvements, particularly in smaller models, suggesting a promising direction for developing more effective language models.'}, 'zh': {'title': '自我反思与强化学习提升语言模型性能', 'desc': '本文提出了一种通过自我反思和强化学习来提升大型语言模型性能的方法。该方法在模型回答错误时，激励其生成更好的自我反思，从而提高解决复杂任务的能力。框架分为两个阶段：首先，模型在失败后生成自我反思的评论；其次，模型在考虑自我反思的情况下再次尝试任务。实验结果显示，在多种模型架构中，性能提升显著，尤其是小型微调模型的表现超过了同类更大模型。'}}}, {'id': 'https://huggingface.co/papers/2506.02387', 'title': 'VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in\n  Multi-Agent Environments', 'url': 'https://huggingface.co/papers/2506.02387', 'abstract': "VS-Bench is a multimodal benchmark designed to evaluate Vision Language Models' strategic reasoning and decision-making in complex multi-agent environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Vision Language Models (VLMs) have expanded their capabilities to interactive agent tasks, yet existing benchmarks remain limited to single-agent or text-only environments. In contrast, real-world scenarios often involve multiple agents interacting within rich visual and linguistic contexts, posing challenges with both multimodal observations and strategic interactions. To bridge this gap, we introduce Visual Strategic Bench (VS-Bench), a multimodal benchmark that evaluates VLMs for strategic reasoning and decision-making in multi-agent environments. VS-Bench comprises eight vision-grounded environments spanning cooperative, competitive, and mixed-motive interactions, designed to assess agents' ability to predict others' future moves and optimize for long-term objectives. We consider two complementary evaluation dimensions, including offline evaluation of strategic reasoning by next-action prediction accuracy and online evaluation of decision-making by normalized episode return. Extensive experiments of fourteen leading VLMs reveal a significant gap between current models and optimal performance, with the best models attaining 47.8% prediction accuracy and 24.3% normalized return. We further conduct in-depth analyses on multimodal observations, test-time scaling, social behaviors, and failure cases of VLM agents. By standardizing the evaluation and highlighting the limitations of existing models, we envision VS-Bench as a foundation for future research on strategic multimodal agents. Code and data are available at https://vs-bench.github.io.", 'score': 45, 'issue_id': 4110, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '829f3546d3ac9345', 'authors': ['Zelai Xu', 'Zhexuan Xu', 'Xiangmin Yi', 'Huining Yuan', 'Xinlei Chen', 'Yi Wu', 'Chao Yu', 'Yu Wang'], 'affiliations': ['Beijing Zhongguancun Academy', 'Shanghai Qi Zhi Institute', 'Tsinghua University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.02387.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#agents', '#benchmark', '#games'], 'emoji': '🤖', 'ru': {'title': 'VS-Bench: Новый рубеж в оценке стратегического мышления мультимодальных ИИ-агентов', 'desc': 'VS-Bench - это мультимодальный бенчмарк для оценки стратегического мышления и принятия решений vision-language моделями в сложных мультиагентных средах. Он включает восемь визуальных сред с кооперативными, соревновательными и смешанными взаимодействиями. Бенчмарк оценивает способность моделей предсказывать будущие действия других агентов и оптимизировать долгосрочные цели. Эксперименты с 14 ведущими VLM показали значительный разрыв между текущими моделями и оптимальной производительностью.'}, 'en': {'title': 'Evaluating Strategic Reasoning in Multi-Agent Environments with VS-Bench', 'desc': 'VS-Bench is a new benchmark created to test Vision Language Models (VLMs) in complex situations where multiple agents interact. Unlike previous benchmarks that focused on single agents or text-only tasks, VS-Bench evaluates how well VLMs can reason and make decisions in environments that include both visual and language elements. It features eight different scenarios that require agents to work together, compete, or navigate mixed motives, assessing their ability to predict actions and achieve long-term goals. The results show that current VLMs still have a long way to go, with significant gaps in their performance, highlighting the need for further research in this area.'}, 'zh': {'title': '多模态智能体的战略推理新基准', 'desc': 'VS-Bench是一个多模态基准，旨在评估视觉语言模型（VLM）在复杂多智能体环境中的战略推理和决策能力。与现有的单智能体或仅文本环境的基准不同，VS-Bench考虑了多个智能体在丰富的视觉和语言背景下的互动。该基准包括八个基于视觉的环境，涵盖合作、竞争和混合动机的互动，评估智能体预测他人未来动作和优化长期目标的能力。通过标准化评估，VS-Bench为未来的战略多模态智能体研究奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2506.03147', 'title': 'UniWorld: High-Resolution Semantic Encoders for Unified Visual\n  Understanding and Generation', 'url': 'https://huggingface.co/papers/2506.03147', 'abstract': "A unified generative framework called UniWorld uses semantic features from visual-language models for image perception and manipulation, outperforming BAGEL with reduced data.  \t\t\t\t\tAI-generated summary \t\t\t\t Although existing unified models deliver strong performance on vision-language understanding and text-to-image generation, their models are limited in exploring image perception and manipulation tasks, which are urgently desired by users for wide applications. Recently, OpenAI released their powerful GPT-4o-Image model for comprehensive image perception and manipulation, achieving expressive capability and attracting community interests. By observing the performance of GPT-4o-Image in our carefully constructed experiments, we infer that GPT-4o-Image leverages features extracted by semantic encoders instead of VAE, while VAEs are considered essential components in many image manipulation models. Motivated by such inspiring observations, we present a unified generative framework named UniWorld based on semantic features provided by powerful visual-language models and contrastive semantic encoders. As a result, we build a strong unified model using only 1% amount of BAGEL's data, which consistently outperforms BAGEL on image editing benchmarks. UniWorld also maintains competitive image understanding and generation capabilities, achieving strong performance across multiple image perception tasks. We fully open-source our models, including model weights, training and evaluation scripts, and datasets.", 'score': 44, 'issue_id': 4110, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '34f1d96b37be24a1', 'authors': ['Bin Lin', 'Zongjian Li', 'Xinhua Cheng', 'Yuwei Niu', 'Yang Ye', 'Xianyi He', 'Shenghai Yuan', 'Wangbo Yu', 'Shaodong Wang', 'Yunyang Ge', 'Yatian Pang', 'Li Yuan'], 'affiliations': ['Peking University, Shenzhen Graduate School', 'Peng Cheng Laboratory', 'Rabbitpre AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.03147.jpg', 'data': {'categories': ['#dataset', '#cv', '#multimodal', '#open_source', '#benchmark'], 'emoji': '🎨', 'ru': {'title': 'UniWorld: Мощная модель для работы с изображениями на основе семантических признаков', 'desc': 'UniWorld - это унифицированная генеративная модель для восприятия и редактирования изображений. Она использует семантические признаки из визуально-языковых моделей вместо вариационных автоэнкодеров. UniWorld превосходит модель BAGEL по качеству редактирования изображений, используя всего 1% данных. Модель также демонстрирует высокие результаты в задачах понимания и генерации изображений.'}, 'en': {'title': 'UniWorld: Efficient Image Manipulation with Semantic Power', 'desc': 'UniWorld is a new generative framework that enhances image perception and manipulation by utilizing semantic features from visual-language models. It outperforms the existing model BAGEL while using only 1% of its data, demonstrating efficiency in data usage. The framework leverages insights from the GPT-4o-Image model, which effectively uses semantic encoders instead of traditional Variational Autoencoders (VAEs). UniWorld not only excels in image editing tasks but also maintains strong capabilities in image understanding and generation across various applications.'}, 'zh': {'title': 'UniWorld：图像感知与操作的新纪元', 'desc': 'UniWorld是一个统一的生成框架，利用视觉语言模型的语义特征来进行图像感知和操作。与BAGEL相比，UniWorld在数据使用上减少了90%，但在图像编辑基准测试中表现更优。该模型不仅在图像理解和生成方面保持竞争力，还在多个图像感知任务中取得了良好的效果。我们将模型权重、训练和评估脚本以及数据集全部开源，方便研究者使用。'}}}, {'id': 'https://huggingface.co/papers/2505.24120', 'title': 'CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning\n  Capabilities of VLMs', 'url': 'https://huggingface.co/papers/2505.24120', 'abstract': 'A new benchmark, CSVQA, evaluates scientific reasoning in vision-language models through domain-specific visual question answering, highlighting the need for improvement in these models.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have demonstrated remarkable progress in multimodal understanding, yet their capabilities for scientific reasoning remains inadequately assessed. Current multimodal benchmarks predominantly evaluate generic image comprehension or text-driven reasoning, lacking authentic scientific contexts that require domain-specific knowledge integration with visual evidence analysis. To fill this gap, we present CSVQA, a diagnostic multimodal benchmark specifically designed for evaluating scientific reasoning through domain-grounded visual question answering.Our benchmark features 1,378 carefully constructed question-answer pairs spanning diverse STEM disciplines, each demanding domain knowledge, integration of visual evidence, and higher-order reasoning. Compared to prior multimodal benchmarks, CSVQA places greater emphasis on real-world scientific content and complex reasoning.We additionally propose a rigorous evaluation protocol to systematically assess whether model predictions are substantiated by valid intermediate reasoning steps based on curated explanations. Our comprehensive evaluation of 15 VLMs on this benchmark reveals notable performance disparities, as even the top-ranked proprietary model attains only 49.6\\% accuracy.This empirical evidence underscores the pressing need for advancing scientific reasoning capabilities in VLMs. Our CSVQA is released at https://huggingface.co/datasets/Skywork/CSVQA.', 'score': 42, 'issue_id': 4116, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '09ca2aeec21b0156', 'authors': ['Ai Jian', 'Weijie Qiu', 'Xiaokun Wang', 'Peiyu Wang', 'Yunzhuo Hao', 'Jiangbo Pei', 'Yichen Wei', 'Yi Peng', 'Xuchen Song'], 'affiliations': ['Kunlun Inc.', 'Skywork AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.24120.jpg', 'data': {'categories': ['#science', '#multimodal', '#benchmark', '#reasoning'], 'emoji': '🔬', 'ru': {'title': 'CSVQA: новый рубеж в оценке научного мышления ИИ', 'desc': 'Представлен новый бенчмарк CSVQA для оценки научного мышления в мультимодальных моделях через задачи ответов на вопросы по изображениям в конкретных научных областях. Бенчмарк содержит 1378 пар вопрос-ответ из различных STEM-дисциплин, требующих применения предметных знаний и анализа визуальных данных. Оценка 15 современных мультимодальных моделей на CSVQA показала значительные различия в производительности, при этом даже лучшая модель достигла точности лишь 49.6%. Результаты подчеркивают необходимость улучшения способностей научного мышления в мультимодальных моделях.'}, 'en': {'title': 'CSVQA: Advancing Scientific Reasoning in Vision-Language Models', 'desc': 'The paper introduces CSVQA, a new benchmark designed to evaluate the scientific reasoning abilities of vision-language models (VLMs) through domain-specific visual question answering. Unlike existing benchmarks that focus on generic image understanding, CSVQA emphasizes the integration of domain knowledge and visual evidence in STEM contexts. It includes 1,378 question-answer pairs that require higher-order reasoning and authentic scientific content. The evaluation of 15 VLMs on this benchmark reveals significant performance gaps, highlighting the need for improvements in their scientific reasoning capabilities.'}, 'zh': {'title': 'CSVQA：提升视觉语言模型的科学推理能力', 'desc': 'CSVQA是一个新的基准，用于评估视觉语言模型在科学推理方面的能力。该基准通过领域特定的视觉问答，强调了这些模型在科学推理中的不足。CSVQA包含1378个精心构建的问题-答案对，涵盖多个STEM学科，要求模型整合领域知识和视觉证据进行高阶推理。我们的评估显示，尽管有些模型表现较好，但最高分的模型准确率仅为49.6%，这表明在科学推理能力上仍需进一步提升。'}}}, {'id': 'https://huggingface.co/papers/2506.02096', 'title': 'SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis', 'url': 'https://huggingface.co/papers/2506.02096', 'abstract': "SynthRL, a scalable pipeline for data synthesis in reinforcement learning with verifiable rewards, enhances visual math reasoning VLMs by generating challenging, verifiable questions.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) trained via reinforcement learning with verifiable reward (RLVR) have shown notable progress in scaling test-time compute effectively. In this work, we investigate how synthesized RL data can further improve RLVR. To this end, we propose SynthRL-a scalable and guaranteed pipeline for automatic data scaling in reasoning-oriented RL training. SynthRL comprises three key stages: (1) selecting seed questions with appropriate distribution, (2) augmenting them into more challenging variants while preserving the original answers, and (3) a guaranteed verification stage that ensures near-perfect correctness and difficulty enhancement. Our empirical experiments demonstrate SynthRL's scalability and effectiveness. When applied to the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable, challenging questions from approximately 8K seed samples. Models trained with our synthesized data achieve consistent gains across five out-of-domain visual math reasoning benchmarks, with a significant improvement over baseline models trained on seed data alone. Notably, detailed analysis reveals that the gains are more pronounced on the most challenging evaluation samples, highlighting SynthRL's effectiveness in eliciting deeper and more complex reasoning patterns.", 'score': 31, 'issue_id': 4110, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '9ea84a7af081829e', 'authors': ['Zijian Wu', 'Jinjie Ni', 'Xiangyan Liu', 'Zichen Liu', 'Hang Yan', 'Michael Qizhe Shieh'], 'affiliations': ['National University of Singapore', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.02096.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#synthetic', '#rl'], 'emoji': '🧠', 'ru': {'title': 'SynthRL: Синтез данных для улучшения математических рассуждений ИИ', 'desc': 'SynthRL - это масштабируемый конвейер для синтеза данных в обучении с подкреплением с проверяемыми наградами. Он улучшает визуальные языковые модели для математических рассуждений, генерируя сложные, проверяемые вопросы. SynthRL включает три ключевых этапа: выбор исходных вопросов, их усложнение с сохранением ответов и верификацию для обеспечения корректности. Эксперименты показали, что модели, обученные на синтезированных данных, достигают стабильных улучшений на пяти тестовых наборах по визуальным математическим рассуждениям.'}, 'en': {'title': 'SynthRL: Elevating Visual Math Reasoning with Scalable Data Synthesis', 'desc': "SynthRL is a novel pipeline designed to enhance reinforcement learning (RL) by synthesizing data with verifiable rewards, specifically for visual math reasoning tasks. It operates in three stages: selecting initial questions, creating more challenging variants while keeping the answers intact, and verifying the correctness of these questions. This approach allows for the generation of over 3,300 additional verifiable questions from a smaller set of seed samples, significantly improving model performance. Empirical results show that models trained with SynthRL's data outperform those trained only on seed data, especially on difficult reasoning tasks, demonstrating its effectiveness in fostering advanced reasoning capabilities."}, 'zh': {'title': 'SynthRL：提升视觉数学推理的智能合成管道', 'desc': 'SynthRL是一种可扩展的数据合成管道，旨在增强强化学习中的可验证奖励（RLVR），特别是在视觉数学推理模型（VLMs）中。该方法通过生成具有挑战性和可验证的问题，来提高模型的推理能力。SynthRL包括三个关键阶段：选择合适分布的种子问题、将其增强为更具挑战性的变体，并确保答案的正确性和难度的提升。实验结果表明，使用SynthRL合成的数据在多个视觉数学推理基准测试中显著提高了模型的表现，尤其是在最具挑战性的样本上。'}}}, {'id': 'https://huggingface.co/papers/2506.03135', 'title': 'OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for\n  Vision Language Models', 'url': 'https://huggingface.co/papers/2506.03135', 'abstract': "A comprehensive benchmark called OmniSpatial evaluates vision-language models' understanding of advanced spatial reasoning tasks, revealing significant limitations across various models.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatial reasoning is a key aspect of cognitive psychology and remains a major bottleneck for current vision-language models (VLMs). While extensive research has aimed to evaluate or improve VLMs' understanding of basic spatial relations, such as distinguishing left from right, near from far, and object counting, these tasks represent only the most fundamental level of spatial reasoning. In this work, we introduce OmniSpatial, a comprehensive and challenging benchmark for spatial reasoning, grounded in cognitive psychology. OmniSpatial covers four major categories: dynamic reasoning, complex spatial logic, spatial interaction, and perspective-taking, with 50 fine-grained subcategories. Through Internet data crawling and careful manual annotation, we construct over 1.5K question-answer pairs. Extensive experiments show that both open- and closed-source VLMs, as well as existing reasoning and spatial understanding models, exhibit significant limitations in comprehensive spatial understanding. We further analyze failure cases and propose potential directions for future research.", 'score': 25, 'issue_id': 4116, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'a7cd11e4c04b1336', 'authors': ['Mengdi Jia', 'Zekun Qi', 'Shaochen Zhang', 'Wenyao Zhang', 'Xinqiang Yu', 'Jiawei He', 'He Wang', 'Li Yi'], 'affiliations': ['Galbot', 'Peking University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Shanghai Qi Zhi Institute', 'Tsinghua University', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.03135.jpg', 'data': {'categories': ['#cv', '#benchmark', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'OmniSpatial: новый рубеж в оценке пространственного мышления ИИ', 'desc': 'OmniSpatial - это комплексный тест для оценки понимания пространственных задач моделями компьютерного зрения и обработки естественного языка. Он охватывает четыре основные категории: динамическое рассуждение, сложную пространственную логику, пространственное взаимодействие и принятие перспективы. Эксперименты показали, что современные модели имеют значительные ограничения в комплексном пространственном понимании. Авторы проанализировали случаи неудач и предложили направления для будущих исследований.'}, 'en': {'title': 'OmniSpatial: Elevating Spatial Reasoning in Vision-Language Models', 'desc': 'This paper introduces OmniSpatial, a new benchmark designed to assess the capabilities of vision-language models (VLMs) in advanced spatial reasoning tasks. It highlights that while VLMs can handle basic spatial relations, they struggle with more complex reasoning, which is crucial for understanding human-like spatial interactions. The benchmark includes four main categories of spatial reasoning, with a total of 50 detailed subcategories, and is supported by over 1,500 carefully crafted question-answer pairs. The findings reveal significant shortcomings in current VLMs, prompting a discussion on future research directions to enhance their spatial reasoning abilities.'}, 'zh': {'title': 'OmniSpatial：提升视觉-语言模型的空间推理能力', 'desc': '本文介绍了一个名为OmniSpatial的基准测试，旨在评估视觉-语言模型在高级空间推理任务中的理解能力。研究发现，当前的视觉-语言模型在空间推理方面存在显著的局限性，尤其是在动态推理、复杂空间逻辑、空间交互和视角转换等方面。OmniSpatial基于认知心理学，涵盖了50个细分类别，并通过网络数据爬取和人工标注构建了1500多个问答对。实验结果表明，无论是开源还是闭源的视觉-语言模型，在全面的空间理解上都表现不佳，本文还分析了失败案例并提出了未来研究的潜在方向。'}}}, {'id': 'https://huggingface.co/papers/2506.02397', 'title': 'OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for\n  Over-Reasoning Mitigation', 'url': 'https://huggingface.co/papers/2506.02397', 'abstract': 'OThink-R1 is introduced to reduce reasoning redundancy in complex problem-solving by classifying reasoning steps as essential or redundant and dynamically switching thinking modes based on task complexity.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advanced large reasoning models (LRMs) leverage extended chain-of-thought (CoT) reasoning to solve complex tasks, achieving state-of-the-art performance. Despite their success, we identify a critical issue: a substantial portion of simple tasks solved by LRMs can also be addressed by non-reasoning LLMs using significantly fewer tokens, indicating the complex reasoning may not always be necessary. To address this, we systematically analyze the reasoning trajectories of LRMs and present a method utilizing identified paradigms and LLM-Judge to classify these trajectories as either Redundant Reasoning or Essential Reasoning. And we introduce OThink-R1, a method that prunes redundant reasoning steps while preserving logical validity. OThink-R1 dynamically employs the non-thinking mode (fast-thinking) for straightforward problems while engaging in deliberate thinking (slow-thinking) for complex problems. Experiments across mathematical and question-answering tasks demonstrate that OThink-R1 reduces reasoning redundancy by almost 23\\% on average without compromising accuracy, offering practical guidelines for efficient reasoning models. The code is available at https://github.com/AgenticIR-Lab/OThink-R1.', 'score': 25, 'issue_id': 4120, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '12cee2b297810e65', 'authors': ['Shengjia Zhang', 'Junjie Wu', 'Jiawei Chen', 'Changwang Zhang', 'Xingyu Lou', 'Wangchunshu Zhou', 'Sheng Zhou', 'Can Wang', 'Jun Wang'], 'affiliations': ['OPPO Research Institute, Shenzhen, China', 'Zhejiang University, Hangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.02397.jpg', 'data': {'categories': ['#training', '#math', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Оптимизация рассуждений ИИ: эффективность без потери точности', 'desc': 'OThink-R1 - это новый метод машинного обучения, направленный на уменьшение избыточности рассуждений при решении сложных задач. Он классифицирует шаги рассуждений как существенные или избыточные и динамически переключает режимы мышления в зависимости от сложности задачи. OThink-R1 использует идентифицированные парадигмы и LLM-Judge для классификации траекторий рассуждений. Эксперименты показывают, что OThink-R1 снижает избыточность рассуждений в среднем на 23% без ущерба для точности.'}, 'en': {'title': 'Optimize Reasoning: Think Smart, Not Hard!', 'desc': 'OThink-R1 is a novel approach designed to enhance reasoning efficiency in complex problem-solving by distinguishing between essential and redundant reasoning steps. It leverages a classification system to identify when complex reasoning is unnecessary, allowing for a switch between fast-thinking and slow-thinking modes based on task complexity. This method significantly reduces reasoning redundancy by approximately 23% while maintaining accuracy in tasks like mathematics and question-answering. The findings suggest that not all tasks require extensive reasoning, and OThink-R1 provides a framework for optimizing reasoning processes in large reasoning models.'}, 'zh': {'title': '动态思维模式，减少推理冗余', 'desc': 'OThink-R1 是一种新方法，旨在减少复杂问题解决中的推理冗余。它通过将推理步骤分类为必要或冗余，并根据任务复杂性动态切换思维模式。对于简单问题，OThink-R1 使用快速思维模式，而对于复杂问题则采用深思熟虑的慢思维模式。实验表明，OThink-R1 平均减少了近23%的推理冗余，同时保持了准确性，为高效推理模型提供了实用指导。'}}}, {'id': 'https://huggingface.co/papers/2506.00123', 'title': 'Visual Embodied Brain: Let Multimodal Large Language Models See, Think,\n  and Control in Spaces', 'url': 'https://huggingface.co/papers/2506.00123', 'abstract': 'VeBrain is a unified framework that integrates multimodal understanding, visual-spatial reasoning, and physical interaction for legged robots, demonstrating superior performance compared to existing methods across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The remarkable progress of Multimodal Large Language Models (MLLMs) has attracted increasing attention to extend them to physical entities like legged robot. This typically requires MLLMs to not only grasp multimodal understanding abilities, but also integrate visual-spatial reasoning and physical interaction capabilities. Nevertheless,existing methods struggle to unify these capabilities due to their fundamental differences.In this paper, we present the Visual Embodied Brain (VeBrain), a unified framework for perception, reasoning, and control in real world. VeBrain reformulates robotic control into common text-based MLLM tasks in the 2D visual space, thus unifying the objectives and mapping spaces of different tasks. Then, a novel robotic adapter is proposed to convert textual control signals from MLLMs to motion policies of real robots. From the data perspective, we further introduce VeBrain-600k, a high-quality instruction dataset encompassing various capabilities of VeBrain. In VeBrain-600k, we take hundreds of hours to collect, curate and annotate the data, and adopt multimodal chain-of-thought(CoT) to mix the different capabilities into a single conversation. Extensive experiments on 13 multimodal benchmarks and 5 spatial intelligence benchmarks demonstrate the superior performance of VeBrain to existing MLLMs like Qwen2.5-VL. When deployed to legged robots and robotic arms, VeBrain shows strong adaptability, flexibility, and compositional capabilities compared to existing methods. For example, compared to Qwen2.5-VL, VeBrain not only achieves substantial gains on MMVet by +5.6%, but also excels in legged robot tasks with +50% average gains.', 'score': 25, 'issue_id': 4114, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '8914927f73dff147', 'authors': ['Gen Luo', 'Ganlin Yang', 'Ziyang Gong', 'Guanzhou Chen', 'Haonan Duan', 'Erfei Cui', 'Ronglei Tong', 'Zhi Hou', 'Tianyi Zhang', 'Zhe Chen', 'Shenglong Ye', 'Lewei Lu', 'Jingbo Wang', 'Wenhai Wang', 'Jifeng Dai', 'Yu Qiao', 'Rongrong Ji', 'Xizhou Zhu'], 'affiliations': ['Nanjing University', 'SenseTime Research', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Tsinghua University', 'University of Science and Technology of China', 'Xiamen University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00123.jpg', 'data': {'categories': ['#games', '#robotics', '#multimodal', '#reasoning', '#benchmark', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'VeBrain: Единый мозг для восприятия, мышления и управления роботами', 'desc': 'VeBrain - это унифицированная система для роботов с ногами, объединяющая мультимодальное понимание, визуально-пространственное мышление и физическое взаимодействие. Она переформулирует управление роботом в текстовые задачи для мультимодальных больших языковых моделей в 2D визуальном пространстве. VeBrain включает новый адаптер для преобразования текстовых сигналов управления в политики движения реальных роботов. Система демонстрирует превосходную производительность по сравнению с существующими методами на различных тестах.'}, 'en': {'title': 'VeBrain: Unifying Multimodal Intelligence for Advanced Robotic Control', 'desc': 'VeBrain is a new framework designed to enhance the capabilities of legged robots by combining multimodal understanding, visual-spatial reasoning, and physical interaction. It reformulates robotic control tasks into text-based tasks that can be processed by Multimodal Large Language Models (MLLMs), allowing for a unified approach to different robotic challenges. The framework includes a unique robotic adapter that translates textual commands from MLLMs into actionable motion policies for robots. Extensive testing shows that VeBrain outperforms existing models, achieving significant improvements in various benchmarks and demonstrating its adaptability in real-world robotic applications.'}, 'zh': {'title': 'VeBrain：四足机器人智能控制的新框架', 'desc': 'VeBrain是一个统一框架，旨在将多模态理解、视觉空间推理和物理交互整合到四足机器人中。该框架通过将机器人控制重新表述为基于文本的多模态大语言模型（MLLM）任务，从而统一了不同任务的目标和映射空间。VeBrain还引入了一种新型的机器人适配器，将MLLM的文本控制信号转换为真实机器人的运动策略。此外，VeBrain-600k是一个高质量的指令数据集，涵盖了VeBrain的多种能力，经过大量数据收集和注释，展示了VeBrain在多模态基准测试中的优越性能。'}}}, {'id': 'https://huggingface.co/papers/2506.01674', 'title': 'MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal\n  LLMs', 'url': 'https://huggingface.co/papers/2506.01674', 'abstract': "MotionSight, a zero-shot method using object-centric visual spotlight and motion blur as prompts, enhances fine-grained video motion understanding and achieves state-of-the-art performance on MotionVid-QA, a large-scale dataset with hierarchical annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite advancements in Multimodal Large Language Models (MLLMs), their proficiency in fine-grained video motion understanding remains critically limited. They often lack inter-frame differencing and tend to average or ignore subtle visual cues. Furthermore, while visual prompting has shown potential in static images, its application to video's temporal complexities, particularly for fine-grained motion understanding, remains largely unexplored. We investigate whether inherent capability can be unlocked and boost MLLMs' motion perception and enable distinct visual signatures tailored to decouple object and camera motion cues. In this study, we introduce MotionSight, a novel zero-shot method pioneering object-centric visual spotlight and motion blur as visual prompts to effectively improve fine-grained motion understanding without training. To convert this into valuable data assets, we curated MotionVid-QA, the first large-scale dataset for fine-grained video motion understanding, with hierarchical annotations including SFT and preference data, {\\Theta}(40K) video clips and {\\Theta}(87K) QAs. Experiments show MotionSight achieves state-of-the-art open-source performance and competitiveness with commercial models. In particular, for fine-grained motion understanding we present a novel zero-shot technique and a large-scale, high-quality dataset. All the code and annotations will be publicly available.", 'score': 19, 'issue_id': 4110, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': 'baebffe54058ae77', 'authors': ['Yipeng Du', 'Tiehan Fan', 'Kepan Nan', 'Rui Xie', 'Penghao Zhou', 'Xiang Li', 'Jian Yang', 'Zhenheng Yang', 'Ying Tai'], 'affiliations': ['ByteDance', 'Nanjing University', 'Nankai University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01674.jpg', 'data': {'categories': ['#dataset', '#cv', '#multimodal', '#open_source', '#games'], 'emoji': '🎥', 'ru': {'title': 'Новый взгляд на движение: MotionSight улучшает понимание видео без обучения', 'desc': 'MotionSight - это новый метод без предварительного обучения для улучшения понимания движения в видео мультимодальными большими языковыми моделями (MLLM). Он использует объектно-ориентированное визуальное выделение и размытие движения в качестве подсказок для моделей. Авторы также создали крупномасштабный датасет MotionVid-QA с иерархическими аннотациями для оценки понимания движения в видео. Эксперименты показывают, что MotionSight достигает лучших результатов среди открытых моделей и конкурентоспособен с коммерческими решениями.'}, 'en': {'title': 'Unlocking Fine-Grained Motion Understanding with MotionSight', 'desc': 'MotionSight is a novel zero-shot method designed to enhance fine-grained video motion understanding by utilizing object-centric visual prompts and motion blur. This approach addresses the limitations of Multimodal Large Language Models (MLLMs) in capturing subtle motion cues across video frames. By introducing a large-scale dataset called MotionVid-QA, which includes hierarchical annotations, the method allows for effective evaluation and improvement of motion perception without the need for extensive training. The results demonstrate that MotionSight achieves state-of-the-art performance, showcasing its potential to unlock advanced motion understanding capabilities in videos.'}, 'zh': {'title': 'MotionSight：提升视频运动理解的新方法', 'desc': 'MotionSight是一种零样本方法，利用以物体为中心的视觉聚焦和运动模糊作为提示，显著提升了细粒度视频运动理解的能力。该方法在MotionVid-QA数据集上取得了最先进的性能，该数据集具有层次化的注释。尽管多模态大型语言模型在视频运动理解方面取得了一些进展，但仍然存在显著的局限性，尤其是在处理细微的视觉线索时。通过引入MotionSight，我们展示了如何在不进行训练的情况下，利用视觉提示来解耦物体和相机运动线索，从而提高运动感知能力。'}}}, {'id': 'https://huggingface.co/papers/2506.03143', 'title': 'GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents', 'url': 'https://huggingface.co/papers/2506.03143', 'abstract': 'GUI-Actor, a VLM-based method using attention for coordinate-free GUI grounding, outperforms existing methods with better generalization and efficient fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t One of the principal challenges in building VLM-powered GUI agents is visual grounding, i.e., localizing the appropriate screen region for action execution based on both the visual content and the textual plans. Most existing work formulates this as a text-based coordinate generation task. However, these approaches suffer from several limitations: weak spatial-semantic alignment, inability to handle ambiguous supervision targets, and a mismatch between the dense nature of screen coordinates and the coarse, patch-level granularity of visual features extracted by models like Vision Transformers. In this paper, we propose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its core, GUI-Actor introduces an attention-based action head that learns to align a dedicated <ACTOR> token with all relevant visual patch tokens, enabling the model to propose one or more action regions in a single forward pass. In line with this, we further design a grounding verifier to evaluate and select the most plausible action region from the candidates proposed for action execution. Extensive experiments show that GUI-Actor outperforms prior state-of-the-art methods on multiple GUI action grounding benchmarks, with improved generalization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B even surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7 with Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by incorporating the verifier, we find that fine-tuning only the newly introduced action head (~100M parameters for 7B model) while keeping the VLM backbone frozen is sufficient to achieve performance comparable to previous state-of-the-art models, highlighting that GUI-Actor can endow the underlying VLM with effective grounding capabilities without compromising its general-purpose strengths.', 'score': 18, 'issue_id': 4112, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '34d1779dffecf9d8', 'authors': ['Qianhui Wu', 'Kanzhi Cheng', 'Rui Yang', 'Chaoyun Zhang', 'Jianwei Yang', 'Huiqiang Jiang', 'Jian Mu', 'Baolin Peng', 'Bo Qiao', 'Reuben Tan', 'Si Qin', 'Lars Liden', 'Qingwei Lin', 'Huan Zhang', 'Tong Zhang', 'Jianbing Zhang', 'Dongmei Zhang', 'Jianfeng Gao'], 'affiliations': ['Microsoft', 'Nanjing University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2506.03143.jpg', 'data': {'categories': ['#multimodal', '#training', '#cv', '#benchmark', '#optimization', '#games'], 'emoji': '🖱️', 'ru': {'title': 'GUI-Actor: Революция в локализации элементов интерфейса с помощью VLM', 'desc': 'GUI-Actor - это метод на основе VLM, использующий механизм внимания для безкоординатной локализации элементов графического интерфейса. Он превосходит существующие методы, демонстрируя лучшую обобщающую способность и эффективное дообучение. GUI-Actor вводит основанную на внимании головную часть для действий, которая учится сопоставлять специальный токен <ACTOR> со всеми релевантными визуальными токенами патчей. Метод также включает верификатор локализации для оценки и выбора наиболее вероятного региона действия из предложенных кандидатов.'}, 'en': {'title': 'Revolutionizing GUI Grounding with Coordinate-Free Attention', 'desc': 'The paper introduces GUI-Actor, a novel method for visual grounding in graphical user interfaces (GUIs) that leverages vision-language models (VLMs) and attention mechanisms. Unlike traditional approaches that generate screen coordinates, GUI-Actor operates in a coordinate-free manner, aligning an action token with relevant visual patches to identify action regions efficiently. This method addresses limitations such as weak spatial-semantic alignment and ambiguity in supervision targets, leading to better generalization across different screen layouts. The results demonstrate that GUI-Actor significantly outperforms existing methods, achieving state-of-the-art performance on various benchmarks while allowing for efficient fine-tuning.'}, 'zh': {'title': 'GUI-Actor：无坐标的高效GUI定位方法', 'desc': '本文提出了一种名为GUI-Actor的基于视觉语言模型（VLM）的方法，用于无坐标的图形用户界面（GUI）定位。该方法通过引入基于注意力的动作头，能够在一次前向传播中将特定的<ACTOR>标记与相关的视觉补丁标记对齐，从而有效地提出一个或多个动作区域。实验结果表明，GUI-Actor在多个GUI动作定位基准上超越了现有的最先进方法，并且在未见过的屏幕分辨率和布局上具有更好的泛化能力。此外，通过引入验证器，GUI-Actor能够在保持VLM主干不变的情况下，仅微调新引入的动作头，便能实现与之前最先进模型相当的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.03065', 'title': 'Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate\n  Video Diffusion Transformers', 'url': 'https://huggingface.co/papers/2506.03065', 'abstract': 'Sparse-vDiT accelerates Video Diffusion Transformer (vDiT) by leveraging sparsity patterns in attention maps, reducing theoretical FLOPs and improving inference speed without significant loss in visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t While Diffusion Transformers (DiTs) have achieved breakthroughs in video generation, this long sequence generation task remains constrained by the quadratic complexity of attention mechanisms, resulting in significant inference latency. Through detailed analysis of attention maps in Video Diffusion Transformer (vDiT), we identify three recurring sparsity patterns: diagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\% attention heads can be skipped. Crucially, these patterns exhibit strong layer-depth and head-position correlations but show limited dependence on the input content. Leveraging these findings, we propose Sparse-vDiT, a sparsity acceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels that replace dense attention with computationally efficient implementations for each identified sparsity pattern. 2) An offline sparse diffusion search algorithm that selects the optimal sparse computation strategy per layer and head via hardware-aware cost modeling. After determining the optimal configuration, we fuse heads within the same layer that share the same attention strategy, enhancing inference efficiency. Integrated into state-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1), Sparse-vDiT achieves 2.09times, 2.38times, and 1.67times theoretical FLOP reduction, and actual inference speedups of 1.76times, 1.85times, and 1.58times, respectively, while maintaining high visual fidelity, with PSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent structural sparsity in vDiTs can be systematically exploited for long video synthesis.', 'score': 18, 'issue_id': 4114, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'c51b4ca89c790b8c', 'authors': ['Pengtao Chen', 'Xianfang Zeng', 'Maosen Zhao', 'Peng Ye', 'Mingzhu Shen', 'Wei Cheng', 'Gang Yu', 'Tao Chen'], 'affiliations': ['Fudan University', 'Imperial College London', 'StepFun', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.03065.jpg', 'data': {'categories': ['#optimization', '#architecture', '#video', '#inference', '#diffusion'], 'emoji': '🎞️', 'ru': {'title': 'Ускорение генерации видео с помощью разреженных трансформеров', 'desc': 'Исследователи предложили метод Sparse-vDiT для ускорения Video Diffusion Transformer (vDiT), используя разреженные паттерны в картах внимания. Они выявили три повторяющихся паттерна разреженности: диагональный, мульти-диагональный и вертикально-полосатый. Sparse-vDiT включает оптимизированные разреженные ядра для каждого паттерна и алгоритм поиска оптимальной стратегии разреженных вычислений. Интеграция Sparse-vDiT в современные модели vDiT позволила значительно сократить количество операций с плавающей запятой и ускорить вывод без существенной потери качества изображения.'}, 'en': {'title': 'Accelerating Video Generation with Sparse Attention Patterns', 'desc': 'The paper introduces Sparse-vDiT, a framework designed to enhance the efficiency of Video Diffusion Transformers (vDiT) by utilizing identified sparsity patterns in attention maps. By analyzing these patterns, the authors found that certain attention heads can be skipped, leading to reduced computational complexity and faster inference times. Sparse-vDiT employs pattern-optimized sparse kernels and an offline search algorithm to determine the best sparse computation strategy for each layer and head. This approach results in significant reductions in theoretical FLOPs and actual inference speedups while preserving high visual quality in generated videos.'}, 'zh': {'title': '利用稀疏性加速视频生成的创新之路', 'desc': 'Sparse-vDiT通过利用注意力图中的稀疏模式，加速了视频扩散变换器（vDiT），在不显著降低视觉质量的情况下，减少了理论计算量（FLOPs）并提高了推理速度。研究发现，vDiT中的注意力图存在对角线、多对角线和垂直条纹等三种稀疏模式，并且可以跳过3-6%的注意力头。我们提出的Sparse-vDiT框架包括优化稀疏内核和离线稀疏扩散搜索算法，以选择每层和每个头的最佳稀疏计算策略。通过将Sparse-vDiT集成到最先进的vDiT模型中，取得了显著的计算效率提升，同时保持了高视觉保真度。'}}}, {'id': 'https://huggingface.co/papers/2505.23061', 'title': 'DINGO: Constrained Inference for Diffusion LLMs', 'url': 'https://huggingface.co/papers/2505.23061', 'abstract': "DINGO, a dynamic programming-based decoding strategy, enhances diffusion language models by enforcing structured output constraints, significantly improving performance on symbolic math and JSON generation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion LLMs have emerged as a promising alternative to conventional autoregressive LLMs, offering significant potential for improved runtime efficiency. However, existing diffusion models lack the ability to provably enforce user-specified formal constraints, such as regular expressions, which makes them unreliable for tasks that require structured outputs, such as fixed-schema JSON generation. Unlike autoregressive models that generate tokens sequentially, diffusion LLMs predict a block of tokens in parallel. This parallelism makes traditional constrained decoding algorithms, which are designed for sequential token prediction, ineffective at preserving the true output distribution. To address this limitation, we propose DINGO, a dynamic programming-based constrained decoding strategy that is both efficient and provably distribution-preserving. DINGO enables sampling of output strings with the highest probability under the model's predicted distribution, while strictly satisfying any user-specified regular expression. On standard symbolic math and JSON generation benchmarks, DINGO achieves up to a 68 percentage point improvement over unconstrained inference", 'score': 16, 'issue_id': 4112, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': 'c215c7998d0a7928', 'authors': ['Tarun Suresh', 'Debangshu Banerjee', 'Shubham Ugare', 'Sasa Misailovic', 'Gagandeep Singh'], 'affiliations': ['Department of Computer Science, University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.23061.jpg', 'data': {'categories': ['#training', '#architecture', '#benchmark', '#optimization', '#diffusion'], 'emoji': '🧮', 'ru': {'title': 'DINGO: Структурированное декодирование для диффузионных языковых моделей', 'desc': 'Статья представляет DINGO - новую стратегию декодирования для диффузионных языковых моделей. DINGO использует динамическое программирование для обеспечения структурированных ограничений вывода, что значительно улучшает производительность в задачах генерации символьной математики и JSON. В отличие от авторегрессионных моделей, диффузионные модели предсказывают блок токенов параллельно, что делает традиционные алгоритмы декодирования с ограничениями неэффективными. DINGO позволяет выбирать выходные строки с наивысшей вероятностью, строго соблюдая заданные регулярные выражения.'}, 'en': {'title': 'DINGO: Structuring Success in Diffusion Language Models', 'desc': 'DINGO is a new decoding strategy that improves diffusion language models by applying structured output constraints. This method allows the models to generate outputs that meet specific requirements, like those found in symbolic math and JSON formats. Unlike traditional autoregressive models that generate text one token at a time, DINGO uses dynamic programming to efficiently handle multiple tokens simultaneously while ensuring the output adheres to user-defined rules. As a result, DINGO significantly enhances the performance of diffusion models, achieving up to a 68 percentage point improvement in relevant tasks.'}, 'zh': {'title': 'DINGO：提升扩散模型的结构化输出能力', 'desc': 'DINGO是一种基于动态规划的解码策略，旨在增强扩散语言模型的性能。它通过强制执行结构化输出约束，显著提高了在符号数学和JSON生成任务上的表现。与传统的自回归模型不同，扩散模型能够并行预测一组标记，这使得传统的约束解码算法无法有效应用。DINGO通过高效且可证明的分布保持方法，确保生成的输出字符串符合用户指定的正则表达式，从而在标准基准测试中实现了显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2506.03136', 'title': 'Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.03136', 'abstract': "CURE, a reinforcement learning framework, improves code and unit test generation accuracy without ground-truth supervision, enhancing performance in various coding and testing tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose CURE, a novel reinforcement learning framework with a dedicated reward design that co-evolves coding and unit test generation capabilities based on their interaction outcomes, without any ground-truth code as supervision. This approach enables flexible and scalable training and allows the unit tester to learn directly from the coder's mistakes. Our derived ReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and Best-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models, outperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They naturally extend to downstream tasks such as test-time scaling and agentic coding-achieving a 8.1% improvement over the base model. For the long-CoT model, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while achieving 64.8% inference efficiency in unit test generation. Notably, we also find that our model can serve as an effective reward model for reinforcement learning on base models. Project: https://github.com/Gen-Verse/CURE", 'score': 14, 'issue_id': 4110, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '6a5362c4ed28a3b0', 'authors': ['Yinjie Wang', 'Ling Yang', 'Ye Tian', 'Ke Shen', 'Mengdi Wang'], 'affiliations': ['ByteDance', 'Peking University', 'Princeton University', 'University of Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2506.03136.jpg', 'data': {'categories': ['#training', '#optimization', '#agents', '#rl', '#games'], 'emoji': '🧠', 'ru': {'title': 'CURE: эволюция кодирования и тестирования без эталонов', 'desc': 'CURE - это новая система обучения с подкреплением для улучшения генерации кода и модульных тестов без использования эталонного кода. Модели ReasonFlux-Coder, созданные с помощью CURE, показывают значительное улучшение точности генерации кода по сравнению с аналогичными моделями. Система позволяет тестировщику учиться непосредственно на ошибках программиста и естественным образом распространяется на смежные задачи. Примечательно, что модель CURE может служить эффективной моделью вознаграждения для обучения с подкреплением базовых моделей.'}, 'en': {'title': 'CURE: Evolving Code and Tests Together for Better Accuracy', 'desc': "CURE is a reinforcement learning framework designed to enhance the accuracy of code and unit test generation without relying on ground-truth supervision. It features a unique reward system that allows coding and testing processes to evolve together based on their interactions. This method enables the unit tester to learn from the coder's errors, leading to improved performance in various coding tasks. The framework's models, such as ReasonFlux-Coder, show significant accuracy improvements over existing models, making it a powerful tool for developers."}, 'zh': {'title': 'CURE：无监督强化学习提升代码生成与测试准确性', 'desc': 'CURE是一个强化学习框架，旨在提高代码和单元测试生成的准确性，而无需真实标签的监督。该框架通过设计专门的奖励机制，使编码和单元测试生成能力能够相互演化，从而直接从编码者的错误中学习。经过优化后，我们的ReasonFlux-Coder模型在代码生成准确性上提高了5.3%，在最佳准确性上提高了9.0%。此外，该模型在单元测试生成中表现出64.8%的推理效率，展现了其在下游任务中的灵活性和可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2506.03126', 'title': 'AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video\n  Generation', 'url': 'https://huggingface.co/papers/2506.03126', 'abstract': 'AnimeShooter, a reference-guided multi-shot animation dataset, enhances coherent animated video generation by incorporating comprehensive hierarchical annotations and visual consistency, and AnimeShooterGen leverages MLLMs and video diffusion models to achieve superior results.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in AI-generated content (AIGC) have significantly accelerated animation production. To produce engaging animations, it is essential to generate coherent multi-shot video clips with narrative scripts and character references. However, existing public datasets primarily focus on real-world scenarios with global descriptions, and lack reference images for consistent character guidance. To bridge this gap, we present AnimeShooter, a reference-guided multi-shot animation dataset. AnimeShooter features comprehensive hierarchical annotations and strong visual consistency across shots through an automated pipeline. Story-level annotations provide an overview of the narrative, including the storyline, key scenes, and main character profiles with reference images, while shot-level annotations decompose the story into consecutive shots, each annotated with scene, characters, and both narrative and descriptive visual captions. Additionally, a dedicated subset, AnimeShooter-audio, offers synchronized audio tracks for each shot, along with audio descriptions and sound sources. To demonstrate the effectiveness of AnimeShooter and establish a baseline for the reference-guided multi-shot video generation task, we introduce AnimeShooterGen, which leverages Multimodal Large Language Models (MLLMs) and video diffusion models. The reference image and previously generated shots are first processed by MLLM to produce representations aware of both reference and context, which are then used as the condition for the diffusion model to decode the subsequent shot. Experimental results show that the model trained on AnimeShooter achieves superior cross-shot visual consistency and adherence to reference visual guidance, which highlight the value of our dataset for coherent animated video generation.', 'score': 14, 'issue_id': 4114, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '3cc5928482bd8262', 'authors': ['Lu Qiu', 'Yizhuo Li', 'Yuying Ge', 'Yixiao Ge', 'Ying Shan', 'Xihui Liu'], 'affiliations': ['ARC Lab, Tencent PCG', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.03126.jpg', 'data': {'categories': ['#video', '#multimodal', '#story_generation', '#diffusion', '#dataset'], 'emoji': '🎬', 'ru': {'title': 'AnimeShooter: новый уровень генерации связной анимации с опорными изображениями', 'desc': 'AnimeShooter - это набор данных для создания многокадровой анимации с использованием опорных изображений. Он включает иерархические аннотации и обеспечивает визуальную согласованность между кадрами. На основе этого датасета разработана модель AnimeShooterGen, использующая мультимодальные языковые модели и видео-диффузию. Эксперименты показали, что AnimeShooterGen превосходит аналоги по согласованности кадров и соответствию опорным изображениям.'}, 'en': {'title': 'Enhancing Animation with Reference-Guided Multi-Shot Datasets', 'desc': 'AnimeShooter is a new dataset designed to improve the generation of coherent multi-shot animations by providing detailed hierarchical annotations and ensuring visual consistency. It includes story-level annotations that outline the narrative and character profiles, as well as shot-level annotations that break down the story into individual scenes with specific details. The dataset also features a subset with synchronized audio tracks to enhance the animation experience. To utilize this dataset, AnimeShooterGen employs Multimodal Large Language Models (MLLMs) and video diffusion models, resulting in improved visual consistency and adherence to character references in generated animations.'}, 'zh': {'title': '提升动画生成的连贯性与一致性', 'desc': 'AnimeShooter是一个参考引导的多镜头动画数据集，旨在提高连贯的动画视频生成。该数据集通过全面的层次注释和视觉一致性，帮助生成具有叙事脚本和角色参考的动画片段。我们还推出了AnimeShooterGen，利用多模态大语言模型（MLLMs）和视频扩散模型，取得了更好的生成效果。实验结果表明，基于AnimeShooter训练的模型在跨镜头视觉一致性和遵循参考视觉指导方面表现优异。'}}}, {'id': 'https://huggingface.co/papers/2505.24714', 'title': 'FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation', 'url': 'https://huggingface.co/papers/2505.24714', 'abstract': 'FinMME is a comprehensive multimodal dataset for financial research and FinScore is an evaluation system that highlights the challenges faced by even advanced models like GPT-4o in the finance domain.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have experienced rapid development in recent years. However, in the financial domain, there is a notable lack of effective and specialized multimodal evaluation datasets. To advance the development of MLLMs in the finance domain, we introduce FinMME, encompassing more than 11,000 high-quality financial research samples across 18 financial domains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We ensure data quality through 20 annotators and carefully designed validation mechanisms. Additionally, we develop FinScore, an evaluation system incorporating hallucination penalties and multi-dimensional capability assessment to provide an unbiased evaluation. Extensive experimental results demonstrate that even state-of-the-art models like GPT-4o exhibit unsatisfactory performance on FinMME, highlighting its challenging nature. The benchmark exhibits high robustness with prediction variations under different prompts remaining below 1%, demonstrating superior reliability compared to existing datasets. Our dataset and evaluation protocol are available at https://huggingface.co/datasets/luojunyu/FinMME and https://github.com/luo-junyu/FinMME.', 'score': 14, 'issue_id': 4110, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'a6dcbb10b5be7f41', 'authors': ['Junyu Luo', 'Zhizhuo Kou', 'Liming Yang', 'Xiao Luo', 'Jinsheng Huang', 'Zhiping Xiao', 'Jingshu Peng', 'Chengzhong Liu', 'Jiaming Ji', 'Xuanzhe Liu', 'Sirui Han', 'Ming Zhang', 'Yike Guo'], 'affiliations': ['HKUST', 'School of Computer Science, Peking University', 'State Key Laboratory for Multimedia Information Processing, PKU-Anker LLM Lab', 'University of California, Los Angeles', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.24714.jpg', 'data': {'categories': ['#dataset', '#hallucinations', '#science', '#multimodal', '#benchmark'], 'emoji': '📊', 'ru': {'title': 'FinMME и FinScore: Новый стандарт для оценки мультимодальных языковых моделей в финансовой сфере', 'desc': 'FinMME - это обширный мультимодальный датасет для финансовых исследований, включающий более 11 000 высококачественных образцов из 18 финансовых областей. Датасет содержит различные типы графиков и диаграмм, а его качество обеспечивается тщательной аннотацией и валидацией. FinScore - это система оценки, учитывающая штрафы за галлюцинации и многомерную оценку возможностей моделей. Эксперименты показали, что даже передовые модели вроде GPT-4o демонстрируют неудовлетворительные результаты на FinMME, что подчеркивает сложность датасета.'}, 'en': {'title': 'FinMME: Elevating Financial AI with Robust Evaluation', 'desc': "The paper introduces FinMME, a new multimodal dataset specifically designed for financial research, which includes over 11,000 samples across various financial domains and asset classes. It addresses the gap in specialized evaluation datasets for Multimodal Large Language Models (MLLMs) in finance. The authors also present FinScore, an evaluation system that assesses model performance while penalizing inaccuracies and measuring multiple capabilities. Experimental results reveal that even advanced models like GPT-4o struggle with the challenges posed by FinMME, underscoring the dataset's robustness and reliability."}, 'zh': {'title': '推动金融领域的多模态研究', 'desc': 'FinMME是一个全面的多模态金融研究数据集，旨在推动金融领域的多模态大语言模型（MLLMs）发展。该数据集包含超过11,000个高质量的金融研究样本，涵盖18个金融领域和6种资产类别，提供10种主要图表类型和21种子类型。为了确保数据质量，研究团队使用了20名注释员和精心设计的验证机制。此外，FinScore评估系统引入了幻觉惩罚和多维能力评估，以提供公正的评估结果，尽管先进模型如GPT-4o在FinMME上表现不佳，显示出其挑战性。'}}}, {'id': 'https://huggingface.co/papers/2506.00070', 'title': 'Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in\n  Robotics', 'url': 'https://huggingface.co/papers/2506.00070', 'abstract': 'Large Vision-Language Models (LVLMs) have recently shown great promise in advancing robotics by combining embodied reasoning with robot control. A common approach involves training on embodied reasoning tasks related to robot control using Supervised Fine-Tuning (SFT). However, SFT datasets are often heuristically constructed and not explicitly optimized for improving robot control. Furthermore, SFT often leads to issues such as catastrophic forgetting and reduced generalization performance. To address these limitations, we introduce Robot-R1, a novel framework that leverages reinforcement learning to enhance embodied reasoning specifically for robot control. Robot-R1 learns to predict the next keypoint state required for task completion, conditioned on the current scene image and environment metadata derived from expert demonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples reasoning-based responses and reinforces those that lead to more accurate predictions. Our experiments show that models trained with Robot-R1 outperform SFT methods on embodied reasoning tasks. Despite having only 7B parameters, Robot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action control, such as spatial and primitive movement reasoning.', 'score': 14, 'issue_id': 4114, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '0a2372063dd0aba7', 'authors': ['Dongyoung Kim', 'Sumin Park', 'Huiwon Jang', 'Jinwoo Shin', 'Jaehyung Kim', 'Younggyo Seo'], 'affiliations': ['KAIST', 'Real World Inc.', 'UC Berkeley', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00070.jpg', 'data': {'categories': ['#optimization', '#robotics', '#training', '#rl', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Robot-R1: Революция в обучении роботов через воплощенные рассуждения', 'desc': 'Статья представляет новый подход Robot-R1 для улучшения воплощенных рассуждений в робототехнике с использованием обучения с подкреплением. В отличие от традиционного метода Supervised Fine-Tuning (SFT), Robot-R1 оптимизирует модель для более точного прогнозирования следующего ключевого состояния, необходимого для выполнения задачи. Эксперименты показывают, что модели, обученные с помощью Robot-R1, превосходят методы SFT в задачах воплощенных рассуждений. Несмотря на то, что модель имеет всего 7 миллиардов параметров, она превосходит GPT-4 в задачах рассуждений, связанных с низкоуровневым управлением действиями.'}, 'en': {'title': 'Reinforcement Learning Revolutionizes Robot Control with Robot-R1', 'desc': 'This paper presents Robot-R1, a new framework that improves robot control by using reinforcement learning instead of traditional Supervised Fine-Tuning (SFT). SFT often suffers from issues like catastrophic forgetting and poorly constructed datasets, which can hinder robot performance. Robot-R1 focuses on predicting the next keypoint state needed for task completion by utilizing current scene images and expert demonstration data. The results show that Robot-R1 outperforms SFT methods, even with fewer parameters, in tasks requiring low-level action control and embodied reasoning.'}, 'zh': {'title': 'Robot-R1：提升机器人控制的具身推理新框架', 'desc': '大型视觉语言模型（LVLMs）在机器人技术中展现出巨大的潜力，通过结合具身推理与机器人控制来推动进步。传统的监督微调（SFT）方法在训练时常常使用启发式构建的数据集，这些数据集并未针对机器人控制进行优化。SFT方法还可能导致灾难性遗忘和泛化性能下降的问题。为了解决这些问题，我们提出了Robot-R1框架，利用强化学习来增强机器人控制的具身推理能力。'}}}, {'id': 'https://huggingface.co/papers/2506.03131', 'title': 'Native-Resolution Image Synthesis', 'url': 'https://huggingface.co/papers/2506.03131', 'abstract': 'A novel generative model, Native-resolution diffusion Transformer (NiT), synthesizes high-resolution and varied aspect ratio images with state-of-the-art performance and zero-shot generalization capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce native-resolution image synthesis, a novel generative modeling paradigm that enables the synthesis of images at arbitrary resolutions and aspect ratios. This approach overcomes the limitations of conventional fixed-resolution, square-image methods by natively handling variable-length visual tokens, a core challenge for traditional techniques. To this end, we introduce the Native-resolution diffusion Transformer (NiT), an architecture designed to explicitly model varying resolutions and aspect ratios within its denoising process. Free from the constraints of fixed formats, NiT learns intrinsic visual distributions from images spanning a broad range of resolutions and aspect ratios. Notably, a single NiT model simultaneously achieves the state-of-the-art performance on both ImageNet-256x256 and 512x512 benchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in advanced large language models, NiT, trained solely on ImageNet, demonstrates excellent zero-shot generalization performance. It successfully generates high-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536) and diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These findings indicate the significant potential of native-resolution modeling as a bridge between visual generative modeling and advanced LLM methodologies.', 'score': 13, 'issue_id': 4111, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '91e9b714c6e83924', 'authors': ['Zidong Wang', 'Lei Bai', 'Xiangyu Yue', 'Wanli Ouyang', 'Yiyuan Zhang'], 'affiliations': ['MMLab, CUHK', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2506.03131.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#cv', '#synthetic', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'NiT: Революция в генерации изображений без ограничений разрешения', 'desc': 'Представлена новая генеративная модель NiT (Native-resolution diffusion Transformer), способная синтезировать изображения высокого разрешения с различными соотношениями сторон. Модель преодолевает ограничения традиционных методов с фиксированным разрешением, обрабатывая визуальные токены переменной длины. NiT демонстрирует передовую производительность на бенчмарках ImageNet и способность к обобщению zero-shot на ранее невиданные разрешения и соотношения сторон. Эти результаты указывают на потенциал нативного моделирования разрешений как моста между визуальными генеративными моделями и продвинутыми методологиями больших языковых моделей.'}, 'en': {'title': 'Revolutionizing Image Synthesis with Native-Resolution Modeling', 'desc': 'The paper presents a new generative model called the Native-resolution diffusion Transformer (NiT) that can create high-resolution images with various aspect ratios. Unlike traditional models that work with fixed-size images, NiT can handle images of any size by using variable-length visual tokens. This model not only achieves top performance on standard image benchmarks but also shows impressive zero-shot generalization, meaning it can generate high-quality images at new resolutions without additional training. The findings suggest that NiT could significantly advance the field of image synthesis by integrating techniques from both visual generative modeling and large language models.'}, 'zh': {'title': '原生分辨率建模：图像生成的新纪元', 'desc': '本文介绍了一种新颖的生成模型——原生分辨率扩散变换器（NiT），它能够合成高分辨率和多种宽高比的图像，表现出卓越的性能和零样本泛化能力。该方法克服了传统固定分辨率图像方法的局限，通过原生处理可变长度的视觉标记，解决了传统技术的核心挑战。NiT架构专门设计用于在去噪过程中显式建模不同的分辨率和宽高比，能够从各种分辨率和宽高比的图像中学习内在的视觉分布。研究表明，NiT在多个基准测试中表现出色，能够生成高保真度的图像，展示了原生分辨率建模在视觉生成建模和先进大语言模型方法之间的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.02497', 'title': 'LumosFlow: Motion-Guided Long Video Generation', 'url': 'https://huggingface.co/papers/2506.02497', 'abstract': 'LumosFlow uses LMTV-DM for key frame generation and LOF-DM followed by MotionControlNet for smooth intermediate frame interpolation, ensuring temporally coherent long video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Long video generation has gained increasing attention due to its widespread applications in fields such as entertainment and simulation. Despite advances, synthesizing temporally coherent and visually compelling long sequences remains a formidable challenge. Conventional approaches often synthesize long videos by sequentially generating and concatenating short clips, or generating key frames and then interpolate the intermediate frames in a hierarchical manner. However, both of them still remain significant challenges, leading to issues such as temporal repetition or unnatural transitions. In this paper, we revisit the hierarchical long video generation pipeline and introduce LumosFlow, a framework introduce motion guidance explicitly. Specifically, we first employ the Large Motion Text-to-Video Diffusion Model (LMTV-DM) to generate key frames with larger motion intervals, thereby ensuring content diversity in the generated long videos. Given the complexity of interpolating contextual transitions between key frames, we further decompose the intermediate frame interpolation into motion generation and post-hoc refinement. For each pair of key frames, the Latent Optical Flow Diffusion Model (LOF-DM) synthesizes complex and large-motion optical flows, while MotionControlNet subsequently refines the warped results to enhance quality and guide intermediate frame generation. Compared with traditional video frame interpolation, we achieve 15x interpolation, ensuring reasonable and continuous motion between adjacent frames. Experiments show that our method can generate long videos with consistent motion and appearance. Code and models will be made publicly available upon acceptance. Our project page: https://jiahaochen1.github.io/LumosFlow/', 'score': 12, 'issue_id': 4111, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'c7f37e23b510618b', 'authors': ['Jiahao Chen', 'Hangjie Yuan', 'Yichen Qian', 'Jingyun Liang', 'Jiazheng Xing', 'Pengwei Liu', 'Weihua Chen', 'Fan Wang', 'Bing Su'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Hupan Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02497.jpg', 'data': {'categories': ['#open_source', '#video', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'LumosFlow: плавная генерация длинных видео с помощью иерархической интерполяции кадров', 'desc': 'LumosFlow - это новый подход к генерации длинных видео с использованием иерархического пайплайна. Система применяет LMTV-DM для создания ключевых кадров с большими интервалами движения, обеспечивая разнообразие контента. Затем LOF-DM синтезирует сложные оптические потоки между ключевыми кадрами, а MotionControlNet улучшает качество промежуточных кадров. LumosFlow позволяет достичь 15-кратной интерполяции, обеспечивая плавное и непрерывное движение между кадрами.'}, 'en': {'title': 'LumosFlow: Smooth Long Video Generation with Motion Guidance', 'desc': 'LumosFlow is a novel framework designed for generating long videos with smooth transitions and coherent motion. It utilizes the Large Motion Text-to-Video Diffusion Model (LMTV-DM) to create key frames that capture significant motion, enhancing the diversity of the video content. For the interpolation of frames between these key frames, it employs the Latent Optical Flow Diffusion Model (LOF-DM) to generate complex motion flows, followed by MotionControlNet for refining the results. This approach significantly improves the quality of long video generation, achieving 15 times faster interpolation while maintaining consistent motion and appearance throughout the video.'}, 'zh': {'title': 'LumosFlow：高效生成连贯长视频的创新框架', 'desc': 'LumosFlow 是一个用于长视频生成的框架，采用 LMTV-DM 生成关键帧，并使用 LOF-DM 和 MotionControlNet 进行平滑的中间帧插值。该方法通过显式引入运动指导，解决了传统方法在生成长视频时面临的时间一致性和视觉连贯性问题。LumosFlow 通过生成具有较大运动间隔的关键帧，确保了生成视频内容的多样性。实验表明，该方法能够生成具有一致运动和外观的长视频，且插值速度比传统方法快 15 倍。'}}}, {'id': 'https://huggingface.co/papers/2506.02528', 'title': 'RelationAdapter: Learning and Transferring Visual Relation with\n  Diffusion Transformers', 'url': 'https://huggingface.co/papers/2506.02528', 'abstract': "RelationAdapter, a lightweight module, enhances Diffusion Transformer models to capture and apply visual transformations effectively using source-target image pairs, improving editing performance and generalization on diverse tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Inspired by the in-context learning mechanism of large language models (LLMs), a new paradigm of generalizable visual prompt-based image editing is emerging. Existing single-reference methods typically focus on style or appearance adjustments and struggle with non-rigid transformations. To address these limitations, we propose leveraging source-target image pairs to extract and transfer content-aware editing intent to novel query images. To this end, we introduce RelationAdapter, a lightweight module that enables Diffusion Transformer (DiT) based models to effectively capture and apply visual transformations from minimal examples. We also introduce Relation252K, a comprehensive dataset comprising 218 diverse editing tasks, to evaluate model generalization and adaptability in visual prompt-driven scenarios. Experiments on Relation252K show that RelationAdapter significantly improves the model's ability to understand and transfer editing intent, leading to notable gains in generation quality and overall editing performance.", 'score': 11, 'issue_id': 4111, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'e8011f9f9decd752', 'authors': ['Yan Gong', 'Yiren Song', 'Yicheng Li', 'Chenglin Li', 'Yin Zhang'], 'affiliations': ['National University of Singapore', 'Zhe Jiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02528.jpg', 'data': {'categories': ['#cv', '#dataset', '#transfer_learning', '#diffusion'], 'emoji': '🖼️', 'ru': {'title': 'RelationAdapter: Умное редактирование изображений по паре примеров', 'desc': 'RelationAdapter - это легковесный модуль, улучшающий работу моделей Diffusion Transformer для захвата и применения визуальных преобразований с использованием пар изображений источник-цель. Он вдохновлен механизмом обучения в контексте больших языковых моделей (LLM) и направлен на обобщаемое редактирование изображений на основе визуальных подсказок. RelationAdapter позволяет эффективно понимать и переносить намерения редактирования, значительно улучшая качество генерации и общую производительность редактирования. Для оценки обобщающей способности и адаптивности модели авторы также представили набор данных Relation252K, содержащий 218 разнообразных задач редактирования.'}, 'en': {'title': 'Enhancing Image Editing with RelationAdapter', 'desc': "The paper introduces RelationAdapter, a new lightweight module designed to enhance Diffusion Transformer models for image editing tasks. It focuses on using source-target image pairs to effectively capture and apply visual transformations, which improves the model's performance on diverse editing tasks. By leveraging this approach, RelationAdapter addresses the limitations of existing methods that struggle with non-rigid transformations and generalization. The authors also present Relation252K, a dataset that includes 218 diverse editing tasks to evaluate the model's adaptability and effectiveness in visual prompt-driven scenarios."}, 'zh': {'title': '提升图像编辑性能的轻量级模块', 'desc': 'RelationAdapter 是一个轻量级模块，旨在增强扩散变换器模型的能力，以有效捕捉和应用视觉变换。它通过源-目标图像对来提取和转移内容感知的编辑意图，从而改善编辑性能和在多样任务上的泛化能力。与现有的单参考方法不同，RelationAdapter 能够处理非刚性变换，提升图像编辑的灵活性。我们还引入了 Relation252K 数据集，以评估模型在视觉提示驱动场景中的泛化和适应能力。'}}}, {'id': 'https://huggingface.co/papers/2506.00910', 'title': 'PCoreSet: Effective Active Learning through Knowledge Distillation from\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2506.00910', 'abstract': 'ActiveKD integrates active learning with knowledge distillation using large vision-language models to efficiently select diverse, unlabeled samples for annotation.  \t\t\t\t\tAI-generated summary \t\t\t\t Knowledge distillation (KD) is a widely used framework for training compact, task-specific models by leveraging the knowledge of teacher models. However, its application to active learning (AL), which aims to minimize annotation costs through iterative sample selection, remains underexplored. This gap stems from the fact that KD typically assumes access to sufficient labeled data, whereas AL operates in data-scarce scenarios where task-specific teacher models are often unavailable. In this paper, we introduce ActiveKD, a framework that integrates AL with KD by leveraging the zero- and few-shot capabilities of large vision-language models (VLMs). A key aspect of ActiveKD is the structured prediction bias of VLMs -- i.e., their predictions form clusters in the probability space. We regard this structure as an inductive bias of the teacher model, capturing generalizable output patterns beneficial to student learning. To exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection strategy that maximizes coverage in the probability space rather than the feature space. PCoreSet strategically selects categorically diverse unlabeled samples, facilitating more efficient transfer of teacher knowledge under limited annotation budgets. Evaluations on 11 datasets show that PCoreSet consistently outperforms existing selection methods within the ActiveKD framework, advancing research at the intersection of AL and KD.', 'score': 10, 'issue_id': 4113, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '375d26a9868ef2fe', 'authors': ['Seongjae Kang', 'Dong Bok Lee', 'Hyungjoon Jang', 'Dongseop Kim', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST', 'VUNO Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2506.00910.jpg', 'data': {'categories': ['#data', '#optimization', '#training', '#dataset', '#transfer_learning'], 'emoji': '🧠', 'ru': {'title': 'ActiveKD: Эффективное обучение с ограниченными данными через синергию активного обучения и дистилляции знаний', 'desc': 'ActiveKD - это новый подход, объединяющий активное обучение и дистилляцию знаний с использованием крупных визуально-языковых моделей для эффективного выбора разнообразных неразмеченных образцов для аннотации. Ключевым аспектом ActiveKD является структурированное смещение предсказаний визуально-языковых моделей, которое рассматривается как индуктивное смещение учительской модели. Для использования этого смещения предлагается стратегия Probabilistic CoreSet (PCoreSet), максимизирующая покрытие в пространстве вероятностей. Оценки на 11 наборах данных показывают, что PCoreSet стабильно превосходит существующие методы выбора в рамках ActiveKD.'}, 'en': {'title': 'Efficient Sample Selection through ActiveKD: Merging Active Learning and Knowledge Distillation', 'desc': 'ActiveKD is a novel framework that combines active learning (AL) with knowledge distillation (KD) to enhance the selection of diverse, unlabeled samples for annotation. It addresses the challenge of limited labeled data by utilizing large vision-language models (VLMs) that can perform well even with few examples. The framework introduces a selection strategy called Probabilistic CoreSet (PCoreSet), which focuses on maximizing coverage in the probability space, allowing for more effective knowledge transfer from teacher models to student models. Evaluations demonstrate that ActiveKD, through PCoreSet, significantly improves sample selection efficiency compared to traditional methods.'}, 'zh': {'title': '主动学习与知识蒸馏的高效结合', 'desc': 'ActiveKD是一个将主动学习与知识蒸馏相结合的框架，旨在高效选择多样化的未标注样本进行标注。知识蒸馏通常需要足够的标注数据，而主动学习则在数据稀缺的情况下工作，因此两者的结合尚未得到充分探索。ActiveKD利用大型视觉-语言模型的零样本和少样本能力，通过概率空间中的结构化预测偏差来优化样本选择。我们提出的概率核心集（PCoreSet）策略能够在有限的标注预算下，选择具有类别多样性的未标注样本，从而更有效地传递教师模型的知识。'}}}, {'id': 'https://huggingface.co/papers/2506.01144', 'title': 'FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video\n  Generation', 'url': 'https://huggingface.co/papers/2506.01144', 'abstract': "FlowMo, a training-free method, enhances motion coherence in pre-trained text-to-video diffusion models by leveraging their own predictions to reduce patch-wise temporal variance.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-video diffusion models are notoriously limited in their ability to model temporal aspects such as motion, physics, and dynamic interactions. Existing approaches address this limitation by retraining the model or introducing external conditioning signals to enforce temporal consistency. In this work, we explore whether a meaningful temporal representation can be extracted directly from the predictions of a pre-trained model without any additional training or auxiliary inputs. We introduce FlowMo, a novel training-free guidance method that enhances motion coherence using only the model's own predictions in each diffusion step. FlowMo first derives an appearance-debiased temporal representation by measuring the distance between latents corresponding to consecutive frames. This highlights the implicit temporal structure predicted by the model. It then estimates motion coherence by measuring the patch-wise variance across the temporal dimension and guides the model to reduce this variance dynamically during sampling. Extensive experiments across multiple text-to-video models demonstrate that FlowMo significantly improves motion coherence without sacrificing visual quality or prompt alignment, offering an effective plug-and-play solution for enhancing the temporal fidelity of pre-trained video diffusion models.", 'score': 7, 'issue_id': 4113, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '7bd7aa77c6143afb', 'authors': ['Ariel Shaulov', 'Itay Hazan', 'Lior Wolf', 'Hila Chefer'], 'affiliations': ['School of Computer Science Tel Aviv University, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2506.01144.jpg', 'data': {'categories': ['#video', '#training', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Улучшение движения в видео без переобучения модели', 'desc': 'FlowMo - это метод без дополнительного обучения, который улучшает согласованность движения в предобученных диффузионных моделях для генерации видео по тексту. Он использует собственные предсказания модели для уменьшения покадровой временной вариативности. FlowMo извлекает временное представление, свободное от влияния внешнего вида, измеряя расстояние между латентными представлениями последовательных кадров. Затем метод оценивает согласованность движения, измеряя вариативность по временной оси, и направляет модель на уменьшение этой вариативности во время семплирования.'}, 'en': {'title': 'Enhancing Motion Coherence in Video Generation Without Training', 'desc': "FlowMo is a novel method that improves the motion coherence of pre-trained text-to-video diffusion models without requiring any additional training. It works by using the model's own predictions to create a temporal representation that captures the dynamics of motion across frames. By measuring the variance in motion across patches, FlowMo guides the model to reduce inconsistencies during the video generation process. This approach enhances the temporal fidelity of the generated videos while maintaining high visual quality and alignment with input prompts."}, 'zh': {'title': 'FlowMo：提升视频生成运动一致性的新方法', 'desc': 'FlowMo是一种无训练的方法，旨在提高预训练文本到视频扩散模型中的运动一致性。它通过利用模型自身的预测，减少了时间维度上的补丁方差，从而增强了运动的连贯性。与传统方法不同，FlowMo不需要重新训练模型或引入外部条件信号，而是直接从模型的预测中提取有意义的时间表示。实验结果表明，FlowMo在多个文本到视频模型中显著提高了运动一致性，同时保持了视觉质量和提示对齐，提供了一种有效的即插即用解决方案。'}}}, {'id': 'https://huggingface.co/papers/2506.03123', 'title': 'DCM: Dual-Expert Consistency Model for Efficient and High-Quality Video\n  Generation', 'url': 'https://huggingface.co/papers/2506.03123', 'abstract': 'Diffusion Models have achieved remarkable results in video synthesis but require iterative denoising steps, leading to substantial computational overhead. Consistency Models have made significant progress in accelerating diffusion models. However, directly applying them to video diffusion models often results in severe degradation of temporal consistency and appearance details. In this paper, by analyzing the training dynamics of Consistency Models, we identify a key conflicting learning dynamics during the distillation process: there is a significant discrepancy in the optimization gradients and loss contributions across different timesteps. This discrepancy prevents the distilled student model from achieving an optimal state, leading to compromised temporal consistency and degraded appearance details. To address this issue, we propose a parameter-efficient Dual-Expert Consistency Model~(DCM), where a semantic expert focuses on learning semantic layout and motion, while a detail expert specializes in fine detail refinement. Furthermore, we introduce Temporal Coherence Loss to improve motion consistency for the semantic expert and apply GAN and Feature Matching Loss to enhance the synthesis quality of the detail expert.Our approach achieves state-of-the-art visual quality with significantly reduced sampling steps, demonstrating the effectiveness of expert specialization in video diffusion model distillation. Our code and models are available at https://github.com/Vchitect/DCM{https://github.com/Vchitect/DCM}.', 'score': 6, 'issue_id': 4122, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'b284c16ff31a205f', 'authors': ['Zhengyao Lv', 'Chenyang Si', 'Tianlin Pan', 'Zhaoxi Chen', 'Kwan-Yee K. Wong', 'Yu Qiao', 'Ziwei Liu'], 'affiliations': ['Nanjing University', 'S-Lab, Nanyang Technological University', 'Shanghai Artificial Intelligence Laboratory', 'The University of Hong Kong', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2506.03123.jpg', 'data': {'categories': ['#training', '#video', '#multimodal', '#diffusion', '#optimization', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'Эффективный синтез видео с помощью двух экспертов', 'desc': 'Авторы статьи предлагают новый подход к ускорению диффузионных моделей для синтеза видео. Они выявили проблему конфликтующей динамики обучения при дистилляции моделей согласованности, что приводит к ухудшению временной согласованности и деталей изображения. Для решения этой проблемы предложена модель Dual-Expert Consistency Model (DCM) с двумя экспертами: семантическим и детализирующим. Также введены новые функции потерь для улучшения согласованности движения и качества синтеза.'}, 'en': {'title': 'Expert Specialization for Enhanced Video Synthesis', 'desc': 'This paper addresses the challenges of using Diffusion Models for video synthesis, particularly the high computational cost due to iterative denoising steps. It highlights the limitations of applying Consistency Models directly to video diffusion, which can lead to poor temporal consistency and loss of detail. The authors propose a Dual-Expert Consistency Model (DCM) that separates the learning tasks into two experts: one for semantic understanding and motion, and another for fine detail refinement. By introducing a Temporal Coherence Loss and utilizing GAN and Feature Matching Loss, the model achieves high visual quality with fewer sampling steps, showcasing the benefits of expert specialization in improving video synthesis.'}, 'zh': {'title': '专家专注，提升视频合成质量', 'desc': '扩散模型在视频合成中取得了显著成果，但需要多次去噪步骤，导致计算开销大。一致性模型在加速扩散模型方面取得了重要进展，但直接应用于视频扩散模型时，往往会导致时间一致性和外观细节的严重退化。本文通过分析一致性模型的训练动态，识别出在蒸馏过程中存在的关键冲突学习动态，导致蒸馏学生模型无法达到最佳状态。为了解决这个问题，我们提出了一种参数高效的双专家一致性模型（DCM），通过语义专家和细节专家的专业化来提高视频扩散模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.01789', 'title': "Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and\n  Accountability", 'url': 'https://huggingface.co/papers/2506.01789', 'abstract': 'High-quality datasets are fundamental to training and evaluating machine learning models, yet their creation-especially with accurate human annotations-remains a significant challenge. Many dataset paper submissions lack originality, diversity, or rigorous quality control, and these shortcomings are often overlooked during peer review. Submissions also frequently omit essential details about dataset construction and properties. While existing tools such as datasheets aim to promote transparency, they are largely descriptive and do not provide standardized, measurable methods for evaluating data quality. Similarly, metadata requirements at conferences promote accountability but are inconsistently enforced. To address these limitations, this position paper advocates for the integration of systematic, rubric-based evaluation metrics into the dataset review process-particularly as submission volumes continue to grow. We also explore scalable, cost-effective methods for synthetic data generation, including dedicated tools and LLM-as-a-judge approaches, to support more efficient evaluation. As a call to action, we introduce DataRubrics, a structured framework for assessing the quality of both human- and model-generated datasets. Leveraging recent advances in LLM-based evaluation, DataRubrics offers a reproducible, scalable, and actionable solution for dataset quality assessment, enabling both authors and reviewers to uphold higher standards in data-centric research. We also release code to support reproducibility of LLM-based evaluations at https://github.com/datarubrics/datarubrics.', 'score': 6, 'issue_id': 4111, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '08a1fd6c4eff1198', 'authors': ['Genta Indra Winata', 'David Anugraha', 'Emmy Liu', 'Alham Fikri Aji', 'Shou-Yi Hung', 'Aditya Parashar', 'Patrick Amadeus Irawan', 'Ruochen Zhang', 'Zheng-Xin Yong', 'Jan Christian Blaise Cruz', 'Niklas Muennighoff', 'Seungone Kim', 'Hanyang Zhao', 'Sudipta Kar', 'Kezia Erina Suryoraharjo', 'M. Farid Adilazuarda', 'En-Shiun Annie Lee', 'Ayu Purwarianti', 'Derry Tanti Wijaya', 'Monojit Choudhury'], 'affiliations': ['Brown University', 'Capital One', 'Carnegie Mellon University', 'Columbia University', 'ITB', 'MBZUAI', 'Monash University', 'Ontario Tech University', 'Oracle', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01789.jpg', 'data': {'categories': ['#open_source', '#data', '#synthetic', '#dataset'], 'emoji': '📊', 'ru': {'title': 'DataRubrics: Новый стандарт качества датасетов в эпоху ИИ', 'desc': 'Эта статья предлагает новый подход к оценке качества наборов данных для машинного обучения. Авторы вводят концепцию DataRubrics - структурированную систему для оценки качества как человеческих, так и сгенерированных моделями датасетов. DataRubrics использует последние достижения в оценке с помощью языковых моделей (LLM) для обеспечения воспроизводимого, масштабируемого и действенного решения. Статья также рассматривает методы синтетической генерации данных и призывает к интеграции систематических метрик оценки в процесс рецензирования датасетов.'}, 'en': {'title': 'Enhancing Dataset Quality with DataRubrics', 'desc': 'This paper discusses the importance of high-quality datasets in machine learning and the challenges in creating them, particularly regarding human annotations. It highlights issues with current dataset submissions, such as lack of originality and inadequate quality control, which are often missed during peer review. The authors propose a new framework called DataRubrics, which introduces systematic, rubric-based evaluation metrics to improve dataset quality assessment. Additionally, they explore methods for generating synthetic data and emphasize the need for reproducibility in evaluations using recent advancements in large language models (LLMs).'}, 'zh': {'title': '提升数据集质量的系统化评估框架', 'desc': '高质量的数据集对于训练和评估机器学习模型至关重要，但创建这些数据集尤其是准确的人类标注仍然是一个重大挑战。许多数据集论文缺乏原创性、多样性或严格的质量控制，且这些问题在同行评审中常常被忽视。本文提倡在数据集审查过程中整合系统化的评分标准，以提高数据质量评估的标准化和可测量性。我们还介绍了DataRubrics，一个用于评估人类和模型生成数据集质量的结构化框架，旨在提升数据驱动研究的标准。'}}}, {'id': 'https://huggingface.co/papers/2506.01274', 'title': 'ReFoCUS: Reinforcement-guided Frame Optimization for Contextual\n  Understanding', 'url': 'https://huggingface.co/papers/2506.01274', 'abstract': "ReFoCUS uses reinforcement learning to optimize frame selection for video-LLM, enhancing reasoning performance in video QA by aligning with model preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in Large Multi-modal Models (LMMs) has enabled effective vision-language reasoning, yet the ability to understand video content remains constrained by suboptimal frame selection strategies. Existing approaches often rely on static heuristics or external retrieval modules to feed frame information into video-LLMs, which may fail to provide the query-relevant information. In this work, we introduce ReFoCUS (Reinforcement-guided Frame Optimization for Contextual UnderStanding), a novel frame-level policy optimization framework that shifts the optimization target from textual responses to visual input selection. ReFoCUS learns a frame selection policy via reinforcement learning, using reward signals derived from a reference LMM to reflect the model's intrinsic preferences for frames that best support temporally grounded responses. To efficiently explore the large combinatorial frame space, we employ an autoregressive, conditional selection architecture that ensures temporal coherence while reducing complexity. Our approach does not require explicit supervision at the frame-level and consistently improves reasoning performance across multiple video QA benchmarks, highlighting the benefits of aligning frame selection with model-internal utility.", 'score': 4, 'issue_id': 4115, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '401bd39fc17a47b7', 'authors': ['Hosu Lee', 'Junho Kim', 'Hyunjun Kim', 'Yong Man Ro'], 'affiliations': ['Integrated Vision and Language Lab, KAIST, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2506.01274.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#video', '#optimization', '#rl', '#benchmark'], 'emoji': '🎞️', 'ru': {'title': 'Умный выбор кадров для лучшего понимания видео ИИ', 'desc': 'ReFoCUS - это новый подход к оптимизации выбора кадров для видео-LLM с использованием обучения с подкреплением. Он улучшает способность модели рассуждать о видеоконтенте, выбирая кадры в соответствии с предпочтениями самой модели. ReFoCUS использует автореггрессивную архитектуру для эффективного исследования пространства кадров и не требует явной разметки на уровне кадров. Этот метод последовательно улучшает производительность в задачах видео-вопросов и ответов на нескольких бенчмарках.'}, 'en': {'title': 'Optimizing Frame Selection for Better Video Reasoning', 'desc': "ReFoCUS is a novel framework that uses reinforcement learning to improve how video-LLMs select frames for video question answering (QA). Instead of relying on fixed rules or external systems, it learns which frames are most relevant to the questions being asked. By optimizing frame selection based on the model's preferences, ReFoCUS enhances the reasoning capabilities of video-LLMs. This method not only simplifies the selection process but also boosts performance on various video QA tasks without needing detailed supervision for each frame."}, 'zh': {'title': '优化视频帧选择，提升推理能力', 'desc': 'ReFoCUS是一种利用强化学习优化视频-大语言模型（video-LLM）帧选择的方法，旨在提高视频问答中的推理性能。该方法通过学习帧选择策略，关注视觉输入的选择，而不是仅仅依赖文本响应。ReFoCUS使用来自参考大多模态模型的奖励信号，反映模型对支持时间相关响应的最佳帧的偏好。通过自回归条件选择架构，ReFoCUS有效探索帧空间，同时保持时间一致性，显著提升了多个视频问答基准的推理性能。'}}}, {'id': 'https://huggingface.co/papers/2506.03096', 'title': 'FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens', 'url': 'https://huggingface.co/papers/2506.03096', 'abstract': 'Contrastive language-image pre-training aligns the features of text-image pairs in a common latent space via distinct encoders for each modality. While this approach achieves impressive performance in several zero-shot tasks, it cannot natively handle multimodal inputs, i.e., encoding image and text into a single feature vector. As a remedy, it is common practice to use additional modules to merge the features extracted by the unimodal encoders. In this work, we present FuseLIP, an alternative architecture for multimodal embedding. Leveraging recent progress in discrete image tokenizers, we propose to use a single transformer model which operates on an extended vocabulary of text and image tokens. This early fusion approach allows the different modalities to interact at each depth of encoding and obtain richer representations compared to common late fusion. We collect new datasets for multimodal pre-training and evaluation, designing challenging tasks for multimodal encoder models. We show that FuseLIP outperforms other approaches in multimodal embedding tasks such as VQA and text-guided image transformation retrieval, while being comparable to baselines on unimodal tasks.', 'score': 3, 'issue_id': 4118, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '6a94488665c463be', 'authors': ['Christian Schlarmann', 'Francesco Croce', 'Nicolas Flammarion', 'Matthias Hein'], 'affiliations': ['Tübingen AI Center University of Tübingen'], 'pdf_title_img': 'assets/pdf/title_img/2506.03096.jpg', 'data': {'categories': ['#architecture', '#dataset', '#games', '#alignment', '#multimodal'], 'emoji': '🔀', 'ru': {'title': 'Ранняя фьюжн для мультимодального ИИ', 'desc': 'FuseLIP - это новая архитектура для мультимодального встраивания, использующая единую трансформерную модель для обработки текстовых и изображенческих токенов. В отличие от традиционных подходов с поздним слиянием, FuseLIP позволяет модальностям взаимодействовать на каждом уровне кодирования. Авторы собрали новые наборы данных для предобучения и оценки модели. FuseLIP превосходит другие подходы в задачах мультимодального встраивания, таких как визуальные вопросы и ответы, сохраняя сопоставимую производительность в одномодальных задачах.'}, 'en': {'title': 'FuseLIP: Unifying Text and Image with Early Fusion for Better Multimodal Understanding', 'desc': 'This paper introduces FuseLIP, a novel architecture for multimodal embedding that combines text and image features using a single transformer model. Unlike traditional methods that merge features from separate encoders, FuseLIP employs an early fusion strategy, allowing text and image tokens to interact throughout the encoding process. This results in richer representations and improved performance on multimodal tasks like visual question answering (VQA) and text-guided image retrieval. The authors also present new datasets for training and evaluating multimodal models, demonstrating that FuseLIP outperforms existing methods in multimodal scenarios while maintaining competitive results in unimodal tasks.'}, 'zh': {'title': 'FuseLIP：多模态嵌入的新方法', 'desc': '这篇论文介绍了一种新的多模态嵌入架构FuseLIP，旨在改进文本和图像的特征对齐。与传统的后期融合方法不同，FuseLIP采用早期融合策略，通过单一的变换器模型处理扩展的文本和图像标记词汇。这样可以在编码的每个深度上实现不同模态的交互，从而获得更丰富的表示。实验结果表明，FuseLIP在多模态嵌入任务中表现优于其他方法，同时在单模态任务上与基线模型相当。'}}}, {'id': 'https://huggingface.co/papers/2506.03079', 'title': 'ORV: 4D Occupancy-centric Robot Video Generation', 'url': 'https://huggingface.co/papers/2506.03079', 'abstract': 'ORV, an Occupancy-centric Robot Video generation framework, uses 4D semantic occupancy sequences to produce photorealistic, temporally consistent, and precisely controllable robot videos, enhancing existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Acquiring real-world robotic simulation data through teleoperation is notoriously time-consuming and labor-intensive. Recently, action-driven generative models have gained widespread adoption in robot learning and simulation, as they eliminate safety concerns and reduce maintenance efforts. However, the action sequences used in these methods often result in limited control precision and poor generalization due to their globally coarse alignment. To address these limitations, we propose ORV, an Occupancy-centric Robot Video generation framework, which utilizes 4D semantic occupancy sequences as a fine-grained representation to provide more accurate semantic and geometric guidance for video generation. By leveraging occupancy-based representations, ORV enables seamless translation of simulation data into photorealistic robot videos, while ensuring high temporal consistency and precise controllability. Furthermore, our framework supports the simultaneous generation of multi-view videos of robot gripping operations - an important capability for downstream robotic learning tasks. Extensive experimental results demonstrate that ORV consistently outperforms existing baseline methods across various datasets and sub-tasks. Demo, Code and Model: https://orangesodahub.github.io/ORV', 'score': 3, 'issue_id': 4115, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '2bfb7d794c03a7ed', 'authors': ['Xiuyu Yang', 'Bohan Li', 'Shaocong Xu', 'Nan Wang', 'Chongjie Ye', 'Zhaoxi Chen', 'Minghan Qin', 'Yikang Ding', 'Xin Jin', 'Hang Zhao', 'Hao Zhao'], 'affiliations': ['AIR, Tsinghua University', 'Beijing Academy of Artificial Intelligence', 'ByteDance', 'Eastern Institute of Technology, Ningbo', 'IIIS, Tsinghua University', 'Megvii Technology', 'National University of Singapore', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2506.03079.jpg', 'data': {'categories': ['#games', '#robotics', '#optimization', '#video'], 'emoji': '🤖', 'ru': {'title': 'ORV: Точный контроль роботов через 4D семантическую занятость', 'desc': 'ORV - это фреймворк для генерации видео с роботами, использующий последовательности 4D семантической занятости. Он позволяет создавать фотореалистичные, согласованные во времени и точно контролируемые видео роботов. ORV улучшает существующие методы, обеспечивая более точное семантическое и геометрическое руководство для генерации видео. Фреймворк поддерживает одновременную генерацию многоракурсных видео операций захвата роботом, что важно для задач обучения роботов.'}, 'en': {'title': 'ORV: Revolutionizing Robot Video Generation with 4D Occupancy Sequences', 'desc': 'The paper introduces ORV, a framework designed for generating robot videos using 4D semantic occupancy sequences. This approach enhances the quality of video generation by providing detailed semantic and geometric information, leading to photorealistic and temporally consistent outputs. ORV addresses the limitations of previous action-driven generative models, which often struggle with control precision and generalization. The framework also allows for the generation of multi-view videos, which is beneficial for various robotic learning applications, demonstrating superior performance compared to existing methods.'}, 'zh': {'title': '以占用为中心的机器人视频生成新框架', 'desc': 'ORV是一个以占用为中心的机器人视频生成框架，利用4D语义占用序列生成逼真的、时间一致的、可精确控制的机器人视频。该方法通过细粒度的占用表示，提供更准确的语义和几何指导，从而克服了现有方法的控制精度和泛化能力不足的问题。ORV能够无缝地将仿真数据转化为高质量的机器人视频，并支持同时生成多视角的视频，适用于后续的机器人学习任务。实验结果表明，ORV在多个数据集和子任务中均优于现有的基线方法。'}}}, {'id': 'https://huggingface.co/papers/2506.02454', 'title': 'Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports\n  From Scratch with Agentic Framework', 'url': 'https://huggingface.co/papers/2506.02454', 'abstract': 'A new framework, Multimodal DeepResearcher, enables Large Language Models to generate high-quality multimodal reports combining text and diverse visualizations through structured textual representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Visualizations play a crucial part in effective communication of concepts and information. Recent advances in reasoning and retrieval augmented generation have enabled Large Language Models (LLMs) to perform deep research and generate comprehensive reports. Despite its progress, existing deep research frameworks primarily focus on generating text-only content, leaving the automated generation of interleaved texts and visualizations underexplored. This novel task poses key challenges in designing informative visualizations and effectively integrating them with text reports. To address these challenges, we propose Formal Description of Visualization (FDV), a structured textual representation of charts that enables LLMs to learn from and generate diverse, high-quality visualizations. Building on this representation, we introduce Multimodal DeepResearcher, an agentic framework that decomposes the task into four stages: (1) researching, (2) exemplar report textualization, (3) planning, and (4) multimodal report generation. For the evaluation of generated multimodal reports, we develop MultimodalReportBench, which contains 100 diverse topics served as inputs along with 5 dedicated metrics. Extensive experiments across models and evaluation methods demonstrate the effectiveness of Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet model, Multimodal DeepResearcher achieves an 82\\% overall win rate over the baseline method.', 'score': 2, 'issue_id': 4111, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '9220f780a7fba411', 'authors': ['Zhaorui Yang', 'Bo Pan', 'Han Wang', 'Yiyao Wang', 'Xingyu Liu', 'Minfeng Zhu', 'Bo Zhang', 'Wei Chen'], 'affiliations': ['State Key Lab of CAD&CG, Zhejiang University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02454.jpg', 'data': {'categories': ['#agents', '#multimodal', '#reasoning', '#optimization', '#games', '#benchmark'], 'emoji': '📊', 'ru': {'title': 'Мультимодальные отчеты: новый уровень генерации контента с помощью ИИ', 'desc': 'Новая система Multimodal DeepResearcher позволяет большим языковым моделям (LLM) создавать качественные мультимодальные отчеты, сочетающие текст и разнообразные визуализации. Система использует структурированное текстовое представление для описания визуализаций, что позволяет LLM эффективно работать с графиками. Процесс генерации разбит на четыре этапа: исследование, создание примера текста отчета, планирование и генерация мультимодального отчета. Эксперименты показали значительное превосходство Multimodal DeepResearcher над базовым методом при использовании одной и той же языковой модели.'}, 'en': {'title': 'Revolutionizing Reports: Text Meets Visualization with Multimodal DeepResearcher', 'desc': 'The paper introduces Multimodal DeepResearcher, a framework that allows Large Language Models (LLMs) to create detailed reports that combine text and various visualizations. It addresses the challenge of integrating informative visualizations with text, which has been largely overlooked in previous research. The framework uses a structured representation called Formal Description of Visualization (FDV) to help LLMs generate high-quality visual content. Through a systematic approach involving research, report creation, planning, and multimodal generation, the framework shows significant improvements in report quality compared to existing methods.'}, 'zh': {'title': '多模态深度研究者：文本与可视化的完美结合', 'desc': '本论文提出了一种新的框架，称为多模态深度研究者（Multimodal DeepResearcher），旨在使大型语言模型（LLMs）能够生成高质量的多模态报告，这些报告结合了文本和多种可视化形式。该框架通过结构化的文本表示，解决了文本与可视化内容交织生成的挑战，提升了信息传达的有效性。我们引入了可视化的正式描述（FDV），使得LLMs能够学习并生成多样化的高质量可视化图表。通过四个阶段的任务分解，该框架展示了其在生成多模态报告方面的有效性，实验结果表明其在性能上优于基线方法。'}}}, {'id': 'https://huggingface.co/papers/2506.02338', 'title': 'One Missing Piece for Open-Source Reasoning Models: A Dataset to\n  Mitigate Cold-Starting Short CoT LLMs in RL', 'url': 'https://huggingface.co/papers/2506.02338', 'abstract': "The Long CoT Collection dataset, generated by short CoT LLMs, enhances general reasoning skills and provides a strong foundation for reinforcement learning, achieving quality comparable to R1.  \t\t\t\t\tAI-generated summary \t\t\t\t With the release of R1, a publicly available large reasoning model (LRM), researchers commonly train new LRMs by training language models on R1's long chain-of-thought (CoT) inferences. While prior works show that LRMs' capabilities can be reproduced through direct distillation, the continued reliance on the existing models (e.g., R1) remains a critical limitation in advancing the field. As a first step toward independent LRM development, this paper explores the possibility of constructing a long CoT dataset with LLMs that are not trained for inference-time scaling. To this end, we present the Long CoT Collection, a dataset of 100K CoT rationales annotated using existing short CoT LLMs. We develop a pipeline that induces o1's novel reasoning strategies into short CoT LLMs, enabling them to think longer and introducing controllability over the thought budget to better manage the overthinking problem. Our extensive analyses validate that our dataset achieves quality comparable to--or slightly below--R1. Furthermore, our experiments demonstrate that training on our dataset not only strengthens general reasoning skills, but also provides a strong foundation for reinforcement learning--models initialized on our data achieve 2-3x larger gains with RLVR.", 'score': 2, 'issue_id': 4111, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'c96bff52eb2a678f', 'authors': ['Hyungjoo Chae', 'Dongjin Kang', 'Jihyuk Kim', 'Beong-woo Kwak', 'Sunghyun Park', 'Haeju Park', 'Jinyoung Yeo', 'Moontae Lee', 'Kyungjae Lee'], 'affiliations': ['LG AI Research', 'University of Illinois Chicago', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02338.jpg', 'data': {'categories': ['#rl', '#transfer_learning', '#dataset', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Длинные цепочки мышления для ИИ: новый подход к обучению моделей рассуждениям', 'desc': 'Статья представляет набор данных Long CoT Collection, созданный с помощью коротких моделей цепочек рассуждений (CoT LLM). Этот датасет содержит 100 тысяч примеров рассуждений и позволяет обучать модели длинным цепочкам мышления. Авторы разработали конвейер, который улучшает стратегии рассуждений коротких моделей и позволяет контролировать объем мыслительного процесса. Эксперименты показывают, что обучение на этом наборе данных улучшает навыки рассуждений и создает хорошую основу для обучения с подкреплением.'}, 'en': {'title': 'Empowering Reasoning with Long CoT Datasets', 'desc': 'The Long CoT Collection dataset is designed to improve reasoning skills in language models by using short chain-of-thought (CoT) models to generate long CoT inferences. This dataset consists of 100,000 annotated rationales that help models think more deeply and manage their reasoning process better. By training on this dataset, models can achieve reasoning quality similar to the established R1 model while also enhancing their performance in reinforcement learning tasks. The research highlights the potential for developing independent large reasoning models without relying solely on existing ones.'}, 'zh': {'title': '提升推理能力的长链思维数据集', 'desc': '本文介绍了一个名为Long CoT Collection的数据集，该数据集由短链思维（CoT）的大型语言模型（LLM）生成，旨在提升一般推理能力。研究表明，通过使用该数据集进行训练，可以获得与现有大型推理模型（如R1）相当的推理质量。我们开发了一种新方法，使短链思维模型能够进行更长时间的推理，并引入了对思维预算的可控性，以解决过度思考的问题。实验结果显示，基于该数据集训练的模型在强化学习任务中表现出显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2506.00413', 'title': 'Accelerating Diffusion LLMs via Adaptive Parallel Decoding', 'url': 'https://huggingface.co/papers/2506.00413', 'abstract': 'Adaptive parallel decoding (APD) enhances the throughput of diffusion large language models (dLLMs) by dynamically adjusting parallel token generation without significantly diminishing quality.  \t\t\t\t\tAI-generated summary \t\t\t\t The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks.', 'score': 2, 'issue_id': 4113, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '42d7cf22876b9eef', 'authors': ['Daniel Israel', 'Guy Van den Broeck', 'Aditya Grover'], 'affiliations': ['Department of Computer Science University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2506.00413.jpg', 'data': {'categories': ['#optimization', '#training', '#benchmark', '#diffusion', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'Ускорение языковых моделей без потери качества', 'desc': 'Статья представляет новый метод адаптивного параллельного декодирования (APD) для диффузионных больших языковых моделей (dLLM). APD динамически регулирует количество токенов, генерируемых параллельно, что позволяет значительно увеличить пропускную способность без существенного ухудшения качества. Метод использует мультипликативную смесь маргинальных вероятностей dLLM и совместной вероятности последовательностей под небольшой вспомогательной авторегрессионной моделью. APD оптимизирован с помощью KV-кэширования и ограничения размера маскированного входа, что обеспечивает гибкий компромисс между пропускной способностью и качеством.'}, 'en': {'title': 'Boosting Speed in Language Models with Adaptive Parallel Decoding', 'desc': 'Adaptive parallel decoding (APD) is a new technique that improves the speed of diffusion large language models (dLLMs) by allowing multiple tokens to be generated at the same time. Traditional methods use autoregressive decoding, which predicts tokens one after another, slowing down the process. APD changes this by dynamically adjusting how many tokens are generated in parallel, balancing speed and quality. By combining probabilities from both dLLMs and a smaller autoregressive model, APD achieves higher throughput with only slight reductions in output quality.'}, 'zh': {'title': '自适应并行解码：提升生成速度与质量的平衡', 'desc': '自适应并行解码（APD）通过动态调整并行生成的标记数量，提升了扩散大型语言模型（dLLMs）的吞吐量，而不会显著降低质量。传统的自回归解码方法使得生成速度受到限制，因为标记是一个接一个地预测的。尽管理论上扩散大型语言模型允许并行生成标记，但在实际应用中，往往难以在不牺牲质量的情况下达到自回归模型的速度。我们的方法通过定义dLLM边际概率与小型自回归模型下序列的联合概率之间的乘法混合，来实现这一目标，并通过启用KV缓存和限制掩码输入的大小进一步优化APD。'}}}, {'id': 'https://huggingface.co/papers/2506.00391', 'title': 'SHARE: An SLM-based Hierarchical Action CorREction Assistant for\n  Text-to-SQL', 'url': 'https://huggingface.co/papers/2506.00391', 'abstract': 'SHARE, an SLM-based Hierarchical Action RECorrection system, enhances LLMs in text-to-SQL by transforming SQL queries into action trajectories and employing a granular refinement process, improving error detection and correction efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Current self-correction approaches in text-to-SQL face two critical limitations: 1) Conventional self-correction methods rely on recursive self-calls of LLMs, resulting in multiplicative computational overhead, and 2) LLMs struggle to implement effective error detection and correction for declarative SQL queries, as they fail to demonstrate the underlying reasoning path. In this work, we propose SHARE, an SLM-based Hierarchical Action corREction assistant that enables LLMs to perform more precise error localization and efficient correction. SHARE orchestrates three specialized Small Language Models (SLMs) in a sequential pipeline, where it first transforms declarative SQL queries into stepwise action trajectories that reveal underlying reasoning, followed by a two-phase granular refinement. We further propose a novel hierarchical self-evolution strategy for data-efficient training. Experimental results demonstrate that SHARE effectively enhances self-correction capabilities while proving robust across various LLMs. Furthermore, our comprehensive analysis shows that SHARE maintains strong performance even in low-resource training settings, which is particularly valuable for text-to-SQL applications with data privacy constraints.', 'score': 2, 'issue_id': 4122, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '2e0ac7f48f4b8959', 'authors': ['Ge Qu', 'Jinyang Li', 'Bowen Qin', 'Xiaolong Li', 'Nan Huo', 'Chenhao Ma', 'Reynold Cheng'], 'affiliations': ['BAAI', 'The Chinese University of Hong Kong, Shenzhen', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.00391.jpg', 'data': {'categories': ['#low_resource', '#training', '#optimization', '#dataset', '#reasoning', '#small_models', '#data'], 'emoji': '🔍', 'ru': {'title': 'Точная коррекция SQL с помощью иерархического анализа действий', 'desc': 'SHARE - это система коррекции действий на основе SLM, которая улучшает работу больших языковых моделей в задаче преобразования текста в SQL. Она трансформирует SQL-запросы в траектории действий и применяет процесс детальной доработки. SHARE использует три специализированные малые языковые модели в последовательном конвейере для более точной локализации ошибок и эффективного исправления. Система также предлагает новую стратегию иерархической самоэволюции для эффективного обучения с ограниченными данными.'}, 'en': {'title': 'SHARE: Enhancing SQL Error Correction with Hierarchical Action Trajectories', 'desc': 'The paper introduces SHARE, a system designed to improve the self-correction capabilities of large language models (LLMs) in text-to-SQL tasks. It addresses the inefficiencies of traditional self-correction methods that rely heavily on recursive calls, which can be computationally expensive. SHARE utilizes a hierarchical approach with three specialized small language models (SLMs) to convert SQL queries into action trajectories, enhancing error detection and correction. The system also incorporates a novel training strategy that allows it to perform well even with limited data, making it suitable for applications where data privacy is a concern.'}, 'zh': {'title': '提升文本到SQL的自我纠正能力', 'desc': 'SHARE是一种基于小型语言模型（SLM）的层次化动作纠正系统，旨在提升大语言模型（LLM）在文本到SQL转换中的表现。它通过将SQL查询转化为逐步的动作轨迹，帮助LLM更好地理解和定位错误。该系统采用了两阶段的细化过程，提高了错误检测和纠正的效率。此外，SHARE还引入了一种新颖的层次自我进化策略，能够在数据资源有限的情况下进行有效训练，特别适用于有数据隐私限制的文本到SQL应用。'}}}, {'id': 'https://huggingface.co/papers/2505.24273', 'title': 'How Much Backtracking is Enough? Exploring the Interplay of SFT and RL\n  in Enhancing LLM Reasoning', 'url': 'https://huggingface.co/papers/2505.24273', 'abstract': 'This study investigates the interplay between supervised fine-tuning and reinforcement learning in large language models, focusing on the role of backtracking in enhancing reasoning capabilities across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent breakthroughs in large language models (LLMs) have effectively improved their reasoning abilities, particularly on mathematical and logical problems that have verifiable answers, through techniques such as supervised finetuning (SFT) and reinforcement learning (RL). Prior research indicates that RL effectively internalizes search strategies, enabling long chain-of-thought (CoT) reasoning, with backtracking emerging naturally as a learned capability. However, the precise benefits of backtracking, specifically, how significantly it contributes to reasoning improvements and the optimal extent of its use, remain poorly understood. In this work, we systematically investigate the dynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc 1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and Self Reference. Our findings highlight that short CoT sequences used in SFT as a warm-up do have moderate contribution to RL training, compared with cold-start RL; however such contribution diminishes when tasks become increasingly difficult. Motivated by this observation, we construct synthetic datasets varying systematically in the number of backtracking steps and conduct controlled experiments to isolate the influence of either the correctness (content) or the structure (i.e., backtrack frequency). We find that (1) longer CoT with backtracks generally induce better and more stable RL training, (2) more challenging problems with larger search space tend to need higher numbers of backtracks during the SFT stage. Additionally, we demonstrate through experiments on distilled data that RL training is largely unaffected by the correctness of long CoT sequences, suggesting that RL prioritizes structural patterns over content correctness. Collectively, our results offer practical insights into designing optimal training strategies to effectively scale reasoning in LLMs.', 'score': 2, 'issue_id': 4118, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '73012fc9edd07bd3', 'authors': ['Hongyi James Cai', 'Junlin Wang', 'Xiaoyin Chen', 'Bhuwan Dhingra'], 'affiliations': ['Duke University', 'Mila - Quebec AI Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.24273.jpg', 'data': {'categories': ['#training', '#rl', '#synthetic', '#reasoning', '#transfer_learning'], 'emoji': '🧠', 'ru': {'title': 'Возврат в рассуждениях: ключ к улучшению LLM', 'desc': 'Исследование изучает взаимодействие между обучением с учителем и обучением с подкреплением в больших языковых моделях (LLM), фокусируясь на роли возврата (backtracking) в улучшении способностей к рассуждению. Авторы проводят эксперименты на восьми задачах рассуждения, включая Судоку и геометрические головоломки. Результаты показывают, что более длинные цепочки рассуждений с возвратами обычно приводят к лучшему и более стабильному обучению с подкреплением. Исследование также демонстрирует, что обучение с подкреплением в основном не зависит от правильности длинных последовательностей рассуждений, а скорее от их структуры.'}, 'en': {'title': 'Enhancing Reasoning in LLMs through Backtracking and Training Synergy', 'desc': 'This paper explores how supervised fine-tuning (SFT) and reinforcement learning (RL) work together to improve reasoning in large language models (LLMs). It specifically examines the role of backtracking, a technique that allows models to revisit previous steps in their reasoning process, and how it enhances performance on various reasoning tasks. The study finds that while short chain-of-thought (CoT) sequences help in RL training, their effectiveness decreases with task difficulty. Ultimately, the research provides valuable insights into optimizing training strategies for better reasoning capabilities in LLMs.'}, 'zh': {'title': '优化推理能力的训练策略', 'desc': '本研究探讨了监督微调与强化学习在大型语言模型中的相互作用，重点关注回溯在增强推理能力中的作用。研究表明，监督微调（SFT）和强化学习（RL）可以有效提升模型在数学和逻辑问题上的推理能力。我们发现，短的链式思维（CoT）序列在SFT阶段对RL训练有一定贡献，但在任务难度增加时，这种贡献会减弱。通过实验，我们证实了回溯的频率和结构对推理训练的重要性，提供了优化训练策略的实用见解。'}}}, {'id': 'https://huggingface.co/papers/2505.16994', 'title': 'R^2ec: Towards Large Recommender Models with Reasoning', 'url': 'https://huggingface.co/papers/2505.16994', 'abstract': 'A unified large recommender model with intrinsic reasoning capabilities is proposed, facilitating interleaved reasoning and recommendation using a reinforcement learning framework called RecPO.  \t\t\t\t\tAI-generated summary \t\t\t\t Large recommender models have extended LLMs as powerful recommenders via encoding or item generation, and recent breakthroughs in LLM reasoning synchronously motivate the exploration of reasoning in recommendation. Current studies usually position LLMs as external reasoning modules to yield auxiliary thought for augmenting conventional recommendation pipelines. However, such decoupled designs are limited in significant resource cost and suboptimal joint optimization. To address these issues, we propose \\name, a unified large recommender model with intrinsic reasoning capabilities. Initially, we reconceptualize the model architecture to facilitate interleaved reasoning and recommendation in the autoregressive process. Subsequently, we propose RecPO, a corresponding reinforcement learning framework that optimizes \\name\\ both the reasoning and recommendation capabilities simultaneously in a single policy update; RecPO introduces a fused reward scheme that solely leverages recommendation labels to simulate the reasoning capability, eliminating dependency on specialized reasoning annotations. Experiments on three datasets with various baselines verify the effectiveness of \\name, showing relative improvements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at https://github.com/YRYangang/RRec.', 'score': 2, 'issue_id': 4111, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '6b76c655943245ca', 'authors': ['Runyang You', 'Yongqi Li', 'Xinyu Lin', 'Xin Zhang', 'Wenjie Wang', 'Wenjie Li', 'Liqiang Nie'], 'affiliations': ['Harbin Institute of Technology (Shenzhen)', 'National University of Singapore', 'The Hong Kong Polytechnic University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.16994.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#architecture', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Единая модель для рассуждений и рекомендаций', 'desc': 'Предложена унифицированная крупная рекомендательная модель с внутренними возможностями рассуждений. Модель использует обучение с подкреплением в рамках фреймворка RecPO для одновременной оптимизации способностей к рассуждению и рекомендациям. Архитектура модели переосмыслена для обеспечения чередующихся рассуждений и рекомендаций в авторегрессивном процессе. Эксперименты на трех наборах данных показали значительное улучшение показателей Hit@5 и NDCG@20 по сравнению с базовыми методами.'}, 'en': {'title': 'Unified Reasoning and Recommendation for Enhanced Performance', 'desc': "This paper introduces a new large recommender model that integrates reasoning capabilities directly into the recommendation process. The model, named RecPO, uses a reinforcement learning framework to optimize both reasoning and recommendation in a unified manner. By allowing these two processes to work together, the model reduces resource costs and improves performance compared to traditional methods that treat them separately. Experiments demonstrate significant improvements in recommendation metrics, showcasing the model's effectiveness in real-world applications."}, 'zh': {'title': '统一推荐模型，推理与推荐的完美结合', 'desc': '本文提出了一种统一的大型推荐模型，具备内在推理能力，能够在推荐过程中实现交错推理与推荐。我们重新构思了模型架构，使其在自回归过程中同时进行推理和推荐。为此，我们引入了RecPO，一个强化学习框架，能够在单次策略更新中同时优化推理和推荐能力。实验结果表明，该模型在多个数据集上相较于基线有显著提升，验证了其有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.03144', 'title': 'MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition\n  Query', 'url': 'https://huggingface.co/papers/2506.03144', 'abstract': "Semantic retrieval is crucial for modern applications yet remains underexplored in current research. Existing datasets are limited to single languages, single images, or singular retrieval conditions, often failing to fully exploit the expressive capacity of visual information as evidenced by maintained performance when images are replaced with captions. However, practical retrieval scenarios frequently involve interleaved multi-condition queries with multiple images. Hence, this paper introduces MERIT, the first multilingual dataset for interleaved multi-condition semantic retrieval, comprising 320,000 queries with 135,000 products in 5 languages, covering 7 distinct product categories. Extensive experiments on MERIT identify existing models's limitation: focusing solely on global semantic information while neglecting specific conditional elements in queries. Consequently, we propose Coral, a novel fine-tuning framework that adapts pre-trained MLLMs by integrating embedding reconstruction to preserve fine-grained conditional elements and contrastive learning to extract comprehensive global semantics. Experiments demonstrate that Coral achieves a 45.9% performance improvement over conventional approaches on MERIT, with strong generalization capabilities validated across 8 established retrieval benchmarks. Collectively, our contributions - a novel dataset, identification of critical limitations in existing approaches, and an innovative fine-tuning framework - establish a foundation for future research in interleaved multi-condition semantic retrieval.", 'score': 1, 'issue_id': 4118, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '5e57a72b2bce8a15', 'authors': ['Wei Chow', 'Yuan Gao', 'Linfeng Li', 'Xian Wang', 'Qi Xu', 'Hang Song', 'Lingdong Kong', 'Ran Zhou', 'Yi Zeng', 'Yidong Cai', 'Botian Jiang', 'Shilin Xu', 'Jiajun Zhang', 'Minghui Qiu', 'Xiangtai Li', 'Tianshu Yang', 'Siliang Tang', 'Juncheng Li'], 'affiliations': ['ByteDance Inc.', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.03144.jpg', 'data': {'categories': ['#training', '#dataset', '#benchmark', '#multilingual', '#transfer_learning', '#open_source'], 'emoji': '🔍', 'ru': {'title': 'Революция в многоязычном семантическом поиске: MERIT и Coral', 'desc': 'Статья представляет MERIT - первый многоязычный набор данных для семантического поиска с несколькими условиями. Авторы выявили ограничения существующих моделей, фокусирующихся только на глобальной семантической информации. Предложена новая структура тонкой настройки Coral, интегрирующая реконструкцию эмбеддингов и контрастное обучение. Эксперименты показывают, что Coral достигает улучшения производительности на 45.9% по сравнению с традиционными подходами на MERIT.'}, 'en': {'title': 'Revolutionizing Semantic Retrieval with MERIT and Coral', 'desc': 'This paper addresses the challenges in semantic retrieval, particularly in multilingual and multi-condition scenarios. It introduces MERIT, a new dataset with 320,000 queries across five languages and seven product categories, which highlights the limitations of current models that overlook specific query conditions. The authors propose Coral, a fine-tuning framework that enhances pre-trained multilingual language models by focusing on both global semantics and fine-grained conditional elements. Experimental results show that Coral significantly outperforms traditional methods, paving the way for improved semantic retrieval in complex environments.'}, 'zh': {'title': '开创多条件语义检索的新纪元', 'desc': '本论文探讨了语义检索在现代应用中的重要性，并指出当前研究的不足之处。现有的数据集通常只限于单一语言或单一图像，未能充分利用视觉信息的表达能力。为此，本文引入了MERIT，这是首个用于交错多条件语义检索的多语言数据集，包含320,000个查询和135,000个产品，覆盖7个不同的产品类别。我们还提出了Coral，一个新的微调框架，通过整合嵌入重建和对比学习，显著提高了模型在MERIT上的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.02510', 'title': 'M^3FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial\n  Meeting Understanding Evaluation Dataset', 'url': 'https://huggingface.co/papers/2506.02510', 'abstract': "A new multilingual, multi-sector, and multi-task benchmark, M³FinMeeting, evaluates large language models' performance in understanding financial meetings across different languages and industries.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent breakthroughs in large language models (LLMs) have led to the development of new benchmarks for evaluating their performance in the financial domain. However, current financial benchmarks often rely on news articles, earnings reports, or announcements, making it challenging to capture the real-world dynamics of financial meetings. To address this gap, we propose a novel benchmark called M^3FinMeeting, which is a multilingual, multi-sector, and multi-task dataset designed for financial meeting understanding. First, M^3FinMeeting supports English, Chinese, and Japanese, enhancing comprehension of financial discussions in diverse linguistic contexts. Second, it encompasses various industry sectors defined by the Global Industry Classification Standard (GICS), ensuring that the benchmark spans a broad range of financial activities. Finally, M^3FinMeeting includes three tasks: summarization, question-answer (QA) pair extraction, and question answering, facilitating a more realistic and comprehensive evaluation of understanding. Experimental results with seven popular LLMs reveal that even the most advanced long-context models have significant room for improvement, demonstrating the effectiveness of M^3FinMeeting as a benchmark for assessing LLMs' financial meeting comprehension skills.", 'score': 1, 'issue_id': 4110, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '903bb57b5cc664ec', 'authors': ['Jie Zhu', 'Junhui Li', 'Yalong Wen', 'Xiandong Li', 'Lifan Guo', 'Feng Chen'], 'affiliations': ['Nanjing University', 'Qwen DianJin Team, Alibaba Cloud Computing', 'School of Computer Science and Technology, Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02510.jpg', 'data': {'categories': ['#dataset', '#machine_translation', '#science', '#multilingual', '#benchmark'], 'emoji': '📊', 'ru': {'title': 'M³FinMeeting: многоязычный бенчмарк для оценки понимания финансовых встреч языковыми моделями', 'desc': 'Статья представляет новый бенчмарк M³FinMeeting для оценки больших языковых моделей в понимании финансовых встреч на разных языках и в разных отраслях. Бенчмарк включает тексты на английском, китайском и японском языках, охватывая различные секторы экономики по классификации GICS. M³FinMeeting содержит три задачи: суммаризацию, извлечение пар вопрос-ответ и вопросно-ответную систему. Эксперименты с семью популярными языковыми моделями показали, что даже самые продвинутые модели имеют значительный потенциал для улучшения в этой области.'}, 'en': {'title': 'M³FinMeeting: Bridging Language Gaps in Financial Comprehension', 'desc': 'The M³FinMeeting benchmark is designed to evaluate large language models (LLMs) in understanding financial meetings across multiple languages and sectors. It addresses the limitations of existing benchmarks that primarily focus on static financial documents like news articles. This new dataset supports English, Chinese, and Japanese, allowing for a broader assessment of multilingual financial discussions. Additionally, it includes tasks such as summarization and question answering, highlighting the need for LLMs to improve their comprehension of dynamic financial interactions.'}, 'zh': {'title': 'M³FinMeeting：金融会议理解的新基准', 'desc': 'M³FinMeeting是一个新的多语言、多行业和多任务的基准，旨在评估大型语言模型在理解金融会议方面的表现。该基准支持英语、中文和日语，增强了对不同语言环境中金融讨论的理解。它涵盖了全球行业分类标准（GICS）定义的多个行业，确保基准能够覆盖广泛的金融活动。通过对七种流行的大型语言模型进行实验，结果显示即使是最先进的长上下文模型在理解能力上仍有很大提升空间，证明了M³FinMeeting作为评估大型语言模型金融会议理解能力的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.02295', 'title': 'QARI-OCR: High-Fidelity Arabic Text Recognition through Multimodal Large\n  Language Model Adaptation', 'url': 'https://huggingface.co/papers/2506.02295', 'abstract': 'Qari-OCR, a series of fine-tuned vision-language models, achieves state-of-the-art performance in Arabic OCR through iterative optimization on specialized datasets, handling diacritics, fonts, layouts, and low-resolution images.  \t\t\t\t\tAI-generated summary \t\t\t\t The inherent complexities of Arabic script; its cursive nature, diacritical marks (tashkeel), and varied typography, pose persistent challenges for Optical Character Recognition (OCR). We present Qari-OCR, a series of vision-language models derived from Qwen2-VL-2B-Instruct, progressively optimized for Arabic through iterative fine-tuning on specialized synthetic datasets. Our leading model, QARI v0.2, establishes a new open-source state-of-the-art with a Word Error Rate (WER) of 0.160, Character Error Rate (CER) of 0.061, and BLEU score of 0.737 on diacritically-rich texts. Qari-OCR demonstrates superior handling of tashkeel, diverse fonts, and document layouts, alongside impressive performance on low-resolution images. Further explorations (QARI v0.3) showcase strong potential for structural document understanding and handwritten text. This work delivers a marked improvement in Arabic OCR accuracy and efficiency, with all models and datasets released to foster further research.', 'score': 1, 'issue_id': 4119, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': 'b031659260f990f9', 'authors': ['Ahmed Wasfy', 'Omer Nacar', 'Abdelakreem Elkhateb', 'Mahmoud Reda', 'Omar Elshehy', 'Adel Ammar', 'Wadii Boulila'], 'affiliations': ['KAND CA Corp.', 'NAMAA', 'Prince Sultan University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02295.jpg', 'data': {'categories': ['#multilingual', '#low_resource', '#dataset', '#synthetic', '#cv', '#open_source', '#optimization'], 'emoji': '📚', 'ru': {'title': 'Прорыв в распознавании арабского текста с помощью глубокого обучения', 'desc': 'Qari-OCR представляет собой серию моделей компьютерного зрения и обработки естественного языка, оптимизированных для оптического распознавания арабского текста. Модели были итеративно дообучены на специализированных синтетических датасетах, что позволило достичь наилучших результатов среди открытых систем в задаче OCR для арабского языка. Qari-OCR успешно справляется со сложностями арабской письменности, включая диакритические знаки, разнообразие шрифтов и макетов документов. Модели демонстрируют высокую эффективность даже при работе с изображениями низкого разрешения.'}, 'en': {'title': 'Revolutionizing Arabic OCR with Qari-OCR', 'desc': 'Qari-OCR is a series of advanced vision-language models specifically designed to improve Optical Character Recognition (OCR) for Arabic text. It addresses the unique challenges of Arabic script, such as its cursive nature and diacritical marks, by using iterative fine-tuning on specialized datasets. The leading model, QARI v0.2, achieves impressive metrics with a Word Error Rate (WER) of 0.160 and a Character Error Rate (CER) of 0.061, setting a new benchmark in the field. This work not only enhances OCR accuracy for Arabic but also supports further research by making all models and datasets publicly available.'}, 'zh': {'title': 'Qari-OCR：阿拉伯语OCR的新突破', 'desc': 'Qari-OCR是一系列经过微调的视觉-语言模型，专门针对阿拉伯语光学字符识别（OCR）进行优化。该模型通过在特定的合成数据集上进行迭代微调，成功处理了阿拉伯语的连写特性、元音符号和多样的字体布局。我们的主要模型QARI v0.2在处理含有元音符号的文本时，达到了0.160的字错误率（WER）和0.061的字符错误率（CER），并在BLEU评分上取得了0.737的优异成绩。Qari-OCR在低分辨率图像的处理上也表现出色，为阿拉伯语OCR的准确性和效率带来了显著提升。'}}}, {'id': 'https://huggingface.co/papers/2506.01716', 'title': 'Self-Challenging Language Model Agents', 'url': 'https://huggingface.co/papers/2506.01716', 'abstract': 'The Self-Challenging framework trains intelligent agents using self-generated tasks defined as Code-as-Task, improving performance in multi-turn tool-use benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models are quickly becoming the foundation for intelligent agents that are capable of using tools. However, training such agents is challenging because it requires human creation and annotation of a diverse set of tasks, tools, and evaluation criteria. In this paper, we propose the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself. The agent first plays the role of challenger and generates a task after interacting with the given tools. The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks. The agent then takes an executor role and trains on those tasks with reinforcement learning using the evaluation feedback as a reward. Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.', 'score': 1, 'issue_id': 4123, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '292019a70795a800', 'authors': ['Yifei Zhou', 'Sergey Levine', 'Jason Weston', 'Xian Li', 'Sainbayar Sukhbaatar'], 'affiliations': ['FAIR at Meta', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2506.01716.jpg', 'data': {'categories': ['#optimization', '#agi', '#training', '#agents', '#rl'], 'emoji': '🤖', 'ru': {'title': 'Самообучение ИИ: агент бросает вызов самому себе', 'desc': 'Статья представляет новый подход к обучению интеллектуальных агентов, называемый Self-Challenging. В этой модели агент сам генерирует задачи в формате Code-as-Task, включающие инструкции, функции верификации и тестовые примеры. Затем агент обучается на этих самостоятельно созданных задачах с помощью обучения с подкреплением. Эксперименты показали, что данный метод значительно улучшает производительность модели Llama-3.1-8B-Instruct в бенчмарках по использованию инструментов, несмотря на использование только самогенерируемых данных.'}, 'en': {'title': 'Empowering Agents Through Self-Generated Challenges', 'desc': "The Self-Challenging framework enhances the training of intelligent agents by allowing them to create their own tasks, known as Code-as-Task. This approach eliminates the need for extensive human-generated tasks, as the agent generates high-quality tasks through its interactions with tools. Each task includes an instruction, a verification function, and defined success and failure cases, which help ensure the tasks are effective for training. The framework utilizes reinforcement learning to improve the agent's performance, achieving significant gains in benchmarks with only self-generated data."}, 'zh': {'title': '自我挑战，智能体训练的新方法', 'desc': '自我挑战框架通过自我生成的任务来训练智能体，这些任务被定义为代码任务，从而提高了在多轮工具使用基准测试中的表现。该框架允许智能体在与工具互动后，首先扮演挑战者角色生成高质量任务。生成的任务包括指令、验证函数以及解决方案和失败案例，确保任务的质量。最终，智能体作为执行者，利用强化学习在这些任务上进行训练，并通过评估反馈作为奖励，取得了显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2505.24362', 'title': 'Knowing Before Saying: LLM Representations Encode Information About\n  Chain-of-Thought Success Before Completion', 'url': 'https://huggingface.co/papers/2505.24362', 'abstract': "We investigate whether the success of a zero-shot Chain-of-Thought (CoT) process can be predicted before completion. We discover that a probing classifier, based on LLM representations, performs well even before a single token is generated, suggesting that crucial information about the reasoning process is already present in the initial steps representations. In contrast, a strong BERT-based baseline, which relies solely on the generated tokens, performs worse, likely because it depends on shallow linguistic cues rather than deeper reasoning dynamics. Surprisingly, using later reasoning steps does not always improve classification. When additional context is unhelpful, earlier representations resemble later ones more, suggesting LLMs encode key information early. This implies reasoning can often stop early without loss. To test this, we conduct early stopping experiments, showing that truncating CoT reasoning still improves performance over not using CoT at all, though a gap remains compared to full reasoning. However, approaches like supervised learning or reinforcement learning designed to shorten CoT chains could leverage our classifier's guidance to identify when early stopping is effective. Our findings provide insights that may support such methods, helping to optimize CoT's efficiency while preserving its benefits.", 'score': 1, 'issue_id': 4118, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '8bbcc31124760dad', 'authors': ['Anum Afzal', 'Florian Matthes', 'Gal Chechik', 'Yftah Ziser'], 'affiliations': ['Bar-Ilan University', 'Nvidia Research', 'Technical University of Munich'], 'pdf_title_img': 'assets/pdf/title_img/2505.24362.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#training', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Раннее предсказание успеха рассуждений в языковых моделях', 'desc': 'Исследователи изучают возможность предсказания успеха процесса рассуждений с нулевым выстрелом (zero-shot Chain-of-Thought) до его завершения. Они обнаружили, что классификатор на основе представлений языковой модели (LLM) показывает хорошие результаты даже до генерации первого токена. Это указывает на то, что ключевая информация о процессе рассуждений уже присутствует в начальных представлениях. Исследование также показало, что раннее прерывание цепочки рассуждений может улучшить производительность по сравнению с отсутствием CoT, хотя и не достигает уровня полного рассуждения.'}, 'en': {'title': 'Unlocking Early Insights in Chain-of-Thought Reasoning', 'desc': 'This paper explores the predictability of success in zero-shot Chain-of-Thought (CoT) reasoning processes before they are fully completed. The authors find that a probing classifier using representations from large language models (LLMs) can effectively predict outcomes even before generating any tokens, indicating that essential reasoning information is present early on. In contrast, a BERT-based model that relies on generated tokens performs poorly, as it focuses on superficial linguistic features rather than deeper reasoning. The study suggests that early stopping in CoT reasoning can still yield better performance than not using CoT at all, and proposes that future methods could utilize their classifier to determine when early stopping is beneficial.'}, 'zh': {'title': '优化推理效率，早期停止也能有效', 'desc': '本研究探讨了零-shot Chain-of-Thought (CoT) 过程的成功是否可以在完成之前进行预测。我们发现，基于大语言模型（LLM）表示的探测分类器在生成第一个标记之前就表现良好，这表明推理过程中的关键信息在初始步骤的表示中已经存在。相比之下，依赖生成标记的强BERT基线表现较差，可能是因为它依赖于浅层语言线索而非更深层的推理动态。我们的实验表明，早期停止推理可以提高性能，尽管与完整推理相比仍有差距，这为优化CoT的效率提供了新的思路。'}}}, {'id': 'https://huggingface.co/papers/2505.18079', 'title': 'Deep Video Discovery: Agentic Search with Tool Use for Long-form Video\n  Understanding', 'url': 'https://huggingface.co/papers/2505.18079', 'abstract': 'The Deep Video Discovery agent uses an autonomous agentic search strategy with large language models to overcome limitations in long-form video understanding, achieving state-of-the-art results on benchmarks like LVBench.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-form video understanding presents significant challenges due to extensive temporal-spatial complexity and the difficulty of question answering under such extended contexts. While Large Language Models (LLMs) have demonstrated considerable advancements in video analysis capabilities and long context handling, they continue to exhibit limitations when processing information-dense hour-long videos. To overcome such limitations, we propose the Deep Video Discovery agent to leverage an agentic search strategy over segmented video clips. Different from previous video agents manually designing a rigid workflow, our approach emphasizes the autonomous nature of agents. By providing a set of search-centric tools on multi-granular video database, our DVD agent leverages the advanced reasoning capability of LLM to plan on its current observation state, strategically selects tools, formulates appropriate parameters for actions, and iteratively refines its internal reasoning in light of the gathered information. We perform comprehensive evaluation on multiple long video understanding benchmarks that demonstrates the advantage of the entire system design. Our DVD agent achieves SOTA performance, significantly surpassing prior works by a large margin on the challenging LVBench dataset. Comprehensive ablation studies and in-depth tool analyses are also provided, yielding insights to further advance intelligent agents tailored for long-form video understanding tasks. The code will be released later.', 'score': 1, 'issue_id': 4117, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '6dbac78671d7d992', 'authors': ['Xiaoyi Zhang', 'Zhaoyang Jia', 'Zongyu Guo', 'Jiahao Li', 'Bin Li', 'Houqiang Li', 'Yan Lu'], 'affiliations': ['Microsoft Research Asia', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.18079.jpg', 'data': {'categories': ['#benchmark', '#video', '#long_context', '#reasoning', '#agents'], 'emoji': '🎥', 'ru': {'title': 'Интеллектуальный агент для глубокого анализа длинных видео', 'desc': 'Статья представляет агента Deep Video Discovery для анализа длинных видео с использованием автономной стратегии поиска на основе больших языковых моделей (LLM). Агент преодолевает ограничения в понимании длинных видео путем сегментации и применения набора инструментов для поиска в многоуровневой видеобазе данных. DVD агент использует продвинутые возможности рассуждения LLM для планирования действий, выбора инструментов и итеративного уточнения своих внутренних рассуждений. Предложенный подход достигает лучших результатов на бенчмарках для понимания длинных видео, значительно превосходя предыдущие работы на наборе данных LVBench.'}, 'en': {'title': 'Revolutionizing Long-Form Video Understanding with Autonomous Agents', 'desc': 'The Deep Video Discovery (DVD) agent introduces an innovative approach to long-form video understanding by utilizing an autonomous agentic search strategy powered by large language models (LLMs). This method addresses the challenges posed by the complexity of temporal and spatial information in lengthy videos, which traditional models struggle to analyze effectively. By segmenting videos and employing a search-centric toolkit, the DVD agent can dynamically plan and refine its reasoning based on real-time observations. The results demonstrate that this system achieves state-of-the-art performance on benchmarks like LVBench, significantly outperforming previous methods.'}, 'zh': {'title': '自主搜索，深度理解长视频', 'desc': '深度视频发现代理使用自主搜索策略，结合大型语言模型，克服了长视频理解中的局限性。该方法通过对分段视频片段进行智能搜索，提升了在复杂时空背景下的问题回答能力。与以往手动设计工作流程的视频代理不同，我们的代理强调自主性，利用多层次视频数据库中的搜索工具进行规划和选择。经过全面评估，我们的代理在长视频理解基准测试中表现出色，显著超越了之前的研究成果。'}}}, {'id': 'https://huggingface.co/papers/2506.02138', 'title': 'Revisiting LRP: Positional Attribution as the Missing Ingredient for\n  Transformer Explainability', 'url': 'https://huggingface.co/papers/2506.02138', 'abstract': 'A specialized LRP method for Transformer explainability considers positional encoding, improving relevance propagation and outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t The development of effective explainability tools for Transformers is a crucial pursuit in deep learning research. One of the most promising approaches in this domain is Layer-wise Relevance Propagation (LRP), which propagates relevance scores backward through the network to the input space by redistributing activation values based on predefined rules. However, existing LRP-based methods for Transformer explainability entirely overlook a critical component of the Transformer architecture: its positional encoding (PE), resulting in violation of the conservation property, and the loss of an important and unique type of relevance, which is also associated with structural and positional features. To address this limitation, we reformulate the input space for Transformer explainability as a set of position-token pairs. This allows us to propose specialized theoretically-grounded LRP rules designed to propagate attributions across various positional encoding methods, including Rotary, Learnable, and Absolute PE. Extensive experiments with both fine-tuned classifiers and zero-shot foundation models, such as LLaMA 3, demonstrate that our method significantly outperforms the state-of-the-art in both vision and NLP explainability tasks. Our code is publicly available.', 'score': 0, 'issue_id': 4115, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': 'c83ce7f2cc033984', 'authors': ['Yarden Bakish', 'Itamar Zimerman', 'Hila Chefer', 'Lior Wolf'], 'affiliations': ['Tel-Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02138.jpg', 'data': {'categories': ['#training', '#interpretability', '#open_source', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Улучшение объяснимости трансформеров с учетом позиционного кодирования', 'desc': 'Статья представляет специализированный метод Layer-wise Relevance Propagation (LRP) для объяснимости моделей-трансформеров. Авторы учитывают позиционное кодирование, что улучшает распространение релевантности и превосходит существующие методы. Новый подход переформулирует входное пространство как набор пар позиция-токен и предлагает теоретически обоснованные правила LRP для различных методов позиционного кодирования. Эксперименты показывают, что метод значительно превосходит современные подходы в задачах объяснимости для компьютерного зрения и обработки естественного языка.'}, 'en': {'title': 'Enhancing Transformer Explainability with Positional Encoding in LRP', 'desc': 'This paper introduces a specialized Layer-wise Relevance Propagation (LRP) method tailored for Transformer models, focusing on the importance of positional encoding in explainability. Traditional LRP methods fail to account for positional encoding, which leads to a loss of critical relevance information tied to the structure and position of tokens. By reformulating the input space into position-token pairs, the authors develop new LRP rules that effectively propagate relevance across different types of positional encodings. The proposed method shows significant improvements over existing techniques in explainability tasks for both vision and natural language processing, validated through extensive experiments.'}, 'zh': {'title': '提升Transformer可解释性的专门LRP方法', 'desc': '本文提出了一种针对Transformer模型的专门化层次相关传播（LRP）方法，考虑了位置编码的影响，从而改善了相关性传播。现有的LRP方法未能充分利用Transformer架构中的位置编码，导致了重要相关性的丢失。我们通过将输入空间重新构建为位置-标记对，提出了理论基础的LRP规则，以适应不同的位置信息编码方法。实验结果表明，我们的方法在视觉和自然语言处理的可解释性任务中显著优于现有技术。'}}}, {'id': 'https://huggingface.co/papers/2506.01565', 'title': 'Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural\n  Understanding and Transcreation', 'url': 'https://huggingface.co/papers/2506.01565', 'abstract': 'Culture is a rich and dynamic domain that evolves across both geography and time. However, existing studies on cultural understanding with vision-language models (VLMs) primarily emphasize geographic diversity, often overlooking the critical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a novel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning ancient Chinese dynasties, serves as a representative cultural heritage that reflects the profound temporal aspects of Chinese culture while remaining highly popular in Chinese contemporary society. Hanfu-Bench comprises two core tasks: cultural visual understanding and cultural image transcreation.The former task examines temporal-cultural feature recognition based on single- or multi-image inputs through multiple-choice visual question answering, while the latter focuses on transforming traditional attire into modern designs through cultural element inheritance and modern context adaptation. Our evaluation shows that closed VLMs perform comparably to non-experts on visual cutural understanding but fall short by 10\\% to human experts, while open VLMs lags further behind non-experts. For the transcreation task, multi-faceted human evaluation indicates that the best-performing model achieves a success rate of only 42\\%. Our benchmark provides an essential testbed, revealing significant challenges in this new direction of temporal cultural understanding and creative adaptation.', 'score': 0, 'issue_id': 4117, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '0251c50d35bd4a50', 'authors': ['Li Zhou', 'Lutong Yu', 'Dongchu Xie', 'Shaohuan Cheng', 'Wenyan Li', 'Haizhou Li'], 'affiliations': ['Chengdu Technological University', 'Shenzhen Research Institute of Big Data', 'The Chinese University of Hong Kong, Shenzhen', 'University of Copenhagen'], 'pdf_title_img': 'assets/pdf/title_img/2506.01565.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#benchmark'], 'emoji': '👘', 'ru': {'title': 'Оценка культурного понимания и креативности ИИ через призму традиционной китайской одежды', 'desc': 'Статья представляет Hanfu-Bench - новый мультимодальный датасет для оценки понимания культурных аспектов моделями компьютерного зрения и обработки естественного языка (VLM). Датасет фокусируется на ханьфу - традиционной китайской одежде, отражающей временные аспекты культуры. Hanfu-Bench включает задачи по визуальному пониманию культуры и культурной транскреации изображений. Результаты показывают, что современные VLM отстают от экспертов-людей в понимании культурных особенностей и креативной адаптации.'}, 'en': {'title': 'Bridging Time and Culture with Hanfu-Bench', 'desc': 'This paper introduces Hanfu-Bench, a new dataset designed to enhance the understanding of cultural evolution over time using vision-language models (VLMs). It focuses on Hanfu, a traditional Chinese garment, to explore both cultural visual understanding and the transformation of traditional attire into modern designs. The study reveals that while closed VLMs perform similarly to non-experts in recognizing cultural features, they still lag behind human experts. Additionally, the transcreation task shows that even the best models struggle to achieve high success rates, highlighting the challenges in temporal cultural understanding and creative adaptation.'}, 'zh': {'title': '探索时间维度的文化理解', 'desc': '本研究提出了Hanfu-Bench，这是一个新颖的多模态数据集，旨在填补视觉语言模型在文化理解中的时间维度缺失。Hanfu作为中国传统服饰，代表了深厚的文化遗产，反映了中国文化的时间特征。数据集包含两个核心任务：文化视觉理解和文化图像再创作，前者通过多选视觉问答评估时间文化特征的识别，后者则关注传统服饰向现代设计的转化。我们的评估显示，封闭式视觉语言模型在视觉文化理解上与非专家相当，但在时间文化理解和创意适应方面仍面临重大挑战。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (5)', '#agi (1)', '#alignment (1)', '#architecture (8)', '#audio', '#benchmark (16)', '#cv (7)', '#data (3)', '#dataset (16)', '#diffusion (9)', '#ethics', '#games (8)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (1)', '#interpretability (1)', '#leakage', '#long_context (1)', '#low_resource (2)', '#machine_translation (1)', '#math (1)', '#multilingual (3)', '#multimodal (13)', '#open_source (7)', '#optimization (17)', '#plp', '#rag', '#reasoning (16)', '#rl (10)', '#rlhf (1)', '#robotics (3)', '#science (3)', '#security', '#small_models (2)', '#story_generation (1)', '#survey', '#synthetic (5)', '#training (18)', '#transfer_learning (5)', '#video (8)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-06-04 15:37',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-04 15:37')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-04 15:37')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    