
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 19 papers. May 9.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">9 Ğ¼Ğ°Ñ</span> | <span id="title-articles-count">19 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-05-08.html">â¬…ï¸ <span id="prev-date">08.05</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-05-12.html">â¡ï¸ <span id="next-date">12.05</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-05.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '9 Ğ¼Ğ°Ñ', 'en': 'May 9', 'zh': '5æœˆ9æ—¥'};
        let feedDateNext = {'ru': '12.05', 'en': '05/12', 'zh': '5æœˆ12æ—¥'};
        let feedDatePrev = {'ru': '08.05', 'en': '05/08', 'zh': '5æœˆ8æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2505.02567', 'title': 'Unified Multimodal Understanding and Generation Models: Advances,\n  Challenges, and Opportunities', 'url': 'https://huggingface.co/papers/2505.02567', 'abstract': "Recent years have seen remarkable progress in both multimodal understanding models and image generation models. Despite their respective successes, these two domains have evolved independently, leading to distinct architectural paradigms: While autoregressive-based architectures have dominated multimodal understanding, diffusion-based models have become the cornerstone of image generation. Recently, there has been growing interest in developing unified frameworks that integrate these tasks. The emergence of GPT-4o's new capabilities exemplifies this trend, highlighting the potential for unification. However, the architectural differences between the two domains pose significant challenges. To provide a clear overview of current efforts toward unification, we present a comprehensive survey aimed at guiding future research. First, we introduce the foundational concepts and recent advancements in multimodal understanding and text-to-image generation models. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. For each category, we analyze the structural designs and innovations introduced by related works. Additionally, we compile datasets and benchmarks tailored for unified models, offering resources for future exploration. Finally, we discuss the key challenges facing this nascent field, including tokenization strategy, cross-modal attention, and data. As this area is still in its early stages, we anticipate rapid advancements and will regularly update this survey. Our goal is to inspire further research and provide a valuable reference for the community. The references associated with this survey are available on GitHub (https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).", 'score': 55, 'issue_id': 3655, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ', 'en': 'May 5', 'zh': '5æœˆ5æ—¥'}, 'hash': '0d49b4c41b7654a0', 'authors': ['Xinjie Zhang', 'Jintao Guo', 'Shanshan Zhao', 'Minghao Fu', 'Lunhao Duan', 'Guo-Hua Wang', 'Qing-Guo Chen', 'Zhao Xu', 'Weihua Luo', 'Kaifu Zhang'], 'affiliations': ['Alibaba Group', 'Hong Kong University of Science and Technology', 'Nanjing University', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.02567.jpg', 'data': {'categories': ['#dataset', '#architecture', '#survey', '#multimodal', '#diffusion', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ¿ÑƒÑ‚ÑŒ Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ˜Ğ˜', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹: Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ.'}, 'en': {'title': 'Bridging the Gap: Unifying Multimodal Understanding and Image Generation', 'desc': 'This paper surveys the integration of multimodal understanding and image generation models, which have traditionally developed separately. It highlights the architectural differences between autoregressive and diffusion-based models, emphasizing the challenges in unifying these approaches. The authors categorize existing unified models into three paradigms: diffusion-based, autoregressive-based, and hybrid methods. They also provide resources such as datasets and benchmarks to support future research in this emerging field.'}, 'zh': {'title': 'ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„æœªæ¥æ¢ç´¢', 'desc': 'è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€ç†è§£æ¨¡å‹å’Œå›¾åƒç”Ÿæˆæ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è¿™ä¸¤ä¸ªé¢†åŸŸçš„å‘å±•ç›¸å¯¹ç‹¬ç«‹ï¼Œå¯¼è‡´äº†ä¸åŒçš„æ¶æ„èŒƒå¼ã€‚è‡ªå›å½’æ¶æ„åœ¨å¤šæ¨¡æ€ç†è§£ä¸­å ä¸»å¯¼åœ°ä½ï¼Œè€Œæ‰©æ•£æ¨¡å‹åˆ™æˆä¸ºå›¾åƒç”Ÿæˆçš„åŸºçŸ³ã€‚æœ¬æ–‡ç»¼è¿°äº†å½“å‰ç»Ÿä¸€æ¡†æ¶çš„åŠªåŠ›ï¼Œä»‹ç»äº†å¤šæ¨¡æ€ç†è§£å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„åŸºç¡€æ¦‚å¿µåŠæœ€æ–°è¿›å±•ï¼Œå¹¶åˆ†æäº†ä¸‰ç§ä¸»è¦çš„ç»Ÿä¸€æ¨¡å‹æ¶æ„ã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†è¿™ä¸€æ–°å…´é¢†åŸŸé¢ä¸´çš„å…³é”®æŒ‘æˆ˜ï¼Œå¹¶æä¾›äº†æœªæ¥ç ”ç©¶çš„å‚è€ƒèµ„æºã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.04588', 'title': 'ZeroSearch: Incentivize the Search Capability of LLMs without Searching', 'url': 'https://huggingface.co/papers/2505.04588', 'abstract': "Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) Uncontrolled Document Quality: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) Prohibitively High API Costs: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce ZeroSearch, a reinforcement learning framework that incentivizes the search capabilities of LLMs without interacting with real search engines. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both relevant and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model's reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module. Remarkably, a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it. Furthermore, it generalizes well across both base and instruction-tuned models of various parameter sizes and is compatible with a wide range of RL algorithms.", 'score': 33, 'issue_id': 3647, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ', 'en': 'May 7', 'zh': '5æœˆ7æ—¥'}, 'hash': '24edc7c3c5e5e23d', 'authors': ['Hao Sun', 'Zile Qiao', 'Jiayan Guo', 'Xuanbo Fan', 'Yingyan Hou', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Yan Zhang'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2505.04588.jpg', 'data': {'categories': ['#rlhf', '#optimization', '#training', '#reasoning', '#rl'], 'emoji': 'ğŸ”', 'ru': {'title': 'ZeroSearch: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ±ĞµĞ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ZeroSearch - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ZeroSearch Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° API. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ZeroSearch ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM, Ğ¿Ñ€Ğ¸Ñ‡ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ 14 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹.'}, 'en': {'title': 'ZeroSearch: Enhancing LLM Search Without Real Engines', 'desc': 'This paper presents ZeroSearch, a novel reinforcement learning framework designed to enhance the search capabilities of large language models (LLMs) without relying on real search engines. It addresses two significant challenges: the unpredictable quality of documents from search engines and the high costs associated with frequent API calls during RL training. ZeroSearch utilizes a supervised fine-tuning approach to create a retrieval module that can generate both relevant and noisy documents, followed by a curriculum-based strategy that gradually increases the difficulty of retrieval tasks. Experimental results show that ZeroSearch can effectively improve LLM search performance, with larger models outperforming traditional search engines.'}, 'zh': {'title': 'æå‡LLMsæœç´¢èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ‰æ•ˆçš„ä¿¡æ¯æœç´¢å¯¹äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†å’Œç”Ÿæˆèƒ½åŠ›è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºZeroSearchçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜LLMsçš„æœç´¢èƒ½åŠ›ï¼Œè€Œæ— éœ€ä¸çœŸå®æœç´¢å¼•æ“äº’åŠ¨ã€‚è¯¥æ–¹æ³•é€šè¿‡è½»é‡çº§çš„ç›‘ç£å¾®è°ƒï¼Œå°†LLMè½¬å˜ä¸ºä¸€ä¸ªæ£€ç´¢æ¨¡å—ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥é™ä½ç”Ÿæˆæ–‡æ¡£çš„è´¨é‡ï¼Œä»¥æ¿€å‘æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒZeroSearchèƒ½å¤Ÿæœ‰æ•ˆæå‡LLMsçš„æœç´¢èƒ½åŠ›ï¼Œå¹¶åœ¨ä¸åŒå‚æ•°è§„æ¨¡çš„æ¨¡å‹ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.03821', 'title': 'Beyond Recognition: Evaluating Visual Perspective Taking in Vision\n  Language Models', 'url': 'https://huggingface.co/papers/2505.03821', 'abstract': "We investigate the ability of Vision Language Models (VLMs) to perform visual perspective taking using a novel set of visual tasks inspired by established human tests. Our approach leverages carefully controlled scenes, in which a single humanoid minifigure is paired with a single object. By systematically varying spatial configurations - such as object position relative to the humanoid minifigure and the humanoid minifigure's orientation - and using both bird's-eye and surface-level views, we created 144 unique visual tasks. Each visual task is paired with a series of 7 diagnostic questions designed to assess three levels of visual cognition: scene understanding, spatial reasoning, and visual perspective taking. Our evaluation of several state-of-the-art models, including GPT-4-Turbo, GPT-4o, Llama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that while they excel in scene understanding, the performance declines significantly on spatial reasoning and further deteriorates on perspective-taking. Our analysis suggests a gap between surface-level object recognition and the deeper spatial and perspective reasoning required for complex visual tasks, pointing to the need for integrating explicit geometric representations and tailored training protocols in future VLM development.", 'score': 19, 'issue_id': 3655, 'pub_date': '2025-05-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ', 'en': 'May 3', 'zh': '5æœˆ3æ—¥'}, 'hash': 'abede452b390c7de', 'authors': ['Gracjan GÃ³ral', 'Alicja Ziarko', 'Piotr MiÅ‚oÅ›', 'MichaÅ‚ Nauman', 'Maciej WoÅ‚czyk', 'MichaÅ‚ KosiÅ„ski'], 'affiliations': ['Faculty of Mathematics, Informatics and Mechanics, University of Warsaw, S. Banacha 2, 02-097 Warsaw, PL', 'Graduate School of Business, Stanford University, Stanford, CA 94305, USA', 'IDEAS NCBR, Chmielna 69, 00-801 Warsaw, PL', 'Institute of Mathematics, Polish Academy of Sciences, J. & J. Sniadeckich 8, 00-656 Warsaw, PL', 'Robot Learning Lab, University of California, Berkeley, CA 94720, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.03821.jpg', 'data': {'categories': ['#cv', '#reasoning', '#multimodal', '#training'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'VLM Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ¾Ñ‚ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM) Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 144 Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ†ĞµĞ½Ñ‹ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ°Ñ‚ÑÑ€Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ³ÑƒÑ€ĞºĞ¾Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ¼ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4-Turbo Ğ¸ Claude Sonnet, Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ†ĞµĞ½, Ğ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ…ÑƒĞ¶Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²Ğ½Ñ‹Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°Ñ… VLM.'}, 'en': {'title': 'Bridging the Gap: Enhancing VLMs for Spatial Reasoning and Perspective Taking', 'desc': "This paper explores how well Vision Language Models (VLMs) can understand visual perspectives through a series of unique tasks. The tasks involve a humanoid figure and an object in various spatial arrangements, designed to test scene understanding, spatial reasoning, and visual perspective taking. The study evaluates several advanced models, finding that while they perform well in recognizing scenes, they struggle with more complex reasoning tasks. The results highlight a significant gap in the models' abilities, suggesting that future developments should focus on incorporating geometric representations and specialized training methods."}, 'zh': {'title': 'æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†è§‰è§†è§’ç†è§£æ–¹é¢çš„èƒ½åŠ›ï¼Œä½¿ç”¨äº†ä¸€ç»„æ–°é¢–çš„è§†è§‰ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡çµæ„Ÿæ¥æºäºäººç±»çš„ç»å…¸æµ‹è¯•ã€‚æˆ‘ä»¬è®¾è®¡äº†144ä¸ªç‹¬ç‰¹çš„è§†è§‰ä»»åŠ¡ï¼Œé€šè¿‡ç³»ç»Ÿåœ°æ”¹å˜ç©ºé—´é…ç½®ï¼Œå¦‚ç‰©ä½“ç›¸å¯¹äºäººå½¢å°äººå¶çš„ä½ç½®å’Œæ–¹å‘ï¼Œæ¥è¯„ä¼°æ¨¡å‹çš„è¡¨ç°ã€‚æ¯ä¸ªè§†è§‰ä»»åŠ¡é…æœ‰7ä¸ªè¯Šæ–­é—®é¢˜ï¼Œæ—¨åœ¨è¯„ä¼°åœºæ™¯ç†è§£ã€ç©ºé—´æ¨ç†å’Œè§†è§‰è§†è§’ç†è§£ä¸‰ä¸ªå±‚æ¬¡çš„è§†è§‰è®¤çŸ¥ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡è¿™äº›å…ˆè¿›æ¨¡å‹åœ¨åœºæ™¯ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç©ºé—´æ¨ç†å’Œè§†è§’ç†è§£æ–¹é¢çš„è¡¨ç°æ˜¾è‘—ä¸‹é™ï¼Œè¡¨æ˜åœ¨å¤æ‚è§†è§‰ä»»åŠ¡ä¸­ï¼Œè¡¨é¢ç‰©ä½“è¯†åˆ«ä¸æ›´æ·±å±‚æ¬¡çš„ç©ºé—´å’Œè§†è§’æ¨ç†ä¹‹é—´å­˜åœ¨å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.00358', 'title': 'R&B: Domain Regrouping and Data Mixture Balancing for Efficient\n  Foundation Model Training', 'url': 'https://huggingface.co/papers/2505.00358', 'abstract': "Data mixing strategies have successfully reduced the costs involved in training language models. While promising, such methods suffer from two flaws. First, they rely on predetermined data domains (e.g., data sources, task types), which may fail to capture critical semantic nuances, leaving performance on the table. Second, these methods scale with the number of domains in a computationally prohibitive way. We address these challenges via R&B, a framework that re-partitions training data based on semantic similarity (Regroup) to create finer-grained domains, and efficiently optimizes the data composition (Balance) by leveraging a Gram matrix induced by domain gradients obtained throughout training. Unlike prior works, it removes the need for additional compute to obtain evaluation information such as losses or gradients. We analyze this technique under standard regularity conditions and provide theoretical insights that justify R&B's effectiveness compared to non-adaptive mixing approaches. Empirically, we demonstrate the effectiveness of R&B on five diverse datasets ranging from natural language to reasoning and multimodal tasks. With as little as 0.01% additional compute overhead, R&B matches or exceeds the performance of state-of-the-art data mixing strategies.", 'score': 17, 'issue_id': 3652, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 Ğ¼Ğ°Ñ', 'en': 'May 1', 'zh': '5æœˆ1æ—¥'}, 'hash': '74b251baea8510bd', 'authors': ['Albert Ge', 'Tzu-Heng Huang', 'John Cooper', 'Avi Trost', 'Ziyi Chu', 'Satya Sai Srinath Namburi GNVV', 'Ziyang Cai', 'Kendall Park', 'Nicholas Roberts', 'Frederic Sala'], 'affiliations': ['University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2505.00358.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#data', '#multimodal'], 'emoji': 'ğŸ”€', 'ru': {'title': 'R&B: Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº R&B Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. R&B Ğ¿ĞµÑ€ĞµĞ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾ÑÑ‚Ğ°Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñƒ Ğ“Ñ€Ğ°Ğ¼Ğ°, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ· Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ R&B Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ½ĞµĞ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğº ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'R&B: Smarter Data Mixing for Language Models', 'desc': 'This paper introduces R&B, a novel framework for improving data mixing strategies in training language models. R&B addresses two main issues: the reliance on fixed data domains and the high computational cost associated with scaling these domains. By regrouping training data based on semantic similarity and optimizing data composition using domain gradients, R&B creates more effective and efficient training domains. The authors provide theoretical insights and empirical evidence showing that R&B can achieve superior performance with minimal additional computational overhead compared to existing methods.'}, 'zh': {'title': 'R&Bï¼šé«˜æ•ˆçš„æ•°æ®æ··åˆæ–°ç­–ç•¥', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®æ··åˆç­–ç•¥R&Bï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•çš„ä¸¤ä¸ªä¸»è¦ç¼ºé™·ã€‚é¦–å…ˆï¼ŒR&Bé€šè¿‡è¯­ä¹‰ç›¸ä¼¼æ€§é‡æ–°åˆ’åˆ†è®­ç»ƒæ•°æ®ï¼Œåˆ›å»ºæ›´ç»†ç²’åº¦çš„æ•°æ®åŸŸï¼Œä»è€Œæ•æ‰åˆ°é‡è¦çš„è¯­ä¹‰ç»†èŠ‚ã€‚å…¶æ¬¡ï¼Œè¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­è·å¾—çš„é¢†åŸŸæ¢¯åº¦çš„GramçŸ©é˜µï¼Œä¼˜åŒ–æ•°æ®ç»„åˆï¼Œé¿å…äº†é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR&Båœ¨å¤šç§æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿä»¥æå°çš„è®¡ç®—æˆæœ¬è¶…è¶Šç°æœ‰çš„æœ€å…ˆè¿›æ•°æ®æ··åˆç­–ç•¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.04512', 'title': 'HunyuanCustom: A Multimodal-Driven Architecture for Customized Video\n  Generation', 'url': 'https://huggingface.co/papers/2505.04512', 'abstract': 'Customized video generation aims to produce videos featuring specific subjects under flexible user-defined conditions, yet existing methods often struggle with identity consistency and limited input modalities. In this paper, we propose HunyuanCustom, a multi-modal customized video generation framework that emphasizes subject consistency while supporting image, audio, video, and text conditions. Built upon HunyuanVideo, our model first addresses the image-text conditioned generation task by introducing a text-image fusion module based on LLaVA for enhanced multi-modal understanding, along with an image ID enhancement module that leverages temporal concatenation to reinforce identity features across frames. To enable audio- and video-conditioned generation, we further propose modality-specific condition injection mechanisms: an AudioNet module that achieves hierarchical alignment via spatial cross-attention, and a video-driven injection module that integrates latent-compressed conditional video through a patchify-based feature-alignment network. Extensive experiments on single- and multi-subject scenarios demonstrate that HunyuanCustom significantly outperforms state-of-the-art open- and closed-source methods in terms of ID consistency, realism, and text-video alignment. Moreover, we validate its robustness across downstream tasks, including audio and video-driven customized video generation. Our results highlight the effectiveness of multi-modal conditioning and identity-preserving strategies in advancing controllable video generation. All the code and models are available at https://hunyuancustom.github.io.', 'score': 15, 'issue_id': 3652, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ', 'en': 'May 7', 'zh': '5æœˆ7æ—¥'}, 'hash': '82e5839ef846d9d8', 'authors': ['Teng Hu', 'Zhentao Yu', 'Zhengguang Zhou', 'Sen Liang', 'Yuan Zhou', 'Qin Lin', 'Qinglin Lu'], 'affiliations': ['Tencent Hunyuan'], 'pdf_title_img': 'assets/pdf/title_img/2505.04512.jpg', 'data': {'categories': ['#open_source', '#video', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'HunyuanCustom - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ°ÑƒĞ´Ğ¸Ğ¾, Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLaVA Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ°Ğ´Ñ€Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ´Ğ»Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾- Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº AudioNet Ğ¸ ÑĞµÑ‚ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ HunyuanCustom Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'HunyuanCustom: Consistent and Multi-Modal Video Generation', 'desc': 'This paper introduces HunyuanCustom, a framework for generating customized videos that maintain subject consistency while accommodating various input types like images, audio, and text. It enhances multi-modal understanding through a text-image fusion module and reinforces identity features across video frames with an image ID enhancement module. Additionally, it incorporates specialized mechanisms for audio and video conditioning, ensuring effective alignment and integration of different modalities. The results show that HunyuanCustom outperforms existing methods in terms of identity consistency, realism, and alignment with text, proving its effectiveness in controllable video generation.'}, 'zh': {'title': 'å¤šæ¨¡æ€å®šåˆ¶è§†é¢‘ç”Ÿæˆçš„åˆ›æ–°ä¹‹è·¯', 'desc': 'å®šåˆ¶è§†é¢‘ç”Ÿæˆæ—¨åœ¨æ ¹æ®ç”¨æˆ·å®šä¹‰çš„æ¡ä»¶ç”Ÿæˆç‰¹å®šä¸»é¢˜çš„è§†é¢‘ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨èº«ä»½ä¸€è‡´æ€§å’Œè¾“å…¥æ¨¡æ€æ–¹é¢å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†HunyuanCustomï¼Œä¸€ä¸ªå¤šæ¨¡æ€å®šåˆ¶è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œå¼ºè°ƒä¸»é¢˜ä¸€è‡´æ€§ï¼Œå¹¶æ”¯æŒå›¾åƒã€éŸ³é¢‘ã€è§†é¢‘å’Œæ–‡æœ¬æ¡ä»¶ã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡å¼•å…¥åŸºäºLLaVAçš„æ–‡æœ¬-å›¾åƒèåˆæ¨¡å—å’Œå›¾åƒIDå¢å¼ºæ¨¡å—ï¼Œè§£å†³äº†å›¾åƒ-æ–‡æœ¬æ¡ä»¶ç”Ÿæˆä»»åŠ¡ï¼Œä»è€Œå¢å¼ºå¤šæ¨¡æ€ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHunyuanCustomåœ¨èº«ä»½ä¸€è‡´æ€§ã€çœŸå®æ„Ÿå’Œæ–‡æœ¬-è§†é¢‘å¯¹é½æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼ŒéªŒè¯äº†å¤šæ¨¡æ€æ¡ä»¶å’Œèº«ä»½ä¿æŒç­–ç•¥åœ¨å¯æ§è§†é¢‘ç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.04622', 'title': 'PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with\n  Auto-Regressive Transformer', 'url': 'https://huggingface.co/papers/2505.04622', 'abstract': 'Shape primitive abstraction, which decomposes complex 3D shapes into simple geometric elements, plays a crucial role in human visual cognition and has broad applications in computer vision and graphics. While recent advances in 3D content generation have shown remarkable progress, existing primitive abstraction methods either rely on geometric optimization with limited semantic understanding or learn from small-scale, category-specific datasets, struggling to generalize across diverse shape categories. We present PrimitiveAnything, a novel framework that reformulates shape primitive abstraction as a primitive assembly generation task. PrimitiveAnything includes a shape-conditioned primitive transformer for auto-regressive generation and an ambiguity-free parameterization scheme to represent multiple types of primitives in a unified manner. The proposed framework directly learns the process of primitive assembly from large-scale human-crafted abstractions, enabling it to capture how humans decompose complex shapes into primitive elements. Through extensive experiments, we demonstrate that PrimitiveAnything can generate high-quality primitive assemblies that better align with human perception while maintaining geometric fidelity across diverse shape categories. It benefits various 3D applications and shows potential for enabling primitive-based user-generated content (UGC) in games. Project page: https://primitiveanything.github.io', 'score': 13, 'issue_id': 3652, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ', 'en': 'May 7', 'zh': '5æœˆ7æ—¥'}, 'hash': '8205883cc18835a6', 'authors': ['Jingwen Ye', 'Yuze He', 'Yanning Zhou', 'Yiqin Zhu', 'Kaiwen Xiao', 'Yong-Jin Liu', 'Wei Yang', 'Xiao Han'], 'affiliations': ['Tencent AIPD, China', 'Tsinghua University, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.04622.jpg', 'data': {'categories': ['#optimization', '#3d', '#cv', '#games'], 'emoji': 'ğŸ§Š', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ñ 3D-Ñ„Ğ¾Ñ€Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PrimitiveAnything - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸ 3D-Ñ„Ğ¾Ñ€Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ². ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¹, Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ±Ğ¾Ñ€Ğ¾Ğº Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ². PrimitiveAnything Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ² Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ… Ñ„Ğ¾Ñ€Ğ¼ Ğ¸ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ğ¸Ğ³Ñ€Ğ°Ñ… Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… 3D-Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Revolutionizing 3D Shape Understanding with PrimitiveAnything', 'desc': 'This paper introduces PrimitiveAnything, a new framework for breaking down complex 3D shapes into simpler geometric parts, which is important for both human understanding and computer applications. Unlike previous methods that either optimize geometry without understanding or rely on small datasets, PrimitiveAnything learns from large-scale human-created examples to improve its generalization across different shape types. The framework uses a shape-conditioned primitive transformer for generating these parts in a structured way, ensuring clarity in how different primitives are represented. The results show that PrimitiveAnything produces high-quality assemblies that align well with human perception, making it useful for various 3D applications, including user-generated content in games.'}, 'zh': {'title': 'å½¢çŠ¶æŠ½è±¡çš„æ–°çªç ´ï¼šPrimitiveAnything', 'desc': 'å½¢çŠ¶åŸå§‹æŠ½è±¡æ˜¯å°†å¤æ‚çš„3Då½¢çŠ¶åˆ†è§£ä¸ºç®€å•å‡ ä½•å…ƒç´ çš„è¿‡ç¨‹ï¼Œè¿™å¯¹äººç±»è§†è§‰è®¤çŸ¥è‡³å…³é‡è¦ï¼Œå¹¶åœ¨è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦ä¸­æœ‰å¹¿æ³›åº”ç”¨ã€‚ç°æœ‰çš„åŸå§‹æŠ½è±¡æ–¹æ³•é€šå¸¸ä¾èµ–äºå‡ ä½•ä¼˜åŒ–ï¼Œç¼ºä¹è¯­ä¹‰ç†è§£ï¼Œæˆ–è€…ä»…ä»å°è§„æ¨¡ã€ç‰¹å®šç±»åˆ«çš„æ•°æ®é›†ä¸­å­¦ä¹ ï¼Œéš¾ä»¥åœ¨å¤šæ ·çš„å½¢çŠ¶ç±»åˆ«ä¸­è¿›è¡Œæ³›åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†PrimitiveAnythingï¼Œä¸€ä¸ªå°†å½¢çŠ¶åŸå§‹æŠ½è±¡é‡æ–°å®šä¹‰ä¸ºåŸå§‹ç»„è£…ç”Ÿæˆä»»åŠ¡çš„æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¤§è§„æ¨¡äººç±»åˆ›ä½œçš„æŠ½è±¡å­¦ä¹ åŸå§‹ç»„è£…è¿‡ç¨‹ï¼Œä»è€Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰äººç±»å¦‚ä½•å°†å¤æ‚å½¢çŠ¶åˆ†è§£ä¸ºåŸå§‹å…ƒç´ ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.04364', 'title': "Benchmarking LLMs' Swarm intelligence", 'url': 'https://huggingface.co/papers/2505.04364', 'abstract': 'Large Language Models (LLMs) show potential for complex reasoning, yet their capacity for emergent coordination in Multi-Agent Systems (MAS) when operating under strict constraints-such as limited local perception and communication, characteristic of natural swarms-remains largely unexplored, particularly concerning the nuances of swarm intelligence. Existing benchmarks often do not fully capture the unique challenges of decentralized coordination that arise when agents operate with incomplete spatio-temporal information. To bridge this gap, we introduce SwarmBench, a novel benchmark designed to systematically evaluate the swarm intelligence capabilities of LLMs acting as decentralized agents. SwarmBench features five foundational MAS coordination tasks within a configurable 2D grid environment, forcing agents to rely primarily on local sensory input (k x k view) and local communication. We propose metrics for coordination effectiveness and analyze emergent group dynamics. Evaluating several leading LLMs in a zero-shot setting, we find significant performance variations across tasks, highlighting the difficulties posed by local information constraints. While some coordination emerges, results indicate limitations in robust planning and strategy formation under uncertainty in these decentralized scenarios. Assessing LLMs under swarm-like conditions is crucial for realizing their potential in future decentralized systems. We release SwarmBench as an open, extensible toolkit-built upon a customizable and scalable physical system with defined mechanical properties. It provides environments, prompts, evaluation scripts, and the comprehensive experimental datasets generated, aiming to foster reproducible research into LLM-based MAS coordination and the theoretical underpinnings of Embodied MAS. Our code repository is available at https://github.com/x66ccff/swarmbench.', 'score': 13, 'issue_id': 3648, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ', 'en': 'May 7', 'zh': '5æœˆ7æ—¥'}, 'hash': '4b0575d2194aee20', 'authors': ['Kai Ruan', 'Mowen Huang', 'Ji-Rong Wen', 'Hao Sun'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.04364.jpg', 'data': {'categories': ['#reasoning', '#agents', '#benchmark', '#open_source', '#multimodal'], 'emoji': 'ğŸ', 'ru': {'title': 'SwarmBench: Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ¾ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SwarmBench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€Ğ¾ĞµĞ²Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. SwarmBench Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² 2D-ÑĞµÑ‚ĞºĞµ, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ñ‹ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸ĞµĞ¼ Ğ¸ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ….'}, 'en': {'title': 'Unlocking Swarm Intelligence in Language Models', 'desc': "This paper explores how Large Language Models (LLMs) can coordinate in Multi-Agent Systems (MAS) under strict constraints, similar to natural swarms. It introduces SwarmBench, a new benchmark that evaluates the swarm intelligence of LLMs by simulating decentralized coordination tasks in a 2D grid environment. The study highlights the challenges of local perception and communication, revealing significant performance variations among LLMs when faced with limited information. The findings emphasize the need for further research into LLMs' capabilities in decentralized scenarios to unlock their potential in future systems."}, 'zh': {'title': 'æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¾¤ä½“æ™ºèƒ½æ½œåŠ›', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ä¸­åœ¨ä¸¥æ ¼çº¦æŸä¸‹çš„åè°ƒèƒ½åŠ›ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ï¼Œå°¤å…¶æ˜¯åœ¨ç¾¤ä½“æ™ºèƒ½çš„ç»†å¾®å·®åˆ«æ–¹é¢ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•å¾€å¾€æ— æ³•å®Œå…¨æ•æ‰åˆ°åœ¨ä¸å®Œæ•´æ—¶ç©ºä¿¡æ¯ä¸‹ï¼Œæ™ºèƒ½ä½“è¿›è¡Œå»ä¸­å¿ƒåŒ–åè°ƒæ‰€é¢ä¸´çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†SwarmBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°LLMsä½œä¸ºå»ä¸­å¿ƒåŒ–æ™ºèƒ½ä½“çš„ç¾¤ä½“æ™ºèƒ½èƒ½åŠ›ã€‚é€šè¿‡è¯„ä¼°å¤šä¸ªé¢†å…ˆçš„LLMsï¼Œæˆ‘ä»¬å‘ç°å®ƒä»¬åœ¨ä»»åŠ¡ä¸­çš„è¡¨ç°å·®å¼‚æ˜¾è‘—ï¼Œçªæ˜¾äº†åœ¨å±€éƒ¨ä¿¡æ¯é™åˆ¶ä¸‹çš„åè°ƒå›°éš¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.04528', 'title': 'Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal\n  Problem-Solving', 'url': 'https://huggingface.co/papers/2505.04528', 'abstract': 'As a seemingly self-explanatory task, problem-solving has been a significant component of science and engineering. However, a general yet concrete formulation of problem-solving itself is missing. With the recent development of AI-based problem-solving agents, the demand for process-level verifiability is rapidly increasing yet underexplored. To fill these gaps, we present a principled formulation of problem-solving as a deterministic Markov decision process; a novel framework, FPS (Formal Problem-Solving), which utilizes existing FTP (formal theorem proving) environments to perform process-verified problem-solving; and D-FPS (Deductive FPS), decoupling solving and answer verification for better human-alignment. The expressiveness, soundness and completeness of the frameworks are proven. We construct three benchmarks on problem-solving: FormalMath500, a formalization of a subset of the MATH500 benchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP benchmarks MiniF2F and PutnamBench. For faithful, interpretable, and human-aligned evaluation, we propose RPE (Restricted Propositional Equivalence), a symbolic approach to determine the correctness of answers by formal verification. We evaluate four prevalent FTP models and two prompting methods as baselines, solving at most 23.77% of FormalMath500, 27.47% of MiniF2F-Solving, and 0.31% of PutnamBench-Solving.', 'score': 7, 'issue_id': 3652, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ', 'en': 'May 7', 'zh': '5æœˆ7æ—¥'}, 'hash': '0e9e0d509e4b4624', 'authors': ['Qi Liu', 'Xinhao Zheng', 'Renqiu Xia', 'Xingzhi Qi', 'Qinxiang Cao', 'Junchi Yan'], 'affiliations': ['Sch. of Computer Science & Sch. of Artificial Intelligence, Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.04528.jpg', 'data': {'categories': ['#math', '#training', '#benchmark', '#alignment', '#interpretability', '#reasoning', '#agents'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ°Ğº Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº FPS (Formal Problem-Solving), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ÑÑ€ĞµĞ´Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ D-FPS (Deductive FPS), Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‰Ğ¸Ğ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñƒ. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ Ñ‚Ñ€Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ RPE Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing Problem-Solving with Formal Frameworks', 'desc': 'This paper addresses the challenge of formalizing problem-solving in science and engineering by proposing a new framework called FPS (Formal Problem-Solving). It treats problem-solving as a deterministic Markov decision process, allowing for process-level verifiability in AI-based agents. The authors introduce D-FPS (Deductive FPS) to separate the solving process from answer verification, enhancing alignment with human reasoning. They also present benchmarks for evaluating problem-solving capabilities and a novel method, RPE (Restricted Propositional Equivalence), for verifying the correctness of solutions through formal methods.'}, 'zh': {'title': 'å½¢å¼åŒ–é—®é¢˜è§£å†³çš„æ–°æ¡†æ¶', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†é—®é¢˜è§£å†³çš„å½¢å¼åŒ–ï¼Œæå‡ºäº†ä¸€ç§å°†é—®é¢˜è§£å†³è§†ä¸ºç¡®å®šæ€§é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹çš„æ¡†æ¶ã€‚ä½œè€…ä»‹ç»äº†FPSï¼ˆæ­£å¼é—®é¢˜è§£å†³ï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨ç°æœ‰çš„æ­£å¼å®šç†è¯æ˜ç¯å¢ƒè¿›è¡Œè¿‡ç¨‹éªŒè¯çš„é—®é¢˜è§£å†³ã€‚ä¸ºäº†æé«˜äººç±»å¯¹é½ï¼Œè®ºæ–‡è¿˜æå‡ºäº†D-FPSï¼ˆæ¼”ç»FPSï¼‰ï¼Œå°†æ±‚è§£ä¸ç­”æ¡ˆéªŒè¯è§£è€¦ã€‚æœ€åï¼Œä½œè€…æ„å»ºäº†ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç¬¦å·æ–¹æ³•RPEæ¥è¯„ä¼°ç­”æ¡ˆçš„æ­£ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.03912', 'title': 'OpenHelix: A Short Survey, Empirical Analysis, and Open-Source\n  Dual-System VLA Model for Robotic Manipulation', 'url': 'https://huggingface.co/papers/2505.03912', 'abstract': 'Dual-system VLA (Vision-Language-Action) architectures have become a hot topic in embodied intelligence research, but there is a lack of sufficient open-source work for further performance analysis and optimization. To address this problem, this paper will summarize and compare the structural designs of existing dual-system architectures, and conduct systematic empirical evaluations on the core design elements of existing dual-system architectures. Ultimately, it will provide a low-cost open-source model for further exploration. Of course, this project will continue to update with more experimental conclusions and open-source models with improved performance for everyone to choose from. Project page: https://openhelix-robot.github.io/.', 'score': 6, 'issue_id': 3652, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ', 'en': 'May 6', 'zh': '5æœˆ6æ—¥'}, 'hash': 'f7347c1b093f9488', 'authors': ['Can Cui', 'Pengxiang Ding', 'Wenxuan Song', 'Shuanghao Bai', 'Xinyang Tong', 'Zirui Ge', 'Runze Suo', 'Wanqi Zhou', 'Yang Liu', 'Bofang Jia', 'Han Zhao', 'Siteng Huang', 'Donglin Wang'], 'affiliations': ['HKUST(GZ)', 'Westlake University', 'Xian Jiaotong University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.03912.jpg', 'data': {'categories': ['#architecture', '#optimization', '#agents', '#open_source', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²ÑƒÑ…ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ñ… VLA Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ´Ğ²ÑƒÑ…ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼ VLA (Vision-Language-Action) Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¸Ñ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¦ĞµĞ»ÑŒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ‚ÑŒÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Empowering Embodied Intelligence with Open-Source VLA Models', 'desc': 'This paper focuses on dual-system Vision-Language-Action (VLA) architectures, which are important for developing embodied intelligence. It highlights the current lack of open-source resources that allow for thorough performance analysis and optimization of these architectures. The authors summarize and compare existing designs and conduct empirical evaluations on their core elements. The goal is to provide a low-cost open-source model that can be continuously updated with new findings and improved performance options for researchers.'}, 'zh': {'title': 'æ¨åŠ¨åŒç³»ç»ŸVLAæ¶æ„çš„å¼€æºæ¢ç´¢', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åŒç³»ç»Ÿè§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¶æ„åœ¨å…·èº«æ™ºèƒ½ç ”ç©¶ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºç›®å‰ç¼ºä¹è¶³å¤Ÿçš„å¼€æºå·¥ä½œæ¥è¿›è¡Œæ€§èƒ½åˆ†æå’Œä¼˜åŒ–ã€‚ä½œè€…æ€»ç»“å¹¶æ¯”è¾ƒäº†ç°æœ‰åŒç³»ç»Ÿæ¶æ„çš„ç»“æ„è®¾è®¡ï¼Œå¹¶å¯¹å…¶æ ¸å¿ƒè®¾è®¡å…ƒç´ è¿›è¡Œäº†ç³»ç»Ÿçš„å®è¯è¯„ä¼°ã€‚æœ€ç»ˆï¼Œæœ¬æ–‡å°†æä¾›ä¸€ä¸ªä½æˆæœ¬çš„å¼€æºæ¨¡å‹ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥æ¢ç´¢å’Œç ”ç©¶ã€‚è¯¥é¡¹ç›®å°†æŒç»­æ›´æ–°ï¼Œæä¾›æ›´å¤šå®éªŒç»“è®ºå’Œæ€§èƒ½æ”¹è¿›çš„å¼€æºæ¨¡å‹ä¾›å¤§å®¶é€‰æ‹©ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.04606', 'title': 'OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue\n  Resolution', 'url': 'https://huggingface.co/papers/2505.04606', 'abstract': "The GitHub issue resolution task aims to resolve issues reported in repositories automatically. With advances in large language models (LLMs), this task has gained increasing attention, and several benchmarks are proposed to evaluate the issue resolution ability of LLMs. However, existing benchmarks have three main limitations. First, current benchmarks focus on a single programming language, limiting the evaluation of issues from repositories across different languages. Second, they usually cover a narrow range of domains, which may fail to represent the diversity of real-world issues. Third, existing benchmarks rely solely on textual information in issue descriptions, overlooking multimodal information such as images in issues. In this paper, we propose OmniGIRL, a GitHub Issue ResoLution benchmark that is multilingual, multimodal, and multi-domain. OmniGIRL includes 959 task instances, which are collected from repositories across four programming languages (i.e., Python, JavaScript, TypeScript, and Java) and eight different domains. Our evaluation shows that current LLMs show limited performances on OmniGIRL. Notably, the best-performing model, GPT-4o, resolves only 8.6% of the issues. Besides, we find that current LLMs struggle to resolve issues requiring understanding images. The best performance is achieved by Claude-3.5-Sonnet, which resolves only 10.5% of the issues with image information. Finally, we analyze the reasons behind current LLMs' failure on OmniGIRL, providing insights for future improvements.", 'score': 5, 'issue_id': 3657, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ', 'en': 'May 7', 'zh': '5æœˆ7æ—¥'}, 'hash': '25e97f182730fc25', 'authors': ['Lianghong Guo', 'Wei Tao', 'Runhan Jiang', 'Yanlin Wang', 'Jiachi Chen', 'Xilin Liu', 'Yuchi Ma', 'Mingzhi Mao', 'Hongyu Zhang', 'Zibin Zheng'], 'affiliations': ['Chongqing University, China', 'Huawei Cloud Computing Technologies Co., Ltd., China', 'Independent Researcher, China', 'Sun Yat-sen University, Zhuhai Key Laboratory of Trusted Large Language Models, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.04606.jpg', 'data': {'categories': ['#benchmark', '#multilingual', '#dataset', '#long_context', '#multimodal'], 'emoji': 'ğŸ™', 'ru': {'title': 'OmniGIRL: Ğ’Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ GitHub', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OmniGIRL - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹, Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ½Ğ° GitHub. OmniGIRL Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 959 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° OmniGIRL, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½ Ğ½ĞµÑƒĞ´Ğ°Ñ‡ LLM Ğ½Ğ° OmniGIRL Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ insights Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'OmniGIRL: A Comprehensive Benchmark for GitHub Issue Resolution', 'desc': 'This paper introduces OmniGIRL, a new benchmark for automatically resolving GitHub issues using large language models (LLMs). Unlike existing benchmarks, OmniGIRL is designed to be multilingual, multimodal, and multi-domain, addressing the limitations of focusing on a single programming language and a narrow range of issues. The benchmark includes 959 instances from four programming languages and eight domains, highlighting the diversity of real-world problems. Evaluation results show that current LLMs perform poorly on this benchmark, particularly in resolving issues that require understanding images, indicating a need for further advancements in model capabilities.'}, 'zh': {'title': 'OmniGIRLï¼šå¤šè¯­è¨€å¤šæ¨¡æ€çš„GitHubé—®é¢˜è§£å†³åŸºå‡†', 'desc': 'æœ¬æ–‡æå‡ºäº†OmniGIRLï¼Œä¸€ä¸ªå¤šè¯­è¨€ã€å¤šæ¨¡æ€å’Œå¤šé¢†åŸŸçš„GitHubé—®é¢˜è§£å†³åŸºå‡†ã€‚ç°æœ‰çš„åŸºå‡†å­˜åœ¨ä¸‰ä¸ªä¸»è¦é™åˆ¶ï¼šåªå…³æ³¨å•ä¸€ç¼–ç¨‹è¯­è¨€ã€è¦†ç›–é¢†åŸŸç‹­çª„ä»¥åŠä»…ä¾èµ–æ–‡æœ¬ä¿¡æ¯ã€‚OmniGIRLåŒ…å«æ¥è‡ªå››ç§ç¼–ç¨‹è¯­è¨€å’Œå…«ä¸ªä¸åŒé¢†åŸŸçš„959ä¸ªä»»åŠ¡å®ä¾‹ï¼Œæ—¨åœ¨æ›´å…¨é¢åœ°è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰çš„è¯­è¨€æ¨¡å‹åœ¨OmniGIRLä¸Šçš„è¡¨ç°æœ‰é™ï¼Œå°¤å…¶åœ¨å¤„ç†éœ€è¦ç†è§£å›¾åƒçš„é—®é¢˜æ—¶è¡¨ç°æ›´å·®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.04601', 'title': 'OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision\n  Encoders for Multimodal Learning', 'url': 'https://huggingface.co/papers/2505.04601', 'abstract': "OpenAI's CLIP, released in early 2021, have long been the go-to choice of vision encoder for building multimodal foundation models. Although recent alternatives such as SigLIP have begun to challenge this status quo, to our knowledge none are fully open: their training data remains proprietary and/or their training recipes are not released. This paper fills this gap with OpenVision, a fully-open, cost-effective family of vision encoders that match or surpass the performance of OpenAI's CLIP when integrated into multimodal frameworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS for training framework and Recap-DataComp-1B for training data -- while revealing multiple key insights in enhancing encoder quality and showcasing practical benefits in advancing multimodal models. By releasing vision encoders spanning from 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible trade-off between capacity and efficiency in building multimodal models: larger models deliver enhanced multimodal performance, while smaller versions enable lightweight, edge-ready multimodal deployments.", 'score': 4, 'issue_id': 3666, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ', 'en': 'May 7', 'zh': '5æœˆ7æ—¥'}, 'hash': '0b9c03d9680cfe04', 'authors': ['Xianhang Li', 'Yanqing Liu', 'Haoqin Tu', 'Hongru Zhu', 'Cihang Xie'], 'affiliations': ['University of California, Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2505.04601.jpg', 'data': {'categories': ['#small_models', '#dataset', '#open_source', '#multimodal', '#architecture'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'OpenVision: Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'OpenVision Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ CLIP Ğ¾Ñ‚ OpenAI Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ğ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ OpenVision Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº CLIPS Ğ¸ Recap-DataComp-1B, Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ´ĞµĞ¸ Ğ¿Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² (Ğ¾Ñ‚ 5,9 Ğ´Ğ¾ 632,1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²), Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. OpenVision Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ» Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ², Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'OpenVision: Open and Efficient Vision Encoders for Multimodal Models', 'desc': "This paper introduces OpenVision, a new family of vision encoders that are fully open and cost-effective, designed to compete with OpenAI's CLIP. OpenVision not only matches but can also surpass CLIP's performance when used in multimodal frameworks like LLaVA. The authors leverage existing methodologies and datasets to improve encoder quality and provide insights into enhancing multimodal models. By offering a range of models with varying parameters, OpenVision allows users to choose between high performance and efficiency for their specific applications."}, 'zh': {'title': 'å¼€æ”¾è§†è§‰ç¼–ç å™¨ï¼Œæå‡å¤šæ¨¡æ€æ¨¡å‹æ€§èƒ½', 'desc': 'OpenVisionæ˜¯ä¸€ä¸ªå®Œå…¨å¼€æ”¾çš„è§†è§‰ç¼–ç å™¨å®¶æ—ï¼Œæ—¨åœ¨ä¸OpenAIçš„CLIPç›¸åª²ç¾æˆ–è¶…è¶Šå…¶æ€§èƒ½ã€‚è¯¥è®ºæ–‡å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨ç°æœ‰çš„è®­ç»ƒæ¡†æ¶å’Œæ•°æ®é›†ï¼Œæå‡ç¼–ç å™¨çš„è´¨é‡ï¼Œå¹¶ä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„å‘å±•æä¾›å®ç”¨çš„å¥½å¤„ã€‚OpenVisionæä¾›äº†ä»590ä¸‡åˆ°6.32äº¿å‚æ•°çš„å¤šç§é€‰æ‹©ï¼Œä½¿å¾—å¼€å‘è€…å¯ä»¥åœ¨æ¨¡å‹å®¹é‡å’Œæ•ˆç‡ä¹‹é—´çµæ´»æƒè¡¡ã€‚é€šè¿‡è¿™äº›å¼€æ”¾çš„èµ„æºï¼ŒOpenVisionä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„æ„å»ºæä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.03570', 'title': 'OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents', 'url': 'https://huggingface.co/papers/2505.03570', 'abstract': 'In this paper, we introduce OSUniverse: a benchmark of complex, multimodal desktop-oriented tasks for advanced GUI-navigation AI agents that focuses on ease of use, extensibility, comprehensive coverage of test cases, and automated validation. We divide the tasks in increasing levels of complexity, from basic precision clicking to multistep, multiapplication tests requiring dexterity, precision, and clear thinking from the agent. In version one of the benchmark, presented here, we have calibrated the complexity of the benchmark test cases to ensure that the SOTA (State of the Art) agents (at the time of publication) do not achieve results higher than 50%, while the average white collar worker can perform all these tasks with perfect accuracy. The benchmark can be scored manually, but we also introduce an automated validation mechanism that has an average error rate less than 2%. Therefore, this benchmark presents solid ground for fully automated measuring of progress, capabilities and the effectiveness of GUI-navigation AI agents over the short and medium-term horizon. The source code of the benchmark is available at https://github.com/agentsea/osuniverse.', 'score': 4, 'issue_id': 3654, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ', 'en': 'May 6', 'zh': '5æœˆ6æ—¥'}, 'hash': 'e87199c8805bce4f', 'authors': ['Mariya Davydova', 'Daniel Jeffries', 'Patrick Barker', 'Arturo MÃ¡rquez Flores', 'SinÃ©ad Ryan'], 'affiliations': ['Kentauros AI Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.03570.jpg', 'data': {'categories': ['#games', '#agents', '#open_source', '#multimodal', '#optimization', '#benchmark'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'OSUniverse: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞµ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ OSUniverse - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… ĞºĞ»Ğ¸ĞºĞ¾Ğ² Ğ´Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…. Ğ¡Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğµ Ğ±Ğ¾Ğ»ĞµĞµ 50% ÑƒÑĞ¿ĞµÑ…Ğ°, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ñ„Ğ¸ÑĞ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ½Ğ¸ĞºĞ¸ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ ÑĞ¾ Ğ²ÑĞµĞ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ¼ĞµĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ³Ñ€ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼ĞµĞ½ĞµĞµ 2%.'}, 'en': {'title': 'OSUniverse: Benchmarking AI Navigation in Complex Desktop Tasks', 'desc': 'This paper presents OSUniverse, a benchmark designed for evaluating advanced AI agents in navigating complex desktop tasks. The tasks are categorized by increasing difficulty, challenging agents with skills like precision and multi-step reasoning. The benchmark is calibrated so that current state-of-the-art agents score below 50%, while average human workers can achieve perfect scores. Additionally, it features an automated validation system with a low error rate, enabling reliable assessment of AI progress in GUI navigation.'}, 'zh': {'title': 'OSUniverseï¼šGUIå¯¼èˆªAIçš„å…¨æ–°åŸºå‡†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†OSUniverseï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é«˜çº§GUIå¯¼èˆªAIä»£ç†çš„å¤æ‚å¤šæ¨¡æ€æ¡Œé¢ä»»åŠ¡åŸºå‡†ï¼Œæ—¨åœ¨æ˜“ç”¨æ€§ã€å¯æ‰©å±•æ€§ã€å…¨é¢è¦†ç›–æµ‹è¯•æ¡ˆä¾‹å’Œè‡ªåŠ¨éªŒè¯æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬å°†ä»»åŠ¡åˆ†ä¸ºä¸åŒå¤æ‚åº¦çš„çº§åˆ«ï¼Œä»åŸºæœ¬çš„ç²¾ç¡®ç‚¹å‡»åˆ°éœ€è¦çµæ´»æ€§ã€ç²¾ç¡®æ€§å’Œæ¸…æ™°æ€ç»´çš„å¤šæ­¥éª¤ã€å¤šåº”ç”¨ç¨‹åºæµ‹è¯•ã€‚åœ¨åŸºå‡†çš„ç¬¬ä¸€ç‰ˆä¸­ï¼Œæˆ‘ä»¬è°ƒæ•´äº†æµ‹è¯•æ¡ˆä¾‹çš„å¤æ‚æ€§ï¼Œä»¥ç¡®ä¿å½“æ—¶çš„æœ€å…ˆè¿›ï¼ˆSOTAï¼‰ä»£ç†çš„ç»“æœä¸è¶…è¿‡50%ï¼Œè€Œæ™®é€šç™½é¢†å·¥äººå¯ä»¥å®Œç¾å®Œæˆæ‰€æœ‰è¿™äº›ä»»åŠ¡ã€‚è¯¥åŸºå‡†å¯ä»¥æ‰‹åŠ¨è¯„åˆ†ï¼ŒåŒæ—¶æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå¹³å‡é”™è¯¯ç‡ä½äº2%çš„è‡ªåŠ¨éªŒè¯æœºåˆ¶ï¼Œä¸ºå…¨é¢è‡ªåŠ¨åŒ–æµ‹é‡GUIå¯¼èˆªAIä»£ç†çš„è¿›å±•ã€èƒ½åŠ›å’Œæœ‰æ•ˆæ€§æä¾›äº†åšå®åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.03418', 'title': 'Knowledge Augmented Complex Problem Solving with Large Language Models:\n  A Survey', 'url': 'https://huggingface.co/papers/2505.03418', 'abstract': 'Problem-solving has been a fundamental driver of human progress in numerous domains. With advancements in artificial intelligence, Large Language Models (LLMs) have emerged as powerful tools capable of tackling complex problems across diverse domains. Unlike traditional computational systems, LLMs combine raw computational power with an approximation of human reasoning, allowing them to generate solutions, make inferences, and even leverage external computational tools. However, applying LLMs to real-world problem-solving presents significant challenges, including multi-step reasoning, domain knowledge integration, and result verification. This survey explores the capabilities and limitations of LLMs in complex problem-solving, examining techniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation, and various LLM-based and tool-based verification techniques. Additionally, we highlight domain-specific challenges in various domains, such as software engineering, mathematical reasoning and proving, data analysis and modeling, and scientific research. The paper further discusses the fundamental limitations of the current LLM solutions and the future directions of LLM-based complex problems solving from the perspective of multi-step reasoning, domain knowledge integration and result verification.', 'score': 4, 'issue_id': 3652, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ', 'en': 'May 6', 'zh': '5æœˆ6æ—¥'}, 'hash': '8417799a01a2ecc2', 'authors': ['Da Zheng', 'Lun Du', 'Junwei Su', 'Yuchen Tian', 'Yuqi Zhu', 'Jintian Zhang', 'Lanning Wei', 'Ningyu Zhang', 'Huajun Chen'], 'affiliations': ['Ant Group, China', 'The University of Hong Kong, China', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.03418.jpg', 'data': {'categories': ['#rl', '#survey', '#math', '#training', '#reasoning', '#science', '#data'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'LLM: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ñ‚Ğ°ĞºĞ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸, ĞºĞ°Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ (Chain-of-Thought), Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ LLM Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Unlocking Complex Problem-Solving with Large Language Models', 'desc': 'This paper surveys the role of Large Language Models (LLMs) in solving complex problems across various fields. It highlights how LLMs combine computational power with human-like reasoning to generate solutions and make inferences. The paper addresses challenges such as multi-step reasoning, integrating domain knowledge, and verifying results when applying LLMs in real-world scenarios. It also discusses specific challenges in areas like software engineering and scientific research, while outlining future directions for improving LLM capabilities in complex problem-solving.'}, 'zh': {'title': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼šå¤æ‚é—®é¢˜è§£å†³çš„æ–°å·¥å…·', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚é—®é¢˜è§£å†³ä¸­çš„èƒ½åŠ›å’Œå±€é™æ€§ã€‚ä¸ä¼ ç»Ÿè®¡ç®—ç³»ç»Ÿä¸åŒï¼ŒLLMsç»“åˆäº†å¼ºå¤§çš„è®¡ç®—èƒ½åŠ›å’Œäººç±»æ¨ç†çš„è¿‘ä¼¼ï¼Œèƒ½å¤Ÿç”Ÿæˆè§£å†³æ–¹æ¡ˆå’Œè¿›è¡Œæ¨ç†ã€‚å°½ç®¡LLMsåœ¨å¤šæ­¥éª¤æ¨ç†ã€é¢†åŸŸçŸ¥è¯†æ•´åˆå’Œç»“æœéªŒè¯æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œä½†å®ƒä»¬åœ¨è½¯ä»¶å·¥ç¨‹ã€æ•°å­¦æ¨ç†ã€æ•°æ®åˆ†æå’Œç§‘å­¦ç ”ç©¶ç­‰é¢†åŸŸçš„åº”ç”¨æ½œåŠ›å·¨å¤§ã€‚æœ¬æ–‡è¿˜è®¨è®ºäº†å½“å‰LLMè§£å†³æ–¹æ¡ˆçš„åŸºæœ¬å±€é™æ€§ä»¥åŠæœªæ¥åœ¨å¤æ‚é—®é¢˜è§£å†³ä¸­çš„å‘å±•æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.04253', 'title': 'LLM-Independent Adaptive RAG: Let the Question Speak for Itself', 'url': 'https://huggingface.co/papers/2505.04253', 'abstract': 'Large Language Models~(LLMs) are prone to hallucinations, and Retrieval-Augmented Generation (RAG) helps mitigate this, but at a high computational cost while risking misinformation. Adaptive retrieval aims to retrieve only when necessary, but existing approaches rely on LLM-based uncertainty estimation, which remain inefficient and impractical. In this study, we introduce lightweight LLM-independent adaptive retrieval methods based on external information. We investigated 27 features, organized into 7 groups, and their hybrid combinations. We evaluated these methods on 6 QA datasets, assessing the QA performance and efficiency. The results show that our approach matches the performance of complex LLM-based methods while achieving significant efficiency gains, demonstrating the potential of external information for adaptive retrieval.', 'score': 2, 'issue_id': 3660, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ', 'en': 'May 7', 'zh': '5æœˆ7æ—¥'}, 'hash': '7ce9a465202a9c3c', 'authors': ['Maria Marina', 'Nikolay Ivanov', 'Sergey Pletenev', 'Mikhail Salnikov', 'Daria Galimzianova', 'Nikita Krayko', 'Vasily Konovalov', 'Alexander Panchenko', 'Viktor Moskvoretskii'], 'affiliations': ['AIRI', 'HSE University', 'MIPT', 'MTS AI', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2505.04253.jpg', 'data': {'categories': ['#dataset', '#optimization', '#rag', '#benchmark', '#hallucinations'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ ÑĞ°Ğ¼Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ 27 Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² 7 Ğ³Ñ€ÑƒĞ¿Ğ¿, Ğ¸ Ğ¸Ñ… Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ±Ñ‹Ğ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞµĞ½Ñ‹ Ğ½Ğ° 6 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Efficient Adaptive Retrieval: Enhancing QA with External Information', 'desc': 'This paper addresses the issue of hallucinations in Large Language Models (LLMs) and presents a solution through Retrieval-Augmented Generation (RAG). The authors propose lightweight, LLM-independent adaptive retrieval methods that utilize external information, aiming to improve efficiency while maintaining performance. They analyze 27 features across 7 groups to create effective hybrid combinations for retrieval. The results indicate that their approach can achieve comparable performance to complex LLM-based methods but with significant efficiency improvements.'}, 'zh': {'title': 'è½»é‡çº§è‡ªé€‚åº”æ£€ç´¢ï¼Œæå‡æ•ˆç‡ä¸å‡†ç¡®æ€§', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å®¹æ˜“å‡ºç°å¹»è§‰ï¼Œè€Œå¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰å¯ä»¥å¸®åŠ©å‡è½»è¿™ä¸€é—®é¢˜ï¼Œä½†ä»£ä»·é«˜æ˜‚ä¸”å¯èƒ½å¯¼è‡´é”™è¯¯ä¿¡æ¯ã€‚è‡ªé€‚åº”æ£€ç´¢æ—¨åœ¨ä»…åœ¨å¿…è¦æ—¶è¿›è¡Œæ£€ç´¢ï¼Œä½†ç°æœ‰æ–¹æ³•ä¾èµ–äºåŸºäºLLMçš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œæ•ˆç‡ä½ä¸‹ä¸”ä¸åˆ‡å®é™…ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤–éƒ¨ä¿¡æ¯çš„è½»é‡çº§LLMç‹¬ç«‹è‡ªé€‚åº”æ£€ç´¢æ–¹æ³•ï¼Œç ”ç©¶äº†27ä¸ªç‰¹å¾å¹¶å°†å…¶ç»„ç»‡æˆ7ä¸ªç»„åŠå…¶æ··åˆç»„åˆã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨6ä¸ªé—®ç­”æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸å¤æ‚çš„LLMæ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶å®ç°äº†æ˜¾è‘—çš„æ•ˆç‡æå‡ï¼Œå±•ç¤ºäº†å¤–éƒ¨ä¿¡æ¯åœ¨è‡ªé€‚åº”æ£€ç´¢ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.03538', 'title': 'RAIL: Region-Aware Instructive Learning for Semi-Supervised Tooth\n  Segmentation in CBCT', 'url': 'https://huggingface.co/papers/2505.03538', 'abstract': 'Semi-supervised learning has become a compelling approach for 3D tooth segmentation from CBCT scans, where labeled data is minimal. However, existing methods still face two persistent challenges: limited corrective supervision in structurally ambiguous or mislabeled regions during supervised training and performance degradation caused by unreliable pseudo-labels on unlabeled data. To address these problems, we propose Region-Aware Instructive Learning (RAIL), a dual-group dual-student, semi-supervised framework. Each group contains two student models guided by a shared teacher network. By alternating training between the two groups, RAIL promotes intergroup knowledge transfer and collaborative region-aware instruction while reducing overfitting to the characteristics of any single model. Specifically, RAIL introduces two instructive mechanisms. Disagreement-Focused Supervision (DFS) Controller improves supervised learning by instructing predictions only within areas where student outputs diverge from both ground truth and the best student, thereby concentrating supervision on structurally ambiguous or mislabeled areas. In the unsupervised phase, Confidence-Aware Learning (CAL) Modulator reinforces agreement in regions with high model certainty while reducing the effect of low-confidence predictions during training. This helps prevent our model from learning unstable patterns and improves the overall reliability of pseudo-labels. Extensive experiments on four CBCT tooth segmentation datasets show that RAIL surpasses state-of-the-art methods under limited annotation. Our code will be available at https://github.com/Tournesol-Saturday/RAIL.', 'score': 2, 'issue_id': 3660, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ', 'en': 'May 6', 'zh': '5æœˆ6æ—¥'}, 'hash': '9e7cb17dec2eda27', 'authors': ['Chuyu Zhao', 'Hao Huang', 'Jiashuo Guo', 'Ziyu Shen', 'Zhongwei Zhou', 'Jie Liu', 'Zekuan Yu'], 'affiliations': ['Academy for Engineering and Technology, Fudan University, Shanghai 200433, China', 'Department of Oral and Maxillofacial Surgery, General Hospital of Ningxia Medical University, Yinchuan 750004, China', 'School of Computer Science & Technology, Beijing Jiaotong University, Beijing 100044, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.03538.jpg', 'data': {'categories': ['#dataset', '#3d', '#optimization', '#transfer_learning', '#training'], 'emoji': 'ğŸ¦·', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ·ÑƒĞ±Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ»ÑƒĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ·ÑƒĞ±Ğ¾Ğ² Ğ½Ğ° ĞšĞ›ĞšĞ¢-ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ…, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Region-Aware Instructive Learning (RAIL). RAIL Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²ÑƒÑ Ğ´Ğ²ÑƒÑ…ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ° ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ´Ğ²Ğ° ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ°, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±Ñ‰Ğ¸Ğ¼ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ²Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°: Disagreement-Focused Supervision (DFS) Controller Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Confidence-Aware Learning (CAL) Modulator Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¼ĞµÑ‚Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RAIL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing 3D Tooth Segmentation with RAIL: A Smart Learning Approach', 'desc': 'This paper presents a new method called Region-Aware Instructive Learning (RAIL) for improving 3D tooth segmentation from CBCT scans using semi-supervised learning. RAIL addresses challenges like limited supervision in ambiguous areas and the unreliability of pseudo-labels by employing a dual-group, dual-student framework with a shared teacher network. It introduces two key mechanisms: Disagreement-Focused Supervision (DFS) to enhance learning in uncertain regions, and Confidence-Aware Learning (CAL) to stabilize predictions by focusing on high-confidence areas. Experimental results demonstrate that RAIL outperforms existing methods, making it a significant advancement in the field of medical image segmentation.'}, 'zh': {'title': 'åŒºåŸŸæ„ŸçŸ¥æŒ‡å¯¼å­¦ä¹ ï¼šæå‡3Dç‰™é½¿åˆ†å‰²çš„åŠç›‘ç£æ–¹æ³•', 'desc': 'åŠç›‘ç£å­¦ä¹ åœ¨CBCTæ‰«æçš„3Dç‰™é½¿åˆ†å‰²ä¸­å˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šåœ¨ç»“æ„æ¨¡ç³Šæˆ–æ ‡æ³¨é”™è¯¯çš„åŒºåŸŸï¼Œç›‘ç£è®­ç»ƒä¸­çš„çº æ­£ç›‘ç£æœ‰é™ï¼›ä»¥åŠåœ¨æœªæ ‡æ³¨æ•°æ®ä¸Šï¼Œç”±äºä¸å¯é çš„ä¼ªæ ‡ç­¾å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŒºåŸŸæ„ŸçŸ¥æŒ‡å¯¼å­¦ä¹ ï¼ˆRAILï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŒç»„åŒå­¦ç”Ÿçš„åŠç›‘ç£æ¡†æ¶ï¼Œé€šè¿‡äº¤æ›¿è®­ç»ƒä¿ƒè¿›ç»„é—´çŸ¥è¯†è½¬ç§»å’Œåä½œæŒ‡å¯¼ï¼ŒåŒæ—¶å‡å°‘å¯¹å•ä¸€æ¨¡å‹ç‰¹å¾çš„è¿‡æ‹Ÿåˆã€‚RAILå¼•å…¥äº†ä¸¤ç§æŒ‡å¯¼æœºåˆ¶ï¼Œåˆ†åˆ«æ˜¯å…³æ³¨åˆ†æ­§çš„ç›‘ç£æ§åˆ¶å™¨å’ŒåŸºäºç½®ä¿¡åº¦çš„å­¦ä¹ è°ƒèŠ‚å™¨ï¼Œä»¥æé«˜æ¨¡å‹åœ¨ä¸ç¡®å®šåŒºåŸŸçš„å­¦ä¹ æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.03105', 'title': 'Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI\n  Knowledge Co-Creation', 'url': 'https://huggingface.co/papers/2505.03105', 'abstract': 'Scientific knowledge creation is fundamentally transforming as humans and AI systems evolve beyond tool-user relationships into co-evolutionary epistemic partnerships. When AlphaFold revolutionized protein structure prediction, researchers described engaging with an epistemic partner that reshaped how they conceptualized fundamental relationships. This article introduces Cognitio Emergens (CE), a framework addressing critical limitations in existing models that focus on static roles or narrow metrics while failing to capture how scientific understanding emerges through recursive human-AI interaction over time. CE integrates three components addressing these limitations: Agency Configurations describing how authority distributes between humans and AI (Directed, Contributory, Partnership), with partnerships dynamically oscillating between configurations rather than following linear progression; Epistemic Dimensions capturing six specific capabilities emerging through collaboration across Discovery, Integration, and Projection axes, creating distinctive "capability signatures" that guide development; and Partnership Dynamics identifying forces shaping how these relationships evolve, particularly the risk of epistemic alienation where researchers lose interpretive control over knowledge they formally endorse. Drawing from autopoiesis theory, social systems theory, and organizational modularity, CE reveals how knowledge co-creation emerges through continuous negotiation of roles, values, and organizational structures. By reconceptualizing human-AI scientific collaboration as fundamentally co-evolutionary, CE offers a balanced perspective that neither uncritically celebrates nor unnecessarily fears AI\'s evolving role, instead providing conceptual tools for cultivating partnerships that maintain meaningful human participation while enabling transformative scientific breakthroughs.', 'score': 1, 'issue_id': 3657, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ', 'en': 'May 6', 'zh': '5æœˆ6æ—¥'}, 'hash': '24cdaf99b9b04dad', 'authors': ['Xule Lin'], 'affiliations': ['Department of Management and Entrepreneurship, Imperial College London'], 'pdf_title_img': 'assets/pdf/title_img/2505.03105.jpg', 'data': {'categories': ['#agents', '#healthcare', '#science', '#ethics', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Cognitio Emergens: Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° ĞºĞ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ˜Ğ˜ Ğ² Ğ½Ğ°ÑƒĞºĞµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Cognitio Emergens (CE) - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ…. CE Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚, ĞºĞ°Ğº Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ÑÑ Ñ€Ğ¾Ğ»Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ¸ Ğ˜Ğ˜, ĞºĞ°ĞºĞ¸Ğµ ÑĞ¿Ğ¸ÑÑ‚ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‚ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ¸ ĞºĞ°ĞºĞ¸Ğµ ÑĞ¸Ğ»Ñ‹ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¸ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¿Ğ°Ñ€Ñ‚Ğ½ĞµÑ€ÑÑ‚Ğ²Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ˜Ğ˜. CE Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ñ€Ğ¾Ğ»ÑŒ Ğ˜Ğ˜ Ğ² Ğ½Ğ°ÑƒĞºĞµ, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ ĞºĞ°Ğº Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¼Ğ°, Ñ‚Ğ°Ğº Ğ¸ Ğ½ĞµĞ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ…Ğ¾Ğ².'}, 'en': {'title': 'Transforming Scientific Collaboration: Humans and AI as Co-Evolutionary Partners', 'desc': 'This paper discusses how the relationship between humans and AI in scientific research is changing from a simple tool-user dynamic to a more collaborative partnership. It introduces a new framework called Cognitio Emergens (CE) that addresses the limitations of existing models by focusing on the evolving roles and interactions between humans and AI over time. CE includes three main components: Agency Configurations that describe how authority is shared, Epistemic Dimensions that outline capabilities developed through collaboration, and Partnership Dynamics that explore how these relationships change. By viewing human-AI collaboration as a co-evolutionary process, the framework aims to enhance scientific understanding while ensuring that human input remains significant in the face of advancing AI capabilities.'}, 'zh': {'title': 'äººç±»ä¸AIçš„å…±åŒè¿›åŒ–ï¼šçŸ¥è¯†åˆ›é€ çš„æ–°è§†è§’', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†äººç±»ä¸äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ä¹‹é—´çš„åˆä½œå…³ç³»å¦‚ä½•ä»ç®€å•çš„å·¥å…·ä½¿ç”¨è€…è½¬å˜ä¸ºå…±åŒè¿›åŒ–çš„çŸ¥è¯†ä¼™ä¼´ã€‚æ–‡ç« ä»‹ç»äº†Cognitio Emergensï¼ˆCEï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹çš„å±€é™æ€§ï¼Œå¼ºè°ƒç§‘å­¦ç†è§£æ˜¯å¦‚ä½•é€šè¿‡äººç±»ä¸AIçš„äº’åŠ¨é€æ­¥å½¢æˆçš„ã€‚CEæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼šä»£ç†é…ç½®ã€è®¤çŸ¥ç»´åº¦å’Œä¼™ä¼´åŠ¨æ€ï¼Œå¸®åŠ©æè¿°äººç±»ä¸AIä¹‹é—´çš„æƒåŠ›åˆ†é…ã€åˆä½œèƒ½åŠ›å’Œå…³ç³»æ¼”å˜ã€‚é€šè¿‡é‡æ–°å®šä¹‰äººç±»ä¸AIçš„ç§‘å­¦åˆä½œï¼ŒCEæä¾›äº†ä¿ƒè¿›æœ‰æ„ä¹‰çš„äººç±»å‚ä¸å’Œç§‘å­¦çªç ´çš„æ¦‚å¿µå·¥å…·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02820', 'title': 'AutoLibra: Agent Metric Induction from Open-Ended Feedback', 'url': 'https://huggingface.co/papers/2505.02820', 'abstract': 'Agents are predominantly evaluated and optimized via task success metrics, which are coarse, rely on manual design from experts, and fail to reward intermediate emergent behaviors. We propose AutoLibra, a framework for agent evaluation, that transforms open-ended human feedback, e.g., "If you find that the button is disabled, don\'t click it again", or "This agent has too much autonomy to decide what to do on its own", into metrics for evaluating fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by grounding feedback to an agent\'s behavior, clustering similar positive and negative behaviors, and creating concrete metrics with clear definitions and concrete examples, which can be used for prompting LLM-as-a-Judge as evaluators. We further propose two meta-metrics to evaluate the alignment of a set of (induced) metrics with open feedback: "coverage" and "redundancy". Through optimizing these meta-metrics, we experimentally demonstrate AutoLibra\'s ability to induce more concrete agent evaluation metrics than the ones proposed in previous agent evaluation benchmarks and discover new metrics to analyze agents. We also present two applications of AutoLibra in agent improvement: First, we show that AutoLibra-induced metrics serve as better prompt-engineering targets than the task success rate on a wide range of text game tasks, improving agent performance over baseline by a mean of 20%. Second, we show that AutoLibra can iteratively select high-quality fine-tuning data for web navigation agents. Our results suggest that AutoLibra is a powerful task-agnostic tool for evaluating and improving language agents.', 'score': 1, 'issue_id': 3661, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ', 'en': 'May 5', 'zh': '5æœˆ5æ—¥'}, 'hash': '52333e4c868e25f8', 'authors': ['Hao Zhu', 'Phil Cuvin', 'Xinkai Yu', 'Charlotte Ka Yee Yan', 'Jason Zhang', 'Diyi Yang'], 'affiliations': ['stanford.edu', 'upenn.edu', 'utoronto.ca'], 'pdf_title_img': 'assets/pdf/title_img/2505.02820.jpg', 'data': {'categories': ['#benchmark', '#agents', '#rlhf', '#alignment', '#optimization'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'AutoLibra: Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹', 'desc': "AutoLibra - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ñ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑÑ…Ğ¾Ğ¶Ğ¸Ñ… Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ‚ĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸. AutoLibra Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ°-Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ 'Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ' Ğ¸ 'Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ' Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ AutoLibra ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."}, 'en': {'title': 'Transforming Feedback into Fine-Grained Agent Evaluation Metrics', 'desc': "The paper introduces AutoLibra, a novel framework for evaluating agents using fine-grained metrics derived from open-ended human feedback. It transforms qualitative feedback into quantitative metrics by clustering behaviors and defining clear examples, allowing for a more nuanced assessment of agent performance. AutoLibra also proposes two meta-metrics, 'coverage' and 'redundancy', to ensure that the evaluation metrics align well with the feedback provided. Experimental results show that AutoLibra improves agent performance significantly compared to traditional success metrics, making it a valuable tool for enhancing language agents."}, 'zh': {'title': 'AutoLibraï¼šæ™ºèƒ½ä½“è¯„ä¼°çš„æ–°æ ‡å‡†', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAutoLibraçš„æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°æ™ºèƒ½ä½“çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†å¼€æ”¾å¼äººç±»åé¦ˆè½¬åŒ–ä¸ºç»†ç²’åº¦çš„è¯„ä¼°æŒ‡æ ‡ï¼Œæ¥æ›´å¥½åœ°è¡¡é‡æ™ºèƒ½ä½“çš„è¡Œä¸ºã€‚AutoLibraé€šè¿‡å¯¹åé¦ˆè¿›è¡Œå½’ç±»å’Œå®šä¹‰ï¼Œç”Ÿæˆå…·ä½“çš„è¯„ä¼°æ ‡å‡†ï¼Œå¹¶åˆ©ç”¨è¿™äº›æ ‡å‡†æ¥ä¼˜åŒ–æ™ºèƒ½ä½“çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAutoLibraèƒ½å¤Ÿç”Ÿæˆæ¯”ç°æœ‰åŸºå‡†æ›´æœ‰æ•ˆçš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶åœ¨å¤šç§ä»»åŠ¡ä¸­æ˜¾è‘—æå‡æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.02393', 'title': 'Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly\n  Detection', 'url': 'https://huggingface.co/papers/2505.02393', 'abstract': 'Most existing video anomaly detectors rely solely on RGB frames, which lack the temporal resolution needed to capture abrupt or transient motion cues, key indicators of anomalous events. To address this limitation, we propose Image-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that synthesizes event representations directly from RGB videos and fuses them with image features through a principled, uncertainty-aware process. The system (i) models heavy-tailed sensor noise with a Student`s-t likelihood, deriving value-level inverse-variance weights via a Laplace approximation; (ii) applies Kalman-style frame-wise updates to balance modalities over time; and (iii) iteratively refines the fused latent state to erase residual cross-modal noise. Without any dedicated event sensor or frame-level labels, IEF-VAD sets a new state of the art across multiple real-world anomaly detection benchmarks. These findings highlight the utility of synthetic event representations in emphasizing motion cues that are often underrepresented in RGB frames, enabling accurate and robust video understanding across diverse applications without requiring dedicated event sensors. Code and models are available at https://github.com/EavnJeong/IEF-VAD.', 'score': 1, 'issue_id': 3651, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ', 'en': 'May 5', 'zh': '5æœˆ5æ—¥'}, 'hash': 'b5c708abbb25e1ce', 'authors': ['Sungheon Jeong', 'Jihong Park', 'Mohsen Imani'], 'affiliations': ['MOLOCO', 'University of California, Irvine'], 'pdf_title_img': 'assets/pdf/title_img/2505.02393.jpg', 'data': {'categories': ['#video', '#benchmark', '#multimodal', '#synthetic'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸Ğ· RGB Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ IEF-VAD Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ RGB-ĞºĞ°Ğ´Ñ€Ñ‹ Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑˆÑƒĞ¼ Ğ´Ğ°Ñ‚Ñ‡Ğ¸ĞºĞ°, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ° ĞšĞ°Ğ»Ğ¼Ğ°Ğ½Ğ° Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ ÑĞ»Ğ¸Ñ‚Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ. IEF-VAD Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Enhancing Video Anomaly Detection with Image-Event Fusion', 'desc': 'The paper introduces a new method called Image-Event Fusion for Video Anomaly Detection (IEF-VAD) that improves the detection of unusual events in videos. Traditional methods rely only on RGB frames, which can miss important motion details. IEF-VAD combines RGB video data with synthetic event representations to enhance the detection process, using advanced techniques to manage noise and improve accuracy. This approach achieves state-of-the-art results in various benchmarks without needing special sensors or labeled data.'}, 'zh': {'title': 'å›¾åƒä¸äº‹ä»¶èåˆï¼Œæå‡è§†é¢‘å¼‚å¸¸æ£€æµ‹çš„å‡†ç¡®æ€§', 'desc': 'ç°æœ‰çš„è§†é¢‘å¼‚å¸¸æ£€æµ‹å™¨ä¸»è¦ä¾èµ–RGBå¸§ï¼Œä½†è¿™äº›å¸§ç¼ºä¹æ•æ‰çªå‘æˆ–ç¬æ€è¿åŠ¨çº¿ç´¢çš„æ—¶é—´åˆ†è¾¨ç‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å›¾åƒ-äº‹ä»¶èåˆçš„è§†é¢‘å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼ˆIEF-VADï¼‰ï¼Œè¯¥æ¡†æ¶ç›´æ¥ä»RGBè§†é¢‘åˆæˆäº‹ä»¶è¡¨ç¤ºï¼Œå¹¶é€šè¿‡ä¸€ç§åŸºäºä¸ç¡®å®šæ€§çš„è¿‡ç¨‹å°†å…¶ä¸å›¾åƒç‰¹å¾èåˆã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼å»ºæ¨¡é‡å°¾ä¼ æ„Ÿå™¨å™ªå£°ï¼Œåº”ç”¨å¡å°”æ›¼é£æ ¼çš„é€å¸§æ›´æ–°æ¥å¹³è¡¡æ—¶é—´ä¸Šçš„æ¨¡æ€ï¼Œå¹¶è¿­ä»£ä¼˜åŒ–èåˆçš„æ½œåœ¨çŠ¶æ€ä»¥æ¶ˆé™¤æ®‹ä½™çš„è·¨æ¨¡æ€å™ªå£°ã€‚IEF-VADåœ¨å¤šä¸ªçœŸå®ä¸–ç•Œçš„å¼‚å¸¸æ£€æµ‹åŸºå‡†ä¸Šè®¾å®šäº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œå±•ç¤ºäº†åˆæˆäº‹ä»¶è¡¨ç¤ºåœ¨å¼ºè°ƒRGBå¸§ä¸­å¸¸è¢«ä½ä¼°çš„è¿åŠ¨çº¿ç´¢æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.01449', 'title': 'COSMOS: Predictable and Cost-Effective Adaptation of LLMs', 'url': 'https://huggingface.co/papers/2505.01449', 'abstract': 'Large language models (LLMs) achieve remarkable performance across numerous tasks by using a diverse array of adaptation strategies. However, optimally selecting a model and adaptation strategy under resource constraints is challenging and often requires extensive experimentation. We investigate whether it is possible to accurately predict both performance and cost without expensive trials. We formalize the strategy selection problem for LLMs and introduce COSMOS, a unified prediction framework that efficiently estimates adaptation outcomes at minimal cost. We instantiate and study the capability of our framework via a pair of powerful predictors: embedding-augmented lightweight proxy models to predict fine-tuning performance, and low-sample scaling laws to forecast retrieval-augmented in-context learning. Extensive evaluation across eight representative benchmarks demonstrates that COSMOS achieves high prediction accuracy while reducing computational costs by 92.72% on average, and up to 98.71% in resource-intensive scenarios. Our results show that efficient prediction of adaptation outcomes is not only feasible but can substantially reduce the computational overhead of LLM deployment while maintaining performance standards.', 'score': 1, 'issue_id': 3665, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 30', 'zh': '4æœˆ30æ—¥'}, 'hash': 'cc6993ec1bb06272', 'authors': ['Jiayu Wang', 'Aws Albarghouthi', 'Frederic Sala'], 'affiliations': ['University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2505.01449.jpg', 'data': {'categories': ['#optimization', '#data', '#inference', '#training', '#benchmark'], 'emoji': 'ğŸ”®', 'ru': {'title': 'COSMOS: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ COSMOS - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞºÑĞ¸-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ fine-tuning Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° COSMOS Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 92.72%.'}, 'en': {'title': 'COSMOS: Smart Predictions for Efficient LLM Adaptation', 'desc': 'This paper addresses the challenge of selecting the best model and adaptation strategy for large language models (LLMs) while managing resource constraints. It introduces COSMOS, a unified prediction framework designed to estimate the performance and cost of different adaptation strategies without the need for extensive trials. The framework utilizes lightweight proxy models and low-sample scaling laws to predict outcomes efficiently. The results indicate that COSMOS can significantly reduce computational costs while maintaining high prediction accuracy across various benchmarks.'}, 'zh': {'title': 'COSMOSï¼šé«˜æ•ˆé¢„æµ‹é€‚åº”ç»“æœçš„æ¡†æ¶', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹é€‰æ‹©æœ€ä½³æ¨¡å‹å’Œé€‚åº”ç­–ç•¥éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æå‡ºäº†COSMOSï¼Œä¸€ä¸ªç»Ÿä¸€çš„é¢„æµ‹æ¡†æ¶ï¼Œå¯ä»¥åœ¨æœ€ä½æˆæœ¬ä¸‹é«˜æ•ˆä¼°è®¡é€‚åº”ç»“æœã€‚è¯¥æ¡†æ¶åˆ©ç”¨è½»é‡çº§ä»£ç†æ¨¡å‹å’Œä½æ ·æœ¬æ‰©å±•æ³•åˆ™æ¥é¢„æµ‹å¾®è°ƒæ€§èƒ½å’Œæ£€ç´¢å¢å¼ºçš„ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒCOSMOSåœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é«˜é¢„æµ‹å‡†ç¡®æ€§ï¼ŒåŒæ—¶å¹³å‡å‡å°‘äº†92.72%çš„è®¡ç®—æˆæœ¬ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (6)', '#agi', '#alignment (2)', '#architecture (3)', '#audio', '#benchmark (9)', '#cv (2)', '#data (3)', '#dataset (5)', '#diffusion (1)', '#ethics (1)', '#games (2)', '#graphs', '#hallucinations (1)', '#healthcare (1)', '#inference (1)', '#interpretability (1)', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math (2)', '#multilingual (1)', '#multimodal (11)', '#open_source (5)', '#optimization (9)', '#plp', '#rag (1)', '#reasoning (6)', '#rl (2)', '#rlhf (2)', '#robotics', '#science (2)', '#security', '#small_models (1)', '#story_generation', '#survey (2)', '#synthetic (1)', '#training (7)', '#transfer_learning (1)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-05-09 00:53',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-05-09 00:53')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-05-09 00:53')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    