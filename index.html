
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 16 papers. May 28.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">28 мая</span> | <span id="title-articles-count">16 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-05-27.html">⬅️ <span id="prev-date">27.05</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-05-29.html">➡️ <span id="next-date">29.05</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-05.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'};
        let feedDateNext = {'ru': '29.05', 'en': '05/29', 'zh': '5月29日'};
        let feedDatePrev = {'ru': '27.05', 'en': '05/27', 'zh': '5月27日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2505.21497', 'title': 'Paper2Poster: Towards Multimodal Poster Automation from Scientific\n  Papers', 'url': 'https://huggingface.co/papers/2505.21497', 'abstract': "Academic poster generation is a crucial yet challenging task in scientific communication, requiring the compression of long-context interleaved documents into a single, visually coherent page. To address this challenge, we introduce the first benchmark and metric suite for poster generation, which pairs recent conference papers with author-designed posters and evaluates outputs on (i)Visual Quality-semantic alignment with human posters, (ii)Textual Coherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic and informational criteria scored by a VLM-as-judge, and notably (iv)PaperQuiz-the poster's ability to convey core paper content as measured by VLMs answering generated quizzes. Building on this benchmark, we propose PosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser distills the paper into a structured asset library; the (b)Planner aligns text-visual pairs into a binary-tree layout that preserves reading order and spatial balance; and the (c)Painter-Commenter loop refines each panel by executing rendering code and using VLM feedback to eliminate overflow and ensure alignment. In our comprehensive evaluation, we find that GPT-4o outputs-though visually appealing at first glance-often exhibit noisy text and poor PaperQuiz scores, and we find that reader engagement is the primary aesthetic bottleneck, as human-designed posters rely largely on visual semantics to convey meaning. Our fully open-source variants (e.g. based on the Qwen-2.5 series) outperform existing 4o-driven multi-agent systems across nearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper into a finalized yet editable .pptx poster - all for just $0.005. These findings chart clear directions for the next generation of fully automated poster-generation models. The code and datasets are available at https://github.com/Paper2Poster/Paper2Poster.", 'score': 17, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '7f740f76be754bce', 'authors': ['Wei Pang', 'Kevin Qinghong Lin', 'Xiangru Jian', 'Xi He', 'Philip Torr'], 'affiliations': ['National University of Singapore', 'University of Oxford', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2505.21497.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#agents', '#science'], 'emoji': '🖼️', 'ru': {'title': 'Автоматическая генерация научных постеров: от статьи к визуализации', 'desc': 'Эта статья представляет первый эталонный тест и набор метрик для генерации академических постеров, сопоставляя недавние научные статьи с постерами, созданными авторами. Исследователи предлагают PosterAgent - многоагентный конвейер, который включает в себя Parser для извлечения ключевой информации, Planner для создания структуры постера и Painter-Commenter для визуального оформления. Оценка показывает, что открытые модели на основе Qwen-2.5 превосходят существующие системы по большинству метрик, используя на 87% меньше токенов. Исследование открывает путь к следующему поколению полностью автоматизированных моделей для создания постеров.'}, 'en': {'title': 'Revolutionizing Academic Poster Generation with PosterAgent', 'desc': 'This paper addresses the challenge of generating academic posters from lengthy scientific documents by introducing a benchmark and metric suite for evaluation. It presents PosterAgent, a multi-agent pipeline that includes a Parser for structuring content, a Planner for layout design, and a Painter-Commenter loop for refining visuals based on feedback. The study evaluates the effectiveness of generated posters using metrics like visual quality, textual coherence, and the ability to convey core content through quizzes. The results show that their open-source approach significantly outperforms existing models while being more efficient in token usage, paving the way for future advancements in automated poster generation.'}, 'zh': {'title': '自动化学术海报生成的新纪元', 'desc': '本论文介绍了一种新的学术海报生成基准和评估指标，旨在将长篇文档压缩为视觉上连贯的单页海报。我们提出了PosterAgent，一个多代理管道，能够有效地解析、规划和绘制海报内容。通过对比不同模型的输出，我们发现人类设计的海报在视觉语义上更具吸引力，而GPT-4o模型虽然外观美观，但文本质量和信息传达能力较差。我们的开源变体在多个指标上超越了现有系统，并且显著减少了所需的计算资源。'}}}, {'id': 'https://huggingface.co/papers/2505.18445', 'title': 'OmniConsistency: Learning Style-Agnostic Consistency from Paired\n  Stylization Data', 'url': 'https://huggingface.co/papers/2505.18445', 'abstract': "OmniConsistency, using large-scale Diffusion Transformers, enhances stylization consistency and generalization in image-to-image pipelines without style degradation.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have advanced image stylization significantly, yet two core challenges persist: (1) maintaining consistent stylization in complex scenes, particularly identity, composition, and fine details, and (2) preventing style degradation in image-to-image pipelines with style LoRAs. GPT-4o's exceptional stylization consistency highlights the performance gap between open-source methods and proprietary models. To bridge this gap, we propose OmniConsistency, a universal consistency plugin leveraging large-scale Diffusion Transformers (DiTs). OmniConsistency contributes: (1) an in-context consistency learning framework trained on aligned image pairs for robust generalization; (2) a two-stage progressive learning strategy decoupling style learning from consistency preservation to mitigate style degradation; and (3) a fully plug-and-play design compatible with arbitrary style LoRAs under the Flux framework. Extensive experiments show that OmniConsistency significantly enhances visual coherence and aesthetic quality, achieving performance comparable to commercial state-of-the-art model GPT-4o.", 'score': 15, 'issue_id': 3990, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 мая', 'en': 'May 24', 'zh': '5月24日'}, 'hash': '0a5e56835e542da2', 'authors': ['Yiren Song', 'Cheng Liu', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2505.18445.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#diffusion', '#cv', '#training'], 'emoji': '🎨', 'ru': {'title': 'Универсальная согласованность стиля в генерации изображений', 'desc': 'OmniConsistency - это универсальный плагин для улучшения согласованности стилизации в задачах преобразования изображений. Он использует крупномасштабные Диффузионные Трансформеры (DiTs) и обучается на парах выровненных изображений для лучшей генерализации. Плагин применяет двухэтапную стратегию прогрессивного обучения, разделяющую изучение стиля и сохранение согласованности. OmniConsistency совместим с произвольными стилевыми LoRA и значительно повышает визуальную согласованность и эстетическое качество изображений.'}, 'en': {'title': 'Achieving Consistent and High-Quality Image Stylization with OmniConsistency', 'desc': 'OmniConsistency is a novel approach that improves the consistency and generalization of image stylization using large-scale Diffusion Transformers. It addresses two main challenges in image-to-image pipelines: ensuring consistent stylization across complex scenes and preventing degradation of style when using style LoRAs. The method introduces a learning framework that focuses on maintaining consistency while allowing for flexible style application. Experimental results demonstrate that OmniConsistency achieves visual quality and coherence comparable to leading commercial models.'}, 'zh': {'title': 'OmniConsistency：提升图像风格一致性的创新方案', 'desc': 'OmniConsistency 是一种利用大规模扩散变换器（Diffusion Transformers）来增强图像到图像管道中的风格一致性和泛化能力的方法。该方法解决了在复杂场景中保持一致风格和防止风格退化的两个主要挑战。OmniConsistency 提供了一种基于对齐图像对的上下文一致性学习框架，并采用两阶段的渐进学习策略来分离风格学习与一致性保持。实验结果表明，OmniConsistency 显著提高了视觉连贯性和美学质量，性能可与商业最先进模型 GPT-4o 相媲美。'}}}, {'id': 'https://huggingface.co/papers/2505.18875', 'title': 'Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via\n  Semantic-Aware Permutation', 'url': 'https://huggingface.co/papers/2505.18875', 'abstract': 'SVG2 is a training-free framework that enhances video generation efficiency and quality by accurately identifying and processing critical tokens using semantic-aware permutation and dynamic budget control.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Transformers (DiTs) are essential for video generation but suffer from significant latency due to the quadratic complexity of attention. By computing only critical tokens, sparse attention reduces computational costs and offers a promising acceleration approach. However, we identify that existing methods fail to approach optimal generation quality under the same computation budget for two reasons: (1) Inaccurate critical token identification: current methods cluster tokens based on position rather than semantics, leading to imprecise aggregated representations. (2) Excessive computation waste: critical tokens are scattered among non-critical ones, leading to wasted computation on GPUs, which are optimized for processing contiguous tokens. In this paper, we propose SVG2, a training-free framework that maximizes identification accuracy and minimizes computation waste, achieving a Pareto frontier trade-off between generation quality and efficiency. The core of SVG2 is semantic-aware permutation, which clusters and reorders tokens based on semantic similarity using k-means. This approach ensures both a precise cluster representation, improving identification accuracy, and a densified layout of critical tokens, enabling efficient computation without padding. Additionally, SVG2 integrates top-p dynamic budget control and customized kernel implementations, achieving up to 2.30x and 1.89x speedup while maintaining a PSNR of up to 30 and 26 on HunyuanVideo and Wan 2.1, respectively.', 'score': 13, 'issue_id': 3990, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 мая', 'en': 'May 24', 'zh': '5月24日'}, 'hash': 'bc68e232d8897ad4', 'authors': ['Shuo Yang', 'Haocheng Xi', 'Yilong Zhao', 'Muyang Li', 'Jintao Zhang', 'Han Cai', 'Yujun Lin', 'Xiuyu Li', 'Chenfeng Xu', 'Kelly Peng', 'Jianfei Chen', 'Song Han', 'Kurt Keutzer', 'Ion Stoica'], 'affiliations': ['MIT', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2505.18875.jpg', 'data': {'categories': ['#diffusion', '#training', '#video', '#optimization'], 'emoji': '🎞️', 'ru': {'title': 'Семантическая оптимизация для быстрой и качественной генерации видео', 'desc': 'SVG2 - это фреймворк для улучшения эффективности и качества генерации видео без дополнительного обучения. Он использует семантически-ориентированную перестановку для точной идентификации и обработки критических токенов. SVG2 применяет кластеризацию k-means для группировки токенов по семантическому сходству, что повышает точность представления. Фреймворк также включает динамический контроль бюджета top-p и оптимизированные ядра, достигая ускорения до 2.30x при сохранении высокого качества генерации.'}, 'en': {'title': 'Maximizing Video Generation Efficiency with SVG2', 'desc': 'SVG2 is a novel framework designed to improve the efficiency and quality of video generation without the need for extensive training. It focuses on accurately identifying critical tokens through semantic-aware permutation, which groups tokens based on their meanings rather than just their positions. This method reduces computational waste by ensuring that critical tokens are processed together, optimizing GPU usage. By implementing dynamic budget control, SVG2 achieves significant speed improvements while maintaining high video quality, demonstrating a balance between performance and resource efficiency.'}, 'zh': {'title': 'SVG2：提升视频生成效率与质量的创新框架', 'desc': 'SVG2是一个无需训练的框架，通过准确识别和处理关键标记，提升视频生成的效率和质量。它采用语义感知的排列和动态预算控制，解决了现有方法在计算预算下生成质量不佳的问题。SVG2通过k-means聚类和重新排列标记，确保了精确的聚类表示，从而提高了识别准确性，并减少了计算浪费。该框架在保持生成质量的同时，实现了高达2.30倍的加速。'}}}, {'id': 'https://huggingface.co/papers/2505.21374', 'title': 'Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?', 'url': 'https://huggingface.co/papers/2505.21374', 'abstract': 'Video-Holmes benchmark evaluates complex video reasoning capabilities of MLLMs using suspense short films and reveals significant challenges in information integration compared to human experts.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in CoT reasoning and RL post-training have been reported to enhance video reasoning capabilities of MLLMs. This progress naturally raises a question: can these models perform complex video reasoning in a manner comparable to human experts? However, existing video benchmarks primarily evaluate visual perception and grounding abilities, with questions that can be answered based on explicit prompts or isolated visual cues. Such benchmarks do not fully capture the intricacies of real-world reasoning, where humans must actively search for, integrate, and analyze multiple clues before reaching a conclusion. To address this issue, we present Video-Holmes, a benchmark inspired by the reasoning process of Sherlock Holmes, designed to evaluate the complex video reasoning capabilities of MLLMs. Video-Holmes consists of 1,837 questions derived from 270 manually annotated suspense short films, which spans seven carefully designed tasks. Each task is constructed by first identifying key events and causal relationships within films, and then designing questions that require models to actively locate and connect multiple relevant visual clues scattered across different video segments. Our comprehensive evaluation of state-of-the-art MLLMs reveals that, while these models generally excel at visual perception, they encounter substantial difficulties with integrating information and often miss critical clues. For example, the best-performing model, Gemini-2.5-Pro, achieves an accuracy of only 45%, with most models scoring below 40%. We aim that Video-Holmes can serve as a "Holmes-test" for multimodal reasoning, motivating models to reason more like humans and emphasizing the ongoing challenges in this field. The benchmark is released in https://github.com/TencentARC/Video-Holmes.', 'score': 12, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '0683137770e562ab', 'authors': ['Junhao Cheng', 'Yuying Ge', 'Teng Wang', 'Yixiao Ge', 'Jing Liao', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'City University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.21374.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#benchmark', '#video'], 'emoji': '🕵️', 'ru': {'title': 'Шерлок Холмс для ИИ: новый вызов в понимании видео', 'desc': 'Video-Holmes - это новый бенчмарк для оценки способностей мультимодальных языковых моделей к сложным рассуждениям на основе видео. Он использует короткометражные фильмы-саспенс и состоит из 1837 вопросов по 270 аннотированным видео, охватывающих 7 специально разработанных задач. Бенчмарк выявил значительные трудности современных моделей в интеграции информации и поиске ключевых подсказок по сравнению с экспертами-людьми. Лучшая модель Gemini-2.5-Pro достигла точности всего 45%, что подчеркивает сложность задачи и необходимость дальнейших исследований в этой области.'}, 'en': {'title': 'Video-Holmes: A New Benchmark for Complex Video Reasoning', 'desc': 'The Video-Holmes benchmark assesses the complex video reasoning abilities of Multimodal Language Models (MLLMs) using suspense short films. It highlights the challenges these models face in integrating information compared to human experts, particularly in real-world reasoning scenarios. The benchmark includes 1,837 questions based on 270 annotated films, requiring models to connect multiple visual clues across different segments. Despite advancements in reasoning techniques, the evaluation shows that even the best models struggle with accuracy, achieving only 45%, indicating significant room for improvement in multimodal reasoning.'}, 'zh': {'title': 'Video-Holmes：激励模型更像人类推理的基准测试', 'desc': 'Video-Holmes基准测试评估了多模态大语言模型（MLLMs）在复杂视频推理方面的能力，特别是通过悬疑短片来揭示与人类专家相比的信息整合挑战。该基准包含来自270部手动注释的悬疑短片的1837个问题，设计了七个任务，要求模型主动寻找和连接分散在不同视频片段中的多个相关视觉线索。尽管现有的MLLMs在视觉感知方面表现良好，但在信息整合上却面临重大困难，许多模型的准确率低于40%。我们希望Video-Holmes能够激励模型更像人类进行推理，并强调这一领域的持续挑战。'}}}, {'id': 'https://huggingface.co/papers/2505.21297', 'title': 'rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale\n  Verified Dataset', 'url': 'https://huggingface.co/papers/2505.21297', 'abstract': 'A large-scale dataset called rStar-Coder enhances code reasoning in LLMs by providing verified code problems and solutions, leading to improved performance on various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Advancing code reasoning in large language models (LLMs) is fundamentally limited by the scarcity of high-difficulty datasets, especially those with verifiable input-output test cases necessary for rigorous solution validation at scale. We introduce rStar-Coder, which significantly improves LLM code reasoning capabilities by constructing a large-scale, verified dataset of 418K competition-level code problems, 580K long-reasoning solutions along with rich test cases of varying difficulty. This is achieved through three core contributions: (1) we curate competitive programming code problems and oracle solutions to synthesize new, solvable problems; (2) we introduce a reliable input-output test case synthesis pipeline that decouples the generation into a three-step input generation method and a mutual verification mechanism for effective output labeling; (3) we augment problems with high-quality, test-case-verified long-reasoning solutions. Extensive experiments on Qwen models (1.5B-14B) across various code reasoning benchmarks demonstrate the superiority of rStar-Coder dataset, achieving leading performance comparable to frontier reasoning LLMs with much smaller model sizes. On LiveCodeBench, rStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%, and Qwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by3.1%. On the more challenging USA Computing Olympiad, our 7B model achieves an average pass@1 accuracy of 16.15%, outperforming the frontier-level QWQ-32B. Code and the dataset will be released at https://github.com/microsoft/rStar.', 'score': 9, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '1fdb1a9edd20c73b', 'authors': ['Yifei Liu', 'Li Lyna Zhang', 'Yi Zhu', 'Bingcheng Dong', 'Xudong Zhou', 'Ning Shang', 'Fan Yang', 'Mao Yang'], 'affiliations': ['Dalian University of Technology', 'Microsoft Research Asia', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21297.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#synthetic', '#reasoning', '#data'], 'emoji': '🧠', 'ru': {'title': 'rStar-Coder: прорыв в обучении языковых моделей рассуждениям о коде', 'desc': 'Исследователи представили rStar-Coder - крупномасштабный датасет для улучшения способностей языковых моделей (LLM) в рассуждениях о коде. Датасет содержит 418 тысяч задач по программированию соревновательного уровня, 580 тысяч подробных решений и тестовые примеры различной сложности. Использование rStar-Coder значительно повысило производительность моделей Qwen на различных бенчмарках, позволив им достичь результатов, сравнимых с передовыми LLM для рассуждений о коде, но при гораздо меньших размерах моделей. На бенчмарке LiveCodeBench модель Qwen2.5-14B улучшила свой результат с 23.3% до 62.5% после обучения на rStar-Coder.'}, 'en': {'title': 'Unlocking Code Reasoning with rStar-Coder', 'desc': 'The paper presents rStar-Coder, a large-scale dataset designed to enhance code reasoning capabilities in large language models (LLMs). It addresses the challenge of limited high-difficulty datasets by providing 418,000 verified code problems and 580,000 long-reasoning solutions, complete with diverse test cases. The dataset is created through a three-step process that includes curating competitive programming problems, synthesizing input-output test cases, and verifying solutions. Experiments show that models trained on rStar-Coder significantly outperform existing benchmarks, demonstrating its effectiveness in improving code reasoning tasks.'}, 'zh': {'title': '提升代码推理能力的rStar-Coder数据集', 'desc': 'rStar-Coder是一个大规模的数据集，旨在提升大语言模型（LLMs）在代码推理方面的能力。该数据集包含418,000个竞争级别的代码问题和580,000个长推理解决方案，配备丰富的测试用例，涵盖不同难度。通过三项核心贡献，rStar-Coder提供了可验证的输入输出测试案例，确保了解决方案的有效性。实验结果显示，使用rStar-Coder的数据集，Qwen模型在多个代码推理基准测试中表现优异，显著提高了模型的准确性。'}}}, {'id': 'https://huggingface.co/papers/2505.20292', 'title': 'OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for\n  Subject-to-Video Generation', 'url': 'https://huggingface.co/papers/2505.20292', 'abstract': "Subject-to-Video (S2V) generation aims to create videos that faithfully incorporate reference content, providing enhanced flexibility in the production of videos. To establish the infrastructure for S2V generation, we propose OpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, and (ii) OpenS2V-5M, a million-scale dataset. In contrast to existing S2V benchmarks inherited from VBench that focus on global and coarse-grained assessment of generated videos, OpenS2V-Eval focuses on the model's ability to generate subject-consistent videos with natural subject appearance and identity fidelity. For these purposes, OpenS2V-Eval introduces 180 prompts from seven major categories of S2V, which incorporate both real and synthetic test data. Furthermore, to accurately align human preferences with S2V benchmarks, we propose three automatic metrics, NexusScore, NaturalScore and GmeScore, to separately quantify subject consistency, naturalness, and text relevance in generated videos. Building on this, we conduct a comprehensive evaluation of 16 representative S2V models, highlighting their strengths and weaknesses across different content. Moreover, we create the first open-source large-scale S2V generation dataset OpenS2V-5M, which consists of five million high-quality 720P subject-text-video triples. Specifically, we ensure subject-information diversity in our dataset by (1) segmenting subjects and building pairing information via cross-video associations and (2) prompting GPT-Image-1 on raw frames to synthesize multi-view representations. Through OpenS2V-Nexus, we deliver a robust infrastructure to accelerate future S2V generation research.", 'score': 9, 'issue_id': 3990, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'e2a8d12789199cde', 'authors': ['Shenghai Yuan', 'Xianyi He', 'Yufan Deng', 'Yang Ye', 'Jinfa Huang', 'Bin Lin', 'Chongyang Ma', 'Jiebo Luo', 'Li Yuan'], 'affiliations': ['Peking University, Shenzhen Graduate School', 'Rabbitpre AI', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2505.20292.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#open_source', '#synthetic', '#video'], 'emoji': '🎬', 'ru': {'title': 'OpenS2V-Nexus: Революция в генерации видео на основе заданного содержания', 'desc': 'Статья представляет OpenS2V-Nexus - инфраструктуру для генерации видео на основе заданного содержания (Subject-to-Video, S2V). Она включает в себя OpenS2V-Eval - детальный бенчмарк для оценки качества генерируемых видео, и OpenS2V-5M - крупномасштабный датасет из 5 миллионов триплетов субъект-текст-видео. Авторы предлагают три автоматические метрики для оценки согласованности субъекта, естественности и релевантности текста в сгенерированных видео. Проведена комплексная оценка 16 репрезентативных S2V моделей, выявляющая их сильные и слабые стороны.'}, 'en': {'title': 'Revolutionizing Video Generation with Subject Fidelity', 'desc': 'The paper introduces Subject-to-Video (S2V) generation, which focuses on creating videos that accurately reflect reference content. It presents OpenS2V-Nexus, a framework that includes OpenS2V-Eval, a detailed benchmark for evaluating video generation, and OpenS2V-5M, a large dataset of five million subject-text-video pairs. Unlike previous benchmarks, OpenS2V-Eval emphasizes the generation of videos that maintain subject consistency and natural appearance. The authors also propose three new metrics to assess generated videos based on subject fidelity, naturalness, and relevance to the input text, facilitating a comprehensive evaluation of various S2V models.'}, 'zh': {'title': '构建视频生成的新基准与数据集', 'desc': '本论文提出了Subject-to-Video (S2V) 生成的基础设施OpenS2V-Nexus，旨在创建忠实于参考内容的视频。我们引入了OpenS2V-Eval，一个细粒度的基准，专注于生成具有自然外观和身份保真度的一致视频。为了评估生成视频的质量，我们设计了三种自动化指标，分别量化主题一致性、自然性和文本相关性。最后，我们创建了一个包含五百万个高质量720P主题-文本-视频三元组的开放源代码数据集OpenS2V-5M，以支持未来的S2V生成研究。'}}}, {'id': 'https://huggingface.co/papers/2505.18943', 'title': 'MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent\n  Systems', 'url': 'https://huggingface.co/papers/2505.18943', 'abstract': "MetaMind, a multi-agent framework inspired by metacognition, enhances LLMs' ability to perform Theory of Mind tasks by decomposing social understanding into hypothesis generation, refinement, and response generation, achieving human-like performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Human social interactions depend on the ability to infer others' unspoken intentions, emotions, and beliefs-a cognitive skill grounded in the psychological concept of Theory of Mind (ToM). While large language models (LLMs) excel in semantic understanding tasks, they struggle with the ambiguity and contextual nuance inherent in human communication. To bridge this gap, we introduce MetaMind, a multi-agent framework inspired by psychological theories of metacognition, designed to emulate human-like social reasoning. MetaMind decomposes social understanding into three collaborative stages: (1) a Theory-of-Mind Agent generates hypotheses user mental states (e.g., intent, emotion), (2) a Domain Agent refines these hypotheses using cultural norms and ethical constraints, and (3) a Response Agent generates contextually appropriate responses while validating alignment with inferred intent. Our framework achieves state-of-the-art performance across three challenging benchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain in ToM reasoning. Notably, it enables LLMs to match human-level performance on key ToM tasks for the first time. Ablation studies confirm the necessity of all components, which showcase the framework's ability to balance contextual plausibility, social appropriateness, and user adaptation. This work advances AI systems toward human-like social intelligence, with applications in empathetic dialogue and culturally sensitive interactions. Code is available at https://github.com/XMZhangAI/MetaMind.", 'score': 8, 'issue_id': 3990, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 мая', 'en': 'May 25', 'zh': '5月25日'}, 'hash': '718f1062d34a47a7', 'authors': ['Xuanming Zhang', 'Yuxuan Chen', 'Min-Hsuan Yeh', 'Yixuan Li'], 'affiliations': ['Tsinghua University', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2505.18943.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#agents', '#reasoning', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'MetaMind: Искусственный интеллект с человеческим социальным пониманием', 'desc': 'MetaMind - это многоагентная система, улучшающая способность больших языковых моделей выполнять задачи теории сознания. Она разбивает социальное понимание на генерацию гипотез, их уточнение и генерацию ответов. MetaMind достигает уровня человека в ключевых задачах теории сознания, показывая улучшение на 35.7% в реальных социальных сценариях. Система использует три агента: агент теории сознания, доменный агент и агент ответов, что позволяет балансировать контекстуальную правдоподобность, социальную уместность и адаптацию к пользователю.'}, 'en': {'title': 'Empowering AI with Human-like Social Intelligence', 'desc': 'MetaMind is a multi-agent framework that enhances large language models (LLMs) by improving their ability to understand human social interactions through Theory of Mind (ToM) tasks. It breaks down social understanding into three key stages: generating hypotheses about mental states, refining these hypotheses with cultural and ethical considerations, and producing contextually appropriate responses. This approach allows LLMs to achieve human-like performance in social reasoning, showing significant improvements in real-world scenarios and ToM reasoning tasks. The framework demonstrates the importance of each component in achieving a balance between contextual relevance and social appropriateness, paving the way for more empathetic AI interactions.'}, 'zh': {'title': 'MetaMind：提升AI的社会智能', 'desc': 'MetaMind是一个多智能体框架，灵感来源于元认知，旨在提升大型语言模型（LLMs）在心智理论任务中的表现。该框架将社会理解分解为三个协作阶段：首先，心智理论代理生成用户心理状态的假设；其次，领域代理利用文化规范和伦理约束来细化这些假设；最后，响应代理生成符合上下文的适当回应。通过这种方式，MetaMind在三个具有挑战性的基准测试中实现了最先进的性能，首次使LLMs在关键的心智理论任务上达到人类水平。'}}}, {'id': 'https://huggingface.co/papers/2505.21333', 'title': 'MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in\n  Video Scenarios', 'url': 'https://huggingface.co/papers/2505.21333', 'abstract': 'MLLMs achieve modest accuracy in video OCR due to motion blur, temporal variations, and visual effects; MME-VideoOCR benchmark reveals limitations in spatio-temporal reasoning and language bias.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have achieved considerable accuracy in Optical Character Recognition (OCR) from static images. However, their efficacy in video OCR is significantly diminished due to factors such as motion blur, temporal variations, and visual effects inherent in video content. To provide clearer guidance for training practical MLLMs, we introduce the MME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR application scenarios. MME-VideoOCR features 10 task categories comprising 25 individual tasks and spans 44 diverse scenarios. These tasks extend beyond text recognition to incorporate deeper comprehension and reasoning of textual content within videos. The benchmark consists of 1,464 videos with varying resolutions, aspect ratios, and durations, along with 2,000 meticulously curated, manually annotated question-answer pairs. We evaluate 18 state-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing model (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained analysis indicates that while existing MLLMs demonstrate strong performance on tasks where relevant texts are contained within a single or few frames, they exhibit limited capability in effectively handling tasks that demand holistic video comprehension. These limitations are especially evident in scenarios that require spatio-temporal reasoning, cross-frame information integration, or resistance to language prior bias. Our findings also highlight the importance of high-resolution visual input and sufficient temporal coverage for reliable OCR in dynamic video scenarios.', 'score': 5, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '25639980b7f7add5', 'authors': ['Yang Shi', 'Huanqian Wang', 'Wulin Xie', 'Huanyao Zhang', 'Lijie Zhao', 'Yi-Fan Zhang', 'Xinfeng Li', 'Chaoyou Fu', 'Zhuoer Wen', 'Wenting Liu', 'Zhuoran Zhang', 'Xinlong Chen', 'Bohan Zeng', 'Sihan Yang', 'Yuanxing Zhang', 'Pengfei Wan', 'Haotian Wang', 'Wenjing Yang'], 'affiliations': ['CASIA', 'CUHKSZ', 'Kuaishou', 'NTU', 'PKU', 'THU', 'XJTU'], 'pdf_title_img': 'assets/pdf/title_img/2505.21333.jpg', 'data': {'categories': ['#multimodal', '#games', '#benchmark', '#reasoning', '#video'], 'emoji': '🎥', 'ru': {'title': 'Ограничения мультимодальных моделей в задаче OCR на видео', 'desc': 'Мультимодальные большие языковые модели (MLLM) показывают невысокую точность в задаче оптического распознавания символов (OCR) на видео из-за размытия при движении, временных вариаций и визуальных эффектов. Авторы представляют бенчмарк MME-VideoOCR, включающий 10 категорий задач и 25 отдельных заданий для оценки возможностей MLLM в видео OCR. Эксперименты на 18 современных MLLM выявили ограничения в пространственно-временном рассуждении и языковых предубеждениях моделей. Исследование подчеркивает важность высокого разрешения и достаточного временного охвата для надежного OCR в динамических видеосценариях.'}, 'en': {'title': 'Enhancing Video OCR: Bridging the Gap in MLLM Performance', 'desc': 'This paper discusses the challenges faced by Multimodal Large Language Models (MLLMs) in performing Optical Character Recognition (OCR) on videos. It highlights that factors like motion blur and temporal variations significantly reduce their accuracy compared to static images. To address these issues, the authors introduce the MME-VideoOCR benchmark, which includes a variety of tasks designed to test spatio-temporal reasoning and language understanding in video contexts. The evaluation of 18 MLLMs reveals that even the best models struggle with comprehensive video comprehension, particularly in scenarios requiring integration of information across multiple frames.'}, 'zh': {'title': '提升视频OCR的多模态基准挑战', 'desc': '多模态大型语言模型（MLLMs）在静态图像的光学字符识别（OCR）中表现良好，但在视频OCR中效果显著下降。这是由于视频内容中的运动模糊、时间变化和视觉效果等因素影响。为了解决这些问题，我们提出了MME-VideoOCR基准，涵盖了多种视频OCR应用场景，包含10个任务类别和25个具体任务。我们的研究表明，现有的MLLMs在处理需要整体视频理解的任务时能力有限，尤其是在时空推理和跨帧信息整合方面。'}}}, {'id': 'https://huggingface.co/papers/2505.20275', 'title': 'ImgEdit: A Unified Image Editing Dataset and Benchmark', 'url': 'https://huggingface.co/papers/2505.20275', 'abstract': 'Recent advancements in generative models have enabled high-fidelity text-to-image generation. However, open-source image-editing models still lag behind their proprietary counterparts, primarily due to limited high-quality data and insufficient benchmarks. To overcome these limitations, we introduce ImgEdit, a large-scale, high-quality image-editing dataset comprising 1.2 million carefully curated edit pairs, which contain both novel and complex single-turn edits, as well as challenging multi-turn tasks. To ensure the data quality, we employ a multi-stage pipeline that integrates a cutting-edge vision-language model, a detection model, a segmentation model, alongside task-specific in-painting procedures and strict post-processing. ImgEdit surpasses existing datasets in both task novelty and data quality. Using ImgEdit, we train ImgEdit-E1, an editing model using Vision Language Model to process the reference image and editing prompt, which outperforms existing open-source models on multiple tasks, highlighting the value of ImgEdit and model design. For comprehensive evaluation, we introduce ImgEdit-Bench, a benchmark designed to evaluate image editing performance in terms of instruction adherence, editing quality, and detail preservation. It includes a basic testsuite, a challenging single-turn suite, and a dedicated multi-turn suite. We evaluate both open-source and proprietary models, as well as ImgEdit-E1, providing deep analysis and actionable insights into the current behavior of image-editing models. The source data are publicly available on https://github.com/PKU-YuanGroup/ImgEdit.', 'score': 5, 'issue_id': 3990, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'ab1a7940b7d4c31c', 'authors': ['Yang Ye', 'Xianyi He', 'Zongjian Li', 'Bin Lin', 'Shenghai Yuan', 'Zhiyuan Yan', 'Bohan Hou', 'Li Yuan'], 'affiliations': ['Peking University, Shenzhen Graduate School', 'Peng Cheng Laboratory', 'Rabbitpre AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.20275.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#open_source', '#optimization', '#cv', '#data'], 'emoji': '🖼️', 'ru': {'title': 'ImgEdit: прорыв в редактировании изображений с помощью ИИ', 'desc': 'Исследователи представили ImgEdit - крупномасштабный набор данных для редактирования изображений, содержащий 1,2 миллиона тщательно отобранных пар изображений до и после редактирования. На основе этого набора данных была обучена модель ImgEdit-E1, использующая мультимодальную языковую модель для обработки изображений и текстовых инструкций. Авторы также разработали бенчмарк ImgEdit-Bench для оценки моделей редактирования изображений. Результаты показывают, что ImgEdit-E1 превосходит существующие открытые модели по нескольким задачам редактирования.'}, 'en': {'title': 'Empowering Open-Source Image Editing with ImgEdit Dataset', 'desc': 'This paper presents ImgEdit, a new dataset designed to improve open-source image-editing models by providing 1.2 million high-quality image edit pairs. The dataset includes both simple and complex editing tasks, ensuring a wide range of challenges for model training. To maintain high data quality, a multi-stage pipeline is used, incorporating advanced models for vision-language processing, detection, and segmentation. The authors also introduce ImgEdit-E1, an editing model that outperforms existing open-source models, and ImgEdit-Bench, a benchmark for evaluating image editing performance across various tasks.'}, 'zh': {'title': 'ImgEdit：高质量图像编辑的突破', 'desc': '最近生成模型的进展使得高保真文本到图像的生成成为可能。然而，开源图像编辑模型仍然落后于专有模型，主要是由于高质量数据的缺乏和基准测试不足。为了解决这些问题，我们推出了ImgEdit，这是一个大规模的高质量图像编辑数据集，包含120万个精心策划的编辑对，涵盖新颖和复杂的单轮编辑以及具有挑战性的多轮任务。我们使用多阶段流程确保数据质量，整合了先进的视觉语言模型、检测模型、分割模型以及特定任务的修复程序和严格的后处理。'}}}, {'id': 'https://huggingface.co/papers/2505.20355', 'title': 'GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient\n  Fine-Tuning', 'url': 'https://huggingface.co/papers/2505.20355', 'abstract': "Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-tuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from a fundamental limitation: overfitting when the bottleneck is widened. It performs best at ranks 32-64, yet its accuracy stagnates or declines at higher ranks, still falling short of full fine-tuning (FFT) performance. We identify the root cause as LoRA's structural bottleneck, which introduces gradient entanglement to the unrelated input channels and distorts gradient propagation. To address this, we introduce a novel structure, Granular Low-Rank Adaptation (GraLoRA) that partitions weight matrices into sub-blocks, each with its own low-rank adapter. With negligible computational or storage cost, GraLoRA overcomes LoRA's limitations, effectively increases the representational capacity, and more closely approximates FFT behavior. Experiments on code generation and commonsense reasoning benchmarks show that GraLoRA consistently outperforms LoRA and other baselines, achieving up to +8.5% absolute gain in Pass@1 on HumanEval+. These improvements hold across model sizes and rank settings, making GraLoRA a scalable and robust solution for PEFT. Code, data, and scripts are available at https://github.com/SqueezeBits/GraLoRA.git", 'score': 3, 'issue_id': 3990, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'd4035428ea14ce6b', 'authors': ['Yeonjoon Jung', 'Daehyun Ahn', 'Hyungjun Kim', 'Taesu Kim', 'Eunhyeok Park'], 'affiliations': ['POSTECH', 'SqueezeBits'], 'pdf_title_img': 'assets/pdf/title_img/2505.20355.jpg', 'data': {'categories': ['#dataset', '#training', '#benchmark', '#optimization'], 'emoji': '🧩', 'ru': {'title': 'GraLoRA: Гранулярная низкоранговая адаптация для эффективной настройки генеративных моделей', 'desc': 'Статья представляет новый метод адаптации моделей машинного обучения под названием Granular Low-Rank Adaptation (GraLoRA). GraLoRA преодолевает ограничения популярного метода Low-Rank Adaptation (LoRA), связанные с переобучением при увеличении ранга. Метод разбивает весовые матрицы на подблоки, каждый со своим низкоранговым адаптером, что позволяет эффективно увеличить репрезентативную способность модели. Эксперименты показывают, что GraLoRA превосходит LoRA и другие базовые методы в задачах генерации кода и здравого смысла, достигая улучшения до 8.5% в метрике Pass@1 на датасете HumanEval+.'}, 'en': {'title': 'GraLoRA: Unlocking the Power of Fine-Tuning with Granular Adaptation', 'desc': 'This paper introduces Granular Low-Rank Adaptation (GraLoRA), a new method designed to improve the performance of Low-Rank Adaptation (LoRA) in fine-tuning generative models. LoRA is effective but struggles with overfitting when the rank is increased, leading to poor accuracy compared to full fine-tuning. GraLoRA addresses this issue by dividing weight matrices into smaller sub-blocks, allowing each to have its own low-rank adapter, which enhances gradient propagation and reduces entanglement. Experimental results demonstrate that GraLoRA significantly outperforms LoRA and other methods, achieving notable improvements in various benchmarks without increasing computational costs.'}, 'zh': {'title': '颗粒低秩适应：超越LoRA的高效微调', 'desc': '低秩适应（LoRA）是一种流行的参数高效微调方法，因其简单有效而受到重视。尽管最近有所改进，LoRA仍然面临一个根本性限制：当瓶颈加宽时容易过拟合。我们提出了一种新结构，称为颗粒低秩适应（GraLoRA），它将权重矩阵划分为子块，每个子块都有自己的低秩适配器，从而克服了LoRA的局限性。实验表明，GraLoRA在代码生成和常识推理基准上表现优于LoRA，具有更强的可扩展性和鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2505.19314', 'title': 'SoloSpeech: Enhancing Intelligibility and Quality in Target Speech\n  Extraction through a Cascaded Generative Pipeline', 'url': 'https://huggingface.co/papers/2505.19314', 'abstract': "Target Speech Extraction (TSE) aims to isolate a target speaker's voice from a mixture of multiple speakers by leveraging speaker-specific cues, typically provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in TSE have primarily employed discriminative models that offer high perceptual quality, these models often introduce unwanted artifacts, reduce naturalness, and are sensitive to discrepancies between training and testing environments. On the other hand, generative models for TSE lag in perceptual quality and intelligibility. To address these challenges, we present SoloSpeech, a novel cascaded generative pipeline that integrates compression, extraction, reconstruction, and correction processes. SoloSpeech features a speaker-embedding-free target extractor that utilizes conditional information from the cue audio's latent space, aligning it with the mixture audio's latent space to prevent mismatches. Evaluated on the widely-used Libri2Mix dataset, SoloSpeech achieves the new state-of-the-art intelligibility and quality in target speech extraction and speech separation tasks while demonstrating exceptional generalization on out-of-domain data and real-world scenarios.", 'score': 2, 'issue_id': 3990, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 мая', 'en': 'May 25', 'zh': '5月25日'}, 'hash': 'd58bb66dfe2fc291', 'authors': ['Helin Wang', 'Jiarui Hai', 'Dongchao Yang', 'Chen Chen', 'Kai Li', 'Junyi Peng', 'Thomas Thebaud', 'Laureano Moro Velazquez', 'Jesus Villalba', 'Najim Dehak'], 'affiliations': ['Brno University of Technology', 'Johns Hopkins University', 'Nanyang Technological University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19314.jpg', 'data': {'categories': ['#audio'], 'emoji': '🎙️', 'ru': {'title': 'SoloSpeech: генеративное извлечение целевой речи нового поколения', 'desc': 'SoloSpeech - это новый генеративный подход к извлечению целевой речи из смеси голосов. Он использует каскадный пайплайн, включающий сжатие, извлечение, реконструкцию и коррекцию. Ключевая особенность - экстрактор целевой речи, работающий без эмбеддингов говорящего и использующий латентное пространство вспомогательного аудио. SoloSpeech достигает наилучших результатов по разборчивости и качеству на датасете Libri2Mix, демонстрируя отличную обобщающую способность.'}, 'en': {'title': 'SoloSpeech: Revolutionizing Target Speech Extraction with Generative Techniques', 'desc': "This paper introduces SoloSpeech, a new approach for Target Speech Extraction (TSE) that effectively isolates a target speaker's voice from a mix of multiple speakers. Unlike traditional discriminative models that can produce artifacts and lack naturalness, SoloSpeech employs a cascaded generative pipeline that includes processes for compression, extraction, reconstruction, and correction. It utilizes a speaker-embedding-free target extractor that aligns the latent spaces of cue audio and mixture audio, enhancing performance and reducing discrepancies. Evaluated on the Libri2Mix dataset, SoloSpeech sets a new benchmark for intelligibility and quality in TSE, showing strong generalization capabilities in diverse real-world scenarios."}, 'zh': {'title': 'SoloSpeech：提升目标语音提取的新方法', 'desc': '目标语音提取（TSE）旨在从多个说话者的混合音频中分离出目标说话者的声音，通常利用作为辅助音频的说话者特征线索。尽管近期的TSE进展主要采用了高感知质量的判别模型，但这些模型常常引入不必要的伪影，降低自然性，并对训练和测试环境之间的差异敏感。另一方面，生成模型在感知质量和可懂性方面滞后。为了解决这些问题，我们提出了SoloSpeech，这是一种新颖的级联生成管道，集成了压缩、提取、重建和修正过程，能够在目标语音提取和语音分离任务中实现新的最先进的可懂性和质量。'}}}, {'id': 'https://huggingface.co/papers/2505.20322', 'title': 'Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering\n  Target Atoms', 'url': 'https://huggingface.co/papers/2505.20322', 'abstract': 'Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This interdependency can limit control precision and sometimes lead to unintended side effects. Recent research has explored the use of sparse autoencoders (SAE) to disentangle knowledge in high-dimensional spaces for steering. However, these applications have been limited to toy tasks owing to the nontrivial issue of locating atomic knowledge components. In this paper, we propose Steering Target Atoms (STA), a novel method that isolates and manipulates disentangled knowledge components to enhance safety. Comprehensive experiments demonstrate the effectiveness of our approach. Further analysis reveals that steering exhibits superior robustness and flexibility, particularly in adversarial scenarios. We also apply the steering strategy to the large reasoning model, confirming its effectiveness in precise reasoning control.', 'score': 2, 'issue_id': 3990, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '63cd6df71ddaefa4', 'authors': ['Mengru Wang', 'Ziwen Xu', 'Shengyu Mao', 'Shumin Deng', 'Zhaopeng Tu', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['NUS-NCS Joint Lab, Singapore', 'National University of Singapore', 'Tencent AI Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.20322.jpg', 'data': {'categories': ['#rl', '#reasoning', '#security', '#training', '#alignment'], 'emoji': '🎯', 'ru': {'title': 'Точное управление языковыми моделями через атомарные компоненты знаний', 'desc': 'Статья представляет новый метод под названием Steering Target Atoms (STA) для точного контроля над генерацией языковых моделей. Метод использует разреженные автоэнкодеры для выделения атомарных компонентов знаний в высокоразмерных пространствах. Эксперименты показывают эффективность STA в повышении безопасности и надежности языковых моделей. Метод демонстрирует особую устойчивость и гибкость в сценариях состязательного машинного обучения.'}, 'en': {'title': 'Enhancing Control in Language Models with Steering Target Atoms', 'desc': 'This paper introduces a new method called Steering Target Atoms (STA) to improve the control over language model generation. It addresses the challenge of intertwined internal representations in large models, which can hinder precise steering and lead to unintended consequences. By isolating and manipulating specific knowledge components, STA enhances the safety and reliability of model outputs. The experiments show that this approach is effective, especially in adversarial situations, and it also improves reasoning control in large models.'}, 'zh': {'title': '提升语言模型安全性的创新方法', 'desc': '本文提出了一种新的方法，称为引导目标原子（STA），旨在提高语言模型生成的安全性和可靠性。通过使用稀疏自编码器（SAE），该方法能够分离和操控高维空间中的知识组件，从而增强对模型行为的控制。实验结果表明，STA在对抗性场景中表现出更强的鲁棒性和灵活性。此外，我们还将这一引导策略应用于大型推理模型，验证了其在精确推理控制中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2505.21491', 'title': 'Frame In-N-Out: Unbounded Controllable Image-to-Video Generation', 'url': 'https://huggingface.co/papers/2505.21491', 'abstract': 'Controllability, temporal coherence, and detail synthesis remain the most critical challenges in video generation. In this paper, we focus on a commonly used yet underexplored cinematic technique known as Frame In and Frame Out. Specifically, starting from image-to-video generation, users can control the objects in the image to naturally leave the scene or provide breaking new identity references to enter the scene, guided by user-specified motion trajectory. To support this task, we introduce a new dataset curated semi-automatically, a comprehensive evaluation protocol targeting this setting, and an efficient identity-preserving motion-controllable video Diffusion Transformer architecture. Our evaluation shows that our proposed approach significantly outperforms existing baselines.', 'score': 1, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'cf3b062e968f421f', 'authors': ['Boyang Wang', 'Xuweiyi Chen', 'Matheus Gadelha', 'Zezhou Cheng'], 'affiliations': ['Adobe Research', 'University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2505.21491.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#games', '#diffusion', '#architecture', '#video'], 'emoji': '🎬', 'ru': {'title': 'Управляемая генерация видео: новый уровень контроля над объектами в кадре', 'desc': 'Статья посвящена улучшению контролируемости, временной согласованности и детализации в генерации видео. Авторы предлагают новый подход к технике Frame In и Frame Out, позволяющий пользователям управлять объектами, входящими в кадр или выходящими из него. Для решения этой задачи был создан новый датасет, разработан протокол оценки и предложена архитектура Diffusion Transformer для генерации видео с сохранением идентичности объектов. Результаты показывают значительное превосходство предложенного метода над существующими базовыми подходами.'}, 'en': {'title': 'Mastering Video Generation with Motion Control', 'desc': 'This paper addresses key challenges in video generation, specifically focusing on controllability, temporal coherence, and detail synthesis. It introduces a novel technique called Frame In and Frame Out, allowing users to manipulate objects in a video scene based on specified motion trajectories. The authors present a new dataset and evaluation protocol tailored for this task, along with a Diffusion Transformer architecture that preserves identity while enabling motion control. Results demonstrate that their method significantly improves upon existing video generation models.'}, 'zh': {'title': '提升视频生成的可控性与一致性', 'desc': '本论文关注视频生成中的可控性、时间一致性和细节合成等关键挑战。我们提出了一种名为“帧进帧出”的电影技术，允许用户控制图像中的对象自然地离开或进入场景。为支持这一任务，我们引入了一个半自动策划的新数据集，并制定了针对该设置的综合评估协议。我们的评估结果表明，所提出的方法在性能上显著优于现有基线。'}}}, {'id': 'https://huggingface.co/papers/2505.21457', 'title': 'Active-O3: Empowering Multimodal Large Language Models with Active\n  Perception via GRPO', 'url': 'https://huggingface.co/papers/2505.21457', 'abstract': "Active vision, also known as active perception, refers to the process of actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. Recently, the use of Multimodal Large Language Models (MLLMs) as central planning and decision-making modules in robotic systems has gained extensive attention. However, despite the importance of active perception in embodied intelligence, there is little to no exploration of how MLLMs can be equipped with or learn active perception capabilities. In this paper, we first provide a systematic definition of MLLM-based active perception tasks. We point out that the recently proposed GPT-o3 model's zoom-in search strategy can be regarded as a special case of active perception; however, it still suffers from low search efficiency and inaccurate region selection. To address these issues, we propose ACTIVE-O3, a purely reinforcement learning based training framework built on top of GRPO, designed to equip MLLMs with active perception capabilities. We further establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across both general open-world tasks, such as small-object and dense object grounding, and domain-specific scenarios, including small object detection in remote sensing and autonomous driving, as well as fine-grained interactive segmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot reasoning abilities on the V* Benchmark, without relying on any explicit reasoning data. We hope that our work can provide a simple codebase and evaluation protocol to facilitate future research on active perception in MLLMs.", 'score': 1, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '6d338aff50466f43', 'authors': ['Muzhi Zhu', 'Hao Zhong', 'Canyu Zhao', 'Zongze Du', 'Zheng Huang', 'Mingyu Liu', 'Hao Chen', 'Cheng Zou', 'Jingdong Chen', 'Ming Yang', 'Chunhua Shen'], 'affiliations': ['Ant Group, China', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.21457.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#optimization', '#rl', '#agents', '#reasoning'], 'emoji': '👁️', 'ru': {'title': 'ACTIVE-O3: Наделение MLLM активным восприятием для эффективного принятия решений', 'desc': 'Статья представляет ACTIVE-O3 - фреймворк обучения с подкреплением для наделения мультимодальных больших языковых моделей (MLLM) возможностями активного восприятия. Авторы определяют задачи активного восприятия для MLLM и создают комплексный набор тестов для оценки ACTIVE-O3 в различных сценариях. Фреймворк демонстрирует улучшенную эффективность поиска и точность выбора регионов по сравнению с предыдущими подходами. ACTIVE-O3 также показывает сильные способности к рассуждениям с нулевым обучением на эталонном тесте V*.'}, 'en': {'title': 'Empowering MLLMs with Active Perception for Smarter Decision-Making', 'desc': 'This paper introduces ACTIVE-O3, a reinforcement learning framework designed to enhance Multimodal Large Language Models (MLLMs) with active perception capabilities. Active perception involves strategically selecting where to focus attention to gather relevant information, which is crucial for effective decision-making in robotics. The authors highlight the limitations of the existing GPT-o3 model in terms of search efficiency and region selection accuracy. By establishing a benchmark suite for evaluating ACTIVE-O3, the paper aims to advance research in active perception for MLLMs, demonstrating strong performance in various tasks without needing explicit reasoning data.'}, 'zh': {'title': '提升机器人主动感知能力的创新框架', 'desc': '主动视觉，也称为主动感知，是指主动选择观察的方式和位置，以获取与任务相关的信息。本文探讨了多模态大型语言模型（MLLMs）在机器人系统中的主动感知能力，提出了一种基于强化学习的训练框架ACTIVE-O3。我们定义了MLLMs的主动感知任务，并指出现有模型在搜索效率和区域选择上存在不足。通过建立综合基准测试套件，ACTIVE-O3在多个任务中展示了强大的零-shot推理能力，推动了主动感知的研究。'}}}, {'id': 'https://huggingface.co/papers/2505.21205', 'title': 'Sci-Fi: Symmetric Constraint for Frame Inbetweening', 'url': 'https://huggingface.co/papers/2505.21205', 'abstract': "Frame inbetweening aims to synthesize intermediate video sequences conditioned on the given start and end frames. Current state-of-the-art methods mainly extend large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs) by incorporating end-frame constraints via directly fine-tuning or omitting training. We identify a critical limitation in their design: Their injections of the end-frame constraint usually utilize the same mechanism that originally imposed the start-frame (single image) constraint. However, since the original I2V-DMs are adequately trained for the start-frame condition in advance, naively introducing the end-frame constraint by the same mechanism with much less (even zero) specialized training probably can't make the end frame have a strong enough impact on the intermediate content like the start frame. This asymmetric control strength of the two frames over the intermediate content likely leads to inconsistent motion or appearance collapse in generated frames. To efficiently achieve symmetric constraints of start and end frames, we propose a novel framework, termed Sci-Fi, which applies a stronger injection for the constraint of a smaller training scale. Specifically, it deals with the start-frame constraint as before, while introducing the end-frame constraint by an improved mechanism. The new mechanism is based on a well-designed lightweight module, named EF-Net, which encodes only the end frame and expands it into temporally adaptive frame-wise features injected into the I2V-DM. This makes the end-frame constraint as strong as the start-frame constraint, enabling our Sci-Fi to produce more harmonious transitions in various scenarios. Extensive experiments prove the superiority of our Sci-Fi compared with other baselines.", 'score': 1, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'a20a9eb43b42208f', 'authors': ['Liuhan Chen', 'Xiaodong Cun', 'Xiaoyu Li', 'Xianyi He', 'Shenghai Yuan', 'Jie Chen', 'Ying Shan', 'Li Yuan'], 'affiliations': ['ARC Lab, Tencent PCG', 'GVC Lab, Great Bay University', 'Rabbitpre Intelligence', 'Shenzhen Graduate School, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21205.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#architecture', '#video', '#training'], 'emoji': '🎞️', 'ru': {'title': 'Симметричное внедрение граничных кадров для улучшенного синтеза видео', 'desc': 'Статья представляет новый подход к синтезу промежуточных видеокадров между заданными начальным и конечным кадрами. Авторы предлагают фреймворк Sci-Fi, который решает проблему асимметричного влияния начального и конечного кадров в существующих методах, основанных на диффузионных моделях преобразования изображения в видео (I2V-DM). Sci-Fi вводит специальный модуль EF-Net для более эффективного внедрения информации о конечном кадре. Эксперименты показывают превосходство предложенного метода над существующими подходами в создании плавных и согласованных переходов между кадрами.'}, 'en': {'title': 'Achieving Harmony in Video Frame Synthesis with Sci-Fi', 'desc': 'This paper presents a new approach called Sci-Fi for generating intermediate video sequences from given start and end frames. The authors identify a limitation in existing methods that treat the start and end frame constraints equally, which can lead to poor video quality. Sci-Fi introduces a novel mechanism using a lightweight module, EF-Net, to enhance the influence of the end frame, ensuring it has a similar impact as the start frame. The results show that Sci-Fi produces smoother and more consistent transitions in generated videos compared to current state-of-the-art techniques.'}, 'zh': {'title': '实现起始帧与结束帧的对称约束', 'desc': '本文提出了一种新的框架，称为Sci-Fi，用于在给定的起始帧和结束帧之间合成中间视频序列。现有的方法主要依赖于大型预训练的图像到视频扩散模型（I2V-DMs），但在引入结束帧约束时存在设计缺陷。我们发现，简单地使用与起始帧相同的机制来引入结束帧约束，可能无法有效影响中间内容，从而导致生成帧的运动不一致或外观崩溃。Sci-Fi通过引入一种改进的机制和轻量级模块EF-Net，使结束帧约束的影响力与起始帧相当，从而实现更和谐的过渡效果。'}}}, {'id': 'https://huggingface.co/papers/2505.21070', 'title': 'Minute-Long Videos with Dual Parallelisms', 'url': 'https://huggingface.co/papers/2505.21070', 'abstract': 'Diffusion Transformer (DiT)-based video diffusion models generate high-quality videos at scale but incur prohibitive processing latency and memory costs for long videos. To address this, we propose a novel distributed inference strategy, termed DualParal. The core idea is that, instead of generating an entire video on a single GPU, we parallelize both temporal frames and model layers across GPUs. However, a naive implementation of this division faces a key limitation: since diffusion models require synchronized noise levels across frames, this implementation leads to the serialization of original parallelisms. We leverage a block-wise denoising scheme to handle this. Namely, we process a sequence of frame blocks through the pipeline with progressively decreasing noise levels. Each GPU handles a specific block and layer subset while passing previous results to the next GPU, enabling asynchronous computation and communication. To further optimize performance, we incorporate two key enhancements. Firstly, a feature cache is implemented on each GPU to store and reuse features from the prior block as context, minimizing inter-GPU communication and redundant computation. Secondly, we employ a coordinated noise initialization strategy, ensuring globally consistent temporal dynamics by sharing initial noise patterns across GPUs without extra resource costs. Together, these enable fast, artifact-free, and infinitely long video generation. Applied to the latest diffusion transformer video generator, our method efficiently produces 1,025-frame videos with up to 6.54times lower latency and 1.48times lower memory cost on 8timesRTX 4090 GPUs.', 'score': 0, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'c777be11d4844462', 'authors': ['Zeqing Wang', 'Bowen Zheng', 'Xingyi Yang', 'Yuecong Xu', 'Xinchao Wang'], 'affiliations': ['Huazhong University of Science and Technology', 'National University of Singapore', 'Xidian University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21070.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#video', '#inference'], 'emoji': '🎞️', 'ru': {'title': 'Ускорение генерации длинных видео с помощью распределенного вывода', 'desc': 'Статья представляет новую стратегию распределенного вывода для видео-диффузионных моделей на основе Diffusion Transformer (DiT), называемую DualParal. Основная идея заключается в параллелизации как временных кадров, так и слоев модели между GPU для снижения задержки обработки и затрат памяти при генерации длинных видео. Авторы используют блочную схему шумоподавления и кэширование признаков для оптимизации производительности. Метод позволяет эффективно генерировать видео длиной 1025 кадров с до 6,54 раз меньшей задержкой и 1,48 раз меньшими затратами памяти на 8 GPU RTX 4090.'}, 'en': {'title': 'Revolutionizing Video Generation with DualPar Efficiency!', 'desc': 'The paper presents a new method called DualPar for improving the efficiency of video generation using Diffusion Transformer (DiT) models. It addresses the high latency and memory costs associated with generating long videos by distributing the workload across multiple GPUs. The method uses a block-wise denoising approach to maintain synchronized noise levels while allowing for parallel processing of frames and model layers. Additionally, it introduces a feature cache and coordinated noise initialization to optimize performance, resulting in faster and more efficient video generation without artifacts.'}, 'zh': {'title': '高效生成无限长视频的创新策略', 'desc': '本论文提出了一种基于扩散变换器（DiT）的视频扩散模型，旨在解决长视频生成时的处理延迟和内存成本问题。我们提出了一种新的分布式推理策略，称为DualParal，通过在多个GPU上并行处理时间帧和模型层来提高效率。为了克服扩散模型对噪声水平同步的要求，我们采用了块级去噪方案，使得每个GPU处理特定的帧块和层，同时实现异步计算和通信。最终，我们的方法在生成高质量视频时显著降低了延迟和内存消耗，能够高效生成无限长的视频。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (3)', '#agi', '#alignment (2)', '#architecture (2)', '#audio (1)', '#benchmark (10)', '#cv (2)', '#data (2)', '#dataset (5)', '#diffusion (5)', '#ethics', '#games (2)', '#graphs', '#hallucinations', '#healthcare', '#inference (1)', '#interpretability', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (6)', '#open_source (3)', '#optimization (7)', '#plp', '#rag', '#reasoning (6)', '#rl (2)', '#rlhf', '#robotics', '#science (1)', '#security (1)', '#small_models', '#story_generation', '#survey', '#synthetic (2)', '#training (5)', '#transfer_learning', '#video (7)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-05-28 02:39',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-05-28 02:39')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-05-28 02:39')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    