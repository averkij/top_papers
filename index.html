
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 12 papers. December 24.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">24 декабря</span> | <span id="title-articles-count">12 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2024-12-23.html">⬅️ <span id="prev-date">23.12</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-12-25.html">➡️ <span id="next-date">25.12</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-12.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '24 декабря', 'en': 'December 24', 'zh': '12月24日'};
        let feedDateNext = {'ru': '25.12', 'en': '12/25', 'zh': '12月25日'};
        let feedDatePrev = {'ru': '23.12', 'en': '12/23', 'zh': '12月23日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2412.17256', 'title': 'B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners', 'url': 'https://huggingface.co/papers/2412.17256', 'abstract': "In the absence of extensive human-annotated data for complex reasoning tasks, self-improvement -- where models are trained on their own outputs -- has emerged as a primary method for enhancing performance. However, the critical factors underlying the mechanism of these iterative self-improving methods remain poorly understood, such as under what conditions self-improvement is effective, and what are the bottlenecks in the current iterations. In this work, we identify and propose methods to monitor two pivotal factors in this iterative process: (1) the model's ability to generate sufficiently diverse responses (exploration); and (2) the effectiveness of external rewards in distinguishing high-quality candidates from lower-quality ones (exploitation). Using mathematical reasoning as a case study, we begin with a quantitative analysis to track the dynamics of exploration and exploitation, discovering that a model's exploratory capabilities rapidly deteriorate over iterations, and the effectiveness of exploiting external rewards diminishes as well. Motivated by these findings, we introduce B-STaR, a Self-Taught Reasoning framework that autonomously adjusts configurations across iterations to Balance exploration and exploitation, thereby optimizing the self-improving effectiveness based on the current policy model and available rewards. Our experiments on mathematical reasoning, coding, and commonsense reasoning demonstrate that B-STaR not only enhances the model's exploratory capabilities throughout training but also achieves a more effective balance between exploration and exploitation, leading to superior performance.", 'score': 21, 'issue_id': 1281, 'pub_date': '2024-12-23', 'pub_date_card': {'ru': '23 декабря', 'en': 'December 23', 'zh': '12月23日'}, 'hash': '1a1ee4818597feae', 'authors': ['Weihao Zeng', 'Yuzhen Huang', 'Lulu Zhao', 'Yijun Wang', 'Zifei Shan', 'Junxian He'], 'affiliations': ['BAAI', 'Tencent', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2412.17256.jpg', 'data': {'categories': ['#optimization', '#training', '#math', '#reasoning', '#rl'], 'emoji': '🔄', 'ru': {'title': 'Баланс исследования и эксплуатации для эффективного самосовершенствования ИИ', 'desc': 'Эта статья посвящена самосовершенствованию моделей машинного обучения в отсутствие большого количества аннотированных данных. Авторы исследуют два ключевых фактора в итеративном процессе самообучения: способность модели генерировать разнообразные ответы (исследование) и эффективность внешних наград в различении высококачественных кандидатов (эксплуатация). На основе анализа они предлагают фреймворк B-STaR, который автоматически балансирует исследование и эксплуатацию для оптимизации процесса самосовершенствования. Эксперименты показывают, что B-STaR улучшает исследовательские возможности модели и достигает лучшего баланса, приводя к повышению производительности в задачах математических рассуждений, кодирования и здравого смысла.'}, 'en': {'title': 'Balancing Exploration and Exploitation for Self-Improvement in ML Models', 'desc': "This paper addresses the challenge of improving machine learning models in the absence of large human-annotated datasets, focusing on self-improvement through iterative training on their own outputs. It identifies two key factors that influence this process: the model's ability to explore diverse responses and the effectiveness of external rewards in selecting high-quality outputs. The authors introduce B-STaR, a framework that dynamically adjusts training configurations to maintain a balance between exploration and exploitation. Experiments show that B-STaR enhances exploratory capabilities and improves overall model performance in reasoning tasks."}, 'zh': {'title': '自我改进：平衡探索与利用的关键', 'desc': '在缺乏大量人工标注数据的复杂推理任务中，自我改进成为提升模型性能的主要方法。本文探讨了自我改进过程中的两个关键因素：模型生成多样化响应的能力（探索）和外部奖励在区分高质量候选项与低质量候选项中的有效性（利用）。通过对数学推理的案例研究，我们发现模型的探索能力在迭代过程中迅速下降，而外部奖励的有效性也随之减弱。为了解决这些问题，我们提出了B-STaR框架，能够在迭代中自我调整配置，以平衡探索与利用，从而优化自我改进的效果。'}}}, {'id': 'https://huggingface.co/papers/2412.14922', 'title': 'RobustFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response', 'url': 'https://huggingface.co/papers/2412.14922', 'abstract': "Supervised fine-tuning (SFT) plays a crucial role in adapting large language models (LLMs) to specific domains or tasks. However, as demonstrated by empirical experiments, the collected data inevitably contains noise in practical applications, which poses significant challenges to model performance on downstream tasks. Therefore, there is an urgent need for a noise-robust SFT framework to enhance model capabilities in downstream tasks. To address this challenge, we introduce a robust SFT framework (RobustFT) that performs noise detection and relabeling on downstream task data. For noise identification, our approach employs a multi-expert collaborative system with inference-enhanced models to achieve superior noise detection. In the denoising phase, we utilize a context-enhanced strategy, which incorporates the most relevant and confident knowledge followed by careful assessment to generate reliable annotations. Additionally, we introduce an effective data selection mechanism based on response entropy, ensuring only high-quality samples are retained for fine-tuning. Extensive experiments conducted on multiple LLMs across five datasets demonstrate RobustFT's exceptional performance in noisy scenarios.", 'score': 17, 'issue_id': 1281, 'pub_date': '2024-12-19', 'pub_date_card': {'ru': '19 декабря', 'en': 'December 19', 'zh': '12月19日'}, 'hash': '9cc4a703e686ea87', 'authors': ['Junyu Luo', 'Xiao Luo', 'Kaize Ding', 'Jingyang Yuan', 'Zhiping Xiao', 'Ming Zhang'], 'affiliations': ['Northwestern University', 'Peking University', 'University of California, Los Angeles', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2412.14922.jpg', 'data': {'categories': ['#data', '#training', '#optimization'], 'emoji': '🧼', 'ru': {'title': 'Чистая дообучка: RobustFT для устойчивых языковых моделей', 'desc': 'Статья представляет новый фреймворк RobustFT для устойчивой дообучения больших языковых моделей (LLM) на зашумленных данных. RobustFT использует систему экспертов для обнаружения шума и переразметки данных. Фреймворк применяет контекстно-улучшенную стратегию для генерации надежных аннотаций и механизм отбора данных на основе энтропии ответов. Эксперименты на пяти наборах данных показали высокую эффективность RobustFT в сценариях с шумом.'}, 'en': {'title': 'Enhancing Language Models with Robust Fine-Tuning', 'desc': 'This paper presents a new framework called RobustFT for supervised fine-tuning (SFT) of large language models (LLMs) that addresses the issue of noisy data in training. The framework includes a multi-expert system for detecting noise in the data, which helps improve the quality of the training process. It also features a context-enhanced strategy for relabeling data, ensuring that only the most relevant and reliable information is used for fine-tuning. Experiments show that RobustFT significantly enhances model performance on downstream tasks, even in the presence of noise.'}, 'zh': {'title': '构建抗噪声的微调框架，提升模型性能', 'desc': '监督微调（SFT）在将大型语言模型（LLMs）适应特定领域或任务中起着重要作用。然而，实际应用中收集的数据不可避免地包含噪声，这对模型在下游任务上的表现造成了重大挑战。因此，迫切需要一种抗噪声的SFT框架，以增强模型在下游任务中的能力。为了解决这个问题，我们提出了一种稳健的SFT框架（RobustFT），该框架在下游任务数据上执行噪声检测和重新标注。'}}}, {'id': 'https://huggingface.co/papers/2412.17451', 'title': 'Diving into Self-Evolving Training for Multimodal Reasoning', 'url': 'https://huggingface.co/papers/2412.17451', 'abstract': "Reasoning ability is essential for Large Multimodal Models (LMMs). In the absence of multimodal chain-of-thought annotated data, self-evolving training, where the model learns from its own outputs, has emerged as an effective and scalable approach for enhancing reasoning abilities. Despite its growing usage, a comprehensive understanding of self-evolving training, particularly in the context of multimodal reasoning, remains limited. In this paper, we delve into the intricacies of self-evolving training for multimodal reasoning, pinpointing three key factors: Training Method, Reward Model, and Prompt Variation. We systematically examine each factor and explore how various configurations affect the training's effectiveness. Our analysis leads to a set of best practices for each factor, aimed at optimizing multimodal reasoning. Furthermore, we explore the Self-Evolution Dynamics during training and the impact of automatic balancing mechanisms in boosting performance. After all the investigations, we present a final recipe for self-evolving training in multimodal reasoning, encapsulating these design choices into a framework we call MSTaR (Multimodal Self-evolving Training for Reasoning), which is universally effective for models with different sizes on various benchmarks, e.g., surpassing the pre-evolved model significantly on 5 multimodal reasoning benchmarks without using additional human annotations, as demonstrated on MiniCPM-V-2.5 (8B), Phi-3.5-Vision (4B) and InternVL2 (2B). We believe this study fills a significant gap in the understanding of self-evolving training for multimodal reasoning and offers a robust framework for future research. Our policy and reward models, as well as the collected data, is released to facilitate further investigation in multimodal reasoning.", 'score': 16, 'issue_id': 1281, 'pub_date': '2024-12-23', 'pub_date_card': {'ru': '23 декабря', 'en': 'December 23', 'zh': '12月23日'}, 'hash': '2866001b23a585e1', 'authors': ['Wei Liu', 'Junlong Li', 'Xiwen Zhang', 'Fan Zhou', 'Yu Cheng', 'Junxian He'], 'affiliations': ['Helixon Research', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2412.17451.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#multimodal', '#reasoning', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Самосовершенствование мультимодальных моделей в искусстве рассуждения', 'desc': 'Статья исследует самоэволюционирующее обучение для улучшения способностей мультимодальных моделей к рассуждению. Авторы выделяют три ключевых фактора: метод обучения, модель вознаграждения и вариации промптов. На основе анализа предлагается набор лучших практик для каждого фактора. Исследователи представляют фреймворк MSTaR для самоэволюционирующего обучения мультимодальному рассуждению. Результаты показывают значительное улучшение производительности на нескольких бенчмарках без использования дополнительных человеческих аннотаций.'}, 'en': {'title': 'Unlocking Reasoning in Multimodal Models with Self-Evolving Training', 'desc': 'This paper focuses on improving reasoning abilities in Large Multimodal Models (LMMs) through a method called self-evolving training, which allows models to learn from their own outputs. The authors identify three critical factors that influence the effectiveness of this training: the Training Method, Reward Model, and Prompt Variation. They provide a detailed analysis of how different configurations of these factors can optimize multimodal reasoning performance. The study culminates in the development of a framework named MSTaR, which demonstrates significant improvements in reasoning tasks across various model sizes and benchmarks without requiring additional human annotations.'}, 'zh': {'title': '自我进化训练：提升多模态推理能力的关键', 'desc': '本文探讨了自我进化训练在多模态推理中的应用，强调了推理能力对大型多模态模型的重要性。我们识别了影响训练效果的三个关键因素：训练方法、奖励模型和提示变体，并系统地分析了这些因素的不同配置。研究结果提供了一套最佳实践，旨在优化多模态推理的训练过程。此外，我们提出了MSTaR框架，展示了自我进化训练在不同规模模型上的普遍有效性，显著超越了预先进化模型的表现。'}}}, {'id': 'https://huggingface.co/papers/2412.16926', 'title': 'Revisiting In-Context Learning with Long Context Language Models', 'url': 'https://huggingface.co/papers/2412.16926', 'abstract': 'In-Context Learning (ICL) is a technique by which language models make predictions based on examples provided in their input context. Previously, their context window size imposed a limit on the number of examples that can be shown, making example selection techniques crucial for identifying the maximally effective set of examples. However, the recent advent of Long Context Language Models (LCLMs) has significantly increased the number of examples that can be included in context, raising an important question of whether ICL performance in a many-shot regime is still sensitive to the method of sample selection. To answer this, we revisit these approaches in the context of LCLMs through extensive experiments on 18 datasets spanning 4 tasks. Surprisingly, we observe that sophisticated example selection techniques do not yield significant improvements over a simple random sample selection method. Instead, we find that the advent of LCLMs has fundamentally shifted the challenge of ICL from that of selecting the most effective examples to that of collecting sufficient examples to fill the context window. Specifically, in certain datasets, including all available examples does not fully utilize the context window; however, by augmenting the examples in context with a simple data augmentation approach, we substantially improve ICL performance by 5%.', 'score': 10, 'issue_id': 1281, 'pub_date': '2024-12-22', 'pub_date_card': {'ru': '22 декабря', 'en': 'December 22', 'zh': '12月22日'}, 'hash': '764c013157322e0d', 'authors': ['Jinheon Baek', 'Sun Jae Lee', 'Prakhar Gupta', 'Geunseob', 'Oh', 'Siddharth Dalmia', 'Prateek Kolhar'], 'affiliations': ['Google DeepMind', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2412.16926.jpg', 'data': {'categories': ['#data', '#optimization', '#training', '#dataset', '#long_context'], 'emoji': '📏', 'ru': {'title': 'Больше примеров - лучше результат: новый взгляд на обучение в контексте для языковых моделей', 'desc': 'Статья исследует эффективность техник выбора примеров для обучения языковых моделей с длинным контекстом (LCLM) методом обучения в контексте (ICL). Авторы провели эксперименты на 18 наборах данных и обнаружили, что сложные методы выбора примеров не дают значительных улучшений по сравнению с простым случайным выбором. Основной вызов теперь заключается не в выборе наиболее эффективных примеров, а в сборе достаточного количества примеров для заполнения контекстного окна. Простая техника аугментации данных позволила улучшить производительность ICL на 5% в некоторых наборах данных.'}, 'en': {'title': 'Maximizing ICL Performance with Long Contexts: Simplicity Over Sophistication', 'desc': 'In-Context Learning (ICL) allows language models to make predictions based on examples in their input. With the introduction of Long Context Language Models (LCLMs), the number of examples that can be included has increased, prompting a reevaluation of example selection methods. The study finds that complex selection techniques do not significantly outperform simple random sampling in many-shot scenarios. Instead, the focus shifts to ensuring enough examples fill the context window, and using data augmentation can enhance ICL performance by 5%.'}, 'zh': {'title': '长上下文模型下的示例选择新挑战', 'desc': '本文探讨了在长上下文语言模型（LCLMs）中，示例选择对上下文学习（ICL）性能的影响。研究发现，尽管复杂的示例选择技术并未显著提高性能，但简单的随机选择方法在许多情况下表现良好。随着LCLMs的出现，ICL的挑战已从选择最有效的示例转变为收集足够的示例以填充上下文窗口。通过简单的数据增强方法，我们在某些数据集上提高了ICL性能，提升幅度达到5%。'}}}, {'id': 'https://huggingface.co/papers/2412.15118', 'title': 'Outcome-Refining Process Supervision for Code Generation', 'url': 'https://huggingface.co/papers/2412.15118', 'abstract': 'Large Language Models have demonstrated remarkable capabilities in code generation, yet they often struggle with complex programming tasks that require deep algorithmic reasoning. While process supervision through learned reward models shows promise in guiding reasoning steps, it requires expensive training data and suffers from unreliable evaluation. We propose Outcome-Refining Process Supervision, a novel paradigm that treats outcome refinement itself as the process to be supervised. Our framework leverages concrete execution signals to ground the supervision of reasoning steps, while using tree-structured exploration to maintain multiple solution trajectories simultaneously. Experiments demonstrate that our approach enables even smaller models to achieve high success accuracy and performance metrics on competitive programming tasks, creates more reliable verification than traditional reward models without requiring training PRMs. Our approach achieves significant improvements across 5 models and 3 datasets: an average of 26.9% increase in correctness and 42.2% in efficiency. The results suggest that providing structured reasoning space with concrete verification signals is crucial for solving complex programming tasks. We open-source all our code and data at: https://github.com/zhuohaoyu/ORPS', 'score': 8, 'issue_id': 1284, 'pub_date': '2024-12-19', 'pub_date_card': {'ru': '19 декабря', 'en': 'December 19', 'zh': '12月19日'}, 'hash': '88c43fc4e946d78c', 'authors': ['Zhuohao Yu', 'Weizheng Gu', 'Yidong Wang', 'Zhengran Zeng', 'Jindong Wang', 'Wei Ye', 'Shikun Zhang'], 'affiliations': ['Microsoft Research', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2412.15118.jpg', 'data': {'categories': ['#optimization', '#dataset', '#training', '#plp', '#reasoning', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Усовершенствование алгоритмического мышления языковых моделей', 'desc': "Эта статья представляет новый подход к улучшению способностей языковых моделей в решении сложных задач программирования. Авторы предлагают метод 'Outcome-Refining Process Supervision', который использует конкретные сигналы выполнения для контроля этапов рассуждения. Подход применяет древовидную структуру для одновременного исследования нескольких траекторий решения. Эксперименты показывают значительное улучшение точности и эффективности моделей на задачах соревновательного программирования без необходимости обучения специальных моделей вознаграждения."}, 'en': {'title': 'Enhancing Code Generation with Outcome-Refining Supervision', 'desc': 'This paper introduces Outcome-Refining Process Supervision (ORPS), a new method to improve code generation in large language models, especially for complex programming tasks. Instead of relying on traditional reward models, ORPS supervises the reasoning process by refining outcomes using concrete execution signals. This approach allows models to explore multiple solution paths simultaneously, enhancing their ability to solve challenging problems. The results show that ORPS significantly boosts the accuracy and efficiency of various models on competitive programming tasks, demonstrating the importance of structured reasoning and reliable verification.'}, 'zh': {'title': '结果精炼：提升编程任务的智能推理能力', 'desc': '大型语言模型在代码生成方面表现出色，但在需要深度算法推理的复杂编程任务中常常遇到困难。我们提出了一种新的监督学习范式——结果精炼过程监督，旨在将结果精炼本身作为需要监督的过程。该框架利用具体的执行信号来指导推理步骤的监督，同时采用树状结构探索来同时维护多个解决方案轨迹。实验表明，我们的方法使得即使是较小的模型也能在竞争性编程任务中实现高成功率和性能指标，显著提高了正确性和效率。'}}}, {'id': 'https://huggingface.co/papers/2412.17747', 'title': 'Deliberation in Latent Space via Differentiable Cache Augmentation', 'url': 'https://huggingface.co/papers/2412.17747', 'abstract': 'Techniques enabling large language models (LLMs) to "think more" by generating and attending to intermediate reasoning steps have shown promise in solving complex problems. However, the standard approaches generate sequences of discrete tokens immediately before responding, and so they can incur significant latency costs and be challenging to optimize. In this work, we demonstrate that a frozen LLM can be augmented with an offline coprocessor that operates on the model\'s key-value (kv) cache. This coprocessor augments the cache with a set of latent embeddings designed to improve the fidelity of subsequent decoding. We train this coprocessor using the language modeling loss from the decoder on standard pretraining data, while keeping the decoder itself frozen. This approach enables the model to learn, in an end-to-end differentiable fashion, how to distill additional computation into its kv-cache. Because the decoder remains unchanged, the coprocessor can operate offline and asynchronously, and the language model can function normally if the coprocessor is unavailable or if a given cache is deemed not to require extra computation. We show experimentally that when a cache is augmented, the decoder achieves lower perplexity on numerous subsequent tokens. Furthermore, even without any task-specific training, our experiments demonstrate that cache augmentation consistently reduces perplexity and improves performance across a range of reasoning-intensive tasks.', 'score': 8, 'issue_id': 1283, 'pub_date': '2024-12-23', 'pub_date_card': {'ru': '23 декабря', 'en': 'December 23', 'zh': '12月23日'}, 'hash': 'b3ee8264ebbee41e', 'authors': ['Luyang Liu', 'Jonas Pfeiffer', 'Jiaxing Wu', 'Jun Xie', 'Arthur Szlam'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2412.17747.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#inference', '#training', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Офлайн-сопроцессор: Улучшение языковых моделей без изменения декодера', 'desc': 'Статья представляет новый подход к улучшению работы больших языковых моделей (LLM) с помощью офлайн-сопроцессора. Этот сопроцессор работает с кэшем ключ-значение модели, добавляя в него латентные эмбеддинги для повышения точности декодирования. Обучение сопроцессора происходит на стандартных данных предобучения, используя функцию потерь языкового моделирования декодера. Эксперименты показывают, что данный метод снижает перплексию и улучшает производительность на задачах, требующих рассуждений, без необходимости в специфическом обучении под задачу.'}, 'en': {'title': 'Enhancing LLMs with Offline Cache Augmentation for Better Reasoning', 'desc': "This paper presents a method to enhance large language models (LLMs) by using an offline coprocessor that improves the model's key-value (kv) cache. The coprocessor adds latent embeddings to the cache, which helps the model generate better responses by refining its reasoning process. By training the coprocessor with language modeling loss while keeping the main decoder unchanged, the system can learn to optimize its computations without increasing latency. Experimental results show that this cache augmentation leads to lower perplexity and better performance on various reasoning tasks, even without specific training for those tasks."}, 'zh': {'title': '增强缓存，提升推理能力！', 'desc': '本文探讨了一种增强大型语言模型（LLM）推理能力的方法。通过引入一个离线协处理器，该协处理器在模型的键值缓存上操作，从而提高后续解码的准确性。我们的方法允许模型以端到端可微分的方式学习如何将额外的计算提炼到其缓存中。实验结果表明，增强缓存后，解码器在多个后续标记上表现出更低的困惑度，且在推理密集型任务中性能显著提升。'}}}, {'id': 'https://huggingface.co/papers/2412.17153', 'title': 'Distilled Decoding 1: One-step Sampling of Image Auto-regressive Models with Flow Matching', 'url': 'https://huggingface.co/papers/2412.17153', 'abstract': "Autoregressive (AR) models have achieved state-of-the-art performance in text and image generation but suffer from slow generation due to the token-by-token process. We ask an ambitious question: can a pre-trained AR model be adapted to generate outputs in just one or two steps? If successful, this would significantly advance the development and deployment of AR models. We notice that existing works that try to speed up AR generation by generating multiple tokens at once fundamentally cannot capture the output distribution due to the conditional dependencies between tokens, limiting their effectiveness for few-step generation. To address this, we propose Distilled Decoding (DD), which uses flow matching to create a deterministic mapping from Gaussian distribution to the output distribution of the pre-trained AR model. We then train a network to distill this mapping, enabling few-step generation. DD doesn't need the training data of the original AR model, making it more practical.We evaluate DD on state-of-the-art image AR models and present promising results on ImageNet-256. For VAR, which requires 10-step generation, DD enables one-step generation (6.3times speed-up), with an acceptable increase in FID from 4.19 to 9.96. For LlamaGen, DD reduces generation from 256 steps to 1, achieving an 217.8times speed-up with a comparable FID increase from 4.11 to 11.35. In both cases, baseline methods completely fail with FID>100. DD also excels on text-to-image generation, reducing the generation from 256 steps to 2 for LlamaGen with minimal FID increase from 25.70 to 28.95. As the first work to demonstrate the possibility of one-step generation for image AR models, DD challenges the prevailing notion that AR models are inherently slow, and opens up new opportunities for efficient AR generation. The project website is at https://imagination-research.github.io/distilled-decoding.", 'score': 6, 'issue_id': 1283, 'pub_date': '2024-12-22', 'pub_date_card': {'ru': '22 декабря', 'en': 'December 22', 'zh': '12月22日'}, 'hash': 'b1968a6263a19386', 'authors': ['Enshu Liu', 'Xuefei Ning', 'Yu Wang', 'Zinan Lin'], 'affiliations': ['Department of EE, Tsinghua University', 'Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.17153.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'Революция в скорости: генерация изображений за один шаг', 'desc': 'Статья представляет новый метод под названием Distilled Decoding (DD) для ускорения генерации авторегрессионных (AR) моделей. DD использует технику flow matching для создания детерминированного отображения от гауссовского распределения к выходному распределению предобученной AR модели. Этот подход позволяет генерировать выходные данные за один или два шага вместо пошагового процесса, значительно ускоряя работу AR моделей. Эксперименты на задачах генерации изображений показывают, что DD может достичь ускорения в 6-217 раз с приемлемым ухудшением качества по метрике FID.'}, 'en': {'title': 'Revolutionizing Autoregressive Generation: Fast and Efficient with Distilled Decoding!', 'desc': "This paper introduces Distilled Decoding (DD), a novel approach to enhance the efficiency of autoregressive (AR) models in generating text and images. Traditional AR models generate outputs token-by-token, which can be slow, but DD aims to enable generation in just one or two steps by creating a deterministic mapping from a Gaussian distribution to the AR model's output distribution. By training a network to distill this mapping, DD allows for rapid generation without requiring the original training data, making it more practical for real-world applications. The results show significant speed-ups in generation times while maintaining acceptable quality, challenging the belief that AR models are inherently slow."}, 'zh': {'title': '蒸馏解码：加速自回归模型生成的革命性方法', 'desc': '自回归（AR）模型在文本和图像生成方面表现出色，但由于逐个生成的过程，速度较慢。本文提出了一种名为蒸馏解码（DD）的方法，旨在将预训练的AR模型适应为仅需一步或两步生成输出。DD通过流匹配创建从高斯分布到AR模型输出分布的确定性映射，从而实现快速生成。实验结果表明，DD在多个图像AR模型上显著提高了生成速度，同时保持了可接受的生成质量。'}}}, {'id': 'https://huggingface.co/papers/2412.17805', 'title': 'Large Motion Video Autoencoding with Cross-modal Video VAE', 'url': 'https://huggingface.co/papers/2412.17805', 'abstract': 'Learning a robust video Variational Autoencoder (VAE) is essential for reducing video redundancy and facilitating efficient video generation. Directly applying image VAEs to individual frames in isolation can result in temporal inconsistencies and suboptimal compression rates due to a lack of temporal compression. Existing Video VAEs have begun to address temporal compression; however, they often suffer from inadequate reconstruction performance. In this paper, we present a novel and powerful video autoencoder capable of high-fidelity video encoding. First, we observe that entangling spatial and temporal compression by merely extending the image VAE to a 3D VAE can introduce motion blur and detail distortion artifacts. Thus, we propose temporal-aware spatial compression to better encode and decode the spatial information. Additionally, we integrate a lightweight motion compression model for further temporal compression. Second, we propose to leverage the textual information inherent in text-to-video datasets and incorporate text guidance into our model. This significantly enhances reconstruction quality, particularly in terms of detail preservation and temporal stability. Third, we further improve the versatility of our model through joint training on both images and videos, which not only enhances reconstruction quality but also enables the model to perform both image and video autoencoding. Extensive evaluations against strong recent baselines demonstrate the superior performance of our method. The project website can be found at~https://yzxing87.github.io/vae/{https://yzxing87.github.io/vae/}.', 'score': 5, 'issue_id': 1285, 'pub_date': '2024-12-23', 'pub_date_card': {'ru': '23 декабря', 'en': 'December 23', 'zh': '12月23日'}, 'hash': '58490715e8055e65', 'authors': ['Yazhou Xing', 'Yang Fei', 'Yingqing He', 'Jingye Chen', 'Jiaxin Xie', 'Xiaowei Chi', 'Qifeng Chen'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2412.17805.jpg', 'data': {'categories': ['#video', '#multimodal', '#optimization', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'Новый подход к созданию эффективных вариационных автоэнкодеров для видео', 'desc': 'Статья представляет новый мощный видео-автоэнкодер для высококачественного кодирования видео. Авторы предлагают раздельное сжатие пространственной и временной информации, чтобы избежать артефактов размытия движения. Они также используют текстовую информацию из датасетов для улучшения качества реконструкции и стабильности во времени. Модель обучается совместно на изображениях и видео, что повышает её универсальность.'}, 'en': {'title': 'Enhancing Video Generation with Temporal-Aware Compression and Text Guidance', 'desc': 'This paper introduces a new video Variational Autoencoder (VAE) that improves video generation by addressing the limitations of existing models. It focuses on enhancing both spatial and temporal compression to avoid issues like motion blur and detail loss. The model incorporates text guidance from text-to-video datasets, which boosts the quality of video reconstruction. Additionally, it is trained on both images and videos, allowing it to effectively encode and decode both types of data, leading to superior performance compared to previous methods.'}, 'zh': {'title': '提升视频编码质量的新方法', 'desc': '本文提出了一种新型的视频变分自编码器（VAE），旨在提高视频编码的质量和效率。我们通过引入时序感知的空间压缩方法，解决了传统3D VAE在运动模糊和细节失真的问题。同时，结合轻量级的运动压缩模型，进一步增强了时序压缩效果。此外，利用文本到视频数据集中的文本信息，提升了重建质量，特别是在细节保留和时序稳定性方面。'}}}, {'id': 'https://huggingface.co/papers/2412.16686', 'title': 'NILE: Internal Consistency Alignment in Large Language Models', 'url': 'https://huggingface.co/papers/2412.16686', 'abstract': "As a crucial step to enhance LLMs alignment with human intentions, Instruction Fine-Tuning (IFT) has a high demand on dataset quality. However, existing IFT datasets often contain knowledge that is inconsistent with LLMs' internal knowledge learned from the pre-training phase, which can greatly affect the efficacy of IFT. To address this issue, we introduce NILE (iNternal consIstency aLignmEnt) framework, aimed at optimizing IFT datasets to unlock LLMs' capability further. NILE operates by eliciting target pre-trained LLM's internal knowledge corresponding to instruction data. The internal knowledge is leveraged to revise the answer in IFT datasets. Additionally, we propose a novel Internal Consistency Filtering (ICF) method to filter training samples, ensuring its high consistency with LLM's internal knowledge. Our experiments demonstrate that NILE-aligned IFT datasets sharply boost LLM performance across multiple LLM ability evaluation datasets, achieving up to 66.6% gain on Arena-Hard and 68.5% on Alpaca-Eval V2. Further analysis confirms that each component of the NILE}framework contributes to these substantial performance improvements, and provides compelling evidence that dataset consistency with pre-trained internal knowledge is pivotal for maximizing LLM potential.", 'score': 4, 'issue_id': 1282, 'pub_date': '2024-12-21', 'pub_date_card': {'ru': '21 декабря', 'en': 'December 21', 'zh': '12月21日'}, 'hash': '0d1e640729e131e5', 'authors': ['Minda Hu', 'Qiyuan Zhang', 'Yufei Wang', 'Bowei He', 'Hongru Wang', 'Jingyan Zhou', 'Liangyou Li', 'Yasheng Wang', 'Chen Ma', 'Irwin King'], 'affiliations': ['City University of Hong Kong', 'Huawei Noahs Ark Lab', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.16686.jpg', 'data': {'categories': ['#dataset', '#training', '#optimization', '#data', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Согласование данных с внутренними знаниями LLM для улучшения инструктивной тонкой настройки', 'desc': 'Статья представляет фреймворк NILE для оптимизации наборов данных для инструктивной тонкой настройки (IFT) языковых моделей. NILE использует внутренние знания предварительно обученной модели для пересмотра ответов в наборах данных IFT. Авторы также предлагают метод фильтрации для обеспечения высокой согласованности данных с внутренними знаниями модели. Эксперименты показывают значительное улучшение производительности модели на различных наборах данных для оценки способностей LLM.'}, 'en': {'title': 'Aligning IFT Datasets for Enhanced LLM Performance', 'desc': "This paper addresses the challenge of aligning large language models (LLMs) with human intentions through Instruction Fine-Tuning (IFT). It highlights the problem of existing IFT datasets containing inconsistent knowledge that conflicts with the LLMs' pre-trained knowledge, which can hinder their performance. To solve this, the authors introduce the NILE framework, which optimizes IFT datasets by aligning them with the internal knowledge of the LLMs. The framework includes a method called Internal Consistency Filtering (ICF) to ensure high consistency, leading to significant performance improvements in LLM evaluations."}, 'zh': {'title': '优化数据集，提升LLM性能的关键', 'desc': '本论文提出了一种名为NILE的框架，旨在优化指令微调（IFT）数据集，以提高大型语言模型（LLMs）与人类意图的一致性。现有的IFT数据集常常包含与LLMs预训练阶段学习的内部知识不一致的信息，这会影响IFT的效果。NILE通过提取目标预训练LLM的内部知识来修正IFT数据集中的答案，并引入了一种新的内部一致性过滤（ICF）方法，以确保训练样本与LLM的内部知识高度一致。实验结果表明，NILE对IFT数据集的优化显著提升了LLM在多个评估数据集上的表现，证明了数据集与预训练内部知识一致性的重要性。'}}}, {'id': 'https://huggingface.co/papers/2412.17498', 'title': 'DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought', 'url': 'https://huggingface.co/papers/2412.17498', 'abstract': "Recently, O1-like models have emerged as representative examples, illustrating the effectiveness of long chain-of-thought (CoT) in reasoning tasks such as math and coding tasks. In this paper, we introduce DRT-o1, an attempt to bring the success of long CoT to neural machine translation (MT). Specifically, in view of the literature books that might involve similes and metaphors, translating these texts to a target language is very difficult in practice due to cultural differences. In such cases, literal translation often fails to convey the intended meaning effectively. Even for professional human translators, considerable thought must be given to preserving semantics throughout the translation process. To simulate LLMs' long thought ability in MT, we first mine sentences containing similes or metaphors from existing literature books, and then develop a multi-agent framework to translate these sentences via long thought. In the multi-agent framework, a translator is used to iteratively translate the source sentence under the suggestions provided by an advisor. To ensure the effectiveness of the long thoughts, an evaluator is also employed to judge whether the translation in the current round is better than the previous one or not. In this manner, we collect tens of thousands of long-thought MT data, which is used to train our DRT-o1. The experimental results on literature translation demonstrate the effectiveness of the DRT-o1. Using Qwen2.5-7B and Qwen2.5-14B as the backbones, the improvement brought by DRT-o1 achieves 7.33~8.26 BLEU and 1.66~3.36 CometScore. Besides, DRT-o1-7B can outperform QwQ-32B-Preview by 7.82 BLEU and 1.46 CometScore, showing its effectiveness. The project is available at https://github.com/krystalan/DRT-o1", 'score': 3, 'issue_id': 1286, 'pub_date': '2024-12-23', 'pub_date_card': {'ru': '23 декабря', 'en': 'December 23', 'zh': '12月23日'}, 'hash': '10ee2563b13fe4ff', 'authors': ['Jiaan Wang', 'Fandong Meng', 'Yunlong Liang', 'Jie Zhou'], 'affiliations': ['Pattern Recognition Center, WeChat AI, Tencent Inc'], 'pdf_title_img': 'assets/pdf/title_img/2412.17498.jpg', 'data': {'categories': ['#agents', '#dataset', '#reasoning', '#translation', '#multilingual', '#long_context'], 'emoji': '🤖', 'ru': {'title': 'Длинная цепочка рассуждений для улучшения машинного перевода художественной литературы', 'desc': 'Статья представляет DRT-o1 - модель для улучшения нейронного машинного перевода литературных текстов с использованием длинной цепочки рассуждений. Авторы предлагают мультиагентный подход, включающий переводчика, советника и оценщика, для итеративного улучшения перевода метафор и сравнений. На основе собранных данных обучается модель DRT-o1, которая значительно превосходит базовые модели по метрикам BLEU и CometScore. Эксперименты показывают эффективность предложенного метода для перевода сложных литературных текстов.'}, 'en': {'title': 'Enhancing Literary Translation with Long Chain-of-Thought Reasoning', 'desc': 'This paper presents DRT-o1, a novel approach to enhance neural machine translation (MT) by leveraging long chain-of-thought (CoT) reasoning. The method specifically addresses the challenges of translating literature that contains similes and metaphors, which often require deeper semantic understanding due to cultural nuances. DRT-o1 employs a multi-agent framework where a translator iteratively refines translations with guidance from an advisor and feedback from an evaluator. Experimental results indicate that DRT-o1 significantly improves translation quality, as evidenced by higher BLEU and CometScore metrics compared to existing models.'}, 'zh': {'title': '长链思维助力文学翻译的突破', 'desc': '本文介绍了一种新的神经机器翻译模型DRT-o1，旨在将长链思维（CoT）的成功应用于文学翻译。由于文化差异，翻译包含比喻和隐喻的文本非常困难，传统的逐字翻译往往无法有效传达原意。为了解决这个问题，研究者们开发了一个多智能体框架，通过迭代翻译和评估来优化翻译结果。实验结果表明，DRT-o1在文学翻译任务中表现出色，显著提高了翻译质量。'}}}, {'id': 'https://huggingface.co/papers/2412.16429', 'title': 'LearnLM: Improving Gemini for Learning', 'url': 'https://huggingface.co/papers/2412.16429', 'abstract': "Today's generative AI systems are tuned to present information by default rather than engage users in service of learning as a human tutor would. To address the wide range of potential education use cases for these systems, we reframe the challenge of injecting pedagogical behavior as one of pedagogical instruction following, where training and evaluation examples include system-level instructions describing the specific pedagogy attributes present or desired in subsequent model turns. This framing avoids committing our models to any particular definition of pedagogy, and instead allows teachers or developers to specify desired model behavior. It also clears a path to improving Gemini models for learning -- by enabling the addition of our pedagogical data to post-training mixtures -- alongside their rapidly expanding set of capabilities. Both represent important changes from our initial tech report. We show how training with pedagogical instruction following produces a LearnLM model (available on Google AI Studio) that is preferred substantially by expert raters across a diverse set of learning scenarios, with average preference strengths of 31\\% over GPT-4o, 11\\% over Claude 3.5, and 13\\% over the Gemini 1.5 Pro model LearnLM was based on.", 'score': 2, 'issue_id': 1286, 'pub_date': '2024-12-21', 'pub_date_card': {'ru': '21 декабря', 'en': 'December 21', 'zh': '12月21日'}, 'hash': '6cae08cebf8bc6ae', 'authors': ['LearnLM Team', 'Abhinit Modi', 'Aditya Srikanth Veerubhotla', 'Aliya Rysbek', 'Andrea Huber', 'Brett Wiltshire', 'Brian Veprek', 'Daniel Gillick', 'Daniel Kasenberg', 'Derek Ahmed', 'Irina Jurenka', 'James Cohan', 'Jennifer She', 'Julia Wilkowski', 'Kaiz Alarakyia', 'Kevin McKee', 'Lisa Wang', 'Markus Kunesch', 'Mike Schaekermann', 'Miruna Pîslar', 'Nikhil Joshi', 'Parsa Mahmoudieh', 'Paul Jhun', 'Sara Wiltberger', 'Shakir Mohamed', 'Shashank Agarwal', 'Shubham Milind Phal', 'Sun Jae Lee', 'Theofilos Strinopoulos', 'Wei-Jen Ko', 'Amy Wang', 'Ankit Anand', 'Avishkar Bhoopchand', 'Dan Wild', 'Divya Pandya', 'Filip Bar', 'Garth Graham', 'Holger Winnemoeller', 'Mahvish Nagda', 'Prateek Kolhar', 'Renee Schneider', 'Shaojian Zhu', 'Stephanie Chan', 'Steve Yadlowsky', 'Viknesh Sounderajah', 'Yannis Assael'], 'affiliations': ['Google'], 'pdf_title_img': 'assets/pdf/title_img/2412.16429.jpg', 'data': {'categories': ['#science', '#alignment', '#training', '#multimodal'], 'emoji': '🎓', 'ru': {'title': 'ИИ-учитель: гибкость и эффективность в образовании', 'desc': "Статья описывает новый подход к обучению моделей искусственного интеллекта для образовательных целей. Авторы предлагают метод 'следования педагогическим инструкциям', который позволяет настраивать поведение модели в соответствии с заданными педагогическими атрибутами. Эксперименты показали, что модель LearnLM, обученная этим методом, значительно превосходит другие крупные языковые модели в различных сценариях обучения. Этот подход открывает новые возможности для улучшения ИИ-систем в образовательной сфере."}, 'en': {'title': 'Empowering AI with Human-Like Teaching Skills', 'desc': 'This paper discusses how current generative AI systems primarily provide information instead of facilitating learning like a human tutor. The authors propose a new approach called pedagogical instruction following, which allows for the inclusion of specific teaching behaviors in the training and evaluation of AI models. This method gives educators the flexibility to define desired pedagogical attributes without being tied to a single definition of pedagogy. The results show that the LearnLM model, trained with this approach, significantly outperforms other models in various learning scenarios, indicating its effectiveness in educational applications.'}, 'zh': {'title': '教学指令跟随：提升生成式AI的学习能力', 'desc': '本文探讨了生成式人工智能系统在教育中的应用，提出了一种新的训练方法，即教学指令跟随。通过这种方法，教师或开发者可以指定期望的教学行为，而不需要固定的教学定义。这种灵活性使得模型能够更好地适应不同的学习场景，并提升其学习能力。研究表明，使用教学指令跟随训练的LearnLM模型在多种学习场景中得到了专家评审的高度认可。'}}}, {'id': 'https://huggingface.co/papers/2412.16720', 'title': 'OpenAI o1 System Card', 'url': 'https://huggingface.co/papers/2412.16720', 'abstract': 'The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning capabilities provide new avenues for improving the safety and robustness of our models. In particular, our models can reason about our safety policies in context when responding to potentially unsafe prompts, through deliberative alignment. This leads to state-of-the-art performance on certain benchmarks for risks such as generating illicit advice, choosing stereotyped responses, and succumbing to known jailbreaks. Training models to incorporate a chain of thought before answering has the potential to unlock substantial benefits, while also increasing potential risks that stem from heightened intelligence. Our results underscore the need for building robust alignment methods, extensively stress-testing their efficacy, and maintaining meticulous risk management protocols. This report outlines the safety work carried out for the OpenAI o1 and OpenAI o1-mini models, including safety evaluations, external red teaming, and Preparedness Framework evaluations.', 'score': 1, 'issue_id': 1286, 'pub_date': '2024-12-21', 'pub_date_card': {'ru': '21 декабря', 'en': 'December 21', 'zh': '12月21日'}, 'hash': '5eed348dd7fb2826', 'authors': ['OpenAI', ':', 'Aaron Jaech', 'Adam Kalai', 'Adam Lerer', 'Adam Richardson', 'Ahmed El-Kishky', 'Aiden Low', 'Alec Helyar', 'Aleksander Madry', 'Alex Beutel', 'Alex Carney', 'Alex Iftimie', 'Alex Karpenko', 'Alex Tachard Passos', 'Alexander Neitz', 'Alexander Prokofiev', 'Alexander Wei', 'Allison Tam', 'Ally Bennett', 'Ananya Kumar', 'Andre Saraiva', 'Andrea Vallone', 'Andrew Duberstein', 'Andrew Kondrich', 'Andrey Mishchenko', 'Andy Applebaum', 'Angela Jiang', 'Ashvin Nair', 'Barret Zoph', 'Behrooz Ghorbani', 'Ben Rossen', 'Benjamin Sokolowsky', 'Boaz Barak', 'Bob McGrew', 'Borys Minaiev', 'Botao Hao', 'Bowen Baker', 'Brandon Houghton', 'Brandon McKinzie', 'Brydon Eastman', 'Camillo Lugaresi', 'Cary Bassin', 'Cary Hudson', 'Chak Ming Li', 'Charles de Bourcy', 'Chelsea Voss', 'Chen Shen', 'Chong Zhang', 'Chris Koch', 'Chris Orsinger', 'Christopher Hesse', 'Claudia Fischer', 'Clive Chan', 'Dan Roberts', 'Daniel Kappler', 'Daniel Levy', 'Daniel Selsam', 'David Dohan', 'David Farhi', 'David Mely', 'David Robinson', 'Dimitris Tsipras', 'Doug Li', 'Dragos Oprica', 'Eben Freeman', 'Eddie Zhang', 'Edmund Wong', 'Elizabeth Proehl', 'Enoch Cheung', 'Eric Mitchell', 'Eric Wallace', 'Erik Ritter', 'Evan Mays', 'Fan Wang', 'Felipe Petroski Such', 'Filippo Raso', 'Florencia Leoni', 'Foivos Tsimpourlas', 'Francis Song', 'Fred von Lohmann', 'Freddie Sulit', 'Geoff Salmon', 'Giambattista Parascandolo', 'Gildas Chabot', 'Grace Zhao', 'Greg Brockman', 'Guillaume Leclerc', 'Hadi Salman', 'Haiming Bao', 'Hao Sheng', 'Hart Andrin', 'Hessam Bagherinezhad', 'Hongyu Ren', 'Hunter Lightman', 'Hyung Won Chung', 'Ian Kivlichan', "Ian O'Connell", 'Ian Osband', 'Ignasi Clavera Gilaberte', 'Ilge Akkaya', 'Ilya Kostrikov', 'Ilya Sutskever', 'Irina Kofman', 'Jakub Pachocki', 'James Lennon', 'Jason Wei', 'Jean Harb', 'Jerry Twore', 'Jiacheng Feng', 'Jiahui Yu', 'Jiayi Weng', 'Jie Tang', 'Jieqi Yu', 'Joaquin Quiñonero Candela', 'Joe Palermo', 'Joel Parish', 'Johannes Heidecke', 'John Hallman', 'John Rizzo', 'Jonathan Gordon', 'Jonathan Uesato', 'Jonathan Uesato', 'Jonathan Ward', 'Joost Huizinga', 'Julie Wang', 'Kai Chen', 'Kai Xiao', 'Karan Singhal', 'Karina Nguyen', 'Karl Cobbe', 'Katy Shi', 'Kayla Wood', 'Kendra Rimbach', 'Keren Gu-Lemberg', 'Keren GuLemberg', 'Kevin Liu', 'Kevin Lu', 'Kevin Stone', 'Kevin Yu', 'Lama Ahmad', 'Lauren Yang', 'Leo Liu', 'Leon Maksin', 'Leyton Ho', 'Liam Fedus', 'Lilian Weng', 'Linden Li', 'Lindsay McCallum', 'Lindsey Held', 'Lorenz Kuhn', 'Lukas Kondraciuk', 'Lukasz Kaiser', 'Luke Metz', 'Madelaine Boyd', 'Maja Trebacz', 'Manas Joglekar', 'Mark Chen', 'Marko Tintor', 'Mason Meyer', 'Matt Jones', 'Matt Kaufer', 'Max Schwarzer', 'Meghan Shah', 'Mehmet Yatbaz', 'Melody Guan', 'Mengyuan Xu', 'Mengyuan Yan', 'Mia Glaese', 'Mianna Chen', 'Mianna Chen', 'Michael Lampe', 'Michael Malek', 'Michele Wang', 'Michelle Fradin', 'Mike McClay', 'Mikhail Pavlov', 'Miles Wang', 'Mingxuan Wang', 'Mira Murati', 'Mo Bavarian', 'Mostafa Rohaninejad', 'Nat McAleese', 'Neil Chowdhury', 'Neil Chowdhury', 'Nick Ryder', 'Nikolas Tezak', 'Noam Brown', 'Ofir Nachum', 'Oleg Boiko', 'Oleg Murk', 'Olivia Watkins', 'Patrick Chao', 'Paul Ashbourne', 'Pavel Izmailov', 'Peter Zhokhov', 'Rachel Dias', 'Rahul Arora', 'Randall Lin', 'Rapha Gontijo Lopes', 'Raz Gaon', 'Reah Miyara', 'Reimar Leike', 'Renny Hwang', 'Rhythm Garg', 'Robin Brown', 'Roshan James', 'Rui Shu', 'Ryan Cheu', 'Ryan Greene', 'Saachi Jain', 'Sam Altman', 'Sam Toizer', 'Sam Toyer', 'Samuel Miserendino', 'Sandhini Agarwal', 'Santiago Hernandez', 'Sasha Baker', 'Scott McKinney', 'Scottie Yan', 'Shengjia Zhao', 'Shengli Hu', 'Shibani Santurkar', 'Shraman Ray Chaudhuri', 'Shuyuan Zhang', 'Siyuan Fu', 'Spencer Papay', 'Steph Lin', 'Suchir Balaji', 'Suvansh Sanjeev', 'Szymon Sidor', 'Tal Broda', 'Aidan Clark', 'Tao Wang', 'Taylor Gordon', 'Ted Sanders', 'Tejal Patwardhan', 'Thibault Sottiaux', 'Thomas Degry', 'Thomas Dimson', 'Tianhao Zheng', 'Timur Garipov', 'Tom Stasi', 'Trapit Bansal', 'Trevor Creech', 'Troy Peterson', 'Tyna Eloundou', 'Valerie Qi', 'Vineet Kosaraju', 'Vinnie Monaco', 'Vitchyr Pong', 'Vlad Fomenko', 'Weiyi Zheng', 'Wenda Zhou', 'Wes McCabe', 'Wojciech Zaremba', 'Yann Dubois', 'Yinghai Lu', 'Yining Chen', 'Young Cha', 'Yu Bai', 'Yuchen He', 'Yuchen Zhang', 'Yunyun Wang', 'Zheng Shao', 'Zhuohan Li'], 'affiliations': ['OpenAI'], 'pdf_title_img': 'assets/pdf/title_img/2412.16720.jpg', 'data': {'categories': ['#security', '#reasoning', '#alignment', '#benchmark', '#healthcare', '#rl', '#training', '#ethics'], 'emoji': '🧠', 'ru': {'title': 'Усиление безопасности ИИ через рассуждения и самоанализ', 'desc': 'Модели серии o1 обучены с помощью масштабного обучения с подкреплением для рассуждений с использованием цепочки мыслей. Эти продвинутые возможности рассуждения открывают новые пути для повышения безопасности и надежности моделей. Модели могут рассуждать о политиках безопасности в контексте при ответе на потенциально небезопасные запросы через делиберативное выравнивание. Это приводит к лучшим результатам на определенных тестах по таким рискам, как генерация незаконных советов, выбор стереотипных ответов и подверженность известным методам обхода ограничений.'}, 'en': {'title': 'Enhancing AI Safety through Chain of Thought Reasoning', 'desc': "The o1 model series utilizes large-scale reinforcement learning to enhance reasoning through a chain of thought approach. This method improves the models' ability to align with safety policies when faced with potentially harmful prompts, thereby increasing their robustness. The models demonstrate superior performance on benchmarks related to generating unsafe content and responding to biased queries. The findings highlight the importance of developing strong alignment techniques and rigorous risk management strategies to ensure the safe deployment of advanced AI systems."}, 'zh': {'title': '思维链推理：提升模型安全性的关键', 'desc': 'o1模型系列通过大规模强化学习进行训练，能够使用思维链进行推理。这种先进的推理能力为提高模型的安全性和稳健性提供了新的途径。特别是，我们的模型能够在响应潜在不安全的提示时，考虑安全政策的上下文，从而实现深思熟虑的对齐。这项研究强调了构建稳健的对齐方法和进行全面的风险管理的重要性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (1)', '#agi', '#alignment (3)', '#architecture (3)', '#audio', '#benchmark (2)', '#cv', '#data (3)', '#dataset (5)', '#diffusion', '#ethics (1)', '#games', '#graphs', '#hallucinations', '#healthcare (1)', '#inference (1)', '#interpretability', '#leakage', '#long_context (2)', '#low_resource', '#machine_translation', '#math (1)', '#multilingual (1)', '#multimodal (3)', '#open_source (1)', '#optimization (9)', '#plp (1)', '#rag', '#reasoning (6)', '#rl (2)', '#rlhf', '#robotics', '#science (1)', '#security (1)', '#small_models', '#story_generation', '#survey', '#synthetic', '#training (10)', '#transfer_learning', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-12-24 08:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-12-24 08:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-12-24 08:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    