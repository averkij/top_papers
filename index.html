
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 14 papers. September 1.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">1 сентября</span> | <span id="title-articles-count">14 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-08-29.html">⬅️ <span id="prev-date">29.08</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-09-02.html">➡️ <span id="next-date">02.09</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-09.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '1 сентября', 'en': 'September 1', 'zh': '9月1日'};
        let feedDateNext = {'ru': '02.09', 'en': '09/02', 'zh': '9月2日'};
        let feedDatePrev = {'ru': '29.08', 'en': '08/29', 'zh': '8月29日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2508.21113', 'title': 'R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs\n  via Bi-Mode Annealing and Reinforce Learning', 'url': 'https://huggingface.co/papers/2508.21113', 'abstract': "R-4B, an auto-thinking multimodal large language model, uses bi-mode annealing and Bi-mode Policy Optimization to adaptively decide on problem-solving strategies, achieving state-of-the-art performance with lower computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking capabilities have demonstrated remarkable performance on complex reasoning problems. However, this thinking process is redundant for simple problems solvable without complex reasoning. To address this inefficiency, we propose R-4B, an auto-thinking MLLM, which can adaptively decide when to think based on problem complexity. The central idea of R-4B is to empower the model with both thinking and non-thinking capabilities using bi-mode annealing, and apply Bi-mode Policy Optimization~(BPO) to improve the model's accuracy in determining whether to activate the thinking process. Specifically, we first train the model on a carefully curated dataset spanning various topics, which contains samples from both thinking and non-thinking modes. Then it undergoes a second phase of training under an improved GRPO framework, where the policy model is forced to generate responses from both modes for each input query. Experimental results show that R-4B achieves state-of-the-art performance across 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks and achieves performance comparable to larger models such as Kimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower computational cost.", 'score': 76, 'issue_id': 5638, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': 'e3b0726caba25eb1', 'authors': ['Jie Jiang', 'Qi Yang', 'Bolin Ni', 'Shiming Xiang', 'Han Hu', 'Houwen Peng'], 'affiliations': ['Institute of Automation, CAS', 'Tencent Hunyuan Team'], 'pdf_title_img': 'assets/pdf/title_img/2508.21113.jpg', 'data': {'categories': ['#training', '#multimodal', '#reasoning', '#benchmark', '#dataset', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'R-4B: Адаптивное мышление для эффективного решения задач', 'desc': 'R-4B - это мультимодальная большая языковая модель с возможностью автоматического мышления. Она использует двухрежимный отжиг и двухрежимную оптимизацию политики для адаптивного выбора стратегии решения задач. Модель способна определять, когда нужно активировать процесс мышления в зависимости от сложности проблемы. R-4B достигает передовых результатов на 25 сложных бенчмарках при меньших вычислительных затратах по сравнению с более крупными моделями.'}, 'en': {'title': 'R-4B: Smart Thinking for Efficient Problem Solving', 'desc': 'R-4B is an innovative multimodal large language model (MLLM) that enhances problem-solving efficiency by using bi-mode annealing and Bi-mode Policy Optimization. This model intelligently decides when to engage in complex reasoning, avoiding unnecessary computations for simpler tasks. By training on a diverse dataset that includes both thinking and non-thinking examples, R-4B learns to optimize its responses based on the complexity of the problem. The results demonstrate that R-4B achieves top performance on 25 benchmarks while maintaining lower computational costs compared to larger models.'}, 'zh': {'title': 'R-4B：智能思考与高效解决的结合', 'desc': 'R-4B是一种自动思考的多模态大型语言模型，能够根据问题的复杂性自适应地决定何时进行思考。它采用双模退火和双模策略优化技术，以提高模型在解决问题时的效率和准确性。通过在多样化的数据集上进行训练，R-4B能够在简单问题上避免冗余的思考过程，从而降低计算成本。实验结果表明，R-4B在25个具有挑战性的基准测试中表现优异，超越了许多现有模型。'}}}, {'id': 'https://huggingface.co/papers/2508.21112', 'title': 'EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for\n  General Robot Control', 'url': 'https://huggingface.co/papers/2508.21112', 'abstract': 'EO-Robotics, comprising EO-1 model and EO-Data1.5M dataset, advances multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training.  \t\t\t\t\tAI-generated summary \t\t\t\t The human ability to seamlessly perform multimodal reasoning and physical interaction in the open world is a core goal for general-purpose embodied intelligent systems. Recent vision-language-action (VLA) models, which are co-trained on large-scale robot and visual-text data, have demonstrated notable progress in general robot control. However, they still fail to achieve human-level flexibility in interleaved reasoning and interaction. In this work, introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is a unified embodied foundation model that achieves superior performance in multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training. The development of EO-1 is based on two key pillars: (i) a unified architecture that processes multimodal inputs indiscriminately (image, text, video, and action), and (ii) a massive, high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains over 1.5 million samples with emphasis on interleaved vision-text-action comprehension. EO-1 is trained through synergies between auto-regressive decoding and flow matching denoising on EO-Data1.5M, enabling seamless robot action generation and multimodal embodied reasoning. Extensive experiments demonstrate the effectiveness of interleaved vision-text-action learning for open-world understanding and generalization, validated through a variety of long-horizon, dexterous manipulation tasks across multiple embodiments. This paper details the architecture of EO-1, the data construction strategy of EO-Data1.5M, and the training methodology, offering valuable insights for developing advanced embodied foundation models.', 'score': 45, 'issue_id': 5638, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': '5bbbfa48bbd5fb7c', 'authors': ['Delin Qu', 'Haoming Song', 'Qizhi Chen', 'Zhaoqing Chen', 'Xianqiang Gao', 'Xinyi Ye', 'Qi Lv', 'Modi Shi', 'Guanghui Ren', 'Cheng Ruan', 'Maoqing Yao', 'Haoran Yang', 'Jiacheng Bao', 'Bin Zhao', 'Dong Wang'], 'affiliations': ['AgiBot', 'Fudan University', 'Northwestern Polytechnical University', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2508.21112.jpg', 'data': {'categories': ['#architecture', '#training', '#agents', '#multimodal', '#agi', '#reasoning', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'Революция в мультимодальном ИИ для роботов: EO-Robotics объединяет зрение, текст и действие', 'desc': 'EO-Robotics представляет собой систему, состоящую из модели EO-1 и датасета EO-Data1.5M, которая продвигает мультимодальное воплощенное рассуждение и управление роботами через смешанное предобучение на основе зрения, текста и действий. Модель EO-1 использует унифицированную архитектуру для обработки мультимодальных входных данных и обучается на массивном высококачественном датасете EO-Data1.5M, содержащем более 1,5 миллиона образцов. Обучение модели происходит с использованием авторегрессивного декодирования и денойзинга методом сопоставления потоков. Эксперименты демонстрируют эффективность смешанного обучения на основе зрения, текста и действий для понимания открытого мира и обобщения на различные задачи манипуляции.'}, 'en': {'title': 'Empowering Robots with Multimodal Reasoning and Control', 'desc': "EO-Robotics introduces a new model called EO-1, which enhances robot control and reasoning by integrating vision, text, and action in its training process. The EO-Data1.5M dataset, containing over 1.5 million samples, supports this model by providing diverse multimodal data for better understanding and interaction. EO-1 utilizes a unified architecture that processes various inputs simultaneously, allowing for more flexible and human-like responses in real-world scenarios. The research demonstrates that interleaved learning of vision, text, and action significantly improves the robot's ability to perform complex tasks and adapt to different environments."}, 'zh': {'title': '提升机器人控制的多模态推理新突破', 'desc': 'EO-Robotics是一个新模型，包含EO-1模型和EO-Data1.5M数据集，旨在通过交替的视觉-文本-动作预训练来提升多模态的具身推理和机器人控制能力。EO-1模型能够处理图像、文本、视频和动作等多种输入，展现出在多模态具身推理和机器人控制方面的优越性能。该模型的训练依赖于一个包含超过150万样本的高质量数据集，强调交替的视觉-文本-动作理解。通过大量实验，验证了交替学习在开放世界理解和泛化中的有效性，提供了构建先进具身基础模型的宝贵见解。'}}}, {'id': 'https://huggingface.co/papers/2508.18106', 'title': 'A.S.E: A Repository-Level Benchmark for Evaluating Security in\n  AI-Generated Code', 'url': 'https://huggingface.co/papers/2508.18106', 'abstract': "A.S.E is a benchmark for evaluating the security of code generated by large language models using real-world repositories and expert-defined rules, revealing insights into model performance and decoding strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t The increasing adoption of large language models (LLMs) in software engineering necessitates rigorous security evaluation of their generated code. However, existing benchmarks are inadequate, as they focus on isolated code snippets, employ unstable evaluation methods that lack reproducibility, and fail to connect the quality of input context with the security of the output. To address these gaps, we introduce A.S.E (AI Code Generation Security Evaluation), a benchmark for repository-level secure code generation. A.S.E constructs tasks from real-world repositories with documented CVEs, preserving full repository context like build systems and cross-file dependencies. Its reproducible, containerized evaluation framework uses expert-defined rules to provide stable, auditable assessments of security, build quality, and generation stability. Our evaluation of leading LLMs on A.S.E reveals three key findings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The security gap between proprietary and open-source models is narrow; Qwen3-235B-A22B-Instruct attains the top security score. (3) Concise, ``fast-thinking'' decoding strategies consistently outperform complex, ``slow-thinking'' reasoning for security patching.", 'score': 43, 'issue_id': 5638, 'pub_date': '2025-08-25', 'pub_date_card': {'ru': '25 августа', 'en': 'August 25', 'zh': '8月25日'}, 'hash': '7b691aaa7b52bcfd', 'authors': ['Keke Lian', 'Bin Wang', 'Lei Zhang', 'Libo Chen', 'Junjie Wang', 'Ziming Zhao', 'Yujiu Yang', 'Haotong Duan', 'Haoran Zhao', 'Shuang Liao', 'Mingda Guo', 'Jiazheng Quan', 'Yilu Zhong', 'Chenhao He', 'Zichuan Chen', 'Jie Wu', 'Haoling Li', 'Zhaoxuan Li', 'Jiongchi Yu', 'Hui Li', 'Dong Zhang'], 'affiliations': ['Fudan University', 'Institute of Information Engineering, Chinese Academy of Sciences', 'Peking University', 'Shanghai Jiao Tong University', 'Singapore Management University', 'Tencent', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.18106.jpg', 'data': {'categories': ['#open_source', '#security', '#benchmark', '#dataset'], 'emoji': '🛡️', 'ru': {'title': 'A.S.E: Новый стандарт оценки безопасности ИИ-генерируемого кода', 'desc': 'A.S.E - это новый бенчмарк для оценки безопасности кода, генерируемого большими языковыми моделями (LLM). Он использует реальные репозитории и правила, определенные экспертами, что позволяет получить более точные результаты по сравнению с существующими методами. A.S.E сохраняет полный контекст репозитория, включая системы сборки и зависимости между файлами. Исследование показало, что Claude-3.7-Sonnet демонстрирует лучшую общую производительность, а разрыв в безопасности между проприетарными и открытыми моделями невелик.'}, 'en': {'title': 'A.S.E: Elevating Security Evaluation for AI-Generated Code', 'desc': 'The paper introduces A.S.E, a benchmark designed to evaluate the security of code generated by large language models (LLMs) in a more realistic context. Unlike previous benchmarks that only assess isolated code snippets, A.S.E uses real-world repositories and incorporates expert-defined rules to ensure reproducibility and stability in evaluations. It focuses on repository-level secure code generation, taking into account the entire context, including build systems and cross-file dependencies. The findings indicate that certain models excel in security performance, and simpler decoding strategies are more effective for generating secure code patches.'}, 'zh': {'title': 'A.S.E：提升代码生成安全性的基准评估', 'desc': 'A.S.E是一个用于评估大型语言模型生成代码安全性的基准，利用真实世界的代码库和专家定义的规则。现有的基准测试方法存在不足，无法有效连接输入上下文的质量与输出的安全性。A.S.E通过构建真实代码库中的任务，保留完整的上下文信息，提供可重复的安全评估。我们的评估结果显示，Claude-3.7-Sonnet在整体表现上最佳，而Qwen3-235B-A22B-Instruct在安全性评分上表现突出。'}}}, {'id': 'https://huggingface.co/papers/2508.20470', 'title': 'Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation', 'url': 'https://huggingface.co/papers/2508.20470', 'abstract': 'Using video data to provide commonsense priors enhances 3D asset generation, enabling spatial consistency and semantic plausibility in 3D content creation.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling laws have validated the success and promise of large-data-trained models in creative generation across text, image, and video domains. However, this paradigm faces data scarcity in the 3D domain, as there is far less of it available on the internet compared to the aforementioned modalities. Fortunately, there exist adequate videos that inherently contain commonsense priors, offering an alternative supervisory signal to mitigate the generalization bottleneck caused by limited native 3D data. On the one hand, videos capturing multiple views of an object or scene provide a spatial consistency prior for 3D generation. On the other hand, the rich semantic information contained within the videos enables the generated content to be more faithful to the text prompts and semantically plausible. This paper explores how to apply the video modality in 3D asset generation, spanning datasets to models. We introduce Droplet3D-4M, the first large-scale video dataset with multi-view level annotations, and train Droplet3D, a generative model supporting both image and dense text input. Extensive experiments validate the effectiveness of our approach, demonstrating its ability to produce spatially consistent and semantically plausible content. Moreover, in contrast to the prevailing 3D solutions, our approach exhibits the potential for extension to scene-level applications. This indicates that the commonsense priors from the videos significantly facilitate 3D creation. We have open-sourced all resources including the dataset, code, technical framework, and model weights: https://dropletx.github.io/.', 'score': 26, 'issue_id': 5642, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': '2bbd88d7f14f5a78', 'authors': ['Xiaochuan Li', 'Guoguang Du', 'Runze Zhang', 'Liang Jin', 'Qi Jia', 'Lihua Lu', 'Zhenhua Guo', 'Yaqian Zhao', 'Haiyang Liu', 'Tianqi Wang', 'Changsheng Li', 'Xiaoli Gong', 'Rengang Li', 'Baoyu Fan'], 'affiliations': ['IEIT System Co., Ltd.', 'Nankai University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.20470.jpg', 'data': {'categories': ['#dataset', '#3d', '#multimodal', '#open_source', '#synthetic'], 'emoji': '🎥', 'ru': {'title': 'Видео как источник здравого смысла для 3D-генерации', 'desc': 'Эта статья исследует применение видеоданных для улучшения генерации 3D-объектов. Авторы представляют датасет Droplet3D-4M с аннотациями многоракурсных видео и модель Droplet3D, способную генерировать 3D-контент по изображениям и текстовым описаниям. Использование видео позволяет улучшить пространственную согласованность и семантическую правдоподобность создаваемых 3D-активов. Эксперименты подтверждают эффективность подхода и его потенциал для применения в генерации сцен.'}, 'en': {'title': 'Enhancing 3D Asset Generation with Video Commonsense Priors', 'desc': 'This paper discusses how using video data can improve the generation of 3D assets by providing commonsense knowledge that helps maintain spatial consistency and semantic accuracy. It highlights the challenge of limited 3D data compared to other media types, like text and images, and proposes leveraging videos as a solution. The authors introduce Droplet3D-4M, a large dataset with multi-view annotations, and a generative model called Droplet3D that can process both images and text. Their experiments show that this approach not only enhances the quality of 3D content but also allows for broader applications in scene generation.'}, 'zh': {'title': '利用视频数据提升3D生成的空间与语义一致性', 'desc': '本论文探讨了如何利用视频数据来增强3D资产生成，提供空间一致性和语义合理性。由于3D领域的数据稀缺，视频中的常识先验成为了一种有效的替代监督信号。视频捕捉的多视角信息为3D生成提供了空间一致性，而丰富的语义信息则使生成的内容更符合文本提示。我们介绍了Droplet3D-4M数据集和Droplet3D生成模型，实验结果表明该方法在3D内容生成中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2508.13618', 'title': 'TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head\n  Synthesis', 'url': 'https://huggingface.co/papers/2508.13618', 'abstract': 'TalkVid, a large-scale, high-quality, and diverse dataset, improves audio-driven talking head synthesis by enhancing generalization across human diversity and revealing subgroup performance disparities.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-driven talking head synthesis has achieved remarkable photorealism, yet state-of-the-art (SOTA) models exhibit a critical failure: they lack generalization to the full spectrum of human diversity in ethnicity, language, and age groups. We argue that this generalization gap is a direct symptom of limitations in existing training data, which lack the necessary scale, quality, and diversity. To address this challenge, we introduce TalkVid, a new large-scale, high-quality, and diverse dataset containing 1244 hours of video from 7729 unique speakers. TalkVid is curated through a principled, multi-stage automated pipeline that rigorously filters for motion stability, aesthetic quality, and facial detail, and is validated against human judgments to ensure its reliability. Furthermore, we construct and release TalkVid-Bench, a stratified evaluation set of 500 clips meticulously balanced across key demographic and linguistic axes. Our experiments demonstrate that a model trained on TalkVid outperforms counterparts trained on previous datasets, exhibiting superior cross-dataset generalization. Crucially, our analysis on TalkVid-Bench reveals performance disparities across subgroups that are obscured by traditional aggregate metrics, underscoring its necessity for future research. Code and data can be found in https://github.com/FreedomIntelligence/TalkVid', 'score': 13, 'issue_id': 5639, 'pub_date': '2025-08-19', 'pub_date_card': {'ru': '19 августа', 'en': 'August 19', 'zh': '8月19日'}, 'hash': '8baf01eb014bc50c', 'authors': ['Shunian Chen', 'Hejin Huang', 'Yexin Liu', 'Zihan Ye', 'Pengcheng Chen', 'Chenghao Zhu', 'Michael Guan', 'Rongsheng Wang', 'Junying Chen', 'Guanbin Li', 'Ser-Nam Lim', 'Harry Yang', 'Benyou Wang'], 'affiliations': ['Sun Yat-sen University', 'The Chinese University of Hong Kong, Shenzhen', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2508.13618.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#ethics', '#transfer_learning', '#dataset', '#cv', '#data'], 'emoji': '🗣️', 'ru': {'title': 'TalkVid: большой и разнообразный датасет для улучшения синтеза говорящих голов', 'desc': 'Представлен новый набор данных TalkVid для улучшения синтеза говорящих голов на основе аудио. Этот датасет содержит 1244 часа видео от 7729 уникальных дикторов и отличается высоким качеством и разнообразием. Модель, обученная на TalkVid, превосходит аналоги по обобщающей способности на различных демографических группах. Также создан стратифицированный набор данных TalkVid-Bench для оценки производительности на различных подгруппах.'}, 'en': {'title': 'Bridging the Diversity Gap in AI with TalkVid', 'desc': 'The paper introduces TalkVid, a new dataset designed to improve audio-driven talking head synthesis by addressing the generalization gap in existing models. This gap arises from the lack of diversity in training data, which often fails to represent various ethnicities, languages, and age groups. TalkVid consists of 1244 hours of video from 7729 unique speakers, curated through a rigorous process to ensure high quality and diversity. The study also presents TalkVid-Bench, an evaluation set that highlights performance disparities among different demographic groups, emphasizing the importance of diverse datasets in machine learning research.'}, 'zh': {'title': 'TalkVid：提升虚拟人头合成的多样性与泛化能力', 'desc': 'TalkVid是一个大规模、高质量和多样化的数据集，旨在改善基于音频的虚拟人头合成技术。现有的模型在处理不同种族、语言和年龄群体时存在泛化能力不足的问题，这主要是由于训练数据的规模和多样性不足。为了解决这个问题，TalkVid包含了1244小时来自7729个独特说话者的视频，经过严格的多阶段自动化筛选，确保了数据的稳定性和美学质量。我们的实验表明，基于TalkVid训练的模型在跨数据集泛化能力上优于以往的数据集，同时也揭示了不同子群体之间的性能差异。'}}}, {'id': 'https://huggingface.co/papers/2508.21767', 'title': 'UItron: Foundational GUI Agent with Advanced Perception and Planning', 'url': 'https://huggingface.co/papers/2508.21767', 'abstract': 'UItron, an open-source foundational model for GUI agents, enhances visual understanding and task planning through advanced perception, grounding, and planning capabilities, achieving superior performance in Chinese app scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t GUI agent aims to enable automated operations on Mobile/PC devices, which is an important task toward achieving artificial general intelligence. The rapid advancement of VLMs accelerates the development of GUI agents, owing to their powerful capabilities in visual understanding and task planning. However, building a GUI agent remains a challenging task due to the scarcity of operation trajectories, the availability of interactive infrastructure, and the limitation of initial capabilities in foundation models. In this work, we introduce UItron, an open-source foundational model for automatic GUI agents, featuring advanced GUI perception, grounding, and planning capabilities. UItron highlights the necessity of systemic data engineering and interactive infrastructure as foundational components for advancing GUI agent development. It not only systematically studies a series of data engineering strategies to enhance training effects, but also establishes an interactive environment connecting both Mobile and PC devices. In training, UItron adopts supervised finetuning over perception and planning tasks in various GUI scenarios, and then develop a curriculum reinforcement learning framework to enable complex reasoning and exploration for online environments. As a result, UItron achieves superior performance in benchmarks of GUI perception, grounding, and planning. In particular, UItron highlights the interaction proficiency with top-tier Chinese mobile APPs, as we identified a general lack of Chinese capabilities even in state-of-the-art solutions. To this end, we manually collect over one million steps of operation trajectories across the top 100 most popular apps, and build the offline and online agent evaluation environments. Experimental results demonstrate that UItron achieves significant progress in Chinese app scenarios, propelling GUI agents one step closer to real-world application.', 'score': 4, 'issue_id': 5639, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 августа', 'en': 'August 29', 'zh': '8月29日'}, 'hash': '4e413654a21d562e', 'authors': ['Zhixiong Zeng', 'Jing Huang', 'Liming Zheng', 'Wenkang Han', 'Yufeng Zhong', 'Lei Chen', 'Longrong Yang', 'Yingjie Chu', 'Yuzhi He', 'Lin Ma'], 'affiliations': ['Meituan'], 'pdf_title_img': 'assets/pdf/title_img/2508.21767.jpg', 'data': {'categories': ['#benchmark', '#rl', '#open_source', '#training', '#optimization', '#reasoning', '#agi', '#dataset', '#data', '#agents'], 'emoji': '🤖', 'ru': {'title': 'UItron: ИИ-агент для автоматизации графических интерфейсов', 'desc': 'UItron - это модель машинного обучения с открытым исходным кодом для автоматизации работы с графическим интерфейсом. Она улучшает визуальное понимание и планирование задач с помощью продвинутых возможностей восприятия, привязки к контексту и планирования. UItron демонстрирует превосходную производительность в сценариях работы с китайскими приложениями. Модель использует стратегии инженерии данных и интерактивную инфраструктуру для повышения эффективности обучения.'}, 'en': {'title': 'UItron: Advancing GUI Agents for Real-World Applications', 'desc': 'UItron is an open-source foundational model designed to enhance the capabilities of GUI agents, focusing on visual understanding and task planning. It addresses challenges in developing GUI agents, such as limited operation trajectories and the need for interactive infrastructure. By employing advanced perception, grounding, and planning techniques, UItron systematically improves training through data engineering and a curriculum reinforcement learning framework. The model demonstrates exceptional performance in Chinese app scenarios, filling a gap in existing solutions and moving closer to practical applications of GUI agents.'}, 'zh': {'title': 'UItron：推动图形用户界面代理的未来', 'desc': 'UItron是一个开源的基础模型，专为图形用户界面（GUI）代理设计，提升了视觉理解和任务规划能力。该模型通过先进的感知、定位和规划功能，在中文应用场景中表现出色。UItron强调了系统数据工程和交互基础设施在GUI代理开发中的重要性，并通过监督微调和强化学习框架来增强训练效果。实验结果表明，UItron在中文应用场景中取得了显著进展，使GUI代理更接近实际应用。'}}}, {'id': 'https://huggingface.co/papers/2508.21365', 'title': 'Think in Games: Learning to Reason in Games via Reinforcement Learning\n  with Large Language Models', 'url': 'https://huggingface.co/papers/2508.21365', 'abstract': 'Think in Games (TiG) framework enables large language models to develop procedural knowledge through interactive game environments, achieving competitive performance with reduced data and computational demands while providing transparent explanations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at complex reasoning tasks such as mathematics and coding, yet they frequently struggle with simple interactive tasks that young children perform effortlessly. This discrepancy highlights a critical gap between declarative knowledge (knowing about something) and procedural knowledge (knowing how to do something). Although traditional reinforcement learning (RL) agents can acquire procedural knowledge through environmental interaction, they often operate as black boxes and require substantial training data. In contrast, LLMs possess extensive world knowledge and reasoning capabilities, but are unable to effectively convert this static knowledge into dynamic decision-making in interactive settings. To address this challenge, we propose Think in Games (TiG), a novel framework that empowers LLMs to develop procedural understanding through direct interaction with game environments, while retaining their inherent reasoning and explanatory abilities. Specifically, TiG reformulates RL-based decision-making as a language modeling task: LLMs generate language-guided policies, which are refined iteratively through online reinforcement learning based on environmental feedback. Our experimental results show that TiG successfully bridges the gap between declarative and procedural knowledge, achieving competitive performance with dramatically lower data and computational demands compared to conventional RL methods. Moreover, TiG provides step-by-step natural language explanations for its decisions, greatly improving transparency and interpretability in complex interactive tasks.', 'score': 4, 'issue_id': 5639, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 августа', 'en': 'August 29', 'zh': '8月29日'}, 'hash': 'cbcbd196468063e9', 'authors': ['Yi Liao', 'Yu Gu', 'Yuan Sui', 'Zining Zhu', 'Yifan Lu', 'Guohua Tang', 'Zhongqian Sun', 'Wei Yang'], 'affiliations': ['Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2508.21365.jpg', 'data': {'categories': ['#training', '#games', '#optimization', '#reasoning', '#interpretability', '#multimodal', '#rl', '#rlhf'], 'emoji': '🎮', 'ru': {'title': 'Обучение ИИ через игры: от знаний к умениям', 'desc': 'Предложена новая система Think in Games (TiG), позволяющая большим языковым моделям развивать процедурные знания через взаимодействие с игровыми средами. TiG преобразует задачу принятия решений на основе обучения с подкреплением в задачу языкового моделирования. Система достигает конкурентоспособных результатов при значительно меньших требованиях к данным и вычислительным ресурсам по сравнению с традиционными методами обучения с подкреплением. Кроме того, TiG обеспечивает прозрачность, предоставляя пошаговые объяснения своих решений на естественном языке.'}, 'en': {'title': 'Empowering Language Models to Learn by Playing', 'desc': 'The Think in Games (TiG) framework allows large language models (LLMs) to learn how to perform tasks through interaction with game environments, enhancing their procedural knowledge. This approach addresses the gap between knowing facts (declarative knowledge) and knowing how to apply them (procedural knowledge), which LLMs often struggle with in interactive scenarios. By treating decision-making in reinforcement learning as a language modeling task, TiG enables LLMs to create and refine policies based on feedback from their environment. The results show that TiG not only requires less data and computation than traditional methods but also offers clear explanations for its actions, making it more transparent and interpretable.'}, 'zh': {'title': '通过游戏思维提升AI的学习能力', 'desc': 'Think in Games (TiG) 框架使大型语言模型能够通过互动游戏环境发展程序性知识。与传统的强化学习方法相比，TiG 在数据和计算需求上显著降低，同时保持了模型的推理和解释能力。该框架将基于强化学习的决策过程重新定义为语言建模任务，使得模型能够生成语言指导的策略，并通过环境反馈进行迭代优化。实验结果表明，TiG 成功弥补了声明性知识与程序性知识之间的差距，提升了复杂互动任务的透明度和可解释性。'}}}, {'id': 'https://huggingface.co/papers/2508.17677', 'title': 'TiKMiX: Take Data Influence into Dynamic Mixture for Language Model\n  Pre-training', 'url': 'https://huggingface.co/papers/2508.17677', 'abstract': "Dynamic adjustment of data mixture based on Group Influence metric improves language model performance by adapting to evolving learning preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t The data mixture used in the pre-training of a language model is a cornerstone of its final performance. However, a static mixing strategy is suboptimal, as the model's learning preferences for various data domains shift dynamically throughout training. Crucially, observing these evolving preferences in a computationally efficient manner remains a significant challenge. To address this, we propose TiKMiX, a method that dynamically adjusts the data mixture according to the model's evolving preferences. TiKMiX introduces Group Influence, an efficient metric for evaluating the impact of data domains on the model. This metric enables the formulation of the data mixing problem as a search for an optimal, influence-maximizing distribution. We solve this via two approaches: TiKMiX-D for direct optimization, and TiKMiX-M, which uses a regression model to predict a superior mixture. We trained models with different numbers of parameters, on up to 1 trillion tokens. TiKMiX-D exceeds the performance of state-of-the-art methods like REGMIX while using just 20% of the computational resources. TiKMiX-M leads to an average performance gain of 2% across 9 downstream benchmarks. Our experiments reveal that a model's data preferences evolve with training progress and scale, and we demonstrate that dynamically adjusting the data mixture based on Group Influence, a direct measure of these preferences, significantly improves performance by mitigating the underdigestion of data seen with static ratios.", 'score': 4, 'issue_id': 5639, 'pub_date': '2025-08-25', 'pub_date_card': {'ru': '25 августа', 'en': 'August 25', 'zh': '8月25日'}, 'hash': '8c118ab21cea1eb2', 'authors': ['Yifan Wang', 'Binbin Liu', 'Fengze Liu', 'Yuanfan Guo', 'Jiyao Deng', 'Xuecheng Wu', 'Weidong Zhou', 'Xiaohuan Zhou', 'Taifeng Wang'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2508.17677.jpg', 'data': {'categories': ['#data', '#optimization', '#training'], 'emoji': '🔄', 'ru': {'title': 'Динамическая оптимизация данных для эффективного обучения языковых моделей', 'desc': 'Статья представляет TiKMiX - метод динамической корректировки смеси данных для предобучения языковых моделей. Авторы вводят метрику Group Influence для эффективной оценки влияния доменов данных на модель. TiKMiX оптимизирует распределение данных, максимизируя эту метрику, что позволяет адаптироваться к меняющимся предпочтениям модели в процессе обучения. Эксперименты показывают, что TiKMiX превосходит современные методы, используя меньше вычислительных ресурсов и улучшая производительность на нисходящих задачах.'}, 'en': {'title': 'Dynamic Data Mixing for Enhanced Language Model Performance', 'desc': "This paper introduces TiKMiX, a novel method for dynamically adjusting the data mixture used in training language models. It leverages a metric called Group Influence to assess how different data domains impact the model's learning preferences, which change over time. By optimizing the data mixture based on these evolving preferences, TiKMiX significantly enhances model performance while being computationally efficient. The results show that TiKMiX outperforms existing methods and achieves better results with fewer resources, demonstrating the importance of adaptive data strategies in machine learning."}, 'zh': {'title': '动态调整数据混合，提升语言模型性能', 'desc': '本文提出了一种名为TiKMiX的方法，用于根据模型的学习偏好动态调整数据混合，以提高语言模型的性能。传统的静态数据混合策略无法适应模型在训练过程中不断变化的偏好。TiKMiX引入了Group Influence这一高效指标，用于评估不同数据领域对模型的影响，从而优化数据混合的分布。通过TiKMiX-D和TiKMiX-M两种方法，我们实现了在计算资源使用上更高效的模型训练，同时在多个基准测试中取得了显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2508.21290', 'title': 'Efficient Code Embeddings from Code Generation Models', 'url': 'https://huggingface.co/papers/2508.21290', 'abstract': 'Jina-code-embeddings uses an autoregressive backbone pre-trained on text and code to generate embeddings for code retrieval, question-answering, and similarity identification.  \t\t\t\t\tAI-generated summary \t\t\t\t jina-code-embeddings is a novel code embedding model suite designed to retrieve code from natural language queries, perform technical question-answering, and identify semantically similar code snippets across programming languages. It makes innovative use of an autoregressive backbone pre-trained on both text and code, generating embeddings via last-token pooling. We outline the training recipe and demonstrate state-of-the-art performance despite the relatively small size of the models, validating this approach to code embedding model construction.', 'score': 3, 'issue_id': 5640, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 августа', 'en': 'August 29', 'zh': '8月29日'}, 'hash': '484a770fa8f460fc', 'authors': ['Daria Kryvosheieva', 'Saba Sturua', 'Michael Günther', 'Scott Martens', 'Han Xiao'], 'affiliations': ['Jina AI GmbH', 'Massachusetts Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2508.21290.jpg', 'data': {'categories': ['#multilingual', '#transfer_learning', '#data', '#dataset', '#games', '#plp', '#small_models', '#training'], 'emoji': '🧠', 'ru': {'title': 'Умные эмбеддинги для эффективной работы с кодом', 'desc': 'Jina-code-embeddings - это новая модель встраивания кода, основанная на авторегрессионной архитектуре, предобученной на тексте и коде. Модель генерирует эмбеддинги для поиска кода, ответов на вопросы и определения семантически похожих фрагментов кода. Несмотря на относительно небольшой размер, модель демонстрирует передовые результаты в различных задачах, связанных с кодом. Авторы описывают методику обучения и валидируют эффективность данного подхода к созданию моделей встраивания кода.'}, 'en': {'title': 'Revolutionizing Code Retrieval with Smart Embeddings', 'desc': 'Jina-code-embeddings is a new model that creates embeddings for code, which helps in finding code based on natural language questions and identifying similar code snippets. It uses an autoregressive backbone that has been trained on both text and code, allowing it to understand and generate relevant embeddings effectively. The model employs a technique called last-token pooling to produce these embeddings, which enhances its performance. Despite being smaller in size compared to other models, it achieves state-of-the-art results in code retrieval and question-answering tasks.'}, 'zh': {'title': '创新代码嵌入模型，提升代码检索与问答能力', 'desc': 'Jina-code-embeddings 是一种新型的代码嵌入模型，旨在通过自然语言查询检索代码、进行技术问答以及识别不同编程语言中语义相似的代码片段。该模型创新性地使用了一个在文本和代码上预训练的自回归骨干网络，通过最后一个标记的池化生成嵌入。我们详细介绍了训练方法，并展示了尽管模型相对较小，但仍能实现最先进的性能。这验证了这种代码嵌入模型构建方法的有效性。'}}}, {'id': 'https://huggingface.co/papers/2508.21148', 'title': 'A Survey of Scientific Large Language Models: From Data Foundations to\n  Agent Frontiers', 'url': 'https://huggingface.co/papers/2508.21148', 'abstract': 'Sci-LLMs are evolving through a co-development with scientific data, addressing unique challenges like multimodal and domain-specific information, and are moving towards autonomous, closed-loop systems in scientific research.  \t\t\t\t\tAI-generated summary \t\t\t\t Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is represented, integrated, and applied in scientific research, yet their progress is shaped by the complex nature of scientific data. This survey presents a comprehensive, data-centric synthesis that reframes the development of Sci-LLMs as a co-evolution between models and their underlying data substrate. We formulate a unified taxonomy of scientific data and a hierarchical model of scientific knowledge, emphasizing the multimodal, cross-scale, and domain-specific challenges that differentiate scientific corpora from general natural language processing datasets. We systematically review recent Sci-LLMs, from general-purpose foundations to specialized models across diverse scientific disciplines, alongside an extensive analysis of over 270 pre-/post-training datasets, showing why Sci-LLMs pose distinct demands -- heterogeneous, multi-scale, uncertainty-laden corpora that require representations preserving domain invariance and enabling cross-modal reasoning. On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols. These data-centric analyses highlight persistent issues in scientific data development and discuss emerging solutions involving semi-automated annotation pipelines and expert validation. Finally, we outline a paradigm shift toward closed-loop systems where autonomous agents based on Sci-LLMs actively experiment, validate, and contribute to a living, evolving knowledge base. Collectively, this work provides a roadmap for building trustworthy, continually evolving artificial intelligence (AI) systems that function as a true partner in accelerating scientific discovery.', 'score': 3, 'issue_id': 5645, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': '3097c905f2f36541', 'authors': ['Ming Hu', 'Chenglong Ma', 'Wei Li', 'Wanghan Xu', 'Jiamin Wu', 'Jucheng Hu', 'Tianbin Li', 'Guohang Zhuang', 'Jiaqi Liu', 'Yingzhou Lu', 'Ying Chen', 'Chaoyang Zhang', 'Cheng Tan', 'Jie Ying', 'Guocheng Wu', 'Shujian Gao', 'Pengcheng Chen', 'Jiashi Lin', 'Haitao Wu', 'Lulu Chen', 'Fengxiang Wang', 'Yuanyuan Zhang', 'Xiangyu Zhao', 'Feilong Tang', 'Encheng Su', 'Junzhi Ning', 'Xinyao Liu', 'Ye Du', 'Changkai Ji', 'Cheng Tang', 'Huihui Xu', 'Ziyang Chen', 'Ziyan Huang', 'Jiyao Liu', 'Pengfei Jiang', 'Yizhou Wang', 'Chen Tang', 'Jianyu Wu', 'Yuchen Ren', 'Siyuan Yan', 'Zhonghua Wang', 'Zhongxing Xu', 'Shiyan Su', 'Shangquan Sun', 'Runkai Zhao', 'Zhisheng Zhang', 'Yu Liu', 'Fudi Wang', 'Yuanfeng Ji', 'Yanzhou Su', 'Hongming Shan', 'Chunmei Feng', 'Jiahao Xu', 'Jiangtao Yan', 'Wenhao Tang', 'Diping Song', 'Lihao Liu', 'Yanyan Huang', 'Lequan Yu', 'Bin Fu', 'Shujun Wang', 'Xiaomeng Li', 'Xiaowei Hu', 'Yun Gu', 'Ben Fei', 'Zhongying Deng', 'Benyou Wang', 'Yuewen Cao', 'Minjie Shen', 'Haodong Duan', 'Jie Xu', 'Yirong Chen', 'Fang Yan', 'Hongxia Hao', 'Jielan Li', 'Jiajun Du', 'Yanbo Wang', 'Imran Razzak', 'Chi Zhang', 'Lijun Wu', 'Conghui He', 'Zhaohui Lu', 'Jinhai Huang', 'Yihao Liu', 'Fenghua Ling', 'Yuqiang Li', 'Aoran Wang', 'Qihao Zheng', 'Nanqing Dong', 'Tianfan Fu', 'Dongzhan Zhou', 'Yan Lu', 'Wenlong Zhang', 'Jin Ye', 'Jianfei Cai', 'Wanli Ouyang', 'Yu Qiao', 'Zongyuan Ge', 'Shixiang Tang', 'Junjun He', 'Chunfeng Song', 'Lei Bai', 'Bowen Zhou'], 'affiliations': ['Beijing Institute of Heart, Lung and Blood Vessel Diseases', 'China Pharmaceutical University', 'Chinese Academy of Sciences', 'Fudan University', 'Fuzhou University', 'Monash University', 'Purdue University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'South China University', 'Stanford University', 'The Chinese University of Hong Kong', 'The Hong Kong Polytechnic University', 'The Hong Kong University of Science and Technology', 'The University of Hong Kong', 'UNC-Chapel Hill', 'University College Dublin', 'University College London', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2508.21148.jpg', 'data': {'categories': ['#benchmark', '#agents', '#survey', '#multimodal', '#data', '#dataset', '#science'], 'emoji': '🧬', 'ru': {'title': 'Sci-LLMs: эволюция искусственного интеллекта в научном познании', 'desc': 'Научные большие языковые модели (Sci-LLMs) развиваются в тесной связи с научными данными, решая уникальные задачи обработки мультимодальной и специализированной информации. В статье представлен комплексный обзор развития Sci-LLMs, включая анализ более 270 наборов данных для предобучения и дообучения моделей. Авторы рассматривают переход от статических тестов к оценке, ориентированной на процесс и открытия, с использованием продвинутых протоколов. Обсуждается парадигма автономных систем на основе Sci-LLMs, способных экспериментировать и вносить вклад в развивающуюся базу знаний.'}, 'en': {'title': 'Transforming Science with Autonomous Language Models', 'desc': 'This paper discusses the evolution of Scientific Large Language Models (Sci-LLMs) and their interaction with scientific data. It highlights the unique challenges posed by scientific datasets, which are often multimodal and domain-specific, requiring specialized approaches compared to general natural language processing. The authors propose a taxonomy of scientific data and review various Sci-LLMs, emphasizing the need for models that can handle heterogeneous and uncertain information. Ultimately, the paper advocates for the development of autonomous systems that can actively engage in scientific research, contributing to a dynamic knowledge base.'}, 'zh': {'title': '科学研究中的智能合作伙伴', 'desc': '科学大型语言模型（Sci-LLMs）正在通过与科学数据的共同发展而不断演变，解决多模态和特定领域信息等独特挑战。这项研究提出了一种以数据为中心的综合框架，将Sci-LLMs的发展视为模型与其基础数据之间的共同进化。我们建立了科学数据的统一分类法和科学知识的层次模型，强调了科学语料库与一般自然语言处理数据集之间的区别。最后，我们展望了向闭环系统的转变，强调基于Sci-LLMs的自主代理如何积极实验、验证并为不断发展的知识库做出贡献。'}}}, {'id': 'https://huggingface.co/papers/2508.21456', 'title': 'Morae: Proactively Pausing UI Agents for User Choices', 'url': 'https://huggingface.co/papers/2508.21456', 'abstract': 'Morae, a UI agent, enhances accessibility for BLV users by involving them in decision-making processes during task execution, using large multimodal models to interpret user queries and UI elements.  \t\t\t\t\tAI-generated summary \t\t\t\t User interface (UI) agents promise to make inaccessible or complex UIs easier to access for blind and low-vision (BLV) users. However, current UI agents typically perform tasks end-to-end without involving users in critical choices or making them aware of important contextual information, thus reducing user agency. For example, in our field study, a BLV participant asked to buy the cheapest available sparkling water, and the agent automatically chose one from several equally priced options, without mentioning alternative products with different flavors or better ratings. To address this problem, we introduce Morae, a UI agent that automatically identifies decision points during task execution and pauses so that users can make choices. Morae uses large multimodal models to interpret user queries alongside UI code and screenshots, and prompt users for clarification when there is a choice to be made. In a study over real-world web tasks with BLV participants, Morae helped users complete more tasks and select options that better matched their preferences, as compared to baseline agents, including OpenAI Operator. More broadly, this work exemplifies a mixed-initiative approach in which users benefit from the automation of UI agents while being able to express their preferences.', 'score': 2, 'issue_id': 5639, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 августа', 'en': 'August 29', 'zh': '8月29日'}, 'hash': '3d7bd580c525eaa6', 'authors': ['Yi-Hao Peng', 'Dingzeyu Li', 'Jeffrey P. Bigham', 'Amy Pavel'], 'affiliations': ['Adobe Research', 'Carnegie Mellon University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2508.21456.jpg', 'data': {'categories': ['#ethics', '#agi', '#multimodal', '#healthcare', '#agents'], 'emoji': '👁️', 'ru': {'title': 'Morae: ИИ-ассистент, расширяющий возможности пользователей с нарушениями зрения', 'desc': 'Статья представляет Morae - агента пользовательского интерфейса, который улучшает доступность для пользователей с нарушениями зрения. Morae использует большие мультимодальные модели для интерпретации запросов пользователей и элементов интерфейса. В отличие от существующих агентов, Morae вовлекает пользователей в процесс принятия решений во время выполнения задач. Исследование показало, что Morae помогает пользователям выполнять больше задач и выбирать варианты, которые лучше соответствуют их предпочтениям.'}, 'en': {'title': 'Empowering BLV Users with Interactive Decision-Making', 'desc': 'Morae is a user interface (UI) agent designed to improve accessibility for blind and low-vision (BLV) users by actively involving them in decision-making during task execution. Unlike traditional UI agents that operate autonomously, Morae identifies key decision points and pauses to allow users to make informed choices, enhancing their agency. It leverages large multimodal models to interpret user queries and UI elements, ensuring that users are aware of their options. In studies, Morae demonstrated improved task completion and user satisfaction compared to standard agents, showcasing a mixed-initiative approach that balances automation with user preference expression.'}, 'zh': {'title': 'Morae：让盲人和低视力用户参与决策的智能代理', 'desc': 'Morae是一种用户界面代理，旨在通过让盲人和低视力用户参与决策过程来提高可访问性。它利用大型多模态模型来理解用户查询和用户界面元素，并在任务执行中自动识别决策点，以便用户可以做出选择。与传统的全自动代理不同，Morae在关键时刻暂停，提示用户进行澄清，从而增强用户的自主性。研究表明，Morae帮助用户完成更多任务，并选择更符合他们偏好的选项。'}}}, {'id': 'https://huggingface.co/papers/2508.21376', 'title': 'AHELM: A Holistic Evaluation of Audio-Language Models', 'url': 'https://huggingface.co/papers/2508.21376', 'abstract': 'AHELM is a comprehensive benchmark for audio-language models that evaluates multiple aspects including fairness, safety, and reasoning across various datasets and models.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluations of audio-language models (ALMs) -- multimodal models that take interleaved audio and text as input and output text -- are hindered by the lack of standardized benchmarks; most benchmarks measure only one or two capabilities and omit evaluative aspects such as fairness or safety. Furthermore, comparison across models is difficult as separate evaluations test a limited number of models and use different prompting methods and inference parameters. To address these shortfalls, we introduce AHELM, a benchmark that aggregates various datasets -- including 2 new synthetic audio-text datasets called PARADE, which evaluates the ALMs on avoiding stereotypes, and CoRe-Bench, which measures reasoning over conversational audio through inferential multi-turn question answering -- to holistically measure the performance of ALMs across 10 aspects we have identified as important to the development and usage of ALMs: audio perception, knowledge, reasoning, emotion detection, bias, fairness, multilinguality, robustness, toxicity, and safety. We also standardize the prompts, inference parameters, and evaluation metrics to ensure equitable comparisons across models. We test 14 open-weight and closed-API ALMs from 3 developers and 3 additional simple baseline systems each consisting of an automatic speech recognizer and a language model. Our results show that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits group unfairness (p=0.01) on ASR tasks whereas most of the other models do not. We also find that the baseline systems perform reasonably well on AHELM, with one ranking 5th overall despite having only speech-to-text capabilities. For transparency, all raw prompts, model generations, and outputs are available on our website at https://crfm.stanford.edu/helm/audio/v1.0.0. AHELM is intended to be a living benchmark and new datasets and models will be added over time.', 'score': 1, 'issue_id': 5639, 'pub_date': '2025-08-29', 'pub_date_card': {'ru': '29 августа', 'en': 'August 29', 'zh': '8月29日'}, 'hash': 'fccdd3f91aa4aebd', 'authors': ['Tony Lee', 'Haoqin Tu', 'Chi Heem Wong', 'Zijun Wang', 'Siwei Yang', 'Yifan Mai', 'Yuyin Zhou', 'Cihang Xie', 'Percy Liang'], 'affiliations': ['Hitachi America, Ltd.', 'Stanford University', 'University of California, Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2508.21376.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#ethics', '#reasoning', '#multimodal', '#audio', '#dataset'], 'emoji': '🎧', 'ru': {'title': 'AHELM: Всесторонняя оценка аудио-языковых моделей', 'desc': 'AHELM - это комплексный бенчмарк для оценки аудио-языковых моделей (ALM). Он измеряет 10 аспектов, включая восприятие аудио, рассуждение, обнаружение эмоций и безопасность, используя различные наборы данных. Бенчмарк стандартизирует промпты, параметры вывода и метрики оценки для справедливого сравнения моделей. Результаты показывают, что Gemini 2.5 Pro лидирует в 5 из 10 аспектов, но проявляет групповую несправедливость в задачах ASR.'}, 'en': {'title': 'AHELM: A Holistic Benchmark for Evaluating Audio-Language Models', 'desc': 'AHELM is a new benchmark designed to evaluate audio-language models (ALMs) on multiple important aspects such as fairness, safety, and reasoning. It combines various datasets, including two new ones, PARADE and CoRe-Bench, to provide a comprehensive assessment of ALMs across ten critical dimensions. By standardizing evaluation methods and metrics, AHELM allows for fair comparisons between different models, addressing the limitations of previous benchmarks. The initial testing of 14 ALMs reveals insights into their performance, highlighting both strengths and weaknesses in areas like bias and group fairness.'}, 'zh': {'title': 'AHELM：音频语言模型的全面评估基准', 'desc': 'AHELM是一个全面的基准测试，用于评估音频语言模型（ALMs），涵盖公平性、安全性和推理等多个方面。该基准整合了多种数据集，包括两个新的合成音频-文本数据集PARADE和CoRe-Bench，以全面测量ALMs在音频感知、知识、推理等10个重要方面的表现。通过标准化提示、推理参数和评估指标，AHELM确保了模型之间的公平比较。我们的测试结果显示，尽管Gemini 2.5 Pro在10个方面中有5个排名第一，但在ASR任务上表现出群体不公平性。'}}}, {'id': 'https://huggingface.co/papers/2508.20085', 'title': 'HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data\n  for Mobile Dexterous Manipulation', 'url': 'https://huggingface.co/papers/2508.20085', 'abstract': 'HERMES is a human-to-robot learning framework that translates human hand motions into robotic behaviors, using reinforcement learning and sim2real transfer for versatile manipulation in diverse environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Leveraging human motion data to impart robots with versatile manipulation skills has emerged as a promising paradigm in robotic manipulation. Nevertheless, translating multi-source human hand motions into feasible robot behaviors remains challenging, particularly for robots equipped with multi-fingered dexterous hands characterized by complex, high-dimensional action spaces. Moreover, existing approaches often struggle to produce policies capable of adapting to diverse environmental conditions. In this paper, we introduce HERMES, a human-to-robot learning framework for mobile bimanual dexterous manipulation. First, HERMES formulates a unified reinforcement learning approach capable of seamlessly transforming heterogeneous human hand motions from multiple sources into physically plausible robotic behaviors. Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth image-based sim2real transfer method for improved generalization to real-world scenarios. Furthermore, to enable autonomous operation in varied and unstructured environments, we augment the navigation foundation model with a closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise alignment of visual goals and effectively bridging autonomous navigation and dexterous manipulation. Extensive experimental results demonstrate that HERMES consistently exhibits generalizable behaviors across diverse, in-the-wild scenarios, successfully performing numerous complex mobile bimanual dexterous manipulation tasks. Project Page:https://gemcollector.github.io/HERMES/.', 'score': 1, 'issue_id': 5640, 'pub_date': '2025-08-27', 'pub_date_card': {'ru': '27 августа', 'en': 'August 27', 'zh': '8月27日'}, 'hash': 'ad1271c7a1097c84', 'authors': ['Zhecheng Yuan', 'Tianming Wei', 'Langzhe Gu', 'Pu Hua', 'Tianhai Liang', 'Yuanpei Chen', 'Huazhe Xu'], 'affiliations': ['Peking University', 'Shanghai Qi Zhi Institute', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.20085.jpg', 'data': {'categories': ['#rl', '#agents', '#optimization', '#transfer_learning', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'От движений человека к умелым рукам робота', 'desc': 'HERMES - это система обучения роботов на основе данных о движениях человеческих рук. Она использует обучение с подкреплением и перенос из симуляции в реальность для создания универсальных манипуляционных навыков. HERMES способна адаптировать движения к различным условиям окружающей среды. Система включает в себя навигационную модель с механизмом локализации для автономной работы в неструктурированных средах.'}, 'en': {'title': 'Bridging Human Motion and Robotic Dexterity with HERMES', 'desc': 'HERMES is a framework that helps robots learn to mimic human hand movements for better manipulation tasks. It uses reinforcement learning to convert various human motions into actions that robots can perform, even with complex hand structures. The framework also addresses the challenge of transferring learned behaviors from simulated environments to real-world applications. By incorporating advanced localization techniques, HERMES enables robots to operate effectively in diverse and unpredictable settings.'}, 'zh': {'title': 'HERMES：人机协作的灵巧操作新框架', 'desc': 'HERMES是一个人机学习框架，旨在将人类手部动作转化为机器人行为。该框架利用强化学习和仿真到现实的转移技术，帮助机器人在多样化环境中进行灵活的操作。HERMES能够将来自多个来源的人类手部动作统一转化为可行的机器人行为，并通过深度图像实现更好的现实适应性。实验结果表明，HERMES在各种复杂的移动双手灵巧操作任务中表现出色，具有良好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2508.14197', 'title': 'CLIPSym: Delving into Symmetry Detection with CLIP', 'url': 'https://huggingface.co/papers/2508.14197', 'abstract': "CLIPSym, a vision-language model using CLIP, enhances symmetry detection through a rotation-equivariant decoder and semantic-aware prompting, outperforming existing methods on standard datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Symmetry is one of the most fundamental geometric cues in computer vision, and detecting it has been an ongoing challenge. With the recent advances in vision-language models,~i.e., CLIP, we investigate whether a pre-trained CLIP model can aid symmetry detection by leveraging the additional symmetry cues found in the natural image descriptions. We propose CLIPSym, which leverages CLIP's image and language encoders and a rotation-equivariant decoder based on a hybrid of Transformer and G-Convolution to detect rotation and reflection symmetries. To fully utilize CLIP's language encoder, we have developed a novel prompting technique called Semantic-Aware Prompt Grouping (SAPG), which aggregates a diverse set of frequent object-based prompts to better integrate the semantic cues for symmetry detection. Empirically, we show that CLIPSym outperforms the current state-of-the-art on three standard symmetry detection datasets (DENDI, SDRW, and LDRS). Finally, we conduct detailed ablations verifying the benefits of CLIP's pre-training, the proposed equivariant decoder, and the SAPG technique. The code is available at https://github.com/timyoung2333/CLIPSym.", 'score': 0, 'issue_id': 5642, 'pub_date': '2025-08-19', 'pub_date_card': {'ru': '19 августа', 'en': 'August 19', 'zh': '8月19日'}, 'hash': 'fe61d1db34c28a8d', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#dataset', '#architecture', '#optimization', '#multimodal', '#cv', '#games'], 'emoji': '🔍', 'ru': {'title': 'CLIPSym: Улучшенное обнаружение симметрии с помощью языковой модели', 'desc': 'CLIPSym - это новая модель для обнаружения симметрии, использующая предобученную модель CLIP. Она сочетает энкодеры изображений и текста CLIP с ротационно-эквивариантным декодером на основе трансформера и G-свертки. Модель использует технику семантически-осведомленной группировки промптов для лучшего учета семантических подсказок при обнаружении симметрии. CLIPSym превосходит современные методы на стандартных наборах данных для обнаружения симметрии.'}, 'en': {'title': 'Enhancing Symmetry Detection with CLIPSym', 'desc': 'CLIPSym is a vision-language model that improves the detection of symmetry in images by using a rotation-equivariant decoder and a new prompting technique. It builds on the capabilities of the CLIP model, which combines image and text understanding, to enhance symmetry detection by incorporating semantic information from image descriptions. The model employs a hybrid architecture that integrates Transformer and G-Convolution to effectively recognize both rotation and reflection symmetries. Experimental results demonstrate that CLIPSym surpasses existing methods on multiple standard datasets, confirming the effectiveness of its innovative approaches.'}, 'zh': {'title': 'CLIPSym：提升对称性检测的新方法', 'desc': 'CLIPSym是一种基于CLIP的视觉-语言模型，旨在提高对称性检测的能力。它采用了一种旋转等变解码器和语义感知提示技术，能够更好地利用自然图像描述中的对称性线索。通过结合Transformer和G-卷积的混合结构，CLIPSym能够有效检测旋转和反射对称性。实验结果表明，CLIPSym在三个标准对称性检测数据集上超越了现有的最先进方法。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (5)', '#agi (3)', '#alignment', '#architecture (2)', '#audio (1)', '#benchmark (6)', '#cv (2)', '#data (5)', '#dataset (10)', '#diffusion', '#ethics (3)', '#games (3)', '#graphs', '#hallucinations', '#healthcare (1)', '#inference', '#interpretability (1)', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (8)', '#open_source (5)', '#optimization (6)', '#plp (1)', '#rag', '#reasoning (5)', '#rl (3)', '#rlhf (1)', '#robotics (1)', '#science (1)', '#security (1)', '#small_models (1)', '#story_generation', '#survey (1)', '#synthetic (1)', '#training (6)', '#transfer_learning (3)', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-09-01 11:10',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-09-01 11:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-09-01 11:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    