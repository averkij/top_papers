
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 18 papers. April 10.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">10 апреля</span> | <span id="title-articles-count">18 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-04-09.html">⬅️ <span id="prev-date">09.04</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-04-11.html">➡️ <span id="next-date">11.04</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-04.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'};
        let feedDateNext = {'ru': '11.04', 'en': '04/11', 'zh': '4月11日'};
        let feedDatePrev = {'ru': '09.04', 'en': '04/09', 'zh': '4月9日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2504.05741', 'title': 'DDT: Decoupled Diffusion Transformer', 'url': 'https://huggingface.co/papers/2504.05741', 'abstract': 'Diffusion transformers have demonstrated remarkable generation quality, albeit requiring longer training iterations and numerous inference steps. In each denoising step, diffusion transformers encode the noisy inputs to extract the lower-frequency semantic component and then decode the higher frequency with identical modules. This scheme creates an inherent optimization dilemma: encoding low-frequency semantics necessitates reducing high-frequency components, creating tension between semantic encoding and high-frequency decoding. To resolve this challenge, we propose a new \\color{ddtD}ecoupled \\color{ddtD}iffusion \\color{ddtT}ransformer~(\\color{ddtDDT}), with a decoupled design of a dedicated condition encoder for semantic extraction alongside a specialized velocity decoder. Our experiments reveal that a more substantial encoder yields performance improvements as model size increases. For ImageNet 256times256, Our DDT-XL/2 achieves a new state-of-the-art performance of {1.31 FID}~(nearly 4times faster training convergence compared to previous diffusion transformers). For ImageNet 512times512, Our DDT-XL/2 achieves a new state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our decoupled architecture enhances inference speed by enabling the sharing self-condition between adjacent denoising steps. To minimize performance degradation, we propose a novel statistical dynamic programming approach to identify optimal sharing strategies.', 'score': 42, 'issue_id': 3159, 'pub_date': '2025-04-08', 'pub_date_card': {'ru': '8 апреля', 'en': 'April 8', 'zh': '4月8日'}, 'hash': '2f4cd9583b2418f3', 'authors': ['Shuai Wang', 'Zhi Tian', 'Weilin Huang', 'Limin Wang'], 'affiliations': ['ByteDance Seed Vision', 'Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2504.05741.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#cv', '#diffusion'], 'emoji': '🧠', 'ru': {'title': 'DDT: Разделяй и властвуй в мире диффузионных трансформеров', 'desc': 'Статья представляет новый подход к архитектуре диффузионных трансформеров, называемый Decoupled Diffusion Transformer (DDT). DDT разделяет процессы кодирования семантики и декодирования высокочастотных компонентов, что позволяет разрешить проблему оптимизации, присущую стандартным диффузионным трансформерам. Эксперименты показывают, что DDT достигает нового уровня производительности на наборе данных ImageNet, значительно ускоряя обучение и улучшая качество генерации изображений. Кроме того, предложенная архитектура позволяет оптимизировать процесс вывода путем разделения условий между соседними шагами денойзинга.'}, 'en': {'title': 'Decoupling for Faster and Better Image Generation', 'desc': 'This paper introduces a new model called the Decoupled Diffusion Transformer (DDT), which addresses the challenges faced by traditional diffusion transformers in generating high-quality outputs. The DDT separates the tasks of semantic encoding and high-frequency decoding, allowing for better optimization and improved performance. Experiments show that as the model size increases, a more powerful encoder leads to significant enhancements in generation quality, achieving state-of-the-art results on ImageNet datasets. Additionally, the decoupled design improves inference speed by optimizing the sharing of self-conditions between denoising steps, while a novel dynamic programming method helps maintain performance.'}, 'zh': {'title': '解耦扩散变换器：提升生成质量与推理速度的创新方案', 'desc': '扩散变换器在生成质量上表现出色，但训练迭代时间较长且推理步骤较多。每个去噪步骤中，扩散变换器对噪声输入进行编码，以提取低频语义成分，然后用相同的模块解码高频成分。这种方案导致了一个固有的优化困境：编码低频语义需要减少高频成分，从而在语义编码和高频解码之间产生紧张关系。为了解决这个问题，我们提出了一种新的解耦扩散变换器（DDT），它采用专门的条件编码器进行语义提取，并配备专门的速度解码器。'}}}, {'id': 'https://huggingface.co/papers/2504.07096', 'title': 'OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training\n  Tokens', 'url': 'https://huggingface.co/papers/2504.07096', 'abstract': 'We present OLMoTrace, the first system that traces the outputs of language models back to their full, multi-trillion-token training data in real time. OLMoTrace finds and shows verbatim matches between segments of language model output and documents in the training text corpora. Powered by an extended version of infini-gram (Liu et al., 2024), our system returns tracing results within a few seconds. OLMoTrace can help users understand the behavior of language models through the lens of their training data. We showcase how it can be used to explore fact checking, hallucination, and the creativity of language models. OLMoTrace is publicly available and fully open-source.', 'score': 25, 'issue_id': 3160, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': '68eafe783b5816f6', 'authors': ['Jiacheng Liu', 'Taylor Blanton', 'Yanai Elazar', 'Sewon Min', 'YenSung Chen', 'Arnavi Chheda-Kothary', 'Huy Tran', 'Byron Bischoff', 'Eric Marsh', 'Michael Schmitz', 'Cassidy Trier', 'Aaron Sarnat', 'Jenna James', 'Jon Borchardt', 'Bailey Kuehl', 'Evie Cheng', 'Karen Farley', 'Sruthi Sreeram', 'Taira Anderson', 'David Albright', 'Carissa Schoenick', 'Luca Soldaini', 'Dirk Groeneveld', 'Rock Yuren Pang', 'Pang Wei Koh', 'Noah A. Smith', 'Sophie Lebrecht', 'Yejin Choi', 'Hannaneh Hajishirzi', 'Ali Farhadi', 'Jesse Dodge'], 'affiliations': ['Allen Institute for AI', 'Stanford University', 'UC Berkeley', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2504.07096.jpg', 'data': {'categories': ['#data', '#hallucinations', '#inference', '#open_source', '#interpretability', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Заглянуть в память языковой модели', 'desc': 'OLMoTrace - это первая система, которая в режиме реального времени отслеживает связь между выводами языковых моделей и их многотриллионным обучающим датасетом. Система находит дословные совпадения между сегментами вывода модели и документами в обучающем корпусе текстов. OLMoTrace использует расширенную версию алгоритма infini-gram и выдает результаты трассировки за несколько секунд. Эта система помогает пользователям понять поведение языковых моделей через призму их обучающих данных, что полезно для проверки фактов, выявления галлюцинаций и исследования креативности моделей.'}, 'en': {'title': 'Trace the Truth: Unveiling Language Model Outputs with OLMoTrace', 'desc': 'OLMoTrace is a groundbreaking system that allows users to trace the outputs of language models back to their extensive training data in real time. It identifies and displays exact matches between the generated text and the original documents from the training corpus. Utilizing an enhanced version of the infini-gram technique, OLMoTrace provides results in just a few seconds. This tool aids in understanding language model behavior, particularly in areas like fact-checking, hallucination, and creative output.'}, 'zh': {'title': '实时追踪语言模型输出的革命性工具', 'desc': 'OLMoTrace是第一个能够实时追踪语言模型输出与其训练数据之间关系的系统。它可以找到语言模型输出片段与训练文本库中文档的逐字匹配。该系统基于扩展版的infini-gram技术，能够在几秒钟内返回追踪结果。OLMoTrace帮助用户通过训练数据理解语言模型的行为，适用于事实检查、幻觉和语言模型的创造力探索。'}}}, {'id': 'https://huggingface.co/papers/2504.07046', 'title': 'A Unified Agentic Framework for Evaluating Conditional Image Generation', 'url': 'https://huggingface.co/papers/2504.07046', 'abstract': "Conditional image generation has gained significant attention for its ability to personalize content. However, the field faces challenges in developing task-agnostic, reliable, and explainable evaluation metrics. This paper introduces CIGEval, a unified agentic framework for comprehensive evaluation of conditional image generation tasks. CIGEval utilizes large multimodal models (LMMs) as its core, integrating a multi-functional toolbox and establishing a fine-grained evaluation framework. Additionally, we synthesize evaluation trajectories for fine-tuning, empowering smaller LMMs to autonomously select appropriate tools and conduct nuanced analyses based on tool outputs. Experiments across seven prominent conditional image generation tasks demonstrate that CIGEval (GPT-4o version) achieves a high correlation of 0.4625 with human assessments, closely matching the inter-annotator correlation of 0.47. Moreover, when implemented with 7B open-source LMMs using only 2.3K training trajectories, CIGEval surpasses the previous GPT-4o-based state-of-the-art method. Case studies on GPT-4o image generation highlight CIGEval's capability in identifying subtle issues related to subject consistency and adherence to control guidance, indicating its great potential for automating evaluation of image generation tasks with human-level reliability.", 'score': 22, 'issue_id': 3167, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': 'ec2d69230afcb841', 'authors': ['Jifang Wang', 'Xue Yang', 'Longyue Wang', 'Zhenran Xu', 'Yiyu Wang', 'Yaowei Wang', 'Weihua Luo', 'Kaifu Zhang', 'Baotian Hu', 'Min Zhang'], 'affiliations': ['Harbin Institute of Technology (Shenzhen), Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.07046.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#benchmark', '#open_source', '#optimization'], 'emoji': '🖼️', 'ru': {'title': 'CIGEval: Революция в оценке генерации изображений', 'desc': 'Статья представляет CIGEval - унифицированную систему для оценки задач условной генерации изображений. CIGEval использует большие мультимодальные модели (LMM) и набор инструментов для детального анализа. Эксперименты показывают, что CIGEval достигает высокой корреляции с оценками людей. Система способна выявлять тонкие проблемы в сгенерированных изображениях, демонстрируя потенциал для автоматизации оценки с надежностью на уровне человека.'}, 'en': {'title': 'CIGEval: Revolutionizing Evaluation in Conditional Image Generation', 'desc': 'This paper presents CIGEval, a new framework designed to evaluate conditional image generation tasks effectively. It addresses the need for reliable and explainable metrics by using large multimodal models (LMMs) and a multi-functional toolbox. CIGEval not only assesses image generation but also fine-tunes smaller LMMs to select the best evaluation tools autonomously. The results show that CIGEval correlates well with human assessments and outperforms previous methods, demonstrating its potential for automating image generation evaluations.'}, 'zh': {'title': 'CIGEval：条件图像生成的全面评估新框架', 'desc': '条件图像生成因其个性化内容的能力而受到广泛关注。然而，该领域在开发任务无关、可靠且可解释的评估指标方面面临挑战。本文介绍了CIGEval，这是一个统一的代理框架，用于全面评估条件图像生成任务。CIGEval利用大型多模态模型（LMMs）作为核心，整合多功能工具箱并建立细粒度评估框架，展示了其在图像生成任务评估中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2504.06514', 'title': 'Missing Premise exacerbates Overthinking: Are Reasoning Models losing\n  Critical Thinking Skill?', 'url': 'https://huggingface.co/papers/2504.06514', 'abstract': "We find that the response length of reasoning LLMs, whether trained by reinforcement learning or supervised learning, drastically increases for ill-posed questions with missing premises (MiP), ending up with redundant and ineffective thinking. This newly introduced scenario exacerbates the general overthinking issue to a large extent, which we name as the MiP-Overthinking. Such failures are against the ``test-time scaling law'' but have been widely observed on multiple datasets we curated with MiP, indicating the harm of cheap overthinking and a lack of critical thinking. Surprisingly, LLMs not specifically trained for reasoning exhibit much better performance on the MiP scenario, producing much shorter responses that quickly identify ill-posed queries. This implies a critical flaw of the current training recipe for reasoning LLMs, which does not encourage efficient thinking adequately, leading to the abuse of thinking patterns. To further investigate the reasons behind such failures, we conduct fine-grained analyses of the reasoning length, overthinking patterns, and location of critical thinking on different types of LLMs. Moreover, our extended ablation study reveals that the overthinking is contagious through the distillation of reasoning models' responses. These results improve the understanding of overthinking and shed novel insights into mitigating the problem.", 'score': 19, 'issue_id': 3160, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': '6ca6e88a5650dba9', 'authors': ['Chenrui Fan', 'Ming Li', 'Lichao Sun', 'Tianyi Zhou'], 'affiliations': ['Lehigh University', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2504.06514.jpg', 'data': {'categories': ['#training', '#rl', '#hallucinations', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Осторожно: языковые модели склонны к избыточным рассуждениям', 'desc': 'В статье исследуется проблема чрезмерного мышления (overthinking) у языковых моделей при ответе на некорректно поставленные вопросы с отсутствующими предпосылками. Авторы обнаружили, что модели, обученные рассуждать, генерируют избыточно длинные и неэффективные ответы в таких ситуациях. Интересно, что модели без специального обучения рассуждениям показывают лучшие результаты, быстро идентифицируя некорректность вопроса. Исследование выявляет недостатки в текущих методах обучения моделей рассуждениям и предлагает новые подходы к решению проблемы чрезмерного мышления.'}, 'en': {'title': 'Tackling MiP-Overthinking in Reasoning LLMs', 'desc': 'This paper investigates how reasoning large language models (LLMs) respond to poorly defined questions that lack necessary information, a situation termed as missing premises (MiP). It finds that these models tend to generate longer, redundant responses, which is a manifestation of what the authors call MiP-Overthinking. Interestingly, LLMs not specifically trained for reasoning perform better in these scenarios, producing shorter and more effective answers. The study highlights a significant flaw in the training methods for reasoning LLMs, suggesting that they do not promote efficient thinking, and proposes further analysis to understand and mitigate the overthinking issue.'}, 'zh': {'title': '揭示推理模型的过度思考问题', 'desc': '我们发现，推理大型语言模型（LLMs）在面对缺乏前提的模糊问题时，响应长度显著增加，导致冗余和无效的思考。这种新引入的场景加剧了普遍的过度思考问题，我们称之为MiP-过度思考。研究表明，当前的推理LLMs训练方法未能有效鼓励高效思考，导致思维模式的滥用。相反，未专门训练用于推理的LLMs在处理模糊问题时表现更好，能够快速识别问题并给出更简短的回答。'}}}, {'id': 'https://huggingface.co/papers/2504.07083', 'title': 'GenDoP: Auto-regressive Camera Trajectory Generation as a Director of\n  Photography', 'url': 'https://huggingface.co/papers/2504.07083', 'abstract': 'Camera trajectory design plays a crucial role in video production, serving as a fundamental tool for conveying directorial intent and enhancing visual storytelling. In cinematography, Directors of Photography meticulously craft camera movements to achieve expressive and intentional framing. However, existing methods for camera trajectory generation remain limited: Traditional approaches rely on geometric optimization or handcrafted procedural systems, while recent learning-based methods often inherit structural biases or lack textual alignment, constraining creative synthesis. In this work, we introduce an auto-regressive model inspired by the expertise of Directors of Photography to generate artistic and expressive camera trajectories. We first introduce DataDoP, a large-scale multi-modal dataset containing 29K real-world shots with free-moving camera trajectories, depth maps, and detailed captions in specific movements, interaction with the scene, and directorial intent. Thanks to the comprehensive and diverse database, we further train an auto-regressive, decoder-only Transformer for high-quality, context-aware camera movement generation based on text guidance and RGBD inputs, named GenDoP. Extensive experiments demonstrate that compared to existing methods, GenDoP offers better controllability, finer-grained trajectory adjustments, and higher motion stability. We believe our approach establishes a new standard for learning-based cinematography, paving the way for future advancements in camera control and filmmaking. Our project website: https://kszpxxzmc.github.io/GenDoP/.', 'score': 17, 'issue_id': 3160, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': '67e58f651d865bad', 'authors': ['Mengchen Zhang', 'Tong Wu', 'Jing Tan', 'Ziwei Liu', 'Gordon Wetzstein', 'Dahua Lin'], 'affiliations': ['Nanyang Technological University', 'Shanghai Artificial Intelligence Laboratory', 'Stanford University', 'The Chinese University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.07083.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#cv'], 'emoji': '🎥', 'ru': {'title': 'ИИ-оператор: новый стандарт компьютерной кинематографии', 'desc': 'Эта статья представляет новый подход к генерации траекторий движения камеры в кинопроизводстве с использованием методов машинного обучения. Авторы создали большой мультимодальный датасет DataDoP, содержащий 29 тысяч реальных кадров с траекториями камеры, картами глубины и подробными описаниями. На основе этих данных была обучена авторегрессионная модель GenDoP на базе архитектуры Transformer для генерации художественных и выразительных движений камеры. Эксперименты показали, что GenDoP превосходит существующие методы по управляемости, точности настройки траекторий и стабильности движения.'}, 'en': {'title': 'Revolutionizing Camera Movement with GenDoP', 'desc': 'This paper presents a novel approach to generating camera trajectories for video production using an auto-regressive model called GenDoP. The model is trained on a large dataset, DataDoP, which includes diverse camera movements, depth information, and detailed captions that reflect directorial intent. Unlike traditional methods that rely on geometric optimization, GenDoP leverages text guidance and RGBD inputs to create more expressive and context-aware camera movements. The results show that GenDoP outperforms existing techniques in terms of controllability, trajectory adjustments, and motion stability, setting a new benchmark for learning-based cinematography.'}, 'zh': {'title': '创新相机轨迹生成，提升视觉叙事效果', 'desc': '本论文介绍了一种新的相机轨迹生成方法，旨在提升视频制作中的视觉叙事效果。我们提出了一个名为GenDoP的自回归模型，利用大型多模态数据集DataDoP，包含29000个真实镜头及其相机轨迹、深度图和详细描述。通过结合文本指导和RGBD输入，GenDoP能够生成高质量、上下文感知的相机运动。实验结果表明，GenDoP在可控性、轨迹调整精细度和运动稳定性方面优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2504.04842', 'title': 'FantasyTalking: Realistic Talking Portrait Generation via Coherent\n  Motion Synthesis', 'url': 'https://huggingface.co/papers/2504.04842', 'abstract': 'Creating a realistic animatable avatar from a single static portrait remains challenging. Existing approaches often struggle to capture subtle facial expressions, the associated global body movements, and the dynamic background. To address these limitations, we propose a novel framework that leverages a pretrained video diffusion transformer model to generate high-fidelity, coherent talking portraits with controllable motion dynamics. At the core of our work is a dual-stage audio-visual alignment strategy. In the first stage, we employ a clip-level training scheme to establish coherent global motion by aligning audio-driven dynamics across the entire scene, including the reference portrait, contextual objects, and background. In the second stage, we refine lip movements at the frame level using a lip-tracing mask, ensuring precise synchronization with audio signals. To preserve identity without compromising motion flexibility, we replace the commonly used reference network with a facial-focused cross-attention module that effectively maintains facial consistency throughout the video. Furthermore, we integrate a motion intensity modulation module that explicitly controls expression and body motion intensity, enabling controllable manipulation of portrait movements beyond mere lip motion. Extensive experimental results show that our proposed approach achieves higher quality with better realism, coherence, motion intensity, and identity preservation. Ours project page: https://fantasy-amap.github.io/fantasy-talking/.', 'score': 12, 'issue_id': 3160, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': '5b592626aeda4ec8', 'authors': ['Mengchao Wang', 'Qiang Wang', 'Fan Jiang', 'Yaqi Fan', 'Yunpeng Zhang', 'Yonggang Qi', 'Kun Zhao', 'Mu Xu'], 'affiliations': ['AMAP, Alibaba Group', 'Beijing University of Posts and Telecommunications'], 'pdf_title_img': 'assets/pdf/title_img/2504.04842.jpg', 'data': {'categories': ['#multimodal', '#video', '#diffusion'], 'emoji': '🗣️', 'ru': {'title': 'Оживление статичных портретов: новый уровень реализма и контроля', 'desc': 'Данная статья представляет новый подход к созданию анимированных аватаров из одного статичного портрета с помощью предобученной видео-диффузионной трансформерной модели. Авторы предлагают двухэтапную стратегию аудиовизуального выравнивания для генерации реалистичных говорящих портретов с контролируемой динамикой движений. Метод включает модуль кросс-внимания для сохранения идентичности лица и модуль модуляции интенсивности движения для управления выразительностью. Экспериментальные результаты показывают улучшение качества, реалистичности и сохранения идентичности по сравнению с существующими подходами.'}, 'en': {'title': 'Realistic Talking Avatars: Synchronizing Motion and Expression', 'desc': "This paper presents a new method for creating realistic animated avatars from a single portrait. It uses a pretrained video diffusion transformer to generate talking portraits that can move in a lifelike way. The approach includes a dual-stage audio-visual alignment strategy to ensure that the avatar's lip movements and body motions are synchronized with audio input. Additionally, it introduces a facial-focused cross-attention module to maintain facial identity and a motion intensity modulation module for controlling the expressiveness of the avatar."}, 'zh': {'title': '生成可控动画头像的新方法', 'desc': '本论文提出了一种新颖的框架，用于从单一静态肖像生成逼真的可动画头像。我们采用了预训练的视频扩散变换器模型，能够生成高保真、连贯的对话肖像，并具备可控的运动动态。核心是双阶段的音视频对齐策略，第一阶段通过音频驱动的动态对齐全场景，第二阶段则使用唇部追踪掩模精细调整唇部动作。实验结果表明，我们的方法在真实感、一致性、运动强度和身份保留方面均优于现有技术。'}}}, {'id': 'https://huggingface.co/papers/2504.07086', 'title': 'A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths\n  to Reproducibility', 'url': 'https://huggingface.co/papers/2504.07086', 'abstract': 'Reasoning has emerged as the next major frontier for language models (LMs), with rapid advances from both academic and industrial labs. However, this progress often outpaces methodological rigor, with many evaluations relying on benchmarking practices that lack transparency, robustness, or statistical grounding. In this work, we conduct a comprehensive empirical study and find that current mathematical reasoning benchmarks are highly sensitive to subtle implementation choices - including decoding parameters, random seeds, prompt formatting, and even hardware and software-framework configurations. Performance gains reported in recent studies frequently hinge on unclear comparisons or unreported sources of variance. To address these issues, we propose a standardized evaluation framework with clearly defined best practices and reporting standards. Using this framework, we reassess recent methods and find that reinforcement learning (RL) approaches yield only modest improvements - far below prior claims - and are prone to overfitting, especially on small-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT) methods show consistently stronger generalization. To foster reproducibility, we release all code, prompts, and model outputs, for reasoning benchmarks, establishing more rigorous foundations for future work.', 'score': 10, 'issue_id': 3163, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': 'b06c700fb29c005d', 'authors': ['Andreas Hochlehnert', 'Hardik Bhatnagar', 'Vishaal Udandarao', 'Samuel Albanie', 'Ameya Prabhu', 'Matthias Bethge'], 'affiliations': ['Tübingen AI Center, University of Tübingen', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2504.07086.jpg', 'data': {'categories': ['#open_source', '#survey', '#rl', '#reasoning', '#benchmark', '#training'], 'emoji': '🧮', 'ru': {'title': 'Стандартизация оценки математических рассуждений языковых моделей', 'desc': 'Статья посвящена проблемам оценки способностей языковых моделей к математическим рассуждениям. Авторы обнаружили, что существующие бенчмарки очень чувствительны к тонким деталям реализации, таким как параметры декодирования и форматирование промптов. Предложена стандартизированная система оценки с четко определенными лучшими практиками. Переоценка недавних методов показала, что обучение с подкреплением дает лишь скромные улучшения, в то время как supervised fine-tuning демонстрирует более стабильную генерализацию.'}, 'en': {'title': 'Standardizing Reasoning Evaluations for Language Models', 'desc': 'This paper addresses the challenges in evaluating reasoning capabilities of language models (LMs) due to inconsistent benchmarking practices. The authors find that current mathematical reasoning benchmarks are sensitive to various implementation factors, which can lead to misleading performance claims. They propose a standardized evaluation framework that includes best practices and clear reporting standards to improve transparency and reproducibility. Their findings suggest that supervised finetuning (SFT) methods outperform reinforcement learning (RL) approaches, which often overfit on smaller benchmarks.'}, 'zh': {'title': '推理能力评估的新标准', 'desc': '本文探讨了语言模型在推理能力方面的进展，指出当前的数学推理基准测试对实现细节非常敏感。研究发现，许多评估方法缺乏透明性和统计基础，导致性能提升的比较不够清晰。我们提出了一个标准化的评估框架，明确了最佳实践和报告标准，并重新评估了现有方法。结果显示，强化学习方法的改进有限，而监督微调方法在泛化能力上表现更强。'}}}, {'id': 'https://huggingface.co/papers/2504.07081', 'title': 'Self-Steering Language Models', 'url': 'https://huggingface.co/papers/2504.07081', 'abstract': 'While test-time reasoning enables language models to tackle complex tasks, searching or planning in natural language can be slow, costly, and error-prone. But even when LMs struggle to emulate the precise reasoning steps needed to solve a problem, they often excel at describing its abstract structure--both how to verify solutions and how to search for them. This paper introduces DisCIPL, a method for "self-steering" LMs where a Planner model generates a task-specific inference program that is executed by a population of Follower models. Our approach equips LMs with the ability to write recursive search procedures that guide LM inference, enabling new forms of verifiable and efficient reasoning. When instantiated with a small Follower (e.g., Llama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models, including GPT-4o and o1, on challenging constrained generation tasks. In decoupling planning from execution, our work opens up a design space of highly-parallelized Monte Carlo inference strategies that outperform standard best-of-N sampling, require no finetuning, and can be implemented automatically by existing LMs.', 'score': 9, 'issue_id': 3166, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': 'b0b90ba5f1b881da', 'authors': ['Gabriel Grand', 'Joshua B. Tenenbaum', 'Vikash K. Mansinghka', 'Alexander K. Lew', 'Jacob Andreas'], 'affiliations': ['Massachusetts Institute of Technology', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2504.07081.jpg', 'data': {'categories': ['#training', '#agents', '#reasoning', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Языковые модели учатся писать программы для самоуправления', 'desc': "Статья представляет метод DisCIPL для 'самоуправления' языковых моделей. В этом подходе модель Planner генерирует специфичную для задачи программу вывода, которую выполняет группа моделей Follower. DisCIPL позволяет языковым моделям создавать рекурсивные процедуры поиска для управления выводом. Метод показывает результаты на уровне гораздо более крупных моделей на сложных задачах генерации с ограничениями. DisCIPL открывает возможности для высокопараллельных стратегий вывода по методу Монте-Карло."}, 'en': {'title': 'Empowering Language Models with Self-Steering Inference Programs', 'desc': 'This paper presents DisCIPL, a novel method that enhances language models (LMs) by allowing them to generate task-specific inference programs through a Planner model. These programs are then executed by multiple Follower models, enabling efficient and verifiable reasoning processes. The approach allows LMs to create recursive search procedures that improve their problem-solving capabilities, even when they struggle with direct reasoning. Remarkably, DisCIPL demonstrates that smaller models can achieve performance comparable to larger models on complex tasks, while also introducing new strategies for parallelized inference without the need for fine-tuning.'}, 'zh': {'title': '自我引导的语言模型推理新方法', 'desc': '本文介绍了一种名为DisCIPL的方法，旨在提高语言模型在复杂任务中的推理能力。该方法通过一个规划模型生成特定任务的推理程序，并由多个跟随模型执行，从而实现自我引导。DisCIPL使语言模型能够编写递归搜索程序，提升推理的可验证性和效率。实验表明，使用小型跟随模型时，DisCIPL在一些受限生成任务上表现出色，甚至超过了更大的模型。'}}}, {'id': 'https://huggingface.co/papers/2504.07089', 'title': 'OmniCaptioner: One Captioner to Rule Them All', 'url': 'https://huggingface.co/papers/2504.07089', 'abstract': 'We propose OmniCaptioner, a versatile visual captioning framework for generating fine-grained textual descriptions across a wide variety of visual domains. Unlike prior methods limited to specific image types (e.g., natural images or geometric visuals), our framework provides a unified solution for captioning natural images, visual text (e.g., posters, UIs, textbooks), and structured visuals (e.g., documents, tables, charts). By converting low-level pixel information into semantically rich textual representations, our framework bridges the gap between visual and textual modalities. Our results highlight three key advantages: (i) Enhanced Visual Reasoning with LLMs, where long-context captions of visual modalities empower LLMs, particularly the DeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii) Improved Image Generation, where detailed captions improve tasks like text-to-image generation and image transformation; and (iii) Efficient Supervised Fine-Tuning (SFT), which enables faster convergence with less data. We believe the versatility and adaptability of OmniCaptioner can offer a new perspective for bridging the gap between language and visual modalities.', 'score': 8, 'issue_id': 3164, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': '9b09a9ebdec71131', 'authors': ['Yiting Lu', 'Jiakang Yuan', 'Zhen Li', 'Shitian Zhao', 'Qi Qin', 'Xinyue Li', 'Le Zhuo', 'Licheng Wen', 'Dongyang Liu', 'Yuewen Cao', 'Xiangchao Yan', 'Xin Li', 'Botian Shi', 'Tao Chen', 'Zhibo Chen', 'Lei Bai', 'Bo Zhang', 'Peng Gao'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2504.07089.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#long_context', '#cv', '#training', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Универсальное описание изображений для улучшения мультимодального ИИ', 'desc': 'OmniCaptioner - это универсальная система для создания подробных текстовых описаний различных визуальных данных. Она может работать с естественными изображениями, визуальным текстом и структурированными визуальными данными, преобразуя пиксельную информацию в семантически богатые текстовые представления. Система улучшает визуальное рассуждение с помощью языковых моделей, повышает качество генерации изображений и обеспечивает эффективное обучение с учителем. OmniCaptioner предлагает новый подход к преодолению разрыва между языковыми и визуальными модальностями.'}, 'en': {'title': 'Bridging Visuals and Text with OmniCaptioner', 'desc': 'OmniCaptioner is a new framework designed for generating detailed text descriptions from various types of images. Unlike previous methods that only work with specific image categories, it can handle natural images, visual text, and structured visuals all in one system. The framework transforms pixel data into meaningful text, connecting visual and textual information effectively. Its advantages include better reasoning with large language models, enhanced image generation capabilities, and faster training with less data.'}, 'zh': {'title': 'OmniCaptioner：视觉与文本的桥梁', 'desc': '我们提出了OmniCaptioner，这是一个多功能的视觉描述框架，能够在多种视觉领域生成细致的文本描述。与之前仅限于特定图像类型的方法不同，我们的框架提供了一个统一的解决方案，可以对自然图像、视觉文本（如海报、用户界面、教科书）和结构化视觉（如文档、表格、图表）进行描述。通过将低级像素信息转换为语义丰富的文本表示，我们的框架弥合了视觉和文本模态之间的差距。我们的研究结果显示，OmniCaptioner在视觉推理、图像生成和高效的监督微调方面具有显著优势。'}}}, {'id': 'https://huggingface.co/papers/2504.05541', 'title': 'Caption Anything in Video: Fine-grained Object-centric Captioning via\n  Spatiotemporal Multimodal Prompting', 'url': 'https://huggingface.co/papers/2504.05541', 'abstract': "We present CAT-V (Caption AnyThing in Video), a training-free framework for fine-grained object-centric video captioning that enables detailed descriptions of user-selected objects through time. CAT-V integrates three key components: a Segmenter based on SAMURAI for precise object segmentation across frames, a Temporal Analyzer powered by TRACE-Uni for accurate event boundary detection and temporal analysis, and a Captioner using InternVL-2.5 for generating detailed object-centric descriptions. Through spatiotemporal visual prompts and chain-of-thought reasoning, our framework generates detailed, temporally-aware descriptions of objects' attributes, actions, statuses, interactions, and environmental contexts without requiring additional training data. CAT-V supports flexible user interactions through various visual prompts (points, bounding boxes, and irregular regions) and maintains temporal sensitivity by tracking object states and interactions across different time segments. Our approach addresses limitations of existing video captioning methods, which either produce overly abstract descriptions or lack object-level precision, enabling fine-grained, object-specific descriptions while maintaining temporal coherence and spatial accuracy. The GitHub repository for this project is available at https://github.com/yunlong10/CAT-V", 'score': 7, 'issue_id': 3171, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': '7e72ed48b045c60e', 'authors': ['Yunlong Tang', 'Jing Bi', 'Chao Huang', 'Susan Liang', 'Daiki Shimada', 'Hang Hua', 'Yunzhong Xiao', 'Yizhi Song', 'Pinxin Liu', 'Mingqian Feng', 'Junjia Guo', 'Zhuo Liu', 'Luchuan Song', 'Ali Vosoughi', 'Jinxi He', 'Liu He', 'Zeliang Zhang', 'Jiebo Luo', 'Chenliang Xu'], 'affiliations': ['CMU', 'Purdue University', 'Sony Group Corporation', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2504.05541.jpg', 'data': {'categories': ['#games', '#reasoning', '#multimodal', '#video', '#optimization', '#open_source', '#interpretability'], 'emoji': '🎥', 'ru': {'title': 'Умное описание любых объектов в видео без обучения', 'desc': 'CAT-V - это новая система для детального описания объектов в видео без дополнительного обучения. Она состоит из трех ключевых компонентов: сегментатора на основе SAMURAI, временного анализатора TRACE-Uni и генератора описаний InternVL-2.5. CAT-V использует пространственно-временные визуальные подсказки и рассуждения по цепочке для создания подробных описаний атрибутов, действий и взаимодействий объектов. Система поддерживает гибкое взаимодействие с пользователем и отслеживает состояния объектов во времени, обеспечивая точные и согласованные описания.'}, 'en': {'title': 'Fine-Grained Video Captioning Made Easy with CAT-V!', 'desc': 'CAT-V (Caption AnyThing in Video) is a novel framework designed for fine-grained video captioning that focuses on user-selected objects. It combines a Segmenter for precise object segmentation, a Temporal Analyzer for detecting event boundaries, and a Captioner for generating detailed descriptions. This framework operates without the need for additional training data, utilizing spatiotemporal visual prompts and chain-of-thought reasoning to create accurate, object-centric captions. By addressing the limitations of existing methods, CAT-V provides detailed descriptions that are both temporally aware and spatially accurate, enhancing user interaction with flexible visual prompts.'}, 'zh': {'title': '细致视频描述，精准物体捕捉', 'desc': 'CAT-V（视频中的任何物体描述）是一个无需训练的框架，专注于细粒度的物体中心视频描述。它结合了三个关键组件：基于SAMURAI的分割器用于精确的物体分割，TRACE-Uni驱动的时间分析器用于准确的事件边界检测，以及使用InternVL-2.5生成详细描述的描述器。通过时空视觉提示和链式思维推理，CAT-V能够生成对象属性、动作、状态、交互和环境上下文的详细描述，同时保持时间敏感性。该方法克服了现有视频描述方法的局限，提供了细致的物体特定描述，同时保持时间一致性和空间准确性。'}}}, {'id': 'https://huggingface.co/papers/2504.07092', 'title': 'Are We Done with Object-Centric Learning?', 'url': 'https://huggingface.co/papers/2504.07092', 'abstract': 'Object-centric learning (OCL) seeks to learn representations that only encode an object, isolated from other objects or background cues in a scene. This approach underpins various aims, including out-of-distribution (OOD) generalization, sample-efficient composition, and modeling of structured environments. Most research has focused on developing unsupervised mechanisms that separate objects into discrete slots in the representation space, evaluated using unsupervised object discovery. However, with recent sample-efficient segmentation models, we can separate objects in the pixel space and encode them independently. This achieves remarkable zero-shot performance on OOD object discovery benchmarks, is scalable to foundation models, and can handle a variable number of slots out-of-the-box. Hence, the goal of OCL methods to obtain object-centric representations has been largely achieved. Despite this progress, a key question remains: How does the ability to separate objects within a scene contribute to broader OCL objectives, such as OOD generalization? We address this by investigating the OOD generalization challenge caused by spurious background cues through the lens of OCL. We propose a novel, training-free probe called Object-Centric Classification with Applied Masks (OCCAM), demonstrating that segmentation-based encoding of individual objects significantly outperforms slot-based OCL methods. However, challenges in real-world applications remain. We provide the toolbox for the OCL community to use scalable object-centric representations, and focus on practical applications and fundamental questions, such as understanding object perception in human cognition. Our code is available https://github.com/AlexanderRubinstein/OCCAM{here}.', 'score': 5, 'issue_id': 3166, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': '8e182461a15aee14', 'authors': ['Alexander Rubinstein', 'Ameya Prabhu', 'Matthias Bethge', 'Seong Joon Oh'], 'affiliations': ['Tubingen AI Center, University of Tubingen'], 'pdf_title_img': 'assets/pdf/title_img/2504.07092.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#transfer_learning', '#cv', '#training', '#science'], 'emoji': '🔍', 'ru': {'title': 'Революция в объектно-центрическом обучении: от слотов к сегментации', 'desc': 'Статья посвящена объектно-центрическому обучению (OCL) в машинном обучении, которое стремится создавать представления, кодирующие только отдельные объекты. Авторы предлагают новый метод OCCAM, основанный на сегментации и кодировании отдельных объектов, который превосходит существующие подходы OCL. Исследование демонстрирует, что такой подход значительно улучшает обобщение вне распределения (OOD) и справляется с проблемой ложных фоновых признаков. Статья открывает новые перспективы для практического применения OCL и изучения фундаментальных вопросов восприятия объектов.'}, 'en': {'title': 'Unlocking Object-Centric Learning for Better Generalization', 'desc': 'This paper discusses Object-Centric Learning (OCL), which aims to create representations that focus solely on individual objects, ignoring other elements in a scene. The authors highlight advancements in segmentation models that allow for effective separation of objects at the pixel level, leading to improved performance in out-of-distribution (OOD) object discovery tasks. They introduce a new method called Object-Centric Classification with Applied Masks (OCCAM), which shows that this segmentation approach outperforms traditional slot-based methods. The paper also addresses ongoing challenges in applying these techniques to real-world scenarios and emphasizes the importance of understanding object perception in human cognition.'}, 'zh': {'title': '实现对象中心表示的突破', 'desc': '对象中心学习（OCL）旨在学习仅编码对象的表示，独立于场景中的其他对象或背景线索。这种方法支持多种目标，包括分布外（OOD）泛化、样本高效组合和结构化环境建模。我们提出了一种新的无训练探测器，称为应用掩码的对象中心分类（OCCAM），显示出基于分割的个体对象编码显著优于基于槽的OCL方法。尽管在实际应用中仍面临挑战，但我们为OCL社区提供了可扩展的对象中心表示工具箱。'}}}, {'id': 'https://huggingface.co/papers/2504.06719', 'title': 'Masked Scene Modeling: Narrowing the Gap Between Supervised and\n  Self-Supervised Learning in 3D Scene Understanding', 'url': 'https://huggingface.co/papers/2504.06719', 'abstract': 'Self-supervised learning has transformed 2D computer vision by enabling models trained on large, unannotated datasets to provide versatile off-the-shelf features that perform similarly to models trained with labels. However, in 3D scene understanding, self-supervised methods are typically only used as a weight initialization step for task-specific fine-tuning, limiting their utility for general-purpose feature extraction. This paper addresses this shortcoming by proposing a robust evaluation protocol specifically designed to assess the quality of self-supervised features for 3D scene understanding. Our protocol uses multi-resolution feature sampling of hierarchical models to create rich point-level representations that capture the semantic capabilities of the model and, hence, are suitable for evaluation with linear probing and nearest-neighbor methods. Furthermore, we introduce the first self-supervised model that performs similarly to supervised models when only off-the-shelf features are used in a linear probing setup. In particular, our model is trained natively in 3D with a novel self-supervised approach based on a Masked Scene Modeling objective, which reconstructs deep features of masked patches in a bottom-up manner and is specifically tailored to hierarchical 3D models. Our experiments not only demonstrate that our method achieves competitive performance to supervised models, but also surpasses existing self-supervised approaches by a large margin. The model and training code can be found at our Github repository (https://github.com/phermosilla/msm).', 'score': 5, 'issue_id': 3168, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': '082fef36edfa1a3f', 'authors': ['Pedro Hermosilla', 'Christian Stippel', 'Leon Sick'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.06719.jpg', 'data': {'categories': ['#benchmark', '#training', '#cv', '#self_supervised', '#open_source', '#3d'], 'emoji': '🧠', 'ru': {'title': 'Революция в самообучении 3D: от изображений к пониманию сцен', 'desc': 'Эта статья представляет новый подход к самообучению в области 3D компьютерного зрения. Авторы предлагают протокол оценки качества самообучаемых признаков для понимания 3D сцен, используя многоуровневую выборку признаков. Они также вводят новую модель самообучения, основанную на задаче Masked Scene Modeling, которая реконструирует глубокие признаки маскированных участков. Эксперименты показывают, что их метод достигает производительности, сопоставимой с supervised моделями, и значительно превосходит существующие self-supervised подходы.'}, 'en': {'title': 'Revolutionizing 3D Scene Understanding with Self-Supervised Learning', 'desc': 'This paper presents a new self-supervised learning approach for 3D scene understanding that enhances feature extraction without relying on labeled data. It introduces a robust evaluation protocol that assesses the quality of self-supervised features using multi-resolution sampling of hierarchical models. The authors propose a novel Masked Scene Modeling objective that reconstructs features from masked patches, allowing the model to learn effectively in a 3D context. Their experiments show that this method not only matches the performance of supervised models but also significantly outperforms existing self-supervised techniques.'}, 'zh': {'title': '自监督学习助力三维场景理解', 'desc': '自监督学习在二维计算机视觉中取得了显著进展，使得在大型未标注数据集上训练的模型能够提供类似于有标签模型的多功能特征。然而，在三维场景理解中，自监督方法通常仅用于权重初始化，限制了其在通用特征提取中的应用。本文提出了一种专门评估自监督特征质量的稳健评估协议，利用多分辨率特征采样创建丰富的点级表示。我们还介绍了第一个在仅使用自监督特征的线性探测设置中表现与监督模型相似的自监督模型，展示了其在三维场景理解中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2504.04010', 'title': 'DiTaiListener: Controllable High Fidelity Listener Video Generation with\n  Diffusion', 'url': 'https://huggingface.co/papers/2504.04010', 'abstract': "Generating naturalistic and nuanced listener motions for extended interactions remains an open problem. Existing methods often rely on low-dimensional motion codes for facial behavior generation followed by photorealistic rendering, limiting both visual fidelity and expressive richness. To address these challenges, we introduce DiTaiListener, powered by a video diffusion model with multimodal conditions. Our approach first generates short segments of listener responses conditioned on the speaker's speech and facial motions with DiTaiListener-Gen. It then refines the transitional frames via DiTaiListener-Edit for a seamless transition. Specifically, DiTaiListener-Gen adapts a Diffusion Transformer (DiT) for the task of listener head portrait generation by introducing a Causal Temporal Multimodal Adapter (CTM-Adapter) to process speakers' auditory and visual cues. CTM-Adapter integrates speakers' input in a causal manner into the video generation process to ensure temporally coherent listener responses. For long-form video generation, we introduce DiTaiListener-Edit, a transition refinement video-to-video diffusion model. The model fuses video segments into smooth and continuous videos, ensuring temporal consistency in facial expressions and image quality when merging short video segments produced by DiTaiListener-Gen. Quantitatively, DiTaiListener achieves the state-of-the-art performance on benchmark datasets in both photorealism (+73.8% in FID on RealTalk) and motion representation (+6.1% in FD metric on VICO) spaces. User studies confirm the superior performance of DiTaiListener, with the model being the clear preference in terms of feedback, diversity, and smoothness, outperforming competitors by a significant margin.", 'score': 5, 'issue_id': 3164, 'pub_date': '2025-04-05', 'pub_date_card': {'ru': '5 апреля', 'en': 'April 5', 'zh': '4月5日'}, 'hash': 'd006058dfff067dc', 'authors': ['Maksim Siniukov', 'Di Chang', 'Minh Tran', 'Hongkun Gong', 'Ashutosh Chaubey', 'Mohammad Soleymani'], 'affiliations': ['University of Southern California Los Angeles, USA'], 'pdf_title_img': 'assets/pdf/title_img/2504.04010.jpg', 'data': {'categories': ['#benchmark', '#diffusion', '#video', '#multimodal'], 'emoji': '🎭', 'ru': {'title': 'DiTaiListener: революция в генерации реалистичных реакций слушателя', 'desc': 'Статья представляет DiTaiListener - новый метод генерации естественных движений слушателя в длительных диалогах с использованием видео-диффузионной модели. DiTaiListener состоит из двух компонентов: DiTaiListener-Gen для создания коротких сегментов реакций слушателя и DiTaiListener-Edit для плавного объединения этих сегментов. Модель использует адаптер CTM для обработки аудио- и визуальных сигналов говорящего, обеспечивая согласованность во времени. Количественные и качественные оценки показывают превосходство DiTaiListener над существующими методами в фотореалистичности и выразительности движений.'}, 'en': {'title': 'DiTaiListener: Realistic Listener Motions for Engaging Interactions', 'desc': "The paper presents DiTaiListener, a novel approach for generating realistic listener motions during extended interactions. It utilizes a video diffusion model that incorporates multimodal conditions, allowing for the generation of listener responses based on the speaker's speech and facial movements. The method consists of two main components: DiTaiListener-Gen, which creates short segments of listener responses, and DiTaiListener-Edit, which refines these segments for smooth transitions. The results show that DiTaiListener outperforms existing methods in both visual quality and motion representation, achieving state-of-the-art performance on benchmark datasets and receiving positive feedback from user studies."}, 'zh': {'title': '自然互动中的听众动作生成新突破', 'desc': '本文介绍了一种名为DiTaiListener的模型，旨在生成自然且细腻的听众动作，以改善长时间互动中的表现。该模型利用视频扩散模型和多模态条件，首先生成基于说话者语音和面部动作的短段听众反应。接着，通过DiTaiListener-Edit对过渡帧进行精细化处理，以确保视频的平滑过渡。实验结果表明，DiTaiListener在视觉真实感和动作表现上均达到了最先进的性能，用户研究也显示其在反馈、多样性和流畅性方面明显优于其他竞争模型。'}}}, {'id': 'https://huggingface.co/papers/2504.06958', 'title': 'VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement\n  Fine-Tuning', 'url': 'https://huggingface.co/papers/2504.06958', 'abstract': 'Recent advancements in reinforcement learning have significantly advanced the reasoning capabilities of multimodal large language models (MLLMs). While approaches such as Group Relative Policy Optimization (GRPO) and rule-based reward mechanisms demonstrate promise in text and image domains, their application to video understanding remains limited. This paper presents a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, aiming to enhance spatio-temporal perception while maintaining general capabilities. Our experiments reveal that RFT is highly data-efficient for task-specific improvements. Through multi-task RFT on spatio-temporal perception objectives with limited samples, we develop VideoChat-R1, a powerful video MLLM that achieves state-of-the-art performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1 boosts performance several-fold in tasks like temporal grounding (+31.8) and object tracking (+31.2). Additionally, it significantly improves on general QA benchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9). Our findings underscore the potential of RFT for specialized task enhancement of Video MLLMs. We hope our work offers valuable insights for future RL research in video MLLMs.', 'score': 4, 'issue_id': 3166, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': 'b88359823eee79f5', 'authors': ['Xinhao Li', 'Ziang Yan', 'Desen Meng', 'Lu Dong', 'Xiangyu Zeng', 'Yinan He', 'Yali Wang', 'Yu Qiao', 'Yi Wang', 'Limin Wang'], 'affiliations': ['Nanjing University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences', 'University of Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.06958.jpg', 'data': {'categories': ['#optimization', '#rl', '#training', '#video', '#multimodal', '#reasoning'], 'emoji': '🎥', 'ru': {'title': 'RFT: Прорыв в пространственно-временном восприятии видео для MLLM', 'desc': 'Статья представляет систематическое исследование применения метода Reinforcement Fine-Tuning (RFT) с использованием Group Relative Policy Optimization (GRPO) для улучшения пространственно-временного восприятия в мультимодальных больших языковых моделях (MLLM) для видео. Авторы разработали VideoChat-R1 - мощную видео MLLM, достигающую передовых результатов в задачах пространственно-временного восприятия без ущерба для способности к диалогу. По сравнению с Qwen2.5-VL-7B, VideoChat-R1 значительно улучшает производительность в таких задачах, как временная локализация и отслеживание объектов. Исследование подчеркивает потенциал RFT для специализированного улучшения видео MLLM в конкретных задачах.'}, 'en': {'title': 'Enhancing Video Understanding with Reinforcement Fine-Tuning', 'desc': "This paper explores the use of Reinforcement Fine-Tuning (RFT) combined with Group Relative Policy Optimization (GRPO) to improve video understanding in multimodal large language models (MLLMs). The authors demonstrate that RFT is effective in enhancing spatio-temporal perception while retaining the model's general capabilities. Their experiments show that the newly developed VideoChat-R1 model significantly outperforms existing models in tasks like temporal grounding and object tracking, achieving state-of-the-art results. The findings highlight the efficiency of RFT for specialized improvements in video MLLMs, paving the way for future research in this area."}, 'zh': {'title': '强化学习助力视频理解的突破', 'desc': '本论文探讨了强化学习在多模态大语言模型（MLLMs）中的应用，特别是视频理解方面。我们提出了一种系统的强化微调（RFT）方法，结合了群体相对策略优化（GRPO），以增强视频模型的时空感知能力。实验结果表明，RFT在特定任务的改进上具有很高的数据效率，开发了名为VideoChat-R1的强大视频MLLM。该模型在时空感知任务上表现出色，同时保持了良好的对话能力，显著提升了多个基准测试的性能。'}}}, {'id': 'https://huggingface.co/papers/2504.05523', 'title': 'Pretraining Language Models for Diachronic Linguistic Change Discovery', 'url': 'https://huggingface.co/papers/2504.05523', 'abstract': 'Large language models (LLMs) have shown potential as tools for scientific discovery. This has engendered growing interest in their use in humanistic disciplines, such as historical linguistics and literary studies. These fields often construct arguments on the basis of delineations like genre, or more inflexibly, time period. Although efforts have been made to restrict inference to specific domains via fine-tuning or model editing, we posit that the only true guarantee is domain-restricted pretraining -- typically, a data- and compute-expensive proposition.   We show that efficient pretraining techniques can produce useful models over corpora too large for easy manual inspection but too small for "typical" LLM approaches. We employ a novel date-attribution pipeline in order to obtain a temporally-segmented dataset of five 10-million-word slices. We train two corresponding five-model batteries over these corpus segments, efficient pretraining and Llama3-8B parameter efficiently finetuned.   We find that the pretrained models are faster to train than the finetuned baselines and that they better respect the historical divisions of our corpus. Emphasizing speed and precision over a-historical comprehensiveness enables a number of novel approaches to hypothesis discovery and testing in our target fields. Taking up diachronic linguistics as a testbed, we show that our method enables the detection of a diverse set of phenomena, including en masse lexical change, non-lexical (grammatical and morphological) change, and word sense introduction/obsolescence. We provide a ready-to-use pipeline that allows extension of our approach to other target fields with only minimal adaptation.', 'score': 4, 'issue_id': 3170, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': '9bdf5f1c61fd329f', 'authors': ['Elisabeth Fittschen', 'Sabrina Li', 'Tom Lippincott', 'Leshem Choshen', 'Craig Messner'], 'affiliations': ['Center for Digital Humanities, Johns Hopkins University, USA', 'IBM Research, MIT, USA', 'University of Hamburg, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2504.05523.jpg', 'data': {'categories': ['#dataset', '#data', '#transfer_learning', '#science', '#training'], 'emoji': '📚', 'ru': {'title': 'Эффективное предобучение языковых моделей для исторической лингвистики', 'desc': 'Эта статья описывает использование эффективных методов предобучения для создания языковых моделей на основе небольших, исторически сегментированных корпусов текстов. Авторы разработали пайплайн для атрибуции дат и создали набор из пяти моделей, каждая обученная на 10 миллионах слов из определенного временного периода. Сравнение с базовыми моделями, дообученными на основе Llama3-8B, показало, что предобученные модели лучше учитывают исторические особенности корпуса. Этот подход позволяет обнаруживать различные лингвистические явления, включая лексические и грамматические изменения, а также появление и устаревание значений слов.'}, 'en': {'title': 'Efficient Pretraining for Targeted Language Models in Humanities', 'desc': 'This paper explores the use of large language models (LLMs) for scientific discovery in humanistic disciplines like historical linguistics and literary studies. The authors argue that domain-restricted pretraining is essential for effective model performance, especially when dealing with specific genres or time periods. They introduce efficient pretraining techniques that allow for the training of models on large datasets that are otherwise too small for traditional LLM methods. Their findings demonstrate that these pretrained models not only train faster but also better adhere to historical context, facilitating new approaches to hypothesis testing in diachronic linguistics.'}, 'zh': {'title': '高效预训练：推动人文学科的科学发现', 'desc': '大型语言模型（LLMs）在科学发现中展现了潜力，尤其是在历史语言学和文学研究等人文学科中。这些领域通常基于体裁或时间段等划分来构建论点。尽管通过微调或模型编辑来限制推理到特定领域的努力已经取得了一定进展，但我们认为，唯一真正的保证是领域限制的预训练，这通常需要大量的数据和计算资源。我们的研究表明，采用高效的预训练技术可以在数据量过大而难以手动检查但又不足以使用“典型”LLM方法的语料库上，产生有用的模型。'}}}, {'id': 'https://huggingface.co/papers/2504.06947', 'title': 'RuOpinionNE-2024: Extraction of Opinion Tuples from Russian News Texts', 'url': 'https://huggingface.co/papers/2504.06947', 'abstract': 'In this paper, we introduce the Dialogue Evaluation shared task on extraction of structured opinions from Russian news texts. The task of the contest is to extract opinion tuples for a given sentence; the tuples are composed of a sentiment holder, its target, an expression and sentiment from the holder to the target. In total, the task received more than 100 submissions. The participants experimented mainly with large language models in zero-shot, few-shot and fine-tuning formats. The best result on the test set was obtained with fine-tuning of a large language model. We also compared 30 prompts and 11 open source language models with 3-32 billion parameters in the 1-shot and 10-shot settings and found the best models and prompts.', 'score': 3, 'issue_id': 3166, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': '00cf6cc0a0c3d1c7', 'authors': ['Natalia Loukachevitch', 'Natalia Tkachenko', 'Anna Lapanitsyna', 'Mikhail Tikhomirov', 'Nicolay Rusnachenko'], 'affiliations': ['Bauman Moscow State Technical University', 'Lomonosov Moscow State University'], 'pdf_title_img': 'assets/pdf/title_img/2504.06947.jpg', 'data': {'categories': ['#dataset', '#machine_translation', '#training', '#multilingual', '#open_source'], 'emoji': '🗣️', 'ru': {'title': 'Извлечение мнений из текста: соревнование по оценке диалогов', 'desc': 'Эта статья представляет задачу извлечения структурированных мнений из русскоязычных новостных текстов. Участники соревнования должны были извлекать кортежи мнений, состоящие из источника мнения, его цели, выражения и сентимента. Большинство участников экспериментировали с большими языковыми моделями в форматах zero-shot, few-shot и fine-tuning. Лучший результат на тестовом наборе был получен с помощью дообучения большой языковой модели.'}, 'en': {'title': 'Extracting Structured Opinions from Russian News Using Language Models', 'desc': 'This paper presents a shared task focused on extracting structured opinions from Russian news articles. The goal is to identify opinion tuples that include a sentiment holder, target, expression, and sentiment direction. Over 100 submissions were received, with participants primarily utilizing large language models in various training formats such as zero-shot, few-shot, and fine-tuning. The best performance on the test set was achieved through fine-tuning a large language model, and the study also evaluated different prompts and open-source models to determine the most effective combinations.'}, 'zh': {'title': '从新闻文本中提取结构化意见的挑战', 'desc': '本文介绍了一个关于从俄语新闻文本中提取结构化意见的对话评估共享任务。该任务要求参赛者为给定句子提取意见元组，这些元组由情感持有者、目标、表达和情感组成。总共收到了超过100个提交，参与者主要使用大型语言模型进行零样本、少样本和微调实验。测试集上最佳结果是通过对大型语言模型进行微调获得的，同时我们还比较了30个提示和11个开源语言模型，参数范围从3亿到32亿，找出了最佳模型和提示。'}}}, {'id': 'https://huggingface.co/papers/2504.03886', 'title': 'WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments', 'url': 'https://huggingface.co/papers/2504.03886', 'abstract': "We present WildGS-SLAM, a robust and efficient monocular RGB SLAM system designed to handle dynamic environments by leveraging uncertainty-aware geometric mapping. Unlike traditional SLAM systems, which assume static scenes, our approach integrates depth and uncertainty information to enhance tracking, mapping, and rendering performance in the presence of moving objects. We introduce an uncertainty map, predicted by a shallow multi-layer perceptron and DINOv2 features, to guide dynamic object removal during both tracking and mapping. This uncertainty map enhances dense bundle adjustment and Gaussian map optimization, improving reconstruction accuracy. Our system is evaluated on multiple datasets and demonstrates artifact-free view synthesis. Results showcase WildGS-SLAM's superior performance in dynamic environments compared to state-of-the-art methods.", 'score': 2, 'issue_id': 3167, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 апреля', 'en': 'April 4', 'zh': '4月4日'}, 'hash': '086d82b3ec2f4b04', 'authors': ['Jianhao Zheng', 'Zihan Zhu', 'Valentin Bieri', 'Marc Pollefeys', 'Songyou Peng', 'Iro Armeni'], 'affiliations': ['ETH Zürich', 'Microsoft', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2504.03886.jpg', 'data': {'categories': ['#benchmark', '#cv', '#3d'], 'emoji': '🌪️', 'ru': {'title': 'Робастный SLAM для динамического мира', 'desc': 'WildGS-SLAM - это устойчивая и эффективная система монокулярного RGB SLAM, предназначенная для работы в динамических средах. Она использует геометрическое картирование с учетом неопределенности, интегрируя информацию о глубине и неопределенности для улучшения отслеживания, картирования и рендеринга в присутствии движущихся объектов. Система вводит карту неопределенности, предсказанную с помощью неглубокого многослойного перцептрона и признаков DINOv2, для удаления динамических объектов. WildGS-SLAM демонстрирует превосходную производительность в динамических средах по сравнению с современными методами.'}, 'en': {'title': 'Dynamic SLAM Redefined: Embracing Uncertainty for Robust Mapping', 'desc': 'WildGS-SLAM is a monocular RGB SLAM system that effectively operates in dynamic environments by incorporating uncertainty-aware geometric mapping. It differs from traditional SLAM systems by utilizing depth and uncertainty data to improve tracking and mapping when moving objects are present. The system employs an uncertainty map, generated by a shallow multi-layer perceptron and DINOv2 features, to facilitate the removal of dynamic objects during the SLAM process. Evaluations on various datasets reveal that WildGS-SLAM achieves high reconstruction accuracy and artifact-free view synthesis, outperforming existing methods in dynamic settings.'}, 'zh': {'title': '动态环境下的高效SLAM解决方案', 'desc': 'WildGS-SLAM是一种强大且高效的单目RGB SLAM系统，专为处理动态环境而设计。与传统的SLAM系统假设静态场景不同，我们的方法结合了深度和不确定性信息，以提高在移动物体存在下的跟踪、映射和渲染性能。我们引入了一种不确定性地图，通过浅层多层感知器和DINOv2特征进行预测，以指导动态物体的去除，从而增强密集束调整和高斯地图优化，提升重建精度。我们的系统在多个数据集上进行了评估，结果显示WildGS-SLAM在动态环境中的表现优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2504.05287', 'title': 'RobustDexGrasp: Robust Dexterous Grasping of General Objects from\n  Single-view Perception', 'url': 'https://huggingface.co/papers/2504.05287', 'abstract': 'Robust grasping of various objects from single-view perception is fundamental for dexterous robots. Previous works often rely on fully observable objects, expert demonstrations, or static grasping poses, which restrict their generalization ability and adaptability to external disturbances. In this paper, we present a reinforcement-learning-based framework that enables zero-shot dynamic dexterous grasping of a wide range of unseen objects from single-view perception, while performing adaptive motions to external disturbances. We utilize a hand-centric object representation for shape feature extraction that emphasizes interaction-relevant local shapes, enhancing robustness to shape variance and uncertainty. To enable effective hand adaptation to disturbances with limited observations, we propose a mixed curriculum learning strategy, which first utilizes imitation learning to distill a policy trained with privileged real-time visual-tactile feedback, and gradually transfers to reinforcement learning to learn adaptive motions under disturbances caused by observation noises and dynamic randomization. Our experiments demonstrate strong generalization in grasping unseen objects with random poses, achieving success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects. We also demonstrate the robustness of our method to various disturbances, including unobserved object movement and external forces, through both quantitative and qualitative evaluations. Project Page: https://zdchan.github.io/Robust_DexGrasp/', 'score': 0, 'issue_id': 3165, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': 'fab0fa176a952987', 'authors': ['Hui Zhang', 'Zijian Wu', 'Linyi Huang', 'Sammy Christen', 'Jie Song'], 'affiliations': ['ETH Zurich, Switzerland', 'HKUST (Guangzhou), China', 'HKUST, Hong Kong (China)'], 'pdf_title_img': 'assets/pdf/title_img/2504.05287.jpg', 'data': {'categories': ['#robotics', '#training', '#games', '#rl', '#optimization'], 'emoji': '🦾', 'ru': {'title': 'Адаптивный захват объектов роботом по одному изображению', 'desc': 'Статья представляет метод обучения с подкреплением для захвата разнообразных объектов роботизированной рукой на основе одного изображения. Авторы используют особое представление объекта, ориентированное на взаимодействие с рукой, что повышает устойчивость к вариациям формы. Предложена стратегия смешанного обучения, сочетающая имитационное обучение и обучение с подкреплением для адаптации к внешним помехам. Эксперименты показывают высокую обобщающую способность метода при захвате новых объектов в симуляции и реальности.'}, 'en': {'title': 'Dynamic Grasping: Robots That Adapt and Overcome!', 'desc': "This paper introduces a reinforcement learning framework for robots to grasp various unseen objects using only a single view. Unlike previous methods that depend on fully visible objects or expert demonstrations, this approach allows for dynamic and adaptive grasping in response to disturbances. The authors utilize a hand-centric object representation to focus on important shape features, improving the robot's ability to handle shape variations. Their mixed curriculum learning strategy combines imitation learning and reinforcement learning, resulting in high success rates for grasping both simulated and real objects under challenging conditions."}, 'zh': {'title': '实现灵巧抓取的零-shot学习新方法', 'desc': '本论文提出了一种基于强化学习的框架，旨在实现从单视角感知中对各种未知物体的零-shot动态灵巧抓取。与以往依赖完全可观察物体或专家演示的方法不同，该方法能够适应外部干扰并进行自适应动作。我们采用以手为中心的物体表示法，提取与交互相关的局部形状特征，从而增强对形状变化和不确定性的鲁棒性。实验结果表明，该方法在抓取未知物体时具有强大的泛化能力，成功率高达97.0%。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (1)', '#agi', '#alignment', '#architecture (2)', '#audio', '#benchmark (6)', '#cv (6)', '#data (2)', '#dataset (4)', '#diffusion (3)', '#ethics', '#games (2)', '#graphs', '#hallucinations (2)', '#healthcare', '#inference (1)', '#interpretability (3)', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation (1)', '#math', '#multilingual (1)', '#multimodal (7)', '#open_source (6)', '#optimization (7)', '#plp', '#rag', '#reasoning (6)', '#rl (4)', '#rlhf', '#robotics (1)', '#science (2)', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic', '#training (11)', '#transfer_learning (2)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-04-10 18:14',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-04-10 18:14')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-04-10 18:14')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    