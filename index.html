
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 28 papers. April 23.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">23 апреля</span> | <span id="title-articles-count">28 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-04-22.html">⬅️ <span id="prev-date">22.04</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-04-24.html">➡️ <span id="next-date">24.04</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-04.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '23 апреля', 'en': 'April 23', 'zh': '4月23日'};
        let feedDateNext = {'ru': '24.04', 'en': '04/24', 'zh': '4月24日'};
        let feedDatePrev = {'ru': '22.04', 'en': '04/22', 'zh': '4月22日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2504.14945', 'title': 'Learning to Reason under Off-Policy Guidance', 'url': 'https://huggingface.co/papers/2504.14945', 'abstract': "Recent advances in large reasoning models (LRMs) demonstrate that sophisticated behaviors such as multi-step reasoning and self-reflection can emerge via reinforcement learning (RL) with simple rule-based rewards. However, existing zero-RL approaches are inherently ``on-policy'', limiting learning to a model's own outputs and failing to acquire reasoning abilities beyond its initial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY guidance), a framework that augments zero-RL with off-policy reasoning traces. LUFFY dynamically balances imitation and exploration by combining off-policy demonstrations with on-policy rollouts during training. Notably, we propose policy shaping via regularized importance sampling to avoid superficial and rigid imitation during mixed-policy training. Remarkably, LUFFY achieves an over +7.0 average gain across six math benchmarks and an advantage of over +6.2 points in out-of-distribution tasks. It also substantially surpasses imitation-based supervised fine-tuning (SFT), particularly in generalization. Analysis shows LUFFY not only imitates effectively but also explores beyond demonstrations, offering a scalable path to train generalizable reasoning models with off-policy guidance.", 'score': 56, 'issue_id': 3358, 'pub_date': '2025-04-21', 'pub_date_card': {'ru': '21 апреля', 'en': 'April 21', 'zh': '4月21日'}, 'hash': 'a0e7730fccfb6534', 'authors': ['Jianhao Yan', 'Yafu Li', 'Zican Hu', 'Zhi Wang', 'Ganqu Cui', 'Xiaoye Qu', 'Yu Cheng', 'Yue Zhang'], 'affiliations': ['Nanjing University', 'Shanghai AI Laboratory', 'The Chinese University of Hong Kong', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2504.14945.jpg', 'data': {'categories': ['#training', '#reasoning', '#math', '#optimization', '#rl'], 'emoji': '🧠', 'ru': {'title': 'LUFFY: обучение рассуждениям с балансом между имитацией и исследованием', 'desc': 'LUFFY - это новый подход к обучению больших моделей рассуждений (LRM) с использованием обучения с подкреплением. В отличие от существующих методов, LUFFY сочетает имитацию готовых решений с исследованием новых стратегий. Это позволяет модели не только копировать демонстрации, но и выходить за их рамки, находя более эффективные способы рассуждений. Результаты показывают значительное улучшение производительности на математических тестах и задачах вне обучающей выборки по сравнению с традиционными методами.'}, 'en': {'title': 'LUFFY: Expanding Reasoning with Off-Policy Learning', 'desc': 'This paper presents LUFFY, a new framework that enhances zero-reinforcement learning (zero-RL) by incorporating off-policy reasoning traces. Unlike traditional zero-RL methods that are limited to on-policy learning, LUFFY allows models to learn from a broader range of experiences by balancing imitation of off-policy demonstrations with exploration of their own outputs. The framework employs policy shaping through regularized importance sampling to ensure that the model does not merely imitate but also develops deeper reasoning capabilities. LUFFY shows significant improvements in performance on math benchmarks and out-of-distribution tasks, demonstrating its effectiveness in training generalizable reasoning models.'}, 'zh': {'title': 'LUFFY：超越初始能力的推理学习', 'desc': '最近的大型推理模型（LRMs）进展表明，通过简单的基于规则的奖励，强化学习（RL）可以产生复杂的行为，如多步推理和自我反思。然而，现有的零强化学习方法本质上是“在政策上”的，这限制了学习仅限于模型自身的输出，无法获得超出初始能力的推理能力。我们提出了LUFFY（在政策外指导下学习推理），该框架通过引入政策外的推理轨迹来增强零强化学习。LUFFY在训练过程中动态平衡模仿和探索，结合了政策外的示范和政策内的回放，显著提高了模型的推理能力和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2504.15271', 'title': 'Eagle 2.5: Boosting Long-Context Post-Training for Frontier\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2504.15271', 'abstract': 'We introduce Eagle 2.5, a family of frontier vision-language models (VLMs) for long-context multimodal learning. Our work addresses the challenges in long video comprehension and high-resolution image understanding, introducing a generalist framework for both tasks. The proposed training framework incorporates Automatic Degrade Sampling and Image Area Preservation, two techniques that preserve contextual integrity and visual details. The framework also includes numerous efficiency optimizations in the pipeline for long-context data training. Finally, we propose Eagle-Video-110K, a novel dataset that integrates both story-level and clip-level annotations, facilitating long-video understanding. Eagle 2.5 demonstrates substantial improvements on long-context multimodal benchmarks, providing a robust solution to the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B achieves 72.4% on Video-MME with 512 input frames, matching the results of top-tier commercial model such as GPT-4o and large-scale open-source models like Qwen2.5-VL-72B and InternVL2.5-78B.', 'score': 48, 'issue_id': 3363, 'pub_date': '2025-04-21', 'pub_date_card': {'ru': '21 апреля', 'en': 'April 21', 'zh': '4月21日'}, 'hash': '069a310bc9c5e733', 'authors': ['Guo Chen', 'Zhiqi Li', 'Shihao Wang', 'Jindong Jiang', 'Yicheng Liu', 'Lidong Lu', 'De-An Huang', 'Wonmin Byeon', 'Matthieu Le', 'Tuomas Rintamaki', 'Tyler Poon', 'Max Ehrlich', 'Tuomas Rintamaki', 'Tyler Poon', 'Tong Lu', 'Limin Wang', 'Bryan Catanzaro', 'Jan Kautz', 'Andrew Tao', 'Zhiding Yu', 'Guilin Liu'], 'affiliations': ['NVIDIA', 'Nanjing University', 'Rutgers University', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2504.15271.jpg', 'data': {'categories': ['#training', '#dataset', '#benchmark', '#optimization', '#multimodal', '#long_context'], 'emoji': '🦅', 'ru': {'title': 'Eagle 2.5: Прорыв в понимании длинного мультимодального контекста', 'desc': 'Eagle 2.5 представляет собой семейство моделей компьютерного зрения и обработки естественного языка для работы с длинным контекстом. Модель решает проблемы понимания длинных видео и изображений высокого разрешения, используя новые методы сохранения контекстуальной целостности и визуальных деталей. В работе также представлен новый датасет Eagle-Video-110K для обучения пониманию длинных видео. Eagle 2.5 демонстрирует значительные улучшения на бенчмарках для мультимодальных задач с длинным контекстом, соперничая с ведущими коммерческими и крупномасштабными открытыми моделями.'}, 'en': {'title': 'Eagle 2.5: Advancing Long-Context Vision-Language Understanding', 'desc': 'Eagle 2.5 is a new family of vision-language models designed to improve understanding of long videos and high-resolution images. It introduces a training framework that uses techniques like Automatic Degrade Sampling and Image Area Preservation to maintain important visual details and context. The model is optimized for efficiency when processing long-context data, making it faster and more effective. Additionally, the Eagle-Video-110K dataset provides comprehensive annotations for better training and evaluation, leading to significant performance gains over existing models.'}, 'zh': {'title': 'Eagle 2.5：长上下文多模态学习的新突破', 'desc': '我们介绍了Eagle 2.5，这是一个用于长上下文多模态学习的前沿视觉-语言模型（VLM）系列。该研究解决了长视频理解和高分辨率图像理解中的挑战，提出了一个通用框架来处理这两项任务。所提出的训练框架结合了自动降级采样和图像区域保留两种技术，以保持上下文完整性和视觉细节。此外，我们还提出了Eagle-Video-110K，这是一个新颖的数据集，集成了故事级和片段级的注释，促进了长视频理解。'}}}, {'id': 'https://huggingface.co/papers/2504.15257', 'title': 'FlowReasoner: Reinforcing Query-Level Meta-Agents', 'url': 'https://huggingface.co/papers/2504.15257', 'abstract': 'This paper proposes a query-level meta-agent named FlowReasoner to automate the design of query-level multi-agent systems, i.e., one system per user query. Our core idea is to incentivize a reasoning-based meta-agent via external execution feedback. Concretely, by distilling DeepSeek R1, we first endow the basic reasoning ability regarding the generation of multi-agent systems to FlowReasoner. Then, we further enhance it via reinforcement learning (RL) with external execution feedback. A multi-purpose reward is designed to guide the RL training from aspects of performance, complexity, and efficiency. In this manner, FlowReasoner is enabled to generate a personalized multi-agent system for each user query via deliberative reasoning. Experiments on both engineering and competition code benchmarks demonstrate the superiority of FlowReasoner. Remarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks. The code is available at https://github.com/sail-sg/FlowReasoner.', 'score': 35, 'issue_id': 3359, 'pub_date': '2025-04-21', 'pub_date_card': {'ru': '21 апреля', 'en': 'April 21', 'zh': '4月21日'}, 'hash': 'edb79ae8d9c372b4', 'authors': ['Hongcheng Gao', 'Yue Liu', 'Yufei He', 'Longxu Dou', 'Chao Du', 'Zhijie Deng', 'Bryan Hooi', 'Min Lin', 'Tianyu Pang'], 'affiliations': ['National University of Singapore', 'Sea AI Lab, Singapore', 'Shanghai Jiao Tong University', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2504.15257.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#reasoning', '#rl', '#agents'], 'emoji': '🤖', 'ru': {'title': 'FlowReasoner: Умный дизайнер многоагентных систем', 'desc': 'В статье представлен FlowReasoner - мета-агент для автоматизированного проектирования многоагентных систем на уровне запросов. Основная идея заключается в стимулировании мета-агента на основе рассуждений с помощью внешней обратной связи. FlowReasoner обучается с помощью дистилляции модели DeepSeek R1 и усиливается через обучение с подкреплением. Эксперименты показывают превосходство FlowReasoner над существующими решениями, в частности, точность повышается на 10.52% по сравнению с o1-mini на трех эталонных наборах данных.'}, 'en': {'title': 'Automating Personalized Multi-Agent Systems with FlowReasoner', 'desc': 'This paper introduces FlowReasoner, a meta-agent designed to automate the creation of multi-agent systems tailored to individual user queries. The approach leverages external execution feedback to enhance the reasoning capabilities of the agent, which is initially based on the DeepSeek R1 model. By employing reinforcement learning, FlowReasoner is trained with a multi-faceted reward system that optimizes for performance, complexity, and efficiency. Experimental results show that FlowReasoner outperforms existing methods, achieving a 10.52% accuracy improvement across various benchmarks.'}, 'zh': {'title': 'FlowReasoner：为每个查询定制智能代理系统', 'desc': '本文提出了一种名为FlowReasoner的查询级元代理，用于自动设计查询级多代理系统，即为每个用户查询构建一个系统。我们的核心思想是通过外部执行反馈来激励基于推理的元代理。具体来说，通过提炼DeepSeek R1，我们首先赋予FlowReasoner基本的多代理系统生成推理能力。然后，通过强化学习（RL）和外部执行反馈进一步增强其能力，设计了一个多用途奖励来指导RL训练，从性能、复杂性和效率等方面进行优化。'}}}, {'id': 'https://huggingface.co/papers/2504.13958', 'title': 'ToolRL: Reward is All Tool Learning Needs', 'url': 'https://huggingface.co/papers/2504.13958', 'abstract': 'Current Large Language Models (LLMs) often undergo supervised fine-tuning (SFT) to acquire tool use capabilities. However, SFT struggles to generalize to unfamiliar or complex tool use scenarios. Recent advancements in reinforcement learning (RL), particularly with R1-like models, have demonstrated promising reasoning and generalization abilities. Yet, reward design for tool use presents unique challenges: multiple tools may be invoked with diverse parameters, and coarse-grained reward signals, such as answer matching, fail to offer the finegrained feedback required for effective learning. In this work, we present the first comprehensive study on reward design for tool selection and application tasks within the RL paradigm. We systematically explore a wide range of reward strategies, analyzing their types, scales, granularity, and temporal dynamics. Building on these insights, we propose a principled reward design tailored for tool use tasks and apply it to train LLMs using Group Relative Policy Optimization (GRPO). Empirical evaluations across diverse benchmarks demonstrate that our approach yields robust, scalable, and stable training, achieving a 17% improvement over base models and a 15% gain over SFT models. These results highlight the critical role of thoughtful reward design in enhancing the tool use capabilities and generalization performance of LLMs. All the codes are released to facilitate future research.', 'score': 32, 'issue_id': 3358, 'pub_date': '2025-04-16', 'pub_date_card': {'ru': '16 апреля', 'en': 'April 16', 'zh': '4月16日'}, 'hash': '53a7757486eb7210', 'authors': ['Cheng Qian', 'Emre Can Acikgoz', 'Qi He', 'Hongru Wang', 'Xiusi Chen', 'Dilek Hakkani-Tür', 'Gokhan Tur', 'Heng Ji'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2504.13958.jpg', 'data': {'categories': ['#training', '#open_source', '#rlhf', '#survey', '#reasoning', '#optimization', '#benchmark', '#rl'], 'emoji': '🛠️', 'ru': {'title': 'Оптимизация вознаграждений для улучшения навыков LLM в использовании инструментов', 'desc': 'Статья представляет первое комплексное исследование разработки функций вознаграждения для задач выбора и применения инструментов в парадигме обучения с подкреплением (RL) для больших языковых моделей (LLM). Авторы систематически изучают различные стратегии вознаграждения и предлагают принципиальный подход к их разработке для задач использования инструментов. Применяя свой метод с использованием Group Relative Policy Optimization (GRPO), они достигают улучшения на 17% по сравнению с базовыми моделями и на 15% по сравнению с моделями, обученными с помощью supervised fine-tuning (SFT). Результаты подчеркивают критическую роль продуманного дизайна функций вознаграждения в улучшении способностей LLM к использованию инструментов и обобщению.'}, 'en': {'title': 'Enhancing Tool Use in LLMs through Smart Reward Design', 'desc': "This paper addresses the challenges of training Large Language Models (LLMs) to effectively use tools through reinforcement learning (RL). It highlights the limitations of supervised fine-tuning (SFT) in generalizing to new tool use scenarios and proposes a novel reward design specifically for tool selection and application tasks. The authors systematically investigate various reward strategies, focusing on their effectiveness in providing fine-grained feedback necessary for learning. Their proposed method, Group Relative Policy Optimization (GRPO), shows significant improvements in training outcomes, demonstrating the importance of well-designed rewards in enhancing LLMs' tool use capabilities."}, 'zh': {'title': '优化奖励设计，提升工具使用能力', 'desc': '当前的大型语言模型（LLMs）通常通过监督微调（SFT）来获得工具使用能力。然而，SFT在面对不熟悉或复杂的工具使用场景时，泛化能力较差。最近，强化学习（RL）特别是R1类模型的进展显示出良好的推理和泛化能力，但工具使用的奖励设计面临独特挑战。本文首次全面研究了在RL范式下工具选择和应用任务的奖励设计，提出了一种针对工具使用任务的原则性奖励设计，并通过实验验证了其在训练LLMs中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2504.13203', 'title': 'X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents', 'url': 'https://huggingface.co/papers/2504.13203', 'abstract': 'Multi-turn interactions with language models (LMs) pose critical safety risks, as harmful intent can be strategically spread across exchanges. Yet, the vast majority of prior work has focused on single-turn safety, while adaptability and diversity remain among the key challenges of multi-turn red-teaming. To address these challenges, we present X-Teaming, a scalable framework that systematically explores how seemingly harmless interactions escalate into harmful outcomes and generates corresponding attack scenarios. X-Teaming employs collaborative agents for planning, attack optimization, and verification, achieving state-of-the-art multi-turn jailbreak effectiveness and diversity with success rates up to 98.1% across representative leading open-weight and closed-source models. In particular, X-Teaming achieves a 96.2% attack success rate against the latest Claude 3.7 Sonnet model, which has been considered nearly immune to single-turn attacks. Building on X-Teaming, we introduce XGuard-Train, an open-source multi-turn safety training dataset that is 20x larger than the previous best resource, comprising 30K interactive jailbreaks, designed to enable robust multi-turn safety alignment for LMs. Our work offers essential tools and insights for mitigating sophisticated conversational attacks, advancing the multi-turn safety of LMs.', 'score': 24, 'issue_id': 3358, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': '43e0ba05f6baef62', 'authors': ['Salman Rahman', 'Liwei Jiang', 'James Shiffer', 'Genglin Liu', 'Sheriff Issaka', 'Md Rizwan Parvez', 'Hamid Palangi', 'Kai-Wei Chang', 'Yejin Choi', 'Saadia Gabriel'], 'affiliations': ['Google', 'Qatar Computing Research Institute', 'Stanford University', 'University of California, Los Angeles', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2504.13203.jpg', 'data': {'categories': ['#alignment', '#training', '#open_source', '#security', '#agents', '#dataset'], 'emoji': '🛡️', 'ru': {'title': 'Укрепление безопасности языковых моделей в многоходовом диалоге', 'desc': 'Статья представляет X-Teaming - фреймворк для исследования многоходовых атак на языковые модели. Он использует коллаборативных агентов для планирования, оптимизации и верификации атак, достигая высокой эффективности взлома различных моделей. На основе X-Teaming создан набор данных XGuard-Train для обучения многоходовой безопасности языковых моделей. Работа предлагает инструменты для защиты от сложных разговорных атак и повышения безопасности ЯМ в многоходовых взаимодействиях.'}, 'en': {'title': 'Enhancing Multi-Turn Safety in Language Models with X-Teaming', 'desc': 'This paper introduces X-Teaming, a framework designed to enhance the safety of multi-turn interactions with language models (LMs). It addresses the challenge of harmful intent spreading across multiple exchanges, which has been largely overlooked in previous research focused on single-turn interactions. X-Teaming utilizes collaborative agents to plan, optimize, and verify attack scenarios, achieving high effectiveness in multi-turn jailbreaks with success rates up to 98.1%. Additionally, the authors present XGuard-Train, a large dataset for training LMs on multi-turn safety, significantly improving the ability to defend against complex conversational attacks.'}, 'zh': {'title': '提升多轮交互安全性的X-Teaming框架', 'desc': '本文提出了一种名为X-Teaming的框架，旨在解决多轮交互中语言模型的安全风险。该框架通过系统性探索无害互动如何演变为有害结果，并生成相应的攻击场景。X-Teaming利用协作代理进行规划、攻击优化和验证，成功率高达98.1%。此外，本文还介绍了XGuard-Train，一个开源的多轮安全训练数据集，规模是之前最佳资源的20倍，包含3万个互动越狱案例，旨在增强语言模型的多轮安全性。'}}}, {'id': 'https://huggingface.co/papers/2504.14396', 'title': 'SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video\n  Generation via Spherical Latent Representation', 'url': 'https://huggingface.co/papers/2504.14396', 'abstract': 'The increasing demand for AR/VR applications has highlighted the need for high-quality 360-degree panoramic content. However, generating high-quality 360-degree panoramic images and videos remains a challenging task due to the severe distortions introduced by equirectangular projection (ERP). Existing approaches either fine-tune pretrained diffusion models on limited ERP datasets or attempt tuning-free methods that still rely on ERP latent representations, leading to discontinuities near the poles. In this paper, we introduce SphereDiff, a novel approach for seamless 360-degree panoramic image and video generation using state-of-the-art diffusion models without additional tuning. We define a spherical latent representation that ensures uniform distribution across all perspectives, mitigating the distortions inherent in ERP. We extend MultiDiffusion to spherical latent space and propose a spherical latent sampling method to enable direct use of pretrained diffusion models. Moreover, we introduce distortion-aware weighted averaging to further improve the generation quality in the projection process. Our method outperforms existing approaches in generating 360-degree panoramic content while maintaining high fidelity, making it a robust solution for immersive AR/VR applications. The code is available here. https://github.com/pmh9960/SphereDiff', 'score': 23, 'issue_id': 3357, 'pub_date': '2025-04-19', 'pub_date_card': {'ru': '19 апреля', 'en': 'April 19', 'zh': '4月19日'}, 'hash': '9688d3d72143f02c', 'authors': ['Minho Park', 'Taewoong Kang', 'Jooyeol Yun', 'Sungwon Hwang', 'Jaegul Choo'], 'affiliations': ['Korea Advanced Institute of Science and Technology (KAIST)'], 'pdf_title_img': 'assets/pdf/title_img/2504.14396.jpg', 'data': {'categories': ['#3d', '#diffusion', '#multimodal', '#open_source', '#video'], 'emoji': '🌐', 'ru': {'title': 'SphereDiff: Бесшовная генерация панорам 360° с помощью диффузионных моделей', 'desc': 'SphereDiff - это новый подход к созданию панорамных изображений и видео с обзором 360 градусов, используя современные диффузионные модели без дополнительной настройки. Метод определяет сферическое латентное представление, которое обеспечивает равномерное распределение по всем ракурсам, уменьшая искажения, присущие эквидистантной проекции. SphereDiff расширяет MultiDiffusion на сферическое латентное пространство и предлагает метод сферической латентной выборки для прямого использования предобученных диффузионных моделей. Кроме того, авторы вводят взвешенное усреднение с учетом искажений для дальнейшего улучшения качества генерации в процессе проекции.'}, 'en': {'title': 'SphereDiff: Seamless 360-Degree Content Generation for AR/VR', 'desc': 'This paper presents SphereDiff, a new method for generating high-quality 360-degree panoramic images and videos using diffusion models. It addresses the challenges of distortions caused by equirectangular projection (ERP) by introducing a spherical latent representation that provides a uniform perspective distribution. SphereDiff enhances the existing MultiDiffusion framework by allowing direct use of pretrained models without the need for additional tuning. The method also incorporates distortion-aware weighted averaging to improve the quality of the generated content, outperforming previous techniques in fidelity and robustness for AR/VR applications.'}, 'zh': {'title': 'SphereDiff：无缝生成360度全景内容的创新方法', 'desc': '随着增强现实和虚拟现实应用的需求增加，高质量的360度全景内容变得尤为重要。然而，由于等距矩形投影（ERP）引入的严重失真，生成高质量的360度全景图像和视频仍然是一个挑战。本文提出了一种名为SphereDiff的新方法，利用最先进的扩散模型实现无缝的360度全景图像和视频生成，且无需额外调优。我们定义了一种球形潜在表示，确保各个视角的均匀分布，从而减轻ERP固有的失真，显著提高生成质量。'}}}, {'id': 'https://huggingface.co/papers/2504.14870', 'title': 'OTC: Optimal Tool Calls via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2504.14870', 'abstract': 'Tool-integrated reasoning (TIR) augments large language models (LLMs) with the ability to invoke external tools, such as search engines and code interpreters, to solve tasks beyond the capabilities of language-only reasoning. While reinforcement learning (RL) has shown promise in improving TIR by optimizing final answer correctness, existing approaches often overlook the efficiency and cost associated with tool usage. This can lead to suboptimal behavior, including excessive tool calls that increase computational and financial overhead, or insufficient tool use that compromises answer quality. In this work, we propose Optimal Tool Call-controlled Policy Optimization (OTC-PO), a simple yet effective RL-based framework that encourages models to produce accurate answers with minimal tool calls. Our method introduces a tool-integrated reward that jointly considers correctness and tool efficiency, promoting high tool productivity. We instantiate this framework within both Proximal Policy Optimization (PPO) and Group Relative Preference Optimization (GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and Qwen-Math across multiple QA benchmarks show that our approach reduces tool calls by up to 73.1\\% and improves tool productivity by up to 229.4\\%, while maintaining comparable answer accuracy. To the best of our knowledge, this is the first RL-based framework that explicitly optimizes tool-use efficiency in TIR.', 'score': 22, 'issue_id': 3371, 'pub_date': '2025-04-21', 'pub_date_card': {'ru': '21 апреля', 'en': 'April 21', 'zh': '4月21日'}, 'hash': '56038971cbf4d8b5', 'authors': ['Hongru Wang', 'Cheng Qian', 'Wanjun Zhong', 'Xiusi Chen', 'Jiahao Qiu', 'Shijue Huang', 'Bowen Jin', 'Mengdi Wang', 'Kam-Fai Wong', 'Heng Ji'], 'affiliations': ['Hong Kong University of Science and Technology', 'Princeton University', 'Sun Yat-sen University', 'The Chinese University of Hong Kong', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2504.14870.jpg', 'data': {'categories': ['#rlhf', '#rl', '#optimization', '#reasoning', '#training'], 'emoji': '🛠️', 'ru': {'title': 'Оптимальное использование инструментов в языковых моделях', 'desc': 'Эта статья представляет новый подход к оптимизации использования инструментов в рассуждениях на основе больших языковых моделей (LLM). Авторы предлагают метод Optimal Tool Call-controlled Policy Optimization (OTC-PO), который улучшает эффективность использования внешних инструментов при сохранении точности ответов. Метод применяет обучение с подкреплением для оптимизации вознаграждения, учитывающего как корректность ответов, так и эффективность использования инструментов. Эксперименты показывают значительное сокращение числа вызовов инструментов и повышение их продуктивности без потери точности ответов.'}, 'en': {'title': 'Optimizing Tool Use for Smarter AI Responses', 'desc': 'This paper introduces a new method called Optimal Tool Call-controlled Policy Optimization (OTC-PO) to enhance the performance of large language models (LLMs) that use external tools for reasoning. The method focuses on improving the efficiency of tool usage while maintaining the accuracy of answers, addressing the common issue of excessive or insufficient tool calls. By implementing a reward system that balances correctness and tool efficiency, OTC-PO encourages models to provide accurate responses with fewer tool interactions. The results demonstrate significant reductions in tool calls and substantial improvements in tool productivity without sacrificing answer quality.'}, 'zh': {'title': '优化工具调用，提高效率！', 'desc': '本文提出了一种名为最优工具调用控制策略优化（OTC-PO）的框架，旨在提高大型语言模型（LLMs）在使用外部工具时的效率。通过引入工具集成奖励，该方法同时考虑了答案的正确性和工具使用的效率，从而减少不必要的工具调用。实验结果表明，OTC-PO能够将工具调用次数减少多达73.1%，并提高工具生产力229.4%，同时保持答案的准确性。该研究是首个基于强化学习的框架，专门优化工具使用效率。'}}}, {'id': 'https://huggingface.co/papers/2504.14603', 'title': 'UFO2: The Desktop AgentOS', 'url': 'https://huggingface.co/papers/2504.14603', 'abstract': 'Recent Computer-Using Agents (CUAs), powered by multimodal large language models (LLMs), offer a promising direction for automating complex desktop workflows through natural language. However, most existing CUAs remain conceptual prototypes, hindered by shallow OS integration, fragile screenshot-based interaction, and disruptive execution.   We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs into practical, system-level automation. UFO2 features a centralized HostAgent for task decomposition and coordination, alongside a collection of application-specialized AppAgent equipped with native APIs, domain-specific knowledge, and a unified GUI--API action layer. This architecture enables robust task execution while preserving modularity and extensibility. A hybrid control detection pipeline fuses Windows UI Automation (UIA) with vision-based parsing to support diverse interface styles. Runtime efficiency is further enhanced through speculative multi-action planning, reducing per-step LLM overhead. Finally, a Picture-in-Picture (PiP) interface enables automation within an isolated virtual desktop, allowing agents and users to operate concurrently without interference.   We evaluate UFO2 across over 20 real-world Windows applications, demonstrating substantial improvements in robustness and execution accuracy over prior CUAs. Our results show that deep OS integration unlocks a scalable path toward reliable, user-aligned desktop automation.', 'score': 22, 'issue_id': 3357, 'pub_date': '2025-04-20', 'pub_date_card': {'ru': '20 апреля', 'en': 'April 20', 'zh': '4月20日'}, 'hash': '81eea84c9d10e4d0', 'authors': ['Chaoyun Zhang', 'He Huang', 'Chiming Ni', 'Jian Mu', 'Si Qin', 'Shilin He', 'Lu Wang', 'Fangkai Yang', 'Pu Zhao', 'Chao Du', 'Liqun Li', 'Yu Kang', 'Zhao Jiang', 'Suzhen Zheng', 'Rujia Wang', 'Jiaxu Qian', 'Minghua Ma', 'Jian-Guang Lou', 'Qingwei Lin', 'Saravan Rajmohan', 'Dongmei Zhang'], 'affiliations': ['Microsoft', 'Nanjing University', 'Peking University', 'ZJU-UIUC Institute'], 'pdf_title_img': 'assets/pdf/title_img/2504.14603.jpg', 'data': {'categories': ['#agents', '#architecture', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'UFO2: Надежная автоматизация Windows с помощью мультиагентной системы и глубокой интеграции с ОС', 'desc': 'UFO2 - это многоагентная система для автоматизации рабочего стола Windows, использующая мультимодальные большие языковые модели. Она включает централизованный HostAgent для декомпозиции задач и набор специализированных AppAgent с нативными API и предметными знаниями. UFO2 использует гибридный подход для обнаружения элементов управления, сочетая Windows UI Automation и компьютерное зрение. Система демонстрирует значительное улучшение надежности и точности выполнения задач по сравнению с предыдущими решениями.'}, 'en': {'title': 'UFO2: Elevating Desktop Automation with Intelligent Agents', 'desc': 'The paper introduces UFO2, a multiagent system designed to enhance the functionality of Computer-Using Agents (CUAs) on Windows desktops. It addresses limitations of existing CUAs by integrating a centralized HostAgent for better task management and specialized AppAgents that utilize native APIs for improved interaction. The system employs a hybrid control detection pipeline that combines UI Automation with vision-based techniques, allowing it to handle various interface styles effectively. Evaluation results indicate that UFO2 significantly improves the robustness and accuracy of desktop automation tasks compared to previous models, showcasing the benefits of deep OS integration.'}, 'zh': {'title': 'UFO2：提升桌面自动化的智能代理系统', 'desc': '本文介绍了一种名为UFO2的多代理AgentOS，旨在通过自然语言实现Windows桌面的复杂工作流程自动化。UFO2采用集中式的HostAgent进行任务分解和协调，并配备了应用专用的AppAgent，利用本地API和领域特定知识来增强系统集成。该架构支持强大的任务执行，同时保持模块化和可扩展性，结合了Windows UI自动化和视觉解析技术，以适应多样化的界面风格。通过在20多个真实Windows应用程序中的评估，UFO2在鲁棒性和执行准确性方面显著优于之前的CUA，展示了深度操作系统集成的潜力。'}}}, {'id': 'https://huggingface.co/papers/2504.15281', 'title': 'StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on\n  3D Gaussians', 'url': 'https://huggingface.co/papers/2504.15281', 'abstract': "3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction but struggles with stylized scenarios (e.g., cartoons, games) due to fragmented textures, semantic misalignment, and limited adaptability to abstract aesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer that integrates multi-modal style conditioning, multi-level semantic alignment, and perceptual quality enhancement. Our key insights include: (1) optimizing only RGB attributes preserves geometric integrity during stylization; (2) disentangling low-, medium-, and high-level semantics is critical for coherent style transfer; (3) scalability across isolated objects and complex scenes is essential for practical deployment. StyleMe3D introduces four novel components: Dynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent space for semantic alignment; Contrastive Style Descriptor (CSD) for localized, content-aware texture transfer; Simultaneously Optimized Scale (SOS) to decouple style details and structural coherence; and 3D Gaussian Quality Assessment (3DG-QA), a differentiable aesthetic prior trained on human-rated data to suppress artifacts and enhance visual harmony. Evaluated on NeRF synthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D outperforms state-of-the-art methods in preserving geometric details (e.g., carvings on sculptures) and ensuring stylistic consistency across scenes (e.g., coherent lighting in landscapes), while maintaining real-time rendering. This work bridges photorealistic 3D GS and artistic stylization, unlocking applications in gaming, virtual worlds, and digital art.", 'score': 21, 'issue_id': 3361, 'pub_date': '2025-04-21', 'pub_date_card': {'ru': '21 апреля', 'en': 'April 21', 'zh': '4月21日'}, 'hash': 'f2788379a7878393', 'authors': ['Cailin Zhuang', 'Yaoqi Hu', 'Xuanyang Zhang', 'Wei Cheng', 'Jiacheng Bao', 'Shengqi Liu', 'Yiying Yang', 'Xianfang Zeng', 'Gang Yu', 'Ming Li'], 'affiliations': ['AIGC Research', 'Guangming Laboratory', 'ShanghaiTech University', 'StepFun'], 'pdf_title_img': 'assets/pdf/title_img/2504.15281.jpg', 'data': {'categories': ['#synthetic', '#3d', '#games', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'StyleMe3D: Мост между фотореалистичным 3D и художественной стилизацией', 'desc': 'StyleMe3D - это новый комплексный подход к стилизации 3D-сцен, использующий технологию 3D Gaussian Splatting. Он решает проблемы фрагментированных текстур, семантического несоответствия и ограниченной адаптивности к абстрактной эстетике при стилизации. Метод включает в себя мультимодальное кондиционирование стиля, многоуровневое семантическое выравнивание и улучшение перцептивного качества. StyleMe3D превосходит современные методы в сохранении геометрических деталей и обеспечении стилистической согласованности, сохраняя при этом возможность рендеринга в реальном времени.'}, 'en': {'title': 'Bridging Realism and Artistry in 3D Style Transfer', 'desc': 'The paper introduces StyleMe3D, a framework designed to enhance 3D Gaussian Splatting (3DGS) for style transfer in stylized scenarios like cartoons and games. It addresses challenges such as fragmented textures and semantic misalignment by optimizing RGB attributes to maintain geometric integrity and disentangling different levels of semantics for coherent style application. The framework includes innovative components like Dynamic Style Score Distillation and Contrastive Style Descriptor to improve texture transfer and aesthetic quality. Evaluations show that StyleMe3D outperforms existing methods in preserving details and ensuring stylistic consistency, making it suitable for applications in gaming and digital art.'}, 'zh': {'title': '风格化3D重建的新突破', 'desc': '3D Gaussian Splatting（3DGS）在真实场景重建方面表现出色，但在风格化场景（如卡通和游戏）中面临挑战。我们提出了StyleMe3D，这是一个全面的3D风格转移框架，集成了多模态风格条件、多层次语义对齐和感知质量增强。我们的关键见解包括：优化RGB属性可以在风格化过程中保持几何完整性；解耦低、中、高层次语义对于一致的风格转移至关重要。StyleMe3D引入了四个新组件，显著提升了风格转移的效果，能够在游戏、虚拟世界和数字艺术等领域应用。'}}}, {'id': 'https://huggingface.co/papers/2504.13367', 'title': 'THOUGHTTERMINATOR: Benchmarking, Calibrating, and Mitigating\n  Overthinking in Reasoning Models', 'url': 'https://huggingface.co/papers/2504.13367', 'abstract': "Reasoning models have demonstrated impressive performance on difficult tasks that traditional language models struggle at. However, many are plagued with the problem of overthinking--generating large amounts of unnecessary tokens which don't improve accuracy on a question. We introduce approximate measures of problem-level difficulty and demonstrate that a clear relationship between problem difficulty and optimal token spend exists, and evaluate how well calibrated a variety of reasoning models are in terms of efficiently allocating the optimal token count. We find that in general, reasoning models are poorly calibrated, particularly on easy problems. To evaluate calibration on easy questions we introduce DUMB500, a dataset of extremely easy math, reasoning, code, and task problems, and jointly evaluate reasoning model on these simple examples and extremely difficult examples from existing frontier benchmarks on the same task domain. Finally, we introduce THOUGHTTERMINATOR, a training-free black box decoding technique that significantly improves reasoning model calibration.", 'score': 16, 'issue_id': 3373, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': 'dda6ced06583f735', 'authors': ['Xiao Pu', 'Michael Saxon', 'Wenyue Hua', 'William Yang Wang'], 'affiliations': ['University of California, Santa Barbara'], 'pdf_title_img': 'assets/pdf/title_img/2504.13367.jpg', 'data': {'categories': ['#reasoning', '#math', '#training', '#dataset', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Эффективное рассуждение: как избежать чрезмерного мышления в ИИ', 'desc': 'Статья представляет исследование проблемы чрезмерного мышления у моделей рассуждений в машинном обучении. Авторы вводят меры сложности задач и демонстрируют связь между сложностью проблемы и оптимальным количеством токенов. Они также представляют набор данных DUMB500 для оценки калибровки моделей на простых задачах. В заключение, авторы предлагают метод THOUGHTTERMINATOR для улучшения калибровки моделей рассуждений без дополнительного обучения.'}, 'en': {'title': 'Optimizing Token Use in Reasoning Models', 'desc': 'This paper addresses the issue of reasoning models in machine learning that tend to generate excessive tokens without enhancing accuracy, a phenomenon known as overthinking. The authors propose new measures to assess problem-level difficulty and establish a connection between this difficulty and the optimal number of tokens needed for effective reasoning. They find that many reasoning models are poorly calibrated, especially when dealing with simpler problems, which can lead to inefficient token usage. To tackle this, they introduce a new dataset called DUMB500 for evaluating model performance on easy tasks and present a novel decoding technique, THOUGHTTERMINATOR, that improves the calibration of these models without requiring additional training.'}, 'zh': {'title': '优化推理模型的标定与效率', 'desc': '推理模型在处理传统语言模型难以应对的复杂任务时表现出色。然而，许多模型存在过度思考的问题，生成大量不必要的标记，反而没有提高问题的准确性。我们引入了问题难度的近似度量，并展示了问题难度与最佳标记使用之间的明确关系。研究发现，推理模型的标定普遍较差，尤其是在简单问题上，并提出了DUMB500数据集来评估模型在简单和困难问题上的表现。'}}}, {'id': 'https://huggingface.co/papers/2504.15133', 'title': 'EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language\n  Models', 'url': 'https://huggingface.co/papers/2504.15133', 'abstract': "In this paper, we introduce EasyEdit2, a framework designed to enable plug-and-play adjustability for controlling Large Language Model (LLM) behaviors. EasyEdit2 supports a wide range of test-time interventions, including safety, sentiment, personality, reasoning patterns, factuality, and language features. Unlike its predecessor, EasyEdit2 features a new architecture specifically designed for seamless model steering. It comprises key modules such as the steering vector generator and the steering vector applier, which enable automatic generation and application of steering vectors to influence the model's behavior without modifying its parameters. One of the main advantages of EasyEdit2 is its ease of use-users do not need extensive technical knowledge. With just a single example, they can effectively guide and adjust the model's responses, making precise control both accessible and efficient. Empirically, we report model steering performance across different LLMs, demonstrating the effectiveness of these techniques. We have released the source code on GitHub at https://github.com/zjunlp/EasyEdit along with a demonstration notebook. In addition, we provide a demo video at https://zjunlp.github.io/project/EasyEdit2/video for a quick introduction.", 'score': 15, 'issue_id': 3362, 'pub_date': '2025-04-21', 'pub_date_card': {'ru': '21 апреля', 'en': 'April 21', 'zh': '4月21日'}, 'hash': '145493f7420cf7aa', 'authors': ['Ziwen Xu', 'Shuxun Wang', 'Kewei Xu', 'Haoming Xu', 'Mengru Wang', 'Xinle Deng', 'Yunzhi Yao', 'Guozhou Zheng', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.15133.jpg', 'data': {'categories': ['#training', '#alignment', '#architecture', '#open_source'], 'emoji': '🎛️', 'ru': {'title': 'Точное управление языковыми моделями без сложных настроек', 'desc': 'EasyEdit2 - это фреймворк для гибкого управления поведением больших языковых моделей (LLM). Он поддерживает широкий спектр вмешательств в режиме реального времени, включая безопасность, тональность, личность, паттерны рассуждений и языковые особенности. Новая архитектура EasyEdit2 содержит ключевые модули для автоматической генерации и применения векторов управления, что позволяет влиять на поведение модели без изменения ее параметров. Эмпирически подтверждена эффективность этих техник для различных LLM.'}, 'en': {'title': 'Effortless Control of Language Models with EasyEdit2', 'desc': "EasyEdit2 is a framework that allows users to easily control the behaviors of Large Language Models (LLMs) without needing deep technical skills. It introduces a new architecture that includes modules for generating and applying steering vectors, which help adjust the model's responses in real-time. This framework supports various interventions such as safety, sentiment, and reasoning patterns, making it versatile for different applications. The ease of use is a key feature, as users can guide the model's behavior with just one example, enhancing accessibility and efficiency in model steering."}, 'zh': {'title': '轻松调整大型语言模型的行为', 'desc': '本文介绍了EasyEdit2，这是一个旨在实现大型语言模型（LLM）行为可调节性的框架。EasyEdit2支持多种测试时干预，包括安全性、情感、个性、推理模式、事实性和语言特征。与前身相比，EasyEdit2采用了新架构，专为无缝模型引导设计，包含关键模块如引导向量生成器和引导向量应用器，能够自动生成和应用引导向量，影响模型行为而无需修改其参数。EasyEdit2的一个主要优点是易于使用，用户只需一个示例即可有效引导和调整模型的响应，使精确控制变得简单高效。'}}}, {'id': 'https://huggingface.co/papers/2504.15280', 'title': 'Seeing from Another Perspective: Evaluating Multi-View Understanding in\n  MLLMs', 'url': 'https://huggingface.co/papers/2504.15280', 'abstract': "Multi-view understanding, the ability to reconcile visual information across diverse viewpoints for effective navigation, manipulation, and 3D scene comprehension, is a fundamental challenge in Multi-Modal Large Language Models (MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive advances in high-level reasoning and planning, they frequently fall short when confronted with multi-view geometric consistency and cross-view correspondence. To comprehensively evaluate the challenges of MLLMs in multi-view scene reasoning, we propose All-Angles Bench, a benchmark of over 2,100 human carefully annotated multi-view question-answer pairs across 90 diverse real-world scenes. Our six tasks (counting, attribute identification, relative distance, relative direction, object manipulation, and camera pose estimation) specifically test model's geometric correspondence and the capacity to align information consistently across views. Our extensive experiments, benchmark on 27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and GPT-4o against human evaluators reveals a substantial performance gap, indicating that current MLLMs remain far from human-level proficiency. Through in-depth analysis, we show that MLLMs are particularly underperforming under two aspects: (1) cross-view correspondence for partially occluded views and (2) establishing the coarse camera poses. These findings highlight the necessity of domain-specific refinements or modules that embed stronger multi-view awareness. We believe that our All-Angles Bench offers valuable insights and contribute to bridging the gap between MLLMs and human-level multi-view understanding. The project and benchmark are publicly available at https://danielchyeh.github.io/All-Angles-Bench/.", 'score': 13, 'issue_id': 3358, 'pub_date': '2025-04-21', 'pub_date_card': {'ru': '21 апреля', 'en': 'April 21', 'zh': '4月21日'}, 'hash': '5f2365c2ff2ff7f8', 'authors': ['Chun-Hsiao Yeh', 'Chenyu Wang', 'Shengbang Tong', 'Ta-Ying Cheng', 'Rouyu Wang', 'Tianzhe Chu', 'Yuexiang Zhai', 'Yubei Chen', 'Shenghua Gao', 'Yi Ma'], 'affiliations': ['HKU', 'NYU', 'TranscEngram', 'UC Berkeley', 'UC Davis', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2504.15280.jpg', 'data': {'categories': ['#multimodal', '#3d', '#survey', '#reasoning', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'Новый бенчмарк выявляет пробелы в многоракурсном понимании у языковых моделей', 'desc': 'Статья представляет новый бенчмарк All-Angles Bench для оценки способности мультимодальных языковых моделей (MLLM) к многоракурсному пониманию сцен. Бенчмарк содержит более 2100 вопросов с ответами по 90 реальным сценам, охватывающим задачи подсчета, идентификации атрибутов, относительного расстояния и направления, манипуляции объектами и оценки положения камеры. Эксперименты показали значительное отставание современных MLLM от человеческого уровня в этих задачах. Выявлены особые трудности моделей с установлением соответствий между ракурсами при частичных окклюзиях и определением приблизительного положения камеры.'}, 'en': {'title': 'Bridging the Gap in Multi-View Understanding for MLLMs', 'desc': 'This paper addresses the challenge of multi-view understanding in Multi-Modal Large Language Models (MLLMs), which is crucial for tasks like navigation and scene comprehension. The authors introduce All-Angles Bench, a benchmark consisting of over 2,100 annotated question-answer pairs designed to evaluate MLLMs on their ability to handle geometric consistency and cross-view correspondence. They conduct experiments on 27 MLLMs, revealing a significant performance gap compared to human evaluators, particularly in handling occluded views and estimating camera poses. The findings suggest that enhancing MLLMs with domain-specific modules for better multi-view awareness is essential for improving their performance.'}, 'zh': {'title': '提升多视角理解，缩小人机差距！', 'desc': '多视角理解是多模态大型语言模型（MLLMs）面临的一个基本挑战，涉及在不同视角下协调视觉信息以实现有效导航和3D场景理解。尽管最近的MLLMs在高层次推理和规划方面取得了显著进展，但在多视角几何一致性和视角间对应关系方面仍然存在不足。为全面评估MLLMs在多视角场景推理中的挑战，我们提出了All-Angles Bench，这是一个包含2100多个人工精心标注的多视角问答对的基准测试。我们的实验表明，当前的MLLMs在与人类评估者的比较中表现出显著的性能差距，强调了增强多视角意识的必要性。'}}}, {'id': 'https://huggingface.co/papers/2504.14655', 'title': 'LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient\n  Training of Code LLMs', 'url': 'https://huggingface.co/papers/2504.14655', 'abstract': 'We introduce LeetCodeDataset, a high-quality benchmark for evaluating and training code-generation models, addressing two key challenges in LLM research: the lack of reasoning-focused coding benchmarks and self-contained training testbeds. By curating LeetCode Python problems with rich metadata, broad coverage, 100+ test cases per problem, and temporal splits (pre/post July 2024), our dataset enables contamination-free evaluation and efficient supervised fine-tuning (SFT). Experiments show reasoning models significantly outperform non-reasoning counterparts, while SFT with only 2.6K model-generated solutions achieves performance comparable to 110K-sample counterparts. The dataset and evaluation framework are available on Hugging Face and Github.', 'score': 13, 'issue_id': 3358, 'pub_date': '2025-04-20', 'pub_date_card': {'ru': '20 апреля', 'en': 'April 20', 'zh': '4月20日'}, 'hash': '4ca6f62bd5518a9d', 'authors': ['Yunhui Xia', 'Wei Shen', 'Yan Wang', 'Jason Klein Liu', 'Huifeng Sun', 'Siyue Wu', 'Jian Hu', 'Xiaolong Xu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.14655.jpg', 'data': {'categories': ['#training', '#open_source', '#reasoning', '#data', '#optimization', '#benchmark', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'LeetCodeDataset: Новый стандарт для оценки ИИ в программировании', 'desc': 'LeetCodeDataset представляет собой новый бенчмарк для оценки и обучения моделей генерации кода. Он решает проблемы отсутствия наборов данных, фокусирующихся на рассуждениях, и самодостаточных тестовых сред для обучения. Датасет содержит задачи с LeetCode на Python с богатыми метаданными, широким охватом и большим количеством тестовых примеров. Эксперименты показывают, что модели с рассуждениями значительно превосходят модели без рассуждений, а обучение с учителем на небольшом наборе сгенерированных решений дает результаты, сравнимые с обучением на гораздо больших выборках.'}, 'en': {'title': 'LeetCodeDataset: Elevating Code Generation with Reasoning!', 'desc': 'The paper presents LeetCodeDataset, a new benchmark designed for assessing and training code-generation models in machine learning. It addresses the challenges of lacking reasoning-focused coding benchmarks and the need for self-contained training environments. The dataset includes a variety of curated Python problems with extensive metadata and numerous test cases, allowing for effective supervised fine-tuning. Results indicate that models utilizing reasoning outperform those that do not, and even a small number of model-generated solutions can yield competitive performance.'}, 'zh': {'title': 'LeetCodeDataset：推理驱动的代码生成基准', 'desc': '我们介绍了LeetCodeDataset，这是一个高质量的基准数据集，用于评估和训练代码生成模型，解决了大型语言模型研究中的两个关键挑战：缺乏以推理为重点的编码基准和自包含的训练测试环境。通过整理LeetCode的Python问题，提供丰富的元数据、广泛的覆盖范围、每个问题超过100个测试用例以及时间分割（2024年7月前后），我们的数据集实现了无污染评估和高效的监督微调（SFT）。实验表明，推理模型的表现显著优于非推理模型，而仅使用2.6K个模型生成的解决方案进行SFT的性能与110K样本的模型相当。该数据集和评估框架已在Hugging Face和Github上发布。'}}}, {'id': 'https://huggingface.co/papers/2504.14899', 'title': 'Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls\n  for Video Generation', 'url': 'https://huggingface.co/papers/2504.14899', 'abstract': 'Camera and human motion controls have been extensively studied for video generation, but existing approaches typically address them separately, suffering from limited data with high-quality annotations for both aspects. To overcome this, we present Uni3C, a unified 3D-enhanced framework for precise control of both camera and human motion in video generation. Uni3C includes two key contributions. First, we propose a plug-and-play control module trained with a frozen video generative backbone, PCDController, which utilizes unprojected point clouds from monocular depth to achieve accurate camera control. By leveraging the strong 3D priors of point clouds and the powerful capacities of video foundational models, PCDController shows impressive generalization, performing well regardless of whether the inference backbone is frozen or fine-tuned. This flexibility enables different modules of Uni3C to be trained in specific domains, i.e., either camera control or human motion control, reducing the dependency on jointly annotated data. Second, we propose a jointly aligned 3D world guidance for the inference phase that seamlessly integrates both scenic point clouds and SMPL-X characters to unify the control signals for camera and human motion, respectively. Extensive experiments confirm that PCDController enjoys strong robustness in driving camera motion for fine-tuned backbones of video generation. Uni3C substantially outperforms competitors in both camera controllability and human motion quality. Additionally, we collect tailored validation sets featuring challenging camera movements and human actions to validate the effectiveness of our method.', 'score': 11, 'issue_id': 3364, 'pub_date': '2025-04-21', 'pub_date_card': {'ru': '21 апреля', 'en': 'April 21', 'zh': '4月21日'}, 'hash': '7a4eac2f553066d7', 'authors': ['Chenjie Cao', 'Jingkai Zhou', 'Shikai Li', 'Jingyun Liang', 'Chaohui Yu', 'Fan Wang', 'Xiangyang Xue', 'Yanwei Fu'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Fudan University', 'Hupan Lab'], 'pdf_title_img': 'assets/pdf/title_img/2504.14899.jpg', 'data': {'categories': ['#video', '#3d', '#games'], 'emoji': '🎥', 'ru': {'title': 'Единый контроль камеры и человека в генерации видео с помощью 3D', 'desc': 'Эта статья представляет Uni3C - унифицированную систему для точного контроля движения камеры и человека при генерации видео. Ключевые компоненты включают модуль PCDController, использующий облака точек для управления камерой, и совместно выровненное 3D-руководство для объединения сигналов управления. Система преодолевает ограничения существующих подходов, связанные с нехваткой качественно аннотированных данных. Эксперименты показывают превосходство Uni3C над конкурентами в контроле камеры и качестве движений человека.'}, 'en': {'title': 'Unified Control for Enhanced Video Generation', 'desc': 'The paper introduces Uni3C, a novel framework designed to enhance video generation by simultaneously controlling camera and human motion. It features a plug-and-play control module called PCDController, which utilizes unprojected point clouds from monocular depth to achieve precise camera control. This approach allows for effective training in specific domains, reducing the need for extensive jointly annotated data. The framework demonstrates superior performance in both camera controllability and human motion quality, validated through extensive experiments and tailored validation sets.'}, 'zh': {'title': '统一控制相机与人类动作的创新框架', 'desc': '本论文提出了一种名为Uni3C的统一3D增强框架，用于视频生成中精确控制相机和人类动作。该框架包含两个主要贡献：首先，提出了一种可插拔的控制模块PCDController，利用单目深度生成的未投影点云实现准确的相机控制。其次，提出了一种联合对齐的3D世界引导方法，将场景点云和SMPL-X角色无缝整合，以统一相机和人类动作的控制信号。实验结果表明，Uni3C在相机可控性和人类动作质量方面显著优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2504.14239', 'title': 'InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to\n  Deliberative Reasoners', 'url': 'https://huggingface.co/papers/2504.14239', 'abstract': 'Multimodal Large Language Models (MLLMs) have powered Graphical User Interface (GUI) Agents, showing promise in automating tasks on computing devices. Recent works have begun exploring reasoning in GUI tasks with encouraging results. However, many current approaches rely on manually designed reasoning templates, which may result in reasoning that is not sufficiently robust and adaptive for complex GUI environments. Meanwhile, some existing agents continue to operate as Reactive Actors, relying primarily on implicit reasoning that may lack sufficient depth for GUI tasks demanding planning and error recovery. We argue that advancing these agents requires a shift from reactive acting towards acting based on deliberate reasoning. To facilitate this transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed through our Actor2Reasoner framework, a reasoning-centric, two-stage training approach designed to progressively evolve agents from Reactive Actors to Deliberative Reasoners. The first stage, Reasoning Injection, focuses on establishing a basic reasoner. We employ Spatial Reasoning Distillation to transfer cross-modal spatial reasoning capabilities from teacher models to MLLMs through trajectories with explicit reasoning steps, enabling models to integrate GUI visual-spatial information with logical reasoning before action generation. The second stage, Deliberation Enhancement, refines the basic reasoner into a deliberative one using Reinforcement Learning. This stage introduces two approaches: Sub-goal Guidance, which rewards models for generating accurate intermediate sub-goals, and Error Recovery Scenario Construction, which creates failure-and-recovery training scenarios from identified prone-to-error steps. Experimental results show InfiGUI-R1 achieves strong performance in GUI grounding and trajectory tasks. Resources at https://github.com/Reallm-Labs/InfiGUI-R1.', 'score': 11, 'issue_id': 3359, 'pub_date': '2025-04-19', 'pub_date_card': {'ru': '19 апреля', 'en': 'April 19', 'zh': '4月19日'}, 'hash': '74a4065180d1bcb9', 'authors': ['Yuhang Liu', 'Pengxiang Li', 'Congkai Xie', 'Xavier Hu', 'Xiaotian Han', 'Shengyu Zhang', 'Hongxia Yang', 'Fei Wu'], 'affiliations': ['Dalian University of Technology', 'Reallm Labs', 'The Hong Kong Polytechnic University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.14239.jpg', 'data': {'categories': ['#games', '#reasoning', '#multimodal', '#rl', '#agents', '#training'], 'emoji': '🤖', 'ru': {'title': 'От реактивных актёров к обдумывающим рассуждателям в GUI-агентах', 'desc': "Исследователи представили новый подход к созданию агентов графического пользовательского интерфейса (GUI) на основе мультимодальных больших языковых моделей (MLLM). Они разработали фреймворк Actor2Reasoner, который трансформирует реактивных агентов в делиберативных рассуждающих агентов через двухэтапный процесс обучения. Первый этап, 'Внедрение рассуждений', использует пространственную дистилляцию для передачи способностей к рассуждениям от учительских моделей. Второй этап, 'Улучшение обдумывания', применяет обучение с подкреплением для усовершенствования способностей агента к планированию и восстановлению после ошибок."}, 'en': {'title': 'From Reactive to Deliberative: Advancing GUI Agents with InfiGUI-R1', 'desc': "This paper presents InfiGUI-R1, a Multimodal Large Language Model (MLLM) designed to enhance Graphical User Interface (GUI) agents by shifting from reactive to deliberative reasoning. The proposed Actor2Reasoner framework employs a two-stage training process: the first stage focuses on Reasoning Injection, where spatial reasoning capabilities are transferred to the MLLM, allowing it to better understand GUI visual-spatial information. The second stage, Deliberation Enhancement, uses Reinforcement Learning to refine the agent's reasoning abilities by rewarding the generation of accurate sub-goals and constructing training scenarios for error recovery. Experimental results demonstrate that InfiGUI-R1 significantly improves performance in tasks involving GUI grounding and trajectory planning."}, 'zh': {'title': '从反应到深思熟虑：提升GUI代理的推理能力', 'desc': '多模态大型语言模型（MLLMs）为图形用户界面（GUI）代理提供了动力，展现了在计算设备上自动化任务的潜力。当前许多方法依赖于手动设计的推理模板，这可能导致在复杂的GUI环境中推理不够稳健和适应。我们提出InfiGUI-R1，这是一个基于MLLM的GUI代理，通过Actor2Reasoner框架开发，旨在将代理从反应型行为者转变为深思熟虑的推理者。该方法包括两个阶段：推理注入和深思熟虑增强，实验结果表明InfiGUI-R1在GUI任务中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2504.13805', 'title': 'LearnAct: Few-Shot Mobile GUI Agent with a Unified Demonstration\n  Benchmark', 'url': 'https://huggingface.co/papers/2504.13805', 'abstract': "Mobile GUI agents show promise in automating tasks but face generalization challenges in diverse real-world scenarios. Traditional approaches using pre-training or fine-tuning with massive datasets struggle with the diversity of mobile applications and user-specific tasks. We propose enhancing mobile GUI agent capabilities through human demonstrations, focusing on improving performance in unseen scenarios rather than pursuing universal generalization through larger datasets. To realize this paradigm, we introduce LearnGUI, the first comprehensive dataset specifically designed for studying demonstration-based learning in mobile GUI agents, comprising 2,252 offline tasks and 101 online tasks with high-quality human demonstrations. We further develop LearnAct, a sophisticated multi-agent framework that automatically extracts knowledge from demonstrations to enhance task completion. This framework integrates three specialized agents: DemoParser for knowledge extraction, KnowSeeker for relevant knowledge retrieval, and ActExecutor for demonstration-enhanced task execution. Our experimental results show significant performance gains in both offline and online evaluations. In offline assessments, a single demonstration improves model performance, increasing Gemini-1.5-Pro's accuracy from 19.3% to 51.7%. In online evaluations, our framework enhances UI-TARS-7B-SFT's task success rate from 18.1% to 32.8%. LearnAct framework and LearnGUI benchmark establish demonstration-based learning as a promising direction for more adaptable, personalized, and deployable mobile GUI agents.", 'score': 8, 'issue_id': 3358, 'pub_date': '2025-04-18', 'pub_date_card': {'ru': '18 апреля', 'en': 'April 18', 'zh': '4月18日'}, 'hash': '56c81be35c66aa14', 'authors': ['Guangyi Liu', 'Pengxiang Zhao', 'Liang Liu', 'Zhiming Chen', 'Yuxiang Chai', 'Shuai Ren', 'Hao Wang', 'Shibo He', 'Wenchao Meng'], 'affiliations': ['Zhejiang University Hangzhou, China', 'vivo AI Lab Hangzhou, China', 'vivo AI Lab ShenZhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.13805.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#transfer_learning', '#agents'], 'emoji': '📱', 'ru': {'title': 'Обучение ИИ-агентов мобильных интерфейсов на демонстрациях пользователей', 'desc': 'Статья представляет новый подход к обучению агентов графического интерфейса мобильных устройств на основе демонстраций пользователей. Авторы предлагают датасет LearnGUI и фреймворк LearnAct, состоящий из трех специализированных агентов для извлечения знаний из демонстраций. Эксперименты показывают значительное улучшение производительности моделей как в офлайн, так и в онлайн оценках. Предложенный подход позволяет создавать более адаптивных и персонализированных агентов для мобильных интерфейсов.'}, 'en': {'title': 'Empowering Mobile GUI Agents with Human Demonstrations', 'desc': 'This paper addresses the challenges faced by mobile GUI agents in generalizing across diverse real-world applications. It critiques traditional methods that rely on large datasets for pre-training or fine-tuning, which often fall short in user-specific tasks. The authors propose a novel approach that leverages human demonstrations to improve the performance of these agents in unseen scenarios. They introduce LearnGUI, a dataset for demonstration-based learning, and LearnAct, a multi-agent framework that enhances task execution by extracting and utilizing knowledge from these demonstrations, leading to significant performance improvements in both offline and online tasks.'}, 'zh': {'title': '基于示范学习的移动GUI代理新方向', 'desc': '移动图形用户界面（GUI）代理在自动化任务方面展现出潜力，但在多样化的现实场景中面临泛化挑战。传统方法依赖于使用大规模数据集进行预训练或微调，难以应对移动应用和用户特定任务的多样性。我们提出通过人类示范来增强移动GUI代理的能力，重点改善在未见场景中的表现，而不是通过更大的数据集追求普遍泛化。为实现这一目标，我们引入了LearnGUI，这是第一个专门设计用于研究基于示范学习的移动GUI代理的综合数据集，包含2252个离线任务和101个在线任务，配有高质量的人类示范。'}}}, {'id': 'https://huggingface.co/papers/2504.08902', 'title': 'LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping', 'url': 'https://huggingface.co/papers/2504.08902', 'abstract': 'Anamorphosis refers to a category of images that are intentionally distorted, making them unrecognizable when viewed directly. Their true form only reveals itself when seen from a specific viewpoint, which can be through some catadioptric device like a mirror or a lens. While the construction of these mathematical devices can be traced back to as early as the 17th century, they are only interpretable when viewed from a specific vantage point and tend to lose meaning when seen normally. In this paper, we revisit these famous optical illusions with a generative twist. With the help of latent rectified flow models, we propose a method to create anamorphic images that still retain a valid interpretation when viewed directly. To this end, we introduce Laplacian Pyramid Warping, a frequency-aware image warping technique key to generating high-quality visuals. Our work extends Visual Anagrams (arXiv:2311.17919) to latent space models and to a wider range of spatial transforms, enabling the creation of novel generative perceptual illusions.', 'score': 7, 'issue_id': 3362, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 апреля', 'en': 'April 11', 'zh': '4月11日'}, 'hash': '3ff884593f7767fe', 'authors': ['Pascal Chang', 'Sergio Sancho', 'Jingwei Tang', 'Markus Gross', 'Vinicius C. Azevedo'], 'affiliations': ['Disney Research Studios', 'ETH Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2504.08902.jpg', 'data': {'categories': ['#cv', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Генеративные анаморфозы: новый взгляд на оптические иллюзии', 'desc': 'Статья представляет новый метод создания анаморфных изображений с использованием генеративных моделей. Авторы применяют модели латентного выпрямленного потока и технику частотно-зависимого искажения изображений, названную Laplacian Pyramid Warping. Это позволяет создавать изображения, которые имеют осмысленную интерпретацию как при прямом просмотре, так и при просмотре с определенного ракурса. Работа расширяет концепцию визуальных анаграмм на модели латентного пространства и более широкий спектр пространственных преобразований.'}, 'en': {'title': 'Revealing Hidden Forms: Generating Anamorphic Images with Latent Models', 'desc': 'This paper explores the concept of anamorphosis, which involves creating images that appear distorted until viewed from a specific angle. The authors introduce a novel approach using latent rectified flow models to generate these images, ensuring they maintain a coherent interpretation even when viewed directly. A key technique presented is Laplacian Pyramid Warping, which allows for frequency-aware image manipulation to produce high-quality visuals. This work builds on previous research in generative models, expanding the possibilities for creating unique visual illusions in latent space.'}, 'zh': {'title': '生成可识别的变形图像新方法', 'desc': '本论文探讨了变形图像（Anamorphosis）的概念，这种图像在直接观看时会失去可识别性，只有从特定视角才能看出其真实形态。我们提出了一种新方法，利用潜在修正流模型生成变形图像，使其在直接观看时仍然具有有效的解释。为此，我们引入了拉普拉斯金字塔变形技术，这是一种关注频率的图像变形方法，能够生成高质量的视觉效果。我们的研究扩展了视觉字谜的应用，适用于潜在空间模型和更广泛的空间变换，创造出新颖的生成感知幻觉。'}}}, {'id': 'https://huggingface.co/papers/2504.15270', 'title': 'An LMM for Efficient Video Understanding via Reinforced Compression of\n  Video Cubes', 'url': 'https://huggingface.co/papers/2504.15270', 'abstract': 'Large Multimodal Models (LMMs) uniformly perceive video frames, creating computational inefficiency for videos with inherently varying temporal information density. This paper present Quicksviewer, an LMM with new perceiving paradigm that partitions a video of nonuniform density into varying cubes using Gumbel Softmax, followed by a unified resampling for each cube to achieve efficient video understanding. This simple and intuitive approach dynamically compress video online based on its temporal density, significantly reducing spatiotemporal redundancy (overall 45times compression rate), while enabling efficient training with large receptive field. We train the model from a language backbone through three progressive stages, each incorporating lengthy videos on average of 420s/1fps thanks to the perceiving efficiency. With only 0.8M total video-text samples for training, our model outperforms the direct baseline employing a fixed partitioning strategy by a maximum of 8.72 in accuracy, demonstrating the effectiveness in performance. On Video-MME, Quicksviewer achieves SOTA under modest sequence lengths using just up to 5\\% of tokens per frame required by baselines. With this paradigm, scaling up the number of input frames reveals a clear power law of the model capabilities. It is also empirically verified that the segments generated by the cubing network can help for analyzing continuous events in videos.', 'score': 6, 'issue_id': 3363, 'pub_date': '2025-04-21', 'pub_date_card': {'ru': '21 апреля', 'en': 'April 21', 'zh': '4月21日'}, 'hash': 'd2b8d9930be8508e', 'authors': ['Ji Qi', 'Yuan Yao', 'Yushi Bai', 'Bin Xu', 'Juanzi Li', 'Zhiyuan Liu', 'Tat-Seng Chua'], 'affiliations': ['National University of Singapore', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.15270.jpg', 'data': {'categories': ['#video', '#training', '#multimodal', '#optimization'], 'emoji': '🎬', 'ru': {'title': 'Умное сжатие видео для эффективного машинного обучения', 'desc': 'Статья представляет Quicksviewer - новую мультимодальную модель для эффективного анализа видео. Модель использует алгоритм Gumbel Softmax для разбиения видео на кубы переменного размера в зависимости от плотности информации. Это позволяет значительно сократить избыточность данных (в 45 раз) и обучать модель на длинных видео. Quicksviewer превосходит базовые модели по точности и эффективности, достигая state-of-the-art результатов на бенчмарке Video-MME.'}, 'en': {'title': 'Efficient Video Understanding with Dynamic Cubes', 'desc': 'This paper introduces Quicksviewer, a Large Multimodal Model (LMM) designed to efficiently process videos with varying temporal information density. By using a novel approach that partitions videos into dynamic cubes with Gumbel Softmax, it reduces computational redundancy and achieves a remarkable 45 times compression rate. The model is trained progressively on lengthy videos, allowing it to outperform traditional fixed partitioning methods by improving accuracy significantly. Quicksviewer demonstrates state-of-the-art performance on Video-MME while requiring only a fraction of the tokens typically needed, showcasing its efficiency and effectiveness in video understanding.'}, 'zh': {'title': 'Quicksviewer：高效视频理解的新方法', 'desc': '本文介绍了一种新的大规模多模态模型Quicksviewer，该模型通过Gumbel Softmax将视频分割成不同密度的立方体，从而提高视频理解的效率。该方法能够根据视频的时间密度动态压缩视频，显著减少时空冗余，压缩率达到45倍。模型通过语言基础进行训练，能够处理平均时长为420秒、帧率为1fps的视频，且仅需0.8M的视频-文本样本即可超越固定分区策略的基线。Quicksviewer在Video-MME上表现出色，使用的token数量仅为基线的5%，显示出其在处理视频时的高效性和准确性。'}}}, {'id': 'https://huggingface.co/papers/2504.15217', 'title': 'DRAGON: Distributional Rewards Optimize Diffusion Generative Models', 'url': 'https://huggingface.co/papers/2504.15217', 'abstract': 'We present Distributional RewArds for Generative OptimizatioN (DRAGON), a versatile framework for fine-tuning media generation models towards a desired outcome. Compared with traditional reinforcement learning with human feedback (RLHF) or pairwise preference approaches such as direct preference optimization (DPO), DRAGON is more flexible. It can optimize reward functions that evaluate either individual examples or distributions of them, making it compatible with a broad spectrum of instance-wise, instance-to-distribution, and distribution-to-distribution rewards. Leveraging this versatility, we construct novel reward functions by selecting an encoder and a set of reference examples to create an exemplar distribution. When cross-modality encoders such as CLAP are used, the reference examples may be of a different modality (e.g., text versus audio). Then, DRAGON gathers online and on-policy generations, scores them to construct a positive demonstration set and a negative set, and leverages the contrast between the two sets to maximize the reward. For evaluation, we fine-tune an audio-domain text-to-music diffusion model with 20 different reward functions, including a custom music aesthetics model, CLAP score, Vendi diversity, and Frechet audio distance (FAD). We further compare instance-wise (per-song) and full-dataset FAD settings while ablating multiple FAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an 81.45% average win rate. Moreover, reward functions based on exemplar sets indeed enhance generations and are comparable to model-based rewards. With an appropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality win rate without training on human preference annotations. As such, DRAGON exhibits a new approach to designing and optimizing reward functions for improving human-perceived quality. Sound examples at https://ml-dragon.github.io/web.', 'score': 5, 'issue_id': 3359, 'pub_date': '2025-04-21', 'pub_date_card': {'ru': '21 апреля', 'en': 'April 21', 'zh': '4月21日'}, 'hash': '31ab92756cb919ff', 'authors': ['Yatong Bai', 'Jonah Casebeer', 'Somayeh Sojoudi', 'Nicholas J. Bryan'], 'affiliations': ['Adobe Research', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2504.15217.jpg', 'data': {'categories': ['#audio', '#rlhf', '#diffusion', '#optimization', '#training'], 'emoji': '🐉', 'ru': {'title': 'DRAGON: гибкая оптимизация генеративных моделей без обратной связи от человека', 'desc': 'DRAGON - это новый метод для тонкой настройки генеративных моделей в области медиа. В отличие от традиционных подходов, DRAGON может оптимизировать функции вознаграждения, оценивающие как отдельные примеры, так и их распределения. Метод использует онлайн-генерации для создания наборов положительных и отрицательных примеров, максимизируя разницу между ними. DRAGON был протестирован на 20 различных функциях вознаграждения для модели генерации музыки, показав высокую эффективность без использования аннотаций человеческих предпочтений.'}, 'en': {'title': 'DRAGON: Revolutionizing Reward Optimization for Media Generation', 'desc': 'The paper introduces DRAGON, a framework designed to enhance media generation models by fine-tuning them with flexible reward functions. Unlike traditional methods like reinforcement learning with human feedback, DRAGON can optimize rewards based on individual examples or distributions, allowing for a wider range of applications. It constructs novel reward functions using encoders and reference examples, which can come from different modalities, such as text and audio. The results show that DRAGON significantly improves generation quality, achieving high win rates in both human evaluations and various reward settings without relying on human preference annotations.'}, 'zh': {'title': 'DRAGON：灵活优化生成模型的奖励框架', 'desc': '本文介绍了一种名为DRAGON的框架，用于微调媒体生成模型以实现期望的结果。与传统的基于人类反馈的强化学习（RLHF）或成对偏好方法相比，DRAGON更加灵活，能够优化评估单个示例或其分布的奖励函数。通过选择编码器和参考示例集，DRAGON构建了新颖的奖励函数，并利用在线生成的对比集来最大化奖励。实验结果表明，DRAGON在多种奖励函数下表现优异，显著提升了生成内容的质量。'}}}, {'id': 'https://huggingface.co/papers/2504.13941', 'title': 'NEMOTRON-CROSSTHINK: Scaling Self-Learning beyond Math Reasoning', 'url': 'https://huggingface.co/papers/2504.13941', 'abstract': 'Large Language Models (LLMs) have shown strong reasoning capabilities, particularly when enhanced through Reinforcement Learning (RL). While prior work has successfully applied RL to mathematical reasoning -- where rules and correctness are well-defined -- generalizing these methods to broader reasoning domains remains challenging due to limited data, the lack of verifiable reward structures, and diverse task requirements. In this work, we propose NEMOTRON-CROSSTHINK, a framework that systematically incorporates multi-domain corpora, including both synthetic and real-world question-answer pairs, into RL training to improve generalization across diverse reasoning tasks. NEMOTRON-CROSSTHINK addresses key challenges by (1) incorporating data from varied sources spanning STEM, humanities, social sciences, etc.; (2) applying structured templates (e.g., multiple-choice and open-ended) to control answer-space complexity; (3) filtering for verifiable answers; and (4) optimizing data blending strategies that utilizes data from multiple sources effectively. Our approach enables scalable and verifiable reward modeling beyond mathematics and demonstrates improved accuracies on both math (MATH-500: +30.1%, AMC23:+27.5%) and non-math reasoning benchmarks (MMLU-PRO: +12.8%, GPQA-DIAMOND: +11.3%, AGIEVAL: +15.1%, SUPERGPQA: +3.8%). Moreover, NEMOTRON-CROSSTHINK exhibits significantly improved response efficiency -- using 28% fewer tokens for correct answers -- highlighting more focused and effective reasoning. Through NEMOTRON-CROSSTHINK, we demonstrate that integrating multi-domain, multi-format data in RL leads to more accurate, efficient, and generalizable LLMs.', 'score': 5, 'issue_id': 3357, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': 'df91b810ea243fdc', 'authors': ['Syeda Nahida Akter', 'Shrimai Prabhumoye', 'Matvei Novikov', 'Seungju Han', 'Ying Lin', 'Evelina Bakhturi', 'Eric Nyberg', 'Yejin Choi', 'Mostofa Patwary', 'Mohammad Shoeybi', 'Bryan Catanzaro'], 'affiliations': ['Boston University', 'Carnegie Mellon University', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2504.13941.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#data', '#rl', '#transfer_learning', '#math'], 'emoji': '🧠', 'ru': {'title': 'Многодоменное обучение с подкреплением для улучшения рассуждений ИИ', 'desc': 'Статья представляет NEMOTRON-CROSSTHINK - фреймворк для улучшения способностей крупных языковых моделей к рассуждению с помощью обучения с подкреплением. Авторы используют разнообразные наборы данных из различных областей знаний и применяют структурированные шаблоны для контроля сложности пространства ответов. Подход демонстрирует улучшение точности как на математических, так и на нематематических тестах рассуждений. NEMOTRON-CROSSTHINK также показывает повышенную эффективность ответов, используя меньше токенов для правильных ответов.'}, 'en': {'title': 'Enhancing LLM Reasoning with Multi-Domain Reinforcement Learning', 'desc': 'This paper introduces NEMOTRON-CROSSTHINK, a new framework that enhances the reasoning abilities of Large Language Models (LLMs) using Reinforcement Learning (RL). It tackles the challenge of generalizing RL methods across various reasoning tasks by incorporating diverse datasets from multiple domains, including STEM and humanities. The framework employs structured templates to manage answer complexity and ensures the use of verifiable answers, leading to improved reward modeling. As a result, NEMOTRON-CROSSTHINK achieves significant accuracy gains on both mathematical and non-mathematical reasoning tasks while also improving response efficiency by reducing token usage.'}, 'zh': {'title': '多领域数据助力推理能力提升', 'desc': '大型语言模型（LLMs）在推理能力方面表现出色，特别是通过强化学习（RL）进行增强。以往的研究成功地将RL应用于数学推理，但将这些方法推广到更广泛的推理领域仍然面临挑战。本文提出了NEMOTRON-CROSSTHINK框架，系统地将多领域语料库纳入RL训练，以提高在不同推理任务中的泛化能力。该方法通过多样化数据源、结构化模板、可验证答案过滤和优化数据混合策略，显著提升了模型在数学和非数学推理基准上的准确性和响应效率。'}}}, {'id': 'https://huggingface.co/papers/2504.15047', 'title': 'RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary\n  Quality-Diversity Search', 'url': 'https://huggingface.co/papers/2504.15047', 'abstract': 'Large Language Models (LLMs) exhibit remarkable capabilities but are susceptible to adversarial prompts that exploit vulnerabilities to produce unsafe or biased outputs. Existing red-teaming methods often face scalability challenges, resource-intensive requirements, or limited diversity in attack strategies. We propose RainbowPlus, a novel red-teaming framework rooted in evolutionary computation, enhancing adversarial prompt generation through an adaptive quality-diversity (QD) search that extends classical evolutionary algorithms like MAP-Elites with innovations tailored for language models. By employing a multi-element archive to store diverse high-quality prompts and a comprehensive fitness function to evaluate multiple prompts concurrently, RainbowPlus overcomes the constraints of single-prompt archives and pairwise comparisons in prior QD methods like Rainbow Teaming. Experiments comparing RainbowPlus to QD methods across six benchmark datasets and four open-source LLMs demonstrate superior attack success rate (ASR) and diversity (Diverse-Score approx 0.84), generating up to 100 times more unique prompts (e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine state-of-the-art methods on the HarmBench dataset with twelve LLMs (ten open-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%, surpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours). Our open-source implementation fosters further advancements in LLM safety, offering a scalable tool for vulnerability assessment. Code and resources are publicly available at https://github.com/knoveleng/rainbowplus, supporting reproducibility and future research in LLM red-teaming.', 'score': 4, 'issue_id': 3357, 'pub_date': '2025-04-21', 'pub_date_card': {'ru': '21 апреля', 'en': 'April 21', 'zh': '4月21日'}, 'hash': '834479c504a9e5f7', 'authors': ['Quy-Anh Dang', 'Chris Ngo', 'Truong-Son Hy'], 'affiliations': ['Knovel Engineering Lab, Singapore', 'University of Alabama at Birmingham, United States', 'VNU University of Science, Vietnam'], 'pdf_title_img': 'assets/pdf/title_img/2504.15047.jpg', 'data': {'categories': ['#security', '#benchmark', '#data', '#open_source', '#dataset'], 'emoji': '🌈', 'ru': {'title': 'RainbowPlus: Эволюционный подход к повышению безопасности языковых моделей', 'desc': 'Статья представляет RainbowPlus - новый фреймворк для тестирования безопасности крупных языковых моделей (LLM), основанный на эволюционных вычислениях. RainbowPlus использует адаптивный поиск качества-разнообразия для генерации разнообразных высококачественных провокационных запросов, преодолевая ограничения существующих методов. Эксперименты показывают, что RainbowPlus превосходит современные методы по успешности атак и разнообразию генерируемых запросов на различных наборах данных и моделях. Фреймворк предоставляет масштабируемый инструмент для оценки уязвимостей LLM, способствуя повышению их безопасности.'}, 'en': {'title': 'RainbowPlus: Evolving Safer Language Models with Diverse Adversarial Prompts', 'desc': 'This paper introduces RainbowPlus, a new framework for testing the safety of Large Language Models (LLMs) against adversarial prompts. It uses evolutionary computation techniques to generate diverse and high-quality prompts that can exploit vulnerabilities in LLMs. By employing a multi-element archive and a comprehensive fitness function, RainbowPlus significantly improves the efficiency and effectiveness of prompt generation compared to previous methods. Experiments show that it achieves a higher attack success rate and generates many more unique prompts, making it a valuable tool for enhancing LLM safety.'}, 'zh': {'title': 'RainbowPlus：提升大型语言模型安全性的创新红队框架', 'desc': '大型语言模型（LLMs）具有出色的能力，但容易受到对抗性提示的影响，这些提示利用了模型的脆弱性，导致不安全或有偏见的输出。现有的红队方法常常面临可扩展性挑战、资源密集型要求或攻击策略的多样性有限。我们提出了RainbowPlus，这是一种基于进化计算的新型红队框架，通过自适应质量多样性（QD）搜索增强对抗性提示生成，扩展了经典的进化算法。实验结果表明，RainbowPlus在攻击成功率和多样性方面优于现有方法，生成的独特提示数量显著增加，展示了其在大型语言模型安全性评估中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2504.14717', 'title': 'TAPIP3D: Tracking Any Point in Persistent 3D Geometry', 'url': 'https://huggingface.co/papers/2504.14717', 'abstract': 'We introduce TAPIP3D, a novel approach for long-term 3D point tracking in monocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized spatio-temporal feature clouds, leveraging depth and camera motion information to lift 2D video features into a 3D world space where camera motion is effectively canceled. TAPIP3D iteratively refines multi-frame 3D motion estimates within this stabilized representation, enabling robust tracking over extended periods. To manage the inherent irregularities of 3D point distributions, we propose a Local Pair Attention mechanism. This 3D contextualization strategy effectively exploits spatial relationships in 3D, forming informative feature neighborhoods for precise 3D trajectory estimation. Our 3D-centric approach significantly outperforms existing 3D point tracking methods and even enhances 2D tracking accuracy compared to conventional 2D pixel trackers when accurate depth is available. It supports inference in both camera coordinates (i.e., unstabilized) and world coordinates, and our results demonstrate that compensating for camera motion improves tracking performance. Our approach replaces the conventional 2D square correlation neighborhoods used in prior 2D and 3D trackers, leading to more robust and accurate results across various 3D point tracking benchmarks. Project Page: https://tapip3d.github.io', 'score': 4, 'issue_id': 3360, 'pub_date': '2025-04-20', 'pub_date_card': {'ru': '20 апреля', 'en': 'April 20', 'zh': '4月20日'}, 'hash': 'b8ab0510eb563950', 'authors': ['Bowei Zhang', 'Lei Ke', 'Adam W. Harley', 'Katerina Fragkiadaki'], 'affiliations': ['Carnegie Mellon University', 'Peking University', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2504.14717.jpg', 'data': {'categories': ['#benchmark', '#3d', '#long_context'], 'emoji': '🎥', 'ru': {'title': 'Революционное 3D отслеживание точек в видео с помощью стабилизированных облаков признаков', 'desc': 'TAPIP3D - это новый метод долгосрочного отслеживания 3D точек в монокулярных RGB и RGB-D видео. Он представляет видео как стабилизированные по камере пространственно-временные облака признаков, используя информацию о глубине и движении камеры. TAPIP3D итеративно уточняет оценки 3D движения в нескольких кадрах, что позволяет надежно отслеживать объекты в течение длительного времени. Метод использует механизм локального парного внимания для эффективной контекстуализации в 3D пространстве.'}, 'en': {'title': 'Revolutionizing 3D Point Tracking with TAPIP3D', 'desc': 'TAPIP3D is a new method designed for tracking 3D points over long periods using monocular RGB and RGB-D videos. It transforms videos into stabilized spatio-temporal feature clouds, which helps to eliminate the effects of camera motion. The method refines 3D motion estimates through a Local Pair Attention mechanism, allowing for better handling of irregular 3D point distributions. TAPIP3D not only improves the accuracy of 3D tracking but also enhances 2D tracking when depth information is available, outperforming traditional tracking methods.'}, 'zh': {'title': 'TAPIP3D：提升3D点跟踪的创新方法', 'desc': '我们提出了一种新方法TAPIP3D，用于在单目RGB和RGB-D视频中进行长期3D点跟踪。TAPIP3D通过深度和相机运动信息，将视频表示为稳定的时空特征云，从而将2D视频特征提升到3D世界空间。该方法通过局部对注意力机制处理3D点分布的不规则性，有效利用3D空间关系，形成信息丰富的特征邻域，以实现精确的3D轨迹估计。我们的3D中心方法在多个3D点跟踪基准测试中显著优于现有方法，并在准确深度可用时提高了2D跟踪精度。'}}}, {'id': 'https://huggingface.co/papers/2504.13099', 'title': 'RF-DETR Object Detection vs YOLOv12 : A Study of Transformer-based and\n  CNN-based Architectures for Single-Class and Multi-Class Greenfruit Detection\n  in Complex Orchard Environments Under Label Ambiguity', 'url': 'https://huggingface.co/papers/2504.13099', 'abstract': "This study conducts a detailed comparison of RF-DETR object detection base model and YOLOv12 object detection model configurations for detecting greenfruits in a complex orchard environment marked by label ambiguity, occlusions, and background blending. A custom dataset was developed featuring both single-class (greenfruit) and multi-class (occluded and non-occluded greenfruits) annotations to assess model performance under dynamic real-world conditions. RF-DETR object detection model, utilizing a DINOv2 backbone and deformable attention, excelled in global context modeling, effectively identifying partially occluded or ambiguous greenfruits. In contrast, YOLOv12 leveraged CNN-based attention for enhanced local feature extraction, optimizing it for computational efficiency and edge deployment. RF-DETR achieved the highest mean Average Precision (mAP50) of 0.9464 in single-class detection, proving its superior ability to localize greenfruits in cluttered scenes. Although YOLOv12N recorded the highest mAP@50:95 of 0.7620, RF-DETR consistently outperformed in complex spatial scenarios. For multi-class detection, RF-DETR led with an mAP@50 of 0.8298, showing its capability to differentiate between occluded and non-occluded fruits, while YOLOv12L scored highest in mAP@50:95 with 0.6622, indicating better classification in detailed occlusion contexts. Training dynamics analysis highlighted RF-DETR's swift convergence, particularly in single-class settings where it plateaued within 10 epochs, demonstrating the efficiency of transformer-based architectures in adapting to dynamic visual data. These findings validate RF-DETR's effectiveness for precision agricultural applications, with YOLOv12 suited for fast-response scenarios. >Index Terms: RF-DETR object detection, YOLOv12, YOLOv13, YOLOv14, YOLOv15, YOLOE, YOLO World, YOLO, You Only Look Once, Roboflow, Detection Transformers, CNNs", 'score': 3, 'issue_id': 3370, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': '8ae55836cd55196b', 'authors': ['Ranjan Sapkota', 'Rahul Harsha Cheppally', 'Ajay Sharda', 'Manoj Karkee'], 'affiliations': ['Biological & Environmental Engineering, Cornell University, Ithaca, 14850, NY, USA', 'Department of Biological and Agricultural Engineering, Kansas State University, Manhattan, 66502, KS, USA'], 'pdf_title_img': 'assets/pdf/title_img/2504.13099.jpg', 'data': {'categories': ['#dataset', '#cv', '#training'], 'emoji': '🍏', 'ru': {'title': 'Трансформеры побеждают в обнаружении незрелых фруктов', 'desc': 'Исследование сравнивает модели обнаружения объектов RF-DETR и YOLOv12 для детектирования незрелых фруктов в сложных условиях сада. RF-DETR, использующая трансформерную архитектуру, показала превосходные результаты в обнаружении частично скрытых фруктов, достигнув наивысшего значения mAP50 0,9464 для одноклассовой задачи. YOLOv12, основанная на сверточных нейронных сетях, продемонстрировала высокую вычислительную эффективность и лучшие результаты в детальной классификации. Анализ динамики обучения показал быструю сходимость RF-DETR, особенно в одноклассовых сценариях.'}, 'en': {'title': 'RF-DETR: The Champion of Greenfruit Detection in Complex Orchards', 'desc': 'This paper compares two object detection models, RF-DETR and YOLOv12, for identifying greenfruits in challenging orchard environments. A custom dataset was created to evaluate their performance on both single-class and multi-class detection tasks, focusing on issues like occlusions and background blending. RF-DETR, with its DINOv2 backbone and deformable attention, showed superior performance in detecting partially obscured fruits, achieving the highest mean Average Precision (mAP50) of 0.9464. In contrast, YOLOv12 excelled in computational efficiency and local feature extraction, making it suitable for quick deployments, but RF-DETR consistently outperformed it in complex scenarios.'}, 'zh': {'title': 'RF-DETR：复杂环境中的绿色水果检测新标杆', 'desc': '本研究详细比较了RF-DETR和YOLOv12两种目标检测模型在复杂果园环境中检测绿色水果的性能。我们开发了一个自定义数据集，包含单类和多类标注，以评估模型在动态现实条件下的表现。RF-DETR模型在全局上下文建模方面表现优异，能够有效识别部分遮挡或模糊的绿色水果，而YOLOv12则通过CNN注意力机制优化了局部特征提取，适合快速响应场景。研究结果表明，RF-DETR在复杂空间场景中表现更佳，适合精确农业应用。'}}}, {'id': 'https://huggingface.co/papers/2504.12186', 'title': 'CoMotion: Concurrent Multi-person 3D Motion', 'url': 'https://huggingface.co/papers/2504.12186', 'abstract': 'We introduce an approach for detecting and tracking detailed 3D poses of multiple people from a single monocular camera stream. Our system maintains temporally coherent predictions in crowded scenes filled with difficult poses and occlusions. Our model performs both strong per-frame detection and a learned pose update to track people from frame to frame. Rather than match detections across time, poses are updated directly from a new input image, which enables online tracking through occlusion. We train on numerous image and video datasets leveraging pseudo-labeled annotations to produce a model that matches state-of-the-art systems in 3D pose estimation accuracy while being faster and more accurate in tracking multiple people through time. Code and weights are provided at https://github.com/apple/ml-comotion', 'score': 2, 'issue_id': 3365, 'pub_date': '2025-04-16', 'pub_date_card': {'ru': '16 апреля', 'en': 'April 16', 'zh': '4月16日'}, 'hash': '8a2a784a06bd50d8', 'authors': ['Alejandro Newell', 'Peiyun Hu', 'Lahav Lipson', 'Stephan R. Richter', 'Vladlen Koltun'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets/pdf/title_img/2504.12186.jpg', 'data': {'categories': ['#training', '#dataset', '#cv', '#3d', '#video'], 'emoji': '🕺', 'ru': {'title': 'Точное отслеживание 3D поз множества людей в реальном времени', 'desc': 'Представлен новый подход к обнаружению и отслеживанию детальных трехмерных поз нескольких людей по видеопотоку с одной монокулярной камеры. Модель сочетает сильное покадровое обнаружение с обученным обновлением поз для отслеживания людей от кадра к кадру. Система поддерживает согласованные во времени предсказания в crowded сценах со сложными позами и окклюзиями. Модель обучена на многочисленных наборах данных изображений и видео с использованием псевдо-разметки, что позволяет достичь точности на уровне state-of-the-art систем в оценке 3D поз, при этом быстрее и точнее отслеживая множество людей во времени.'}, 'en': {'title': 'Real-Time 3D Pose Tracking from a Single Camera', 'desc': 'This paper presents a novel method for detecting and tracking the 3D poses of multiple individuals using a single camera. The system is designed to maintain consistent predictions even in crowded environments where people may obstruct each other. Instead of relying on matching detections over time, the model updates poses directly from new images, allowing it to effectively track individuals even when they are temporarily hidden. The approach is trained on various datasets with pseudo-labeled data, achieving high accuracy in 3D pose estimation while improving the speed and reliability of tracking multiple subjects.'}, 'zh': {'title': '单目摄像头下的多人3D姿态实时跟踪', 'desc': '本文提出了一种从单个单目摄像头流中检测和跟踪多人的详细3D姿态的方法。该系统在拥挤场景中保持时间一致的预测，能够处理复杂的姿态和遮挡问题。我们的模型同时进行强大的逐帧检测和学习的姿态更新，从而实现逐帧跟踪。通过利用伪标注的注释在多个图像和视频数据集上进行训练，我们的模型在3D姿态估计精度上与最先进的系统相匹配，同时在多人的时间跟踪上更快且更准确。'}}}, {'id': 'https://huggingface.co/papers/2504.14738', 'title': 'PROMPTEVALS: A Dataset of Assertions and Guardrails for Custom\n  Production Large Language Model Pipelines', 'url': 'https://huggingface.co/papers/2504.14738', 'abstract': 'Large language models (LLMs) are increasingly deployed in specialized production data processing pipelines across diverse domains -- such as finance, marketing, and e-commerce. However, when running them in production across many inputs, they often fail to follow instructions or meet developer expectations. To improve reliability in these applications, creating assertions or guardrails for LLM outputs to run alongside the pipelines is essential. Yet, determining the right set of assertions that capture developer requirements for a task is challenging. In this paper, we introduce PROMPTEVALS, a dataset of 2087 LLM pipeline prompts with 12623 corresponding assertion criteria, sourced from developers using our open-source LLM pipeline tools. This dataset is 5x larger than previous collections. Using a hold-out test split of PROMPTEVALS as a benchmark, we evaluated closed- and open-source models in generating relevant assertions. Notably, our fine-tuned Mistral and Llama 3 models outperform GPT-4o by 20.93% on average, offering both reduced latency and improved performance. We believe our dataset can spur further research in LLM reliability, alignment, and prompt engineering.', 'score': 1, 'issue_id': 3375, 'pub_date': '2025-04-20', 'pub_date_card': {'ru': '20 апреля', 'en': 'April 20', 'zh': '4月20日'}, 'hash': '1dc6ca73af53f954', 'authors': ['Reya Vir', 'Shreya Shankar', 'Harrison Chase', 'Will Fu-Hinthorn', 'Aditya Parameswaran'], 'affiliations': ['LangChain', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2504.14738.jpg', 'data': {'categories': ['#open_source', '#training', '#alignment', '#benchmark', '#dataset'], 'emoji': '🛡️', 'ru': {'title': 'PROMPTEVALS: повышение надежности языковых моделей через критерии проверки', 'desc': 'Эта статья представляет PROMPTEVALS - набор данных из 2087 промптов для языковых моделей (LLM) с 12623 соответствующими критериями проверки, полученными от разработчиков. Авторы оценили способность различных моделей генерировать релевантные проверки, используя тестовую выборку PROMPTEVALS. Их дообученные модели Mistral и Llama 3 превзошли GPT-4 на 20.93% в среднем, обеспечивая меньшую задержку и лучшую производительность. Этот датасет может стимулировать дальнейшие исследования в области надежности LLM, их выравнивания и инженерии промптов.'}, 'en': {'title': 'Enhancing LLM Reliability with PROMPTEVALS Dataset', 'desc': 'This paper addresses the challenges of ensuring that large language models (LLMs) perform reliably in production environments. It introduces PROMPTEVALS, a comprehensive dataset containing prompts and corresponding assertion criteria, which helps developers set expectations for LLM outputs. The dataset is significantly larger than previous ones, allowing for better evaluation of model performance. The authors demonstrate that their fine-tuned models outperform existing models like GPT-4o, highlighting the importance of tailored assertions for improving LLM reliability.'}, 'zh': {'title': '提升大型语言模型的可靠性与性能', 'desc': '大型语言模型（LLMs）在金融、市场营销和电子商务等多个领域的生产数据处理管道中越来越多地被使用。然而，在处理大量输入时，它们常常无法遵循指令或满足开发者的期望。为提高这些应用的可靠性，创建输出的断言或保护措施是至关重要的。本文介绍了PROMPTEVALS数据集，包含2087个LLM管道提示和12623个相应的断言标准，旨在帮助开发者更好地定义任务要求。'}}}, {'id': 'https://huggingface.co/papers/2504.14032', 'title': 'LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision\n  Foundation Models', 'url': 'https://huggingface.co/papers/2504.14032', 'abstract': 'Vision foundation models (VFMs) such as DINOv2 and CLIP have achieved impressive results on various downstream tasks, but their limited feature resolution hampers performance in applications requiring pixel-level understanding. Feature upsampling offers a promising direction to address this challenge. In this work, we identify two critical factors for enhancing feature upsampling: the upsampler architecture and the training objective. For the upsampler architecture, we introduce a coordinate-based cross-attention transformer that integrates the high-resolution images with coordinates and low-resolution VFM features to generate sharp, high-quality features. For the training objective, we propose constructing high-resolution pseudo-groundtruth features by leveraging class-agnostic masks and self-distillation. Our approach effectively captures fine-grained details and adapts flexibly to various input and feature resolutions. Through experiments, we demonstrate that our approach significantly outperforms existing feature upsampling techniques across various downstream tasks. Our code is released at https://github.com/andrehuang/loftup.', 'score': 1, 'issue_id': 3376, 'pub_date': '2025-04-18', 'pub_date_card': {'ru': '18 апреля', 'en': 'April 18', 'zh': '4月18日'}, 'hash': '4d10fef9255485db', 'authors': ['Haiwen Huang', 'Anpei Chen', 'Volodymyr Havrylov', 'Andreas Geiger', 'Dan Zhang'], 'affiliations': ['Tubingen AI Center', 'University of Tubingen'], 'pdf_title_img': 'assets/pdf/title_img/2504.14032.jpg', 'data': {'categories': ['#architecture', '#optimization', '#cv', '#training'], 'emoji': '🔍', 'ru': {'title': 'Точность в деталях: новый метод повышения разрешения признаков в компьютерном зрении', 'desc': 'Эта статья представляет новый подход к улучшению разрешения признаков в моделях компьютерного зрения. Авторы предлагают архитектуру апсемплера на основе трансформера с механизмом внимания, учитывающим координаты. Для обучения используется метод псевдо-разметки с использованием агностических к классам масок и самодистилляции. Эксперименты показывают, что данный метод значительно превосходит существующие техники повышения разрешения признаков на различных задачах.'}, 'en': {'title': 'Enhancing Vision Models with Advanced Feature Upsampling', 'desc': 'This paper addresses the limitations of Vision Foundation Models (VFMs) like DINOv2 and CLIP, which struggle with pixel-level tasks due to low feature resolution. The authors propose a novel upsampling method that combines a coordinate-based cross-attention transformer with high-resolution images and low-resolution VFM features to produce detailed outputs. They also introduce a new training objective that uses class-agnostic masks and self-distillation to create high-resolution pseudo-groundtruth features. Their experiments show that this approach significantly improves performance on various downstream tasks compared to existing methods.'}, 'zh': {'title': '提升视觉模型特征解析力的创新方法', 'desc': '本文提出了一种新的特征上采样方法，以解决视觉基础模型（VFM）在像素级理解任务中的性能限制。我们引入了一种基于坐标的交叉注意力变换器架构，将高分辨率图像与坐标和低分辨率VFM特征结合，从而生成清晰的高质量特征。此外，我们通过利用无类掩码和自蒸馏构建高分辨率伪真实特征，优化了训练目标。实验结果表明，我们的方法在多个下游任务中显著优于现有的特征上采样技术。'}}}, {'id': 'https://huggingface.co/papers/2504.10642', 'title': 'SilVar-Med: A Speech-Driven Visual Language Model for Explainable\n  Abnormality Detection in Medical Imaging', 'url': 'https://huggingface.co/papers/2504.10642', 'abstract': 'Medical Visual Language Models have shown great potential in various healthcare applications, including medical image captioning and diagnostic assistance. However, most existing models rely on text-based instructions, limiting their usability in real-world clinical environments especially in scenarios such as surgery, text-based interaction is often impractical for physicians. In addition, current medical image analysis models typically lack comprehensive reasoning behind their predictions, which reduces their reliability for clinical decision-making. Given that medical diagnosis errors can have life-changing consequences, there is a critical need for interpretable and rational medical assistance. To address these challenges, we introduce an end-to-end speech-driven medical VLM, SilVar-Med, a multimodal medical image assistant that integrates speech interaction with VLMs, pioneering the task of voice-based communication for medical image analysis. In addition, we focus on the interpretation of the reasoning behind each prediction of medical abnormalities with a proposed reasoning dataset. Through extensive experiments, we demonstrate a proof-of-concept study for reasoning-driven medical image interpretation with end-to-end speech interaction. We believe this work will advance the field of medical AI by fostering more transparent, interactive, and clinically viable diagnostic support systems. Our code and dataset are publicly available at SiVar-Med.', 'score': 1, 'issue_id': 3370, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': '0d6f2d2772934b0e', 'authors': ['Tan-Hanh Pham', 'Chris Ngo', 'Trong-Duong Bui', 'Minh Luu Quang', 'Tan-Huong Pham', 'Truong-Son Hy'], 'affiliations': ['Can Tho University of Medicine and Pharmacy, Vietnam', 'Florida Institute of Technology, USA', 'Knovel Engineering Lab, Singapore', 'University of Alabama at Birmingham, USA', 'Vietnam Military Medical University, 108 Military Central Hospital, Vietnam'], 'pdf_title_img': 'assets/pdf/title_img/2504.10642.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#healthcare', '#dataset', '#open_source', '#reasoning'], 'emoji': '🩺', 'ru': {'title': 'Голосовой ИИ-ассистент для анализа медицинских изображений с прозрачным обоснованием диагноза', 'desc': 'Исследователи представили SilVar-Med - мультимодальную модель для анализа медицинских изображений с голосовым управлением. Модель объединяет речевое взаимодействие с визуально-языковыми моделями (VLM), что позволяет использовать ее в реальных клинических условиях, например, во время операций. SilVar-Med также фокусируется на интерпретации рассуждений, стоящих за каждым предсказанием медицинских аномалий, используя специально разработанный набор данных. Эксперименты показали эффективность модели для анализа медицинских изображений с речевым взаимодействием и обоснованием решений.'}, 'en': {'title': 'Revolutionizing Medical Diagnosis with Voice-Driven AI', 'desc': 'This paper presents SilVar-Med, a novel speech-driven medical visual language model (VLM) designed to enhance medical image analysis through voice interaction. Unlike traditional models that depend on text-based instructions, SilVar-Med allows physicians to communicate verbally, making it more practical for real-world clinical settings, especially during surgeries. The model also emphasizes interpretability by providing reasoning behind its predictions, addressing the critical need for reliable decision-making in healthcare. Through experiments, the authors demonstrate the effectiveness of this multimodal approach, aiming to improve transparency and usability in medical AI applications.'}, 'zh': {'title': '语音驱动的医疗图像助手，提升诊断透明度', 'desc': '本文介绍了一种新的医疗视觉语言模型SilVar-Med，它结合了语音交互和医疗图像分析，旨在提高临床环境中的可用性。现有模型主要依赖文本指令，这在手术等场景中并不实用。SilVar-Med通过语音驱动的方式，提供了对医疗异常预测的解释，增强了模型的可靠性。我们的研究表明，这种方法可以推动医疗人工智能的发展，提供更透明和互动的诊断支持系统。'}}}, {'id': 'https://huggingface.co/papers/2504.15266', 'title': 'Roll the dice & look before you leap: Going beyond the creative limits\n  of next-token prediction', 'url': 'https://huggingface.co/papers/2504.15266', 'abstract': 'We design a suite of minimal algorithmic tasks that are a loose abstraction of open-ended real-world tasks. This allows us to cleanly and controllably quantify the creative limits of the present-day language model. Much like real-world tasks that require a creative, far-sighted leap of thought, our tasks require an implicit, open-ended stochastic planning step that either (a) discovers new connections in an abstract knowledge graph (like in wordplay, drawing analogies, or research) or (b) constructs new patterns (like in designing math problems or new proteins). In these tasks, we empirically and conceptually argue how next-token learning is myopic and memorizes excessively; comparatively, multi-token approaches, namely teacherless training and diffusion models, excel in producing diverse and original output. Secondly, in our tasks, we find that to elicit randomness from the Transformer without hurting coherence, it is better to inject noise right at the input layer (via a method we dub hash-conditioning) rather than defer to temperature sampling from the output layer. Thus, our work offers a principled, minimal test-bed for analyzing open-ended creative skills, and offers new arguments for going beyond next-token learning and softmax-based sampling. We make part of the code available under https://github.com/chenwu98/algorithmic-creativity', 'score': 0, 'issue_id': 3379, 'pub_date': '2025-04-21', 'pub_date_card': {'ru': '21 апреля', 'en': 'April 21', 'zh': '4月21日'}, 'hash': '275fb49a9db11221', 'authors': ['Vaishnavh Nagarajan', 'Chen Henry Wu', 'Charles Ding', 'Aditi Raghunathan'], 'affiliations': ['Carnegie Mellon University, Pittsburgh, US', 'Google Research, US'], 'pdf_title_img': 'assets/pdf/title_img/2504.15266.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#creativity', '#data', '#training', '#optimization', '#diffusion'], 'emoji': '🧠', 'ru': {'title': 'Преодоление ограничений обучения следующего токена для творческих задач', 'desc': 'Статья представляет набор минимальных алгоритмических задач, абстрагирующих открытые реальные задачи, для оценки творческих возможностей языковых моделей. Авторы утверждают, что для таких задач, требующих неявного стохастического планирования, подходы с обучением нескольких токенов (например, безучительное обучение и диффузионные модели) превосходят традиционное обучение следующего токена. Исследование показывает, что для внесения случайности в трансформер лучше вводить шум на уровне входного слоя, а не использовать температурную выборку на выходе. Работа предлагает принципиальный минимальный набор тестов для анализа открытых творческих навыков и новые аргументы в пользу выхода за рамки обучения следующего токена.'}, 'en': {'title': 'Unlocking AI Creativity Beyond Next-Token Learning', 'desc': 'This paper introduces a set of simple algorithmic tasks that mimic complex real-world challenges to evaluate the creative capabilities of current language models. The tasks require the models to engage in stochastic planning, either by finding new connections in knowledge graphs or by creating novel patterns. The authors argue that traditional next-token learning is limited and often leads to excessive memorization, while multi-token methods like teacherless training and diffusion models produce more varied and innovative results. Additionally, they propose a new technique called hash-conditioning to enhance randomness in model outputs without sacrificing coherence, providing a framework for assessing creative skills in AI.'}, 'zh': {'title': '探索语言模型的创造性极限', 'desc': '我们设计了一系列最小化的算法任务，这些任务是现实世界开放式任务的抽象。这些任务可以帮助我们量化当前语言模型的创造性极限。与需要创造性思维的现实任务类似，这些任务要求隐含的开放式随机规划步骤，发现知识图谱中的新联系或构建新模式。我们的研究表明，多标记方法在生成多样和原创的输出方面表现优于单标记学习。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (6)', '#agents (5)', '#agi', '#alignment (3)', '#architecture (3)', '#audio (1)', '#benchmark (11)', '#cv (4)', '#data (4)', '#dataset (10)', '#diffusion (3)', '#ethics', '#games (3)', '#graphs', '#hallucinations', '#healthcare (1)', '#inference', '#interpretability (1)', '#leakage', '#long_context (2)', '#low_resource', '#machine_translation', '#math (3)', '#multilingual', '#multimodal (10)', '#open_source (9)', '#optimization (10)', '#plp', '#rag', '#reasoning (10)', '#rl (6)', '#rlhf (3)', '#robotics', '#science', '#security (2)', '#small_models', '#story_generation', '#survey (2)', '#synthetic (1)', '#training (16)', '#transfer_learning (2)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-04-23 00:52',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-04-23 00:52')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-04-23 00:52')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    