
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 22 papers. March 20.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">20 Ğ¼Ğ°Ñ€Ñ‚Ğ°</span> | <span id="title-articles-count">22 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-03-19.html">â¬…ï¸ <span id="prev-date">19.03</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-03-21.html">â¡ï¸ <span id="next-date">21.03</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-03.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'};
        let feedDateNext = {'ru': '21.03', 'en': '03/21', 'zh': '3æœˆ21æ—¥'};
        let feedDatePrev = {'ru': '19.03', 'en': '03/19', 'zh': '3æœˆ19æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.13288', 'title': 'Ï†-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time\n  Exploration and Exploitation', 'url': 'https://huggingface.co/papers/2503.13288', 'abstract': 'Inference-time optimization scales computation to derive deliberate reasoning steps for effective performance. While previous search-based strategies address the short-sightedness of auto-regressive generation, the vast search space leads to excessive exploration and insufficient exploitation. To strike an efficient balance to derive the optimal step, we frame the decoding strategy as foresight sampling, leveraging simulated future steps to obtain globally optimal step estimation. Built on it, we propose a novel decoding strategy, named phi-Decoding. To provide a precise and expressive estimation of step value, phi-Decoding approximates two distributions via foresight and clustering. Sampling from the joint distribution, the optimal steps can be selected for exploitation. To support adaptive computation allocation, we propose in-width and in-depth pruning strategies, featuring a light-weight solution to achieve inference efficiency. Extensive experiments across seven benchmarks show phi-Decoding outperforms strong baselines in both performance and efficiency. Additional analysis demonstrates its generalization across various LLMs and scalability across a wide range of computing budgets. The code will be released at https://github.com/xufangzhi/phi-Decoding, and the open-source PyPI package is coming soon.', 'score': 38, 'issue_id': 2805, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': '8a067ffdfadeb974', 'authors': ['Fangzhi Xu', 'Hang Yan', 'Chang Ma', 'Haiteng Zhao', 'Jun Liu', 'Qika Lin', 'Zhiyong Wu'], 'affiliations': ['National University of Singapore', 'Peking University', 'Shanghai AI Lab', 'The University of Hong Kong', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.13288.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#inference', '#open_source', '#training', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ phi-Decoding Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ÑˆĞ°Ğ³Ğ¾Ğ². phi-Decoding Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ²Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑˆĞ°Ğ³Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ñ€ÑƒĞ½Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Optimizing Inference with phi-Decoding: Balancing Exploration and Exploitation', 'desc': 'This paper introduces phi-Decoding, a new decoding strategy that optimizes inference-time computation in machine learning models. It addresses the limitations of previous search-based methods by balancing exploration and exploitation through foresight sampling, which simulates future steps for better decision-making. The approach uses clustering to approximate distributions, allowing for the selection of optimal steps during the decoding process. Extensive experiments demonstrate that phi-Decoding significantly improves both performance and efficiency across various benchmarks and large language models (LLMs).'}, 'zh': {'title': 'ä¼˜åŒ–æ¨ç†æ•ˆç‡çš„phi-Decodingç­–ç•¥', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§£ç ç­–ç•¥ï¼Œç§°ä¸ºphi-Decodingï¼Œæ—¨åœ¨ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—æ•ˆç‡ã€‚é€šè¿‡å‰ç»é‡‡æ ·ï¼Œphi-Decodingèƒ½å¤Ÿåˆ©ç”¨æ¨¡æ‹Ÿçš„æœªæ¥æ­¥éª¤æ¥è·å¾—å…¨å±€æœ€ä¼˜çš„æ­¥éª¤ä¼°è®¡ï¼Œä»è€Œå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿‘ä¼¼ä¸¤ä¸ªåˆ†å¸ƒæ¥æä¾›ç²¾ç¡®çš„æ­¥éª¤ä»·å€¼ä¼°è®¡ï¼Œå¹¶é€šè¿‡è”åˆåˆ†å¸ƒè¿›è¡Œé‡‡æ ·ä»¥é€‰æ‹©æœ€ä½³æ­¥éª¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œphi-Decodingåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¸Šå‡ä¼˜äºç°æœ‰çš„å¼ºåŸºçº¿ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œè®¡ç®—é¢„ç®—ä¸‹å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15265', 'title': 'DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2503.15265', 'abstract': 'Triangle meshes play a crucial role in 3D applications for efficient manipulation and rendering. While auto-regressive methods generate structured meshes by predicting discrete vertex tokens, they are often constrained by limited face counts and mesh incompleteness. To address these challenges, we propose DeepMesh, a framework that optimizes mesh generation through two key innovations: (1) an efficient pre-training strategy incorporating a novel tokenization algorithm, along with improvements in data curation and processing, and (2) the introduction of Reinforcement Learning (RL) into 3D mesh generation to achieve human preference alignment via Direct Preference Optimization (DPO). We design a scoring standard that combines human evaluation with 3D metrics to collect preference pairs for DPO, ensuring both visual appeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh generates meshes with intricate details and precise topology, outperforming state-of-the-art methods in both precision and quality. Project page: https://zhaorw02.github.io/DeepMesh/', 'score': 32, 'issue_id': 2802, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '37236f5315cc8aef', 'authors': ['Ruowen Zhao', 'Junliang Ye', 'Zhengyi Wang', 'Guangce Liu', 'Yiwen Chen', 'Yikai Wang', 'Jun Zhu'], 'affiliations': ['Nanyang Technological University', 'ShengShu', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.15265.jpg', 'data': {'categories': ['#optimization', '#data', '#alignment', '#rlhf', '#rl', '#3d'], 'emoji': 'ğŸ”·', 'ru': {'title': 'DeepMesh: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-ÑĞµÑ‚Ğ¾Ğº Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹', 'desc': 'DeepMesh - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑĞµÑ‚Ğ¾Ğº. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. DeepMesh Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 3D-ÑĞµÑ‚Ğ¾Ğº, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Direct Preference Optimization. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞµÑ‚ĞºĞ¸ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ.'}, 'en': {'title': 'DeepMesh: Elevating 3D Mesh Generation with Human-Centric Learning', 'desc': 'This paper introduces DeepMesh, a new framework for generating 3D triangle meshes that enhances both quality and precision. It utilizes a unique pre-training strategy with an innovative tokenization method, improving how data is curated and processed. Additionally, it incorporates Reinforcement Learning to align mesh generation with human preferences through Direct Preference Optimization. By conditioning on point clouds and images, DeepMesh produces detailed and accurately structured meshes, surpassing existing methods in performance.'}, 'zh': {'title': 'DeepMeshï¼šä¼˜åŒ–3Dç½‘æ ¼ç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'ä¸‰è§’ç½‘æ ¼åœ¨3Dåº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œæ“ä½œå’Œæ¸²æŸ“ã€‚ä¼ ç»Ÿçš„è‡ªå›å½’æ–¹æ³•é€šè¿‡é¢„æµ‹ç¦»æ•£çš„é¡¶ç‚¹æ ‡è®°ç”Ÿæˆç»“æ„åŒ–ç½‘æ ¼ï¼Œä½†å¸¸å¸¸å—åˆ°é¢æ•°é™åˆ¶å’Œç½‘æ ¼ä¸å®Œæ•´æ€§çš„å›°æ‰°ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DeepMeshæ¡†æ¶ï¼Œé€šè¿‡ä¸¤é¡¹å…³é”®åˆ›æ–°æ¥ä¼˜åŒ–ç½‘æ ¼ç”Ÿæˆï¼šä¸€ç§é«˜æ•ˆçš„é¢„è®­ç»ƒç­–ç•¥å’Œå°†å¼ºåŒ–å­¦ä¹ å¼•å…¥3Dç½‘æ ¼ç”Ÿæˆã€‚DeepMeshèƒ½å¤Ÿç”Ÿæˆç»†èŠ‚ä¸°å¯Œã€æ‹“æ‰‘ç²¾ç¡®çš„ç½‘æ ¼ï¼Œä¸”åœ¨ç²¾åº¦å’Œè´¨é‡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15485', 'title': 'TULIP: Towards Unified Language-Image Pretraining', 'url': 'https://huggingface.co/papers/2503.15485', 'abstract': 'Despite the recent success of image-text contrastive models like CLIP and SigLIP, these models often struggle with vision-centric tasks that demand high-fidelity image understanding, such as counting, depth estimation, and fine-grained object recognition. These models, by performing language alignment, tend to prioritize high-level semantics over visual understanding, weakening their image understanding. On the other hand, vision-focused models are great at processing visual information but struggle to understand language, limiting their flexibility for language-driven tasks. In this work, we introduce TULIP, an open-source, drop-in replacement for existing CLIP-like models. Our method leverages generative data augmentation, enhanced image-image and text-text contrastive learning, and image/text reconstruction regularization to learn fine-grained visual features while preserving global semantic alignment. Our approach, scaling to over 1B parameters, outperforms existing state-of-the-art (SOTA) models across multiple benchmarks, establishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to a 2times enhancement over SigLIP on RxRx1 in linear probing for few-shot classification, and improving vision-language models, achieving over 3times higher scores than SigLIP on MMVP. Our code/checkpoints are available at https://tulip-berkeley.github.io', 'score': 27, 'issue_id': 2800, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': 'd4b870742a020d5a', 'authors': ['Zineng Tang', 'Long Lian', 'Seun Eisape', 'XuDong Wang', 'Roei Herzig', 'Adam Yala', 'Alane Suhr', 'Trevor Darrell', 'David M. Chan'], 'affiliations': ['University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.15485.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#benchmark', '#architecture', '#open_source', '#cv'], 'emoji': 'ğŸŒ·', 'ru': {'title': 'TULIP: Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TULIP - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. TULIP ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼. TULIP Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ state-of-the-art Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… zero-shot ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ few-shot Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'TULIP: Bridging Vision and Language for Enhanced Image Understanding', 'desc': 'This paper presents TULIP, a new model designed to improve image understanding in tasks like counting and depth estimation, which are challenging for existing image-text contrastive models. TULIP enhances visual feature learning by using generative data augmentation and advanced contrastive learning techniques, while still maintaining a connection to high-level semantics. It scales effectively to over 1 billion parameters and demonstrates superior performance on various benchmarks, setting new records in zero-shot classification and few-shot tasks. The model aims to bridge the gap between vision-centric and language-centric approaches, providing a more flexible solution for vision-language tasks.'}, 'zh': {'title': 'TULIPï¼šæå‡å›¾åƒç†è§£çš„æ–°æ–¹æ³•', 'desc': 'å°½ç®¡åƒCLIPå’ŒSigLIPè¿™æ ·çš„å›¾åƒ-æ–‡æœ¬å¯¹æ¯”æ¨¡å‹å–å¾—äº†æˆåŠŸï¼Œä½†å®ƒä»¬åœ¨éœ€è¦é«˜ä¿çœŸå›¾åƒç†è§£çš„è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†TULIPï¼Œè¿™æ˜¯ä¸€ç§å¼€æºçš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ—¨åœ¨é€šè¿‡ç”Ÿæˆæ•°æ®å¢å¼ºå’Œå¯¹æ¯”å­¦ä¹ æ¥æé«˜å›¾åƒç†è§£èƒ½åŠ›ã€‚TULIPèƒ½å¤Ÿå­¦ä¹ ç»†ç²’åº¦çš„è§†è§‰ç‰¹å¾ï¼ŒåŒæ—¶ä¿æŒå…¨å±€è¯­ä¹‰çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒTULIPåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†é›¶-shotæ€§èƒ½å’Œå°‘-shotåˆ†ç±»çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15475', 'title': 'Cube: A Roblox View of 3D Intelligence', 'url': 'https://huggingface.co/papers/2503.15475', 'abstract': 'Foundation models trained on vast amounts of data have demonstrated remarkable reasoning and generation capabilities in the domains of text, images, audio and video. Our goal at Roblox is to build such a foundation model for 3D intelligence, a model that can support developers in producing all aspects of a Roblox experience, from generating 3D objects and scenes to rigging characters for animation to producing programmatic scripts describing object behaviors. We discuss three key design requirements for such a 3D foundation model and then present our first step towards building such a model. We expect that 3D geometric shapes will be a core data type and describe our solution for 3D shape tokenizer. We show how our tokenization scheme can be used in applications for text-to-shape generation, shape-to-text generation and text-to-scene generation. We demonstrate how these applications can collaborate with existing large language models (LLMs) to perform scene analysis and reasoning. We conclude with a discussion outlining our path to building a fully unified foundation model for 3D intelligence.', 'score': 19, 'issue_id': 2800, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '89037dc780448ff8', 'authors': ['Foundation AI Team', 'Kiran Bhat', 'Nishchaie Khanna', 'Karun Channa', 'Tinghui Zhou', 'Yiheng Zhu', 'Xiaoxia Sun', 'Charles Shang', 'Anirudh Sudarshan', 'Maurice Chu', 'Daiqing Li', 'Kangle Deng', 'Jean-Philippe Fauconnier', 'Tijmen Verhulsdonck', 'Maneesh Agrawala', 'Kayvon Fatahalian', 'Alexander Weiss', 'Christian Reiser', 'Ravi Kiran Chirravuri', 'Ravali Kandur', 'Alejandro Pelaez', 'Akash Garg', 'Michael Palleschi', 'Jessica Wang', 'Skylar Litz', 'Leon Liu', 'Anying Li', 'David Harmon', 'Derek Liu', 'Liangjun Feng', 'Denis Goupil', 'Lukas Kuczynski', 'Jihyun Yoon', 'Naveen Marri', 'Peiye Zhuang', 'Yinan Zhang', 'Brian Yin', 'Haomiao Jiang', 'Marcel van Workum', 'Thomas Lane', 'Bryce Erickson', 'Salil Pathare', 'Kyle Price', 'Anupam Singh', 'David Baszucki'], 'affiliations': ['Foundation AI team, Roblox', 'Roblox'], 'pdf_title_img': 'assets/pdf/title_img/2503.15475.jpg', 'data': {'categories': ['#games', '#multimodal', '#3d', '#reasoning'], 'emoji': 'ğŸ§Š', 'ru': {'title': '3D-Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚: Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ 3D-Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² Roblox. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½Ğ° Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ°Ğ¼ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ²ÑĞµÑ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ğµ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ 3D-Ñ„Ğ¾Ñ€Ğ¼ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸Ğ· Ñ„Ğ¾Ñ€Ğ¼ Ğ¸ ÑÑ†ĞµĞ½ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾ ÑÑ†ĞµĞ½Ğ°Ñ….'}, 'en': {'title': 'Building the Future of 3D Intelligence in Roblox', 'desc': 'This paper presents a vision for creating a foundation model specifically designed for 3D intelligence, which can assist developers in various aspects of creating Roblox experiences. The authors emphasize the importance of 3D geometric shapes as a fundamental data type and introduce a novel 3D shape tokenizer to facilitate this process. They explore applications such as text-to-shape and shape-to-text generation, demonstrating how these can work alongside large language models for enhanced scene analysis and reasoning. The paper outlines the initial steps taken towards building a comprehensive model that integrates these capabilities into a unified framework for 3D content creation.'}, 'zh': {'title': 'æ„å»º3Dæ™ºèƒ½çš„åŸºç¡€æ¨¡å‹', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•ä¸º3Dæ™ºèƒ½æ„å»ºåŸºç¡€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ”¯æŒå¼€å‘è€…åœ¨Robloxå¹³å°ä¸Šç”Ÿæˆ3Då¯¹è±¡ã€åœºæ™¯å’ŒåŠ¨ç”»è§’è‰²ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªå…³é”®è®¾è®¡è¦æ±‚ï¼Œå¹¶ä»‹ç»äº†æ„å»º3Då½¢çŠ¶æ ‡è®°å™¨çš„åˆæ­¥æ­¥éª¤ã€‚æˆ‘ä»¬çš„æ ‡è®°åŒ–æ–¹æ¡ˆå¯ä»¥åº”ç”¨äºæ–‡æœ¬åˆ°å½¢çŠ¶ç”Ÿæˆã€å½¢çŠ¶åˆ°æ–‡æœ¬ç”Ÿæˆå’Œæ–‡æœ¬åˆ°åœºæ™¯ç”Ÿæˆç­‰ä»»åŠ¡ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†å¦‚ä½•ä¸ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åä½œï¼Œä»¥å®ç°åœºæ™¯åˆ†æå’Œæ¨ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15417', 'title': 'Temporal Regularization Makes Your Video Generator Stronger', 'url': 'https://huggingface.co/papers/2503.15417', 'abstract': 'Temporal quality is a critical aspect of video generation, as it ensures consistent motion and realistic dynamics across frames. However, achieving high temporal coherence and diversity remains challenging. In this work, we explore temporal augmentation in video generation for the first time, and introduce FluxFlow for initial investigation, a strategy designed to enhance temporal quality. Operating at the data level, FluxFlow applies controlled temporal perturbations without requiring architectural modifications. Extensive experiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow significantly improves temporal coherence and diversity across various video generation models, including U-Net, DiT, and AR-based architectures, while preserving spatial fidelity. These findings highlight the potential of temporal augmentation as a simple yet effective approach to advancing video generation quality.', 'score': 18, 'issue_id': 2801, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '8eb262eda880d162', 'authors': ['Harold Haodong Chen', 'Haojian Huang', 'Xianfeng Wu', 'Yexin Liu', 'Yajing Bai', 'Wen-Jie Shu', 'Harry Yang', 'Ser-Nam Lim'], 'affiliations': ['Everlyn AI', 'HKU', 'HKUST', 'UCF'], 'pdf_title_img': 'assets/pdf/title_img/2503.15417.jpg', 'data': {'categories': ['#benchmark', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'FluxFlow: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ FluxFlow, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FluxFlow Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'Enhancing Video Generation with Temporal Augmentation', 'desc': 'This paper addresses the challenge of maintaining consistent motion and realistic dynamics in video generation, focusing on the aspect of temporal quality. It introduces a novel method called FluxFlow, which applies controlled temporal perturbations to enhance temporal coherence and diversity in generated videos. The approach operates at the data level, meaning it does not require changes to the underlying model architecture. Experimental results on standard benchmarks show that FluxFlow significantly improves the performance of various video generation models while maintaining high spatial fidelity.'}, 'zh': {'title': 'æå‡è§†é¢‘ç”Ÿæˆçš„æ—¶é—´è´¨é‡', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†è§†é¢‘ç”Ÿæˆä¸­çš„æ—¶é—´è´¨é‡é—®é¢˜ï¼Œå¼ºè°ƒäº†åœ¨å¸§ä¹‹é—´ä¿æŒä¸€è‡´è¿åŠ¨å’ŒçœŸå®åŠ¨æ€çš„é‡è¦æ€§ã€‚æˆ‘ä»¬é¦–æ¬¡å¼•å…¥äº†æ—¶é—´å¢å¼ºæŠ€æœ¯ï¼Œå¹¶æå‡ºäº†FluxFlowç­–ç•¥ï¼Œä»¥æé«˜è§†é¢‘ç”Ÿæˆçš„æ—¶é—´è´¨é‡ã€‚FluxFlowåœ¨æ•°æ®å±‚é¢è¿›è¡Œæ“ä½œï¼Œé€šè¿‡æ§åˆ¶æ—¶é—´æ‰°åŠ¨æ¥å¢å¼ºæ—¶é—´ä¸€è‡´æ€§å’Œå¤šæ ·æ€§ï¼Œè€Œæ— éœ€ä¿®æ”¹æ¨¡å‹æ¶æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFluxFlowåœ¨å¤šä¸ªè§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸Šæ˜¾è‘—æ”¹å–„äº†æ—¶é—´ä¸€è‡´æ€§å’Œå¤šæ ·æ€§ï¼ŒåŒæ—¶ä¿æŒäº†ç©ºé—´ä¿çœŸåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14868', 'title': 'Efficient Personalization of Quantized Diffusion Model without\n  Backpropagation', 'url': 'https://huggingface.co/papers/2503.14868', 'abstract': 'Diffusion models have shown remarkable performance in image synthesis, but they demand extensive computational and memory resources for training, fine-tuning and inference. Although advanced quantization techniques have successfully minimized memory usage for inference, training and fine-tuning these quantized models still require large memory possibly due to dequantization for accurate computation of gradients and/or backpropagation for gradient-based algorithms. However, memory-efficient fine-tuning is particularly desirable for applications such as personalization that often must be run on edge devices like mobile phones with private data. In this work, we address this challenge by quantizing a diffusion model with personalization via Textual Inversion and by leveraging a zeroth-order optimization on personalization tokens without dequantization so that it does not require gradient and activation storage for backpropagation that consumes considerable memory. Since a gradient estimation using zeroth-order optimization is quite noisy for a single or a few images in personalization, we propose to denoise the estimated gradient by projecting it onto a subspace that is constructed with the past history of the tokens, dubbed Subspace Gradient. In addition, we investigated the influence of text embedding in image generation, leading to our proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for sampling with effective diffusion timesteps. Our method achieves comparable performance to prior methods in image and text alignment scores for personalizing Stable Diffusion with only forward passes while reducing training memory demand up to 8.2times.', 'score': 18, 'issue_id': 2802, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '8555fb94242b412c', 'authors': ['Hoigi Seo', 'Wongi Jeong', 'Kyungryeol Lee', 'Se Young Chun'], 'affiliations': ['Dept. of Electrical and Computer Engineering, INMC & IPAI Seoul National University, Republic of Korea'], 'pdf_title_img': 'assets/pdf/title_img/2503.14868.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#cv', '#training', '#inference'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞ°Ğ¼ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼: Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ĞºÑ€Ğ°ĞµĞ²Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ´Ğ»Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ 'Subspace Gradient', Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ° Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° 'Partial Uniform Timestep Sampling' Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸."}, 'en': {'title': 'Efficient Personalization of Diffusion Models with Subspace Gradient', 'desc': 'This paper presents a method to improve the efficiency of training diffusion models for image synthesis, particularly for personalization on edge devices. It introduces a quantization technique that allows for fine-tuning without the need for dequantization, thus saving memory during gradient computation. The authors propose a novel approach called Subspace Gradient to reduce noise in gradient estimation by utilizing historical data from personalization tokens. Additionally, they explore the impact of text embeddings on image generation, leading to a new sampling method that optimizes diffusion timesteps, achieving significant memory savings while maintaining performance.'}, 'zh': {'title': 'é«˜æ•ˆä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹çš„å†…å­˜ä¼˜åŒ–', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†è®­ç»ƒå’Œå¾®è°ƒéœ€è¦å¤§é‡è®¡ç®—å’Œå†…å­˜èµ„æºã€‚å°½ç®¡å…ˆè¿›çš„é‡åŒ–æŠ€æœ¯å¯ä»¥å‡å°‘æ¨ç†æ—¶çš„å†…å­˜ä½¿ç”¨ï¼Œä½†è®­ç»ƒè¿™äº›é‡åŒ–æ¨¡å‹ä»ç„¶éœ€è¦å¤§é‡å†…å­˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡æ–‡æœ¬åæ¼”å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œé‡åŒ–çš„æ–¹æ³•ï¼Œå¹¶åˆ©ç”¨é›¶é˜¶ä¼˜åŒ–åœ¨ä¸å»é‡åŒ–çš„æƒ…å†µä¸‹è¿›è¡Œä¸ªæ€§åŒ–å¾®è°ƒï¼Œä»è€Œå‡å°‘å†…å­˜æ¶ˆè€—ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒå’Œæ–‡æœ¬å¯¹é½å¾—åˆ†ä¸Šä¸ä¹‹å‰çš„æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶å°†è®­ç»ƒå†…å­˜éœ€æ±‚é™ä½äº†å¤šè¾¾8.2å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.11557', 'title': 'VERIFY: A Benchmark of Visual Explanation and Reasoning for\n  Investigating Multimodal Reasoning Fidelity', 'url': 'https://huggingface.co/papers/2503.11557', 'abstract': 'Visual reasoning is central to human cognition, enabling individuals to interpret and abstractly understand their environment. Although recent Multimodal Large Language Models (MLLMs) have demonstrated impressive performance across language and vision-language tasks, existing benchmarks primarily measure recognition-based skills and inadequately assess true visual reasoning capabilities. To bridge this critical gap, we introduce VERIFY, a benchmark explicitly designed to isolate and rigorously evaluate the visual reasoning capabilities of state-of-the-art MLLMs. VERIFY compels models to reason primarily from visual information, providing minimal textual context to reduce reliance on domain-specific knowledge and linguistic biases. Each problem is accompanied by a human-annotated reasoning path, making it the first to provide in-depth evaluation of model decision-making processes. Additionally, we propose novel metrics that assess visual reasoning fidelity beyond mere accuracy, highlighting critical imbalances in current model reasoning patterns. Our comprehensive benchmarking of leading MLLMs uncovers significant limitations, underscoring the need for a balanced and holistic approach to both perception and reasoning. For more teaser and testing, visit our project page (https://verify-eqh.pages.dev/).', 'score': 18, 'issue_id': 2812, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 14', 'zh': '3æœˆ14æ—¥'}, 'hash': '468daf59bbaf8821', 'authors': ['Jing Bi', 'Junjia Guo', 'Susan Liang', 'Guangyu Sun', 'Luchuan Song', 'Yunlong Tang', 'Jinxi He', 'Jiarui Wu', 'Ali Vosoughi', 'Chen Chen', 'Chenliang Xu'], 'affiliations': ['University of Central Florida', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2503.11557.jpg', 'data': {'categories': ['#cv', '#interpretability', '#benchmark', '#multimodal', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'VERIFY: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VERIFY Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ², VERIFY Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… MLLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ VERIFY Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑÑ… Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'VERIFY: Elevating Visual Reasoning in MLLMs', 'desc': 'This paper introduces VERIFY, a new benchmark aimed at evaluating the visual reasoning abilities of Multimodal Large Language Models (MLLMs). Unlike existing benchmarks that focus on recognition tasks, VERIFY emphasizes reasoning from visual data with minimal textual support, reducing biases from language. Each task includes a human-annotated reasoning path, allowing for a deeper understanding of how models make decisions. The study also presents new metrics to assess visual reasoning fidelity, revealing significant limitations in current MLLMs and advocating for a more balanced approach to perception and reasoning.'}, 'zh': {'title': 'è§†è§‰æ¨ç†èƒ½åŠ›çš„å…¨æ–°è¯„ä¼°', 'desc': 'è§†è§‰æ¨ç†æ˜¯äººç±»è®¤çŸ¥çš„é‡è¦éƒ¨åˆ†ï¼Œä½¿äººä»¬èƒ½å¤Ÿç†è§£å’ŒæŠ½è±¡åœ°ç†è§£ç¯å¢ƒã€‚å°½ç®¡æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è¯­è¨€å’Œè§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç°æœ‰åŸºå‡†ä¸»è¦æµ‹é‡è¯†åˆ«èƒ½åŠ›ï¼Œæœªèƒ½å……åˆ†è¯„ä¼°çœŸæ­£çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VERIFYåŸºå‡†ï¼Œä¸“é—¨è®¾è®¡ç”¨äºä¸¥æ ¼è¯„ä¼°æœ€å…ˆè¿›çš„MLLMçš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡æä¾›æœ€å°çš„æ–‡æœ¬ä¸Šä¸‹æ–‡ï¼ŒVERIFYä¿ƒä½¿æ¨¡å‹ä¸»è¦ä¾èµ–è§†è§‰ä¿¡æ¯è¿›è¡Œæ¨ç†ï¼Œä»è€Œå‡å°‘å¯¹ç‰¹å®šé¢†åŸŸçŸ¥è¯†å’Œè¯­è¨€åè§çš„ä¾èµ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14891', 'title': 'MetaLadder: Ascending Mathematical Solution Quality via\n  Analogical-Problem Reasoning Transfer', 'url': 'https://huggingface.co/papers/2503.14891', 'abstract': 'Large Language Models (LLMs) have demonstrated promising capabilities in solving mathematical reasoning tasks, leveraging Chain-of-Thought (CoT) data as a vital component in guiding answer generation. Current paradigms typically generate CoT and answers directly for a given problem, diverging from human problem-solving strategies to some extent. Humans often solve problems by recalling analogous cases and leveraging their solutions to reason about the current task. Inspired by this cognitive process, we propose MetaLadder, a novel framework that explicitly prompts LLMs to recall and reflect on meta-problems, those structurally or semantically analogous problems, alongside their CoT solutions before addressing the target problem. Additionally, we introduce a problem-restating mechanism to enhance the model\'s comprehension of the target problem by regenerating the original question, which further improves reasoning accuracy. Therefore, the model can achieve reasoning transfer from analogical problems, mimicking human-like "learning from examples" and generalization abilities. Extensive experiments on mathematical benchmarks demonstrate that our MetaLadder significantly boosts LLMs\' problem-solving accuracy, largely outperforming standard CoT-based methods (10.3\\% accuracy gain) and other methods. Our code and data has been released at https://github.com/LHL3341/MetaLadder.', 'score': 15, 'issue_id': 2811, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '0d8dbba5f4b7283d', 'authors': ['Honglin Lin', 'Zhuoshi Pan', 'Yu Li', 'Qizhi Pei', 'Xin Gao', 'Mengzhang Cai', 'Conghui He', 'Lijun Wu'], 'affiliations': ['Renmin University of China', 'Shanghai AI Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.14891.jpg', 'data': {'categories': ['#transfer_learning', '#math', '#reasoning', '#training', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MetaLadder: ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ MetaLadder Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. MetaLadder Ğ¿Ğ¾Ğ±ÑƒĞ¶Ğ´Ğ°ĞµÑ‚ LLM Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ñ‚ÑŒ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµÑ‚Ğ°-Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ğ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ´ Ñ‚ĞµĞ¼, ĞºĞ°Ğº Ğ¿Ñ€Ğ¸ÑÑ‚ÑƒĞ¿Ğ¸Ñ‚ÑŒ Ğº Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'MetaLadder: Enhancing LLMs with Analogical Reasoning for Better Problem Solving', 'desc': "This paper introduces MetaLadder, a new framework designed to improve the mathematical reasoning abilities of Large Language Models (LLMs). It emphasizes the importance of recalling and reflecting on similar past problems, known as meta-problems, to enhance the model's problem-solving process. By incorporating a problem-restating mechanism, the framework helps the model better understand the target problem, leading to improved reasoning accuracy. Experimental results show that MetaLadder significantly outperforms traditional Chain-of-Thought methods, achieving a notable increase in accuracy on mathematical tasks."}, 'zh': {'title': 'å€Ÿé‰´ç±»æ¯”é—®é¢˜ï¼Œæå‡æ¨ç†èƒ½åŠ›', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å±•ç°äº†è‰¯å¥½çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ•°æ®æ¥æŒ‡å¯¼ç­”æ¡ˆç”Ÿæˆã€‚å½“å‰çš„æ–¹æ³•é€šå¸¸ç›´æ¥ä¸ºç»™å®šé—®é¢˜ç”ŸæˆCoTå’Œç­”æ¡ˆï¼Œè¿™ä¸äººç±»çš„è§£é¢˜ç­–ç•¥æœ‰æ‰€ä¸åŒã€‚äººç±»é€šå¸¸é€šè¿‡å›å¿†ç±»ä¼¼æ¡ˆä¾‹åŠå…¶è§£å†³æ–¹æ¡ˆæ¥æ¨ç†å½“å‰ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºçš„MetaLadderæ¡†æ¶ï¼Œæ˜ç¡®å¼•å¯¼LLMså›å¿†å’Œåæ€ç»“æ„æˆ–è¯­ä¹‰ä¸Šç›¸ä¼¼çš„å…ƒé—®é¢˜åŠå…¶CoTè§£å†³æ–¹æ¡ˆï¼Œä»è€Œæé«˜æ¨ç†å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15354', 'title': 'Optimizing Decomposition for Optimal Claim Verification', 'url': 'https://huggingface.co/papers/2503.15354', 'abstract': 'Current research on the Decompose-Then-Verify paradigm for evaluating the factuality of long-form text typically treats decomposition and verification in isolation, overlooking their interactions and potential misalignment. We find that existing decomposition policies, typically hand-crafted demonstrations, do not align well with downstream verifiers in terms of atomicity -- a novel metric quantifying information density -- leading to suboptimal verification results. We formulate finding the optimal decomposition policy for optimal verification as a bilevel optimization problem. To approximate a solution for this strongly NP-hard problem, we propose dynamic decomposition, a reinforcement learning framework that leverages verifier feedback to learn a policy for dynamically decomposing claims to verifier-preferred atomicity. Experimental results show that dynamic decomposition outperforms existing decomposition policies, improving verification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on average across varying verifiers, datasets, and atomcities of input claims.', 'score': 13, 'issue_id': 2811, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '1b88801be56f7c8f', 'authors': ['Yining Lu', 'Noah Ziems', 'Hy Dang', 'Meng Jiang'], 'affiliations': ['University of Notre Dame, South Bend, IN'], 'pdf_title_img': 'assets/pdf/title_img/2503.15354.jpg', 'data': {'categories': ['#long_context', '#rlhf', '#optimization', '#rl', '#benchmark', '#hallucinations'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Decompose-Then-Verify Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑÑƒÑÑ‚ÑÑ Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½ĞµĞ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 0.12.'}, 'en': {'title': 'Optimizing Decomposition for Better Verification in Text Factuality', 'desc': 'This paper addresses the challenges in the Decompose-Then-Verify approach for assessing the factuality of long-form text. It highlights that traditional decomposition methods do not effectively align with verification processes, particularly in terms of atomicity, which measures the density of information. The authors propose a bilevel optimization framework to find the best decomposition policy that enhances verification outcomes. They introduce a reinforcement learning method called dynamic decomposition, which adapts based on verifier feedback, resulting in improved verification confidence and accuracy across different scenarios.'}, 'zh': {'title': 'åŠ¨æ€åˆ†è§£ï¼šæå‡é•¿æ–‡æœ¬éªŒè¯çš„æœ‰æ•ˆæ€§', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨è¯„ä¼°é•¿æ–‡æœ¬äº‹å®æ€§æ—¶ï¼Œåˆ†è§£-éªŒè¯èŒƒå¼çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å‘ç°ç°æœ‰çš„åˆ†è§£ç­–ç•¥ä¸ä¸‹æ¸¸éªŒè¯å™¨ä¹‹é—´å­˜åœ¨ä¸ä¸€è‡´ï¼Œå¯¼è‡´éªŒè¯ç»“æœä¸ç†æƒ³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†å¯»æ‰¾æœ€ä½³åˆ†è§£ç­–ç•¥è§†ä¸ºä¸€ä¸ªåŒå±‚ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŠ¨æ€åˆ†è§£çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŠ¨æ€åˆ†è§£åœ¨ä¸åŒéªŒè¯å™¨å’Œæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†éªŒè¯ä¿¡å¿ƒå’Œå‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12532', 'title': 'STEVE: AStep Verification Pipeline for Computer-use Agent Training', 'url': 'https://huggingface.co/papers/2503.12532', 'abstract': 'Developing AI agents to autonomously manipulate graphical user interfaces is a long challenging task. Recent advances in data scaling law inspire us to train computer-use agents with a scaled instruction set, yet using behavior cloning to train agents still requires immense high-quality trajectories. To meet the scalability need, we designed STEVE, a step verification pipeline for computer-use agent training. First, we establish a large instruction set for computer-use agents and collect trajectory data with some suboptimal agents. GPT-4o is used to verify the correctness of each step in the trajectories based on the screens before and after the action execution, assigning each step with a binary label. Last, we adopt the Kahneman and Tversky Optimization to optimize the agent from the binary stepwise labels. Extensive experiments manifest that our agent outperforms supervised finetuning by leveraging both positive and negative actions within a trajectory. Also, STEVE enables us to train a 7B vision-language model as a computer-use agent, achieving leading performance in the challenging live desktop environment WinAgentArena with great efficiency at a reduced cost. Code and data: https://github.com/FanbinLu/STEVE.', 'score': 12, 'issue_id': 2800, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 16', 'zh': '3æœˆ16æ—¥'}, 'hash': '185728a70d3b80d0', 'authors': ['Fanbin Lu', 'Zhisheng Zhong', 'Ziqin Wei', 'Shu Liu', 'Chi-Wing Fu', 'Jiaya Jia'], 'affiliations': ['CUHK', 'HKUST', 'SmartMore'], 'pdf_title_img': 'assets/pdf/title_img/2503.12532.jpg', 'data': {'categories': ['#optimization', '#games', '#agents', '#training', '#cv'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'STEVE: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ STEVE - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ ÑÑƒĞ±Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ GPT-4. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞšĞ°Ğ½ĞµĞ¼Ğ°Ğ½Ğ°-Ğ¢Ğ²ĞµÑ€ÑĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ³Ğ¾ ÑÑ‚Ğ¾Ğ»Ğ°.'}, 'en': {'title': 'STEVE: Optimizing AI Agents for GUI Manipulation Efficiently', 'desc': "This paper presents STEVE, a novel step verification pipeline designed to enhance the training of AI agents for manipulating graphical user interfaces. By utilizing a large instruction set and collecting trajectory data from suboptimal agents, STEVE verifies the correctness of each action using GPT-4o, which labels steps as correct or incorrect. The approach incorporates Kahneman and Tversky Optimization to refine the agent's learning process based on these binary labels, allowing the agent to learn from both successful and unsuccessful actions. The results demonstrate that STEVE significantly improves performance in complex environments like WinAgentArena while being more cost-effective than traditional supervised finetuning methods."}, 'zh': {'title': 'æ™ºèƒ½ä»£ç†è®­ç»ƒçš„æ–°çªç ´ï¼šSTEVE', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSTEVEçš„æ­¥éª¤éªŒè¯ç®¡é“ï¼Œç”¨äºè®­ç»ƒè®¡ç®—æœºä½¿ç”¨ä»£ç†ã€‚æˆ‘ä»¬é¦–å…ˆå»ºç«‹äº†ä¸€ä¸ªå¤§å‹æŒ‡ä»¤é›†ï¼Œå¹¶æ”¶é›†äº†ä¸€äº›æ¬¡ä¼˜ä»£ç†çš„è½¨è¿¹æ•°æ®ã€‚é€šè¿‡ä½¿ç”¨GPT-4oéªŒè¯æ¯ä¸ªæ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œå¹¶ä¸ºæ¯ä¸ªæ­¥éª¤åˆ†é…äºŒå…ƒæ ‡ç­¾ï¼Œæœ€åé‡‡ç”¨å¡å°¼æ›¼å’Œç‰¹æ²ƒæ–¯ä¼˜åŒ–æ–¹æ³•æ¥ä¼˜åŒ–ä»£ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ä»£ç†åœ¨å¤æ‚çš„æ¡Œé¢ç¯å¢ƒä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œä¸”è®­ç»ƒæ•ˆç‡é«˜ã€æˆæœ¬ä½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15264', 'title': 'LEGION: Learning to Ground and Explain for Synthetic Image Detection', 'url': 'https://huggingface.co/papers/2503.15264', 'abstract': 'The rapid advancements in generative technology have emerged as a double-edged sword. While offering powerful tools that enhance convenience, they also pose significant social concerns. As defenders, current synthetic image detection methods often lack artifact-level textual interpretability and are overly focused on image manipulation detection, and current datasets usually suffer from outdated generators and a lack of fine-grained annotations. In this paper, we introduce SynthScars, a high-quality and diverse dataset consisting of 12,236 fully synthetic images with human-expert annotations. It features 4 distinct image content types, 3 categories of artifacts, and fine-grained annotations covering pixel-level segmentation, detailed textual explanations, and artifact category labels. Furthermore, we propose LEGION (LEarning to Ground and explain for Synthetic Image detectiON), a multimodal large language model (MLLM)-based image forgery analysis framework that integrates artifact detection, segmentation, and explanation. Building upon this capability, we further explore LEGION as a controller, integrating it into image refinement pipelines to guide the generation of higher-quality and more realistic images. Extensive experiments show that LEGION outperforms existing methods across multiple benchmarks, particularly surpassing the second-best traditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score. Moreover, the refined images generated under its guidance exhibit stronger alignment with human preferences. The code, model, and dataset will be released.', 'score': 8, 'issue_id': 2805, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '4fb3a12e4b0a70e9', 'authors': ['Hengrui Kang', 'Siwei Wen', 'Zichen Wen', 'Junyan Ye', 'Weijia Li', 'Peilin Feng', 'Baichuan Zhou', 'Bin Wang', 'Dahua Lin', 'Linfeng Zhang', 'Conghui He'], 'affiliations': ['Beihang University', 'SenseTime Research', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'Sun Yat-Sen University'], 'pdf_title_img': 'assets/pdf/title_img/2503.15264.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#synthetic', '#alignment', '#cv', '#multimodal', '#dataset'], 'emoji': 'ğŸ”', 'ru': {'title': 'LEGION: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… SynthScars, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ· 12,236 Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ LEGION - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. LEGION Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ², ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğµ. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Synthetic Image Detection with LEGION and SynthScars', 'desc': 'This paper addresses the challenges in detecting synthetic images, which have become increasingly prevalent due to advancements in generative technology. It introduces SynthScars, a comprehensive dataset of 12,236 synthetic images with detailed annotations, including pixel-level segmentation and artifact categories. The authors propose LEGION, a multimodal large language model framework that enhances synthetic image detection by integrating artifact detection, segmentation, and explanation. Experimental results demonstrate that LEGION significantly outperforms existing methods, leading to improved image quality and alignment with human preferences.'}, 'zh': {'title': 'æå‡åˆæˆå›¾åƒæ£€æµ‹çš„æ™ºèƒ½åŒ–ä¸ç²¾ç¡®åº¦', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†SynthScarsï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«12,236å¼ å®Œå…¨åˆæˆå›¾åƒçš„é«˜è´¨é‡å¤šæ ·åŒ–æ•°æ®é›†ï¼Œé…æœ‰äººå·¥ä¸“å®¶æ³¨é‡Šã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†å››ç§ä¸åŒçš„å›¾åƒå†…å®¹ç±»å‹å’Œä¸‰ç±»ä¼ªå½±ï¼Œæä¾›äº†åƒç´ çº§åˆ†å‰²ã€è¯¦ç»†æ–‡æœ¬è§£é‡Šå’Œä¼ªå½±ç±»åˆ«æ ‡ç­¾çš„ç»†ç²’åº¦æ³¨é‡Šã€‚æˆ‘ä»¬è¿˜æå‡ºäº†LEGIONï¼Œä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å›¾åƒä¼ªé€ åˆ†ææ¡†æ¶ï¼Œèƒ½å¤Ÿæ•´åˆä¼ªå½±æ£€æµ‹ã€åˆ†å‰²å’Œè§£é‡ŠåŠŸèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLEGIONåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç”Ÿæˆçš„å›¾åƒæ›´ç¬¦åˆäººç±»åå¥½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14505', 'title': 'MusicInfuser: Making Video Diffusion Listen and Dance', 'url': 'https://huggingface.co/papers/2503.14505', 'abstract': 'We introduce MusicInfuser, an approach for generating high-quality dance videos that are synchronized to a specified music track. Rather than attempting to design and train a new multimodal audio-video model, we show how existing video diffusion models can be adapted to align with musical inputs by introducing lightweight music-video cross-attention and a low-rank adapter. Unlike prior work requiring motion capture data, our approach fine-tunes only on dance videos. MusicInfuser achieves high-quality music-driven video generation while preserving the flexibility and generative capabilities of the underlying models. We introduce an evaluation framework using Video-LLMs to assess multiple dimensions of dance generation quality. The project page and code are available at https://susunghong.github.io/MusicInfuser.', 'score': 8, 'issue_id': 2801, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 18', 'zh': '3æœˆ18æ—¥'}, 'hash': '61ef56ac402491c0', 'authors': ['Susung Hong', 'Ira Kemelmacher-Shlizerman', 'Brian Curless', 'Steven M. Seitz'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.14505.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#diffusion', '#multimodal', '#video'], 'emoji': 'ğŸ’ƒ', 'ru': {'title': 'Ğ¢Ğ°Ğ½Ñ†ÑƒĞ¹ Ğ¿Ğ¾Ğ´ Ğ¼ÑƒĞ·Ñ‹ĞºÑƒ: Ğ˜Ğ˜-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€Ğ¸Ñ‚Ğ¼Ğµ Ğ¼ĞµĞ»Ğ¾Ğ´Ğ¸Ğ¸', 'desc': 'MusicInfuser - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ°Ğ½Ñ†ĞµĞ²Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¾Ñ€Ğ¾Ğ¶ĞºĞ¾Ğ¹. ĞĞ½ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¾Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚, MusicInfuser Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ‚Ğ°Ğ½Ñ†ĞµĞ²Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ°Ğ½Ñ†ĞµĞ² Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM.'}, 'en': {'title': 'Syncing Dance with Music: Introducing MusicInfuser', 'desc': "MusicInfuser is a novel method for creating high-quality dance videos that match a chosen music track. It leverages existing video diffusion models by incorporating music-video cross-attention and a low-rank adapter, rather than building a new model from scratch. This approach allows for fine-tuning on dance videos without the need for motion capture data, enhancing the model's efficiency. Additionally, MusicInfuser includes a new evaluation framework using Video-LLMs to measure various aspects of dance generation quality."}, 'zh': {'title': 'éŸ³ä¹ä¸èˆè¹ˆçš„å®Œç¾èåˆ', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†MusicInfuserï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿæˆé«˜è´¨é‡èˆè¹ˆè§†é¢‘çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿä¸æŒ‡å®šçš„éŸ³ä¹è½¨é“åŒæ­¥ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥è½»é‡çº§çš„éŸ³ä¹-è§†é¢‘äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å’Œä½ç§©é€‚é…å™¨ï¼Œå±•ç¤ºäº†å¦‚ä½•è°ƒæ•´ç°æœ‰çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ä»¥é€‚åº”éŸ³ä¹è¾“å…¥ã€‚ä¸ä¹‹å‰éœ€è¦è¿åŠ¨æ•æ‰æ•°æ®çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…åœ¨èˆè¹ˆè§†é¢‘ä¸Šè¿›è¡Œå¾®è°ƒã€‚MusicInfuseråœ¨ä¿æŒåº•å±‚æ¨¡å‹çš„çµæ´»æ€§å’Œç”Ÿæˆèƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è´¨é‡çš„éŸ³ä¹é©±åŠ¨è§†é¢‘ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12769', 'title': 'ViSpeak: Visual Instruction Feedback in Streaming Videos', 'url': 'https://huggingface.co/papers/2503.12769', 'abstract': 'Recent advances in Large Multi-modal Models (LMMs) are primarily focused on offline video understanding. Instead, streaming video understanding poses great challenges to recent models due to its time-sensitive, omni-modal and interactive characteristics. In this work, we aim to extend the streaming video understanding from a new perspective and propose a novel task named Visual Instruction Feedback in which models should be aware of visual contents and learn to extract instructions from them. For example, when users wave their hands to agents, agents should recognize the gesture and start conversations with welcome information. Thus, following instructions in visual modality greatly enhances user-agent interactions. To facilitate research, we define seven key subtasks highly relevant to visual modality and collect the ViSpeak-Instruct dataset for training and the ViSpeak-Bench for evaluation. Further, we propose the ViSpeak model, which is a SOTA streaming video understanding LMM with GPT-4o-level performance on various streaming video understanding benchmarks. After finetuning on our ViSpeak-Instruct dataset, ViSpeak is equipped with basic visual instruction feedback ability, serving as a solid baseline for future research.', 'score': 7, 'issue_id': 2800, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': '0913c8e386f3aae5', 'authors': ['Shenghao Fu', 'Qize Yang', 'Yuan-Ming Li', 'Yi-Xing Peng', 'Kun-Yu Lin', 'Xihan Wei', 'Jian-Fang Hu', 'Xiaohua Xie', 'Wei-Shi Zheng'], 'affiliations': ['Guangdong Province Key Laboratory of Information Security Technology, China', 'Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China', 'Pazhou Laboratory (Huangpu), China', 'Peng Cheng Laboratory, China', 'School of Computer Science and Engineering, Sun Yat-sen University, China', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.12769.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#benchmark', '#video', '#agents'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'ViSpeak: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ' Ğ´Ğ»Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ViSpeak, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ°Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ½Ğ¸Ñ… Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ViSpeak-Instruct Ğ¸ ViSpeak-Bench. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ViSpeak Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ GPT-4 Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾."}, 'en': {'title': 'Enhancing User-Agent Interaction through Visual Instruction Feedback', 'desc': 'This paper introduces a new approach to understanding streaming video using Large Multi-modal Models (LMMs). It focuses on a task called Visual Instruction Feedback, where models learn to interpret visual cues, like hand gestures, to enhance interactions with users. The authors present the ViSpeak model, which achieves state-of-the-art performance in streaming video understanding after being trained on a newly created dataset. This work aims to improve user-agent communication by enabling agents to respond appropriately to visual instructions.'}, 'zh': {'title': 'æå‡ç”¨æˆ·ä¸ä»£ç†äº’åŠ¨çš„è§†è§‰æŒ‡ä»¤åé¦ˆ', 'desc': 'è¿‘å¹´æ¥ï¼Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰åœ¨ç¦»çº¿è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œæµåª’ä½“è§†é¢‘ç†è§£ç”±äºå…¶æ—¶é—´æ•æ„Ÿæ€§ã€å…¨æ¨¡æ€å’Œäº¤äº’ç‰¹æ€§ï¼Œå¯¹ç°æœ‰æ¨¡å‹æå‡ºäº†å·¨å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°ä»»åŠ¡ï¼Œç§°ä¸ºè§†è§‰æŒ‡ä»¤åé¦ˆï¼Œæ¨¡å‹éœ€è¦ç†è§£è§†è§‰å†…å®¹å¹¶ä»ä¸­æå–æŒ‡ä»¤ï¼Œä»è€Œå¢å¼ºç”¨æˆ·ä¸ä»£ç†ä¹‹é—´çš„äº’åŠ¨ã€‚æˆ‘ä»¬å®šä¹‰äº†ä¸ƒä¸ªä¸è§†è§‰æ¨¡æ€é«˜åº¦ç›¸å…³çš„å­ä»»åŠ¡ï¼Œå¹¶æ”¶é›†äº†ViSpeak-Instructæ•°æ®é›†ç”¨äºè®­ç»ƒï¼ŒViSpeak-Benchç”¨äºè¯„ä¼°ï¼ŒåŒæ—¶æå‡ºäº†ViSpeakæ¨¡å‹ï¼Œå±•ç¤ºäº†åœ¨æµåª’ä½“è§†é¢‘ç†è§£åŸºå‡†ä¸Šçš„å…ˆè¿›æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.11227', 'title': 'GKG-LLM: A Unified Framework for Generalized Knowledge Graph\n  Construction', 'url': 'https://huggingface.co/papers/2503.11227', 'abstract': 'The construction of Generalized Knowledge Graph (GKG), including knowledge graph, event knowledge graph and commonsense knowledge graph, is fundamental for various natural language processing tasks. Current studies typically construct these types of graph separately, overlooking holistic insights and potential unification that could be beneficial in computing resources and usage perspectives. However, a key challenge in developing a unified framework for GKG is obstacles arising from task-specific differences. In this study, we propose a unified framework for constructing generalized knowledge graphs to address this challenge. First, we collect data from 15 sub-tasks in 29 datasets across the three types of graphs, categorizing them into in-sample, counter-task, and out-of-distribution (OOD) data. Then, we propose a three-stage curriculum learning fine-tuning framework, by iteratively injecting knowledge from the three types of graphs into the Large Language Models. Extensive experiments show that our proposed model improves the construction of all three graph types across in-domain, OOD and counter-task data.', 'score': 7, 'issue_id': 2805, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 14', 'zh': '3æœˆ14æ—¥'}, 'hash': '2e48a3554a1036a6', 'authors': ['Jian Zhang', 'Bifan Wei', 'Shihao Qi', 'haiping Zhu', 'Jun Liu', 'Qika Lin'], 'affiliations': ['National University of Singapore', 'School of Computer Science and Technology, Xian Jiaotong University, Xian, China', 'School of Continuing Education, Xian Jiaotong University, Xian, China', 'Shaanxi Province Key Laboratory of Big Data Knowledge Engineering, Xian Jiaotong University, Xian, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.11227.jpg', 'data': {'categories': ['#graphs', '#transfer_learning', '#data', '#training', '#dataset'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ (GKG), Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ³Ñ€Ğ°Ñ„Ñ‹ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºÑƒÑ€Ğ¸ĞºÑƒĞ»ÑƒĞ¼Ğ° Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° 15 Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸Ğ· 29 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ, ĞºĞ¾Ğ½Ñ‚Ñ€-Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ¸ out-of-distribution Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµÑ… Ñ‚Ñ€ĞµÑ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Unifying Knowledge Graphs for Enhanced NLP Performance', 'desc': 'This paper presents a unified framework for constructing Generalized Knowledge Graphs (GKG) that integrates knowledge graphs, event knowledge graphs, and commonsense knowledge graphs. The authors identify the challenge of task-specific differences that hinder the unification of these graphs, which can lead to inefficiencies in resource usage. They propose a three-stage curriculum learning approach that fine-tunes Large Language Models by incorporating knowledge from all three graph types. Experimental results demonstrate that this framework enhances the performance of GKG construction across various data distributions, including in-sample, out-of-distribution, and counter-task scenarios.'}, 'zh': {'title': 'ç»Ÿä¸€æ„å»ºå¹¿ä¹‰çŸ¥è¯†å›¾è°±çš„æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¹¿ä¹‰çŸ¥è¯†å›¾è°±ï¼ˆGKGï¼‰æ„å»ºæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰ç ”ç©¶ä¸­å„ç±»çŸ¥è¯†å›¾è°±åˆ†å¼€æ„å»ºçš„é—®é¢˜ã€‚æˆ‘ä»¬ä»29ä¸ªæ•°æ®é›†ä¸­æ”¶é›†äº†15ä¸ªå­ä»»åŠ¡çš„æ•°æ®ï¼Œå¹¶å°†å…¶åˆ†ç±»ä¸ºæ ·æœ¬å†…ã€å¯¹æŠ—ä»»åŠ¡å’Œåˆ†å¸ƒå¤–æ•°æ®ã€‚é€šè¿‡ä¸‰é˜¶æ®µçš„è¯¾ç¨‹å­¦ä¹ å¾®è°ƒæ¡†æ¶ï¼Œæˆ‘ä»¬å°†ä¸‰ç§ç±»å‹çš„çŸ¥è¯†é€æ­¥æ³¨å…¥åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ ·æœ¬å†…ã€åˆ†å¸ƒå¤–å’Œå¯¹æŠ—ä»»åŠ¡æ•°æ®ä¸Šå‡æå‡äº†ä¸‰ç§çŸ¥è¯†å›¾è°±çš„æ„å»ºæ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.13360', 'title': 'Mitigating Visual Forgetting via Take-along Visual Conditioning for\n  Multi-modal Long CoT Reasoning', 'url': 'https://huggingface.co/papers/2503.13360', 'abstract': "Recent advancements in Large Language Models (LLMs) have demonstrated enhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting to advanced, product-oriented solutions like OpenAI o1. During our re-implementation of this model, we noticed that in multimodal tasks requiring visual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to maintain focus on the visual information, in other words, MLLMs suffer from a gradual decline in attention to visual information as reasoning progresses, causing text-over-relied outputs. To investigate this, we ablate image inputs during long-chain reasoning. Concretely, we truncate the reasoning process midway, then re-complete the reasoning process with the input image removed. We observe only a ~2% accuracy drop on MathVista's test-hard subset, revealing the model's textual outputs dominate the following reasoning process. Motivated by this, we propose Take-along Visual Conditioning (TVC), a strategy that shifts image input to critical reasoning stages and compresses redundant visual tokens via dynamic pruning. This methodology helps the model retain attention to the visual components throughout the reasoning. Our approach achieves state-of-the-art performance on average across five mathematical reasoning benchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in enhancing multimodal reasoning systems.", 'score': 5, 'issue_id': 2804, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': '8877e2e1a13921d1', 'authors': ['Hai-Long Sun', 'Zhun Sun', 'Houwen Peng', 'Han-Jia Ye'], 'affiliations': ['National Key Laboratory for Novel Software Technology, Nanjing University', 'School of Artificial Intelligence, Nanjing University', 'Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2503.13360.jpg', 'data': {'categories': ['#training', '#long_context', '#reasoning', '#multimodal', '#math'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‚ĞµÑ€ÑÑÑ‚ Ñ„Ğ¾ĞºÑƒÑ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Take-along Visual Conditioning (TVC), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑ‰Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ TVC Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ½Ğ° 3.4%.'}, 'en': {'title': 'Enhancing Visual Attention in Multimodal Reasoning with TVC', 'desc': 'This paper discusses the limitations of Multimodal Large Language Models (MLLMs) in maintaining attention to visual information during complex reasoning tasks. The authors found that MLLMs tend to rely more on text as reasoning progresses, leading to a decline in accuracy when visual inputs are removed. To address this issue, they propose a new method called Take-along Visual Conditioning (TVC), which strategically integrates visual inputs at critical stages of reasoning and reduces unnecessary visual data. Their approach significantly improves performance on mathematical reasoning tasks, achieving state-of-the-art results across multiple benchmarks.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨ç†çš„è§†è§‰å…³æ³¨åŠ›', 'desc': 'æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†èƒ½åŠ›ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶æ˜¯ä»é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºæ¼”å˜åˆ°æ›´å…ˆè¿›çš„äº§å“å¯¼å‘è§£å†³æ–¹æ¡ˆã€‚åœ¨æˆ‘ä»¬çš„æ¨¡å‹é‡æ–°å®ç°è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å‘ç°å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨éœ€è¦è§†è§‰è¾“å…¥çš„ä»»åŠ¡ä¸­ï¼ˆå¦‚å‡ ä½•é—®é¢˜ï¼‰éš¾ä»¥ä¿æŒå¯¹è§†è§‰ä¿¡æ¯çš„å…³æ³¨ï¼Œå¯¼è‡´æ–‡æœ¬è¾“å‡ºè¿‡äºä¾èµ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºâ€œéšè¡Œè§†è§‰æ¡ä»¶â€ï¼ˆTVCï¼‰çš„ç­–ç•¥ï¼Œé€šè¿‡åœ¨å…³é”®æ¨ç†é˜¶æ®µå¼•å…¥å›¾åƒè¾“å…¥ï¼Œå¹¶åŠ¨æ€ä¿®å‰ªå†—ä½™çš„è§†è§‰æ ‡è®°ï¼Œå¸®åŠ©æ¨¡å‹åœ¨æ•´ä¸ªæ¨ç†è¿‡ç¨‹ä¸­ä¿æŒå¯¹è§†è§‰æˆåˆ†çš„å…³æ³¨ã€‚æˆ‘ä»¬çš„ç ”ç©¶åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šå®ç°äº†æœ€æ–°çš„æœ€ä½³æ€§èƒ½ï¼Œè¯æ˜äº†TVCåœ¨å¢å¼ºå¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12963', 'title': 'Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based\n  Spatiotemporal Diffusion for Audio-driven Talking Portrait', 'url': 'https://huggingface.co/papers/2503.12963', 'abstract': 'Audio-driven single-image talking portrait generation plays a crucial role in virtual reality, digital human creation, and filmmaking. Existing approaches are generally categorized into keypoint-based and image-based methods. Keypoint-based methods effectively preserve character identity but struggle to capture fine facial details due to the fixed points limitation of the 3D Morphable Model. Moreover, traditional generative networks face challenges in establishing causality between audio and keypoints on limited datasets, resulting in low pose diversity. In contrast, image-based approaches produce high-quality portraits with diverse details using the diffusion network but incur identity distortion and expensive computational costs. In this work, we propose KDTalker, the first framework to combine unsupervised implicit 3D keypoint with a spatiotemporal diffusion model. Leveraging unsupervised implicit 3D keypoints, KDTalker adapts facial information densities, allowing the diffusion process to model diverse head poses and capture fine facial details flexibly. The custom-designed spatiotemporal attention mechanism ensures accurate lip synchronization, producing temporally consistent, high-quality animations while enhancing computational efficiency. Experimental results demonstrate that KDTalker achieves state-of-the-art performance regarding lip synchronization accuracy, head pose diversity, and execution efficiency.Our codes are available at https://github.com/chaolongy/KDTalker.', 'score': 5, 'issue_id': 2808, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': '2cf2c79f5c5a0177', 'authors': ['Chaolong Yang', 'Kai Yao', 'Yuyao Yan', 'Chenru Jiang', 'Weiguang Zhao', 'Jie Sun', 'Guangliang Cheng', 'Yifei Zhang', 'Bin Dong', 'Kaizhu Huang'], 'affiliations': ['Ant Group, Hangzhou, 310000, China', 'Department of Computer Science, University of Liverpool, Liverpool, L69 7 ZX, UK', 'Department of Foundational Mathematics, Xian Jiaotong-Liverpool University, Suzhou, 215123, China', 'Department of Mechatronics and Robotics, Xian Jiaotong-Liverpool University, Suzhou, 215123, China', 'Digital Innovation Research Center, Duke Kunshan University, Kunshan, 215316, China', 'Ricoh Software Research Center, Beijing, 100027, China', 'School of Robotic, Xian Jiaotong-Liverpool University, Suzhou, 215123, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.12963.jpg', 'data': {'categories': ['#architecture', '#cv', '#audio', '#diffusion', '#games', '#multimodal', '#video'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'KDTalker: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ²', 'desc': 'KDTalker - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞĞ½ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ 3D ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. KDTalker Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ñ‹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ¸ Ğ¼ĞµĞ»ĞºĞ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ»Ğ¸Ñ†Ğ°. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³ÑƒĞ± Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ KDTalker Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ÑƒĞ±, Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¿Ğ¾Ğ· Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'KDTalker: Revolutionizing Talking Portraits with Audio and 3D Keypoints', 'desc': 'This paper introduces KDTalker, a novel framework for generating talking portraits from audio inputs. It combines unsupervised implicit 3D keypoints with a spatiotemporal diffusion model to enhance facial detail and pose diversity. Unlike traditional methods, KDTalker effectively synchronizes lip movements with audio while maintaining character identity. The results show that KDTalker outperforms existing techniques in lip synchronization accuracy and computational efficiency.'}, 'zh': {'title': 'KDTalkerï¼šéŸ³é¢‘é©±åŠ¨çš„é«˜æ•ˆè¯´è¯è‚–åƒç”Ÿæˆ', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºKDTalkerçš„æ¡†æ¶ï¼Œç”¨äºéŸ³é¢‘é©±åŠ¨çš„å•å›¾åƒè¯´è¯è‚–åƒç”Ÿæˆã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ— ç›‘ç£éšå¼3Då…³é”®ç‚¹å’Œæ—¶ç©ºæ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿçµæ´»æ•æ‰ç»†è…»çš„é¢éƒ¨ç»†èŠ‚å’Œå¤šæ ·çš„å¤´éƒ¨å§¿æ€ã€‚KDTalkeré€šè¿‡è‡ªå®šä¹‰çš„æ—¶ç©ºæ³¨æ„æœºåˆ¶ï¼Œç¡®ä¿äº†å‡†ç¡®çš„å”‡éƒ¨åŒæ­¥ï¼Œç”Ÿæˆé«˜è´¨é‡ä¸”æ—¶é—´ä¸€è‡´çš„åŠ¨ç”»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKDTalkeråœ¨å”‡éƒ¨åŒæ­¥ç²¾åº¦ã€å¤´éƒ¨å§¿æ€å¤šæ ·æ€§å’Œæ‰§è¡Œæ•ˆç‡æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14830', 'title': 'Decompositional Neural Scene Reconstruction with Generative Diffusion\n  Prior', 'url': 'https://huggingface.co/papers/2503.14830', 'abstract': 'Decompositional reconstruction of 3D scenes, with complete shapes and detailed texture of all objects within, is intriguing for downstream applications but remains challenging, particularly with sparse views as input. Recent approaches incorporate semantic or geometric regularization to address this issue, but they suffer significant degradation in underconstrained areas and fail to recover occluded regions. We argue that the key to solving this problem lies in supplementing missing information for these areas. To this end, we propose DP-Recon, which employs diffusion priors in the form of Score Distillation Sampling (SDS) to optimize the neural representation of each individual object under novel views. This provides additional information for the underconstrained areas, but directly incorporating diffusion prior raises potential conflicts between the reconstruction and generative guidance. Therefore, we further introduce a visibility-guided approach to dynamically adjust the per-pixel SDS loss weights. Together these components enhance both geometry and appearance recovery while remaining faithful to input images. Extensive experiments across Replica and ScanNet++ demonstrate that our method significantly outperforms SOTA methods. Notably, it achieves better object reconstruction under 10 views than the baselines under 100 views. Our method enables seamless text-based editing for geometry and appearance through SDS optimization and produces decomposed object meshes with detailed UV maps that support photorealistic Visual effects (VFX) editing. The project page is available at https://dp-recon.github.io/.', 'score': 3, 'issue_id': 2813, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '7263c31fc82817f1', 'authors': ['Junfeng Ni', 'Yu Liu', 'Ruijie Lu', 'Zirui Zhou', 'Song-Chun Zhu', 'Yixin Chen', 'Siyuan Huang'], 'affiliations': ['Peking University', 'State Key Laboratory of General Artificial Intelligence, BIGAI', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.14830.jpg', 'data': {'categories': ['#3d', '#cv', '#diffusion', '#optimization'], 'emoji': 'ğŸ”', 'ru': {'title': 'DP-Recon: Ğ ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ 3D-ÑÑ†ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ²', 'desc': 'DP-Recon - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 3D-ÑÑ†ĞµĞ½, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Score Distillation Sampling (SDS) Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑĞ»Ğ°Ğ±Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… ÑÑ†ĞµĞ½Ñ‹. DP-Recon Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ°Ñ….'}, 'en': {'title': 'Revolutionizing 3D Scene Reconstruction with DP-Recon!', 'desc': 'This paper presents DP-Recon, a method for reconstructing 3D scenes from sparse views while maintaining detailed shapes and textures of objects. The approach utilizes diffusion priors through Score Distillation Sampling (SDS) to fill in missing information in underconstrained areas, improving the reconstruction of occluded regions. To address potential conflicts between reconstruction accuracy and generative guidance, a visibility-guided mechanism is introduced to adjust loss weights dynamically. Experimental results show that DP-Recon outperforms state-of-the-art methods, achieving superior object reconstruction even with fewer input views, and enabling advanced text-based editing for visual effects.'}, 'zh': {'title': 'DP-Reconï¼šæå‡3Dåœºæ™¯é‡å»ºçš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDP-Reconçš„æ–¹æ³•ï¼Œç”¨äºä»ç¨€ç–è§†å›¾é‡å»º3Dåœºæ™¯ï¼Œæ—¨åœ¨æ¢å¤æ‰€æœ‰ç‰©ä½“çš„å®Œæ•´å½¢çŠ¶å’Œç»†è‡´çº¹ç†ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£å…ˆéªŒå’Œå¾—åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰æ¥ä¼˜åŒ–æ¯ä¸ªç‰©ä½“çš„ç¥ç»è¡¨ç¤ºï¼Œä»è€Œä¸ºæ¬ çº¦æŸåŒºåŸŸæä¾›é¢å¤–ä¿¡æ¯ã€‚ä¸ºäº†é¿å…é‡å»ºä¸ç”ŸæˆæŒ‡å¯¼ä¹‹é—´çš„å†²çªï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸€ç§å¯è§æ€§å¼•å¯¼çš„æ–¹æ³•ï¼ŒåŠ¨æ€è°ƒæ•´æ¯ä¸ªåƒç´ çš„SDSæŸå¤±æƒé‡ã€‚é€šè¿‡åœ¨Replicaå’ŒScanNet++æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒï¼ŒDP-Reconåœ¨ç‰©ä½“é‡å»ºæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨è§†å›¾æ•°é‡è¾ƒå°‘çš„æƒ…å†µä¸‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14434', 'title': 'LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as\n  Evolutionary Optimizers', 'url': 'https://huggingface.co/papers/2503.14434', 'abstract': 'Automated feature engineering plays a critical role in improving predictive model performance for tabular learning tasks. Traditional automated feature engineering methods are limited by their reliance on pre-defined transformations within fixed, manually designed search spaces, often neglecting domain knowledge. Recent advances using Large Language Models (LLMs) have enabled the integration of domain knowledge into the feature engineering process. However, existing LLM-based approaches use direct prompting or rely solely on validation scores for feature selection, failing to leverage insights from prior feature discovery experiments or establish meaningful reasoning between feature generation and data-driven performance. To address these challenges, we propose LLM-FE, a novel framework that combines evolutionary search with the domain knowledge and reasoning capabilities of LLMs to automatically discover effective features for tabular learning tasks. LLM-FE formulates feature engineering as a program search problem, where LLMs propose new feature transformation programs iteratively, and data-driven feedback guides the search process. Our results demonstrate that LLM-FE consistently outperforms state-of-the-art baselines, significantly enhancing the performance of tabular prediction models across diverse classification and regression benchmarks.', 'score': 2, 'issue_id': 2814, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 18', 'zh': '3æœˆ18æ—¥'}, 'hash': '0f8980b0d2167f3d', 'authors': ['Nikhil Abhyankar', 'Parshin Shojaee', 'Chandan K. Reddy'], 'affiliations': ['Department of Computer Science, Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2503.14434.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#reasoning', '#data', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'LLM-FE: Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'LLM-FE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. LLM-FE Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼, Ğ³Ğ´Ğµ LLM Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ° Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLM-FE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Feature Engineering with LLMs', 'desc': 'This paper introduces LLM-FE, a new framework for automated feature engineering that enhances predictive model performance in tabular data tasks. Unlike traditional methods that rely on fixed transformations, LLM-FE utilizes Large Language Models (LLMs) to incorporate domain knowledge and reasoning into the feature generation process. The framework treats feature engineering as a program search problem, allowing LLMs to iteratively propose feature transformations while using data-driven feedback to refine their search. Experimental results show that LLM-FE outperforms existing methods, leading to better outcomes in various classification and regression tasks.'}, 'zh': {'title': 'æ™ºèƒ½ç‰¹å¾å·¥ç¨‹ï¼Œæå‡é¢„æµ‹æ¨¡å‹æ€§èƒ½ï¼', 'desc': 'è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹åœ¨æé«˜è¡¨æ ¼å­¦ä¹ ä»»åŠ¡çš„é¢„æµ‹æ¨¡å‹æ€§èƒ½ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚ä¼ ç»Ÿçš„è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹æ–¹æ³•å—é™äºé¢„å®šä¹‰çš„å˜æ¢å’Œå›ºå®šçš„æ‰‹åŠ¨è®¾è®¡æœç´¢ç©ºé—´ï¼Œå¾€å¾€å¿½è§†é¢†åŸŸçŸ¥è¯†ã€‚æœ€è¿‘ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›å±•ä½¿å¾—å°†é¢†åŸŸçŸ¥è¯†èå…¥ç‰¹å¾å·¥ç¨‹è¿‡ç¨‹æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬æå‡ºçš„LLM-FEæ¡†æ¶ç»“åˆäº†è¿›åŒ–æœç´¢ä¸LLMsçš„é¢†åŸŸçŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å‘ç°æœ‰æ•ˆçš„ç‰¹å¾ï¼Œä»è€Œæ˜¾è‘—æå‡è¡¨æ ¼é¢„æµ‹æ¨¡å‹çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15450', 'title': 'SkyLadder: Better and Faster Pretraining via Context Window Scheduling', 'url': 'https://huggingface.co/papers/2503.15450', 'abstract': 'Recent advancements in LLM pretraining have featured ever-expanding context windows to process longer sequences. However, our pilot study reveals that models pretrained with shorter context windows consistently outperform their long-context counterparts under a fixed token budget. This finding motivates us to explore an optimal context window scheduling strategy to better balance long-context capability with pretraining efficiency. To this end, we propose SkyLadder, a simple yet effective approach that implements a short-to-long context window transition. SkyLadder preserves strong standard benchmark performance, while matching or exceeding baseline results on long context tasks. Through extensive experiments, we pre-train 1B-parameter models (up to 32K context) and 3B-parameter models (8K context) on 100B tokens, demonstrating that SkyLadder yields consistent gains of up to 3.7% on common benchmarks, while achieving up to 22% faster training speeds compared to baselines. The code is at https://github.com/sail-sg/SkyLadder.', 'score': 1, 'issue_id': 2814, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '92dcfd1093b6ecb7', 'authors': ['Tongyao Zhu', 'Qian Liu', 'Haonan Wang', 'Shiqi Chen', 'Xiangming Gu', 'Tianyu Pang', 'Min-Yen Kan'], 'affiliations': ['City University of Hong Kong', 'National University of Singapore', 'Sea AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.15450.jpg', 'data': {'categories': ['#optimization', '#architecture', '#benchmark', '#long_context', '#training'], 'emoji': 'ğŸªœ', 'ru': {'title': 'SkyLadder: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… Ğ¾ĞºĞ½Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ĞºĞ½Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ SkyLadder, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¾Ñ‚ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğº Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¾ĞºĞ½Ğ°Ğ¼ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. SkyLadder ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SkyLadder Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ´Ğ¾ 3.7% Ğ½Ğ° Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 22% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'SkyLadder: Optimizing Context Windows for Efficient LLM Training', 'desc': 'This paper discusses the impact of context window sizes in the pretraining of large language models (LLMs). It finds that models trained with shorter context windows perform better than those with longer ones when the total number of tokens is fixed. To address this, the authors introduce SkyLadder, a strategy that transitions from short to long context windows during training. Their experiments show that SkyLadder not only maintains high performance on standard benchmarks but also improves training speed significantly.'}, 'zh': {'title': 'SkyLadderï¼šä¼˜åŒ–ä¸Šä¸‹æ–‡çª—å£çš„é«˜æ•ˆé¢„è®­ç»ƒç­–ç•¥', 'desc': 'æœ€è¿‘åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢„è®­ç»ƒæ–¹é¢çš„è¿›å±•ï¼Œé‡‡ç”¨äº†è¶Šæ¥è¶Šå¤§çš„ä¸Šä¸‹æ–‡çª—å£æ¥å¤„ç†æ›´é•¿çš„åºåˆ—ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„åˆæ­¥ç ”ç©¶è¡¨æ˜ï¼Œåœ¨å›ºå®šçš„æ ‡è®°é¢„ç®—ä¸‹ï¼Œä½¿ç”¨è¾ƒçŸ­ä¸Šä¸‹æ–‡çª—å£é¢„è®­ç»ƒçš„æ¨¡å‹è¡¨ç°ä¼˜äºé•¿ä¸Šä¸‹æ–‡æ¨¡å‹ã€‚è¿™ä¸€å‘ç°ä¿ƒä½¿æˆ‘ä»¬æ¢ç´¢ä¸€ç§æœ€ä½³çš„ä¸Šä¸‹æ–‡çª—å£è°ƒåº¦ç­–ç•¥ï¼Œä»¥æ›´å¥½åœ°å¹³è¡¡é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›ä¸é¢„è®­ç»ƒæ•ˆç‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†SkyLadderï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå®æ–½çŸ­åˆ°é•¿çš„ä¸Šä¸‹æ–‡çª—å£è¿‡æ¸¡ï¼Œä¿æŒå¼ºå¤§çš„åŸºå‡†æ€§èƒ½ï¼ŒåŒæ—¶åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸ŠåŒ¹é…æˆ–è¶…è¿‡åŸºçº¿ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15055', 'title': 'ELTEX: A Framework for Domain-Driven Synthetic Data Generation', 'url': 'https://huggingface.co/papers/2503.15055', 'abstract': "We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework for generating high-quality synthetic training data in specialized domains. While Large Language Models (LLMs) have shown impressive general capabilities, their performance in specialized domains like cybersecurity remains limited by the scarcity of domain-specific training data. ELTEX addresses this challenge by systematically integrating explicit domain indicator extraction with dynamic prompting to preserve critical domain knowledge throughout the generation process. We demonstrate ELTEX's effectiveness in the context of blockchain-related cyberattack detection, where we fine-tune Gemma-2B using various combinations of real and ELTEX-generated data. Our results show that the ELTEX-enhanced model achieves performance competitive with GPT-4 across both standard classification metrics and uncertainty calibration, while requiring significantly fewer computational resources. We release a curated synthetic dataset of social media texts for cyberattack detection in blockchain. Our work demonstrates that domain-driven synthetic data generation can effectively bridge the performance gap between resource-efficient models and larger architectures in specialized domains.", 'score': 1, 'issue_id': 2802, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '785ffad4856eb286', 'authors': ['Arina Razmyslovich', 'Kseniia Murasheva', 'Sofia Sedlova', 'Julien Capitaine', 'Eugene Dmitriev'], 'affiliations': ['Distributed Networks Institute (DNI)', 'Technologies MÃ©sozoÃ¯ques'], 'pdf_title_img': 'assets/pdf/title_img/2503.15055.jpg', 'data': {'categories': ['#science', '#data', '#training', '#synthetic', '#dataset'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'ELTEX: Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ¯Ğœ Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…', 'desc': 'ELTEX - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ. ĞĞ½ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ğ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ELTEX Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ĞºĞ¸Ğ±ĞµÑ€Ğ°Ñ‚Ğ°Ğº Ğ² Ğ±Ğ»Ğ¾ĞºÑ‡ĞµĞ¹Ğ½Ğµ, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Gemma-2B, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ GPT-4. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ….'}, 'en': {'title': 'Bridging the Data Gap in Cybersecurity with ELTEX', 'desc': 'The paper introduces ELTEX, a framework designed to create high-quality synthetic training data specifically for specialized fields like cybersecurity. It tackles the issue of limited domain-specific data that affects the performance of Large Language Models (LLMs) in these areas. By combining domain indicator extraction with dynamic prompting, ELTEX ensures that essential domain knowledge is maintained during data generation. The framework is validated through its application in blockchain cyberattack detection, showing that it can enhance model performance while being more resource-efficient than larger models like GPT-4.'}, 'zh': {'title': 'é¢†åŸŸé©±åŠ¨çš„åˆæˆæ•°æ®ç”Ÿæˆï¼Œæå‡æ¨¡å‹æ€§èƒ½ï¼', 'desc': 'ELTEXï¼ˆé«˜æ•ˆLLMä»¤ç‰Œæå–ï¼‰æ˜¯ä¸€ä¸ªé’ˆå¯¹ç‰¹å®šé¢†åŸŸç”Ÿæˆé«˜è´¨é‡åˆæˆè®­ç»ƒæ•°æ®çš„æ¡†æ¶ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€šç”¨èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç½‘ç»œå®‰å…¨ç­‰ä¸“ä¸šé¢†åŸŸçš„è¡¨ç°å—åˆ°é¢†åŸŸç‰¹å®šè®­ç»ƒæ•°æ®ç¨€ç¼ºçš„é™åˆ¶ã€‚ELTEXé€šè¿‡ç³»ç»Ÿåœ°æ•´åˆæ˜¾å¼é¢†åŸŸæŒ‡ç¤ºç¬¦æå–å’ŒåŠ¨æ€æç¤ºï¼Œç¡®ä¿åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿ç•™å…³é”®çš„é¢†åŸŸçŸ¥è¯†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒELTEXå¢å¼ºçš„æ¨¡å‹åœ¨åŒºå—é“¾ç›¸å…³çš„ç½‘ç»œæ”»å‡»æ£€æµ‹ä¸­ï¼Œæ€§èƒ½ä¸GPT-4ç›¸å½“ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†è®¡ç®—èµ„æºçš„éœ€æ±‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.13517', 'title': 'CURIE: Evaluating LLMs On Multitask Scientific Long Context\n  Understanding and Reasoning', 'url': 'https://huggingface.co/papers/2503.13517', 'abstract': 'Scientific problem-solving involves synthesizing information while applying expert knowledge. We introduce CURIE, a scientific long-Context Understanding,Reasoning and Information Extraction benchmark to measure the potential of Large Language Models (LLMs) in scientific problem-solving and assisting scientists in realistic workflows. This benchmark introduces ten challenging tasks with a total of 580 problems and solution pairs curated by experts in six disciplines - materials science, condensed matter physics, quantum computing, geospatial analysis, biodiversity, and proteins - covering both experimental and theoretical work-flows in science. We evaluate a range of closed and open LLMs on tasks in CURIE which requires domain expertise, comprehension of long in-context information,and multi-step reasoning. While Gemini Flash 2.0 and Claude-3 show consistent high comprehension across domains, the popular GPT-4o and command-R+ fail dramatically on protein sequencing tasks. With the best performance at 32% there is much room for improvement for all models. We hope that insights gained from CURIE can guide the future development of LLMs in sciences. Evaluation code and data are in https://github.com/google/curie', 'score': 1, 'issue_id': 2810, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 14', 'zh': '3æœˆ14æ—¥'}, 'hash': '56a7b2a9ca22936c', 'authors': ['Hao Cui', 'Zahra Shamsi', 'Gowoon Cheon', 'Xuejian Ma', 'Shutong Li', 'Maria Tikhanovskaya', 'Peter Norgaard', 'Nayantara Mudur', 'Martyna Plomecka', 'Paul Raccuglia', 'Yasaman Bahri', 'Victor V. Albert', 'Pranesh Srinivasan', 'Haining Pan', 'Philippe Faist', 'Brian Rohr', 'Michael J. Statt', 'Dan Morris', 'Drew Purves', 'Elise Kleeman', 'Ruth Alcantara', 'Matthew Abraham', 'Muqthar Mohammad', 'Ean Phing VanLee', 'Chenfei Jiang', 'Elizabeth Dorfman', 'Eun-Ah Kim', 'Michael P Brenner', 'Viren Jain', 'Sameera Ponda', 'Subhashini Venugopalan'], 'affiliations': ['Cornell', 'FU Berlin', 'Google', 'Harvard', 'Modelyst', 'NIST', 'Rutgers', 'UMD College Park', 'University of Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2503.13517.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#science', '#dataset', '#multimodal', '#long_context'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'CURIE: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ´Ğ»Ñ Ğ˜Ğ˜ Ğ² Ğ½Ğ°ÑƒĞºĞµ', 'desc': 'CURIE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 580 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· 6 Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM Ğ½Ğ° CURIE Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ 32% ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ°Ğ´ĞµÑÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ CURIE Ğ¿Ğ¾Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ² Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ¼ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğ¸ LLM Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'CURIE: Advancing LLMs for Scientific Problem-Solving', 'desc': 'The paper presents CURIE, a benchmark designed to evaluate the capabilities of Large Language Models (LLMs) in scientific problem-solving. It includes ten complex tasks with 580 curated problems across various scientific fields, emphasizing the need for domain expertise and multi-step reasoning. The evaluation reveals that while some models like Gemini Flash 2.0 and Claude-3 perform well, others like GPT-4o struggle significantly, particularly in protein sequencing tasks. The findings suggest that there is substantial potential for improving LLMs to better assist scientists in their workflows.'}, 'zh': {'title': 'CURIEï¼šæ¨åŠ¨ç§‘å­¦é—®é¢˜è§£å†³çš„è¯­è¨€æ¨¡å‹åŸºå‡†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†CURIEï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç§‘å­¦é—®é¢˜è§£å†³ä¸­çš„èƒ½åŠ›çš„åŸºå‡†ã€‚CURIEåŒ…å«åä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå…±580ä¸ªé—®é¢˜å’Œè§£å†³æ–¹æ¡ˆï¼Œæ¶µç›–ææ–™ç§‘å­¦ã€å‡èšæ€ç‰©ç†ã€é‡å­è®¡ç®—ã€åœ°ç†ç©ºé—´åˆ†æã€ç”Ÿç‰©å¤šæ ·æ€§å’Œè›‹ç™½è´¨ç­‰å…­ä¸ªå­¦ç§‘ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šç§å°é—­å’Œå¼€æ”¾çš„LLMsï¼Œå‘ç°è™½ç„¶Gemini Flash 2.0å’ŒClaude-3åœ¨å„ä¸ªé¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†GPT-4oå’Œcommand-R+åœ¨è›‹ç™½è´¨æµ‹åºä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚CURIEçš„ç»“æœä¸ºæœªæ¥LLMsåœ¨ç§‘å­¦é¢†åŸŸçš„å‘å±•æä¾›äº†é‡è¦çš„æŒ‡å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15478', 'title': 'SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning\n  Tasks', 'url': 'https://huggingface.co/papers/2503.15478', 'abstract': 'Large language model (LLM) agents need to perform multi-turn interactions in real-world tasks. However, existing multi-turn RL algorithms for optimizing LLM agents fail to perform effective credit assignment over multiple turns while leveraging the generalization capabilities of LLMs and it remains unclear how to develop such algorithms. To study this, we first introduce a new benchmark, ColBench, where an LLM agent interacts with a human collaborator over multiple turns to solve realistic tasks in backend programming and frontend design. Building on this benchmark, we propose a novel RL algorithm, SWEET-RL (RL with Step-WisE Evaluation from Training-time information), that uses a carefully designed optimization objective to train a critic model with access to additional training-time information. The critic provides step-level rewards for improving the policy model. Our experiments demonstrate that SWEET-RL achieves a 6% absolute improvement in success and win rates on ColBench compared to other state-of-the-art multi-turn RL algorithms, enabling Llama-3.1-8B to match or exceed the performance of GPT4-o in realistic collaborative content creation.', 'score': 0, 'issue_id': 2814, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': 'a4dcf1557e92629c', 'authors': ['Yifei Zhou', 'Song Jiang', 'Yuandong Tian', 'Jason Weston', 'Sergey Levine', 'Sainbayar Sukhbaatar', 'Xian Li'], 'affiliations': ['FAIR at Meta', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.15478.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#games', '#rl', '#agents', '#rlhf', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'SWEET-RL: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ SWEET-RL Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ…. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½ÑƒÑ Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ColBench, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ²ĞµĞ±-Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SWEET-RL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° 6% Ğ¿Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ColBench.'}, 'en': {'title': 'Enhancing LLMs with SWEET-RL for Better Multi-Turn Interactions', 'desc': 'This paper addresses the challenge of optimizing large language model (LLM) agents for multi-turn interactions in real-world tasks. Existing reinforcement learning (RL) methods struggle with credit assignment across multiple turns, which affects their performance. To tackle this, the authors introduce ColBench, a benchmark for evaluating LLM agents in collaborative tasks, and propose a new RL algorithm called SWEET-RL. This algorithm enhances the training process by using a critic model that provides step-level rewards, leading to improved success rates in collaborative content creation tasks.'}, 'zh': {'title': 'æå‡å¤šè½®äº¤äº’çš„å¼ºåŒ–å­¦ä¹ æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šè½®äº¤äº’ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒæŒ‡å‡ºç°æœ‰çš„å¤šè½®å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•åœ¨æœ‰æ•ˆçš„ä¿¡ç”¨åˆ†é…æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ColBenchï¼Œæ—¨åœ¨è¯„ä¼°LLMä»£ç†ä¸äººç±»åä½œè§£å†³å®é™…ä»»åŠ¡çš„èƒ½åŠ›ã€‚åŸºäºæ­¤åŸºå‡†ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•SWEET-RLï¼Œè¯¥ç®—æ³•é€šè¿‡è®¾è®¡ä¼˜åŒ–ç›®æ ‡æ¥è®­ç»ƒä¸€ä¸ªè¯„è®ºæ¨¡å‹ï¼Œä»è€Œæä¾›é€æ­¥å¥–åŠ±ä»¥æ”¹å–„ç­–ç•¥æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSWEET-RLåœ¨ColBenchä¸Šç›¸è¾ƒäºå…¶ä»–å…ˆè¿›çš„å¤šè½®RLç®—æ³•ï¼ŒæˆåŠŸç‡å’Œèƒœç‡æé«˜äº†6%ï¼Œä½¿å¾—Llama-3.1-8Båœ¨å®é™…åä½œå†…å®¹åˆ›ä½œä¸­èƒ½å¤Ÿä¸GPT4-oçš„è¡¨ç°ç›¸åŒ¹é…æˆ–è¶…è¶Šã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (3)', '#agi', '#alignment (2)', '#architecture (4)', '#audio (1)', '#benchmark (13)', '#cv (7)', '#data (4)', '#dataset (5)', '#diffusion (4)', '#ethics', '#games (4)', '#graphs (1)', '#hallucinations (1)', '#healthcare', '#inference (2)', '#interpretability (2)', '#leakage', '#long_context (4)', '#low_resource', '#machine_translation', '#math (2)', '#multilingual', '#multimodal (9)', '#open_source (3)', '#optimization (10)', '#plp', '#rag', '#reasoning (6)', '#rl (3)', '#rlhf (3)', '#robotics', '#science (2)', '#security', '#small_models', '#story_generation', '#survey', '#synthetic (2)', '#training (10)', '#transfer_learning (2)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-03-20 19:08',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-20 19:08')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-20 19:08')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    