
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 47 papers. October 8.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">8 октября</span> | <span id="title-articles-count">47 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-10-07.html">⬅️ <span id="prev-date">07.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-10-09.html">➡️ <span id="next-date">09.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-10.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'};
        let feedDateNext = {'ru': '09.10', 'en': '10/09', 'zh': '10月9日'};
        let feedDatePrev = {'ru': '07.10', 'en': '10/07', 'zh': '10月7日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2510.04871', 'title': 'Less is More: Recursive Reasoning with Tiny Networks', 'url': 'https://huggingface.co/papers/2510.04871', 'abstract': 'Tiny Recursive Model (TRM) achieves high generalization on complex puzzle tasks using a small, two-layer network with minimal parameters, outperforming larger language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Hierarchical Reasoning Model (HRM) is a novel approach using two small neural networks recursing at different frequencies. This biologically inspired method beats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze, and ARC-AGI while trained with small models (27M parameters) on small data (around 1000 examples). HRM holds great promise for solving hard problems with small networks, but it is not yet well understood and may be suboptimal. We propose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach that achieves significantly higher generalization than HRM, while using a single tiny network with only 2 layers. With only 7M parameters, TRM obtains 45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs (e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the parameters.', 'score': 72, 'issue_id': 6309, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': '6a09a2f2d85ae333', 'authors': ['Alexia Jolicoeur-Martineau'], 'affiliations': ['Samsung SAIL Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2510.04871.jpg', 'data': {'categories': ['#agi', '#training', '#reasoning', '#small_models'], 'emoji': '🔄', 'ru': {'title': 'Маленькая рекурсия побеждает гигантов: 7M параметров против LLM', 'desc': 'Исследователи представили Tiny Recursive Model (TRM) — миниатюрную рекурсивную модель всего из 2 слоёв и 7 миллионов параметров, которая решает сложные логические задачи лучше больших языковых моделей. TRM показывает 45% точности на ARC-AGI-1 и 8% на ARC-AGI-2, превосходя большинство LLM при использовании менее 0.01% их параметров. Модель обучается на малых данных (около 1000 примеров) и успешно справляется с задачами типа судоку, лабиринтов и ARC-AGI. Этот подход, вдохновлённый биологическими принципами, демонстрирует, что для сложного рассуждения не всегда нужны огромные AI-модели.'}, 'en': {'title': "Small Network, Big Solutions: TRM's Power in Puzzle Solving", 'desc': 'The Tiny Recursive Model (TRM) is a compact neural network architecture designed to solve complex puzzle tasks with high generalization. It utilizes a simple two-layer structure with only 7 million parameters, significantly fewer than larger language models. TRM outperforms existing models on challenging tasks like ARC-AGI, demonstrating that smaller networks can achieve competitive accuracy. This approach highlights the potential of minimalistic designs in machine learning for effective problem-solving.'}, 'zh': {'title': '小型递归模型，超越大型语言模型的解决方案', 'desc': 'Tiny Recursive Model (TRM) 是一种新颖的递归推理模型，使用一个只有两层的小型神经网络，参数仅为700万。它在复杂的拼图任务上表现出色，超越了许多大型语言模型。TRM 的设计灵感来源于生物学，能够在小数据集上实现高泛化能力。尽管 TRM 结构简单，但在解决困难问题时展现了巨大的潜力。'}}}, {'id': 'https://huggingface.co/papers/2509.24107', 'title': 'Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and\n  Synthesis for SLMs', 'url': 'https://huggingface.co/papers/2509.24107', 'abstract': 'Fathom-DeepResearch, an agentic system with specialized models for web search and report synthesis, achieves state-of-the-art performance on open-ended information-seeking tasks and diverse reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Tool-integrated reasoning has emerged as a key focus for enabling agentic applications. Among these, DeepResearch Agents have gained significant attention for their strong performance on complex, open-ended information-seeking tasks. We introduce Fathom-DeepResearch, an agentic system composed of two specialized models. The first is Fathom-Search-4B, a DeepSearch model trained from Qwen3-4B and optimized for evidence-based investigation through live web search and targeted webpage querying. Its training combines three advances: (i) DUETQA, a 5K-sample dataset generated via multi-agent self-play that enforces strict web-search dependence and heterogeneous source grounding; (ii) RAPO, a zero-overhead extension of GRPO that stabilizes multi-turn Reinforcement Learning with Verifiable Rewards through curriculum pruning, reward-aware advantage scaling, and per-prompt replay buffers; and (iii) a steerable step-level reward that classifies each tool call by cognitive behavior and marginal utility, enabling explicit control over search trajectory breadth, depth, and horizon. These improvements enable reliable extension of tool-calling beyond 20 calls when warranted. The second is Fathom-Synthesizer-4B, trained from Qwen3-4B, which converts multi-turn DeepSearch traces into structured, citation-dense DeepResearch Reports for comprehensive synthesis. Evaluated on DeepSearch benchmarks (SimpleQA, FRAMES, WebWalker, Seal0, MuSiQue) and DeepResearch-Bench, the system achieves state-of-the-art performance in the open-weights category while demonstrating strong generalization to diverse reasoning tasks including HLE, AIME-25, GPQA-Diamond, and MedQA.', 'score': 57, 'issue_id': 6301, 'pub_date': '2025-09-28', 'pub_date_card': {'ru': '28 сентября', 'en': 'September 28', 'zh': '9月28日'}, 'hash': '17db556aca22a151', 'authors': ['Shreyas Singh', 'Kunal Singh', 'Pradeep Moturi'], 'affiliations': ['Fractal AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2509.24107.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#agents', '#rl', '#dataset', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Глубокий веб-поиск с помощью специализированных агентов', 'desc': 'Представлена система Fathom-DeepResearch, состоящая из двух специализированных моделей по 4 миллиарда параметров для сложных информационных задач. Первая модель оптимизирована для веб-поиска и запросов к страницам, обучена с помощью мультиагентного подхода и может делать более 20 последовательных вызовов инструментов. Вторая модель преобразует результаты поиска в структурированные отчёты с цитатами. Система достигает state-of-the-art результатов среди open-weights моделей на бенчмарках для поиска информации и различных задачах рассуждений.'}, 'en': {'title': 'Revolutionizing Web Search and Report Synthesis with Fathom-DeepResearch', 'desc': 'Fathom-DeepResearch is an advanced agentic system designed for effective web search and report synthesis. It consists of two specialized models: Fathom-Search-4B, which excels in evidence-based investigations through live web searches, and Fathom-Synthesizer-4B, which transforms search results into structured reports. The system incorporates innovative techniques like DUETQA for dataset generation and RAPO for enhancing reinforcement learning stability. With its state-of-the-art performance on various benchmarks, Fathom-DeepResearch demonstrates exceptional capabilities in handling complex information-seeking and reasoning tasks.'}, 'zh': {'title': '智能搜索与报告合成的未来', 'desc': 'Fathom-DeepResearch 是一个智能系统，专门用于网络搜索和报告合成，能够在开放式信息检索任务和多样化推理任务中表现出色。该系统由两个专门模型组成：Fathom-Search-4B 和 Fathom-Synthesizer-4B。Fathom-Search-4B 通过实时网络搜索和针对网页查询进行证据基础调查，采用了多项先进技术来优化其性能。Fathom-Synthesizer-4B 则将多轮 DeepSearch 追踪转换为结构化的、引用密集的 DeepResearch 报告，确保信息的全面合成。'}}}, {'id': 'https://huggingface.co/papers/2510.06217', 'title': 'TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular\n  Reasoning', 'url': 'https://huggingface.co/papers/2510.06217', 'abstract': 'TaTToo, a novel table-grounded Process Reward Model, enhances tabular reasoning by explicitly addressing table-specific operations and integrating tool-based verification, leading to significant performance improvements over existing PRMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Process Reward Models (PRMs) have recently emerged as a powerful framework for enhancing the reasoning capabilities of large reasoning models (LRMs), particularly in the context of test-time scaling (TTS). However, their potential for supervising LRMs on tabular reasoning domains remains underexplored. Through detailed empirical analyses, we identify that existing PRMs, though widely adopted for supervising text-only reasoning steps, struggle with table-specific operations such as sub-table retrieval and schema interaction, leading to critical performance bottlenecks. To address this limitation, we propose TaTToo, a novel table-grounded PRM framework that (i) reasons explicitly over tabular reasoning steps and (ii) integrates tool-based verification to provide precise reward supervision. Concretely, we first design a scalable data curation pipeline that constructs over 60k high-quality step-level annotations by integrating table verification rationales with tool-based executions. Building on the collected data, we train TaTToo with a dual-stage paradigm: cold-start supervised fine-tuning to capture tool-use reasoning patterns, followed by reinforcement learning with tool-grounded reward shaping to align our model with table-based verification. We provide a comprehensive evaluation of the policy improvement induced by our newly designed PRM. Across 5 challenging tabular reasoning benchmarks covering numerical reasoning, fact-checking, and data analysis, TaTToo improves downstream policy LRMs by 30.9% at inference, surpasses strong PRM baselines such as Qwen-2.5-Math-PRM-72B with only 8B parameters, and demonstrates strong generalizability across diverse TTS strategies.', 'score': 55, 'issue_id': 6298, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'bdefddb943fa9266', 'authors': ['Jiaru Zou', 'Soumya Roy', 'Vinay Kumar Verma', 'Ziyi Wang', 'David Wipf', 'Pan Lu', 'Sumit Negi', 'James Zou', 'Jingrui He'], 'affiliations': ['Amazon', 'Purdue University', 'Stanford University', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2510.06217.jpg', 'data': {'categories': ['#reasoning', '#rl', '#training', '#data', '#benchmark', '#optimization', '#dataset'], 'emoji': '📊', 'ru': {'title': 'TaTToo: Process Reward Model с инструментами для работы с таблицами', 'desc': 'Исследователи представили TaTToo — новую Process Reward Model для улучшения рассуждений над табличными данными в LLM. Существующие PRM плохо справляются с табличными операциями вроде извлечения подтаблиц и работы со схемой данных. TaTToo решает эту проблему через явное моделирование табличных операций и интеграцию инструментов для верификации шагов рассуждений. Модель с 8 миллиардами параметров превосходит базовые PRM с 72B параметрами и улучшает точность на 30.9% в задачах численного анализа, fact-checking и работы с данными.'}, 'en': {'title': 'Revolutionizing Tabular Reasoning with TaTToo!', 'desc': 'TaTToo is a new framework designed to improve how large reasoning models handle tables in machine learning. It focuses on specific operations related to tables, like retrieving sub-tables and interacting with schemas, which previous models struggled with. By using a combination of supervised learning and reinforcement learning, TaTToo provides better guidance for reasoning tasks involving tables. The results show that TaTToo significantly enhances performance on various tabular reasoning challenges, outperforming existing models with fewer parameters.'}, 'zh': {'title': 'TaTToo：提升表格推理的新方法', 'desc': 'TaTToo是一种新颖的基于表格的过程奖励模型，旨在提升表格推理能力。它通过明确处理表格特定操作和整合工具验证，显著改善了现有过程奖励模型的性能。研究表明，现有的过程奖励模型在处理表格推理时存在瓶颈，而TaTToo通过设计可扩展的数据整理管道和双阶段训练方法，克服了这些限制。经过评估，TaTToo在多个表格推理基准测试中表现优异，提升了下游大规模推理模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2509.26328', 'title': 'Fast-dLLM v2: Efficient Block-Diffusion LLM', 'url': 'https://huggingface.co/papers/2509.26328', 'abstract': "Fast-dLLM v2, a block diffusion language model, efficiently converts pretrained autoregressive models for parallel text generation, achieving significant speedup without compromising accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive (AR) large language models (LLMs) have achieved remarkable performance across a wide range of natural language tasks, yet their inherent sequential decoding limits inference efficiency. In this work, we propose Fast-dLLM v2, a carefully designed block diffusion language model (dLLM) that efficiently adapts pretrained AR models into dLLMs for parallel text generation, requiring only approximately 1B tokens of fine-tuning. This represents a 500x reduction in training data compared to full-attention diffusion LLMs such as Dream (580B tokens), while preserving the original model's performance. Our approach introduces a novel training recipe that combines a block diffusion mechanism with a complementary attention mask, enabling blockwise bidirectional context modeling without sacrificing AR training objectives. To further accelerate decoding, we design a hierarchical caching mechanism: a block-level cache that stores historical context representations across blocks, and a sub-block cache that enables efficient parallel generation within partially decoded blocks. Coupled with our parallel decoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR decoding without compromising generation quality. Extensive experiments across diverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR baselines in accuracy, while delivering state-of-the-art efficiency among dLLMs - marking a significant step toward the practical deployment of fast and accurate LLMs. Code and model will be publicly released.", 'score': 32, 'issue_id': 6298, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': 'd003eca6c18f4d37', 'authors': ['Chengyue Wu', 'Hao Zhang', 'Shuchen Xue', 'Shizhe Diao', 'Yonggan Fu', 'Zhijian Liu', 'Pavlo Molchanov', 'Ping Luo', 'Song Han', 'Enze Xie'], 'affiliations': ['MIT', 'NVIDIA', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2509.26328.jpg', 'data': {'categories': ['#training', '#inference', '#open_source', '#diffusion', '#optimization', '#architecture'], 'emoji': '⚡', 'ru': {'title': 'Быстрая параллельная генерация текста через блочную диффузию', 'desc': 'Статья представляет Fast-dLLM v2 — блочную диффузионную языковую модель, которая эффективно преобразует предобученные авторегрессионные LLM для параллельной генерации текста. Ключевое преимущество подхода в том, что требуется всего 1 миллиард токенов для дообучения (в 500 раз меньше, чем у полноценных диффузионных моделей). Модель использует блочный механизм диффузии с иерархической системой кэширования, что позволяет сохранять контекст и генерировать текст параллельно внутри блоков. В результате достигается ускорение в 2.5 раза по сравнению со стандартной авторегрессионной генерацией без потери качества.'}, 'en': {'title': 'Speed Meets Accuracy: Fast-dLLM v2 Revolutionizes Text Generation', 'desc': "Fast-dLLM v2 is a block diffusion language model that transforms pretrained autoregressive models for faster text generation. It achieves this by using a novel block diffusion mechanism and a complementary attention mask, allowing for efficient parallel processing while maintaining the model's accuracy. The model requires significantly less fine-tuning data, only about 1 billion tokens, compared to traditional methods that need hundreds of billions. With a hierarchical caching system, Fast-dLLM v2 can generate text up to 2.5 times faster than standard autoregressive decoding, making it a powerful tool for natural language tasks."}, 'zh': {'title': '快速高效的块扩散语言模型', 'desc': 'Fast-dLLM v2是一种块扩散语言模型，能够高效地将预训练的自回归模型转换为并行文本生成模型。该模型仅需约10亿个标记进行微调，相比于全注意力扩散模型，训练数据减少了500倍，同时保持了原始模型的性能。通过引入块扩散机制和互补注意力掩码，Fast-dLLM v2实现了块级双向上下文建模，并设计了分层缓存机制以加速解码。实验结果表明，Fast-dLLM v2在准确性上与自回归基线相当或更优，同时在效率上达到了最先进的水平。'}}}, {'id': 'https://huggingface.co/papers/2510.05592', 'title': 'In-the-Flow Agentic System Optimization for Effective Planning and Tool\n  Use', 'url': 'https://huggingface.co/papers/2510.05592', 'abstract': 'AgentFlow, a trainable agentic framework with in-the-flow optimization, enhances reasoning in large language models by coordinating specialized modules and outperforms top baselines across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Outcome-driven reinforcement learning has advanced reasoning in large language models (LLMs), but prevailing tool-augmented approaches train a single, monolithic policy that interleaves thoughts and tool calls under full context; this scales poorly with long horizons and diverse tools and generalizes weakly to new scenarios. Agentic systems offer a promising alternative by decomposing work across specialized modules, yet most remain training-free or rely on offline training decoupled from the live dynamics of multi-turn interaction. We introduce AgentFlow, a trainable, in-the-flow agentic framework that coordinates four modules (planner, executor, verifier, generator) through an evolving memory and directly optimizes its planner inside the multi-turn loop. To train on-policy in live environments, we propose Flow-based Group Refined Policy Optimization (Flow-GRPO), which tackles long-horizon, sparse-reward credit assignment by converting multi-turn optimization into a sequence of tractable single-turn policy updates. It broadcasts a single, verifiable trajectory-level outcome to every turn to align local planner decisions with global success and stabilizes learning with group-normalized advantages. Across ten benchmarks, AgentFlow with a 7B-scale backbone outperforms top-performing baselines with average accuracy gains of 14.9% on search, 14.0% on agentic, 14.5% on mathematical, and 4.1% on scientific tasks, even surpassing larger proprietary models like GPT-4o. Further analyses confirm the benefits of in-the-flow optimization, showing improved planning, enhanced tool-calling reliability, and positive scaling with model size and reasoning turns.', 'score': 25, 'issue_id': 6308, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '72e152fd27b3b07f', 'authors': ['Zhuofeng Li', 'Haoxiang Zhang', 'Seungju Han', 'Sheng Liu', 'Jianwen Xie', 'Yu Zhang', 'Yejin Choi', 'James Zou', 'Pan Lu'], 'affiliations': ['Lambda', 'Stanford University', 'Texas A&M University', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2510.05592.jpg', 'data': {'categories': ['#training', '#agents', '#rl', '#optimization', '#reasoning', '#architecture'], 'emoji': '🔄', 'ru': {'title': 'Обучаемая агентная система с оптимизацией в процессе взаимодействия', 'desc': 'AgentFlow — это обучаемый фреймворк, который улучшает рассуждения в LLM через координацию четырёх специализированных модулей: планировщика, исполнителя, верификатора и генератора. В отличие от монолитных подходов, система разделяет задачи между модулями и оптимизирует планировщик прямо в процессе многошаговых взаимодействий с помощью нового алгоритма Flow-GRPO. Этот алгоритм решает проблему распределения наград на длинных горизонтах, преобразуя многошаговую оптимизацию в последовательность одношаговых обновлений политики. AgentFlow с моделью на 7 миллиардов параметров превосходит более крупные проприетарные модели вроде GPT-4o на различных задачах поиска, агентных, математических и научных бенчмарках.'}, 'en': {'title': 'AgentFlow: Optimizing Reasoning in AI with Modular Coordination', 'desc': 'AgentFlow is a new framework designed to improve reasoning in large language models by using a trainable system that coordinates multiple specialized modules. Unlike traditional methods that use a single policy for all tasks, AgentFlow breaks down the process into four modules: planner, executor, verifier, and generator, which work together dynamically. It employs a novel training method called Flow-GRPO that allows the model to learn in real-time during interactions, making it more effective at handling complex tasks with long-term goals. The results show that AgentFlow significantly outperforms existing models on various benchmarks, demonstrating its ability to enhance reasoning and tool usage in AI applications.'}, 'zh': {'title': 'AgentFlow：智能推理的新纪元', 'desc': 'AgentFlow 是一个可训练的智能框架，通过在流程中优化来增强大型语言模型的推理能力。它协调四个专门模块（规划者、执行者、验证者和生成器），并在多轮交互中直接优化规划者。与传统的单一策略方法不同，AgentFlow 通过流式组精炼策略优化（Flow-GRPO）来处理长时间跨度和稀疏奖励的问题。实验结果表明，AgentFlow 在多个基准测试中表现优异，平均准确率提高了14.9%，显示出其在推理和工具调用方面的显著优势。'}}}, {'id': 'https://huggingface.co/papers/2510.03270', 'title': 'CoDA: Coding LM via Diffusion Adaptation', 'url': 'https://huggingface.co/papers/2510.03270', 'abstract': 'CoDA, a 1.7B-parameter diffusion coder, achieves competitive performance with smaller models through confidence-guided sampling and is released with open-source tools.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion language models promise bidirectional context and infilling capabilities that autoregressive coders lack, yet practical systems remain heavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU with a fully open-source training pipeline. CoDA pairs large-scale diffusion pre-training with code-centric mid-training and instruction tuning, enabling confidence-guided sampling that keeps inference latency competitive. On Humaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses diffusion models up to 7B parameters. Our release includes model checkpoints, evaluation harnesses, and TPU training pipelines to accelerate research on lightweight diffusion-based coding assistants.', 'score': 25, 'issue_id': 6298, 'pub_date': '2025-09-27', 'pub_date_card': {'ru': '27 сентября', 'en': 'September 27', 'zh': '9月27日'}, 'hash': '49ba9b2131a56dc8', 'authors': ['Haolin Chen', 'Shiyu Wang', 'Can Qin', 'Bo Pang', 'Zuxin Liu', 'Jielin Qiu', 'Jianguo Zhang', 'Yingbo Zhou', 'Zeyuan Chen', 'Ran Xu', 'Shelby Heinecke', 'Silvio Savarese', 'Caiming Xiong', 'Huan Wang', 'Weiran Yao'], 'affiliations': ['Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.03270.jpg', 'data': {'categories': ['#inference', '#training', '#open_source', '#diffusion', '#small_models', '#dataset'], 'emoji': '🌊', 'ru': {'title': 'Легковесный диффузионный кодер с направленной генерацией', 'desc': 'CoDA — это языковая модель на основе диффузии с 1.7 миллиардами параметров, специально обученная для генерации кода. В отличие от авторегрессивных моделей, диффузионные модели используют двунаправленный контекст и лучше справляются с заполнением пропусков в коде. CoDA обучалась на TPU с использованием открытого пайплайна и применяет confidence-guided sampling для ускорения инференса. На бенчмарках HumanEval и MBPP модель показывает результаты, сопоставимые с диффузионными моделями размером до 7B параметров, при этом оставаясь компактной.'}, 'en': {'title': 'CoDA: Lightweight Diffusion Coding with Competitive Performance', 'desc': 'CoDA is a diffusion coder with 1.7 billion parameters that competes effectively with larger models by using confidence-guided sampling. It leverages a unique training approach that combines large-scale diffusion pre-training with code-focused mid-training and instruction tuning. This allows CoDA to maintain low inference latency while providing advanced capabilities like bidirectional context and infilling. The model is open-source, including tools and checkpoints to support further research in lightweight diffusion-based coding assistants.'}, 'zh': {'title': 'CoDA：轻量级扩散编码的未来', 'desc': 'CoDA是一种具有17亿参数的扩散编码器，通过信心引导采样实现了与更小模型的竞争性能。它结合了大规模的扩散预训练和以代码为中心的中期训练，以及指令调优，从而保持了推理延迟的竞争力。CoDA在Humaneval、MBPP和EvalPlus等基准测试中，表现与高达70亿参数的扩散模型相当或更好。我们发布了模型检查点、评估工具和TPU训练管道，以加速轻量级扩散编码助手的研究。'}}}, {'id': 'https://huggingface.co/papers/2510.04162', 'title': 'Drax: Speech Recognition with Discrete Flow Matching', 'url': 'https://huggingface.co/papers/2510.04162', 'abstract': 'Drax, a discrete flow matching framework for ASR, achieves state-of-the-art recognition accuracy with improved efficiency by constructing an audio-conditioned probability path.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion and flow-based non-autoregressive (NAR) models have shown strong promise in large language modeling, however, their potential for automatic speech recognition (ASR) remains largely unexplored. We propose Drax, a discrete flow matching framework for ASR that enables efficient parallel decoding. To better align training with inference, we construct an audio-conditioned probability path that guides the model through trajectories resembling likely intermediate inference errors, rather than direct random noise to target transitions. Our theoretical analysis links the generalization gap to divergences between training and inference occupancies, controlled by cumulative velocity errors, thereby motivating our design choice. Empirical evaluation demonstrates that our approach attains recognition accuracy on par with state-of-the-art speech models while offering improved accuracy-efficiency trade-offs, highlighting discrete flow matching as a promising direction for advancing NAR ASR.', 'score': 22, 'issue_id': 6309, 'pub_date': '2025-10-05', 'pub_date_card': {'ru': '5 октября', 'en': 'October 5', 'zh': '10月5日'}, 'hash': 'be6cacf8dbe4e879', 'authors': ['Aviv Navon', 'Aviv Shamsian', 'Neta Glazer', 'Yael Segal-Feldman', 'Gill Hetz', 'Joseph Keshet', 'Ethan Fetaya'], 'affiliations': ['aiOla Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.04162.jpg', 'data': {'categories': ['#diffusion', '#audio', '#optimization'], 'emoji': '🎤', 'ru': {'title': 'Drax: эффективное распознавание речи через дискретное flow matching', 'desc': 'Исследователи представили Drax — новый framework для автоматического распознавания речи (ASR) на основе discrete flow matching. Вместо традиционного авторегрессионного подхода используется параллельное декодирование с вероятностным путём, обусловленным аудио. Ключевая идея — обучать модель на траекториях, похожих на ошибки инференса, а не на случайном шуме, что уменьшает разрыв между обучением и применением. Результаты показывают точность на уровне state-of-the-art моделей при улучшенном балансе между качеством и скоростью работы.'}, 'en': {'title': 'Drax: Efficient and Accurate ASR with Discrete Flow Matching', 'desc': 'Drax is a new framework designed for automatic speech recognition (ASR) that uses discrete flow matching to enhance recognition accuracy and efficiency. It introduces an audio-conditioned probability path that helps the model navigate through common inference errors, improving the decoding process. The framework addresses the gap between training and inference by managing cumulative velocity errors, which helps in better generalization. Empirical results show that Drax achieves competitive accuracy compared to leading speech models while also being more efficient, suggesting a valuable advancement in non-autoregressive ASR techniques.'}, 'zh': {'title': 'Drax：提升ASR效率与准确性的离散流匹配框架', 'desc': 'Drax是一种用于自动语音识别（ASR）的离散流匹配框架，通过构建音频条件概率路径，实现了更高的识别准确率和效率。该框架支持高效的并行解码，能够更好地将训练与推理对齐。我们通过理论分析将泛化误差与训练和推理的占用差异联系起来，从而推动了设计选择。实证评估表明，Drax在识别准确性上与最先进的语音模型相当，同时在准确性和效率之间提供了更好的权衡。'}}}, {'id': 'https://huggingface.co/papers/2510.04081', 'title': 'Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model\n  Reasoning', 'url': 'https://huggingface.co/papers/2510.04081', 'abstract': "Caco, a code-assisted chain-of-thought framework, automates the generation of high-quality, verifiable, and diverse reasoning data, enhancing the performance of large language models on mathematical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning capability is pivotal for Large Language Models (LLMs) to solve complex tasks, yet achieving reliable and scalable reasoning remains challenging. While Chain-of-Thought (CoT) prompting has become a mainstream approach, existing methods often suffer from uncontrolled generation, insufficient quality, and limited diversity in reasoning paths. Recent efforts leverage code to enhance CoT by grounding reasoning in executable steps, but such methods are typically constrained to predefined mathematical problems, hindering scalability and generalizability. In this work, we propose Caco (Code-Assisted Chain-of-ThOught), a novel framework that automates the synthesis of high-quality, verifiable, and diverse instruction-CoT reasoning data through code-driven augmentation. Unlike prior work, Caco first fine-tunes a code-based CoT generator on existing math and programming solutions in a unified code format, then scales the data generation to a large amount of diverse reasoning traces. Crucially, we introduce automated validation via code execution and rule-based filtering to ensure logical correctness and structural diversity, followed by reverse-engineering filtered outputs into natural language instructions and language CoTs to enrich task adaptability. This closed-loop process enables fully automated, scalable synthesis of reasoning data with guaranteed executability. Experiments on our created Caco-1.3M dataset demonstrate that Caco-trained models achieve strong competitive performance on mathematical reasoning benchmarks, outperforming existing strong baselines. Further analysis reveals that Caco's code-anchored verification and instruction diversity contribute to superior generalization across unseen tasks. Our work establishes a paradigm for building self-sustaining, trustworthy reasoning systems without human intervention.", 'score': 18, 'issue_id': 6299, 'pub_date': '2025-10-05', 'pub_date_card': {'ru': '5 октября', 'en': 'October 5', 'zh': '10月5日'}, 'hash': '4f2356e6d1057b79', 'authors': ['Honglin Lin', 'Qizhi Pei', 'Xin Gao', 'Zhuoshi Pan', 'Yu Li', 'Juntao Li', 'Conghui He', 'Lijun Wu'], 'affiliations': ['OpenDataLab, Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2510.04081.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#benchmark', '#math', '#data', '#training'], 'emoji': '🔢', 'ru': {'title': 'Код как основа для автоматической генерации качественных рассуждений', 'desc': 'Caco — это фреймворк для автоматической генерации высококачественных данных для обучения математическим рассуждениям в LLM с использованием кода. Система создает цепочки рассуждений (chain-of-thought) в виде исполняемого кода, автоматически проверяет их корректность через выполнение, а затем преобразует обратно в естественный язык. Благодаря этому подходу был создан датасет Caco-1.3M, который обеспечивает разнообразие и верифицируемость обучающих примеров без участия человека. Эксперименты показали, что модели, обученные на этих данных, превосходят существующие решения на математических бенчмарках и лучше генерализуются на новые задачи.'}, 'en': {'title': 'Caco: Automating High-Quality Reasoning for LLMs', 'desc': 'Caco is a new framework designed to improve the reasoning abilities of large language models (LLMs) in solving mathematical problems. It automates the creation of high-quality reasoning data by using code to generate diverse and verifiable reasoning paths. This approach addresses the limitations of traditional Chain-of-Thought (CoT) methods, which often struggle with quality and scalability. By incorporating automated validation and a closed-loop synthesis process, Caco ensures that the generated reasoning data is both executable and adaptable to various tasks, leading to better performance on mathematical reasoning benchmarks.'}, 'zh': {'title': 'Caco：自动化高质量推理数据生成的创新框架', 'desc': 'Caco是一个代码辅助的思维链框架，旨在自动生成高质量、可验证和多样化的推理数据，从而提升大型语言模型在数学推理任务上的表现。该框架通过代码驱动的增强方法，解决了现有思维链方法在生成控制、质量不足和推理路径有限等问题。Caco首先在统一的代码格式上微调代码基础的思维链生成器，然后扩展数据生成以获得大量多样化的推理轨迹。通过代码执行和基于规则的过滤，Caco确保了逻辑正确性和结构多样性，从而实现了完全自动化和可扩展的推理数据合成。'}}}, {'id': 'https://huggingface.co/papers/2510.06052', 'title': 'MixReasoning: Switching Modes to Think', 'url': 'https://huggingface.co/papers/2510.06052', 'abstract': 'MixReasoning dynamically adjusts reasoning depth in models to improve efficiency without sacrificing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning models enhance performance by tackling problems in a step-by-step manner, decomposing them into sub-problems and exploring long chains of thought before producing an answer. However, applying extended reasoning to every step introduces substantial redundancy, as sub-problems vary widely in difficulty and complexity: a small number of pivotal steps are genuinely challenging and decisive for the final answer, while many others only involve straightforward revisions or simple computations. Therefore, a natural idea is to endow reasoning models with the ability to adaptively respond to this variation, rather than treating all steps with the same level of elaboration. To this end, we propose MixReasoning, a framework that dynamically adjusts the depth of reasoning within a single response. The resulting chain of thought then becomes a mixture of detailed reasoning on difficult steps and concise inference on simpler ones. Experiments on GSM8K, MATH-500, and AIME show that MixReasoning shortens reasoning length and substantially improves efficiency without compromising accuracy.', 'score': 16, 'issue_id': 6303, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'b40a91a849b36453', 'authors': ['Haiquan Lu', 'Gongfan Fang', 'Xinyin Ma', 'Qi Li', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2510.06052.jpg', 'data': {'categories': ['#reasoning', '#math', '#training'], 'emoji': '🎚️', 'ru': {'title': 'Адаптивная глубина рассуждений: думай глубоко только там, где это нужно', 'desc': 'Статья представляет MixReasoning — фреймворк, который динамически адаптирует глубину рассуждений модели в зависимости от сложности каждого шага решения задачи. Вместо того чтобы применять одинаково детальное пошаговое рассуждение ко всем этапам, система определяет сложные ключевые шаги, требующие развёрнутого анализа, и простые шаги, где достаточно краткого вывода. Такой подход значительно сокращает длину chain of thought и повышает эффективность работы модели без потери точности. Эксперименты на датасетах GSM8K, MATH-500 и AIME подтверждают преимущества адаптивного рассуждения.'}, 'en': {'title': 'Dynamic Reasoning Depth for Efficient AI Performance', 'desc': 'MixReasoning is a novel framework that optimizes reasoning depth in machine learning models to enhance efficiency while maintaining accuracy. It recognizes that not all reasoning steps require the same level of detail, as some are more complex than others. By dynamically adjusting the depth of reasoning, MixReasoning allows models to focus on challenging sub-problems with detailed analysis, while simplifying the approach for easier tasks. Experiments demonstrate that this method reduces the overall reasoning length and significantly boosts performance on various benchmarks without sacrificing the quality of the answers.'}, 'zh': {'title': '动态调整推理深度，提高效率与准确性', 'desc': 'MixReasoning是一种动态调整推理深度的框架，旨在提高模型的效率而不牺牲准确性。传统的推理模型通过逐步解决问题来提升性能，但在每一步都进行深入推理会导致冗余，因为子问题的难度和复杂性差异很大。MixReasoning允许模型根据问题的复杂性自适应地调整推理深度，从而在困难步骤上进行详细推理，而在简单步骤上进行简洁推理。实验结果表明，MixReasoning在不降低准确性的情况下，显著缩短了推理长度，提高了效率。'}}}, {'id': 'https://huggingface.co/papers/2510.06062', 'title': 'ASPO: Asymmetric Importance Sampling Policy Optimization', 'url': 'https://huggingface.co/papers/2510.06062', 'abstract': 'ASPO addresses the imbalance in token weighting during OSRL by flipping Importance Sampling ratios and incorporating a soft dual-clipping mechanism, improving training stability and performance in LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent Large Language Model (LLM) post-training methods rely on token-level clipping mechanisms during Reinforcement Learning (RL). However, we identify a fundamental flaw in this Outcome-Supervised RL (OSRL) paradigm: the Importance Sampling (IS) ratios of positive-advantage tokens are mismatched, leading to unbalanced token weighting for positive and negative tokens. This mismatch suppresses the update of low-probability tokens while over-amplifying already high-probability ones. To address this, we propose Asymmetric Importance Sampling Policy Optimization (ASPO), which uses a simple yet effective strategy that flips the IS ratios of positive-advantage tokens, aligning their update direction with the learning dynamics of negative ones. AIS further incorporates a soft dual-clipping mechanism to stabilize extreme updates while maintaining gradient flow. Comprehensive experiments on coding and mathematical reasoning benchmarks demonstrate that ASPO significantly mitigates premature convergence, improves training stability, and enhances final performance over strong GRPO-based baselines. Our analysis provides new insights into the role of token-level weighting in OSRL and highlights the critical importance of correcting IS in LLM RL. The code and models of ASPO are available at https://github.com/wizard-III/Archer2.0.', 'score': 10, 'issue_id': 6301, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '3090778074ce994d', 'authors': ['Jiakang Wang', 'Runze Liu', 'Lei Lin', 'Wenping Hu', 'Xiu Li', 'Fuzheng Zhang', 'Guorui Zhou', 'Kun Gai'], 'affiliations': ['Kuaishou Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.06062.jpg', 'data': {'categories': ['#training', '#rl', '#optimization'], 'emoji': '⚖️', 'ru': {'title': 'Асимметричная важность токенов для стабильного обучения LLM', 'desc': 'Исследователи обнаружили фундаментальную проблему в методах обучения с подкреплением для больших языковых моделей: несбалансированное взвешивание токенов с положительными и отрицательными преимуществами. Традиционные подходы подавляют обновление низковероятностных токенов и чрезмерно усиливают высоковероятностные. Предложенный метод ASPO решает эту проблему путем «переворачивания» коэффициентов Importance Sampling для токенов с положительным преимуществом и использования механизма мягкого двойного клиппинга. Эксперименты показали значительное улучшение стабильности обучения и итоговой производительности на задачах программирования и математических рассуждений.'}, 'en': {'title': 'Balancing Token Weighting for Better LLM Training', 'desc': 'The paper introduces Asymmetric Importance Sampling Policy Optimization (ASPO) to improve training in Large Language Models (LLMs) during Outcome-Supervised Reinforcement Learning (OSRL). It identifies a problem with the Importance Sampling ratios, which causes an imbalance in how positive and negative tokens are weighted, leading to ineffective learning. ASPO addresses this by flipping the ratios for positive-advantage tokens and adding a soft dual-clipping mechanism to stabilize updates. Experiments show that ASPO enhances training stability and performance, reducing premature convergence compared to existing methods.'}, 'zh': {'title': '优化令牌加权，提升训练稳定性', 'desc': 'ASPO（不对称重要性采样策略优化）解决了在结果监督强化学习（OSRL）中令牌加权不平衡的问题。通过翻转正优势令牌的重要性采样比率，ASPO使得正负令牌的更新方向一致，从而提高了训练的稳定性和性能。此外，ASPO还引入了一种软双剪切机制，以稳定极端更新并保持梯度流动。实验结果表明，ASPO在编码和数学推理基准测试中显著改善了训练效果，减少了过早收敛现象。'}}}, {'id': 'https://huggingface.co/papers/2510.05571', 'title': 'Presenting a Paper is an Art: Self-Improvement Aesthetic Agents for\n  Academic Presentations', 'url': 'https://huggingface.co/papers/2510.05571', 'abstract': 'EvoPresent, a self-improvement agent framework using multi-task reinforcement learning, enhances academic paper promotion by generating coherent narratives, aesthetically pleasing designs, and realistic presentations, supported by a comprehensive benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t The promotion of academic papers has become an important means of enhancing research visibility. However, existing automated methods struggle limited storytelling, insufficient aesthetic quality, and constrained self-adjustment, making it difficult to achieve efficient and engaging dissemination. At the heart of those challenges is a simple principle: there is no way to improve it when you cannot evaluate it right. To address this, we introduce EvoPresent, a self-improvement agent framework that unifies coherent narratives, aesthetic-aware designs, and realistic presentation delivery via virtual characters. Central to EvoPresent is PresAesth, a multi-task reinforcement learning (RL) aesthetic model that provides reliable aesthetic scoring, defect adjustment, and comparative feedback, enabling iterative self-improvement even under limited aesthetic training data. To systematically evaluate the methods, we introduce EvoPresent Benchmark, a comprehensive benchmark comprising: Presentation Generation Quality, built on 650 top-tier AI conference papers with multimodal resources (slides, videos and scripts) to assess both content and design; and Aesthetic Awareness, consisting of 2,000 slide pairs with varying aesthetic levels, supporting joint training and evaluation on scoring, defect adjustment, and comparison. Our findings highlight that (i) High-quality feedback is essential for agent self-improvement, while initial capability alone does not guarantee effective self-correction. (ii) Automated generation pipelines exhibit a trade-off between visual design and content construction. (iii) Multi-task RL training shows stronger generalization in aesthetic awareness tasks.', 'score': 10, 'issue_id': 6309, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '8aef2a97aec62eb1', 'authors': ['Chengzhi Liu', 'Yuzhe Yang', 'Kaiwen Zhou', 'Zhen Zhang', 'Yue Fan', 'Yannan Xie', 'Peng Qi', 'Xin Eric Wang'], 'affiliations': ['Uniphore', 'University of California, Santa Barbara', 'University of California, Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2510.05571.jpg', 'data': {'categories': ['#benchmark', '#story_generation', '#rl', '#multimodal', '#agents', '#optimization', '#games'], 'emoji': '🎨', 'ru': {'title': 'Самосовершенствующийся агент для создания презентаций научных статей', 'desc': 'EvoPresent — это фреймворк агента, который автоматически создаёт презентации научных работ, используя multi-task reinforcement learning для улучшения качества. В основе системы лежит модель PresAesth, которая оценивает эстетику слайдов, исправляет недостатки и даёт сравнительную обратную связь для итеративного самосовершенствования. Авторы представили комплексный бенчмарк на базе 650 статей с топовых AI-конференций и 2000 пар слайдов для оценки качества контента и дизайна. Исследование показало, что качественная обратная связь критически важна для самоулучшения агента, а multi-task RL демонстрирует лучшую генерализацию в задачах оценки эстетики.'}, 'en': {'title': 'Enhancing Academic Paper Promotion with EvoPresent: A Self-Improvement Framework', 'desc': 'EvoPresent is a framework designed to improve the promotion of academic papers using multi-task reinforcement learning. It addresses challenges in storytelling, aesthetic quality, and self-adjustment by integrating coherent narratives and visually appealing designs. The core of EvoPresent is the PresAesth model, which provides aesthetic scoring and feedback for iterative self-improvement. The framework is evaluated using the EvoPresent Benchmark, which assesses presentation quality and aesthetic awareness based on a large dataset of academic resources.'}, 'zh': {'title': 'EvoPresent：提升学术论文推广的智能代理框架', 'desc': 'EvoPresent是一个自我改进的智能代理框架，利用多任务强化学习来提升学术论文的推广效果。它通过生成连贯的叙述、美观的设计和逼真的演示，解决了现有自动化方法在讲故事、审美质量和自我调整方面的不足。EvoPresent的核心是PresAesth，一个多任务强化学习美学模型，能够提供可靠的美学评分和缺陷调整，支持在有限的美学训练数据下进行迭代自我改进。通过EvoPresent基准测试，我们评估了演示生成质量和美学意识，发现高质量反馈对代理自我改进至关重要。'}}}, {'id': 'https://huggingface.co/papers/2509.23379', 'title': 'CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical\n  Contrastive Decoding', 'url': 'https://huggingface.co/papers/2509.23379', 'abstract': 'Clinical Contrastive Cecoding (CCD) enhances radiology report generation by integrating structured clinical signals, reducing medical hallucinations without altering the base MLLM.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have recently achieved remarkable progress in radiology by integrating visual perception with natural language understanding. However, they often generate clinically unsupported descriptions, known as medical hallucinations, which pose serious risks in medical applications that demand accuracy and image-grounded outputs. Through empirical analysis, we find that prompt-induced hallucinations remain prevalent in radiology MLLMs, largely due to over-sensitivity to clinical sections. To address this, we introduce Clinical Contrastive Cecoding (CCD), a training-free and retrieval-free inference framework that integrates structured clinical signals from task-specific radiology expert models. CCD introduces a dual-stage contrastive mechanism to refine token-level logits during generation, thereby enhancing clinical fidelity without modifying the base MLLM. Experiments on three datasets and multiple models demonstrate that CCD consistently improves overall performance on radiology report generation (RRG). On the MIMIC-CXR dataset, it yields up to a 17% improvement in RadGraph-F1 when applied to state-of-the-art RRG models. Our approach provides a lightweight and generalisable solution for mitigating medical hallucinations, effectively bridging expert models and MLLMs in radiology.', 'score': 10, 'issue_id': 6308, 'pub_date': '2025-09-27', 'pub_date_card': {'ru': '27 сентября', 'en': 'September 27', 'zh': '9月27日'}, 'hash': '5dc19169d1633b88', 'authors': ['Xi Zhang', 'Zaiqiao Meng', 'Jake Lever', 'Edmond S. L. Ho'], 'affiliations': ['School of Computing Science, University of Glasgow, UK'], 'pdf_title_img': 'assets/pdf/title_img/2509.23379.jpg', 'data': {'categories': ['#healthcare', '#training', '#multimodal', '#hallucinations', '#data', '#science'], 'emoji': '\U0001fa7b', 'ru': {'title': 'Борьба с медицинскими галлюцинациями в AI-радиологии через контрастивное декодирование', 'desc': 'Статья представляет метод Clinical Contrastive Decoding (CCD) для улучшения генерации радиологических отчётов с помощью мультимодальных LLM. Проблема заключается в том, что модели часто генерируют клинически необоснованные описания (медицинские галлюцинации), что опасно в медицинских приложениях. CCD использует структурированные клинические сигналы от экспертных моделей и двухэтапный контрастивный механизм для уточнения генерации на уровне токенов. Метод не требует дообучения модели и показывает улучшение до 17% по метрике RadGraph-F1 на датасете MIMIC-CXR.'}, 'en': {'title': 'Enhancing Radiology Reports with Clinical Contrastive Cecoding', 'desc': 'Clinical Contrastive Cecoding (CCD) is a novel framework designed to improve the accuracy of radiology report generation by incorporating structured clinical signals. It addresses the issue of medical hallucinations, which are incorrect or unsupported descriptions generated by multimodal large language models (MLLMs). CCD employs a dual-stage contrastive mechanism to refine the output of these models without altering their underlying architecture. Experimental results show that CCD significantly enhances performance, achieving up to a 17% improvement in clinical accuracy on the MIMIC-CXR dataset.'}, 'zh': {'title': '临床对比编码：提升放射学报告的准确性', 'desc': '临床对比编码（CCD）通过整合结构化临床信号，增强了放射学报告生成，减少了医学幻觉的发生，而不改变基础的大型多模态语言模型（MLLM）。研究发现，放射学MLLM在生成过程中容易受到临床部分的过度敏感影响，导致生成不支持临床的描述。CCD采用双阶段对比机制，在生成过程中精炼标记级别的逻辑值，从而提高临床准确性。实验结果表明，CCD在多个数据集和模型上均能显著提升放射学报告生成的整体性能。'}}}, {'id': 'https://huggingface.co/papers/2510.06208', 'title': 'ShapeGen4D: Towards High Quality 4D Shape Generation from Videos', 'url': 'https://huggingface.co/papers/2510.06208', 'abstract': 'A video-to-4D shape generation framework uses temporal attention, time-aware point sampling, and noise sharing to produce dynamic 3D representations from videos, enhancing temporal stability and perceptual fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t Video-conditioned 4D shape generation aims to recover time-varying 3D geometry and view-consistent appearance directly from an input video. In this work, we introduce a native video-to-4D shape generation framework that synthesizes a single dynamic 3D representation end-to-end from the video. Our framework introduces three key components based on large-scale pre-trained 3D models: (i) a temporal attention that conditions generation on all frames while producing a time-indexed dynamic representation; (ii) a time-aware point sampling and 4D latent anchoring that promote temporally consistent geometry and texture; and (iii) noise sharing across frames to enhance temporal stability. Our method accurately captures non-rigid motion, volume changes, and even topological transitions without per-frame optimization. Across diverse in-the-wild videos, our method improves robustness and perceptual fidelity and reduces failure modes compared with the baselines.', 'score': 9, 'issue_id': 6308, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '08bc6b783f4e2e13', 'authors': ['Jiraphon Yenphraphai', 'Ashkan Mirzaei', 'Jianqi Chen', 'Jiaxu Zou', 'Sergey Tulyakov', 'Raymond A. Yeh', 'Peter Wonka', 'Chaoyang Wang'], 'affiliations': ['KAUST', 'Purdue University', 'Snap'], 'pdf_title_img': 'assets/pdf/title_img/2510.06208.jpg', 'data': {'categories': ['#video', '#3d'], 'emoji': '🎬', 'ru': {'title': 'От видео к динамической 3D-форме: генерация 4D-представлений с временной согласованностью', 'desc': 'Статья представляет новый фреймворк для генерации 4D-форм из видео, который создаёт динамическое 3D-представление напрямую из входного видео. Метод использует temporal attention для учёта всех кадров одновременно, time-aware сэмплирование точек для временной согласованности геометрии и текстур, а также разделение шума между кадрами для стабильности. Подход построен на основе предобученных 3D-моделей большого масштаба и работает end-to-end без покадровой оптимизации. Система успешно справляется с non-rigid деформациями, изменениями объёма и даже топологическими трансформациями объектов, демонстрируя улучшенную робастность на реальных видео.'}, 'en': {'title': 'Transforming Videos into Dynamic 3D Shapes with Precision', 'desc': 'This paper presents a novel framework for generating dynamic 3D shapes from videos, referred to as video-to-4D shape generation. It employs temporal attention to ensure that the generated 3D representation is consistent across all frames of the video. The framework also incorporates time-aware point sampling and noise sharing to enhance the stability and quality of the output, capturing complex motions and changes in geometry. Overall, the approach significantly improves the robustness and visual fidelity of 3D reconstructions compared to existing methods.'}, 'zh': {'title': '视频驱动的动态3D形状生成', 'desc': '本文提出了一种视频到4D形状生成的框架，旨在从视频中生成动态的3D表示。该框架利用时间注意力机制、时间感知点采样和噪声共享等技术，增强了生成结果的时间稳定性和感知真实感。通过大规模预训练的3D模型，该方法能够准确捕捉非刚性运动、体积变化和拓扑转变。与基线方法相比，我们的方法在多样化的视频中表现出更强的鲁棒性和感知真实感。'}}}, {'id': 'https://huggingface.co/papers/2510.06131', 'title': 'Discrete Diffusion Models with MLLMs for Unified Medical Multimodal\n  Generation', 'url': 'https://huggingface.co/papers/2510.06131', 'abstract': 'MeDiM, a medical discrete diffusion model, integrates multimodal biomedical data by learning shared distributions across images, text, and clinical notes, achieving high-fidelity generation and enhanced downstream performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in generative medical models are constrained by modality-specific scenarios that hinder the integration of complementary evidence from imaging, pathology, and clinical notes. This fragmentation limits their evolution into foundation models that can learn and reason across the full spectrum of biomedical data. We propose MeDiM, the first medical discrete diffusion model that learns shared distributions across modalities without modality-specific components. MeDiM unifies multiple generative tasks: translating between images and text, and jointly producing image-report pairs across domains in response to prompts. Built on a discrete diffusion framework, MeDiM bridges vision and language representations through a shared probabilistic space. To enable unified and flexible medical generation, we employ a multimodal large language model (MLLM) as the diffusion backbone, leveraging its prior knowledge and cross-modal reasoning. Two key designs are introduced: (1) removing the causal attention mask for bidirectional context, and (2) injecting continuous timestep embeddings for diffusion awareness. Experiments demonstrate high-fidelity medical generation (FID 16.60 on MIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR 0.2650 and 0.2580). Jointly generated image-report pairs further enhance downstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2, plus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supports coherent and clinically grounded multimodal outputs.', 'score': 9, 'issue_id': 6299, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '0641a3ba669f441d', 'authors': ['Jiawei Mao', 'Yuhan Wang', 'Lifeng Chen', 'Can Zhao', 'Yucheng Tang', 'Dong Yang', 'Liangqiong Qu', 'Daguang Xu', 'Yuyin Zhou'], 'affiliations': ['NVIDIA', 'UC Santa Cruz', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.06131.jpg', 'data': {'categories': ['#diffusion', '#science', '#healthcare', '#multimodal'], 'emoji': '🏥', 'ru': {'title': 'Единая диффузионная модель для всех медицинских модальностей', 'desc': 'MeDiM — это первая медицинская модель дискретной диффузии, которая объединяет разные типы биомедицинских данных (изображения, текст и клинические записи) в едином вероятностном пространстве без модально-специфичных компонентов. В основе модели лежит мультимодальная LLM с модифицированной архитектурой: убрана каузальная маска внимания для двунаправленного контекста и добавлены непрерывные timestep embeddings для диффузионного процесса. Модель способна переводить между изображениями и текстом, а также генерировать согласованные пары медицинских изображений и отчётов по текстовым запросам. Эксперименты показывают высокое качество генерации и улучшение результатов на downstream-задачах при использовании совместно сгенерированных пар изображение-отчёт.'}, 'en': {'title': 'Unifying Biomedical Data with MeDiM: A Multimodal Diffusion Revolution', 'desc': "MeDiM is a novel medical discrete diffusion model designed to integrate various types of biomedical data, such as images, text, and clinical notes. It overcomes the limitations of traditional models that operate within specific modalities by learning shared distributions across all data types. By utilizing a multimodal large language model as its backbone, MeDiM can generate high-quality medical outputs and translate between different modalities effectively. The model's innovative design features, like bidirectional context and continuous timestep embeddings, contribute to its superior performance in generating coherent and clinically relevant multimodal outputs."}, 'zh': {'title': 'MeDiM：医学多模态生成的创新桥梁', 'desc': 'MeDiM是一种医学离散扩散模型，能够通过学习图像、文本和临床记录之间的共享分布来整合多模态生物医学数据。该模型克服了传统生成医学模型在特定模态场景下的局限性，实现了高保真度的生成和增强的下游性能。MeDiM统一了多种生成任务，包括图像与文本之间的翻译，以及根据提示共同生成跨领域的图像-报告对。通过使用多模态大型语言模型作为扩散骨干，MeDiM在一个共享的概率空间中桥接了视觉和语言表示。'}}}, {'id': 'https://huggingface.co/papers/2510.06182', 'title': 'Mixing Mechanisms: How Language Models Retrieve Bound Entities\n  In-Context', 'url': 'https://huggingface.co/papers/2510.06182', 'abstract': 'Language models use positional, lexical, and reflexive mechanisms to bind and retrieve entities, with a causal model achieving high accuracy in predicting next tokens across various tasks and input lengths.  \t\t\t\t\tAI-generated summary \t\t\t\t A key component of in-context reasoning is the ability of language models (LMs) to bind entities for later retrieval. For example, an LM might represent "Ann loves pie" by binding "Ann" to "pie", allowing it to later retrieve "Ann" when asked "Who loves pie?" Prior research on short lists of bound entities found strong evidence that LMs implement such retrieval via a positional mechanism, where "Ann" is retrieved based on its position in context. In this work, we find that this mechanism generalizes poorly to more complex settings; as the number of bound entities in context increases, the positional mechanism becomes noisy and unreliable in middle positions. To compensate for this, we find that LMs supplement the positional mechanism with a lexical mechanism (retrieving "Ann" using its bound counterpart "pie") and a reflexive mechanism (retrieving "Ann" through a direct pointer). Through extensive experiments on nine models and ten binding tasks, we uncover a consistent pattern in how LMs mix these mechanisms to drive model behavior. We leverage these insights to develop a causal model combining all three mechanisms that estimates next token distributions with 95% agreement. Finally, we show that our model generalizes to substantially longer inputs of open-ended text interleaved with entity groups, further demonstrating the robustness of our findings in more natural settings. Overall, our study establishes a more complete picture of how LMs bind and retrieve entities in-context.', 'score': 8, 'issue_id': 6300, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'ebd0b1b2a51e6c0a', 'authors': ['Yoav Gur-Arieh', 'Mor Geva', 'Atticus Geiger'], 'affiliations': ['Blavatnik School of Computer Science and AI, Tel Aviv University', 'Goodfire', 'Pr(Ai)2R Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.06182.jpg', 'data': {'categories': ['#reasoning', '#long_context', '#data', '#multimodal', '#interpretability', '#architecture'], 'emoji': '🔗', 'ru': {'title': 'Три механизма связывания сущностей в языковых моделях', 'desc': 'Исследование показывает, как языковые модели связывают и извлекают сущности в контексте, используя три различных механизма. Позиционный механизм работает хорошо для коротких списков, но становится ненадёжным при увеличении количества связанных сущностей. LLM компенсируют это лексическим механизмом (поиск через связанные слова) и рефлексивным механизмом (прямые указатели). Разработанная каузальная модель, объединяющая все три механизма, предсказывает следующий токен с точностью 95% и работает даже на длинных текстах.'}, 'en': {'title': 'Unraveling Entity Binding in Language Models', 'desc': 'This paper explores how language models (LMs) bind and retrieve entities during in-context reasoning. It identifies three mechanisms used by LMs: positional, lexical, and reflexive, which help in accurately predicting the next tokens. The study reveals that while the positional mechanism works well for short lists of entities, it struggles with longer contexts, leading to the use of lexical and reflexive mechanisms for better retrieval. By developing a causal model that integrates these mechanisms, the authors achieve high accuracy in predicting token distributions across various tasks and input lengths.'}, 'zh': {'title': '语言模型的实体绑定与检索机制', 'desc': '本研究探讨了语言模型如何在上下文中绑定和检索实体。我们发现，传统的基于位置的机制在复杂情况下表现不佳，因此语言模型还使用了词汇机制和反射机制来提高检索的准确性。通过对九种模型和十个绑定任务的广泛实验，我们揭示了语言模型如何混合使用这些机制来驱动模型行为。最终，我们开发了一个结合三种机制的因果模型，能够在更长的输入文本中有效地预测下一个标记。'}}}, {'id': 'https://huggingface.co/papers/2510.03506', 'title': 'OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit\n  Flows', 'url': 'https://huggingface.co/papers/2510.03506', 'abstract': 'OneFlow, a non-autoregressive multimodal model, achieves superior performance in text-image generation and understanding tasks with reduced computational cost compared to autoregressive and diffusion-based models.  \t\t\t\t\tAI-generated summary \t\t\t\t We present OneFlow, the first non-autoregressive multimodal model that enables variable-length and concurrent mixed-modal generation. Unlike autoregressive models that enforce rigid causal ordering between text and image generation, OneFlow combines an insertion-based Edit Flow for discrete text tokens with Flow Matching for image latents. OneFlow enables concurrent text-image synthesis with hierarchical sampling that prioritizes content over grammar. Through controlled experiments across model sizes from 1B to 8B, we demonstrate that OneFlow outperforms autoregressive baselines on both generation and understanding tasks while using up to 50% fewer training FLOPs. OneFlow surpasses both autoregressive and diffusion-based approaches while unlocking new capabilities for concurrent generation, iterative refinement, and natural reasoning-like generation.', 'score': 8, 'issue_id': 6306, 'pub_date': '2025-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': 'faa9eab196ef87b3', 'authors': ['John Nguyen', 'Marton Havasi', 'Tariq Berrada', 'Luke Zettlemoyer', 'Ricky T. Q. Chen'], 'affiliations': ['CNRS', 'FAIR at Meta', 'Grenoble INP', 'Inria', 'LJK, France', 'Univ. Grenoble Alpes'], 'pdf_title_img': 'assets/pdf/title_img/2510.03506.jpg', 'data': {'categories': ['#training', '#optimization', '#games', '#architecture', '#diffusion', '#multimodal'], 'emoji': '🔄', 'ru': {'title': 'Одновременная генерация текста и изображений без авторегрессии', 'desc': 'OneFlow — это первая неавторегрессионная мультимодальная модель, способная генерировать текст и изображения одновременно и переменной длины. Модель комбинирует Edit Flow для дискретных текстовых токенов с Flow Matching для латентных представлений изображений, что позволяет избежать жёсткого причинно-следственного порядка генерации. OneFlow превосходит авторегрессионные базовые модели в задачах генерации и понимания, используя на 50% меньше вычислительных ресурсов при обучении. Модель открывает новые возможности для одновременной генерации, итеративного улучшения результатов и генерации, похожей на естественное рассуждение.'}, 'en': {'title': 'OneFlow: Revolutionizing Text-Image Generation with Efficiency and Flexibility', 'desc': 'OneFlow is a groundbreaking non-autoregressive multimodal model designed for generating and understanding text and images. It utilizes an innovative insertion-based Edit Flow for text and Flow Matching for images, allowing for simultaneous generation without the strict sequence required by traditional autoregressive models. This approach not only enhances the efficiency of the generation process but also improves performance on various tasks while consuming significantly fewer computational resources. Through extensive testing, OneFlow has shown to outperform existing models, paving the way for more advanced and flexible multimodal applications.'}, 'zh': {'title': 'OneFlow：高效的文本图像生成新模式', 'desc': 'OneFlow是一种非自回归的多模态模型，能够同时生成文本和图像，且计算成本低于自回归和扩散模型。与自回归模型不同，OneFlow不需要严格的因果顺序，而是结合了基于插入的编辑流和图像潜变量的流匹配。该模型通过分层采样优先考虑内容而非语法，实现了文本和图像的并发合成。实验结果表明，OneFlow在生成和理解任务上均优于自回归基线，同时训练所需的FLOPs减少了50%。'}}}, {'id': 'https://huggingface.co/papers/2510.05485', 'title': 'TensorBLEU: Vectorized GPU-based BLEU Score Implementation for\n  Per-Sentence In-Training Evaluation', 'url': 'https://huggingface.co/papers/2510.05485', 'abstract': 'TensorBLEU is a GPU-accelerated BLEU metric implementation for efficient in-training evaluation of natural language processing models, offering significant speedups over CPU-based methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern natural language processing models have achieved unprecedented scale, yet the tools for their evaluation often remain a computational bottleneck, limiting the pace of research. This is particularly acute for in-training evaluation metrics, such as per-sentence reward signals in Reinforcement Learning, which must operate efficiently on batches of token IDs directly on the GPU. In this paper, we introduce TensorBLEU, a novel implementation of the BLEU metric designed from the ground up for this specific use case. Our approach is fully vectorized for GPU-accelerated, per-sentence computation within PyTorch and introduces a memory-efficient counting mechanism. By creating a compact, batch-specific dictionary of n-grams using torch.unique, our method avoids the prohibitive memory costs of traditional hashing-based vectorization, making it practical for large-vocabulary models. We benchmark TensorBLEU against NLTK, the standard library for token-ID-based BLEU calculation on the CPU. Experiments show that TensorBLEU provides speedups of over 13x on consumer-grade GPUs (NVIDIA T4) and exceeding 40x on data-center-class hardware (NVIDIA A100). This performance transforms a significant bottleneck into a negligible part of the training loop. By clearly defining its role as a "Token-ID BLEU" for development purposes and open-sourcing our implementation, we provide a powerful tool for accelerating research in areas like RL-based model fine-tuning.', 'score': 7, 'issue_id': 6306, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'af039fb3475e6646', 'authors': ['Adam Filipek'], 'affiliations': ['Reactive AI'], 'pdf_title_img': 'assets/pdf/title_img/2510.05485.jpg', 'data': {'categories': ['#training', '#open_source', '#optimization', '#benchmark'], 'emoji': '⚡', 'ru': {'title': 'Молниеносная BLEU метрика на GPU для обучения моделей', 'desc': 'TensorBLEU — это реализация метрики BLEU, оптимизированная специально для работы на GPU во время обучения моделей NLP. Основная инновация заключается в эффективной векторизованной работе с n-граммами через создание компактного батч-специфичного словаря с помощью torch.unique, что решает проблему огромных затрат памяти. На GPU NVIDIA T4 метрика работает в 13 раз быстрее CPU-версии из NLTK, а на A100 — более чем в 40 раз быстрее. Это особенно важно для Reinforcement Learning подходов к файн-тюнингу, где метрика используется для вычисления reward сигналов прямо во время обучения.'}, 'en': {'title': 'Accelerating NLP Evaluation with TensorBLEU', 'desc': 'TensorBLEU is a new implementation of the BLEU metric that runs on GPUs, making it much faster than traditional CPU methods. It is specifically designed for evaluating natural language processing models during training, which is crucial for tasks like Reinforcement Learning. By using a unique memory-efficient approach to count n-grams, TensorBLEU can handle large vocabularies without the high memory costs of older methods. Benchmarks show that it can be over 13 times faster on consumer GPUs and more than 40 times faster on high-end data center GPUs, significantly speeding up the training process.'}, 'zh': {'title': 'TensorBLEU：加速自然语言处理模型评估的利器', 'desc': 'TensorBLEU是一种针对自然语言处理模型的BLEU指标的GPU加速实现，旨在提高训练过程中的评估效率。它通过在PyTorch中进行完全向量化的每句计算，显著提升了计算速度，尤其是在处理大规模词汇模型时。该方法采用内存高效的计数机制，避免了传统哈希向量化的高内存成本。实验结果表明，TensorBLEU在消费级GPU上速度提升超过13倍，在数据中心级硬件上超过40倍，极大地减少了训练过程中的计算瓶颈。'}}}, {'id': 'https://huggingface.co/papers/2510.05560', 'title': 'HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video', 'url': 'https://huggingface.co/papers/2510.05560', 'abstract': "HoloScene is an interactive 3D reconstruction framework that achieves geometry completeness, object interactivity, physical plausibility, photorealistic rendering, and realistic physical properties through an energy-based optimization problem.  \t\t\t\t\tAI-generated summary \t\t\t\t Digitizing the physical world into accurate simulation-ready virtual environments offers significant opportunities in a variety of fields such as augmented and virtual reality, gaming, and robotics. However, current 3D reconstruction and scene-understanding methods commonly fall short in one or more critical aspects, such as geometry completeness, object interactivity, physical plausibility, photorealistic rendering, or realistic physical properties for reliable dynamic simulation. To address these limitations, we introduce HoloScene, a novel interactive 3D reconstruction framework that simultaneously achieves these requirements. HoloScene leverages a comprehensive interactive scene-graph representation, encoding object geometry, appearance, and physical properties alongside hierarchical and inter-object relationships. Reconstruction is formulated as an energy-based optimization problem, integrating observational data, physical constraints, and generative priors into a unified, coherent objective. Optimization is efficiently performed via a hybrid approach combining sampling-based exploration with gradient-based refinement. The resulting digital twins exhibit complete and precise geometry, physical stability, and realistic rendering from novel viewpoints. Evaluations conducted on multiple benchmark datasets demonstrate superior performance, while practical use-cases in interactive gaming and real-time digital-twin manipulation illustrate HoloScene's broad applicability and effectiveness. Project page: https://xiahongchi.github.io/HoloScene.", 'score': 6, 'issue_id': 6299, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'fcf790fe9803a797', 'authors': ['Hongchi Xia', 'Chih-Hao Lin', 'Hao-Yu Hsu', 'Quentin Leboutet', 'Katelyn Gao', 'Michael Paulitsch', 'Benjamin Ummenhofer', 'Shenlong Wang'], 'affiliations': ['Intel', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2510.05560.jpg', 'data': {'categories': ['#3d', '#optimization', '#games', '#benchmark'], 'emoji': '🏗️', 'ru': {'title': 'Цифровые двойники с физикой и фотореализмом', 'desc': "HoloScene — это фреймворк для интерактивной 3D-реконструкции физического мира в виртуальные среды, готовые к симуляции. Система использует граф сцены, который кодирует геометрию объектов, их внешний вид, физические свойства и взаимосвязи между ними. Реконструкция формулируется как задача энергетической оптимизации, объединяющая данные наблюдений, физические ограничения и генеративные prior'ы через гибридный подход с sampling и градиентными методами. Результат — полные цифровые двойники с точной геометрией, физической стабильностью и фотореалистичным рендерингом для AR/VR, игр и робототехники."}, 'en': {'title': 'Revolutionizing 3D Reconstruction with HoloScene', 'desc': 'HoloScene is a cutting-edge framework for creating interactive 3D reconstructions that meet essential criteria for realistic simulations. It addresses common shortcomings in existing methods by ensuring geometry completeness, object interactivity, physical plausibility, and photorealistic rendering. The framework utilizes an energy-based optimization approach that combines observational data and physical constraints to produce accurate digital twins. Its effectiveness is demonstrated through superior performance on benchmark datasets and practical applications in gaming and digital-twin manipulation.'}, 'zh': {'title': 'HoloScene：实现真实感的交互式3D重建', 'desc': 'HoloScene是一个交互式3D重建框架，旨在实现几何完整性、物体交互性、物理合理性、照片级渲染和真实的物理属性。该框架通过能量优化问题来整合观察数据、物理约束和生成先验，形成一个统一的目标。HoloScene利用全面的交互场景图表示，编码物体的几何形状、外观和物理属性，同时考虑层次和物体间的关系。通过结合基于采样的探索和基于梯度的细化，优化过程高效进行，最终生成的数字双胞胎在新视角下展现出完整精确的几何形状和真实的渲染效果。'}}}, {'id': 'https://huggingface.co/papers/2510.05432', 'title': 'AInstein: Assessing the Feasibility of AI-Generated Approaches to\n  Research Problems', 'url': 'https://huggingface.co/papers/2510.05432', 'abstract': 'AInstein evaluates the problem-solving capabilities of large language models by testing their ability to generate valid solutions to AI research problems using only pretrained knowledge, revealing both their potential and limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet it remains unclear whether such success reflects genuine reasoning or sophisticated recall. We introduce AInstein, a framework for testing whether LLMs can generate valid solutions to AI research problems using only their pretrained parametric knowledge -- without domain-specific fine-tuning, retrieval augmentation, or other external aids. Our approach extracts distilled problem statements from high-quality ICLR 2025 submissions, then tasks specialized solver agents with proposing and refining technical solutions through iterative critique loops, mimicking the cycles of proposal, review, and revision central to scientific inquiry. We evaluate AInstein on 1,214 ICLR papers stratified by acceptance tier (Oral, Spotlight, Poster), using an LLM-as-a-judge paradigm guided by a structured rubric, complemented by targeted manual checks. Performance is assessed with three metrics: Success Rate (does the solution address the problem?), Rediscovery (does it align with human-proposed methods?), and Novelty (does it yield valid, original approaches?). Our results reveal that while LLMs can rediscover feasible solutions and occasionally propose creative alternatives, their problem-solving ability remains fragile and highly sensitive to framing. These findings provide the first large-scale evidence on the extent to which LLMs can act as autonomous scientific problem-solvers, highlighting both their latent potential and their current limitations.', 'score': 6, 'issue_id': 6298, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': 'effef15175939fca', 'authors': ['Shambhavi Mishra', 'Gaurav Sahu', 'Marco Pedersoli', 'Laurent Charlin', 'Jose Dolz', 'Christopher Pal'], 'affiliations': ['Canada CIFAR AI Chair', 'HEC Montreal', 'International Laboratory on Learning Systems (ILLS)', 'LIVIA, ETS Montreal', 'Mila Quebec AI Institute', 'Polytechnique Montreal', 'ServiceNow Research', 'Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2510.05432.jpg', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#rlhf', '#agents'], 'emoji': '🧪', 'ru': {'title': 'Может ли AI стать самостоятельным исследователем в машинном обучении?', 'desc': 'Исследование представляет AInstein — фреймворк для оценки способности больших языковых моделей (LLM) решать исследовательские задачи в области AI, используя только предобученные знания без файнтюнинга или внешних источников. Модели анализируют реальные задачи из статей ICLR 2025 и предлагают решения через итеративные циклы генерации и критики, имитируя научный процесс. Оценка проводится по трём метрикам: успешность решения, способность переоткрыть существующие методы и генерация новых валидных подходов. Результаты показывают, что LLM могут находить осмысленные решения и иногда предлагать креативные альтернативы, но их способности остаются хрупкими и сильно зависят от формулировки задачи.'}, 'en': {'title': 'AInstein: Unveiling the Problem-Solving Power of LLMs', 'desc': "The paper introduces AInstein, a framework designed to evaluate the problem-solving abilities of large language models (LLMs) in generating valid solutions to AI research problems using only their pretrained knowledge. It tests LLMs without any fine-tuning or external aids, focusing on their capacity to produce solutions through iterative critique loops similar to scientific review processes. The evaluation is based on 1,214 ICLR papers and uses metrics like Success Rate, Rediscovery, and Novelty to assess the LLMs' performance. The findings indicate that while LLMs can rediscover existing solutions and occasionally suggest novel ideas, their problem-solving capabilities are fragile and highly dependent on how problems are framed."}, 'zh': {'title': '评估大型语言模型的科学问题解决能力', 'desc': 'AInstein是一个评估大型语言模型（LLMs）解决问题能力的框架。它测试这些模型在没有领域特定微调或外部帮助的情况下，是否能够生成有效的AI研究问题解决方案。通过提取高质量ICLR 2025提交的精炼问题陈述，AInstein模拟科学研究中的提案、审查和修订循环。研究结果表明，尽管LLMs能够重新发现可行的解决方案并偶尔提出创造性的替代方案，但它们的解决问题能力仍然脆弱，且对问题的表述非常敏感。'}}}, {'id': 'https://huggingface.co/papers/2510.06036', 'title': 'Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?', 'url': 'https://huggingface.co/papers/2510.06036', 'abstract': "Research identifies a mechanism called the refusal cliff in large reasoning models, where refusal intentions drop sharply before output generation, and proposes a method to improve safety by focusing on specific attention heads and training examples.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models (LRMs) with multi-step reasoning capabilities have shown remarkable problem-solving abilities, yet they exhibit concerning safety vulnerabilities that remain poorly understood. In this work, we investigate why safety alignment fails in reasoning models through a mechanistic interpretability lens. Using a linear probing approach to trace refusal intentions across token positions, we discover a striking phenomenon termed as refusal cliff: many poorly-aligned reasoning models correctly identify harmful prompts and maintain strong refusal intentions during their thinking process, but experience a sharp drop in refusal scores at the final tokens before output generation. This suggests that these models are not inherently unsafe; rather, their refusal intentions are systematically suppressed. Through causal intervention analysis, we identify a sparse set of attention heads that negatively contribute to refusal behavior. Ablating just 3\\% of these heads can reduce attack success rates below 10\\%. Building on these mechanistic insights, we propose Cliff-as-a-Judge, a novel data selection method that identifies training examples exhibiting the largest refusal cliff to efficiently repair reasoning models' safety alignment. This approach achieves comparable safety improvements using only 1.7\\% of the vanilla safety training data, demonstrating a less-is-more effect in safety alignment.", 'score': 5, 'issue_id': 6301, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '0d9aa29b1d58dbd6', 'authors': ['Qingyu Yin', 'Chak Tou Leong', 'Linyi Yang', 'Wenxuan Huang', 'Wenjie Li', 'Xiting Wang', 'Jaehong Yoon', 'YunXing', 'XingYu', 'Jinjin Gu'], 'affiliations': ['East China Normal University', 'Hong Kong Polytechnic University', 'INSAIT', 'Nanyang Technological University', 'Renmin University', 'Southern University of Science and Technology', 'Xiaohongshu Inc.', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.06036.jpg', 'data': {'categories': ['#reasoning', '#architecture', '#data', '#alignment', '#interpretability', '#training'], 'emoji': '🧗', 'ru': {'title': 'Обрыв отказа: почему безопасные модели внезапно становятся опасными', 'desc': 'Исследователи обнаружили феномен «обрыва отказа» в больших reasoning-моделях: модели правильно распознают вредоносные запросы в процессе рассуждения, но резко теряют намерение отказать прямо перед генерацией ответа. Используя методы механистической интерпретируемости, учёные выявили небольшой набор attention heads, которые подавляют безопасное поведение модели. На основе этих находок был разработан метод Cliff-as-a-Judge, который позволяет восстановить безопасность модели, используя всего 1.7% обучающих данных по сравнению с обычным подходом. Абляция всего 3% проблемных attention heads снижает успешность атак до уровня ниже 10%.'}, 'en': {'title': 'Understanding and Mitigating the Refusal Cliff in Reasoning Models', 'desc': "This paper explores a phenomenon called the refusal cliff in large reasoning models (LRMs), where the models show a significant drop in their intention to refuse harmful prompts just before generating an output. The authors use mechanistic interpretability to analyze how these models can recognize harmful inputs but fail to maintain their refusal intentions at the final stages of processing. They identify specific attention heads that contribute negatively to this behavior and demonstrate that reducing the influence of just a small percentage of these heads can greatly enhance the models' safety. Additionally, they introduce a new method called Cliff-as-a-Judge, which selects training examples that highlight the refusal cliff, allowing for effective safety improvements with minimal data."}, 'zh': {'title': '揭示推理模型的拒绝悬崖机制', 'desc': '本研究探讨了大型推理模型中的拒绝悬崖机制，发现这些模型在生成输出前拒绝意图会急剧下降。通过线性探测方法，我们追踪了拒绝意图在标记位置的变化，发现许多模型在思考过程中能够识别有害提示，但在输出前的最后几个标记处拒绝意图却显著降低。我们通过因果干预分析，识别出少量对拒绝行为产生负面影响的注意力头，去除这些头可以显著降低攻击成功率。基于这些发现，我们提出了一种新的数据选择方法，利用拒绝悬崖的特征来高效修复推理模型的安全对齐。'}}}, {'id': 'https://huggingface.co/papers/2510.05367', 'title': 'LightCache: Memory-Efficient, Training-Free Acceleration for Video\n  Generation', 'url': 'https://huggingface.co/papers/2510.05367', 'abstract': 'The paper proposes stage-specific strategies to accelerate diffusion model inference in video generation, reducing memory usage and maintaining quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Training-free acceleration has emerged as an advanced research area in video generation based on diffusion models. The redundancy of latents in diffusion model inference provides a natural entry point for acceleration. In this paper, we decompose the inference process into the encoding, denoising, and decoding stages, and observe that cache-based acceleration methods often lead to substantial memory surges in the latter two stages. To address this problem, we analyze the characteristics of inference across different stages and propose stage-specific strategies for reducing memory consumption: 1) Asynchronous Cache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same time, we ensure that the time overhead introduced by these three strategies remains lower than the acceleration gains themselves. Compared with the baseline, our approach achieves faster inference speed and lower memory usage, while maintaining quality degradation within an acceptable range. The Code is available at https://github.com/NKUShaw/LightCache .', 'score': 5, 'issue_id': 6299, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': 'da5c54f2d1fce894', 'authors': ['Yang Xiao', 'Gen Li', 'Kaiyuan Deng', 'Yushu Wu', 'Zheng Zhan', 'Yanzhi Wang', 'Xiaolong Ma', 'Bo Hui'], 'affiliations': ['Clemson University', 'Microsoft Research', 'Northeastern University', 'The University of Arizona', 'University of Tulsa'], 'pdf_title_img': 'assets/pdf/title_img/2510.05367.jpg', 'data': {'categories': ['#diffusion', '#open_source', '#video', '#optimization', '#inference'], 'emoji': '🎬', 'ru': {'title': 'Ускорение видеогенерации через управление памятью на разных стадиях', 'desc': 'Исследователи предложили метод ускорения генерации видео с помощью диффузионных моделей без дополнительного обучения. Они разделили процесс инференса на три стадии (кодирование, шумоподавление и декодирование) и обнаружили проблему резкого роста потребления памяти. Для каждой стадии разработаны специфические стратегии: асинхронный обмен кэша, разбиение признаков на части и нарезка латентных представлений при декодировании. В результате достигнуто ускорение работы модели при снижении потребления памяти с сохранением приемлемого качества генерируемого видео.'}, 'en': {'title': 'Accelerating Video Generation with Stage-Specific Strategies', 'desc': 'This paper focuses on improving the efficiency of video generation using diffusion models by introducing stage-specific strategies. It identifies that the inference process can be broken down into three stages: encoding, denoising, and decoding, and highlights the memory issues that arise during the latter two stages. The authors propose methods such as Asynchronous Cache Swapping, Feature Chunking, and Slicing Latents to optimize memory usage without significantly increasing processing time. Overall, their approach results in faster inference speeds and reduced memory consumption while keeping quality loss minimal.'}, 'zh': {'title': '阶段特定策略加速视频生成推理', 'desc': '本文提出了针对扩散模型在视频生成中的推理加速的阶段特定策略，旨在减少内存使用并保持生成质量。我们将推理过程分解为编码、去噪和解码三个阶段，并发现基于缓存的加速方法在后两个阶段常常导致内存激增。为了解决这个问题，我们分析了不同阶段推理的特征，并提出了三种减少内存消耗的策略：异步缓存交换、特征块和切片潜变量解码。同时，我们确保这三种策略引入的时间开销低于加速带来的收益。与基线相比，我们的方法实现了更快的推理速度和更低的内存使用，同时保持了可接受范围内的质量下降。'}}}, {'id': 'https://huggingface.co/papers/2510.05342', 'title': 'Margin Adaptive DPO: Leveraging Reward Model for Granular Control in\n  Preference Optimization', 'url': 'https://huggingface.co/papers/2510.05342', 'abstract': 'MADPO, a margin-adaptive method, enhances preference alignment in large language models by providing instance-level adaptive weighting to the DPO loss, improving performance across datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Direct Preference Optimization (DPO) has emerged as a simple and effective method for aligning large language models. However, its reliance on a fixed temperature parameter leads to suboptimal training on diverse preference data, causing overfitting on easy examples and under-learning from informative ones. Recent methods have emerged to counter this. While IPO addresses general overfitting, its uniform regularization can be overly conservative. The more targeted approach of beta-DPO suffers from its own limitations: its batch-level adaptation applies a single, compromised temperature to mixed-margin pairs, its linear update rule can produce unstable negative beta values, and its filtering mechanism discards potentially useful training signals. In this work, we introduce Margin-Adaptive Direct Preference Optimization (MADPO), a method that provides a stable, data-preserving, and instance-level solution. MADPO employs a practical two-step approach: it first trains a reward model to estimate preference margins and then uses these margins to apply a continuous, adaptive weight to the DPO loss for each individual training sample. This re-weighting scheme creates an effective target margin that is amplified for hard pairs and dampened for easy pairs, allowing for granular control over the learning signal. We provide a comprehensive theoretical analysis, proving that MADPO has a well-behaved optimization landscape and is robust to reward model estimation errors. We validate our theory with experiments on a sentiment generation task, where MADPO consistently and significantly outperforms strong baselines across datasets of varying quality. It achieves performance gains of up to +33.3\\% on High Quality data and +10.5\\% on Low Quality data over the next-best method. Our results establish MADPO as a more robust and principled approach to preference alignment.', 'score': 5, 'issue_id': 6298, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': '1a257d25fefce283', 'authors': ['Hyung Gyu Rho'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2510.05342.jpg', 'data': {'categories': ['#optimization', '#alignment', '#training', '#rlhf'], 'emoji': '⚖️', 'ru': {'title': 'Адаптивные веса для каждого примера делают обучение по предпочтениям эффективнее', 'desc': 'MADPO — это новый метод выравнивания больших языковых моделей по предпочтениям, который решает проблему DPO с фиксированной температурой. Метод сначала обучает reward model для оценки сложности каждой пары примеров, а затем применяет индивидуальные адаптивные веса к loss-функции DPO для каждого sample. Это позволяет модели больше учиться на сложных примерах и меньше переобучаться на простых, в отличие от предыдущих методов с батч-уровневой или uniformной регуляризацией. Эксперименты показывают улучшение до +33.3% на качественных данных по сравнению с baseline методами.'}, 'en': {'title': 'Enhancing Preference Alignment with Adaptive Weighting', 'desc': "MADPO, or Margin-Adaptive Direct Preference Optimization, is a novel method designed to improve the alignment of large language models by adapting the weighting of the DPO loss at the instance level. This approach addresses the limitations of previous methods by providing a continuous and adaptive weight based on the estimated preference margins for each training sample. By amplifying the learning signal for difficult examples and reducing it for easier ones, MADPO enhances the model's ability to learn from diverse datasets effectively. Experimental results demonstrate that MADPO significantly outperforms existing methods, achieving notable performance improvements across various data quality levels."}, 'zh': {'title': '边际自适应优化，提升模型偏好对齐', 'desc': 'MADPO是一种边际自适应方法，通过为DPO损失提供实例级自适应加权，增强了大型语言模型的偏好对齐能力。该方法首先训练一个奖励模型来估计偏好边际，然后根据这些边际为每个训练样本应用连续的自适应权重。MADPO的重加权方案对困难样本增强信号，对简单样本减弱信号，从而实现了对学习信号的细致控制。实验结果表明，MADPO在情感生成任务中显著优于其他强基线，证明了其在偏好对齐方面的稳健性和有效性。'}}}, {'id': 'https://huggingface.co/papers/2510.05318', 'title': 'BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language\n  Models via Lens of Dynamic Interactions', 'url': 'https://huggingface.co/papers/2510.05318', 'abstract': "BIRD-INTERACT is a benchmark for multi-turn text-to-SQL tasks that simulates realistic database assistant challenges through dynamic interactions, hierarchical knowledge bases, and autonomous decision-making.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated remarkable performance on single-turn text-to-SQL tasks, but real-world database applications predominantly require multi-turn interactions to handle ambiguous queries, execution errors, and evolving user requirements. Existing multi-turn benchmarks fall short by treating conversation histories as static context or limiting evaluation to read-only operations, failing to reflect production-grade database assistant challenges. We introduce BIRD-INTERACT, a benchmark that restores this realism through: (1) a comprehensive interaction environment coupling each database with a hierarchical knowledge base, metadata files, and a function-driven user simulator, enabling models to solicit clarifications, retrieve knowledge, and recover from errors without human supervision; (2) two evaluation settings consisting of a pre-defined conversational protocol (c-Interact) and an open-ended agentic setting (a-Interact) where models autonomously decide when to query the user simulator or explore the environment; (3) a challenging task suite covering the full CRUD spectrum for business-intelligence and operational use cases, guarded by executable test cases. Each task features ambiguous and follow-up sub-tasks requiring dynamic interaction. The suite comprises BIRD-INTERACT-FULL (600 tasks, up to 11,796 interactions) for comprehensive performance assessment, and BIRD-INTERACT-LITE (300 tasks with simplified databases) for detailed behavioral analysis and rapid method development. Our empirical results highlight BIRD-INTERACT's difficulty: GPT-5 completes only 8.67% of tasks in c-Interact and 17.00% in a-Interact. Analysis via memory grafting and Interaction Test-time Scaling validates the importance of effective interaction for complex, dynamic text-to-SQL tasks.", 'score': 5, 'issue_id': 6305, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': '88e68f710dc7b7fc', 'authors': ['Nan Huo', 'Xiaohan Xu', 'Jinyang Li', 'Per Jacobsson', 'Shipei Lin', 'Bowen Qin', 'Binyuan Hui', 'Xiaolong Li', 'Ge Qu', 'Shuzheng Si', 'Linheng Han', 'Edward Alexander', 'Xintong Zhu', 'Rui Qin', 'Ruihan Yu', 'Yiyao Jin', 'Feige Zhou', 'Weihao Zhong', 'Yun Chen', 'Hongyu Liu', 'Chenhao Ma', 'Fatma Ozcan', 'Yannis Papakonstantinou', 'Reynold Cheng'], 'affiliations': ['Google Cloud', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.05318.jpg', 'data': {'categories': ['#agents', '#interpretability', '#benchmark', '#reasoning'], 'emoji': '🗣️', 'ru': {'title': 'Реалистичный бенчмарк для многоходовых SQL-диалогов с базами данных', 'desc': 'Статья представляет BIRD-INTERACT — новый бенчмарк для оценки LLM в многоходовых задачах преобразования текста в SQL. В отличие от существующих бенчмарков, BIRD-INTERACT моделирует реальные сценарии работы с базами данных: модели должны уточнять неоднозначные запросы пользователей, исправлять ошибки выполнения и работать с полным спектром CRUD-операций. Бенчмарк включает 600 задач с иерархическими базами знаний и симулятором пользователя, позволяющим моделям автономно принимать решения о взаимодействии. Результаты показывают высокую сложность задач: даже GPT-5 успешно справляется только с 8-17% заданий в зависимости от режима взаимодействия.'}, 'en': {'title': 'BIRD-INTERACT: Elevating Multi-Turn Text-to-SQL Challenges', 'desc': 'BIRD-INTERACT is a new benchmark designed for multi-turn text-to-SQL tasks, addressing the limitations of existing benchmarks that do not accurately simulate real-world database interactions. It creates a dynamic environment where models can engage in conversations, ask for clarifications, and recover from errors autonomously, reflecting the complexities of actual database applications. The benchmark includes two evaluation settings: a structured conversational protocol and an open-ended setting, allowing for a range of interactions. Empirical results show that even advanced models like GPT-5 struggle with these tasks, emphasizing the need for effective interaction in complex text-to-SQL scenarios.'}, 'zh': {'title': 'BIRD-INTERACT：真实多轮交互的数据库助手挑战', 'desc': 'BIRD-INTERACT是一个用于多轮文本到SQL任务的基准测试，旨在模拟现实数据库助手面临的挑战。与现有的基准不同，BIRD-INTERACT通过动态交互和分层知识库，允许模型在没有人工干预的情况下进行自主决策和错误恢复。该基准提供了两种评估设置，分别是预定义的对话协议和开放式自主设置，涵盖了商业智能和操作用例的完整CRUD任务。实验结果表明，BIRD-INTERACT的任务难度较高，现有大型语言模型在完成这些任务时表现不佳。'}}}, {'id': 'https://huggingface.co/papers/2510.05251', 'title': 'Let it Calm: Exploratory Annealed Decoding for Verifiable Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2510.05251', 'abstract': "Exploratory Annealed Decoding (EAD) improves sample efficiency in reinforcement learning with verifiable rewards by dynamically adjusting the sampling temperature during generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) is a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs), yet its success hinges on effective exploration. An ideal exploration strategy must navigate two fundamental challenges: it must preserve sample quality while also ensuring training stability. While standard fixed-temperature sampling is simple, it struggles to balance these competing demands, as high temperatures degrade sample quality and low temperatures limit discovery. In this work, we propose a simpler and more effective strategy, Exploratory Annealed Decoding (EAD), grounded in the insight that exploration is most impactful on early tokens which define a sequence's semantic direction. EAD implements an intuitive **explore-at-the-beginning, exploit-at-the-end** strategy by annealing the sampling temperature from high to low during generation. This dynamic schedule encourages meaningful, high-level diversity at the start, then gradually lowers the temperature to preserve sample quality and keep the sampling distribution close to the target policy, which is essential for stable training. We demonstrate that EAD is a lightweight, plug-and-play method that significantly improves sample efficiency, consistently outperforming fixed-temperature sampling across various RLVR algorithms and model sizes. Our work suggests that aligning exploration with the natural dynamics of sequential generation offers a robust path to improving LLM reasoning.", 'score': 5, 'issue_id': 6313, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': 'b57b7195043b802a', 'authors': ['Chenghao Yang', 'Lin Gui', 'Chenxiao Yang', 'Victor Veitch', 'Lizhu Zhang', 'Zhuokai Zhao'], 'affiliations': ['Data Science Institute, University of Chicago', 'Department of Computer Science, University of Chicago', 'Department of Statistics, University of Chicago', 'Meta AI', 'Toyota Technological Institute at Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2510.05251.jpg', 'data': {'categories': ['#reasoning', '#training', '#optimization', '#rl'], 'emoji': '🌡️', 'ru': {'title': 'Исследуй сначала, используй потом: динамическая температура для обучения LLM', 'desc': 'Статья предлагает метод Exploratory Annealed Decoding (EAD) для улучшения обучения с подкреплением в языковых моделях. Ключевая идея — динамически менять температуру сэмплирования во время генерации: высокая температура в начале для исследования разных направлений, затем постепенное снижение для качества результата. Это помогает балансировать между исследованием новых решений и стабильностью обучения, особенно важно для первых токенов, которые определяют семантическое направление ответа. EAD показывает лучшую эффективность по сравнению с фиксированной температурой на разных алгоритмах обучения с подкреплением и размерах моделей.'}, 'en': {'title': 'Dynamic Exploration for Enhanced Learning Efficiency', 'desc': 'Exploratory Annealed Decoding (EAD) enhances sample efficiency in reinforcement learning with verifiable rewards by adjusting the sampling temperature dynamically. This method addresses the challenges of maintaining sample quality while ensuring training stability during exploration. EAD employs a strategy of exploring more at the beginning of the sequence and exploiting at the end, which allows for high-level diversity initially and preserves quality later. The results show that EAD outperforms traditional fixed-temperature sampling methods, making it a valuable tool for improving the reasoning capabilities of large language models.'}, 'zh': {'title': '探索性退火解码：提升样本效率的创新策略', 'desc': '探索性退火解码（EAD）是一种提高强化学习中可验证奖励样本效率的方法。它通过在生成过程中动态调整采样温度，解决了样本质量与训练稳定性之间的矛盾。EAD采用了“开始探索，结束利用”的策略，初期高温采样促进多样性，后期低温采样保持样本质量。实验表明，EAD在多种强化学习算法和模型规模中，显著优于固定温度采样，提升了样本效率。'}}}, {'id': 'https://huggingface.co/papers/2510.04506', 'title': 'GRACE: Generative Representation Learning via Contrastive Policy\n  Optimization', 'url': 'https://huggingface.co/papers/2510.04506', 'abstract': 'GRACE uses contrastive policy optimization to train LLMs as generative agents that produce interpretable rationales, improving embeddings and transparency.  \t\t\t\t\tAI-generated summary \t\t\t\t Prevailing methods for training Large Language Models (LLMs) as text encoders rely on contrastive losses that treat the model as a black box function, discarding its generative and reasoning capabilities in favor of static embeddings. We introduce GRACE (Generative Representation Learning via Contrastive Policy Optimization), a novel framework that reimagines contrastive signals not as losses to be minimized, but as rewards that guide a generative policy. In GRACE, the LLM acts as a policy that produces explicit, human-interpretable rationales--structured natural language explanations of its semantic understanding. These rationales are then encoded into high-quality embeddings via mean pooling. Using policy gradient optimization, we train the model with a multi-component reward function that maximizes similarity between query positive pairs and minimizes similarity with negatives. This transforms the LLM from an opaque encoder into an interpretable agent whose reasoning process is transparent and inspectable. On MTEB benchmark, GRACE yields broad cross category gains: averaged over four backbones, the supervised setting improves overall score by 11.5% over base models, and the unsupervised variant adds 6.9%, while preserving general capabilities. This work treats contrastive objectives as rewards over rationales, unifying representation learning with generation to produce stronger embeddings and transparent rationales. The model, data and code are available at https://github.com/GasolSun36/GRACE.', 'score': 5, 'issue_id': 6313, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': 'efc35bb65ba488c2', 'authors': ['Jiashuo Sun', 'Shixuan Liu', 'Zhaochen Su', 'Xianrui Zhong', 'Pengcheng Jiang', 'Bowen Jin', 'Peiran Li', 'Weijia Shi', 'Jiawei Han'], 'affiliations': ['Australian National University', 'Hong Kong University of Science and Technology', 'University of Illinois Urbana-Champaign', 'University of Washington', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2510.04506.jpg', 'data': {'categories': ['#reasoning', '#agents', '#rl', '#interpretability', '#optimization', '#training', '#benchmark'], 'emoji': '🎭', 'ru': {'title': 'Обучение языковых моделей через генерацию объяснений с контрастным подкреплением', 'desc': 'Исследователи предложили метод GRACE, который обучает большие языковые модели (LLM) генерировать понятные человеку объяснения своих решений вместо создания непрозрачных эмбеддингов. Метод использует контрастную оптимизацию политик, где сигналы контрастного обучения рассматриваются как награды для reinforcement learning агента. Модель создаёт текстовые обоснования (rationales) своего понимания семантики, которые затем преобразуются в эмбеддинги высокого качества. На бенчмарке MTEB метод показал улучшение на 11.5% в supervised режиме и 6.9% в unsupervised по сравнению с базовыми моделями, сохраняя при этом прозрачность процесса рассуждений.'}, 'en': {'title': 'Transforming LLMs into Interpretable Generative Agents with GRACE', 'desc': "GRACE is a new framework that enhances the training of Large Language Models (LLMs) by using contrastive policy optimization. Instead of treating the model as a black box, GRACE allows the LLM to generate clear and interpretable rationales for its decisions, improving transparency. It uses a reward-based approach to guide the model's learning, maximizing the similarity of positive examples while minimizing that of negative ones. This results in better embeddings and a more understandable reasoning process, achieving significant performance improvements on benchmark tests."}, 'zh': {'title': 'GRACE：将对比优化转化为可解释的生成能力', 'desc': 'GRACE是一种新的框架，通过对比策略优化来训练大型语言模型（LLMs），使其能够生成可解释的推理。这种方法将对比信号视为奖励，而不是简单的损失，从而引导生成策略。GRACE使得LLM能够生成结构化的自然语言解释，提升了模型的透明度和可解释性。通过多组件奖励函数的策略梯度优化，GRACE在多个基准测试中显著提高了模型的表现。'}}}, {'id': 'https://huggingface.co/papers/2510.02300', 'title': 'Equilibrium Matching: Generative Modeling with Implicit Energy-Based\n  Models', 'url': 'https://huggingface.co/papers/2510.02300', 'abstract': 'Equilibrium Matching (EqM) is a generative modeling framework that learns an equilibrium gradient of an implicit energy landscape, enabling efficient sampling and outperforming traditional diffusion and flow models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Equilibrium Matching (EqM), a generative modeling framework built from an equilibrium dynamics perspective. EqM discards the non-equilibrium, time-conditional dynamics in traditional diffusion and flow-based generative models and instead learns the equilibrium gradient of an implicit energy landscape. Through this approach, we can adopt an optimization-based sampling process at inference time, where samples are obtained by gradient descent on the learned landscape with adjustable step sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation performance of diffusion/flow models empirically, achieving an FID of 1.90 on ImageNet 256times256. EqM is also theoretically justified to learn and sample from the data manifold. Beyond generation, EqM is a flexible framework that naturally handles tasks including partially noised image denoising, OOD detection, and image composition. By replacing time-conditional velocities with a unified equilibrium landscape, EqM offers a tighter bridge between flow and energy-based models and a simple route to optimization-driven inference.', 'score': 4, 'issue_id': 6306, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': '2955dd166f02c0c7', 'authors': ['Runqian Wang', 'Yilun Du'], 'affiliations': ['Harvard University', 'MIT'], 'pdf_title_img': 'assets/pdf/title_img/2510.02300.jpg', 'data': {'categories': ['#inference', '#optimization', '#cv', '#diffusion', '#multimodal'], 'emoji': '⚖️', 'ru': {'title': 'Генерация через энергетический ландшафт вместо диффузии', 'desc': 'Equilibrium Matching (EqM) — это новый подход к генеративному моделированию, который отказывается от зависящей от времени динамики традиционных диффузионных моделей и flow-based моделей. Вместо этого EqM обучает равновесный градиент неявного энергетического ландшафта, что позволяет использовать методы оптимизации для семплирования с адаптивным размером шага. Модель достигает FID 1.90 на ImageNet 256×256, превосходя классические диффузионные модели, и естественным образом решает задачи шумоподавления, OOD-детекции и композиции изображений. EqM создаёт связь между flow-моделями и energy-based моделями через единый энергетический ландшафт.'}, 'en': {'title': 'Harnessing Equilibrium for Superior Generative Modeling', 'desc': 'Equilibrium Matching (EqM) is a new framework for generative modeling that focuses on learning the equilibrium gradient of an implicit energy landscape. Unlike traditional models that rely on time-dependent dynamics, EqM uses a stable equilibrium approach to improve sampling efficiency. This method allows for optimization-based sampling during inference, where samples are generated through gradient descent on the learned landscape. Empirical results show that EqM outperforms existing diffusion and flow models, achieving impressive performance metrics while also being adaptable for various tasks like image denoising and out-of-distribution detection.'}, 'zh': {'title': '平衡匹配：高效生成的新方法', 'desc': '平衡匹配（EqM）是一种生成建模框架，旨在学习隐式能量景观的平衡梯度，从而实现高效采样。与传统的扩散和流模型不同，EqM摒弃了非平衡的时间条件动态，专注于平衡状态的学习。通过优化基础的采样过程，EqM在推理时能够通过梯度下降获取样本，并且在生成性能上超越了传统模型。除了生成任务，EqM还灵活地处理部分噪声图像去噪、异常检测和图像合成等任务。'}}}, {'id': 'https://huggingface.co/papers/2510.05137', 'title': 'Demystifying deep search: a holistic evaluation with hint-free multi-hop\n  questions and factorised metrics', 'url': 'https://huggingface.co/papers/2510.05137', 'abstract': "WebDetective is a benchmark for evaluating multi-hop reasoning in RAG systems and web agents, addressing issues of reasoning path leakage and single-pass evaluation, and introducing a framework to improve knowledge utilization and refusal behavior.  \t\t\t\t\tAI-generated summary \t\t\t\t RAG (Retrieval-Augmented Generation) systems and web agents are increasingly evaluated on multi-hop deep search tasks, yet current practice suffers from two major limitations. First, most benchmarks leak the reasoning path in the question text, allowing models to follow surface cues rather than discover reasoning chains autonomously. Second, evaluation is typically reduced to a single pass rate, which collapses diverse behaviours into one score and obscures whether failures stem from inadequate search, poor knowledge use, or inappropriate refusal. To address these issues, we present WebDetective, a benchmark of hint-free multi-hop questions paired with a controlled Wikipedia sandbox that ensures full traceability of model actions, and a holistic evaluation framework that separates search sufficiency, knowledge utilisation, and refusal behaviour. Our evaluation of 25 state-of-the-art models reveals systematic weaknesses across all architectures: models struggle with knowledge utilisation despite having sufficient evidence and demonstrate near-absent appropriate refusal when evidence is lacking. These patterns expose a fundamental gap: today's systems excel at executing given reasoning paths but fail when required to discover them. We develop an agentic workflow, EvidenceLoop, that explicitly targets the challenges our benchmark identifies, incorporating verification loops and systematic evidence tracking that improve both search and synthesis capabilities. This baseline demonstrates that WebDetective's diagnostic framework can guide concrete architectural improvements, establishing our benchmark as a critical tool for developing genuinely autonomous reasoning systems rather than pattern-following agents.", 'score': 4, 'issue_id': 6298, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 октября', 'en': 'October 1', 'zh': '10月1日'}, 'hash': 'd780a77899923141', 'authors': ['Maojia Song', 'Renhang Liu', 'Xinyu Wang', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Soujanya Poria', 'Jingren Zhou'], 'affiliations': ['Nanyang Technological University (NTU)', 'Singapore University of Technology and Design (SUTD)', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.05137.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#rag', '#benchmark', '#leakage', '#architecture', '#agents'], 'emoji': '🔍', 'ru': {'title': 'WebDetective: Как научить AI-агентов думать самостоятельно, а не следовать подсказкам', 'desc': 'WebDetective — это новый бенчмарк для оценки многошаговых рассуждений в RAG-системах и веб-агентах. Существующие тесты содержат «утечку» пути рассуждений прямо в вопросе, позволяя моделям просто следовать поверхностным подсказкам вместо самостоятельного построения цепочки мыслей. Авторы создали контролируемую среду на основе Wikipedia и детальную систему оценки, которая раздельно измеряет качество поиска информации, использование знаний и способность отказаться от ответа при недостатке данных. Тестирование 25 современных моделей выявило критическую проблему: системы хорошо выполняют заданные пути рассуждений, но проваливаются, когда нужно самостоятельно их обнаружить.'}, 'en': {'title': 'WebDetective: Enhancing Multi-Hop Reasoning in AI Systems', 'desc': 'WebDetective is a new benchmark designed to evaluate how well RAG systems and web agents can perform multi-hop reasoning without leaking reasoning paths in the questions. It addresses the limitations of current evaluation methods that oversimplify model performance into a single score, which can hide specific weaknesses in knowledge utilization and refusal behavior. The benchmark includes hint-free questions and a controlled environment to track model actions, allowing for a more detailed analysis of how models search for information and use knowledge. The findings reveal that many models struggle with effectively utilizing available evidence and often fail to refuse when they lack sufficient information, highlighting the need for improvements in autonomous reasoning capabilities.'}, 'zh': {'title': 'WebDetective：提升多跳推理的评估标准', 'desc': 'WebDetective是一个用于评估RAG系统和网络代理的多跳推理基准，旨在解决推理路径泄漏和单次评估的问题。该基准提供无提示的多跳问题，并配备一个受控的维基百科沙箱，以确保模型行为的可追溯性。通过对25个最先进模型的评估，我们发现这些模型在知识利用方面存在系统性弱点，尽管有足够的证据，但在缺乏证据时几乎没有适当的拒绝行为。我们开发了EvidenceLoop工作流程，专门针对基准识别的挑战，改进了搜索和综合能力。'}}}, {'id': 'https://huggingface.co/papers/2510.06219', 'title': 'Human3R: Everyone Everywhere All at Once', 'url': 'https://huggingface.co/papers/2510.06219', 'abstract': 'Human3R is a unified, feed-forward framework for real-time 4D human-scene reconstruction from monocular videos, achieving state-of-the-art performance with a single model and minimal dependencies.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Human3R, a unified, feed-forward framework for online 4D human-scene reconstruction, in the world frame, from casually captured monocular videos. Unlike previous approaches that rely on multi-stage pipelines, iterative contact-aware refinement between humans and scenes, and heavy dependencies, e.g., human detection, depth estimation, and SLAM pre-processing, Human3R jointly recovers global multi-person SMPL-X bodies ("everyone"), dense 3D scene ("everywhere"), and camera trajectories in a single forward pass ("all-at-once"). Our method builds upon the 4D online reconstruction model CUT3R, and uses parameter-efficient visual prompt tuning, to strive to preserve CUT3R\'s rich spatiotemporal priors, while enabling direct readout of multiple SMPL-X bodies. Human3R is a unified model that eliminates heavy dependencies and iterative refinement. After being trained on the relatively small-scale synthetic dataset BEDLAM for just one day on one GPU, it achieves superior performance with remarkable efficiency: it reconstructs multiple humans in a one-shot manner, along with 3D scenes, in one stage, at real-time speed (15 FPS) with a low memory footprint (8 GB). Extensive experiments demonstrate that Human3R delivers state-of-the-art or competitive performance across tasks, including global human motion estimation, local human mesh recovery, video depth estimation, and camera pose estimation, with a single unified model. We hope that Human3R will serve as a simple yet strong baseline, be easily extended for downstream applications.Code available in https://fanegg.github.io/Human3R', 'score': 3, 'issue_id': 6304, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'aa48bcc34ddfe0e6', 'authors': ['Yue Chen', 'Xingyu Chen', 'Yuxuan Xue', 'Anpei Chen', 'Yuliang Xiu', 'Gerard Pons-Moll'], 'affiliations': ['Max Planck Institute for Informatics', 'Uni of Tubingen, Tubingen AI Center', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2510.06219.jpg', 'data': {'categories': ['#cv', '#video', '#3d'], 'emoji': '🎥', 'ru': {'title': 'Реальная 4D реконструкция сцен с людьми из видео', 'desc': 'Human3R — это новая система для реконструкции 4D сцен с людьми из обычных видео, которая работает в реальном времени. В отличие от других методов, Human3R не требует сложных этапов обработки и дополнительных зависимостей, таких как обнаружение людей или оценка глубины. Модель объединяет восстановление тел, сцен и траекторий камеры в одном процессе, что делает её очень эффективной. Human3R показывает высокую производительность даже при обучении на небольших данных и может стать основой для других приложений.'}, 'en': {'title': 'Real-Time 4D Human-Scene Reconstruction Made Simple', 'desc': 'Human3R is a novel framework designed for real-time 4D reconstruction of humans and scenes from single-camera videos. It simplifies the reconstruction process by using a single feed-forward model, avoiding the need for complex multi-stage pipelines and heavy dependencies like human detection or depth estimation. The model efficiently reconstructs multiple human bodies and dense 3D scenes simultaneously, achieving high performance with minimal computational resources. With its ability to operate at real-time speeds and low memory usage, Human3R sets a new standard for efficient human-scene reconstruction in machine learning.'}, 'zh': {'title': 'Human3R：实时4D人类场景重建的统一框架', 'desc': 'Human3R是一个统一的前馈框架，能够实时从单目视频中重建4D人类场景。与以往依赖多阶段流程和重迭精细化的方法不同，Human3R可以在一次前向传递中同时恢复多个人体模型、密集的3D场景和相机轨迹。该方法基于CUT3R模型，采用高效的视觉提示调优，保持了丰富的时空先验，同时实现了多个人体模型的直接输出。经过一天的训练，Human3R在小规模合成数据集上表现出色，能够以实时速度重建多个角色和场景，具有低内存占用。'}}}, {'id': 'https://huggingface.co/papers/2510.06218', 'title': 'EgoNight: Towards Egocentric Vision Understanding at Night with a\n  Challenging Benchmark', 'url': 'https://huggingface.co/papers/2510.06218', 'abstract': 'EgoNight is a comprehensive benchmark for nighttime egocentric vision, focusing on visual question answering and revealing performance gaps between day and night conditions for multimodal large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Most existing benchmarks for egocentric vision understanding focus primarily on daytime scenarios, overlooking the low-light conditions that are inevitable in real-world applications. To investigate this gap, we present EgoNight, the first comprehensive benchmark for nighttime egocentric vision, with visual question answering (VQA) as the core task. A key feature of EgoNight is the introduction of day-night aligned videos, which enhance night annotation quality using the daytime data and reveal clear performance gaps between lighting conditions. To achieve this, we collect both synthetic videos rendered by Blender and real-world recordings, ensuring that scenes and actions are visually and temporally aligned. Leveraging these paired videos, we construct EgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and refinement through extensive human verification. Each QA pair is double-checked by annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs across 90 videos, spanning 12 diverse QA types, with more than 300 hours of human work. Evaluations of state-of-the-art multimodal large language models (MLLMs) reveal substantial performance drops when transferring from day to night, underscoring the challenges of reasoning under low-light conditions. Beyond VQA, EgoNight also introduces two auxiliary tasks, day-night correspondence retrieval and egocentric depth estimation at night, that further explore the boundaries of existing models. We believe EgoNight-VQA provides a strong foundation for advancing application-driven egocentric vision research and for developing models that generalize across illumination domains. All the data and code will be made available upon acceptance.', 'score': 3, 'issue_id': 6298, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'a863fc0993d7f2f6', 'authors': ['Deheng Zhang', 'Yuqian Fu', 'Runyi Yang', 'Yang Miao', 'Tianwen Qian', 'Xu Zheng', 'Guolei Sun', 'Ajad Chhatkuli', 'Xuanjing Huang', 'Yu-Gang Jiang', 'Luc Van Gool', 'Danda Pani Paudel'], 'affiliations': ['East China Normal University', 'Fudan University', 'HKUST(GZ)', 'INSAIT, Sofia University St. Kliment Ohridski', 'Nankai University'], 'pdf_title_img': 'assets/pdf/title_img/2510.06218.jpg', 'data': {'categories': ['#multimodal', '#long_context', '#benchmark', '#games', '#transfer_learning', '#synthetic', '#cv'], 'emoji': '🌙', 'ru': {'title': 'Проверка AI-зрения в темноте от первого лица', 'desc': 'EgoNight — это первый комплексный бенчмарк для оценки egocentric-видения в ночных условиях с фокусом на visual question answering. Датасет включает 3658 пар вопрос-ответ на 90 видео, записанных как в дневное, так и в ночное время с выровненными сценами и действиями. Исследование показало значительное падение производительности современных multimodal LLM при переходе от дневных к ночным условиям. Помимо VQA, бенчмарк включает дополнительные задачи: поиск соответствий день-ночь и оценку глубины в ночных условиях для более полного тестирования моделей.'}, 'en': {'title': 'Bridging the Gap: Nighttime Vision for AI', 'desc': 'EgoNight is a new benchmark designed to improve nighttime egocentric vision, particularly in visual question answering (VQA). It highlights the performance differences of multimodal large language models (MLLMs) when operating in low-light conditions compared to daytime scenarios. The benchmark includes day-night aligned videos to enhance the quality of night annotations and reveals significant performance drops in models when transitioning from day to night. Additionally, EgoNight introduces tasks like day-night correspondence retrieval and egocentric depth estimation to further challenge and advance current models in this field.'}, 'zh': {'title': '夜间视觉问答的新基准：EgoNight', 'desc': 'EgoNight是一个全面的基准测试，专注于夜间自我中心视觉，特别是视觉问答（VQA）任务。现有的自我中心视觉基准主要集中在白天场景，忽视了低光照条件下的应用需求。EgoNight通过引入日夜对齐的视频，提升了夜间标注的质量，并揭示了不同光照条件下的性能差距。该基准包含3658个问答对，支持多种任务，旨在推动自我中心视觉研究的发展。'}}}, {'id': 'https://huggingface.co/papers/2510.05156', 'title': 'VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation', 'url': 'https://huggingface.co/papers/2510.05156', 'abstract': "VeriGuard is a framework that ensures formal safety guarantees for LLM-based agents through offline validation and online monitoring.  \t\t\t\t\tAI-generated summary \t\t\t\t The deployment of autonomous AI agents in sensitive domains, such as healthcare, introduces critical risks to safety, security, and privacy. These agents may deviate from user objectives, violate data handling policies, or be compromised by adversarial attacks. Mitigating these dangers necessitates a mechanism to formally guarantee that an agent's actions adhere to predefined safety constraints, a challenge that existing systems do not fully address. We introduce VeriGuard, a novel framework that provides formal safety guarantees for LLM-based agents through a dual-stage architecture designed for robust and verifiable correctness. The initial offline stage involves a comprehensive validation process. It begins by clarifying user intent to establish precise safety specifications. VeriGuard then synthesizes a behavioral policy and subjects it to both testing and formal verification to prove its compliance with these specifications. This iterative process refines the policy until it is deemed correct. Subsequently, the second stage provides online action monitoring, where VeriGuard operates as a runtime monitor to validate each proposed agent action against the pre-verified policy before execution. This separation of the exhaustive offline validation from the lightweight online monitoring allows formal guarantees to be practically applied, providing a robust safeguard that substantially improves the trustworthiness of LLM agents.", 'score': 3, 'issue_id': 6300, 'pub_date': '2025-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': 'cfcbeb2c67a2faa9', 'authors': ['Lesly Miculicich', 'Mihir Parmar', 'Hamid Palangi', 'Krishnamurthy Dj Dvijotham', 'Mirko Montanari', 'Tomas Pfister', 'Long T. Le'], 'affiliations': ['Google Cloud AI', 'Google Cloud AI Research', 'Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2510.05156.jpg', 'data': {'categories': ['#inference', '#alignment', '#agents', '#security', '#healthcare'], 'emoji': '🛡️', 'ru': {'title': 'Формальные гарантии безопасности для LLM-агентов', 'desc': 'VeriGuard — это фреймворк для обеспечения формальных гарантий безопасности AI-агентов на основе LLM в критических областях вроде здравоохранения. Система работает в два этапа: офлайн-валидация создаёт и формально верифицирует поведенческую политику агента на соответствие требованиям безопасности, а онлайн-мониторинг проверяет каждое действие агента перед его выполнением. Такое разделение позволяет проводить тщательную проверку заранее, а во время работы использовать лёгкий мониторинг. Подход защищает от отклонений агента от целей пользователя, нарушений политик обработки данных и adversarial-атак.'}, 'en': {'title': 'Ensuring Safety in AI Agents with VeriGuard', 'desc': "VeriGuard is a framework designed to ensure the safety of large language model (LLM)-based agents by providing formal guarantees through a two-stage process. The first stage involves offline validation, where user intent is clarified to create specific safety specifications, and a behavioral policy is synthesized and rigorously tested for compliance. The second stage focuses on online monitoring, where the agent's actions are continuously validated against the pre-verified policy before they are executed. This approach enhances the reliability of AI agents in sensitive areas by ensuring they adhere to safety constraints and reducing risks associated with their deployment."}, 'zh': {'title': 'VeriGuard：确保智能体安全的双阶段框架', 'desc': 'VeriGuard是一个框架，旨在为基于大型语言模型（LLM）的智能体提供正式的安全保障。它通过离线验证和在线监控的双阶段架构，确保智能体的行为符合预定义的安全约束。首先，在离线阶段，VeriGuard通过明确用户意图来建立安全规范，并合成行为策略，经过测试和正式验证以确保合规。然后，在在线阶段，VeriGuard作为运行时监控器，验证每个提议的智能体动作，以确保其符合预先验证的政策，从而提高LLM智能体的可信度。'}}}, {'id': 'https://huggingface.co/papers/2510.05122', 'title': 'CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support\n  Conversation', 'url': 'https://huggingface.co/papers/2510.05122', 'abstract': 'CARE is a framework that enhances cognitive reasoning in emotional support conversations through reinforcement learning, improving response quality and empathy without relying on large-scale synthetic data.  \t\t\t\t\tAI-generated summary \t\t\t\t Emotional Support Conversation (ESC) plays a vital role in alleviating psychological stress and providing emotional value through dialogue. While recent studies have largely focused on data augmentation and synthetic corpus construction, they often overlook the deeper cognitive reasoning processes that underpin effective emotional support. To address this gap, we propose CARE, a novel framework that strengthens reasoning in ESC without relying on large-scale synthetic data. CARE leverages the original ESC training set to guide models in generating logically coherent and supportive responses, thereby explicitly enhancing cognitive reasoning. Building on this foundation, we further employ reinforcement learning to refine and reinforce the reasoning process. Experimental results demonstrate that CARE significantly improves both the logical soundness and supportive quality of responses, advancing the development of empathetic, cognitively robust, and human-like emotional support systems.', 'score': 3, 'issue_id': 6301, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': 'a08f3bb063148c1c', 'authors': ['Jie Zhu', 'Yuanchen Zhou', 'Shuo Jiang', 'Junhui Li', 'Lifan Guo', 'Feng Chen', 'Chi Zhang', 'Fang Kong'], 'affiliations': ['Qwen DianJin Team, Alibaba Cloud Computing', 'School of Computer Science and Technology, Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2510.05122.jpg', 'data': {'categories': ['#rlhf', '#rl', '#reasoning'], 'emoji': '🤗', 'ru': {'title': 'Усиление когнитивного мышления для эмоциональной поддержки', 'desc': 'Статья представляет CARE — фреймворк для улучшения диалоговых систем эмоциональной поддержки. В отличие от существующих подходов, которые фокусируются на синтетических данных, CARE развивает когнитивное мышление модели, используя оригинальный набор данных для обучения. Система применяет reinforcement learning для генерации логически связных и поддерживающих ответов. Эксперименты показывают значительное улучшение качества эмпатии и логической последовательности ответов без необходимости в масштабных синтетических корпусах.'}, 'en': {'title': 'Enhancing Emotional Support with Cognitive Reasoning', 'desc': 'CARE is a framework designed to improve emotional support conversations (ESC) by enhancing cognitive reasoning through reinforcement learning. Unlike previous methods that depend on large amounts of synthetic data, CARE focuses on using existing training data to create more coherent and empathetic responses. The framework emphasizes the importance of logical reasoning in generating supportive dialogue, which is crucial for effective emotional support. Experimental results show that CARE significantly boosts the quality and empathy of responses, making AI systems more human-like in their interactions.'}, 'zh': {'title': 'CARE：提升情感支持对话的认知推理能力', 'desc': 'CARE是一个框架，通过强化学习增强情感支持对话中的认知推理，提升响应质量和同理心，而不依赖于大规模的合成数据。情感支持对话在缓解心理压力和提供情感价值方面起着重要作用。以往的研究主要集中在数据增强和合成语料库的构建上，忽视了有效情感支持背后的深层认知推理过程。CARE利用原始的情感支持对话训练集，引导模型生成逻辑连贯和支持性的响应，从而显著提升认知推理能力。'}}}, {'id': 'https://huggingface.co/papers/2510.06107', 'title': 'Distributional Semantics Tracing: A Framework for Explaining\n  Hallucinations in Large Language Models', 'url': 'https://huggingface.co/papers/2510.06107', 'abstract': "A framework called Distributional Semantics Tracing identifies the layers and pathways in Transformers where hallucinations occur, revealing a correlation between internal semantic coherence and hallucination rates.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are prone to hallucination, the generation of plausible yet factually incorrect statements. This work investigates the intrinsic, architectural origins of this failure mode through three primary contributions.First, to enable the reliable tracing of internal semantic failures, we propose Distributional Semantics Tracing (DST), a unified framework that integrates established interpretability techniques to produce a causal map of a model's reasoning, treating meaning as a function of context (distributional semantics). Second, we pinpoint the model's layer at which a hallucination becomes inevitable, identifying a specific commitment layer where a model's internal representations irreversibly diverge from factuality. Third, we identify the underlying mechanism for these failures. We observe a conflict between distinct computational pathways, which we interpret using the lens of dual-process theory: a fast, heuristic associative pathway (akin to System 1) and a slow, deliberate contextual pathway (akin to System 2), leading to predictable failure modes such as Reasoning Shortcut Hijacks. Our framework's ability to quantify the coherence of the contextual pathway reveals a strong negative correlation (rho = -0.863) with hallucination rates, implying that these failures are predictable consequences of internal semantic weakness. The result is a mechanistic account of how, when, and why hallucinations occur within the Transformer architecture.", 'score': 2, 'issue_id': 6304, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'b743e310dc0ab5e3', 'authors': ['Gagan Bhatia', 'Somayajulu G Sripada', 'Kevin Allan', 'Jacobo Azcona'], 'affiliations': ['University of Aberdeen'], 'pdf_title_img': 'assets/pdf/title_img/2510.06107.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#hallucinations', '#inference', '#interpretability'], 'emoji': '🔍', 'ru': {'title': 'Карта галлюцинаций: как и где LLM теряют связь с реальностью', 'desc': 'Исследователи разработали метод Distributional Semantics Tracing (DST), который позволяет отследить, на каких именно слоях трансформера возникают галлюцинации у больших языковых моделей. Оказалось, что существует критический «слой фиксации», после которого модель уже не может вернуться к правдивому ответу. Причина галлюцинаций кроется в конфликте двух вычислительных путей: быстрого ассоциативного (как Система 1 в психологии) и медленного контекстуального (Система 2). Обнаружена сильная корреляция между внутренней семантической согласованностью модели и частотой галлюцинаций, что делает эти ошибки предсказуемыми.'}, 'en': {'title': 'Tracing Hallucinations in Transformers: Understanding the Roots of AI Errors', 'desc': "This paper introduces a framework called Distributional Semantics Tracing (DST) to analyze where and why hallucinations occur in Transformer models. It combines various interpretability techniques to create a causal map of a model's reasoning, focusing on how meaning is influenced by context. The authors identify a specific layer in the model where hallucinations become unavoidable and explain the underlying mechanisms using dual-process theory, highlighting the conflict between fast and slow reasoning pathways. Their findings show a strong negative correlation between the coherence of the contextual pathway and hallucination rates, suggesting that these errors stem from weaknesses in the model's internal semantics."}, 'zh': {'title': '揭示变换器模型中的幻觉机制', 'desc': '本文提出了一种名为分布语义追踪（DST）的框架，用于识别变换器模型中幻觉发生的层次和路径。研究发现，模型的内部语义一致性与幻觉发生率之间存在显著的负相关关系。通过分析模型的计算路径，作者揭示了幻觉的根本机制，指出了快速启发式路径与慢速上下文路径之间的冲突。最终，本文为理解变换器架构中幻觉的发生提供了机制性解释。'}}}, {'id': 'https://huggingface.co/papers/2510.06030', 'title': 'Adaptive Pruning for Increased Robustness and Reduced Computational\n  Overhead in Gaussian Process Accelerated Saddle Point Searches', 'url': 'https://huggingface.co/papers/2510.06030', 'abstract': 'Geometry-aware optimal transport and active pruning enhance Gaussian process regression for efficient saddle point searches on high-dimensional energy surfaces.  \t\t\t\t\tAI-generated summary \t\t\t\t Gaussian process (GP) regression provides a strategy for accelerating saddle point searches on high-dimensional energy surfaces by reducing the number of times the energy and its derivatives with respect to atomic coordinates need to be evaluated. The computational overhead in the hyperparameter optimization can, however, be large and make the approach inefficient. Failures can also occur if the search ventures too far into regions that are not represented well enough by the GP model. Here, these challenges are resolved by using geometry-aware optimal transport measures and an active pruning strategy using a summation over Wasserstein-1 distances for each atom-type in farthest-point sampling, selecting a fixed-size subset of geometrically diverse configurations to avoid rapidly increasing cost of GP updates as more observations are made. Stability is enhanced by permutation-invariant metric that provides a reliable trust radius for early-stopping and a logarithmic barrier penalty for the growth of the signal variance. These physically motivated algorithmic changes prove their efficacy by reducing to less than a half the mean computational time on a set of 238 challenging configurations from a previously published data set of chemical reactions. With these improvements, the GP approach is established as, a robust and scalable algorithm for accelerating saddle point searches when the evaluation of the energy and atomic forces requires significant computational effort.', 'score': 2, 'issue_id': 6312, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'dd655c94092fb8a5', 'authors': ['Rohit Goswami', 'Hannes Jónsson'], 'affiliations': ['Science Institute & Faculty of Physical Sciences University of Iceland, 107 Reykjavík, Iceland', 'University of Iceland, 107 Reykjavík, Iceland'], 'pdf_title_img': 'assets/pdf/title_img/2510.06030.jpg', 'data': {'categories': ['#optimization', '#data', '#science', '#training'], 'emoji': '🏔️', 'ru': {'title': 'Ускорение поиска седловых точек с помощью геометрически осведомлённой гауссовской регрессии', 'desc': 'Статья посвящена ускорению поиска седловых точек на высокоразмерных энергетических поверхностях с помощью гауссовской регрессии. Авторы решают проблему растущих вычислительных затрат, используя оптимальный транспорт с учётом геометрии молекул и стратегию активного отсечения данных на основе расстояний Вассерштейна. Предложенный метод выбирает геометрически разнообразные конфигурации фиксированного размера и использует перестановочно-инвариантную метрику для определения надёжного радиуса доверия. На наборе из 238 сложных химических реакций алгоритм сократил среднее время вычислений более чем вдвое, что делает подход масштабируемым и робастным для задач квантовой химии.'}, 'en': {'title': 'Enhancing Gaussian Processes for Efficient Energy Surface Searches', 'desc': 'This paper presents enhancements to Gaussian process (GP) regression for more efficient saddle point searches in high-dimensional energy landscapes. It introduces geometry-aware optimal transport and an active pruning strategy to manage the computational costs associated with hyperparameter optimization and GP updates. By employing Wasserstein-1 distances, the method selects a diverse set of configurations, ensuring better representation of the energy surface. The proposed improvements significantly reduce computational time, making GP regression a more robust tool for complex energy evaluations in chemical reactions.'}, 'zh': {'title': '几何感知与主动修剪提升高斯过程回归效率', 'desc': '本文提出了一种改进的高斯过程回归方法，用于高维能量表面的鞍点搜索。通过几何感知的最优传输和主动修剪策略，减少了能量及其导数的计算次数，从而提高了效率。该方法利用Wasserstein-1距离进行远thest点采样，选择几何多样性的配置子集，避免了计算成本的快速增加。实验结果表明，该方法在238个复杂配置上将平均计算时间减少了一半，证明了其在鞍点搜索中的有效性和可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2510.05396', 'title': 'Scalable In-context Ranking with Generative Models', 'url': 'https://huggingface.co/papers/2510.05396', 'abstract': "BlockRank optimizes in-context ranking by enforcing inter-document block sparsity and enhancing query-document relevance, improving efficiency and scalability in large-scale information retrieval.  \t\t\t\t\tAI-generated summary \t\t\t\t In-context Ranking (ICR) is an emerging paradigm for Information Retrieval (IR), which leverages contextual understanding of LLMs by directly incorporating the task description, candidate documents, and the query into the model's input prompt and tasking the LLM to identify relevant document(s). While it is effective, efficiency is a significant challenge in this paradigm, especially as the candidate list grows due to quadratic/super-linear scaling of attention operation with context length. To this end, this paper first identifies inherent and exploitable structures in the attention of LLMs finetuned for ICR: (1) inter-document block sparsity: attention is dense within each document block but sparse across different documents in the context; and (2) query-document block relevance: the attention scores from certain query tokens to a document block in middle layers strongly correlate with that document's actual relevance. Motivated by these observations, we introduce BlockRank (Blockwise In-context Ranking), a novel method that adapts the attention operation in an LLM by (a) architecturally enforcing the observed inter-document block sparsity, reducing attention complexity from quadratic to linear without loss in performance, and (b) optimizing query-document block relevance for true relevant documents during fine-tuning using an auxiliary contrastive training objective, improving retrieval in attention. Experiments on BEIR, MSMarco and NQ with Mistral-7B demonstrate that FLARE Mistral matches or outperforms existing SOTA listwise rankers and controlled fine-tuned baseline while being significantly more efficient at inference (4.7x for 100 MSMarco documents in context) and scaling gracefully to long-context shortlists, around 500 documents in-context (approximately 100K context length) within a second, presenting a scalable and effective solution for ICR.", 'score': 2, 'issue_id': 6315, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': '55ca388238eda44f', 'authors': ['Nilesh Gupta', 'Chong You', 'Srinadh Bhojanapalli', 'Sanjiv Kumar', 'Inderjit Dhillon', 'Felix Yu'], 'affiliations': ['Google', 'Google DeepMind', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2510.05396.jpg', 'data': {'categories': ['#benchmark', '#training', '#long_context', '#data', '#architecture', '#optimization'], 'emoji': '📚', 'ru': {'title': 'BlockRank: Эффективное ранжирование в контексте с LLM', 'desc': 'В статье представлена новая методика BlockRank для оптимизации ранжирования в контексте, которая улучшает эффективность и масштабируемость в задачах информационного поиска. BlockRank использует особенности внимания LLM, такие как междокументная разреженность и релевантность блоков запроса-документа, чтобы снизить сложность внимания с квадратичной до линейной. Это достигается за счёт архитектурного внедрения разреженности и оптимизации релевантности блоков во время дообучения. Эксперименты показывают, что BlockRank превосходит существующие методы по эффективности и скорости обработки больших списков документов.'}, 'en': {'title': 'Efficient In-Context Ranking with BlockRank', 'desc': 'BlockRank is a new method designed to improve in-context ranking (ICR) in information retrieval by focusing on the structure of attention in large language models (LLMs). It identifies that attention is dense within document blocks but sparse across different documents, allowing for a more efficient attention mechanism. By enforcing inter-document block sparsity, BlockRank reduces the complexity of attention operations from quadratic to linear, maintaining performance while enhancing efficiency. Additionally, it optimizes the relevance of query-document pairs during training, leading to better retrieval results and scalability for large candidate lists.'}, 'zh': {'title': 'BlockRank：提升信息检索效率的新方法', 'desc': '本文提出了一种名为BlockRank的新方法，旨在优化上下文排名，特别是在信息检索中。通过强制文档间块稀疏性和增强查询-文档相关性，BlockRank提高了大规模信息检索的效率和可扩展性。研究表明，LLM的注意力机制在文档块内是密集的，而在不同文档之间是稀疏的，这一特性被有效利用。实验结果显示，BlockRank在处理长上下文时表现出色，显著提高了检索效率。'}}}, {'id': 'https://huggingface.co/papers/2510.03978', 'title': 'No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language\n  Models', 'url': 'https://huggingface.co/papers/2510.03978', 'abstract': 'Extending the context length of text encoders in vision-language models improves performance on biomedical caption tasks by utilizing longer and more detailed descriptions.  \t\t\t\t\tAI-generated summary \t\t\t\t Embedding vision-language models (VLMs) are typically pretrained with short text windows (<77 tokens), which forces the truncation of long-format captions. Yet, the distribution of biomedical captions from large-scale open source literature reveals that a huge portion of captions far exceed 77 tokens. To this end, we investigate the impact of pretraining on long-format biomedical captions by extending the context length of text encoders in VLMs. We find that longer context (thus, enabling additional supervision provided in long-format captions) correlates with better retrieval and classification performance. Given this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M image-caption pairs enriched with context-aware descriptions from full-text articles, providing longer and additional textual supervision. Using BIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a text encoder supporting windows of up to 512 tokens. Our model extends context capacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in Recall@1 and +2% average improvements in classification, while also converging faster than short-context. Our results demonstrate that long-context modeling is a promising direction for advancing biomedical VLMs.', 'score': 2, 'issue_id': 6301, 'pub_date': '2025-10-04', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}, 'hash': 'bb2f9d23dbe6e4fa', 'authors': ['Min Woo Sun', 'Alejandro Lozano', 'Javier Gamazo Tejero', 'Vishwesh Nath', 'Xiao Xiao Sun', 'James Burgess', 'Yuhui Zhang', 'Kun Yuan', 'Robert Tibshirani', 'Sean Huver', 'Serena Yeung-Levy'], 'affiliations': ['NVIDIA, USA', 'Stanford University, USA'], 'pdf_title_img': 'assets/pdf/title_img/2510.03978.jpg', 'data': {'categories': ['#data', '#benchmark', '#long_context', '#healthcare', '#dataset', '#multimodal'], 'emoji': '🔬', 'ru': {'title': 'Длинный контекст для биомедицинских изображений: больше слов — лучше результат', 'desc': 'Исследователи обнаружили, что стандартные vision-language модели с коротким контекстом (до 77 токенов) теряют важную информацию при обработке биомедицинских описаний изображений, которые часто содержат гораздо больше текста. Они создали датасет BIOMEDICA-LongCAP из 1 миллиона пар изображение-текст с расширенными описаниями из научных статей и обучили модель BMC-LongCLIP, поддерживающую до 512 токенов. Новая модель увеличила контекстное окно в 6.6 раз и сократила потери токенов с 55% до 2.2%, что привело к улучшению точности поиска изображений на 30% и классификации на 2%. Результаты показывают, что использование длинного контекста является перспективным направлением для развития мультимодальных моделей в биомедицине.'}, 'en': {'title': 'Unlocking the Power of Long Contexts in Biomedical Captioning', 'desc': 'This paper explores how increasing the context length of text encoders in vision-language models (VLMs) can enhance performance on biomedical captioning tasks. Traditional VLMs are limited to short text windows, which often leads to the loss of important information in longer biomedical captions. By extending the context length to 512 tokens, the authors introduce a new dataset, BIOMEDICA-LongCAP, which includes 1 million image-caption pairs with detailed descriptions. The results show that this approach significantly improves retrieval and classification metrics, highlighting the benefits of long-context modeling in biomedical applications.'}, 'zh': {'title': '扩展上下文，提升生物医学模型性能', 'desc': '本文探讨了在视觉语言模型中扩展文本编码器的上下文长度对生物医学描述任务的影响。传统的视觉语言模型通常使用较短的文本窗口，这导致长格式的描述被截断。研究发现，使用更长的上下文可以提高检索和分类的性能，因为它允许模型利用更详细的描述。为此，作者引入了一个新的数据集BIOMEDICA-LongCAP，并训练了支持长上下文的生物医学视觉语言模型BMC-LongCLIP，显著提升了模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2510.02341', 'title': 'DRIFT: Learning from Abundant User Dissatisfaction in Real-World\n  Preference Learning', 'url': 'https://huggingface.co/papers/2510.02341', 'abstract': 'DRIFT, a dissatisfaction-refined iterative preference training method, improves large language models using implicit user dissatisfaction signals, achieving better performance than existing methods on real-world datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world large language model deployments (e.g., conversational AI systems, code generation assistants) naturally generate abundant implicit user dissatisfaction (DSAT) signals, as users iterate toward better answers through refinements, corrections, and expressed preferences, while explicit satisfaction (SAT) feedback is scarce. Existing preference learning approaches are poorly aligned with this data profile, as they rely on costly human annotations or assume plentiful positive responses. In this paper, we introduce DRIFT (Dissatisfaction-Refined Iterative preFerence Training), which anchors training on real-world DSAT signals and samples positives dynamically from the evolving policy. Empirically, DRIFT models trained on real-world WildFeedback datasets and synthetic UltraFeedback datasets achieve up to +6.23\\% (7B) / +7.61\\% (14B) on WildBench Task Score and up to +8.95\\% (7B) / +12.29\\% (14B) on AlpacaEval2 win rate over base models, outperforming strong baseline methods such as iterative DPO and SPIN. At larger scales, the improvements are particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on WildBench. Further analysis shows that DRIFT also preserves exploratory capacity, yielding more diverse high-reward solutions rather than collapsing to narrow subsets. Theoretically, we demonstrate that this design preserves preference margins and avoids the gradient degeneration. These results show that DRIFT is an effective and scalable recipe for real-world post-training that leverages the most abundant and informative signal. The code and data are available at https://github.com/cacayaya/DRIFT.git.', 'score': 2, 'issue_id': 6298, 'pub_date': '2025-09-27', 'pub_date_card': {'ru': '27 сентября', 'en': 'September 27', 'zh': '9月27日'}, 'hash': '1bde94710320cd61', 'authors': ['Yifan Wang', 'Bolian Li', 'Junlin Wu', 'Zhaoxuan Tan', 'Zheli Liu', 'Ruqi Zhang', 'Ananth Grama', 'Qingkai Zeng'], 'affiliations': ['College of Computer Science, Nankai University', 'Department of Computer Science and Engineering, University of Notre Dame', 'Department of Computer Science, Purdue University', 'Department of Computer Science, Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2510.02341.jpg', 'data': {'categories': ['#optimization', '#alignment', '#training', '#rlhf'], 'emoji': '🔄', 'ru': {'title': 'Обучение LLM на недовольстве пользователей: превращаем негатив в качество', 'desc': 'В статье представлен метод DRIFT для обучения больших языковых моделей на основе сигналов недовольства пользователей. В реальных системах пользователи часто итеративно улучшают ответы через уточнения и исправления, создавая неявные сигналы неудовлетворённости, в то время как явная положительная обратная связь встречается редко. DRIFT использует эти сигналы недовольства как якорь для обучения и динамически генерирует положительные примеры из развивающейся политики модели. На датасетах WildFeedback и UltraFeedback метод показывает улучшение до +12.29% по win rate на AlpacaEval2, превосходя базовые методы как DPO и SPIN, при этом сохраняя разнообразие генерируемых решений.'}, 'en': {'title': 'Harnessing User Dissatisfaction for Better Language Models', 'desc': 'The paper introduces DRIFT, a novel training method for large language models that utilizes implicit user dissatisfaction signals to enhance performance. Unlike traditional methods that depend on explicit positive feedback, DRIFT focuses on refining preferences based on real-world user interactions, which often include dissatisfaction. This approach allows for dynamic sampling of positive responses, leading to improved model performance on various tasks. Empirical results demonstrate that DRIFT significantly outperforms existing methods, particularly in larger models, while also maintaining diversity in generated outputs.'}, 'zh': {'title': '利用用户不满信号提升语言模型性能', 'desc': 'DRIFT是一种基于用户不满信号的迭代偏好训练方法，旨在提升大型语言模型的性能。该方法利用真实世界中的用户不满信号，动态采样正反馈，从而更好地适应用户的需求。实验结果表明，使用DRIFT训练的模型在多个任务上显著超越了传统方法，尤其是在大规模模型上表现尤为突出。DRIFT不仅提高了模型的性能，还保持了探索能力，能够生成更多样化的高奖励解决方案。'}}}, {'id': 'https://huggingface.co/papers/2510.06213', 'title': 'Training Dynamics Impact Post-Training Quantization Robustness', 'url': 'https://huggingface.co/papers/2510.06213', 'abstract': 'Quantization robustness in large language models is influenced by learning rate and other hyperparameters, not dataset scale, as demonstrated through controlled training experiments.  \t\t\t\t\tAI-generated summary \t\t\t\t While post-training quantization is widely adopted for efficient deployment of large language models, the mechanisms underlying quantization robustness remain unclear. We conduct a comprehensive analysis of quantization degradation across open-source language model training trajectories up to 32B parameters and 15T training tokens to accurately assess the relationship between training dynamics and quantization performance. Our key finding is that quantization errors in large-scale training runs are driven by a complex interplay between learning rate and other training hyperparameters. Specifically, once learning rates decay, validation loss and quantization error diverge, largely independent of training data scale. To investigate interventions on the training dynamics and identify specific configurations that can modulate quantization robustness favorably, we train our own models in controlled experiments up to 100B tokens. Our results challenge the assumption that increasing dataset scale inherently compromises quantization effectiveness, demonstrating instead that strategic training hyperparameter interventions can improve quantization quality at scale.', 'score': 1, 'issue_id': 6307, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '301cc349b005a081', 'authors': ['Albert Catalan-Tatjer', 'Niccolò Ajroldi', 'Jonas Geiping'], 'affiliations': ['ELLIS Institute Tubingen', 'Max Planck Institute for Intelligent Systems', 'Tubingen AI Center'], 'pdf_title_img': 'assets/pdf/title_img/2510.06213.jpg', 'data': {'categories': ['#optimization', '#inference', '#training'], 'emoji': '⚖️', 'ru': {'title': 'Устойчивость к квантизации зависит от гиперпараметров, а не от объёма данных', 'desc': 'Исследователи изучили, как post-training квантизация работает в больших языковых моделях до 32B параметров, обученных на 15T токенах. Оказалось, что ошибки квантизации зависят от сложного взаимодействия learning rate и других гиперпараметров обучения, а не от масштаба датасета. После снижения learning rate метрики валидации и ошибки квантизации начинают расходиться независимо от объёма обучающих данных. Правильный подбор гиперпараметров позволяет улучшить качество квантизации даже при увеличении масштаба обучения.'}, 'en': {'title': 'Hyperparameters Over Dataset Size: Key to Quantization Robustness', 'desc': 'This paper explores how the robustness of quantization in large language models is affected by training hyperparameters, particularly the learning rate, rather than the size of the dataset. Through controlled experiments, the authors analyze the relationship between training dynamics and quantization performance across various model sizes and training token counts. They find that as learning rates decrease, the validation loss and quantization error begin to diverge, indicating that these factors are largely independent of the amount of training data. The study suggests that by strategically adjusting hyperparameters, it is possible to enhance quantization quality, challenging the notion that larger datasets automatically lead to worse quantization outcomes.'}, 'zh': {'title': '量化鲁棒性：超参数的力量', 'desc': '在大型语言模型中，量化鲁棒性受到学习率和其他超参数的影响，而与数据集规模无关。通过控制训练实验，我们分析了量化性能与训练动态之间的关系。研究发现，学习率的衰减会导致验证损失和量化误差的分歧，这一现象与训练数据规模基本无关。我们的结果表明，通过调整训练超参数，可以在大规模训练中改善量化质量。'}}}, {'id': 'https://huggingface.co/papers/2510.06139', 'title': 'Deforming Videos to Masks: Flow Matching for Referring Video\n  Segmentation', 'url': 'https://huggingface.co/papers/2510.06139', 'abstract': "FlowRVS addresses the challenges of Referring Video Object Segmentation by reformulating the task as a continuous flow problem, leveraging pretrained T2V models and achieving state-of-the-art results.  \t\t\t\t\tAI-generated summary \t\t\t\t Referring Video Object Segmentation (RVOS) requires segmenting specific objects in a video guided by a natural language description. The core challenge of RVOS is to anchor abstract linguistic concepts onto a specific set of pixels and continuously segment them through the complex dynamics of a video. Faced with this difficulty, prior work has often decomposed the task into a pragmatic `locate-then-segment' pipeline. However, this cascaded design creates an information bottleneck by simplifying semantics into coarse geometric prompts (e.g, point), and struggles to maintain temporal consistency as the segmenting process is often decoupled from the initial language grounding. To overcome these fundamental limitations, we propose FlowRVS, a novel framework that reconceptualizes RVOS as a conditional continuous flow problem. This allows us to harness the inherent strengths of pretrained T2V models, fine-grained pixel control, text-video semantic alignment, and temporal coherence. Instead of conventional generating from noise to mask or directly predicting mask, we reformulate the task by learning a direct, language-guided deformation from a video's holistic representation to its target mask. Our one-stage, generative approach achieves new state-of-the-art results across all major RVOS benchmarks. Specifically, achieving a J&F of 51.1 in MeViS (+1.6 over prior SOTA) and 73.3 in the zero shot Ref-DAVIS17 (+2.7), demonstrating the significant potential of modeling video understanding tasks as continuous deformation processes.", 'score': 1, 'issue_id': 6298, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '27bd729b0bb095f1', 'authors': ['Zanyi Wang', 'Dengyang Jiang', 'Liuzhuozheng Li', 'Sizhe Dang', 'Chengzu Li', 'Harry Yang', 'Guang Dai', 'Mengmeng Wang', 'Jingdong Wang'], 'affiliations': ['Baidu', 'SGIT AI Lab, State Grid Corporation of China', 'The Hong Kong University of Science and Technology', 'The University of Tokyo', 'University of California, San Diego', 'University of Cambridge', 'Zhejiang University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2510.06139.jpg', 'data': {'categories': ['#video', '#multimodal', '#benchmark', '#games', '#alignment'], 'emoji': '🌊', 'ru': {'title': 'Сегментация видео через непрерывную деформацию под управлением текста', 'desc': 'FlowRVS решает задачу сегментации объектов в видео по текстовому описанию (RVOS), переформулируя её как проблему непрерывного потока. Вместо традиционного подхода «найти-затем-сегментировать», метод использует pretrained text-to-video модели для прямой деформации видео-представления в целевую маску под управлением языкового описания. Это позволяет обеспечить точный контроль на уровне пикселей, семантическое выравнивание текста и видео, а также временную согласованность. Подход достигает state-of-the-art результатов на всех основных бенчмарках RVOS, включая MeViS и Ref-DAVIS17.'}, 'en': {'title': 'Revolutionizing Video Segmentation with Continuous Flow', 'desc': 'FlowRVS introduces a new way to tackle Referring Video Object Segmentation (RVOS) by treating it as a continuous flow problem. This approach allows the model to better connect language descriptions to specific video pixels, improving the segmentation process. Unlike previous methods that separate locating and segmenting tasks, FlowRVS maintains a unified framework that enhances temporal consistency and semantic understanding. By leveraging pretrained T2V models, it achieves state-of-the-art performance on major RVOS benchmarks, showcasing the effectiveness of continuous deformation in video understanding.'}, 'zh': {'title': 'FlowRVS：视频物体分割的新思路', 'desc': 'FlowRVS提出了一种新方法来解决视频物体分割中的引用问题，将其重新定义为一个连续流动问题。该方法利用预训练的文本到视频模型，实现了对视频中目标的精细像素控制和语义对齐。与传统的“定位-再分割”流程不同，FlowRVS通过直接学习语言引导的变形，从视频的整体表示到目标掩膜，保持了时间一致性。该框架在主要的引用视频物体分割基准测试中取得了新的最先进结果，展示了将视频理解任务建模为连续变形过程的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2510.06071', 'title': 'Benchmark It Yourself (BIY): Preparing a Dataset and Benchmarking AI\n  Models for Scatterplot-Related Tasks', 'url': 'https://huggingface.co/papers/2510.06071', 'abstract': "A benchmark for scatterplot-specific tasks using synthetic datasets evaluates AI models' performance in counting clusters and identifying outliers, with mixed results for localization tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t AI models are increasingly used for data analysis and visualization, yet benchmarks rarely address scatterplot-specific tasks, limiting insight into performance. To address this gap for one of the most common chart types, we introduce a synthetic, annotated dataset of over 18,000 scatterplots from six data generators and 17 chart designs, and a benchmark based on it. We evaluate proprietary models from OpenAI and Google using N-shot prompting on five distinct tasks derived from annotations of cluster bounding boxes, their center coordinates, and outlier coordinates. OpenAI models and Gemini 2.5 Flash, especially when prompted with examples, are viable options for counting clusters and, in Flash's case, outliers (90%+ Accuracy). However, the results for localization-related tasks are unsatisfactory: Precision and Recall are near or below 50%, except for Flash in outlier identification (65.01%). Furthermore, the impact of chart design on performance appears to be a secondary factor, but it is advisable to avoid scatterplots with wide aspect ratios (16:9 and 21:9) or those colored randomly. Supplementary materials are available at https://github.com/feedzai/biy-paper.", 'score': 1, 'issue_id': 6308, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'e1656709eb874610', 'authors': ['João Palmeiro', 'Diogo Duarte', 'Rita Costa', 'Pedro Bizarro'], 'affiliations': ['Feedzai'], 'pdf_title_img': 'assets/pdf/title_img/2510.06071.jpg', 'data': {'categories': ['#benchmark', '#synthetic', '#dataset', '#optimization'], 'emoji': '📊', 'ru': {'title': 'LLM учатся читать графики, но не всегда понимают где что находится', 'desc': 'Исследователи создали бенчмарк для оценки способности AI-моделей анализировать диаграммы рассеяния (scatterplots), используя синтетический датасет из более чем 18000 графиков. Модели OpenAI и Gemini 2.5 Flash показали высокую точность (более 90%) в подсчёте кластеров и выбросов при использовании N-shot prompting. Однако задачи локализации оказались сложными: точность и полнота составили около 50% или ниже, за исключением Flash в определении выбросов (65%). Дизайн графиков оказывает второстепенное влияние на результаты, но рекомендуется избегать широких соотношений сторон и случайной раскраски.'}, 'en': {'title': 'Benchmarking AI for Scatterplot Analysis', 'desc': 'This paper presents a benchmark for evaluating AI models on scatterplot-specific tasks, focusing on counting clusters and identifying outliers. It introduces a synthetic dataset of over 18,000 scatterplots created from various data generators and chart designs. The study assesses the performance of models from OpenAI and Google using N-shot prompting across five tasks related to cluster and outlier detection. While some models show high accuracy in counting clusters and identifying outliers, their performance in localization tasks is generally poor, highlighting the need for improved methods in this area.'}, 'zh': {'title': '散点图任务的AI性能评估基准', 'desc': '本论文提出了一个针对散点图特定任务的基准测试，使用合成数据集来评估人工智能模型在计数聚类和识别异常值方面的表现。我们创建了一个包含超过18,000个散点图的合成注释数据集，并基于此进行评估。通过对OpenAI和Google的专有模型进行N-shot提示，结果显示这些模型在计数聚类和异常值识别方面表现良好，但在定位任务上表现不佳。研究还发现，图表设计对性能的影响较小，但建议避免使用宽纵横比的散点图。'}}}, {'id': 'https://huggingface.co/papers/2510.06056', 'title': 'Scientific Algorithm Discovery by Augmenting AlphaEvolve with Deep\n  Research', 'url': 'https://huggingface.co/papers/2510.06056', 'abstract': 'DeepEvolve integrates deep research with algorithm evolution to propose, refine, implement, and test new hypotheses, improving initial algorithms across various scientific domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models hold promise as scientific assistants, yet existing agents either rely solely on algorithm evolution or on deep research in isolation, both of which face critical limitations. Pure algorithm evolution, as in AlphaEvolve, depends only on the internal knowledge of LLMs and quickly plateaus in complex domains, while pure deep research proposes ideas without validation, resulting in unrealistic or unimplementable solutions. We present DeepEvolve, an agent that integrates deep research with algorithm evolution, uniting external knowledge retrieval, cross-file code editing, and systematic debugging under a feedback-driven iterative loop. Each iteration not only proposes new hypotheses but also refines, implements, and tests them, avoiding both shallow improvements and unproductive over-refinements. Across nine benchmarks in chemistry, mathematics, biology, materials, and patents, DeepEvolve consistently improves the initial algorithm, producing executable new algorithms with sustained gains. By bridging the gap between unguided evolution and research without grounding, DeepEvolve provides a reliable framework for advancing scientific algorithm discovery. Our code is available at https://github.com/liugangcode/deepevolve.', 'score': 1, 'issue_id': 6311, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'a4df9d07d5dc32b9', 'authors': ['Gang Liu', 'Yihan Zhu', 'Jie Chen', 'Meng Jiang'], 'affiliations': ['MIT-IBM Watson AI Lab, IBM Research', 'University of Notre Dame'], 'pdf_title_img': 'assets/pdf/title_img/2510.06056.jpg', 'data': {'categories': ['#science', '#benchmark', '#agents', '#optimization', '#data', '#math'], 'emoji': '🧬', 'ru': {'title': 'Эволюция алгоритмов через глубокое исследование и валидацию', 'desc': 'DeepEvolve — это AI-агент, который объединяет глубокое исследование с эволюцией алгоритмов для научных открытий. В отличие от чистой эволюции алгоритмов (как в AlphaEvolve), которая опирается только на внутренние знания LLM и быстро достигает плато, DeepEvolve интегрирует поиск внешних знаний, редактирование кода и систематическую отладку в итеративном цикле с обратной связью. Каждая итерация не только предлагает новые гипотезы, но и уточняет, реализует и тестирует их, избегая как поверхностных улучшений, так и непродуктивных переработок. В экспериментах на девяти бенчмарках из химии, математики, биологии, материаловедения и патентов DeepEvolve стабильно улучшает исходные алгоритмы, создавая работоспособные решения с устойчивым ростом качества.'}, 'en': {'title': 'Bridging Research and Evolution for Better Algorithms', 'desc': 'DeepEvolve is a novel approach that combines deep research with algorithm evolution to enhance the development of scientific algorithms. Unlike existing methods that either focus solely on evolving algorithms or conducting research without practical validation, DeepEvolve integrates both strategies. It utilizes a feedback-driven iterative loop to propose, refine, implement, and test new hypotheses, ensuring that improvements are both meaningful and executable. This method has shown consistent success across various scientific fields, leading to significant advancements in algorithm performance.'}, 'zh': {'title': 'DeepEvolve：科学算法发现的新框架', 'desc': 'DeepEvolve 是一种将深度研究与算法进化相结合的智能体，旨在提出、完善、实施和测试新的假设，从而在多个科学领域中改进初始算法。与仅依赖算法进化或单独进行深度研究的现有智能体不同，DeepEvolve 通过反馈驱动的迭代循环，整合了外部知识检索、跨文件代码编辑和系统调试。每次迭代不仅提出新的假设，还对其进行完善、实施和测试，避免了表面改进和无效的过度完善。通过在化学、数学、生物学、材料和专利等九个基准测试中，DeepEvolve 一直在持续改进初始算法，生成可执行的新算法。'}}}, {'id': 'https://huggingface.co/papers/2510.05681', 'title': 'Verifier-free Test-Time Sampling for Vision Language Action Models', 'url': 'https://huggingface.co/papers/2510.05681', 'abstract': "MG-Select, a novel test-time scaling framework for Vision-Language-Action models, improves performance by using KL divergence from a reference distribution generated with masked inputs, achieving significant gains in both in-distribution and out-of-distribution tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action models (VLAs) have demonstrated remarkable performance in robot control. However, they remain fundamentally limited in tasks that require high precision due to their single-inference paradigm. While test-time scaling approaches using external verifiers have shown promise, they require additional training and fail to generalize to unseen conditions. We propose Masking Distribution Guided Selection (MG-Select), a novel test-time scaling framework for VLAs that leverages the model's internal properties without requiring additional training or external modules. Our approach utilizes KL divergence from a reference action token distribution as a confidence metric for selecting the optimal action from multiple candidates. We introduce a reference distribution generated by the same VLA but with randomly masked states and language conditions as inputs, ensuring maximum uncertainty while remaining aligned with the target task distribution. Additionally, we propose a joint training strategy that enables the model to learn both conditional and unconditional distributions by applying dropout to state and language conditions, thereby further improving the quality of the reference distribution. Our experiments demonstrate that MG-Select achieves significant performance improvements, including a 28%/35% improvement in real-world in-distribution/out-of-distribution tasks, along with a 168% relative gain on RoboCasa pick-and-place tasks trained with 30 demonstrations.", 'score': 1, 'issue_id': 6309, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'd398ec0b5549236c', 'authors': ['Suhyeok Jang', 'Dongyoung Kim', 'Changyeon Kim', 'Youngsuk Kim', 'Jinwoo Shin'], 'affiliations': ['KAIST', 'RLWRLD', 'Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2510.05681.jpg', 'data': {'categories': ['#cv', '#training', '#robotics', '#agents', '#optimization'], 'emoji': '🎯', 'ru': {'title': 'Умный выбор действий робота через маскирование входных данных', 'desc': 'В статье представлен MG-Select — новый метод улучшения работы Vision-Language-Action моделей для управления роботами на этапе тестирования. Подход использует KL-дивергенцию между распределением действий модели и референсным распределением, полученным при случайном маскировании входных данных, для выбора оптимального действия из нескольких кандидатов. Метод не требует дополнительного обучения или внешних модулей, используя только внутренние свойства самой модели. Эксперименты показывают улучшение на 28-35% на реальных задачах и 168% прирост на задачах манипуляции объектами.'}, 'en': {'title': 'Enhancing VLA Performance with MG-Select Framework', 'desc': "MG-Select is a new framework designed to enhance the performance of Vision-Language-Action (VLA) models during test time. It uses KL divergence to compare the model's action choices against a reference distribution created from masked inputs, allowing for better decision-making without needing extra training. This method helps the model select the best action from several options by measuring confidence based on internal properties. The results show that MG-Select significantly boosts performance in both familiar and unfamiliar tasks, making it a valuable advancement in robot control applications."}, 'zh': {'title': 'MG-Select：提升视觉-语言-动作模型性能的新方法', 'desc': 'MG-Select是一种新颖的测试时缩放框架，专为视觉-语言-动作模型设计。它通过使用从掩蔽输入生成的参考分布的KL散度来提高模型的性能，显著提升了模型在分布内和分布外任务的表现。该方法不需要额外的训练或外部模块，利用模型的内部特性来选择最佳动作。实验结果表明，MG-Select在真实世界任务中实现了显著的性能提升，特别是在RoboCasa的拾取和放置任务中表现尤为突出。'}}}, {'id': 'https://huggingface.co/papers/2510.04087', 'title': 'A Contextual Quality Reward Model for Reliable and Efficient Best-of-N\n  Sampling', 'url': 'https://huggingface.co/papers/2510.04087', 'abstract': 'A new framework using an outside option in preference data collection and modeling improves reliability and efficiency in preference alignment techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern preference alignment techniques, such as Best-of-N (BoN) sampling, rely on reward models trained with pairwise comparison data. While effective at learning relative preferences, this paradigm fails to capture a signal of response acceptability, leaving systems vulnerable to selecting the least bad of many unacceptable options. This is particularly problematic for hard prompts, where the risk of such false acceptances increases with the number of samples. In this paper, we address this critical reliability gap by introducing a new data collection and modeling framework. By augmenting preference data with an outside option, inspired by discrete choice models, we train a reward model that can distinguish not just what is better, but what is good enough. We leverage this capability to create an adaptive inference strategy, best of mini-N in-loop, which partitions the generation budget into sequential loops with a calibrated, early-exit condition. Our experiments show that when tuned as an alignment guardrail, it reduces reliability failures by 70\\%, and when tuned as an inference accelerator, it improves average inference speed by over 22\\% in IMDB-sentiment setting. We thus provide a principled and flexible framework for practitioners to explicitly manage the trade-off between reliability and computational efficiency.', 'score': 1, 'issue_id': 6298, 'pub_date': '2025-10-05', 'pub_date_card': {'ru': '5 октября', 'en': 'October 5', 'zh': '10月5日'}, 'hash': '77a7f937000c3b18', 'authors': ['Hyung Gyu Rho'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2510.04087.jpg', 'data': {'categories': ['#inference', '#training', '#data', '#rlhf', '#alignment'], 'emoji': '🚦', 'ru': {'title': 'Научить AI понимать, что хорошо, а не только что лучше', 'desc': 'Исследователи предлагают новый подход к обучению моделей предпочтений, добавляя «внешнюю опцию» в данные сравнений. Традиционные reward models умеют определять, какой ответ лучше, но не могут понять, является ли ответ вообще приемлемым. Новый метод позволяет модели различать не только относительное качество, но и абсолютную приемлемость ответов, что особенно важно для сложных запросов. Адаптивная стратегия генерации с ранним выходом снижает количество неприемлемых ответов на 70% и ускоряет inference на 22%.'}, 'en': {'title': 'Enhancing Preference Alignment with Outside Options', 'desc': 'This paper presents a new framework for improving preference alignment techniques in machine learning by incorporating an outside option in data collection. Traditional methods, like Best-of-N sampling, often fail to identify acceptable responses, leading to poor choices among suboptimal options. The proposed framework enhances reward models by allowing them to recognize not only better options but also those that are sufficiently good. Experimental results demonstrate significant improvements in reliability and efficiency, reducing failures by 70% and increasing inference speed by over 22%.'}, 'zh': {'title': '提升偏好对齐的可靠性与效率', 'desc': '本文提出了一种新的框架，通过在偏好数据收集和建模中引入外部选项，提升了偏好对齐技术的可靠性和效率。现代的偏好对齐技术，如最佳N（BoN）采样，依赖于通过成对比较数据训练的奖励模型，虽然能有效学习相对偏好，但未能捕捉响应可接受性的信号。我们通过引入外部选项，训练出能够区分不仅是更好而是足够好的奖励模型，从而解决了可靠性缺口。实验结果表明，该框架在对齐和推理加速方面均显著提高了性能，提供了一个灵活的管理可靠性与计算效率之间权衡的工具。'}}}, {'id': 'https://huggingface.co/papers/2509.21499', 'title': 'On Code-Induced Reasoning in LLMs', 'url': 'https://huggingface.co/papers/2509.21499', 'abstract': 'Systematic investigation reveals that large language models are more sensitive to structural than semantic code perturbations, with implications for training data design.  \t\t\t\t\tAI-generated summary \t\t\t\t Code data has been shown to enhance the reasoning capabilities of large language models (LLMs), but it remains unclear which aspects of code are most responsible. We investigate this question with a systematic, data-centric framework. We construct parallel instruction datasets in ten programming languages and apply controlled perturbations that selectively disrupt structural or semantic properties of code. We then finetune LLMs from five model families and eight scales on each variant and evaluate their performance on natural language, math, and code tasks. Across 3,331 experiments, our results show that LLMs are more vulnerable to structural perturbations than semantic ones, particularly on math and code tasks. Appropriate abstractions like pseudocode and flowcharts can be as effective as code, while encoding the same information with fewer tokens without adhering to original syntax can often retain or even improve performance. Remarkably, even corrupted code with misleading signals remains competitive when surface-level regularities persist. Finally, syntactic styles also shape task-specific gains with Python favoring natural language reasoning and lower-level languages such as Java and Rust favoring math. Through our systematic framework, we aim to provide insight into how different properties of code influence reasoning and inform the design of training data for enhancing LLM reasoning capabilities.', 'score': 1, 'issue_id': 6311, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': '4194d2df93542366', 'authors': ['Abdul Waheed', 'Zhen Wu', 'Carolyn Rosé', 'Daphne Ippolito'], 'affiliations': ['Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2509.21499.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#data', '#optimization', '#training', '#plp'], 'emoji': '🏗️', 'ru': {'title': 'Структура кода важнее смысла для обучения LLM', 'desc': 'Исследователи провели систематический анализ того, как различные свойства программного кода влияют на способности к рассуждению у больших языковых моделей. Проведя более 3000 экспериментов с контролируемыми изменениями кода на десяти языках программирования, они обнаружили, что LLM гораздо более чувствительны к нарушениям структуры кода, чем к семантическим искажениям. Оказалось, что псевдокод и блок-схемы могут быть столь же эффективны для обучения, как и реальный код, а даже повреждённый код с неверными сигналами остаётся конкурентоспособным при сохранении поверхностных паттернов. Исследование также показало, что выбор языка программирования имеет значение: Python лучше подходит для задач на естественном языке, тогда как низкоуровневые языки вроде Java и Rust — для математических задач.'}, 'en': {'title': 'Understanding LLM Sensitivity: Structure Over Semantics in Code', 'desc': 'This paper investigates how large language models (LLMs) respond to changes in code structure versus changes in code meaning. The authors create datasets in ten programming languages and apply specific alterations to either the structural or semantic aspects of the code. Their findings reveal that LLMs are more affected by structural changes, especially in tasks related to math and code. The study suggests that using simplified representations like pseudocode can maintain or even enhance model performance, indicating the importance of training data design for improving LLM reasoning.'}, 'zh': {'title': '结构优先：大型语言模型对代码扰动的敏感性', 'desc': '本研究系统地探讨了大型语言模型（LLMs）对代码结构和语义扰动的敏感性。结果表明，LLMs对结构性扰动的脆弱性高于语义性扰动，尤其是在数学和代码任务中。我们发现，适当的抽象形式如伪代码和流程图可以有效替代代码，同时用更少的标记编码相同信息，甚至可能提高性能。此外，不同的语法风格对任务特定的表现也有影响，Python更有利于自然语言推理，而Java和Rust等低级语言则更适合数学任务。'}}}, {'id': 'https://huggingface.co/papers/2510.06101', 'title': 'The Valley of Code Reasoning: Scaling Knowledge Distillation of Large\n  Language Models', 'url': 'https://huggingface.co/papers/2510.06101', 'abstract': 'Research on distilling coding skills from large language models to smaller ones reveals a "valley of code reasoning" where performance initially decreases with more data before improving sharply, and that small models benefit more from easier questions during distillation.  \t\t\t\t\tAI-generated summary \t\t\t\t Distilling the thinking traces of a Large Language Model (LLM) with reasoning capabilities into a smaller model has been proven effective. Yet, there is a scarcity of work done on how model performances scale with the quantity of distillation data. In this work, we study the scaling trend of distilling competitive coding skills on two small non-reasoning LLMs. We validate the hypothesis that there is a valley of code reasoning: downstream performance on competitive coding first drops as data quantity increases, then it steadily increases in a sharper-than-log-linear fashion. Having identified the trend, we further fine-tune the models at two different distillation stages on the same data to ground conclusions on their respective learning phases. We learn that across stages in the low and medium-low data regimes, small models benefit significantly from easier coding questions than from harder ones. We also find that, surprisingly, the correctness of outputs in training data makes no difference to distillation outcomes. Our work represents a step forward in understanding the training dynamics of code reasoning distillation outside intuition', 'score': 0, 'issue_id': 6310, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'ca03a603a5277862', 'authors': ['Muyu He', 'Muhammad Ali Shafique', 'Anand Kumar', 'Tsach Mackey', 'Nazneen Rajani'], 'affiliations': ['Collinear AI', 'DeepSeek', 'OpenCodeReasoning (OCR)', 'OpenThoughts', 'QwQ Team', 'rStar-Coder'], 'pdf_title_img': 'assets/pdf/title_img/2510.06101.jpg', 'data': {'categories': ['#transfer_learning', '#reasoning', '#small_models', '#training', '#optimization'], 'emoji': '🏔️', 'ru': {'title': 'Долина рассуждений: как маленькие модели учатся программировать через дистилляцию', 'desc': 'Исследователи изучали, как передать навыки программирования от больших языковых моделей к маленьким через дистилляцию. Они обнаружили эффект «долины рассуждений о коде»: при увеличении объёма данных для дистилляции производительность сначала падает, а затем резко возрастает. Оказалось, что маленькие модели лучше обучаются на простых задачах по программированию, чем на сложных, особенно на ранних этапах. Удивительно, но корректность ответов в обучающих данных не влияет на результаты дистилляции.'}, 'en': {'title': 'Navigating the Valley of Code Reasoning in Model Distillation', 'desc': "This paper explores how to transfer coding skills from large language models (LLMs) to smaller models through a process called distillation. It identifies a phenomenon termed the 'valley of code reasoning,' where the performance of smaller models initially declines as more training data is introduced, before improving significantly. The research shows that smaller models perform better when trained on simpler coding questions rather than more complex ones during the distillation process. Additionally, it reveals that the accuracy of the training data does not impact the effectiveness of the distillation, providing new insights into the training dynamics of code reasoning."}, 'zh': {'title': '揭示编码推理的谷底与提升之路', 'desc': '本研究探讨了从大型语言模型（LLM）提炼编码技能到小型模型的过程。我们发现存在一个“编码推理谷”，即在数据量增加时，模型性能最初会下降，随后又会迅速提升。小型模型在提炼过程中更容易从简单问题中获益，而不是复杂问题。我们的研究为理解编码推理提炼的训练动态提供了新的视角。'}}}, {'id': 'https://huggingface.co/papers/2510.05934', 'title': 'Revisiting Modeling and Evaluation Approaches in Speech Emotion\n  Recognition: Considering Subjectivity of Annotators and Ambiguity of Emotions', 'url': 'https://huggingface.co/papers/2510.05934', 'abstract': "Embracing minority ratings, multiple annotators, and multi-emotion predictions in speech emotion recognition improves system robustness and alignment with human perception.  \t\t\t\t\tAI-generated summary \t\t\t\t Over the past two decades, speech emotion recognition (SER) has received growing attention. To train SER systems, researchers collect emotional speech databases annotated by crowdsourced or in-house raters who select emotions from predefined categories. However, disagreements among raters are common. Conventional methods treat these disagreements as noise, aggregating labels into a single consensus target. While this simplifies SER as a single-label task, it ignores the inherent subjectivity of human emotion perception. This dissertation challenges such assumptions and asks: (1) Should minority emotional ratings be discarded? (2) Should SER systems learn from only a few individuals' perceptions? (3) Should SER systems predict only one emotion per sample?   Psychological studies show that emotion perception is subjective and ambiguous, with overlapping emotional boundaries. We propose new modeling and evaluation perspectives: (1) Retain all emotional ratings and represent them with soft-label distributions. Models trained on individual annotator ratings and jointly optimized with standard SER systems improve performance on consensus-labeled tests. (2) Redefine SER evaluation by including all emotional data and allowing co-occurring emotions (e.g., sad and angry). We propose an ``all-inclusive rule'' that aggregates all ratings to maximize diversity in label representation. Experiments on four English emotion databases show superior performance over majority and plurality labeling. (3) Construct a penalization matrix to discourage unlikely emotion combinations during training. Integrating it into loss functions further improves performance. Overall, embracing minority ratings, multiple annotators, and multi-emotion predictions yields more robust and human-aligned SER systems.", 'score': 0, 'issue_id': 6304, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '18b4f51d0ffd5cdd', 'authors': ['Huang-Cheng Chou', 'Chi-Chun Lee'], 'affiliations': ['ACII 2017 (國際情感計算會議)', 'APSIPA (Transactions on Signal and Information Processing)', 'Google (東亞學生旅行獎勵)', 'IEEE (Transactions on Affective Computing)', 'ISCA (Student Advisory Committee)', '中華扶輪 (扶輪獎學金)', '傑出人才發展基金會', '國家科學技術委員會', '國立清華大學 (校長博士生卓越獎學金)', '國立清華大學 電機工程學系', '國際口語溝通學會 (INTERSPEECH 2022)', '聯詠科技 (聯詠博士獎學金)', '計算語言學與中文語言處理學會'], 'pdf_title_img': 'assets/pdf/title_img/2510.05934.jpg', 'data': {'categories': ['#audio', '#alignment', '#interpretability'], 'emoji': '🎭', 'ru': {'title': 'Субъективность эмоций как преимущество: учёт мнения меньшинства в распознавании речи', 'desc': 'Диссертация посвящена распознаванию эмоций в речи (SER) и предлагает отказаться от традиционного подхода, где разногласия между аннотаторами считаются шумом. Авторы показывают, что использование всех оценок эмоций, включая мнения меньшинства, и представление их в виде soft-label распределений улучшает качество моделей. Предложено новое правило агрегации данных, которое сохраняет разнообразие эмоциональных меток и позволяет предсказывать несколько одновременных эмоций. Эксперименты на четырёх англоязычных датасетах подтверждают, что учёт субъективности восприятия эмоций и работа с множественными аннотаторами делают SER-системы более робастными и согласованными с человеческим восприятием.'}, 'en': {'title': 'Enhancing Speech Emotion Recognition through Diversity in Emotion Representation', 'desc': 'This paper discusses improvements in speech emotion recognition (SER) by incorporating minority ratings, multiple annotators, and multi-emotion predictions. It argues that traditional methods, which simplify emotional data into single labels, overlook the complexity of human emotion perception. By using soft-label distributions and allowing for co-occurring emotions, the proposed approach enhances model performance and aligns better with human understanding of emotions. The findings demonstrate that embracing diverse emotional inputs leads to more robust SER systems.'}, 'zh': {'title': '拥抱多样性，提升语音情感识别的鲁棒性', 'desc': '这篇论文探讨了语音情感识别（SER）中的情感标注问题，强调了少数标注和多情感预测的重要性。传统方法将标注者之间的分歧视为噪声，简单地聚合成单一标签，忽视了情感感知的主观性。论文提出保留所有情感评分，并使用软标签分布来训练模型，从而提高系统的性能。通过引入惩罚矩阵和多情感预测，研究表明这种方法能更好地与人类情感感知对齐，增强系统的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2510.04514', 'title': 'ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in\n  Complex Chart Question Answering', 'url': 'https://huggingface.co/papers/2510.04514', 'abstract': "ChartAgent, a novel agentic framework, performs visual reasoning directly within charts, achieving state-of-the-art accuracy on ChartBench and ChartX benchmarks by iteratively decomposing queries and using specialized visual actions.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent multimodal LLMs have shown promise in chart-based visual question answering, but their performance declines sharply on unannotated charts, those requiring precise visual interpretation rather than relying on textual shortcuts. To address this, we introduce ChartAgent, a novel agentic framework that explicitly performs visual reasoning directly within the chart's spatial domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively decomposes queries into visual subtasks and actively manipulates and interacts with chart images through specialized actions such as drawing annotations, cropping regions (e.g., segmenting pie slices, isolating bars), and localizing axes, using a library of chart-specific vision tools to fulfill each subtask. This iterative reasoning process closely mirrors human cognitive strategies for chart comprehension. ChartAgent achieves state-of-the-art accuracy on the ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07% absolute gain overall and 17.31% on unannotated, numerically intensive queries. Furthermore, our analyses show that ChartAgent is (a) effective across diverse chart types, (b) achieve the highest scores across varying visual and reasoning complexity levels, and (c) serves as a plug-and-play framework that boosts performance across diverse underlying LLMs. Our work is among the first to demonstrate visually grounded reasoning for chart understanding using tool-augmented multimodal agents.", 'score': 0, 'issue_id': 6313, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': 'b607bdb92d04b3d9', 'authors': ['Rachneet Kaur', 'Nishan Srishankar', 'Zhen Zeng', 'Sumitra Ganesh', 'Manuela Veloso'], 'affiliations': ['J.P. Morgan AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.04514.jpg', 'data': {'categories': ['#reasoning', '#agents', '#multimodal', '#interpretability', '#benchmark', '#cv'], 'emoji': '📊', 'ru': {'title': 'Визуальное мышление для понимания графиков через агентные инструменты', 'desc': 'ChartAgent — это новый агентный фреймворк для анализа графиков и диаграмм, который выполняет визуальное рассуждение непосредственно в пространстве изображения. В отличие от текстовых методов chain-of-thought, система итеративно разбивает запросы на визуальные подзадачи и активно манипулирует изображениями через специализированные инструменты: рисует аннотации, вырезает области, локализует оси. ChartAgent достигает state-of-the-art результатов на бенчмарках ChartBench и ChartX, превосходя предыдущие методы на 16-17% на графиках без текстовых подсказок. Фреймворк работает как plug-and-play решение, улучшая производительность различных базовых LLM моделей.'}, 'en': {'title': 'ChartAgent: Mastering Visual Reasoning in Charts', 'desc': 'ChartAgent is a new framework designed for visual reasoning in charts, which enhances the ability to answer questions about chart data. It works by breaking down complex queries into smaller visual tasks and using specific actions to interact with the chart, such as drawing or cropping. This method allows ChartAgent to achieve high accuracy on benchmarks like ChartBench and ChartX, outperforming previous models significantly. The framework is versatile, effective across different types of charts, and can improve the performance of various underlying language models.'}, 'zh': {'title': 'ChartAgent：图表理解的新智能框架', 'desc': 'ChartAgent是一种新颖的智能框架，能够直接在图表中进行视觉推理。它通过将查询迭代分解为视觉子任务，并使用专门的视觉操作来与图表图像进行交互，从而实现了在ChartBench和ChartX基准测试中达到最先进的准确率。与传统的文本推理不同，ChartAgent采用了更接近人类认知策略的方式，能够有效处理各种类型的图表。该框架不仅在未注释的图表上表现出色，还能提升多种基础大型语言模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2510.00880', 'title': 'HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate\n  Hallucinations in Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2510.00880', 'abstract': 'HalluGuard, a 4B-parameter Small Reasoning Model, effectively mitigates hallucinations in Retrieval-Augmented Generation by classifying document-claim pairs and providing evidence-grounded justifications, achieving high balanced accuracy on the LLM-AggreFact benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) excel in many NLP tasks but remain prone to hallucinations, limiting trust in real-world applications. We present HalluGuard, a 4B-parameter Small Reasoning Model (SRM) for mitigating hallucinations in Retrieval-Augmented Generation (RAG). HalluGuard classifies document-claim pairs as grounded or hallucinated and produces evidence-grounded justifications for transparency. Our approach combines (i) a domain-agnostic synthetic dataset derived from FineWeb and refined through multi-stage curation and data reformation, (ii) synthetic grounded and hallucinated claims, and (iii) preference-based fine-tuning with Odds Ratio Preference Optimization to distill large-model reasoning into a smaller backbone. On the RAGTruth subset of the LLM-AggreFact benchmark, HalluGuard achieves 84.0% balanced accuracy (BAcc), rivaling specialized models, MiniCheck (7B; 84.0%) and Granite Guardian 3.3 (8B; 82.2%) while using roughly half their parameters. Over the full benchmark it reaches 75.7% BAcc, matching larger general-purpose LLMs such as GPT-4o (75.9%). We will release HalluGuard and datasets under Apache 2.0 upon acceptance.', 'score': 0, 'issue_id': 6304, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 октября', 'en': 'October 1', 'zh': '10月1日'}, 'hash': '436314721ab72bf1', 'authors': ['Loris Bergeron', 'Ioana Buhnila', 'Jérôme François', 'Radu State'], 'affiliations': ['ATILF, University of LorraineCNRS', 'Banque de Luxembourg', 'Center for Data Science in Humanities, Chosun University', 'SnT, University of Luxembourg'], 'pdf_title_img': 'assets/pdf/title_img/2510.00880.jpg', 'data': {'categories': ['#training', '#hallucinations', '#dataset', '#rag', '#small_models', '#synthetic', '#benchmark', '#optimization'], 'emoji': '🛡️', 'ru': {'title': 'HalluGuard: Защита от галлюцинаций в генерации текста', 'desc': 'HalluGuard — это модель с 4 миллиардами параметров, которая помогает уменьшить галлюцинации в Retrieval-Augmented Generation. Она классифицирует пары документов и утверждений, определяя, какие из них основаны на фактах, а какие — на галлюцинациях. Модель использует синтетические данные и специальную настройку для достижения высокой точности. HalluGuard показывает результаты, сопоставимые с более крупными моделями, но с меньшим количеством параметров.'}, 'en': {'title': 'HalluGuard: Reducing Hallucinations with Small Reasoning Power', 'desc': 'HalluGuard is a Small Reasoning Model with 4 billion parameters designed to reduce hallucinations in Retrieval-Augmented Generation tasks. It works by classifying document-claim pairs as either grounded or hallucinated and provides justifications based on evidence to enhance transparency. The model is trained on a synthetic dataset that has been carefully curated and refined, allowing it to perform effectively with fewer parameters than larger models. HalluGuard achieves competitive accuracy on the LLM-AggreFact benchmark, demonstrating its capability to rival larger models while maintaining efficiency.'}, 'zh': {'title': 'HalluGuard：减少幻觉，提高生成透明度', 'desc': 'HalluGuard是一种具有40亿参数的小型推理模型，旨在减少检索增强生成中的幻觉现象。它通过对文档-声明对进行分类，判断其是否为真实依据，并提供基于证据的解释，从而提高透明度。该模型在LLM-AggreFact基准测试的RAGTruth子集上达到了84.0%的平衡准确率，表现与一些专门模型相当。HalluGuard结合了合成数据集和偏好优化的微调方法，成功将大型模型的推理能力提炼到更小的模型中。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (11)', '#agi (1)', '#alignment (7)', '#architecture (8)', '#audio (2)', '#benchmark (18)', '#cv (5)', '#data (11)', '#dataset (8)', '#diffusion (7)', '#ethics', '#games (5)', '#graphs', '#hallucinations (3)', '#healthcare (4)', '#inference (8)', '#interpretability (7)', '#leakage (1)', '#long_context (4)', '#low_resource', '#machine_translation', '#math (3)', '#multilingual', '#multimodal (11)', '#open_source (4)', '#optimization (25)', '#plp (1)', '#rag (2)', '#reasoning (18)', '#rl (8)', '#rlhf (5)', '#robotics (1)', '#science (5)', '#security (1)', '#small_models (4)', '#story_generation (1)', '#survey', '#synthetic (3)', '#training (24)', '#transfer_learning (2)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-10-08 23:10',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-10-08 23:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-10-08 23:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    