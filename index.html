
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 18 papers. April 18.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">18 апреля</span> | <span id="title-articles-count">18 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-04-17.html">⬅️ <span id="prev-date">17.04</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-04-21.html">➡️ <span id="next-date">21.04</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-04.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '18 апреля', 'en': 'April 18', 'zh': '4月18日'};
        let feedDateNext = {'ru': '21.04', 'en': '04/21', 'zh': '4月21日'};
        let feedDatePrev = {'ru': '17.04', 'en': '04/17', 'zh': '4月17日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2504.13161', 'title': 'CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for\n  Language Model Pre-training', 'url': 'https://huggingface.co/papers/2504.13161', 'abstract': 'Pre-training datasets are typically collected from web content and lack inherent domain divisions. For instance, widely used datasets like Common Crawl do not include explicit domain labels, while manually curating labeled datasets such as The Pile is labor-intensive. Consequently, identifying an optimal pre-training data mixture remains a challenging problem, despite its significant benefits for pre-training performance. To address these challenges, we propose CLustering-based Iterative Data Mixture Bootstrapping (CLIMB), an automated framework that discovers, evaluates, and refines data mixtures in a pre-training setting. Specifically, CLIMB embeds and clusters large-scale datasets in a semantic space and then iteratively searches for optimal mixtures using a smaller proxy model and a predictor. When continuously trained on 400B tokens with this mixture, our 1B model exceeds the state-of-the-art Llama-3.2-1B by 2.0%. Moreover, we observe that optimizing for a specific domain (e.g., Social Sciences) yields a 5% improvement over random sampling. Finally, we introduce ClimbLab, a filtered 1.2-trillion-token corpus with 20 clusters as a research playground, and ClimbMix, a compact yet powerful 400-billion-token dataset designed for efficient pre-training that delivers superior performance under an equal token budget. We analyze the final data mixture, elucidating the characteristics of an optimal data mixture. Our data is available at: https://research.nvidia.com/labs/lpr/climb/', 'score': 54, 'issue_id': 3306, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': 'cf9b6f2a06448097', 'authors': ['Shizhe Diao', 'Yu Yang', 'Yonggan Fu', 'Xin Dong', 'Dan Su', 'Markus Kliegl', 'Zijia Chen', 'Peter Belcak', 'Yoshi Suhara', 'Hongxu Yin', 'Mostofa Patwary', 'Yingyan', 'Lin', 'Jan Kautz', 'Pavlo Molchanov'], 'affiliations': ['Georgia Institute of Technology, USA', 'NVIDIA', 'OpenAI'], 'pdf_title_img': 'assets/pdf/title_img/2504.13161.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#optimization', '#data'], 'emoji': '🧗', 'ru': {'title': 'Восхождение к оптимальным данным для предобучения языковых моделей', 'desc': 'Статья представляет CLIMB - автоматизированный фреймворк для обнаружения, оценки и улучшения смесей данных для предварительного обучения языковых моделей. CLIMB использует кластеризацию в семантическом пространстве и итеративный поиск оптимальных смесей с помощью прокси-модели. Применение CLIMB позволило создать модель, превосходящую Llama-3.2-1B на 2%, а также улучшить результаты для конкретных доменов. Авторы также представляют ClimbLab и ClimbMix - наборы данных для исследований и эффективного предобучения.'}, 'en': {'title': 'Optimizing Pre-Training Data with CLIMB for Superior Model Performance', 'desc': 'This paper introduces CLustering-based Iterative Data Mixture Bootstrapping (CLIMB), a novel framework for optimizing pre-training datasets in machine learning. CLIMB automates the process of discovering and refining data mixtures by embedding and clustering large datasets in a semantic space, which helps in identifying the best combinations for training models. The results show that a model trained on a carefully optimized mixture of 400 billion tokens outperforms existing models, demonstrating the importance of domain-specific data selection. Additionally, the paper presents ClimbLab and ClimbMix, two new datasets designed to facilitate research and improve pre-training efficiency.'}, 'zh': {'title': '优化预训练数据的智能框架', 'desc': '本论文提出了一种名为CLIMB的自动化框架，用于优化预训练数据的混合。CLIMB通过在语义空间中嵌入和聚类大规模数据集，迭代搜索最佳数据组合。实验表明，使用这种混合数据进行训练的模型在性能上超过了现有的最先进模型，并且针对特定领域的优化可以显著提高效果。最后，我们还推出了ClimbLab和ClimbMix两个数据集，以支持进一步的研究和高效的预训练。'}}}, {'id': 'https://huggingface.co/papers/2504.13146', 'title': 'Antidistillation Sampling', 'url': 'https://huggingface.co/papers/2504.13146', 'abstract': "Frontier models that generate extended reasoning traces inadvertently produce rich token sequences that can facilitate model distillation. Recognizing this vulnerability, model owners may seek sampling strategies that limit the effectiveness of distillation without compromising model performance. Antidistillation sampling provides exactly this capability. By strategically modifying a model's next-token probability distribution, antidistillation sampling poisons reasoning traces, rendering them significantly less effective for distillation while preserving the model's practical utility. For further details, see https://antidistillation.com.", 'score': 45, 'issue_id': 3304, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': '6aa117b5c441eb3d', 'authors': ['Yash Savani', 'Asher Trockman', 'Zhili Feng', 'Avi Schwarzschild', 'Alexander Robey', 'Marc Finzi', 'J. Zico Kolter'], 'affiliations': ['Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2504.13146.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training'], 'emoji': '🛡️', 'ru': {'title': 'Защита моделей от дистилляции с помощью антидистилляции', 'desc': 'В статье рассматривается проблема защиты моделей от дистилляции, когда модели генерируют сложные последовательности токенов. Авторы предлагают метод антидистилляционной выборки, который изменяет распределение вероятностей следующего токена. Это делает следы рассуждений менее полезными для дистилляции, сохраняя при этом производительность модели. Таким образом, можно защитить интеллектуальную собственность моделей, не ухудшая их работу.'}, 'en': {'title': 'Protecting Model Knowledge with Antidistillation Sampling', 'desc': "This paper discusses a method called antidistillation sampling, which aims to protect advanced models from being easily distilled into simpler versions. Distillation is a process where a complex model's knowledge is transferred to a smaller model, but this can be exploited if the complex model generates detailed reasoning traces. Antidistillation sampling works by altering the probability distribution of the next token generated by the model, making the reasoning less useful for distillation. This approach allows the model to maintain its performance while reducing the risk of its knowledge being easily extracted."}, 'zh': {'title': '抗蒸馏采样：保护模型性能的创新策略', 'desc': '前沿模型生成的推理轨迹会产生丰富的标记序列，这些序列可以帮助模型蒸馏。为了应对这一漏洞，模型拥有者可能会寻找采样策略，以限制蒸馏的有效性，同时不影响模型性能。抗蒸馏采样正是提供这种能力的策略。通过战略性地修改模型的下一个标记概率分布，抗蒸馏采样可以破坏推理轨迹，使其在蒸馏中变得不那么有效，同时保持模型的实际效用。'}}}, {'id': 'https://huggingface.co/papers/2504.12322', 'title': 'A Strategic Coordination Framework of Small LLMs Matches Large LLMs in\n  Data Synthesis', 'url': 'https://huggingface.co/papers/2504.12322', 'abstract': 'While data synthesis and distillation are promising strategies to enhance small language models, current approaches heavily rely on Large Language Models (LLMs), which suffer from high computational costs, environmental inefficiency, and potential biases inherited from monolithic architectures. In contrast, smaller LLMs are more accessible and sustainable, but their individual capabilities often fall short in generating high-quality, diverse, and reliable data. Inspired by collaborative human processes (e.g., peer review), we propose a multiple small LLMs involved framework, GRA, that aggregates specialized roles across small LLMs to iterative refinement and quality control typically achieved by a single large LLM. In this collaborative framework, multiple small LLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a peer-review-inspired data synthesis pipeline. The Generator proposes initial data samples, the Reviewer critiques their quality and diversity, and the Adjudicator resolves conflicts to finalize the output. By decomposing the synthesis process into specialized sub-tasks, collaborative small LLMs can achieve data-level parity with large LLM-based distillation. Through experiments across multiple benchmarks, we demonstrate that GRA-produced data matches or exceeds the quality of single large LLM outputs, e.g., Qwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large models for high-quality data synthesis, advocating instead for strategic coordination of smaller agents. Our datasets, models, and code are publicly available at https://github.com/GX-XinGao/GRA.', 'score': 20, 'issue_id': 3307, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 апреля', 'en': 'April 11', 'zh': '4月11日'}, 'hash': 'cc80e8015cf7f78d', 'authors': ['Xin Gao', 'Qizhi Pei', 'Zinan Tang', 'Yu Li', 'Honglin Lin', 'Jiang Wu', 'Conghui He', 'Lijun Wu'], 'affiliations': ['Renmin University of China', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2504.12322.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#synthetic', '#small_models', '#open_source'], 'emoji': '🤝', 'ru': {'title': 'Коллаборативный синтез данных: малые модели, большие результаты', 'desc': 'Статья представляет новый подход к синтезу данных с использованием малых языковых моделей (LLM) вместо крупных. Авторы предлагают фреймворк GRA, в котором несколько малых LLM выполняют роли Генератора, Рецензента и Арбитра, имитируя процесс экспертной оценки. Этот метод позволяет достичь качества данных, сравнимого с результатами крупных LLM, но с меньшими вычислительными затратами. Эксперименты показывают, что данные, созданные GRA, не уступают или превосходят по качеству выходные данные крупных LLM, таких как Qwen-2.5-72B-Instruct.'}, 'en': {'title': 'Collaborative Small Models for High-Quality Data Synthesis', 'desc': 'This paper introduces a framework called GRA, which uses multiple small language models (LLMs) to improve data synthesis and distillation. Instead of relying on a single large LLM, GRA assigns specialized roles to smaller models: a Generator creates data samples, a Reviewer evaluates their quality, and an Adjudicator resolves any discrepancies. This collaborative approach mimics peer review processes, allowing for iterative refinement and quality control. The results show that the data produced by GRA can match or even surpass the quality of data generated by large LLMs, suggesting that smaller models can be effectively coordinated for high-quality outcomes.'}, 'zh': {'title': '小模型协作，超越大模型的高质量数据合成', 'desc': '本文提出了一种名为GRA的框架，旨在通过多个小型语言模型（LLMs）协作来生成高质量的数据。该框架模拟了同行评审的过程，分配了生成器、审阅者和裁决者等不同角色，以实现数据合成的迭代优化和质量控制。通过将合成过程分解为专门的子任务，GRA能够在数据质量上与大型语言模型相媲美。实验结果表明，GRA生成的数据在质量上与单一大型模型的输出相当或更优，挑战了依赖大型模型进行高质量数据合成的必要性。'}}}, {'id': 'https://huggingface.co/papers/2504.12626', 'title': 'Packing Input Frame Context in Next-Frame Prediction Models for Video\n  Generation', 'url': 'https://huggingface.co/papers/2504.12626', 'abstract': 'We present a neural network structure, FramePack, to train next-frame (or next-frame-section) prediction models for video generation. The FramePack compresses input frames to make the transformer context length a fixed number regardless of the video length. As a result, we are able to process a large number of frames using video diffusion with computation bottleneck similar to image diffusion. This also makes the training video batch sizes significantly higher (batch sizes become comparable to image diffusion training). We also propose an anti-drifting sampling method that generates frames in inverted temporal order with early-established endpoints to avoid exposure bias (error accumulation over iterations). Finally, we show that existing video diffusion models can be finetuned with FramePack, and their visual quality may be improved because the next-frame prediction supports more balanced diffusion schedulers with less extreme flow shift timesteps.', 'score': 19, 'issue_id': 3303, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': 'fd1688a4e26dbb32', 'authors': ['Lvmin Zhang', 'Maneesh Agrawala'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2504.12626.jpg', 'data': {'categories': ['#architecture', '#optimization', '#diffusion', '#video', '#training'], 'emoji': '🎞️', 'ru': {'title': 'FramePack: эффективное предсказание кадров для генерации видео', 'desc': 'FramePack - это новая структура нейронной сети для предсказания следующего кадра в видео. Она сжимает входные кадры, что позволяет обрабатывать большое количество кадров с вычислительной сложностью, сравнимой с диффузией изображений. Авторы также предлагают метод сэмплирования, предотвращающий накопление ошибок. FramePack может быть использован для дообучения существующих моделей видеодиффузии, потенциально улучшая их визуальное качество.'}, 'en': {'title': 'FramePack: Efficient Video Generation with Next-Frame Prediction', 'desc': 'The paper introduces FramePack, a novel neural network architecture designed for predicting the next frame in video generation. By compressing input frames, FramePack ensures that the transformer can handle a fixed context length, making it efficient for processing long videos. This approach allows for larger training batch sizes, similar to those used in image diffusion, while maintaining computational efficiency. Additionally, the authors present an anti-drifting sampling method to mitigate exposure bias, enhancing the quality of generated frames and improving existing video diffusion models through fine-tuning.'}, 'zh': {'title': 'FramePack：提升视频生成的下一帧预测能力', 'desc': '我们提出了一种神经网络结构，FramePack，用于训练视频生成的下一帧预测模型。FramePack通过压缩输入帧，使得变换器的上下文长度固定，无论视频长度如何。这样，我们能够使用与图像扩散相似的计算瓶颈处理大量帧，从而显著提高视频训练的批量大小。我们还提出了一种反漂移采样方法，以避免迭代过程中的曝光偏差，从而提高生成帧的质量。'}}}, {'id': 'https://huggingface.co/papers/2504.12369', 'title': 'WORLDMEM: Long-term Consistent World Simulation with Memory', 'url': 'https://huggingface.co/papers/2504.12369', 'abstract': 'World simulation has gained increasing popularity due to its ability to model virtual environments and predict the consequences of actions. However, the limited temporal context window often leads to failures in maintaining long-term consistency, particularly in preserving 3D spatial consistency. In this work, we present WorldMem, a framework that enhances scene generation with a memory bank consisting of memory units that store memory frames and states (e.g., poses and timestamps). By employing a memory attention mechanism that effectively extracts relevant information from these memory frames based on their states, our method is capable of accurately reconstructing previously observed scenes, even under significant viewpoint or temporal gaps. Furthermore, by incorporating timestamps into the states, our framework not only models a static world but also captures its dynamic evolution over time, enabling both perception and interaction within the simulated world. Extensive experiments in both virtual and real scenarios validate the effectiveness of our approach.', 'score': 19, 'issue_id': 3303, 'pub_date': '2025-04-16', 'pub_date_card': {'ru': '16 апреля', 'en': 'April 16', 'zh': '4月16日'}, 'hash': '79cf162a1b60f887', 'authors': ['Zeqi Xiao', 'Yushi Lan', 'Yifan Zhou', 'Wenqi Ouyang', 'Shuai Yang', 'Yanhong Zeng', 'Xingang Pan'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Shanghai AI Laboratory', 'Wangxuan Institute of Computer Technology, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2504.12369.jpg', 'data': {'categories': ['#multimodal', '#3d', '#long_context'], 'emoji': '🌐', 'ru': {'title': 'WorldMem: Улучшение долгосрочной согласованности в симуляции миров с помощью памяти', 'desc': 'WorldMem - это фреймворк для улучшения симуляции виртуальных миров с помощью банка памяти. Он использует механизм внимания для извлечения релевантной информации из кадров памяти на основе их состояний. Это позволяет точно реконструировать ранее наблюдаемые сцены даже при значительных изменениях ракурса или временных промежутках. Включение временных меток в состояния позволяет моделировать не только статичный мир, но и его динамическое развитие во времени.'}, 'en': {'title': 'Enhancing World Simulation with Memory-Driven Consistency', 'desc': 'This paper introduces WorldMem, a novel framework designed to improve world simulation by utilizing a memory bank that stores various memory frames and states. The framework addresses the challenge of maintaining long-term consistency in 3D spatial representations, which is often hindered by limited temporal context. By implementing a memory attention mechanism, WorldMem can retrieve relevant information from stored memory frames, allowing for accurate scene reconstruction despite changes in viewpoint or time. Additionally, the integration of timestamps enables the framework to model both static and dynamic aspects of the environment, enhancing interaction and perception in simulated worlds.'}, 'zh': {'title': '增强世界模拟的一致性与动态性', 'desc': '世界模拟因其建模虚拟环境和预测行为后果的能力而越来越受欢迎。然而，有限的时间上下文窗口常常导致长期一致性维护的失败，特别是在保持三维空间一致性方面。在这项工作中，我们提出了WorldMem框架，通过一个包含记忆单元的记忆库来增强场景生成，这些记忆单元存储记忆帧和状态（例如，姿势和时间戳）。通过采用记忆注意机制，我们的方法能够准确重建先前观察到的场景，即使在显著的视角或时间间隔下也能有效提取相关信息。'}}}, {'id': 'https://huggingface.co/papers/2504.13169', 'title': 'Generate, but Verify: Reducing Hallucination in Vision-Language Models\n  with Retrospective Resampling', 'url': 'https://huggingface.co/papers/2504.13169', 'abstract': 'Vision-Language Models (VLMs) excel at visual understanding but often suffer from visual hallucinations, where they generate descriptions of nonexistent objects, actions, or concepts, posing significant risks in safety-critical applications. Existing hallucination mitigation methods typically follow one of two paradigms: generation adjustment, which modifies decoding behavior to align text with visual inputs, and post-hoc verification, where external models assess and correct outputs. While effective, generation adjustment methods often rely on heuristics and lack correction mechanisms, while post-hoc verification is complicated, typically requiring multiple models and tending to reject outputs rather than refine them. In this work, we introduce REVERSE, a unified framework that integrates hallucination-aware training with on-the-fly self-verification. By leveraging a new hallucination-verification dataset containing over 1.3M semi-synthetic samples, along with a novel inference-time retrospective resampling technique, our approach enables VLMs to both detect hallucinations during generation and dynamically revise those hallucinations. Our evaluations show that REVERSE achieves state-of-the-art hallucination reduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO and 28% on HaloQuest. Our dataset, model, and code are available at: https://reverse-vlm.github.io.', 'score': 18, 'issue_id': 3304, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': 'f1ebb64cfce24e47', 'authors': ['Tsung-Han Wu', 'Heekyung Lee', 'Jiaxin Ge', 'Joseph E. Gonzalez', 'Trevor Darrell', 'David M. Chan'], 'affiliations': ['POSTECH', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2504.13169.jpg', 'data': {'categories': ['#dataset', '#inference', '#hallucinations', '#data', '#cv'], 'emoji': '👁️', 'ru': {'title': 'REVERSE: самокорректирующиеся VLM без галлюцинаций', 'desc': 'Исследователи представили REVERSE - новый подход к снижению визуальных галлюцинаций в моделях компьютерного зрения и обработки естественного языка (VLM). Метод объединяет обучение с учетом галлюцинаций и самопроверку в режиме реального времени. REVERSE использует новый датасет из 1,3 млн полусинтетических образцов и технику ретроспективной выборки во время вывода. Эксперименты показали, что REVERSE превосходит существующие методы на 12-28% по снижению галлюцинаций на стандартных бенчмарках.'}, 'en': {'title': 'REVERSE: Correcting Visual Hallucinations in VLMs Dynamically', 'desc': 'This paper addresses the issue of visual hallucinations in Vision-Language Models (VLMs), where models incorrectly describe non-existent elements. The authors propose a new framework called REVERSE, which combines hallucination-aware training with real-time self-verification to improve the accuracy of VLM outputs. By utilizing a large dataset of semi-synthetic samples, REVERSE can identify and correct hallucinations during the generation process. The results demonstrate that this approach significantly reduces hallucinations, outperforming existing methods in various benchmarks.'}, 'zh': {'title': 'REVERSE：动态修正视觉幻觉的统一框架', 'desc': '视觉语言模型（VLMs）在视觉理解方面表现出色，但常常出现视觉幻觉，即生成不存在的物体、动作或概念的描述，这在安全关键应用中带来了重大风险。现有的幻觉缓解方法通常分为两类：生成调整和后期验证。生成调整方法依赖启发式规则，缺乏有效的修正机制，而后期验证则复杂，通常需要多个模型，并倾向于拒绝输出而不是进行修正。我们提出的REVERSE框架结合了幻觉感知训练和实时自我验证，能够在生成过程中检测幻觉并动态修正，从而显著降低幻觉的发生。'}}}, {'id': 'https://huggingface.co/papers/2504.13122', 'title': 'VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference\n  Optimization for Large Video Models', 'url': 'https://huggingface.co/papers/2504.13122', 'abstract': 'Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown promise in video understanding but often suffer from misalignment with human intuition and video hallucination issues. To address these challenges, we introduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal Direct Preference Optimization. VistaDPO enhances text-video preference alignment across three hierarchical levels: i) Instance Level, aligning overall video content with responses; ii) Temporal Level, aligning video temporal semantics with event descriptions; and iii) Perceptive Level, aligning spatial objects with language tokens. Given the lack of datasets for fine-grained video-language preference alignment, we construct VistaDPO-7k, a dataset of 7.2K QA pairs annotated with chosen and rejected responses, along with spatial-temporal grounding information such as timestamps, keyframes, and bounding boxes. Extensive experiments on benchmarks such as Video Hallucination, Video QA, and Captioning performance tasks demonstrate that VistaDPO significantly improves the performance of existing LVMs, effectively mitigating video-language misalignment and hallucination. The code and data are available at https://github.com/HaroldChen19/VistaDPO.', 'score': 15, 'issue_id': 3303, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': 'efbc3b240498ce70', 'authors': ['Haojian Huang', 'Haodong Chen', 'Shengqiong Wu', 'Meng Luo', 'Jinlan Fu', 'Xinya Du', 'Hanwang Zhang', 'Hao Fei'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'The Hong Kong University of Science and Technology', 'The University of Hong Kong', 'University of Texas at Dallas'], 'pdf_title_img': 'assets/pdf/title_img/2504.13122.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#alignment', '#hallucinations', '#dataset'], 'emoji': '🎬', 'ru': {'title': 'VistaDPO: Точное согласование видео и языка на всех уровнях', 'desc': 'VistaDPO - это новая система для оптимизации предпочтений в видео с использованием иерархического пространственно-временного подхода. Она улучшает согласование текста и видео на трех уровнях: общем содержании, временной семантике и пространственных объектах. Авторы создали датасет VistaDPO-7k с 7200 парами вопросов-ответов, аннотированными предпочтительными и отвергнутыми ответами, а также временными метками и ограничивающими рамками. Эксперименты показали, что VistaDPO значительно улучшает работу существующих больших видеомоделей, эффективно снижая рассогласование видео и языка и галлюцинации.'}, 'en': {'title': 'Aligning Video and Language: Introducing VistaDPO', 'desc': 'This paper presents VistaDPO, a new framework designed to improve the alignment between video content and human language understanding in Large Video Models (LVMs). It operates on three hierarchical levels: aligning overall video content with user responses, matching temporal semantics of videos with event descriptions, and correlating spatial objects with language tokens. To support this framework, the authors created a dataset called VistaDPO-7k, which includes 7.2K question-answer pairs with detailed annotations for better preference alignment. The results show that VistaDPO enhances the performance of LVMs by reducing issues related to video hallucination and misalignment with human intuition.'}, 'zh': {'title': 'VistaDPO：提升视频理解的偏好对齐', 'desc': '本文介绍了一种新框架VistaDPO，用于视频层次空间-时间直接偏好优化，旨在解决大型视频模型（LVMs）在视频理解中的人类直觉不一致和视频幻觉问题。VistaDPO通过三个层次增强文本-视频偏好对齐：实例层、时间层和感知层，分别对齐视频内容、时间语义和空间对象。为了支持细粒度视频-语言偏好对齐，研究团队构建了VistaDPO-7k数据集，包含7200个问答对及其空间-时间信息。实验结果表明，VistaDPO显著提升了现有LVMs的性能，有效减轻了视频-语言的不一致性和幻觉现象。'}}}, {'id': 'https://huggingface.co/papers/2504.13055', 'title': 'NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation', 'url': 'https://huggingface.co/papers/2504.13055', 'abstract': 'Recent advances in reinforcement learning (RL) have strengthened the reasoning capabilities of vision-language models (VLMs). However, enhancing policy exploration to more effectively scale test-time compute remains underexplored in VLMs. In addition, VLMs continue to struggle with imperfect visual perception, which in turn affects the subsequent reasoning process. To this end, we propose NoisyRollout, a simple yet effective RL approach that mixes trajectories from both clean and moderately distorted images to introduce targeted diversity in visual perception and the resulting reasoning patterns. Without additional training cost, NoisyRollout enhances the exploration capabilities of VLMs by incorporating a vision-oriented inductive bias. Furthermore, NoisyRollout employs a noise annealing schedule that gradually reduces distortion strength over training, ensuring benefit from noisy signals early while maintaining training stability and scalability in later stages. With just 2.1K training samples, NoisyRollout achieves state-of-the-art performance among open-source RL-tuned models on 5 out-of-domain benchmarks spanning both reasoning and perception tasks, while preserving comparable or even better in-domain performance.', 'score': 12, 'issue_id': 3307, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': '058d7aa285231e64', 'authors': ['Xiangyan Liu', 'Jinjie Ni', 'Zijian Wu', 'Chao Du', 'Longxu Dou', 'Haonan Wang', 'Tianyu Pang', 'Michael Qizhe Shieh'], 'affiliations': ['National University of Singapore', 'Sea AI Lab, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2504.13055.jpg', 'data': {'categories': ['#rlhf', '#rl', '#training', '#multimodal', '#reasoning', '#optimization'], 'emoji': '🎭', 'ru': {'title': 'NoisyRollout: Улучшение исследования и рассуждений ВЯМ через контролируемый визуальный шум', 'desc': 'Статья представляет новый подход в обучении с подкреплением для визуально-языковых моделей под названием NoisyRollout. Этот метод улучшает способности моделей к исследованию и рассуждению, смешивая траектории чистых и умеренно искаженных изображений. NoisyRollout использует график уменьшения шума, постепенно снижая силу искажений в процессе обучения. Метод достигает передовых результатов на пяти внедоменных бенчмарках, используя всего 2100 обучающих примеров.'}, 'en': {'title': 'Enhancing VLMs with NoisyRollout for Better Reasoning and Perception', 'desc': 'This paper introduces NoisyRollout, a reinforcement learning method designed to improve vision-language models (VLMs) by enhancing their policy exploration capabilities. It addresses the challenge of imperfect visual perception by mixing clean and distorted image trajectories, which helps diversify the reasoning patterns of the models. The approach uses a noise annealing schedule to gradually reduce distortion, allowing the model to benefit from noisy inputs during early training while ensuring stability later on. Remarkably, NoisyRollout achieves state-of-the-art results on various benchmarks with minimal training samples, demonstrating its effectiveness in both reasoning and perception tasks.'}, 'zh': {'title': 'NoisyRollout：提升视觉-语言模型的推理能力', 'desc': '最近，强化学习（RL）的进展增强了视觉-语言模型（VLM）的推理能力。然而，在VLM中，增强策略探索以更有效地扩展测试时间计算仍然未被充分研究。此外，VLM在视觉感知不完美方面仍然面临挑战，这影响了后续的推理过程。为此，我们提出了NoisyRollout，这是一种简单而有效的RL方法，通过混合干净和适度失真的图像轨迹，引入目标多样性，从而改善视觉感知和推理模式。'}}}, {'id': 'https://huggingface.co/papers/2504.12364', 'title': 'DMM: Building a Versatile Image Generation Model via Distillation-Based\n  Model Merging', 'url': 'https://huggingface.co/papers/2504.12364', 'abstract': 'The success of text-to-image (T2I) generation models has spurred a proliferation of numerous model checkpoints fine-tuned from the same base model on various specialized datasets. This overwhelming specialized model production introduces new challenges for high parameter redundancy and huge storage cost, thereby necessitating the development of effective methods to consolidate and unify the capabilities of diverse powerful models into a single one. A common practice in model merging adopts static linear interpolation in the parameter space to achieve the goal of style mixing. However, it neglects the features of T2I generation task that numerous distinct models cover sundry styles which may lead to incompatibility and confusion in the merged model. To address this issue, we introduce a style-promptable image generation pipeline which can accurately generate arbitrary-style images under the control of style vectors. Based on this design, we propose the score distillation based model merging paradigm (DMM), compressing multiple models into a single versatile T2I model. Moreover, we rethink and reformulate the model merging task in the context of T2I generation, by presenting new merging goals and evaluation protocols. Our experiments demonstrate that DMM can compactly reorganize the knowledge from multiple teacher models and achieve controllable arbitrary-style generation.', 'score': 10, 'issue_id': 3306, 'pub_date': '2025-04-16', 'pub_date_card': {'ru': '16 апреля', 'en': 'April 16', 'zh': '4月16日'}, 'hash': '49755535f03790d4', 'authors': ['Tianhui Song', 'Weixin Feng', 'Shuai Wang', 'Xubin Li', 'Tiezheng Ge', 'Bo Zheng', 'Limin Wang'], 'affiliations': ['Nanjing University', 'Nanjing University, Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2504.12364.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#cv', '#diffusion', '#training', '#dataset'], 'emoji': '🎨', 'ru': {'title': 'Объединение T2I моделей для гибкой генерации стилей', 'desc': 'В статье представлен новый метод объединения моделей генерации изображений по тексту (T2I). Авторы предлагают конвейер генерации изображений, управляемый векторами стилей, и парадигму слияния моделей на основе дистилляции оценок (DMM). Этот подход позволяет сжать несколько моделей в одну универсальную T2I модель. Эксперименты показывают, что DMM может эффективно реорганизовать знания из нескольких учительских моделей и достичь контролируемой генерации изображений произвольного стиля.'}, 'en': {'title': 'Unifying Diverse T2I Models with Style-Promptable Merging', 'desc': 'This paper addresses the challenges posed by the proliferation of specialized text-to-image (T2I) generation models, which often lead to high parameter redundancy and storage costs. The authors propose a novel approach called score distillation based model merging (DMM) that consolidates multiple models into a single, versatile T2I model. Unlike traditional methods that use static linear interpolation, DMM incorporates style vectors to enable accurate generation of images in various styles while avoiding incompatibility issues. The paper also introduces new goals and evaluation protocols for model merging in the context of T2I generation, demonstrating that DMM effectively reorganizes knowledge from multiple models for controllable style generation.'}, 'zh': {'title': '高效合并多模型，实现可控图像生成', 'desc': '本文探讨了文本到图像生成模型（T2I）的合并问题。随着众多模型在不同数据集上微调，产生了大量的专用模型，这导致了参数冗余和存储成本高的问题。为了解决这些问题，本文提出了一种基于评分蒸馏的模型合并方法（DMM），能够将多个模型压缩为一个多功能的T2I模型。实验结果表明，DMM能够有效整合多个教师模型的知识，实现可控的任意风格图像生成。'}}}, {'id': 'https://huggingface.co/papers/2504.05506', 'title': 'ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question\n  Answering', 'url': 'https://huggingface.co/papers/2504.05506', 'abstract': 'Charts are ubiquitous, as people often use them to analyze data, answer questions, and discover critical insights. However, performing complex analytical tasks with charts requires significant perceptual and cognitive effort. Chart Question Answering (CQA) systems automate this process by enabling models to interpret and reason with visual representations of data. However, existing benchmarks like ChartQA lack real-world diversity and have recently shown performance saturation with modern large vision-language models (LVLMs). To address these limitations, we introduce ChartQAPro, a new benchmark that includes 1,341 charts from 157 diverse sources, spanning various chart types, including infographics and dashboards, and featuring 1,948 questions in various types, such as multiple-choice, conversational, hypothetical, and unanswerable questions, to better reflect real-world challenges. Our evaluations with 21 models show a substantial performance drop for LVLMs on ChartQAPro; e.g., Claude Sonnet 3.5 scores 90.5% on ChartQA but only 55.81% on ChartQAPro, underscoring the complexity of chart reasoning. We complement our findings with detailed error analyses and ablation studies, identifying key challenges and opportunities for advancing LVLMs in chart understanding and reasoning. We release ChartQAPro at https://github.com/vis-nlp/ChartQAPro.', 'score': 9, 'issue_id': 3303, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'}, 'hash': 'a727d08eac22e920', 'authors': ['Ahmed Masry', 'Mohammed Saidul Islam', 'Mahir Ahmed', 'Aayush Bajaj', 'Firoz Kabir', 'Aaryaman Kartha', 'Md Tahmid Rahman Laskar', 'Mizanur Rahman', 'Shadikur Rahman', 'Mehrad Shahmohammadi', 'Megh Thakkar', 'Md Rizwan Parvez', 'Enamul Hoque', 'Shafiq Joty'], 'affiliations': ['Dialpad Inc., Canada', 'MILA - Quebec AI Institute, Canada', 'Nanyang Technological University, Singapore', 'Qatar Computing Research Institute (QCRI)', 'RBC, Canada', 'Salesforce Research, USA', 'York University, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2504.05506.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#cv', '#dataset', '#reasoning'], 'emoji': '📊', 'ru': {'title': 'ChartQAPro: новый вызов для ИИ в понимании графиков', 'desc': 'ChartQAPro - новый набор данных для задачи ответов на вопросы по графикам. Он включает более 1300 разнообразных графиков из реальных источников и около 2000 вопросов различных типов. Тестирование показало, что современные мультимодальные языковые модели значительно хуже справляются с ChartQAPro по сравнению с предыдущими наборами данных. Авторы провели детальный анализ ошибок и выявили ключевые проблемы в понимании и рассуждении о графиках для языковых моделей.'}, 'en': {'title': 'ChartQAPro: Elevating Chart Understanding for AI', 'desc': 'This paper introduces ChartQAPro, a new benchmark designed to improve Chart Question Answering (CQA) systems by providing a more diverse and realistic set of charts and questions. Unlike previous benchmarks, ChartQAPro includes 1,341 charts from 157 sources and features various question types, which better reflect the complexities of real-world data analysis. The study reveals that modern large vision-language models (LVLMs) struggle significantly with this new benchmark, demonstrating a performance drop from 90.5% on the previous ChartQA to only 55.81% on ChartQAPro. Through error analyses and ablation studies, the authors identify challenges in chart reasoning that can guide future improvements in LVLMs.'}, 'zh': {'title': '提升图表问答系统的挑战与机遇', 'desc': '本论文介绍了ChartQAPro，这是一个新的基准测试，旨在提高图表问答系统的性能。它包含来自157个不同来源的1,341个图表，涵盖多种图表类型，并提供1,948个多样化的问题。通过对21个模型的评估，我们发现现代大型视觉语言模型在ChartQAPro上的表现显著下降，显示出图表推理的复杂性。我们的研究还包括详细的错误分析和消融研究，以识别图表理解和推理中的关键挑战和机遇。'}}}, {'id': 'https://huggingface.co/papers/2504.13145', 'title': 'Exploring Expert Failures Improves LLM Agent Tuning', 'url': 'https://huggingface.co/papers/2504.13145', 'abstract': 'Large Language Models (LLMs) have shown tremendous potential as agents, excelling at tasks that require multiple rounds of reasoning and interactions. Rejection Sampling Fine-Tuning (RFT) has emerged as an effective method for finetuning LLMs as agents: it first imitates expert-generated successful trajectories and further improves agentic skills through iterative fine-tuning on successful, self-generated trajectories. However, since the expert (e.g., GPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler scenarios, many complex subtasks remain unsolved and persistently out-of-distribution (OOD). Upon investigating these challenging subtasks, we discovered that previously failed expert trajectories can often provide valuable guidance, e.g., plans and key actions, that can significantly improve agent exploration efficiency and acquisition of critical skills. Motivated by these observations, we propose Exploring Expert Failures (EEF), which identifies beneficial actions from failed expert trajectories and integrates them into the training dataset. Potentially harmful actions are meticulously excluded to prevent contamination of the model learning process. By leveraging the beneficial actions in expert failures, EEF successfully solves some previously unsolvable subtasks and improves agent tuning performance. Remarkably, our approach achieved a 62\\% win rate in WebShop, outperforming RFT (53. 6\\%) and GPT-4 (35. 6\\%), and to the best of our knowledge, setting a new state-of-the-art as the first method to surpass a score of 0.81 in WebShop and exceed 81 in SciWorld.', 'score': 7, 'issue_id': 3305, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': '597cd9806c8a07ff', 'authors': ['Li-Cheng Lan', 'Andrew Bai', 'Minhao Cheng', 'Ruochen Wang', 'Cho-Jui Hsieh', 'Tianyi Zhou'], 'affiliations': ['OpenAI', 'Pennsylvania State University', 'UCLA', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2504.13145.jpg', 'data': {'categories': ['#reasoning', '#agents', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Учимся на ошибках: новый метод обучения ИИ-агентов', 'desc': 'Статья представляет новый метод обучения больших языковых моделей (LLM) для выполнения сложных задач. Метод называется Exploring Expert Failures (EEF) и использует информацию из неудачных попыток экспертной модели для улучшения процесса обучения. EEF превзошел предыдущие методы, такие как Rejection Sampling Fine-Tuning (RFT), в задачах WebShop и SciWorld. Авторы показывают, что использование полезных действий из неудачных экспертных траекторий может значительно улучшить эффективность исследования и приобретение критических навыков агентом.'}, 'en': {'title': 'Learning from Mistakes: Enhancing LLMs with Expert Failures', 'desc': 'This paper introduces Exploring Expert Failures (EEF), a novel approach to enhance the performance of Large Language Models (LLMs) in complex tasks. EEF builds on Rejection Sampling Fine-Tuning (RFT) by utilizing insights from previously failed expert trajectories to identify beneficial actions that can improve agent exploration and skill acquisition. By integrating these valuable actions into the training dataset while excluding harmful ones, EEF addresses previously unsolvable subtasks and boosts overall agent performance. The results demonstrate that EEF outperforms existing methods, achieving a 62% win rate in WebShop and setting new benchmarks in task performance.'}, 'zh': {'title': '从失败中学习，提升智能体能力', 'desc': '大型语言模型（LLMs）在多轮推理和交互任务中表现出色。拒绝采样微调（RFT）是一种有效的微调方法，通过模仿专家生成的成功轨迹并在自生成的成功轨迹上进行迭代微调来提升模型的能力。然而，由于RFT偏向于简单场景，许多复杂子任务仍然未能解决。我们提出的探索专家失败（EEF）方法，通过从失败的专家轨迹中提取有益的行动，显著提高了模型的探索效率和关键技能的获取。'}}}, {'id': 'https://huggingface.co/papers/2504.12395', 'title': 'InstantCharacter: Personalize Any Characters with a Scalable Diffusion\n  Transformer Framework', 'url': 'https://huggingface.co/papers/2504.12395', 'abstract': 'Current learning-based subject customization approaches, predominantly relying on U-Net architectures, suffer from limited generalization ability and compromised image quality. Meanwhile, optimization-based methods require subject-specific fine-tuning, which inevitably degrades textual controllability. To address these challenges, we propose InstantCharacter, a scalable framework for character customization built upon a foundation diffusion transformer. InstantCharacter demonstrates three fundamental advantages: first, it achieves open-domain personalization across diverse character appearances, poses, and styles while maintaining high-fidelity results. Second, the framework introduces a scalable adapter with stacked transformer encoders, which effectively processes open-domain character features and seamlessly interacts with the latent space of modern diffusion transformers. Third, to effectively train the framework, we construct a large-scale character dataset containing 10-million-level samples. The dataset is systematically organized into paired (multi-view character) and unpaired (text-image combinations) subsets. This dual-data structure enables simultaneous optimization of identity consistency and textual editability through distinct learning pathways. Qualitative experiments demonstrate the advanced capabilities of InstantCharacter in generating high-fidelity, text-controllable, and character-consistent images, setting a new benchmark for character-driven image generation. Our source code is available at https://github.com/Tencent/InstantCharacter.', 'score': 6, 'issue_id': 3308, 'pub_date': '2025-04-16', 'pub_date_card': {'ru': '16 апреля', 'en': 'April 16', 'zh': '4月16日'}, 'hash': '4000d7f0fc525f6d', 'authors': ['Jiale Tao', 'Yanbing Zhang', 'Qixun Wang', 'Yiji Cheng', 'Haofan Wang', 'Xu Bai', 'Zhengguang Zhou', 'Ruihuang Li', 'Linqing Wang', 'Chunyu Wang', 'Qin Lin', 'Qinglin Lu'], 'affiliations': ['Hunyuan, Tencent', 'InstantX Team'], 'pdf_title_img': 'assets/pdf/title_img/2504.12395.jpg', 'data': {'categories': ['#open_source', '#data', '#diffusion', '#dataset', '#benchmark', '#cv'], 'emoji': '🎭', 'ru': {'title': 'Мгновенная кастомизация персонажей с сохранением высокого качества и текстового контроля', 'desc': 'InstantCharacter - это новый подход к кастомизации персонажей, основанный на диффузионном трансформере. Он решает проблемы ограниченной обобщаемости и ухудшения качества изображений, характерные для существующих методов. Фреймворк использует масштабируемый адаптер со стековыми энкодерами трансформеров для обработки характеристик персонажей. Для обучения был создан большой набор данных из 10 миллионов образцов, организованный в парные и непарные подмножества.'}, 'en': {'title': 'InstantCharacter: Revolutionizing Character Customization with Diffusion Transformers', 'desc': 'This paper introduces InstantCharacter, a new framework for customizing characters using machine learning. Unlike traditional methods that struggle with generalization and image quality, InstantCharacter leverages a diffusion transformer to achieve high-fidelity results across various character styles and poses. The framework includes a scalable adapter with transformer encoders that effectively manage character features and interact with the latent space of diffusion models. Additionally, it utilizes a large-scale dataset of 10 million samples to optimize both identity consistency and textual editability, demonstrating superior performance in generating controllable character images.'}, 'zh': {'title': 'InstantCharacter：高保真角色定制的新标准', 'desc': '本文提出了一种名为InstantCharacter的角色定制框架，旨在解决现有学习基础方法在图像质量和泛化能力上的不足。该框架基于扩散变换器，能够在多样化的角色外观、姿势和风格中实现开放域个性化，同时保持高保真度。InstantCharacter引入了可扩展的适配器，利用堆叠的变换器编码器有效处理开放域角色特征，并与现代扩散变换器的潜在空间无缝交互。此外，构建了一个包含千万级样本的大规模角色数据集，以支持框架的有效训练，优化身份一致性和文本可编辑性。'}}}, {'id': 'https://huggingface.co/papers/2504.07959', 'title': 'CCMNet: Leveraging Calibrated Color Correction Matrices for Cross-Camera\n  Color Constancy', 'url': 'https://huggingface.co/papers/2504.07959', 'abstract': "Computational color constancy, or white balancing, is a key module in a camera's image signal processor (ISP) that corrects color casts from scene lighting. Because this operation occurs in the camera-specific raw color space, white balance algorithms must adapt to different cameras. This paper introduces a learning-based method for cross-camera color constancy that generalizes to new cameras without retraining. Our method leverages pre-calibrated color correction matrices (CCMs) available on ISPs that map the camera's raw color space to a standard space (e.g., CIE XYZ). Our method uses these CCMs to transform predefined illumination colors (i.e., along the Planckian locus) into the test camera's raw space. The mapped illuminants are encoded into a compact camera fingerprint embedding (CFE) that enables the network to adapt to unseen cameras. To prevent overfitting due to limited cameras and CCMs during training, we introduce a data augmentation technique that interpolates between cameras and their CCMs. Experimental results across multiple datasets and backbones show that our method achieves state-of-the-art cross-camera color constancy while remaining lightweight and relying only on data readily available in camera ISPs.", 'score': 6, 'issue_id': 3311, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 апреля', 'en': 'April 10', 'zh': '4月10日'}, 'hash': 'f4c62f5c36856c86', 'authors': ['Dongyoung Kim', 'Mahmoud Afifi', 'Dongyun Kim', 'Michael S. Brown', 'Seon Joo Kim'], 'affiliations': ['AI Center - Toronto, Samsung Electronics', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2504.07959.jpg', 'data': {'categories': ['#training', '#cv', '#dataset'], 'emoji': '📸', 'ru': {'title': 'Адаптивный баланс белого для любой камеры', 'desc': 'Эта статья представляет новый метод машинного обучения для коррекции баланса белого в камерах, который может адаптироваться к новым камерам без переобучения. Метод использует предварительно откалиброванные матрицы коррекции цвета (CCM) для создания компактного встраивания-отпечатка камеры (CFE). Авторы также вводят технику аугментации данных, интерполирующую между камерами и их CCM для предотвращения переобучения. Экспериментальные результаты показывают, что метод достигает современного уровня в межкамерной цветовой константности, оставаясь при этом легковесным.'}, 'en': {'title': 'Adaptive Color Constancy Across Cameras Without Retraining', 'desc': 'This paper presents a novel approach to computational color constancy, specifically focusing on white balancing in images captured by different cameras. The proposed method utilizes pre-calibrated color correction matrices (CCMs) to adaptively transform illumination colors into the raw color space of new cameras without the need for retraining. By creating a compact camera fingerprint embedding (CFE), the model can effectively generalize to unseen cameras, enhancing its versatility. Additionally, a data augmentation technique is introduced to mitigate overfitting, ensuring robust performance across various datasets and camera types.'}, 'zh': {'title': '跨相机颜色恒常性的创新方法', 'desc': '本文介绍了一种基于学习的跨相机颜色恒常性方法，旨在解决不同相机的白平衡问题。该方法利用预先校准的颜色校正矩阵（CCM），将相机的原始颜色空间映射到标准空间。通过将预定义的照明颜色转换为测试相机的原始空间，生成紧凑的相机指纹嵌入（CFE），使网络能够适应未见过的相机。实验结果表明，该方法在多个数据集上实现了最先进的跨相机颜色恒常性，同时保持轻量级，依赖于相机ISP中现成的数据。'}}}, {'id': 'https://huggingface.co/papers/2504.13171', 'title': 'Sleep-time Compute: Beyond Inference Scaling at Test-time', 'url': 'https://huggingface.co/papers/2504.13171', 'abstract': 'Scaling test-time compute has emerged as a key ingredient for enabling large language models (LLMs) to solve difficult problems, but comes with high latency and inference cost. We introduce sleep-time compute, which allows models to "think" offline about contexts before queries are presented: by anticipating what queries users might ask and pre-computing useful quantities, we can significantly reduce the compute requirements at test-time. To demonstrate the efficacy of our method, we create modified versions of two reasoning tasks - Stateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can reduce the amount of test-time compute needed to achieve the same accuracy by ~ 5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time compute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic and 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic, which extends GSM-Symbolic by including multiple related queries per context. By amortizing sleep-time compute across related queries about the same context using Multi-Query GSM-Symbolic, we can decrease the average cost per query by 2.5x. We then conduct additional analysis to understand when sleep-time compute is most effective, finding the predictability of the user query to be well correlated with the efficacy of sleep-time compute. Finally, we conduct a case-study of applying sleep-time compute to a realistic agentic SWE task.', 'score': 4, 'issue_id': 3310, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': '094d6f648b644327', 'authors': ['Kevin Lin', 'Charlie Snell', 'Yu Wang', 'Charles Packer', 'Sarah Wooders', 'Ion Stoica', 'Joseph E. Gonzalez'], 'affiliations': ['Letta', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2504.13171.jpg', 'data': {'categories': ['#optimization', '#inference', '#reasoning', '#agents'], 'emoji': '💤', 'ru': {'title': 'Оптимизация LLM: думай заранее, отвечай быстрее', 'desc': "Эта статья представляет концепцию 'вычислений во время сна' для больших языковых моделей (LLM). Метод предполагает предварительные вычисления для ожидаемых запросов, что значительно снижает вычислительные затраты во время выполнения. Авторы демонстрируют эффективность подхода на модифицированных версиях задач рассуждения, показывая пятикратное сокращение необходимых вычислений при сохранении точности. Исследование также выявляет, что предсказуемость пользовательских запросов коррелирует с эффективностью метода 'вычислений во время сна'."}, 'en': {'title': 'Optimize Query Processing with Sleep-Time Compute', 'desc': 'This paper presents a novel approach called sleep-time compute, which allows large language models (LLMs) to prepare for user queries by pre-computing relevant information offline. By anticipating potential questions, the model can significantly reduce the computational load during test-time, achieving up to 5 times less compute while maintaining accuracy. The authors demonstrate that scaling sleep-time compute can enhance accuracy by up to 18% on specific reasoning tasks. Additionally, they introduce Multi-Query GSM-Symbolic, which optimizes the processing of multiple related queries, further decreasing the average cost per query by 2.5 times.'}, 'zh': {'title': '睡眠时间计算：提升语言模型效率的新方法', 'desc': '本文提出了一种新的计算方法，称为睡眠时间计算，旨在减少大型语言模型在测试时的计算需求。通过在用户提出查询之前，模型可以离线思考并预计算有用的信息，从而降低延迟和推理成本。研究表明，使用睡眠时间计算可以在保持相同准确率的情况下，将测试时的计算需求减少约5倍，并在某些任务上提高准确率。我们还引入了多查询GSM-符号化，允许在同一上下文中处理多个相关查询，从而进一步降低每个查询的平均成本。'}}}, {'id': 'https://huggingface.co/papers/2504.12157', 'title': 'FocusedAD: Character-centric Movie Audio Description', 'url': 'https://huggingface.co/papers/2504.12157', 'abstract': 'Movie Audio Description (AD) aims to narrate visual content during dialogue-free segments, particularly benefiting blind and visually impaired (BVI) audiences. Compared with general video captioning, AD demands plot-relevant narration with explicit character name references, posing unique challenges in movie understanding.To identify active main characters and focus on storyline-relevant regions, we propose FocusedAD, a novel framework that delivers character-centric movie audio descriptions. It includes: (i) a Character Perception Module(CPM) for tracking character regions and linking them to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues from prior ADs and subtitles via learnable soft prompts; and (iii) a Focused Caption Module(FCM) that generates narrations enriched with plot-relevant details and named characters. To overcome limitations in character identification, we also introduce an automated pipeline for building character query banks. FocusedAD achieves state-of-the-art performance on multiple benchmarks, including strong zero-shot results on MAD-eval-Named and our newly proposed Cinepile-AD dataset. Code and data will be released at https://github.com/Thorin215/FocusedAD .', 'score': 4, 'issue_id': 3306, 'pub_date': '2025-04-16', 'pub_date_card': {'ru': '16 апреля', 'en': 'April 16', 'zh': '4月16日'}, 'hash': '2bfe38936e42dd11', 'authors': ['Xiaojun Ye', 'Chun Wang', 'Yiren Song', 'Sheng Zhou', 'Liangcheng Li', 'Jiajun Bu'], 'affiliations': ['National University of Singapore Singapore', 'Zhejiang University China'], 'pdf_title_img': 'assets/pdf/title_img/2504.12157.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#audio', '#benchmark', '#video', '#dataset', '#science'], 'emoji': '🎬', 'ru': {'title': 'Умное аудиоописание фильмов с фокусом на персонажах', 'desc': 'Статья представляет FocusedAD - новый фреймворк для создания аудиоописаний фильмов, ориентированных на персонажей. Система включает модуль восприятия персонажей (CPM) для отслеживания и именования персонажей, динамический модуль предыстории (DPM) для внедрения контекстных подсказок, и модуль фокусированных подписей (FCM) для генерации релевантных описаний с именами персонажей. FocusedAD также предлагает автоматизированный процесс создания банков запросов персонажей для улучшения их идентификации. Фреймворк достигает наилучших результатов на нескольких бенчмарках, включая сильные результаты в режиме zero-shot.'}, 'en': {'title': 'FocusedAD: Enhancing Movie Accessibility with Character-Centric Audio Descriptions', 'desc': 'This paper presents FocusedAD, a new framework designed to create audio descriptions for movies, specifically targeting blind and visually impaired audiences. It addresses the unique challenges of audio description by focusing on character-centric narration that includes character names and relevant plot details. The framework consists of three main components: a Character Perception Module for tracking characters, a Dynamic Prior Module for incorporating contextual information, and a Focused Caption Module for generating detailed narrations. FocusedAD demonstrates superior performance on various benchmarks, including zero-shot evaluations, showcasing its effectiveness in enhancing movie accessibility.'}, 'zh': {'title': '以角色为中心的电影音频描述', 'desc': '电影音频描述（AD）旨在为盲人和视力障碍者在无对话的片段中叙述视觉内容。与一般视频字幕相比，AD需要与情节相关的叙述，并明确提及角色名称，这给电影理解带来了独特的挑战。我们提出了FocusedAD框架，通过角色感知模块、动态先验模块和聚焦字幕模块，提供以角色为中心的电影音频描述。FocusedAD在多个基准测试中实现了最先进的性能，包括在MAD-eval-Named和新提出的Cinepile-AD数据集上的强大零样本结果。'}}}, {'id': 'https://huggingface.co/papers/2504.13079', 'title': 'Retrieval-Augmented Generation with Conflicting Evidence', 'url': 'https://huggingface.co/papers/2504.13079', 'abstract': 'Large language model (LLM) agents are increasingly employing retrieval-augmented generation (RAG) to improve the factuality of their responses. However, in practice, these systems often need to handle ambiguous user queries and potentially conflicting information from multiple sources while also suppressing inaccurate information from noisy or irrelevant documents. Prior work has generally studied and addressed these challenges in isolation, considering only one aspect at a time, such as handling ambiguity or robustness to noise and misinformation. We instead consider multiple factors simultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and Misinformation in Documents), a new dataset that simulates complex and realistic scenarios for conflicting evidence for a user query, including ambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent approach in which LLM agents debate over the merits of an answer over multiple rounds, allowing an aggregator to collate responses corresponding to disambiguated entities while discarding misinformation and noise, thereby handling diverse sources of conflict jointly. We demonstrate the effectiveness of MADAM-RAG using both closed and open-source models on AmbigDocs -- which requires presenting all valid answers for ambiguous queries -- improving over strong RAG baselines by up to 11.40% and on FaithEval -- which requires suppressing misinformation -- where we improve by up to 15.80% (absolute) with Llama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for existing RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match score). While MADAM-RAG begins to address these conflicting factors, our analysis indicates that a substantial gap remains especially when increasing the level of imbalance in supporting evidence and misinformation.', 'score': 3, 'issue_id': 3304, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': '13305db862567e7f', 'authors': ['Han Wang', 'Archiki Prasad', 'Elias Stengel-Eskin', 'Mohit Bansal'], 'affiliations': ['University of North Carolina at Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2504.13079.jpg', 'data': {'categories': ['#interpretability', '#dataset', '#rag', '#optimization', '#agents', '#hallucinations'], 'emoji': '🤖', 'ru': {'title': 'Многоагентный подход для борьбы с неоднозначностью и дезинформацией в RAG-системах', 'desc': 'Статья представляет новый подход к решению проблем неоднозначности и дезинформации в системах генерации текста с использованием извлечения информации (RAG). Авторы предлагают датасет RAMDocs, моделирующий сложные сценарии с противоречивыми данными, и метод MADAM-RAG, использующий несколько агентов на основе больших языковых моделей для обсуждения ответов. MADAM-RAG показывает улучшение результатов на 11.40% на датасете AmbigDocs и на 15.80% на FaithEval по сравнению с базовыми методами RAG. Однако, несмотря на прогресс, остаются значительные проблемы, особенно при увеличении дисбаланса между поддерживающими и дезинформирующими данными.'}, 'en': {'title': 'Enhancing LLM Accuracy through Debate and Retrieval', 'desc': 'This paper discusses the challenges faced by large language model (LLM) agents when using retrieval-augmented generation (RAG) to provide accurate responses to user queries. It introduces RAMDocs, a new dataset designed to simulate complex scenarios involving ambiguity, misinformation, and noise in documents. The authors propose MADAM-RAG, a multi-agent system where LLMs debate answers over multiple rounds, allowing for better aggregation of responses while filtering out inaccuracies. The results show that MADAM-RAG significantly improves performance on tasks requiring disambiguation and misinformation suppression compared to traditional RAG methods, although challenges remain in handling conflicting evidence.'}, 'zh': {'title': '多代理辩论：提升语言模型的准确性与鲁棒性', 'desc': '大型语言模型（LLM）代理越来越多地使用检索增强生成（RAG）来提高回答的准确性。然而，这些系统在处理模糊用户查询和来自多个来源的潜在冲突信息时，常常需要抑制来自嘈杂或无关文档的不准确信息。本文提出了RAMDocs数据集，模拟了复杂的用户查询场景，并提出了MADAM-RAG多代理方法，通过多轮辩论来处理答案的优劣，从而有效整合不同来源的响应。实验结果表明，MADAM-RAG在处理模糊查询和抑制错误信息方面显著优于现有的RAG基线。'}}}, {'id': 'https://huggingface.co/papers/2504.12782', 'title': 'Set You Straight: Auto-Steering Denoising Trajectories to Sidestep\n  Unwanted Concepts', 'url': 'https://huggingface.co/papers/2504.12782', 'abstract': 'Ensuring the ethical deployment of text-to-image models requires effective techniques to prevent the generation of harmful or inappropriate content. While concept erasure methods offer a promising solution, existing finetuning-based approaches suffer from notable limitations. Anchor-free methods risk disrupting sampling trajectories, leading to visual artifacts, while anchor-based methods rely on the heuristic selection of anchor concepts. To overcome these shortcomings, we introduce a finetuning framework, dubbed ANT, which Automatically guides deNoising Trajectories to avoid unwanted concepts. ANT is built on a key insight: reversing the condition direction of classifier-free guidance during mid-to-late denoising stages enables precise content modification without sacrificing early-stage structural integrity. This inspires a trajectory-aware objective that preserves the integrity of the early-stage score function field, which steers samples toward the natural image manifold, without relying on heuristic anchor concept selection. For single-concept erasure, we propose an augmentation-enhanced weight saliency map to precisely identify the critical parameters that most significantly contribute to the unwanted concept, enabling more thorough and efficient erasure. For multi-concept erasure, our objective function offers a versatile plug-and-play solution that significantly boosts performance. Extensive experiments demonstrate that ANT achieves state-of-the-art results in both single and multi-concept erasure, delivering high-quality, safe outputs without compromising the generative fidelity. Code is available at https://github.com/lileyang1210/ANT', 'score': 2, 'issue_id': 3305, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': '6c74497f6fd8b96b', 'authors': ['Leyang Li', 'Shilin Lu', 'Yan Ren', 'Adams Wai-Kin Kong'], 'affiliations': ['Nanyang Technological University, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2504.12782.jpg', 'data': {'categories': ['#cv', '#data', '#training', '#optimization', '#ethics'], 'emoji': '🎨', 'ru': {'title': 'ANT: Точное удаление нежелательных концепций из генеративных моделей изображений', 'desc': 'Статья представляет новый метод ANT для удаления нежелательных концепций из генеративных моделей изображений. ANT использует обращение направления условной генерации на средних и поздних этапах шумоподавления для точной модификации контента. Метод сохраняет целостность ранних этапов генерации, не полагаясь на эвристический выбор якорных концепций. ANT показывает отличные результаты как для удаления одиночных, так и множественных концепций, обеспечивая высокое качество и безопасность сгенерированных изображений.'}, 'en': {'title': 'ANT: Ethical Image Generation Through Smart Concept Erasure', 'desc': 'This paper presents ANT, a new finetuning framework designed to improve the ethical deployment of text-to-image models by effectively erasing harmful or inappropriate content. ANT addresses the limitations of existing methods by guiding denoising trajectories to avoid unwanted concepts without disrupting the image quality. It introduces a trajectory-aware objective that maintains the integrity of early-stage image features while allowing for precise content modification. The framework also includes an innovative weight saliency map for identifying critical parameters in single-concept erasure and offers a versatile solution for multi-concept erasure, achieving state-of-the-art results in generating safe and high-quality images.'}, 'zh': {'title': 'ANT：高效去除不当内容的文本到图像模型框架', 'desc': '本文提出了一种名为ANT的微调框架，用于确保文本到图像模型的伦理部署，特别是防止生成有害或不当内容。ANT通过自动引导去噪轨迹，避免了现有方法中的一些局限性，如锚点方法的启发式选择和无锚点方法导致的视觉伪影。该框架利用了在去噪中后期反转分类器无指导的条件方向的关键见解，从而实现了精确的内容修改，同时保持了早期阶段的结构完整性。通过增强的权重显著性图，ANT能够有效识别并去除单一或多个不当概念，实验结果表明其在去除效果和生成质量上均达到了最先进的水平。'}}}, {'id': 'https://huggingface.co/papers/2504.13181', 'title': 'Perception Encoder: The best visual embeddings are not at the output of\n  the network', 'url': 'https://huggingface.co/papers/2504.13181', 'abstract': 'We introduce Perception Encoder (PE), a state-of-the-art encoder for image and video understanding trained via simple vision-language learning. Traditionally, vision encoders have relied on a variety of pretraining objectives, each tailored to specific downstream tasks such as classification, captioning, or localization. Surprisingly, after scaling our carefully tuned image pretraining recipe and refining with our robust video data engine, we find that contrastive vision-language training alone can produce strong, general embeddings for all of these downstream tasks. There is only one caveat: these embeddings are hidden within the intermediate layers of the network. To draw them out, we introduce two alignment methods, language alignment for multimodal language modeling, and spatial alignment for dense prediction. Together with the core contrastive checkpoint, our PE family of models achieves state-of-the-art performance on a wide variety of tasks, including zero-shot image and video classification and retrieval; document, image, and video Q&A; and spatial tasks such as detection, depth estimation, and tracking. To foster further research, we are releasing our models, code, and a novel dataset of synthetically and human-annotated videos.', 'score': 1, 'issue_id': 3313, 'pub_date': '2025-04-17', 'pub_date_card': {'ru': '17 апреля', 'en': 'April 17', 'zh': '4月17日'}, 'hash': '3c5cea92d3dd07c4', 'authors': ['Daniel Bolya', 'Po-Yao Huang', 'Peize Sun', 'Jang Hyun Cho', 'Andrea Madotto', 'Chen Wei', 'Tengyu Ma', 'Jiale Zhi', 'Jathushan Rajasegaran', 'Hanoona Rasheed', 'Junke Wang', 'Marco Monteiro', 'Hu Xu', 'Shiyu Dong', 'Nikhila Ravi', 'Daniel Li', 'Piotr Dollár', 'Christoph Feichtenhofer'], 'affiliations': ['Fudan University', 'MBZUAI', 'Meta FAIR', 'Meta Reality Labs', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2504.13181.jpg', 'data': {'categories': ['#synthetic', '#multimodal', '#dataset', '#alignment', '#open_source', '#cv'], 'emoji': '🧠', 'ru': {'title': 'Универсальный энкодер для понимания изображений и видео', 'desc': 'Исследователи представляют Perception Encoder (PE) - современный энкодер для понимания изображений и видео, обученный с помощью простого обучения на основе зрения и языка. В отличие от традиционных подходов, PE использует только контрастивное обучение для создания универсальных эмбеддингов. Авторы вводят методы языкового и пространственного выравнивания для извлечения этих эмбеддингов из промежуточных слоев сети. Модели семейства PE достигают передовых результатов в широком спектре задач компьютерного зрения и обработки естественного языка.'}, 'en': {'title': 'Unlocking Versatile Visual Understanding with Perception Encoder', 'desc': "The paper presents the Perception Encoder (PE), an advanced model designed for understanding images and videos through vision-language learning. Unlike traditional encoders that depend on specific pretraining tasks, PE utilizes contrastive vision-language training to create versatile embeddings applicable to various tasks. The authors introduce two alignment techniques to extract these embeddings from the network's intermediate layers, enhancing performance in tasks like classification and question answering. By releasing their models and a new dataset, they aim to encourage further exploration in the field of multimodal learning."}, 'zh': {'title': '感知编码器：图像与视频理解的新突破', 'desc': '我们介绍了一种名为感知编码器（PE）的先进编码器，用于图像和视频理解，采用简单的视觉-语言学习进行训练。传统的视觉编码器依赖于多种预训练目标，针对特定的下游任务如分类、描述或定位进行优化。令人惊讶的是，通过扩展我们精心调整的图像预训练方案，并结合强大的视频数据引擎，发现对比视觉-语言训练可以为所有这些下游任务生成强大的通用嵌入。为了提取这些嵌入，我们引入了两种对齐方法，分别用于多模态语言建模和密集预测，从而使我们的PE模型在多种任务上实现了最先进的性能。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (3)', '#agi', '#alignment (2)', '#architecture (1)', '#audio (1)', '#benchmark (4)', '#cv (7)', '#data (4)', '#dataset (11)', '#diffusion (3)', '#ethics (1)', '#games', '#graphs', '#hallucinations (3)', '#healthcare', '#inference (2)', '#interpretability (2)', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (7)', '#open_source (4)', '#optimization (9)', '#plp', '#rag (1)', '#reasoning (5)', '#rl (1)', '#rlhf (1)', '#robotics', '#science (1)', '#security', '#small_models (1)', '#story_generation', '#survey', '#synthetic (3)', '#training (7)', '#transfer_learning', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-04-18 14:10',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-04-18 14:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-04-18 14:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    