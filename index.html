
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 25 papers. March 26.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">26 марта</span> | <span id="title-articles-count">25 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-03-25.html">⬅️ <span id="prev-date">25.03</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-03-27.html">➡️ <span id="next-date">27.03</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-03.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'};
        let feedDateNext = {'ru': '27.03', 'en': '03/27', 'zh': '3月27日'};
        let feedDatePrev = {'ru': '25.03', 'en': '03/25', 'zh': '3月25日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.19325', 'title': 'Long-Context Autoregressive Video Modeling with Next-Frame Prediction', 'url': 'https://huggingface.co/papers/2503.19325', 'abstract': 'Long-context autoregressive modeling has significantly advanced language generation, but video generation still struggles to fully utilize extended temporal contexts. To investigate long-context video modeling, we introduce Frame AutoRegressive (FAR), a strong baseline for video autoregressive modeling. Just as language models learn causal dependencies between tokens (i.e., Token AR), FAR models temporal causal dependencies between continuous frames, achieving better convergence than Token AR and video diffusion transformers. Building on FAR, we observe that long-context vision modeling faces challenges due to visual redundancy. Existing RoPE lacks effective temporal decay for remote context and fails to extrapolate well to long video sequences. Additionally, training on long videos is computationally expensive, as vision tokens grow much faster than language tokens. To tackle these issues, we propose balancing locality and long-range dependency. We introduce FlexRoPE, an test-time technique that adds flexible temporal decay to RoPE, enabling extrapolation to 16x longer vision contexts. Furthermore, we propose long short-term context modeling, where a high-resolution short-term context window ensures fine-grained temporal consistency, while an unlimited long-term context window encodes long-range information using fewer tokens. With this approach, we can train on long video sequences with a manageable token context length. We demonstrate that FAR achieves state-of-the-art performance in both short- and long-video generation, providing a simple yet effective baseline for video autoregressive modeling.', 'score': 56, 'issue_id': 2896, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '543c7dbfad83ed73', 'authors': ['Yuchao Gu', 'Weijia Mao', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.19325.jpg', 'data': {'categories': ['#training', '#multimodal', '#long_context', '#video'], 'emoji': '🎬', 'ru': {'title': 'FAR: эффективное моделирование длинных видеопоследовательностей', 'desc': 'Статья представляет Frame AutoRegressive (FAR) - новый подход к авторегрессионному моделированию видео с длинным контекстом. Авторы вводят FlexRoPE - технику, позволяющую экстраполировать модель на контексты в 16 раз длиннее обучающих. Предлагается комбинировать моделирование краткосрочного и долгосрочного контекста для эффективной обработки длинных видеопоследовательностей. FAR демонстрирует наилучшие результаты в генерации как коротких, так и длинных видео.'}, 'en': {'title': 'Revolutionizing Video Generation with Frame AutoRegressive Modeling', 'desc': 'This paper presents Frame AutoRegressive (FAR), a new method for generating videos by modeling the temporal dependencies between frames. FAR improves upon existing models by addressing the challenges of visual redundancy and the limitations of current positional encoding techniques like RoPE. The authors introduce FlexRoPE, which allows for flexible temporal decay, enabling the model to handle longer video sequences effectively. By combining short-term and long-term context modeling, FAR achieves state-of-the-art results in video generation while maintaining computational efficiency.'}, 'zh': {'title': '长时间上下文视频生成的新突破', 'desc': '本文介绍了一种新的视频自回归建模方法，称为Frame AutoRegressive (FAR)，旨在解决长时间上下文视频生成中的挑战。FAR通过建模连续帧之间的时间因果关系，超越了传统的语言模型，取得了更好的收敛效果。为了应对视觉冗余和计算成本问题，本文提出了FlexRoPE技术，能够灵活调整远程上下文的时间衰减，并引入了长短期上下文建模方法，以确保时间一致性。实验结果表明，FAR在短视频和长视频生成任务中均达到了最先进的性能，成为视频自回归建模的有效基线。'}}}, {'id': 'https://huggingface.co/papers/2503.18931', 'title': 'CoMP: Continual Multimodal Pre-training for Vision Foundation Models', 'url': 'https://huggingface.co/papers/2503.18931', 'abstract': 'Pre-trained Vision Foundation Models (VFMs) provide strong visual representations for a wide range of applications. In this paper, we continually pre-train prevailing VFMs in a multimodal manner such that they can effortlessly process visual inputs of varying sizes and produce visual representations that are more aligned with language representations, regardless of their original pre-training process. To this end, we introduce CoMP, a carefully designed multimodal pre-training pipeline. CoMP uses a Continual Rotary Position Embedding to support native resolution continual pre-training, and an Alignment Loss between visual and textual features through language prototypes to align multimodal representations. By three-stage training, our VFMs achieve remarkable improvements not only in multimodal understanding but also in other downstream tasks such as classification and segmentation. Remarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA with a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5 mIoU on ADE20K under frozen chunk evaluation.', 'score': 23, 'issue_id': 2897, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '59128d6a0bd3862c', 'authors': ['Yitong Chen', 'Lingchen Meng', 'Wujian Peng', 'Zuxuan Wu', 'Yu-Gang Jiang'], 'affiliations': ['Shanghai Innovation Institute', 'Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18931.jpg', 'data': {'categories': ['#alignment', '#multimodal', '#optimization', '#training', '#cv'], 'emoji': '🔀', 'ru': {'title': 'Универсальное мультимодальное дообучение для улучшения визуальных моделей', 'desc': 'Статья представляет CoMP - новый метод мультимодального дообучения предобученных моделей компьютерного зрения (VFM). CoMP использует непрерывное ротационное позиционное кодирование для обработки изображений разного размера и функцию выравнивания для согласования визуальных и текстовых представлений. Трехэтапное обучение значительно улучшает результаты как в мультимодальном понимании, так и в задачах классификации и сегментации. Модель CoMP-SigLIP достигает высоких показателей на различных бенчмарках, сохраняя при этом хорошую точность на ImageNet-1K и ADE20K.'}, 'en': {'title': 'Enhancing Visual Models with Multimodal Pre-Training', 'desc': 'This paper presents a method to enhance Vision Foundation Models (VFMs) by using a multimodal pre-training approach. The proposed method, called CoMP, allows VFMs to process visual inputs of different sizes and align visual representations with language representations. CoMP employs a Continual Rotary Position Embedding and an Alignment Loss to improve the integration of visual and textual features. The results show significant performance gains in multimodal tasks and other applications like classification and segmentation, demonstrating the effectiveness of the proposed training pipeline.'}, 'zh': {'title': '提升视觉模型的多模态预训练方法', 'desc': '本文介绍了一种新的多模态预训练方法CoMP，用于提升视觉基础模型（VFM）的表现。通过持续的多模态预训练，模型能够处理不同大小的视觉输入，并生成与语言表示更一致的视觉表示。CoMP采用了持续旋转位置嵌入和视觉与文本特征之间的对齐损失，以实现多模态表示的对齐。经过三阶段训练，我们的模型在多模态理解和其他下游任务上都取得了显著的提升。'}}}, {'id': 'https://huggingface.co/papers/2503.19622', 'title': 'Exploring Hallucination of Large Multimodal Models in Video\n  Understanding: Benchmark, Analysis and Mitigation', 'url': 'https://huggingface.co/papers/2503.19622', 'abstract': 'The hallucination of large multimodal models (LMMs), providing responses that appear correct but are actually incorrect, limits their reliability and applicability. This paper aims to study the hallucination problem of LMMs in video modality, which is dynamic and more challenging compared to static modalities like images and text. From this motivation, we first present a comprehensive benchmark termed HAVEN for evaluating hallucinations of LMMs in video understanding tasks. It is built upon three dimensions, i.e., hallucination causes, hallucination aspects, and question formats, resulting in 6K questions. Then, we quantitatively study 7 influential factors on hallucinations, e.g., duration time of videos, model sizes, and model reasoning, via experiments of 16 LMMs on the presented benchmark. In addition, inspired by recent thinking models like OpenAI o1, we propose a video-thinking model to mitigate the hallucinations of LMMs via supervised reasoning fine-tuning (SRFT) and direct preference optimization (TDPO)-- where SRFT enhances reasoning capabilities while TDPO reduces hallucinations in the thinking process. Extensive experiments and analyses demonstrate the effectiveness. Remarkably, it improves the baseline by 7.65% in accuracy on hallucination evaluation and reduces the bias score by 4.5%. The code and data are public at https://github.com/Hongcheng-Gao/HAVEN.', 'score': 22, 'issue_id': 2897, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'c727aefeccdca380', 'authors': ['Hongcheng Gao', 'Jiashu Qu', 'Jingyi Tang', 'Baolong Bi', 'Yue Liu', 'Hongyu Chen', 'Li Liang', 'Li Su', 'Qingming Huang'], 'affiliations': ['Beijing Jiaotong University', 'Key Lab of Intell. Info. Process., Inst. of Comput. Tech., CAS', 'National University of Singapore', 'University of Chinese Academy of Sciences', 'University of Cincinnati'], 'pdf_title_img': 'assets/pdf/title_img/2503.19622.jpg', 'data': {'categories': ['#video', '#multimodal', '#benchmark', '#hallucinations', '#training', '#reasoning'], 'emoji': '🎥', 'ru': {'title': 'Борьба с галлюцинациями в видеоанализе: новый бенчмарк и мыслящая модель', 'desc': "Статья представляет комплексный подход к изучению проблемы галлюцинаций в крупных мультимодальных моделях (LMM) при обработке видео. Авторы создали бенчмарк HAVEN для оценки галлюцинаций LMM в задачах понимания видео, охватывающий различные аспекты и форматы вопросов. Исследование включает анализ семи факторов, влияющих на галлюцинации, и эксперименты с 16 различными LMM. Предложена модель 'video-thinking' для снижения галлюцинаций с использованием методов SRFT и TDPO, показавшая значительное улучшение точности и снижение предвзятости."}, 'en': {'title': 'Mitigating Hallucinations in Video Understanding with HAVEN', 'desc': 'This paper addresses the issue of hallucinations in large multimodal models (LMMs), particularly in video understanding tasks. It introduces a benchmark called HAVEN, which evaluates hallucinations based on various factors such as causes, aspects, and question formats, comprising 6,000 questions. The authors analyze seven influential factors affecting hallucinations and propose a novel video-thinking model that employs supervised reasoning fine-tuning and direct preference optimization to reduce these hallucinations. Experimental results show a significant improvement in accuracy and a reduction in bias, demonstrating the effectiveness of the proposed methods.'}, 'zh': {'title': '解决视频模态中的幻觉问题', 'desc': '本文研究了大型多模态模型（LMMs）在视频理解任务中的幻觉问题，这种问题使得模型的输出看似正确但实际上不准确。我们提出了一个名为HAVEN的基准，用于评估LMMs在视频模态下的幻觉，涵盖了幻觉的原因、方面和问题格式等三个维度，共包含6000个问题。通过对16个LMMs进行实验，我们定量分析了影响幻觉的7个因素，如视频时长、模型规模和推理能力。最后，我们提出了一种视频思维模型，通过监督推理微调（SRFT）和直接偏好优化（TDPO）来减轻幻觉现象，实验结果显示该方法在准确性上提高了7.65%。'}}}, {'id': 'https://huggingface.co/papers/2503.19385', 'title': 'Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing', 'url': 'https://huggingface.co/papers/2503.19385', 'abstract': 'We propose an inference-time scaling approach for pretrained flow models. Recently, inference-time scaling has gained significant attention in LLMs and diffusion models, improving sample quality or better aligning outputs with user preferences by leveraging additional computation. For diffusion models, particle sampling has allowed more efficient scaling due to the stochasticity at intermediate denoising steps. On the contrary, while flow models have gained popularity as an alternative to diffusion models--offering faster generation and high-quality outputs in state-of-the-art image and video generative models--efficient inference-time scaling methods used for diffusion models cannot be directly applied due to their deterministic generative process. To enable efficient inference-time scaling for flow models, we propose three key ideas: 1) SDE-based generation, enabling particle sampling in flow models, 2) Interpolant conversion, broadening the search space and enhancing sample diversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of computational resources across timesteps to maximize budget utilization. Our experiments show that SDE-based generation, particularly variance-preserving (VP) interpolant-based generation, improves the performance of particle sampling methods for inference-time scaling in flow models. Additionally, we demonstrate that RBF with VP-SDE achieves the best performance, outperforming all previous inference-time scaling approaches.', 'score': 22, 'issue_id': 2896, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'e0ead8fbe973f326', 'authors': ['Jaihoon Kim', 'Taehoon Yoon', 'Jisung Hwang', 'Minhyuk Sung'], 'affiliations': ['KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.19385.jpg', 'data': {'categories': ['#inference', '#diffusion', '#optimization', '#video'], 'emoji': '🌊', 'ru': {'title': 'Эффективное масштабирование потоковых моделей при выводе', 'desc': 'Статья предлагает метод масштабирования во время вывода для предобученных потоковых моделей. Авторы вводят три ключевые идеи: генерация на основе стохастических дифференциальных уравнений (SDE), преобразование интерполянтов и адаптивное распределение вычислительных ресурсов. Эксперименты показывают, что генерация на основе SDE, особенно с сохранением дисперсии, улучшает производительность методов выборки частиц для масштабирования во время вывода в потоковых моделях. Предложенный подход превосходит предыдущие методы масштабирования во время вывода.'}, 'en': {'title': 'Enhancing Flow Models with Efficient Inference-Time Scaling', 'desc': 'This paper introduces a new method for improving flow models during inference, which is the process of generating outputs from trained models. The authors focus on three innovative techniques: using stochastic differential equations (SDE) for particle sampling, converting interpolants to increase diversity in generated samples, and implementing Rollover Budget Forcing (RBF) to optimize computational resource allocation. These methods allow flow models to achieve better sample quality and efficiency, similar to advancements seen in diffusion models. The results show that their approach significantly enhances performance, making flow models more competitive in generating high-quality images and videos.'}, 'zh': {'title': '流模型的高效推理时间缩放新方法', 'desc': '本文提出了一种针对预训练流模型的推理时间缩放方法。近年来，推理时间缩放在大语言模型和扩散模型中受到广泛关注，通过利用额外的计算来提高样本质量或更好地符合用户偏好。尽管流模型作为扩散模型的替代方案越来越受欢迎，但由于其确定性生成过程，现有的扩散模型推理时间缩放方法无法直接应用于流模型。我们提出了三种关键思想，以实现流模型的高效推理时间缩放：基于SDE的生成、插值转换和自适应计算资源分配。'}}}, {'id': 'https://huggingface.co/papers/2503.19903', 'title': 'Scaling Vision Pre-Training to 4K Resolution', 'url': 'https://huggingface.co/papers/2503.19903', 'abstract': 'High-resolution perception of visual details is crucial for daily tasks. Current vision pre-training, however, is still limited to low resolutions (e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images. We introduce PS3 that scales CLIP-style vision pre-training to 4K resolution with a near-constant cost. Instead of contrastive learning on global image representation, PS3 is pre-trained by selectively processing local regions and contrasting them with local detailed captions, enabling high-resolution representation learning with greatly reduced computational overhead. The pre-trained PS3 is able to both encode the global image at low resolution and selectively process local high-resolution regions based on their saliency or relevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the resulting model, named VILA-HD, significantly improves high-resolution visual perception compared to baselines without high-resolution vision pre-training such as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks appealing scaling properties of VILA-HD, including scaling up resolution for free and scaling up test-time compute for better performance. Compared to state of the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL across multiple benchmarks and achieves better efficiency than latest token pruning approaches. Finally, we find current benchmarks do not require 4K-resolution perception, which motivates us to propose 4KPro, a new benchmark of image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs, including a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x speedup over Qwen2-VL.', 'score': 14, 'issue_id': 2898, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '7b81adffd1f39557', 'authors': ['Baifeng Shi', 'Boyi Li', 'Han Cai', 'Yao Lu', 'Sifei Liu', 'Marco Pavone', 'Jan Kautz', 'Song Han', 'Trevor Darrell', 'Pavlo Molchanov', 'Hongxu Yin'], 'affiliations': ['NVIDIA', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.19903.jpg', 'data': {'categories': ['#optimization', '#training', '#cv', '#multimodal', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'PS3: Эффективное предобучение для восприятия изображений сверхвысокого разрешения', 'desc': 'PS3 - это новый метод предобучения моделей компьютерного зрения, позволяющий работать с изображениями сверхвысокого разрешения (4K) при почти постоянных вычислительных затратах. Вместо контрастивного обучения на глобальном представлении изображения, PS3 обрабатывает выборочно локальные области и сопоставляет их с детальными текстовыми описаниями. Применение PS3 в мультимодальных языковых моделях (MLLM) значительно улучшает восприятие деталей изображений высокого разрешения по сравнению с базовыми моделями. PS3 также позволяет масштабировать разрешение и вычислительные ресурсы без дополнительных затрат, что приводит к повышению производительности.'}, 'en': {'title': 'Scaling Vision Pre-Training to 4K with PS3', 'desc': 'This paper presents PS3, a novel approach to vision pre-training that enables high-resolution image processing at 4K resolution while maintaining a near-constant computational cost. By focusing on local regions of images and contrasting them with detailed captions, PS3 enhances the learning of visual representations without the heavy resource demands typically associated with high-resolution data. The resulting model, VILA-HD, demonstrates significant improvements in visual perception tasks compared to existing models, achieving better efficiency and performance across various benchmarks. Additionally, the authors introduce 4KPro, a new benchmark for image question answering at 4K resolution, where VILA-HD shows superior results over previous models.'}, 'zh': {'title': '高分辨率视觉感知的新突破', 'desc': '本论文介绍了一种名为PS3的视觉预训练方法，能够以接近恒定的成本将CLIP风格的视觉预训练扩展到4K分辨率。PS3通过选择性处理局部区域并与局部详细描述进行对比，来实现高分辨率表示学习，从而大幅降低计算开销。预训练后的PS3能够在低分辨率下编码全局图像，并根据文本提示的显著性或相关性选择性处理局部高分辨率区域。最终，基于PS3的多模态大语言模型VILA-HD在多个基准测试中显著提升了高分辨率视觉感知能力，超越了现有的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2503.14905', 'title': 'Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection\n  with Artifact Explanation', 'url': 'https://huggingface.co/papers/2503.14905', 'abstract': 'With the rapid advancement of Artificial Intelligence Generated Content (AIGC) technologies, synthetic images have become increasingly prevalent in everyday life, posing new challenges for authenticity assessment and detection. Despite the effectiveness of existing methods in evaluating image authenticity and locating forgeries, these approaches often lack human interpretability and do not fully address the growing complexity of synthetic data. To tackle these challenges, we introduce FakeVLM, a specialized large multimodal model designed for both general synthetic image and DeepFake detection tasks. FakeVLM not only excels in distinguishing real from fake images but also provides clear, natural language explanations for image artifacts, enhancing interpretability. Additionally, we present FakeClue, a comprehensive dataset containing over 100,000 images across seven categories, annotated with fine-grained artifact clues in natural language. FakeVLM demonstrates performance comparable to expert models while eliminating the need for additional classifiers, making it a robust solution for synthetic data detection. Extensive evaluations across multiple datasets confirm the superiority of FakeVLM in both authenticity classification and artifact explanation tasks, setting a new benchmark for synthetic image detection. The dataset and code will be released in: https://github.com/opendatalab/FakeVLM.', 'score': 14, 'issue_id': 2900, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'}, 'hash': 'e8053458773c179b', 'authors': ['Siwei Wen', 'Junyan Ye', 'Peilin Feng', 'Hengrui Kang', 'Zichen Wen', 'Yize Chen', 'Jiang Wu', 'Wenjun Wu', 'Conghui He', 'Weijia Li'], 'affiliations': ['Beihang University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'Sun Yat-Sen University', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2503.14905.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#benchmark', '#dataset', '#synthetic', '#cv'], 'emoji': '🕵️', 'ru': {'title': 'FakeVLM: Интерпретируемое обнаружение синтетических изображений с помощью мультимодальной модели', 'desc': 'FakeVLM - это специализированная большая мультимодальная модель для обнаружения синтетических изображений и DeepFake. Она не только различает реальные и поддельные изображения, но и предоставляет понятные объяснения артефактов на естественном языке. Авторы также представили набор данных FakeClue с более чем 100 000 изображений, аннотированных подробными подсказками об артефактах. FakeVLM демонстрирует производительность на уровне экспертных моделей, устанавливая новый стандарт в обнаружении синтетических изображений.'}, 'en': {'title': 'FakeVLM: Unmasking Synthetic Images with Clarity', 'desc': 'This paper introduces FakeVLM, a large multimodal model designed to detect synthetic images and DeepFakes while providing human-readable explanations for its decisions. Unlike existing methods, FakeVLM enhances interpretability by explaining image artifacts in natural language, making it easier for users to understand the detection process. The model is evaluated on a new dataset called FakeClue, which contains over 100,000 annotated images, allowing for fine-grained analysis of image authenticity. Results show that FakeVLM performs comparably to expert models without needing extra classifiers, establishing a new standard in synthetic image detection.'}, 'zh': {'title': 'FakeVLM：合成图像检测的新标杆', 'desc': '随着人工智能生成内容（AIGC）技术的快速发展，合成图像在日常生活中变得越来越普遍，这给真实性评估和检测带来了新的挑战。现有的方法虽然在评估图像真实性和定位伪造方面有效，但往往缺乏人类可解释性，无法完全应对合成数据日益复杂的情况。为了解决这些问题，我们提出了FakeVLM，这是一种专门针对合成图像和深度伪造检测任务的大型多模态模型。FakeVLM不仅在区分真实与伪造图像方面表现出色，还能提供清晰的自然语言解释，增强了可解释性。'}}}, {'id': 'https://huggingface.co/papers/2503.19855', 'title': 'Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time\n  Thinking', 'url': 'https://huggingface.co/papers/2503.19855', 'abstract': "Recent advances in large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where extended reasoning processes substantially enhance model performance. Despite this, current models are constrained by limitations in handling long texts and reinforcement learning (RL) training efficiency. To address these issues, we propose a simple yet effective test-time scaling approach Multi-round Thinking. This method iteratively refines model reasoning by leveraging previous answers as prompts for subsequent rounds. Extensive experiments across multiple models, including QwQ-32B and DeepSeek-R1, consistently show performance improvements on various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and LiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round 1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a similar increase from 79.7% to 82.0%. These results confirm that Multi-round Thinking is a broadly applicable, straightforward approach to achieving stable enhancements in model performance, underscoring its potential for future developments in test-time scaling techniques. The key prompt: {Original question prompt} The assistant's previous answer is: <answer> {last round answer} </answer>, and please re-answer.", 'score': 8, 'issue_id': 2897, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'c9d16e0d2423104a', 'authors': ['Xiaoyu Tian', 'Sitong Zhao', 'Haotian Wang', 'Shuaiting Chen', 'Yunjie Ji', 'Yiping Peng', 'Han Zhao', 'Xiangang Li'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.19855.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#optimization', '#training', '#reasoning'], 'emoji': '🔄', 'ru': {'title': 'Итеративное улучшение ответов ИИ: простой путь к повышению точности', 'desc': "Статья представляет новый подход к масштабированию языковых моделей во время тестирования, называемый 'Многораундовое мышление'. Этот метод итеративно улучшает рассуждения модели, используя предыдущие ответы в качестве подсказок для последующих раундов. Эксперименты с различными моделями, включая QwQ-32B и DeepSeek-R1, показали стабильное улучшение производительности на нескольких бенчмарках. Результаты подтверждают, что 'Многораундовое мышление' - это широко применимый подход для повышения эффективности языковых моделей."}, 'en': {'title': 'Enhancing Model Performance with Multi-round Thinking', 'desc': "This paper introduces a new method called Multi-round Thinking to improve the performance of large language models (LLMs) during test-time scaling. The approach involves iteratively refining the model's reasoning by using previous answers as prompts for subsequent rounds of questioning. Experiments show that this method leads to significant accuracy improvements across various benchmarks, demonstrating its effectiveness in enhancing model performance. The results indicate that Multi-round Thinking is a simple yet powerful technique that can be widely applied to improve LLMs' reasoning capabilities."}, 'zh': {'title': '多轮思考：提升模型推理的有效方法', 'desc': '本文介绍了一种名为多轮思考的测试时间扩展方法，旨在提高大型语言模型的推理能力。该方法通过将之前的答案作为后续轮次的提示，迭代地优化模型的推理过程。实验结果表明，使用多轮思考后，多个模型在不同基准测试上的表现均有显著提升。比如，QwQ-32B在AIME 2024数据集上的准确率从80.3%提升至82.1%。'}}}, {'id': 'https://huggingface.co/papers/2503.13964', 'title': 'MDocAgent: A Multi-Modal Multi-Agent Framework for Document\n  Understanding', 'url': 'https://huggingface.co/papers/2503.13964', 'abstract': "Document Question Answering (DocQA) is a very common task. Existing methods using Large Language Models (LLMs) or Large Vision Language Models (LVLMs) and Retrieval Augmented Generation (RAG) often prioritize information from a single modal, failing to effectively integrate textual and visual cues. These approaches struggle with complex multi-modal reasoning, limiting their performance on real-world documents. We present MDocAgent (A Multi-Modal Multi-Agent Framework for Document Understanding), a novel RAG and multi-agent framework that leverages both text and image. Our system employs five specialized agents: a general agent, a critical agent, a text agent, an image agent and a summarizing agent. These agents engage in multi-modal context retrieval, combining their individual insights to achieve a more comprehensive understanding of the document's content. This collaborative approach enables the system to synthesize information from both textual and visual components, leading to improved accuracy in question answering. Preliminary experiments on five benchmarks like MMLongBench, LongDocURL demonstrate the effectiveness of our MDocAgent, achieve an average improvement of 12.1% compared to current state-of-the-art method. This work contributes to the development of more robust and comprehensive DocQA systems capable of handling the complexities of real-world documents containing rich textual and visual information. Our data and code are available at https://github.com/aiming-lab/MDocAgent.", 'score': 8, 'issue_id': 2900, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': 'ac3677766b7897a0', 'authors': ['Siwei Han', 'Peng Xia', 'Ruiyi Zhang', 'Tong Sun', 'Yun Li', 'Hongtu Zhu', 'Huaxiu Yao'], 'affiliations': ['Adobe Research', 'UNC-Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2503.13964.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#benchmark', '#optimization', '#agents', '#open_source', '#rag'], 'emoji': '🤖', 'ru': {'title': 'Мультиагентный подход к пониманию документов: объединяя текст и изображения', 'desc': 'Статья представляет MDocAgent - новую мультимодальную мультиагентную систему для понимания документов. В отличие от существующих методов, использующих большие языковые модели или RAG, MDocAgent эффективно интегрирует текстовую и визуальную информацию. Система использует пять специализированных агентов для многомодального поиска контекста и синтеза информации. Эксперименты показали улучшение точности ответов на вопросы в среднем на 12.1% по сравнению с современными методами.'}, 'en': {'title': 'MDocAgent: Uniting Text and Images for Smarter Document Understanding', 'desc': 'This paper introduces MDocAgent, a new framework designed for Document Question Answering (DocQA) that effectively combines text and image data. Unlike existing methods that often focus on a single type of information, MDocAgent utilizes a multi-agent system with five specialized agents to enhance multi-modal reasoning. These agents work together to retrieve and synthesize information from both textual and visual sources, improving the accuracy of answers to questions about documents. Preliminary tests show that MDocAgent outperforms current leading methods by an average of 12.1%, demonstrating its potential for better handling complex real-world documents.'}, 'zh': {'title': '多模态文档理解的新突破', 'desc': '本文介绍了一种新的文档问答系统MDocAgent，它结合了文本和图像信息，旨在提高多模态文档理解的能力。现有的方法往往只关注单一模态，导致在复杂的多模态推理中表现不佳。MDocAgent采用了五个专门的代理，分别负责不同的任务，通过协作检索多模态上下文，从而更全面地理解文档内容。实验结果表明，MDocAgent在多个基准测试中表现优异，平均提高了12.1%的准确率，展示了其在处理现实世界文档中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.19910', 'title': 'CoLLM: A Large Language Model for Composed Image Retrieval', 'url': 'https://huggingface.co/papers/2503.19910', 'abstract': 'Composed Image Retrieval (CIR) is a complex task that aims to retrieve images based on a multimodal query. Typical training data consists of triplets containing a reference image, a textual description of desired modifications, and the target image, which are expensive and time-consuming to acquire. The scarcity of CIR datasets has led to zero-shot approaches utilizing synthetic triplets or leveraging vision-language models (VLMs) with ubiquitous web-crawled image-caption pairs. However, these methods have significant limitations: synthetic triplets suffer from limited scale, lack of diversity, and unnatural modification text, while image-caption pairs hinder joint embedding learning of the multimodal query due to the absence of triplet data. Moreover, existing approaches struggle with complex and nuanced modification texts that demand sophisticated fusion and understanding of vision and language modalities. We present CoLLM, a one-stop framework that effectively addresses these limitations. Our approach generates triplets on-the-fly from image-caption pairs, enabling supervised training without manual annotation. We leverage Large Language Models (LLMs) to generate joint embeddings of reference images and modification texts, facilitating deeper multimodal fusion. Additionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset comprising 3.4M samples, and refine existing CIR benchmarks (CIRR and Fashion-IQ) to enhance evaluation reliability. Experimental results demonstrate that CoLLM achieves state-of-the-art performance across multiple CIR benchmarks and settings. MTCIR yields competitive results, with up to 15% performance improvement. Our refined benchmarks provide more reliable evaluation metrics for CIR models, contributing to the advancement of this important field.', 'score': 7, 'issue_id': 2896, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '63f36082e6c27f3e', 'authors': ['Chuong Huynh', 'Jinyu Yang', 'Ashish Tawari', 'Mubarak Shah', 'Son Tran', 'Raffay Hamid', 'Trishul Chilimbi', 'Abhinav Shrivastava'], 'affiliations': ['Amazon', 'Center for Research in Computer Vision, University of Central Florida', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2503.19910.jpg', 'data': {'categories': ['#optimization', '#synthetic', '#dataset', '#multimodal', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'CoLLM: Революция в композиционном поиске изображений', 'desc': 'Статья представляет CoLLM - новый фреймворк для решения задачи композиционного поиска изображений (CIR). CoLLM генерирует обучающие триплеты на лету из пар изображение-подпись, что позволяет обучаться без ручной разметки. Авторы используют большие языковые модели для создания совместных эмбеддингов изображений и текстов модификации, улучшая мультимодальное слияние. Также представлен новый крупномасштабный датасет MTCIR и уточнены существующие бенчмарки для более надежной оценки моделей CIR.'}, 'en': {'title': 'Revolutionizing Composed Image Retrieval with CoLLM', 'desc': 'This paper introduces CoLLM, a novel framework for Composed Image Retrieval (CIR) that overcomes the challenges of limited training data. It generates triplets from existing image-caption pairs, allowing for supervised training without the need for manual annotations. By utilizing Large Language Models (LLMs), CoLLM creates joint embeddings that enhance the understanding of complex multimodal queries. The authors also present the Multi-Text CIR (MTCIR) dataset, which significantly improves evaluation metrics and demonstrates state-of-the-art performance in CIR tasks.'}, 'zh': {'title': 'CoLLM：复合图像检索的新突破', 'desc': '本文介绍了一种名为CoLLM的框架，用于解决复合图像检索（CIR）中的数据稀缺问题。该框架通过从图像-文本对中动态生成三元组，避免了手动标注的需求，从而实现了监督学习。我们利用大型语言模型（LLMs）生成参考图像和修改文本的联合嵌入，促进了多模态的深度融合。此外，我们还推出了一个包含340万样本的大规模数据集MTCIR，并改进了现有的CIR基准，以提高评估的可靠性。'}}}, {'id': 'https://huggingface.co/papers/2503.19470', 'title': 'ReSearch: Learning to Reason with Search for LLMs via Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2503.19470', 'abstract': 'Large Language Models (LLMs) have shown remarkable capabilities in reasoning, exemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating reasoning with external search processes remains challenging, especially for complex multi-hop questions requiring multiple retrieval steps. We propose ReSearch, a novel framework that trains LLMs to Reason with Search via reinforcement learning without using any supervised data on reasoning steps. Our approach treats search operations as integral components of the reasoning chain, where when and how to perform searches is guided by text-based thinking, and search results subsequently influence further reasoning. We train ReSearch on Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct extensive experiments. Despite being trained on only one dataset, our models demonstrate strong generalizability across various benchmarks. Analysis reveals that ReSearch naturally elicits advanced reasoning capabilities such as reflection and self-correction during the reinforcement learning process.', 'score': 5, 'issue_id': 2897, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '3e50fa3f4f4a6c0c', 'authors': ['Mingyang Chen', 'Tianpeng Li', 'Haoze Sun', 'Yijie Zhou', 'Chenzheng Zhu', 'Fan Yang', 'Zenan Zhou', 'Weipeng Chen', 'Haofen Wang', 'Jeff Z. Pan', 'Wen Zhang', 'Huajun Chen'], 'affiliations': ['Baichuan Inc.', 'The University of Edinburgh', 'Tongji University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.19470.jpg', 'data': {'categories': ['#rl', '#benchmark', '#rag', '#training', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Усиление рассуждений ИИ через интеграцию поиска', 'desc': 'Авторы предлагают новый фреймворк ReSearch, который обучает большие языковые модели (LLM) рассуждать с использованием поиска через обучение с подкреплением. Модель учится интегрировать операции поиска в цепочку рассуждений, где текстовое мышление направляет, когда и как выполнять поиск. Эксперименты показывают, что, несмотря на обучение только на одном наборе данных, модели демонстрируют сильную обобщаемость на различных бенчмарках. Анализ выявляет, что ReSearch естественным образом вызывает продвинутые способности рассуждения, такие как рефлексия и самокоррекция.'}, 'en': {'title': 'Empowering LLMs: Reasoning Meets Search with ReSearch', 'desc': 'This paper introduces ReSearch, a new framework that enhances Large Language Models (LLMs) by integrating reasoning with external search processes. It uses reinforcement learning to train LLMs to effectively decide when and how to perform searches, treating these operations as key parts of the reasoning process. The framework is tested on Qwen2.5 models and shows strong performance across different benchmarks, even though it was trained on a single dataset. Notably, ReSearch enables advanced reasoning skills like reflection and self-correction, showcasing its potential for complex question answering.'}, 'zh': {'title': '推理与搜索的完美结合', 'desc': '大型语言模型（LLMs）在推理方面表现出色，但将推理与外部搜索过程结合仍然具有挑战性，尤其是对于复杂的多跳问题。我们提出了ReSearch，一个新颖的框架，通过强化学习训练LLMs进行搜索推理，而不使用任何监督数据。该方法将搜索操作视为推理链的核心部分，搜索的时机和方式由基于文本的思维指导，搜索结果进一步影响推理过程。我们的实验表明，尽管只在一个数据集上训练，ReSearch模型在多个基准测试中展现出强大的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.18446', 'title': 'Latent Space Super-Resolution for Higher-Resolution Image Generation\n  with Diffusion Models', 'url': 'https://huggingface.co/papers/2503.18446', 'abstract': 'In this paper, we propose LSRNA, a novel framework for higher-resolution (exceeding 1K) image generation using diffusion models by leveraging super-resolution directly in the latent space. Existing diffusion models struggle with scaling beyond their training resolutions, often leading to structural distortions or content repetition. Reference-based methods address the issues by upsampling a low-resolution reference to guide higher-resolution generation. However, they face significant challenges: upsampling in latent space often causes manifold deviation, which degrades output quality. On the other hand, upsampling in RGB space tends to produce overly smoothed outputs. To overcome these limitations, LSRNA combines Latent space Super-Resolution (LSR) for manifold alignment and Region-wise Noise Addition (RNA) to enhance high-frequency details. Our extensive experiments demonstrate that integrating LSRNA outperforms state-of-the-art reference-based methods across various resolutions and metrics, while showing the critical role of latent space upsampling in preserving detail and sharpness. The code is available at https://github.com/3587jjh/LSRNA.', 'score': 5, 'issue_id': 2897, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'd3e203fb399d6eee', 'authors': ['Jinho Jeong', 'Sangmin Han', 'Jinwoo Kim', 'Seon Joo Kim'], 'affiliations': ['Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18446.jpg', 'data': {'categories': ['#open_source', '#diffusion', '#3d', '#optimization', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'LSRNA: Суперразрешение в латентном пространстве для генерации детализированных изображений', 'desc': 'LSRNA - это новый фреймворк для генерации изображений высокого разрешения (более 1K) с использованием диффузионных моделей. Он решает проблемы существующих методов, которые часто приводят к искажениям структуры или повторению контента при масштабировании. LSRNA сочетает суперразрешение в латентном пространстве (LSR) для выравнивания многообразия и добавление шума по регионам (RNA) для улучшения высокочастотных деталей. Эксперименты показывают, что LSRNA превосходит современные методы на основе референсов по различным разрешениям и метрикам.'}, 'en': {'title': 'Enhancing High-Resolution Image Generation with LSRNA', 'desc': 'This paper introduces LSRNA, a new framework designed to generate high-resolution images (over 1K) using diffusion models by applying super-resolution techniques in the latent space. Traditional diffusion models often struggle with generating images at higher resolutions, leading to issues like distortions and repeated content. The proposed method addresses these challenges by utilizing Latent space Super-Resolution (LSR) for better alignment and Region-wise Noise Addition (RNA) to improve detail in the generated images. Experimental results show that LSRNA significantly outperforms existing reference-based methods, highlighting the importance of latent space upsampling for maintaining image quality.'}, 'zh': {'title': 'LSRNA：超分辨率生成的创新框架', 'desc': '本文提出了一种新颖的框架LSRNA，用于生成高分辨率（超过1K）的图像，利用扩散模型直接在潜在空间中进行超分辨率处理。现有的扩散模型在超出训练分辨率时常常出现结构失真或内容重复的问题。参考基础的方法通过将低分辨率参考图像上采样来指导高分辨率生成，但在潜在空间中上采样会导致流形偏差，从而降低输出质量。LSRNA结合了潜在空间超分辨率（LSR）和区域噪声添加（RNA），有效提升了高频细节，实验结果表明其在各个分辨率和指标上均优于现有的参考基础方法。'}}}, {'id': 'https://huggingface.co/papers/2503.19041', 'title': 'LookAhead Tuning: Safer Language Models via Partial Answer Previews', 'url': 'https://huggingface.co/papers/2503.19041', 'abstract': "Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often undermines their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, which comprises two simple, low-resource, and effective data-driven methods that modify training data by previewing partial answer prefixes. Both methods aim to preserve the model's inherent safety mechanisms by minimizing perturbations to initial token distributions. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs. Code is released at https://github.com/zjunlp/LookAheadTuning.", 'score': 3, 'issue_id': 2896, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '8495b5a09ddf5611', 'authors': ['Kangwei Liu', 'Mengru Wang', 'Yujie Luo', 'Lin Yuan', 'Mengshu Sun', 'Ningyu Zhang', 'Lei Liang', 'Zhiqiang Zhang', 'Jun Zhou', 'Huajun Chen'], 'affiliations': ['Ant Group', 'Zhejiang University', 'Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph'], 'pdf_title_img': 'assets/pdf/title_img/2503.19041.jpg', 'data': {'categories': ['#training', '#alignment', '#low_resource'], 'emoji': '🔍', 'ru': {'title': 'Безопасная адаптация языковых моделей с сохранением производительности', 'desc': 'Статья представляет новый метод тонкой настройки больших языковых моделей под названием LookAhead Tuning. Этот подход позволяет адаптировать модели к конкретным доменам, сохраняя при этом их изначальную безопасность. Метод основан на модификации обучающих данных путем предварительного просмотра частичных префиксов ответов. Эксперименты показывают, что LookAhead Tuning эффективно поддерживает безопасность модели без ущерба для производительности на целевых задачах.'}, 'en': {'title': 'LookAhead Tuning: Safeguarding LLMs During Fine-Tuning', 'desc': "This paper presents LookAhead Tuning, a novel approach to fine-tuning large language models (LLMs) while preserving their safety alignment. The method involves two data-driven techniques that adjust training data by examining partial answer prefixes, which helps maintain the model's safety mechanisms. By minimizing changes to the initial token distributions, LookAhead Tuning effectively prevents safety degradation during the adaptation process. Experimental results show that this approach not only safeguards model safety but also ensures strong performance on various downstream tasks."}, 'zh': {'title': 'LookAhead Tuning：安全微调大型语言模型的新方法', 'desc': '本论文介绍了一种名为LookAhead Tuning的技术，旨在解决在微调大型语言模型（LLMs）时安全性下降的问题。该方法通过预览部分答案前缀，采用两种简单且低资源的数据驱动方法来修改训练数据。其目标是通过最小化初始标记分布的扰动，保持模型固有的安全机制。实验结果表明，LookAhead Tuning能够有效维护模型的安全性，同时在下游任务中保持强大的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.19907', 'title': 'FullDiT: Multi-Task Video Generative Foundation Model with Full\n  Attention', 'url': 'https://huggingface.co/papers/2503.19907', 'abstract': 'Current video generative foundation models primarily focus on text-to-video tasks, providing limited control for fine-grained video content creation. Although adapter-based approaches (e.g., ControlNet) enable additional controls with minimal fine-tuning, they encounter challenges when integrating multiple conditions, including: branch conflicts between independently trained adapters, parameter redundancy leading to increased computational cost, and suboptimal performance compared to full fine-tuning. To address these challenges, we introduce FullDiT, a unified foundation model for video generation that seamlessly integrates multiple conditions via unified full-attention mechanisms. By fusing multi-task conditions into a unified sequence representation and leveraging the long-context learning ability of full self-attention to capture condition dynamics, FullDiT reduces parameter overhead, avoids conditions conflict, and shows scalability and emergent ability. We further introduce FullBench for multi-task video generation evaluation. Experiments demonstrate that FullDiT achieves state-of-the-art results, highlighting the efficacy of full-attention in complex multi-task video generation.', 'score': 2, 'issue_id': 2910, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '7505b749f1328947', 'authors': ['Xuan Ju', 'Weicai Ye', 'Quande Liu', 'Qiulin Wang', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Kun Gai', 'Qiang Xu'], 'affiliations': ['Kuaishou Technology', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.19907.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#video', '#long_context', '#games'], 'emoji': '🎬', 'ru': {'title': 'FullDiT: Единая модель для многозадачной генерации видео', 'desc': 'FullDiT - это новая модель для генерации видео, которая объединяет множество условий через механизмы полного внимания. Она решает проблемы существующих подходов, таких как конфликты между адаптерами и избыточность параметров. FullDiT использует единое представление последовательности для различных задач и способность полного самовнимания к обучению на длинном контексте. Эксперименты показывают, что FullDiT достигает передовых результатов в сложных задачах генерации видео с несколькими условиями.'}, 'en': {'title': 'FullDiT: Unifying Control for Advanced Video Generation', 'desc': 'This paper presents FullDiT, a novel video generative model that enhances control over video content creation by integrating multiple conditions effectively. Unlike existing adapter-based methods, FullDiT utilizes a unified full-attention mechanism to manage various input conditions without the issues of parameter redundancy and branch conflicts. The model captures the dynamics of conditions through a unified sequence representation, which improves scalability and performance. Additionally, the authors introduce FullBench, a new evaluation framework for assessing multi-task video generation, demonstrating that FullDiT achieves state-of-the-art results in this domain.'}, 'zh': {'title': '全注意力，视频生成的新突破', 'desc': '当前的视频生成基础模型主要集中在文本到视频的任务上，提供的细粒度视频内容创作控制有限。虽然基于适配器的方法（如ControlNet）可以在最小微调的情况下实现额外控制，但在整合多个条件时面临挑战，如独立训练的适配器之间的分支冲突、参数冗余导致的计算成本增加，以及与完全微调相比的性能不足。为了解决这些问题，我们提出了FullDiT，这是一种统一的视频生成基础模型，通过统一的全注意力机制无缝整合多个条件。实验表明，FullDiT在复杂的多任务视频生成中实现了最先进的结果，突显了全注意力在捕捉条件动态方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.19123', 'title': 'Overcoming Vocabulary Mismatch: Vocabulary-agnostic Teacher Guided\n  Language Modeling', 'url': 'https://huggingface.co/papers/2503.19123', 'abstract': 'Using large teacher models to guide the training of smaller student models has become the prevailing paradigm for efficient and effective learning. However, vocabulary mismatches between teacher and student language models pose significant challenges in language modeling, resulting in divergent token sequences and output distributions. To overcome these limitations, we propose Vocabulary-agnostic Teacher Guided Language Modeling (VocAgnoLM), a novel approach that bridges the gap caused by vocabulary mismatch through two key methods: (1) Token-level Lexical Alignment, which aligns token sequences across mismatched vocabularies, and (2) Teacher Guided Loss, which leverages the loss of teacher model to guide effective student training. We demonstrate its effectiveness in language modeling with 1B student model using various 7B teacher models with different vocabularies. Notably, with Qwen2.5-Math-Instruct, a teacher model sharing only about 6% of its vocabulary with TinyLlama, VocAgnoLM achieves a 46% performance improvement compared to naive continual pretraining. Furthermore, we demonstrate that VocAgnoLM consistently benefits from stronger teacher models, providing a robust solution to vocabulary mismatches in language modeling.', 'score': 2, 'issue_id': 2907, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'b5efb0f50380f253', 'authors': ['Haebin Shin', 'Lei Ji', 'Xiao Liu', 'Yeyun Gong'], 'affiliations': ['KAIST AI', 'Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.19123.jpg', 'data': {'categories': ['#multilingual', '#optimization', '#transfer_learning', '#small_models', '#training'], 'emoji': '🔠', 'ru': {'title': 'Преодоление языкового барьера в обучении языковых моделей', 'desc': 'Статья представляет новый подход к обучению языковых моделей под названием VocAgnoLM. Этот метод решает проблему несоответствия словарей между учительской и ученической моделями с помощью лексического выравнивания токенов и использования функции потерь учительской модели. VocAgnoLM показывает значительное улучшение производительности по сравнению с обычным дообучением, особенно когда словари моделей сильно различаются. Авторы демонстрируют, что их подход позволяет эффективно использовать более сильные учительские модели для обучения меньших ученических моделей.'}, 'en': {'title': 'Bridging Vocabulary Gaps for Better Language Learning', 'desc': "This paper introduces a new method called Vocabulary-agnostic Teacher Guided Language Modeling (VocAgnoLM) to improve the training of smaller language models using larger teacher models. It addresses the problem of vocabulary mismatches that can lead to different token sequences and output distributions between the teacher and student models. VocAgnoLM employs two main techniques: Token-level Lexical Alignment to synchronize token sequences and Teacher Guided Loss to utilize the teacher's loss for better student training. The results show that this approach significantly enhances performance, achieving a 46% improvement in language modeling tasks, especially when using stronger teacher models."}, 'zh': {'title': '打破词汇壁垒，提升语言建模效率', 'desc': '本文提出了一种新的语言建模方法，称为Vocabulary-agnostic Teacher Guided Language Modeling（VocAgnoLM），旨在解决教师模型与学生模型之间的词汇不匹配问题。该方法通过两种关键技术实现：一是令牌级词汇对齐，二是教师引导损失，帮助学生模型更有效地学习。实验表明，VocAgnoLM在使用不同词汇的教师模型时，能够显著提高学生模型的性能，尤其是在词汇重叠较少的情况下。该方法为语言建模中的词汇不匹配问题提供了有效的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2503.18783', 'title': 'Frequency Dynamic Convolution for Dense Image Prediction', 'url': 'https://huggingface.co/papers/2503.18783', 'abstract': '', 'score': 2, 'issue_id': 2897, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '0be08263b46c092e', 'authors': ['Linwei Chen', 'Lin Gu', 'Liang Li', 'Chenggang Yan', 'Ying Fu'], 'affiliations': ['Beijing Institute of Technology', 'Chinese Academy of Sciences', 'Hangzhou Dianzi University', 'RIKEN', 'The University of Tokyo', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18783.jpg', 'data': {'categories': [], 'emoji': '🤖', 'ru': {'title': 'Эффективное обучение больших языковых моделей', 'desc': 'В статье рассматривается новый подход к обучению больших языковых моделей (LLM), который позволяет значительно сократить время и ресурсы, необходимые для их тренировки. Авторы предлагают использовать метод оптимизации, который адаптируется к особенностям данных, что повышает эффективность обучения. Эксперименты показывают, что предложенный метод позволяет достичь более высоких результатов на стандартных тестах. Это открывает новые возможности для применения LLM в различных областях, таких как обработка естественного языка и генерация текста.'}, 'en': {'title': 'Hybrid Models: Bridging Spatial and Temporal Learning', 'desc': "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time-series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."}, 'zh': {'title': '提升预测准确性的创新算法', 'desc': '这篇论文探讨了一种新的机器学习算法，旨在提高模型的预测准确性。作者提出了一种改进的特征选择方法，可以有效减少数据维度，同时保留重要信息。实验结果表明，该算法在多个数据集上表现优于传统方法。通过优化模型的训练过程，研究者希望推动机器学习在实际应用中的效果。'}}}, {'id': 'https://huggingface.co/papers/2503.17973', 'title': 'PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable\n  Objects from Videos', 'url': 'https://huggingface.co/papers/2503.17973', 'abstract': 'Creating a physical digital twin of a real-world object has immense potential in robotics, content creation, and XR. In this paper, we present PhysTwin, a novel framework that uses sparse videos of dynamic objects under interaction to produce a photo- and physically realistic, real-time interactive virtual replica. Our approach centers on two key components: (1) a physics-informed representation that combines spring-mass models for realistic physical simulation, generative shape models for geometry, and Gaussian splats for rendering; and (2) a novel multi-stage, optimization-based inverse modeling framework that reconstructs complete geometry, infers dense physical properties, and replicates realistic appearance from videos. Our method integrates an inverse physics framework with visual perception cues, enabling high-fidelity reconstruction even from partial, occluded, and limited viewpoints. PhysTwin supports modeling various deformable objects, including ropes, stuffed animals, cloth, and delivery packages. Experiments show that PhysTwin outperforms competing methods in reconstruction, rendering, future prediction, and simulation under novel interactions. We further demonstrate its applications in interactive real-time simulation and model-based robotic motion planning.', 'score': 2, 'issue_id': 2910, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 марта', 'en': 'March 23', 'zh': '3月23日'}, 'hash': '91f6aafdb1c387f0', 'authors': ['Hanxiao Jiang', 'Hao-Yu Hsu', 'Kaifeng Zhang', 'Hsin-Ni Yu', 'Shenlong Wang', 'Yunzhu Li'], 'affiliations': ['Columbia University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2503.17973.jpg', 'data': {'categories': ['#optimization', '#3d', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'PhysTwin: Создание интерактивных цифровых двойников из видео', 'desc': 'Статья представляет PhysTwin - новую систему для создания фото- и физически реалистичных цифровых двойников объектов на основе видео. Ключевые компоненты включают физически обоснованное представление, сочетающее модели пружинных масс, генеративные модели форм и гауссовы сплаты. Применяется многоэтапная оптимизационная система обратного моделирования для реконструкции геометрии, физических свойств и внешнего вида. PhysTwin превосходит конкурирующие методы в реконструкции, рендеринге и симуляции взаимодействий с объектами.'}, 'en': {'title': 'Realistic Digital Twins: Bridging the Physical and Virtual Worlds', 'desc': 'This paper introduces PhysTwin, a framework designed to create realistic digital twins of dynamic objects using sparse video data. It employs a physics-informed representation that integrates spring-mass models for physical simulation, generative shape models for geometry, and Gaussian splats for rendering. The framework utilizes a multi-stage optimization process to reconstruct object geometry, infer physical properties, and achieve realistic appearances from limited video inputs. PhysTwin excels in modeling various deformable objects and demonstrates superior performance in reconstruction and simulation tasks, making it valuable for robotics and interactive applications.'}, 'zh': {'title': '物理数字双胞胎：真实与虚拟的完美结合', 'desc': '本文介绍了一种名为PhysTwin的新框架，旨在创建真实物体的数字双胞胎。该框架利用稀疏视频捕捉动态物体的交互，生成逼真且可实时互动的虚拟复制品。PhysTwin结合了物理信息表示和多阶段优化逆建模框架，能够从视频中重建完整几何形状、推断物理属性并复制真实外观。实验结果表明，PhysTwin在重建、渲染和模拟方面优于其他方法，具有广泛的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.17237', 'title': 'Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID', 'url': 'https://huggingface.co/papers/2503.17237', 'abstract': 'Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal infrared video is inherently challenging due to low contrast, environmental noise, and small target sizes. This paper provides a straightforward approach to address multi-UAV tracking in thermal infrared video, leveraging recent advances in detection and tracking. Instead of relying on the YOLOv5 with the DeepSORT pipeline, we present a tracking framework built on YOLOv12 and BoT-SORT, enhanced with tailored training and inference strategies. We evaluate our approach following the metrics from the 4th Anti-UAV Challenge and demonstrate competitive performance. Notably, we achieve strong results without using contrast enhancement or temporal information fusion to enrich UAV features, highlighting our approach as a "Strong Baseline" for the multi-UAV tracking task. We provide implementation details, in-depth experimental analysis, and a discussion of potential improvements. The code is available at https://github.com/wish44165/YOLOv12-BoT-SORT-ReID .', 'score': 2, 'issue_id': 2900, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '03b184a7ba5377e7', 'authors': ['Yu-Hsi Chen'], 'affiliations': ['The University of Melbourne, Parkville, Australia'], 'pdf_title_img': 'assets/pdf/title_img/2503.17237.jpg', 'data': {'categories': ['#video', '#cv', '#training', '#benchmark'], 'emoji': '🚁', 'ru': {'title': 'Эффективное отслеживание БПЛА на тепловизионном видео без дополнительной обработки', 'desc': 'Статья представляет новый подход к отслеживанию множественных БПЛА на инфракрасном видео с использованием YOLOv12 и BoT-SORT. Авторы разработали фреймворк, который превосходит стандартную связку YOLOv5 и DeepSORT, не прибегая к улучшению контраста или слиянию временной информации. Метод показал конкурентоспособные результаты на метриках 4-го Anti-UAV Challenge. Исследователи предоставляют подробности реализации, экспериментальный анализ и обсуждение возможных улучшений.'}, 'en': {'title': 'Revolutionizing UAV Tracking with YOLOv12 and BoT-SORT', 'desc': 'This paper addresses the challenge of detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal infrared video, which is difficult due to low visibility and small sizes of the targets. The authors propose a new tracking framework that utilizes YOLOv12 and BoT-SORT, moving away from the traditional YOLOv5 and DeepSORT methods. Their approach includes specific training and inference strategies that enhance performance without relying on contrast enhancement or temporal information. The results show that their method performs competitively in the 4th Anti-UAV Challenge, establishing it as a strong baseline for future multi-UAV tracking research.'}, 'zh': {'title': '热红外视频中的多无人机跟踪新方法', 'desc': '本论文针对热红外视频中多无人机（UAV）的检测与跟踪问题，提出了一种简单有效的方法。我们采用了YOLOv12和BoT-SORT构建跟踪框架，并通过定制的训练和推理策略进行增强。我们的评估基于第四届反无人机挑战赛的指标，结果显示出竞争力的性能。值得注意的是，我们在不使用对比度增强或时间信息融合的情况下，依然取得了良好的结果，标志着我们的方法是多无人机跟踪任务的“强基线”。'}}}, {'id': 'https://huggingface.co/papers/2503.16965', 'title': 'When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only\n  Training For Human-Centered Decision Making', 'url': 'https://huggingface.co/papers/2503.16965', 'abstract': "Embodied decision-making is fundamental for AI agents operating in real-world environments. While Visual Language Models (VLMs) have advanced this capability, they still struggle with complex decisions, particularly in human-centered situations that require deep reasoning about human needs and values. In this study, we systematically evaluate open-sourced VLMs on multimodal human-centered decision-making tasks. We find that LLMs receiving only textual descriptions unexpectedly outperform their VLM counterparts of similar scale that process actual images, suggesting that visual alignment may hinder VLM abilities. To address this challenge, we propose a novel text-only training approach with synthesized textual data. This method strengthens VLMs' language components and transfers the learned abilities to multimodal inference, eliminating the need for expensive image-text paired data. Furthermore, we show that VLMs can achieve substantial performance gains through self-improvement, using training data generated by their LLM counterparts rather than relying on larger teacher models like GPT-4. Our findings establish a more efficient and scalable approach to enhancing VLMs' human-centered decision-making capabilities, opening new avenues for optimizing VLMs through self-improvement mechanisms.", 'score': 2, 'issue_id': 2896, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': 'ca2e599ff0665dfe', 'authors': ['Zhe Hu', 'Jing Li', 'Yu Yin'], 'affiliations': ['Department of Computer and Data Sciences, Case Western Reserve University', 'Department of Computing, The Hong Kong Polytechnic University', 'Research Centre for Data Science & Artificial Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2503.16965.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#training', '#agents', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Самосовершенствование VLM через текстовое обучение', 'desc': 'Это исследование сосредоточено на оценке визуальных языковых моделей (VLM) в задачах принятия решений, ориентированных на человека. Авторы обнаружили, что языковые модели, работающие только с текстом, превосходят VLM аналогичного масштаба, обрабатывающие изображения. Для решения этой проблемы предложен новый подход к обучению, использующий синтезированные текстовые данные. Исследование также демонстрирует, что VLM могут значительно улучшить свою производительность через самосовершенствование, используя данные, сгенерированные их LLM-аналогами.'}, 'en': {'title': 'Enhancing VLMs through Text-Only Training and Self-Improvement', 'desc': 'This paper explores the challenges faced by Visual Language Models (VLMs) in making complex decisions that involve understanding human needs and values. The authors find that VLMs that rely on visual data often perform worse than those using only text, indicating that visual information may complicate decision-making. To improve VLM performance, they propose a new training method that uses synthesized textual data, enhancing the language understanding of VLMs without needing paired image-text data. Additionally, the study demonstrates that VLMs can improve their decision-making abilities by learning from their own generated data, rather than depending on larger models, making the training process more efficient and scalable.'}, 'zh': {'title': '提升VLM人类中心决策能力的新方法', 'desc': '本研究探讨了视觉语言模型（VLMs）在复杂人类中心决策中的表现。我们发现，仅使用文本描述的语言模型（LLMs）在某些任务上意外地超越了处理图像的VLMs，这表明视觉对齐可能会限制VLM的能力。为了解决这个问题，我们提出了一种新的仅基于文本的训练方法，利用合成文本数据来增强VLM的语言能力。我们的研究结果表明，通过自我改进，VLMs可以显著提升其人类中心决策能力，开辟了优化VLM的新途径。'}}}, {'id': 'https://huggingface.co/papers/2503.19777', 'title': 'LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary\n  Semantic Segmentation', 'url': 'https://huggingface.co/papers/2503.19777', 'abstract': 'We propose a training-free method for open-vocabulary semantic segmentation using Vision-and-Language Models (VLMs). Our approach enhances the initial per-patch predictions of VLMs through label propagation, which jointly optimizes predictions by incorporating patch-to-patch relationships. Since VLMs are primarily optimized for cross-modal alignment and not for intra-modal similarity, we use a Vision Model (VM) that is observed to better capture these relationships. We address resolution limitations inherent to patch-based encoders by applying label propagation at the pixel level as a refinement step, significantly improving segmentation accuracy near class boundaries. Our method, called LPOSS+, performs inference over the entire image, avoiding window-based processing and thereby capturing contextual interactions across the full image. LPOSS+ achieves state-of-the-art performance among training-free methods, across a diverse set of datasets. Code: https://github.com/vladan-stojnic/LPOSS', 'score': 1, 'issue_id': 2905, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'adee7b684f0c25f1', 'authors': ['Vladan Stojnić', 'Yannis Kalantidis', 'Jiří Matas', 'Giorgos Tolias'], 'affiliations': ['NAVER LABS Europe', 'VRG, FEE, Czech Technical University in Prague'], 'pdf_title_img': 'assets/pdf/title_img/2503.19777.jpg', 'data': {'categories': ['#open_source', '#inference', '#cv', '#training'], 'emoji': '🖼️', 'ru': {'title': 'Улучшение сегментации изображений без обучения с помощью распространения меток', 'desc': 'В статье предлагается метод семантической сегментации изображений с открытым словарем без обучения, использующий модели зрения и языка (VLM). Метод улучшает начальные попиксельные предсказания VLM с помощью распространения меток, оптимизируя предсказания с учетом отношений между патчами. Для захвата внутримодальных сходств используется отдельная модель зрения (VM). Применение распространения меток на уровне пикселей позволяет преодолеть ограничения разрешения, присущие кодировщикам на основе патчей.'}, 'en': {'title': 'Revolutionizing Semantic Segmentation with Training-Free Label Propagation', 'desc': 'This paper introduces a novel method for open-vocabulary semantic segmentation that does not require training. The method leverages Vision-and-Language Models (VLMs) and enhances their predictions by using label propagation to optimize relationships between image patches. To improve accuracy, especially near class boundaries, the authors apply label propagation at the pixel level, addressing the limitations of patch-based encoders. The proposed method, LPOSS+, outperforms existing training-free approaches and effectively captures contextual interactions across the entire image.'}, 'zh': {'title': '无训练的开放词汇语义分割新方法', 'desc': '我们提出了一种无训练的开放词汇语义分割方法，利用视觉与语言模型（VLMs）。该方法通过标签传播增强了VLMs的初始每个补丁预测，优化了补丁之间的关系。我们使用视觉模型（VM）来更好地捕捉这些关系，并在像素级别应用标签传播，以解决基于补丁编码器的分辨率限制，从而显著提高了类边界附近的分割精度。我们的LPOSS+方法在整个图像上进行推理，避免了基于窗口的处理，能够捕捉全图的上下文交互，并在无训练方法中实现了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.19356', 'title': 'Can Vision-Language Models Answer Face to Face Questions in the\n  Real-World?', 'url': 'https://huggingface.co/papers/2503.19356', 'abstract': 'AI models have made significant strides in recent years in their ability to describe and answer questions about real-world images. They have also made progress in the ability to converse with users in real-time using audio input. This raises the question: have we reached the point where AI models, connected to a camera and microphone, can converse with users in real-time about scenes and events that are unfolding live in front of the camera? This has been a long-standing goal in AI and is a prerequisite for real-world AI assistants and humanoid robots to interact with humans in everyday situations. In this work, we introduce a new dataset and benchmark, the Qualcomm Interactive Video Dataset (IVD), which allows us to assess the extent to which existing models can support these abilities, and to what degree these capabilities can be instilled through fine-tuning. The dataset is based on a simple question-answering setup, where users ask questions that the system has to answer, in real-time, based on the camera and audio input. We show that existing models fall far behind human performance on this task, and we identify the main sources for the performance gap. However, we also show that for many of the required perceptual skills, fine-tuning on this form of data can significantly reduce this gap.', 'score': 1, 'issue_id': 2909, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '139d332c36986584', 'authors': ['Reza Pourreza', 'Rishit Dagli', 'Apratim Bhattacharyya', 'Sunny Panchal', 'Guillaume Berger', 'Roland Memisevic'], 'affiliations': ['Qualcomm AI Research', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2503.19356.jpg', 'data': {'categories': ['#benchmark', '#agi', '#multimodal', '#optimization', '#cv', '#dataset', '#training', '#audio', '#interpretability'], 'emoji': '🤖', 'ru': {'title': 'На пути к ИИ, понимающему мир в реальном времени', 'desc': 'Статья представляет новый набор данных и эталонный тест - Qualcomm Interactive Video Dataset (IVD), для оценки способности моделей искусственного интеллекта (ИИ) взаимодействовать с пользователями в реальном времени на основе видео- и аудиовхода. Исследование показывает, что существующие модели значительно отстают от человеческой производительности в этой задаче. Авторы выявляют основные причины разрыва в производительности между ИИ и человеком. Тем не менее, исследование демонстрирует, что дообучение на подобных данных может существенно сократить этот разрыв для многих необходимых перцептивных навыков.'}, 'en': {'title': 'Bridging the Gap: Real-Time AI Conversations with Live Input', 'desc': 'This paper discusses advancements in AI models that can describe and answer questions about real-world images and engage in real-time conversations using audio. The authors introduce the Qualcomm Interactive Video Dataset (IVD), which serves as a benchmark to evaluate how well current models can interact with users based on live camera and audio input. The study reveals that while existing models do not yet match human performance in this interactive task, fine-tuning these models on the new dataset can improve their perceptual skills significantly. Ultimately, this work highlights the potential for developing AI assistants and humanoid robots that can effectively communicate in everyday situations.'}, 'zh': {'title': '迈向实时对话的人工智能新阶段', 'desc': '近年来，人工智能模型在描述和回答现实世界图像方面取得了显著进展。它们在实时与用户进行音频对话的能力上也有所提升。本文提出了一个新的数据集和基准，名为高通互动视频数据集（IVD），用于评估现有模型在实时场景和事件对话中的能力。研究表明，尽管现有模型在此任务上的表现远低于人类，但通过微调可以显著缩小这一差距。'}}}, {'id': 'https://huggingface.co/papers/2503.19207', 'title': 'FRESA:Feedforward Reconstruction of Personalized Skinned Avatars from\n  Few Images', 'url': 'https://huggingface.co/papers/2503.19207', 'abstract': 'We present a novel method for reconstructing personalized 3D human avatars with realistic animation from only a few images. Due to the large variations in body shapes, poses, and cloth types, existing methods mostly require hours of per-subject optimization during inference, which limits their practical applications. In contrast, we learn a universal prior from over a thousand clothed humans to achieve instant feedforward generation and zero-shot generalization. Specifically, instead of rigging the avatar with shared skinning weights, we jointly infer personalized avatar shape, skinning weights, and pose-dependent deformations, which effectively improves overall geometric fidelity and reduces deformation artifacts. Moreover, to normalize pose variations and resolve coupled ambiguity between canonical shapes and skinning weights, we design a 3D canonicalization process to produce pixel-aligned initial conditions, which helps to reconstruct fine-grained geometric details. We then propose a multi-frame feature aggregation to robustly reduce artifacts introduced in canonicalization and fuse a plausible avatar preserving person-specific identities. Finally, we train the model in an end-to-end framework on a large-scale capture dataset, which contains diverse human subjects paired with high-quality 3D scans. Extensive experiments show that our method generates more authentic reconstruction and animation than state-of-the-arts, and can be directly generalized to inputs from casually taken phone photos. Project page and code is available at https://github.com/rongakowang/FRESA.', 'score': 1, 'issue_id': 2907, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '051b6e95d11816f7', 'authors': ['Rong Wang', 'Fabian Prada', 'Ziyan Wang', 'Zhongshi Jiang', 'Chengxiang Yin', 'Junxuan Li', 'Shunsuke Saito', 'Igor Santesteban', 'Javier Romero', 'Rohan Joshi', 'Hongdong Li', 'Jason Saragih', 'Yaser Sheikh'], 'affiliations': ['Australian National University', 'Meta Reality Labs Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.19207.jpg', 'data': {'categories': ['#3d'], 'emoji': '🧍', 'ru': {'title': 'Мгновенное создание реалистичных 3D-аватаров по нескольким фотографиям', 'desc': 'В статье представлен новый метод создания персонализированных 3D-аватаров людей с реалистичной анимацией на основе всего нескольких изображений. Авторы обучили универсальную модель на тысяче примеров одетых людей, что позволяет мгновенно генерировать аватары без дополнительной оптимизации. Метод совместно оценивает форму аватара, веса скиннинга и зависящие от позы деформации, что улучшает геометрическую точность. Предложенный процесс 3D-канонизации и агрегации признаков помогает восстанавливать мелкие детали и сохранять индивидуальность человека.'}, 'en': {'title': 'Instant 3D Avatars from Few Images!', 'desc': "This paper introduces a new technique for creating personalized 3D human avatars with realistic animations using just a few images. Unlike previous methods that require extensive optimization for each individual, this approach leverages a universal model learned from a large dataset of clothed humans, allowing for quick and efficient avatar generation. The method improves the accuracy of the avatar's shape and movement by jointly inferring skinning weights and pose-dependent deformations, which minimizes visual artifacts. Additionally, a 3D canonicalization process is implemented to align poses and enhance geometric details, resulting in high-quality reconstructions that can be generated from casual photos."}, 'zh': {'title': '个性化3D头像重建的新方法', 'desc': '我们提出了一种新方法，可以仅通过几张图片重建个性化的3D人类头像，并实现逼真的动画。现有方法通常需要在推理过程中对每个对象进行数小时的优化，这限制了它们的实际应用。我们的技术通过学习来自一千多名穿衣人类的通用先验，实现了即时前馈生成和零样本泛化。我们设计了一种3D标准化过程，以解决姿势变化和形状之间的模糊性，从而提高几何精度并减少变形伪影。'}}}, {'id': 'https://huggingface.co/papers/2503.19065', 'title': 'WikiAutoGen: Towards Multi-Modal Wikipedia-Style Article Generation', 'url': 'https://huggingface.co/papers/2503.19065', 'abstract': 'Knowledge discovery and collection are intelligence-intensive tasks that traditionally require significant human effort to ensure high-quality outputs. Recent research has explored multi-agent frameworks for automating Wikipedia-style article generation by retrieving and synthesizing information from the internet. However, these methods primarily focus on text-only generation, overlooking the importance of multimodal content in enhancing informativeness and engagement. In this work, we introduce WikiAutoGen, a novel system for automated multimodal Wikipedia-style article generation. Unlike prior approaches, WikiAutoGen retrieves and integrates relevant images alongside text, enriching both the depth and visual appeal of generated content. To further improve factual accuracy and comprehensiveness, we propose a multi-perspective self-reflection mechanism, which critically assesses retrieved content from diverse viewpoints to enhance reliability, breadth, and coherence, etc. Additionally, we introduce WikiSeek, a benchmark comprising Wikipedia articles with topics paired with both textual and image-based representations, designed to evaluate multimodal knowledge generation on more challenging topics. Experimental results show that WikiAutoGen outperforms previous methods by 8%-29% on our WikiSeek benchmark, producing more accurate, coherent, and visually enriched Wikipedia-style articles. We show some of our generated examples in https://wikiautogen.github.io/ .', 'score': 1, 'issue_id': 2908, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'aaba6b8dd7c54bf2', 'authors': ['Zhongyu Yang', 'Jun Chen', 'Dannong Xu', 'Junjie Fei', 'Xiaoqian Shen', 'Liangbing Zhao', 'Chun-Mei Feng', 'Mohamed Elhoseiny'], 'affiliations': ['IHPC, A*STAR', 'King Abdullah University of Science and Technology', 'Lanzhou University', 'The University of Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2503.19065.jpg', 'data': {'categories': ['#story_generation', '#benchmark', '#interpretability', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'WikiAutoGen: ИИ создает мультимодальные статьи Википедии', 'desc': 'Статья представляет WikiAutoGen - новую систему для автоматизированного создания мультимодальных статей в стиле Википедии. В отличие от предыдущих подходов, WikiAutoGen извлекает и интегрирует релевантные изображения вместе с текстом, обогащая глубину и визуальную привлекательность генерируемого контента. Система использует механизм многоперспективной саморефлексии для улучшения фактической точности и полноты. Авторы также представляют WikiSeek - эталонный набор данных для оценки мультимодальной генерации знаний по более сложным темам.'}, 'en': {'title': 'Enhancing Wikipedia Articles with Multimodal Automation', 'desc': 'This paper presents WikiAutoGen, a system designed to automate the generation of Wikipedia-style articles using both text and images. Unlike previous methods that focused solely on text, WikiAutoGen enhances the informativeness and engagement of articles by integrating relevant images. The system employs a multi-perspective self-reflection mechanism to ensure the accuracy and coherence of the content by evaluating it from various viewpoints. Experimental results demonstrate that WikiAutoGen significantly outperforms existing approaches, achieving higher quality in multimodal article generation.'}, 'zh': {'title': '自动生成多模态维基百科文章的新方法', 'desc': '本研究提出了一种新的系统WikiAutoGen，用于自动生成多模态的维基百科风格文章。与以往仅关注文本生成的方法不同，WikiAutoGen同时检索和整合相关图像，增强了生成内容的深度和视觉吸引力。为了提高事实准确性和全面性，我们引入了一种多角度自我反思机制，从不同视角评估检索内容，以增强可靠性和连贯性。此外，我们还推出了WikiSeek基准，旨在评估在更具挑战性主题上的多模态知识生成。'}}}, {'id': 'https://huggingface.co/papers/2503.18893', 'title': 'xKV: Cross-Layer SVD for KV-Cache Compression', 'url': 'https://huggingface.co/papers/2503.18893', 'abstract': "Large Language Models (LLMs) with long context windows enable powerful applications but come at the cost of high memory consumption to store the Key and Value states (KV-Cache). Recent studies attempted to merge KV-cache from multiple layers into shared representations, yet these approaches either require expensive pretraining or rely on assumptions of high per-token cosine similarity across layers which generally does not hold in practice. We find that the dominant singular vectors are remarkably well-aligned across multiple layers of the KV-Cache. Exploiting this insight, we propose xKV, a simple post-training method that applies Singular Value Decomposition (SVD) on the KV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers into a shared low-rank subspace, significantly reducing KV-Cache sizes. Through extensive evaluations on the RULER long-context benchmark with widely-used LLMs (e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates than state-of-the-art inter-layer technique while improving accuracy by 2.7%. Moreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA) (e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding tasks without performance degradation. These results highlight xKV's strong capability and versatility in addressing memory bottlenecks for long-context LLM inference. Our code is publicly available at: https://github.com/abdelfattah-lab/xKV.", 'score': 1, 'issue_id': 2909, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'b933f4d58f9fb03a', 'authors': ['Chi-Chih Chang', 'Chien-Yu Lin', 'Yash Akhauri', 'Wei-Cheng Lin', 'Kai-Chiang Wu', 'Luis Ceze', 'Mohamed S. Abdelfattah'], 'affiliations': ['Cornell University', 'National Yang Ming Chiao Tung University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2503.18893.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#open_source', '#long_context', '#architecture', '#inference'], 'emoji': '🗜️', 'ru': {'title': 'xKV: Эффективное сжатие памяти для LLM с длинным контекстом', 'desc': 'Исследователи предложили метод xKV для сжатия памяти KV-Cache в больших языковых моделях (LLM) с длинным контекстным окном. Метод использует сингулярное разложение (SVD) для объединения KV-Cache нескольких слоев в общее низкоранговое подпространство. xKV достигает до 6.8-кратного сжатия по сравнению с современными методами, при этом улучшая точность на 2.7% на бенчмарке RULER. Метод также совместим с Multi-Head Latent Attention (MLA) и показывает 3-кратное сжатие на задачах кодирования без потери производительности.'}, 'en': {'title': 'xKV: Efficient Memory Management for Long-Context LLMs', 'desc': 'This paper introduces xKV, a method designed to reduce the memory usage of Key and Value states (KV-Cache) in Large Language Models (LLMs) with long context windows. By applying Singular Value Decomposition (SVD) to the KV-Cache of grouped layers, xKV consolidates these caches into a shared low-rank subspace, leading to significant size reductions. The method achieves up to 6.8 times higher compression rates compared to existing techniques while also improving model accuracy by 2.7%. Additionally, xKV is compatible with Multi-Head Latent Attention models, demonstrating its effectiveness in various coding tasks without sacrificing performance.'}, 'zh': {'title': 'xKV：高效压缩长上下文模型的关键技术', 'desc': '大型语言模型（LLMs）在处理长上下文时具有强大的应用能力，但需要消耗大量内存来存储键值缓存（KV-Cache）。最近的研究尝试将多个层的KV缓存合并为共享表示，但这些方法通常需要昂贵的预训练或依赖于层间高余弦相似性的假设，这在实际中并不成立。我们发现，KV缓存的主奇异向量在多个层之间对齐得非常好。基于这一发现，我们提出了xKV，这是一种简单的后训练方法，通过对分组层的KV缓存应用奇异值分解（SVD），显著减少了KV缓存的大小。'}}}, {'id': 'https://huggingface.co/papers/2503.17361', 'title': 'Gumbel-Softmax Flow Matching with Straight-Through Guidance for\n  Controllable Biological Sequence Generation', 'url': 'https://huggingface.co/papers/2503.17361', 'abstract': 'Flow matching in the continuous simplex has emerged as a promising strategy for DNA sequence design, but struggles to scale to higher simplex dimensions required for peptide and protein generation. We introduce Gumbel-Softmax Flow and Score Matching, a generative framework on the simplex based on a novel Gumbel-Softmax interpolant with a time-dependent temperature. Using this interpolant, we introduce Gumbel-Softmax Flow Matching by deriving a parameterized velocity field that transports from smooth categorical distributions to distributions concentrated at a single vertex of the simplex. We alternatively present Gumbel-Softmax Score Matching which learns to regress the gradient of the probability density. Our framework enables high-quality, diverse generation and scales efficiently to higher-dimensional simplices. To enable training-free guidance, we propose Straight-Through Guided Flows (STGFlow), a classifier-based guidance method that leverages straight-through estimators to steer the unconditional velocity field toward optimal vertices of the simplex. STGFlow enables efficient inference-time guidance using classifiers pre-trained on clean sequences, and can be used with any discrete flow method. Together, these components form a robust framework for controllable de novo sequence generation. We demonstrate state-of-the-art performance in conditional DNA promoter design, sequence-only protein generation, and target-binding peptide design for rare disease treatment.', 'score': 1, 'issue_id': 2896, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': 'b5389a3e5ab241c3', 'authors': ['Sophia Tang', 'Yinuo Zhang', 'Alexander Tong', 'Pranam Chatterjee'], 'affiliations': ['Center of Computational Biology, Duke-NUS Medical School', 'Department of Biomedical Engineering, Duke University', 'Department of Biostatistics and Bioinformatics, Duke University', 'Department of Computer Science, Duke University', 'Management and Technology Program, University of Pennsylvania', 'Mila, Quebec AI Institute', 'Université de Montréal'], 'pdf_title_img': 'assets/pdf/title_img/2503.17361.jpg', 'data': {'categories': ['#diffusion', '#data', '#optimization', '#training', '#architecture', '#healthcare'], 'emoji': '🧬', 'ru': {'title': 'Новый подход к генерации биологических последовательностей на симплексе', 'desc': 'Статья представляет новый метод генеративного моделирования на симплексе, называемый Gumbel-Softmax Flow and Score Matching. Авторы вводят новый интерполянт Gumbel-Softmax с зависящей от времени температурой и используют его для создания параметризованного поля скоростей. Метод позволяет получать высококачественные и разнообразные результаты, эффективно масштабируясь на симплексы высокой размерности. Также предложен метод Straight-Through Guided Flows для управления генерацией без дополнительного обучения.'}, 'en': {'title': 'Revolutionizing Sequence Generation with Gumbel-Softmax Flows', 'desc': 'This paper presents a new method called Gumbel-Softmax Flow and Score Matching for generating DNA sequences, peptides, and proteins. It introduces a Gumbel-Softmax interpolant that helps in transitioning between smooth categorical distributions and specific points in a simplex, which is a mathematical space used for these types of data. The authors also propose a technique called Straight-Through Guided Flows (STGFlow) that uses classifiers to guide the generation process towards optimal outcomes without needing extensive training. Overall, this framework allows for efficient and high-quality generation of biological sequences, achieving impressive results in various applications.'}, 'zh': {'title': '高效生成高维序列的创新框架', 'desc': '本文提出了一种新的生成框架，称为Gumbel-Softmax流和评分匹配，旨在解决DNA序列设计中的高维简单形问题。通过引入时间依赖的Gumbel-Softmax插值，我们能够在简单形上实现高质量和多样化的生成。该框架还包括一种名为STGFlow的分类器引导方法，能够在推理时有效地引导生成过程。我们的研究在条件DNA启动子设计、序列生成的蛋白质和靶向结合肽的设计中展示了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.11849', 'title': 'Towards a Unified Copernicus Foundation Model for Earth Vision', 'url': 'https://huggingface.co/papers/2503.11849', 'abstract': "Advances in Earth observation (EO) foundation models have unlocked the potential of big satellite data to learn generic representations from space, benefiting a wide range of downstream applications crucial to our planet. However, most existing efforts remain limited to fixed spectral sensors, focus solely on the Earth's surface, and overlook valuable metadata beyond imagery. In this work, we take a step towards next-generation EO foundation models with three key components: 1) Copernicus-Pretrain, a massive-scale pretraining dataset that integrates 18.7M aligned images from all major Copernicus Sentinel missions, spanning from the Earth's surface to its atmosphere; 2) Copernicus-FM, a unified foundation model capable of processing any spectral or non-spectral sensor modality using extended dynamic hypernetworks and flexible metadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark with 15 hierarchical downstream tasks ranging from preprocessing to specialized applications for each Sentinel mission. Our dataset, model, and benchmark greatly improve the scalability, versatility, and multimodal adaptability of EO foundation models, while also creating new opportunities to connect EO, weather, and climate research. Codes, datasets and models are available at https://github.com/zhu-xlab/Copernicus-FM.", 'score': 1, 'issue_id': 2904, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '73a149c0c20956cf', 'authors': ['Yi Wang', 'Zhitong Xiong', 'Chenying Liu', 'Adam J. Stewart', 'Thomas Dujardin', 'Nikolaos Ioannis Bountos', 'Angelos Zavras', 'Franziska Gerken', 'Ioannis Papoutsis', 'Laura Leal-Taixé', 'Xiao Xiang Zhu'], 'affiliations': ['Harokopio University of Athens', 'Munich Center for Machine Learning', 'NVIDIA', 'National Technical University of Athens & National Observatory of Athens', 'Technical University of Munich'], 'pdf_title_img': 'assets/pdf/title_img/2503.11849.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#cv', '#dataset'], 'emoji': '🛰️', 'ru': {'title': 'Универсальная модель машинного обучения для комплексного анализа спутниковых данных', 'desc': 'Статья представляет новый подход к созданию фундаментальных моделей для наблюдения Земли. Авторы разработали Copernicus-FM - унифицированную модель, способную обрабатывать различные сенсорные модальности с использованием расширенных динамических гиперсетей. Для обучения модели был создан массивный набор данных Copernicus-Pretrain, включающий 18,7 миллионов изображений со спутников миссии Copernicus Sentinel. Также представлен бенчмарк Copernicus-Bench для оценки модели на 15 иерархических задачах.'}, 'en': {'title': 'Unlocking Earth Observation with Advanced Foundation Models', 'desc': "This paper presents advancements in Earth observation (EO) foundation models that utilize large satellite datasets to enhance learning from space imagery. The authors introduce Copernicus-Pretrain, a comprehensive dataset with 18.7 million aligned images from various Copernicus Sentinel missions, which includes data from both the Earth's surface and atmosphere. They also propose Copernicus-FM, a versatile foundation model that can handle different types of sensor data and incorporates metadata for improved analysis. Finally, the paper outlines Copernicus-Bench, a benchmark for evaluating the model's performance across 15 diverse tasks, thereby enhancing the scalability and adaptability of EO applications."}, 'zh': {'title': '地球观测基础模型的未来：多模态与可扩展性', 'desc': '本论文介绍了地球观测基础模型的进展，利用大规模卫星数据学习通用表示，促进了多种重要应用的发展。我们提出了三个关键组件：Copernicus-Pretrain，一个包含1870万对齐图像的大规模预训练数据集，涵盖地球表面到大气层的所有主要Copernicus Sentinel任务；Copernicus-FM，一个统一的基础模型，能够处理任何光谱或非光谱传感器的模态，并使用扩展的动态超网络和灵活的元数据编码；以及Copernicus-Bench，一个系统的评估基准，包含15个层次的下游任务，从预处理到每个Sentinel任务的专业应用。我们的工作显著提高了地球观测基础模型的可扩展性、多功能性和多模态适应性，同时为连接地球观测、天气和气候研究创造了新的机会。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (2)', '#agi (1)', '#alignment (2)', '#architecture (2)', '#audio (1)', '#benchmark (13)', '#cv (8)', '#data (1)', '#dataset (4)', '#diffusion (3)', '#ethics', '#games (1)', '#graphs', '#hallucinations (1)', '#healthcare (1)', '#inference (3)', '#interpretability (3)', '#leakage', '#long_context (4)', '#low_resource (1)', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (11)', '#open_source (4)', '#optimization (14)', '#plp', '#rag (2)', '#reasoning (4)', '#rl (1)', '#rlhf', '#robotics (1)', '#science', '#security', '#small_models (1)', '#story_generation (1)', '#survey', '#synthetic (2)', '#training (13)', '#transfer_learning (2)', '#video (5)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-03-26 16:14',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-26 16:14')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-26 16:14')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    