
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 26 papers. March 17.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">17 марта</span> | <span id="title-articles-count">26 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-03-14.html">⬅️ <span id="prev-date">14.03</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-03-18.html">➡️ <span id="next-date">18.03</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-03.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'};
        let feedDateNext = {'ru': '18.03', 'en': '03/18', 'zh': '3月18日'};
        let feedDatePrev = {'ru': '14.03', 'en': '03/14', 'zh': '3月14日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.11647', 'title': 'ReCamMaster: Camera-Controlled Generative Rendering from A Single Video', 'url': 'https://huggingface.co/papers/2503.11647', 'abstract': 'Camera control has been actively studied in text or image conditioned video generation tasks. However, altering camera trajectories of a given video remains under-explored, despite its importance in the field of video creation. It is non-trivial due to the extra constraints of maintaining multiple-frame appearance and dynamic synchronization. To address this, we present ReCamMaster, a camera-controlled generative video re-rendering framework that reproduces the dynamic scene of an input video at novel camera trajectories. The core innovation lies in harnessing the generative capabilities of pre-trained text-to-video models through a simple yet powerful video conditioning mechanism -- its capability often overlooked in current research. To overcome the scarcity of qualified training data, we construct a comprehensive multi-camera synchronized video dataset using Unreal Engine 5, which is carefully curated to follow real-world filming characteristics, covering diverse scenes and camera movements. It helps the model generalize to in-the-wild videos. Lastly, we further improve the robustness to diverse inputs through a meticulously designed training strategy. Extensive experiments tell that our method substantially outperforms existing state-of-the-art approaches and strong baselines. Our method also finds promising applications in video stabilization, super-resolution, and outpainting. Project page: https://jianhongbai.github.io/ReCamMaster/', 'score': 76, 'issue_id': 2730, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '7e72838ea84ed904', 'authors': ['Jianhong Bai', 'Menghan Xia', 'Xiao Fu', 'Xintao Wang', 'Lianrui Mu', 'Jinwen Cao', 'Zuozhu Liu', 'Haoji Hu', 'Xiang Bai', 'Pengfei Wan', 'Di Zhang'], 'affiliations': ['CUHK', 'HUST', 'Kuaishou Technology', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.11647.jpg', 'data': {'categories': ['#dataset', '#optimization', '#training', '#video', '#games'], 'emoji': '🎥', 'ru': {'title': 'Управление камерой в видео с помощью генеративных моделей', 'desc': 'ReCamMaster - это генеративная система для изменения траектории камеры в видео. Она использует предобученные модели text-to-video и механизм видео-кондиционирования для воспроизведения динамической сцены с новых ракурсов. Для обучения был создан специальный датасет синхронизированных мультикамерных видео в Unreal Engine 5. Система превосходит существующие подходы и находит применение в стабилизации, суперразрешении и аутпейнтинге видео.'}, 'en': {'title': 'ReCamMaster: Mastering Camera Control in Video Generation', 'desc': 'This paper introduces ReCamMaster, a novel framework for generating videos with controlled camera trajectories. It addresses the challenge of maintaining visual consistency and dynamic synchronization across multiple frames when altering camera paths. The framework leverages pre-trained text-to-video models and a specially curated multi-camera synchronized video dataset to enhance its performance. Experimental results demonstrate that ReCamMaster significantly outperforms existing methods, showcasing its potential for applications like video stabilization and super-resolution.'}, 'zh': {'title': '重塑视频动态，掌控相机轨迹', 'desc': '本论文研究了在文本或图像条件下生成视频时的相机控制问题。尽管改变视频的相机轨迹很重要，但这一领域的研究仍然较少。我们提出了ReCamMaster，一个基于生成模型的视频重渲染框架，能够在新的相机轨迹下重现输入视频的动态场景。通过构建一个多相机同步视频数据集，并采用精心设计的训练策略，我们的方法在多种输入下表现出色，超越了现有的最先进技术。'}}}, {'id': 'https://huggingface.co/papers/2503.07677', 'title': 'PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference\n  Time by Leveraging Sparsity', 'url': 'https://huggingface.co/papers/2503.07677', 'abstract': 'Diffusion models have shown impressive results in generating high-quality conditional samples using guidance techniques such as Classifier-Free Guidance (CFG). However, existing methods often require additional training or neural function evaluations (NFEs), making them incompatible with guidance-distilled models. Also, they rely on heuristic approaches that need identifying target layers. In this work, we propose a novel and efficient method, termed PLADIS, which boosts pre-trained models (U-Net/Transformer) by leveraging sparse attention. Specifically, we extrapolate query-key correlations using softmax and its sparse counterpart in the cross-attention layer during inference, without requiring extra training or NFEs. By leveraging the noise robustness of sparse attention, our PLADIS unleashes the latent potential of text-to-image diffusion models, enabling them to excel in areas where they once struggled with newfound effectiveness. It integrates seamlessly with guidance techniques, including guidance-distilled models. Extensive experiments show notable improvements in text alignment and human preference, offering a highly efficient and universally applicable solution.', 'score': 64, 'issue_id': 2730, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '913b88ac595cc8b6', 'authors': ['Kwanyoung Kim', 'Byeongsu Sim'], 'affiliations': ['Samsung Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.07677.jpg', 'data': {'categories': ['#training', '#cv', '#diffusion', '#inference'], 'emoji': '🖼️', 'ru': {'title': 'PLADIS: Эффективное улучшение генерации изображений с помощью разреженного внимания', 'desc': 'Статья представляет новый метод под названием PLADIS для улучшения предобученных моделей диффузии в задаче генерации изображений по текстовому описанию. PLADIS использует разреженное внимание для экстраполяции корреляций запрос-ключ в слое кросс-внимания во время вывода, не требуя дополнительного обучения или оценок нейронных функций. Метод хорошо сочетается с существующими техниками направленной генерации, включая модели с дистиллированным направлением. Эксперименты показывают значительное улучшение соответствия текста и изображения, а также предпочтений пользователей.'}, 'en': {'title': 'Unlocking the Power of Diffusion Models with Sparse Attention', 'desc': "This paper introduces PLADIS, a new method that enhances pre-trained diffusion models like U-Net and Transformer by using sparse attention techniques. Unlike previous methods that needed extra training or evaluations, PLADIS operates efficiently during inference by utilizing query-key correlations in the cross-attention layer. This approach improves the models' performance in generating high-quality images from text prompts without the need for additional training. The results demonstrate significant advancements in text alignment and user preference, making PLADIS a versatile solution for various applications in text-to-image generation."}, 'zh': {'title': 'PLADIS：高效提升扩散模型的稀疏注意力方法', 'desc': '扩散模型在生成高质量条件样本方面表现出色，尤其是使用无分类器引导（CFG）等技术。然而，现有方法通常需要额外的训练或神经功能评估（NFE），这使得它们与引导蒸馏模型不兼容。本文提出了一种新颖高效的方法，称为PLADIS，通过利用稀疏注意力来增强预训练模型（如U-Net/Transformer）。PLADIS在推理过程中利用交叉注意力层中的softmax和稀疏对应物，提升文本到图像的扩散模型的潜力，显著改善文本对齐和人类偏好。'}}}, {'id': 'https://huggingface.co/papers/2503.11646', 'title': 'Adversarial Data Collection: Human-Collaborative Perturbations for\n  Efficient and Robust Robotic Imitation Learning', 'url': 'https://huggingface.co/papers/2503.11646', 'abstract': 'The pursuit of data efficiency, where quality outweighs quantity, has emerged as a cornerstone in robotic manipulation, especially given the high costs associated with real-world data collection. We propose that maximizing the informational density of individual demonstrations can dramatically reduce reliance on large-scale datasets while improving task performance. To this end, we introduce Adversarial Data Collection, a Human-in-the-Loop (HiL) framework that redefines robotic data acquisition through real-time, bidirectional human-environment interactions. Unlike conventional pipelines that passively record static demonstrations, ADC adopts a collaborative perturbation paradigm: during a single episode, an adversarial operator dynamically alters object states, environmental conditions, and linguistic commands, while the tele-operator adaptively adjusts actions to overcome these evolving challenges. This process compresses diverse failure-recovery behaviors, compositional task variations, and environmental perturbations into minimal demonstrations. Our experiments demonstrate that ADC-trained models achieve superior compositional generalization to unseen task instructions, enhanced robustness to perceptual perturbations, and emergent error recovery capabilities. Strikingly, models trained with merely 20% of the demonstration volume collected through ADC significantly outperform traditional approaches using full datasets. These advances bridge the gap between data-centric learning paradigms and practical robotic deployment, demonstrating that strategic data acquisition, not merely post-hoc processing, is critical for scalable, real-world robot learning. Additionally, we are curating a large-scale ADC-Robotics dataset comprising real-world manipulation tasks with adversarial perturbations. This benchmark will be open-sourced to facilitate advancements in robotic imitation learning.', 'score': 31, 'issue_id': 2731, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': 'efdf1296bc567414', 'authors': ['Siyuan Huang', 'Yue Liao', 'Siyuan Feng', 'Shu Jiang', 'Si Liu', 'Hongsheng Li', 'Maoqing Yao', 'Guanghui Ren'], 'affiliations': ['Agibot', 'Beihang University', 'MMLab, CUHK', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.11646.jpg', 'data': {'categories': ['#robotics', '#benchmark', '#dataset', '#optimization', '#open_source', '#agents', '#training', '#data'], 'emoji': '🤖', 'ru': {'title': 'Меньше данных, больше эффективности: революция в обучении роботов', 'desc': 'Статья представляет новый подход к сбору данных для обучения роботов манипуляции - Adversarial Data Collection (ADC). ADC использует взаимодействие человека-оператора и среды в реальном времени для создания информационно насыщенных демонстраций. Эксперименты показывают, что модели, обученные на ADC-данных, достигают лучшей композиционной генерализации и устойчивости к возмущениям, чем традиционные подходы. Авторы также создают открытый набор данных ADC-Robotics для продвижения исследований в области имитационного обучения роботов.'}, 'en': {'title': 'Maximizing Data Efficiency in Robotic Learning with Adversarial Collection', 'desc': 'This paper introduces a new method called Adversarial Data Collection (ADC) to improve robotic manipulation by focusing on data efficiency. Instead of relying on large datasets, ADC enhances the quality of data through real-time interactions between humans and robots, allowing for dynamic adjustments during demonstrations. By using an adversarial approach, the framework captures a wide range of behaviors and challenges in fewer demonstrations, leading to better performance in unseen tasks. The results show that models trained with ADC can generalize better and recover from errors more effectively, even with significantly less data than traditional methods.'}, 'zh': {'title': '对抗性数据收集：提升机器人学习效率的关键', 'desc': '本论文提出了一种新的数据收集方法，称为对抗性数据收集（ADC），旨在提高机器人操作的效率。通过实时的人机交互，ADC能够在动态环境中收集高信息密度的演示数据，从而减少对大规模数据集的依赖。实验表明，使用ADC训练的模型在面对未见任务指令时表现出更好的组合泛化能力和对环境干扰的鲁棒性。最终，ADC方法显著提高了机器人学习的实用性，展示了战略性数据获取的重要性。'}}}, {'id': 'https://huggingface.co/papers/2503.11224', 'title': 'Technologies on Effectiveness and Efficiency: A Survey of State Spaces\n  Models', 'url': 'https://huggingface.co/papers/2503.11224', 'abstract': 'State Space Models (SSMs) have emerged as a promising alternative to the popular transformer-based models and have been increasingly gaining attention. Compared to transformers, SSMs excel at tasks with sequential data or longer contexts, demonstrating comparable performances with significant efficiency gains. In this survey, we provide a coherent and systematic overview for SSMs, including their theoretical motivations, mathematical formulations, comparison with existing model classes, and various applications. We divide the SSM series into three main sections, providing a detailed introduction to the original SSM, the structured SSM represented by S4, and the selective SSM typified by Mamba. We put an emphasis on technicality, and highlight the various key techniques introduced to address the effectiveness and efficiency of SSMs. We hope this manuscript serves as an introduction for researchers to explore the theoretical foundations of SSMs.', 'score': 21, 'issue_id': 2731, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': 'fb4219d497e59f64', 'authors': ['Xingtai Lv', 'Youbang Sun', 'Kaiyan Zhang', 'Shang Qu', 'Xuekai Zhu', 'Yuchen Fan', 'Yi Wu', 'Ermo Hua', 'Xinwei Long', 'Ning Ding', 'Bowen Zhou'], 'affiliations': ['Department of Electronic Engineering, Tsinghua University', 'Robotics Institute, Carnegie Mellon University', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2503.11224.jpg', 'data': {'categories': ['#architecture', '#long_context', '#survey', '#math', '#training'], 'emoji': '🧠', 'ru': {'title': 'SSM: Эффективная альтернатива трансформерам для обработки последовательностей', 'desc': 'Это обзор моделей пространства состояний (SSM) как альтернативы трансформерам в машинном обучении. SSM показывают сравнимую производительность с трансформерами, но более эффективны для задач с последовательными данными и длинным контекстом. В статье рассматриваются теоретические основы, математические формулировки и применения SSM. Авторы выделяют три основных типа SSM: оригинальные, структурированные (например, S4) и селективные (например, Mamba).'}, 'en': {'title': 'Unlocking Efficiency: The Power of State Space Models', 'desc': 'State Space Models (SSMs) are gaining popularity as an efficient alternative to transformer models, especially for sequential data and longer contexts. This paper provides a comprehensive overview of SSMs, detailing their theoretical foundations, mathematical formulations, and comparisons with other model types. It categorizes SSMs into three sections: the original SSM, the structured S4 model, and the selective Mamba model, emphasizing their unique techniques for improved performance. The goal is to guide researchers in understanding and exploring the potential of SSMs in various applications.'}, 'zh': {'title': '状态空间模型：高效处理序列数据的新选择', 'desc': '状态空间模型（SSMs）作为一种有前景的替代方案，逐渐受到关注，尤其是在处理序列数据或长上下文任务时表现优异。与流行的变换器模型相比，SSMs在效率上有显著提升，同时在性能上也能与之媲美。本文对SSMs进行了系统的概述，包括其理论动机、数学公式、与现有模型的比较以及各种应用。我们将SSM系列分为三个主要部分，详细介绍了原始SSM、结构化SSM（如S4）和选择性SSM（如Mamba），并强调了提高SSM有效性和效率的关键技术。'}}}, {'id': 'https://huggingface.co/papers/2503.11069', 'title': 'API Agents vs. GUI Agents: Divergence and Convergence', 'url': 'https://huggingface.co/papers/2503.11069', 'abstract': 'Large language models (LLMs) have evolved beyond simple text generation to power software agents that directly translate natural language commands into tangible actions. While API-based LLM agents initially rose to prominence for their robust automation capabilities and seamless integration with programmatic endpoints, recent progress in multimodal LLM research has enabled GUI-based LLM agents that interact with graphical user interfaces in a human-like manner. Although these two paradigms share the goal of enabling LLM-driven task automation, they diverge significantly in architectural complexity, development workflows, and user interaction models.   This paper presents the first comprehensive comparative study of API-based and GUI-based LLM agents, systematically analyzing their divergence and potential convergence. We examine key dimensions and highlight scenarios in which hybrid approaches can harness their complementary strengths. By proposing clear decision criteria and illustrating practical use cases, we aim to guide practitioners and researchers in selecting, combining, or transitioning between these paradigms. Ultimately, we indicate that continuing innovations in LLM-based automation are poised to blur the lines between API- and GUI-driven agents, paving the way for more flexible, adaptive solutions in a wide range of real-world applications.', 'score': 19, 'issue_id': 2730, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '29e714954ed20978', 'authors': ['Chaoyun Zhang', 'Shilin He', 'Liqun Li', 'Si Qin', 'Yu Kang', 'Qingwei Lin', 'Dongmei Zhang'], 'affiliations': ['Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2503.11069.jpg', 'data': {'categories': ['#multimodal', '#survey', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Сравнение API и GUI агентов на основе LLM: путь к гибридным решениям', 'desc': 'Это исследование сравнивает агентов на основе больших языковых моделей (LLM), работающих через API и через графический интерфейс. Авторы анализируют различия в архитектуре, разработке и взаимодействии с пользователем для обоих подходов. Они предлагают критерии выбора и описывают сценарии, где гибридные решения могут быть эффективны. Исследование показывает, что инновации в автоматизации на основе LLM стирают границы между этими парадигмами.'}, 'en': {'title': 'Bridging the Gap: API and GUI LLM Agents Unite', 'desc': 'This paper explores the evolution of large language models (LLMs) from simple text generators to sophisticated software agents that can perform tasks based on natural language commands. It compares two main types of LLM agents: API-based agents, which automate tasks through programmatic interfaces, and GUI-based agents, which interact with graphical user interfaces like humans. The study highlights the differences in their architecture, development processes, and user interactions, while also discussing how hybrid approaches can leverage the strengths of both paradigms. The authors provide decision criteria and practical examples to help users choose the right approach for their needs, suggesting that future advancements will further integrate these two types of agents.'}, 'zh': {'title': 'API与GUI代理的比较与融合之路', 'desc': '大型语言模型（LLMs）已经从简单的文本生成发展到能够将自然语言命令直接转化为实际操作的软件代理。本文首次全面比较了基于API的LLM代理和基于GUI的LLM代理，分析了它们在架构复杂性、开发工作流程和用户交互模型上的显著差异。我们探讨了关键维度，并强调了混合方法在利用两者互补优势方面的场景。最终，我们指出LLM驱动的自动化创新将模糊API和GUI代理之间的界限，为各种实际应用提供更灵活、适应性强的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2503.11514', 'title': 'Exploring the Vulnerabilities of Federated Learning: A Deep Dive into\n  Gradient Inversion Attacks', 'url': 'https://huggingface.co/papers/2503.11514', 'abstract': 'Federated Learning (FL) has emerged as a promising privacy-preserving collaborative model training paradigm without sharing raw data. However, recent studies have revealed that private information can still be leaked through shared gradient information and attacked by Gradient Inversion Attacks (GIA). While many GIA methods have been proposed, a detailed analysis, evaluation, and summary of these methods are still lacking. Although various survey papers summarize existing privacy attacks in FL, few studies have conducted extensive experiments to unveil the effectiveness of GIA and their associated limiting factors in this context. To fill this gap, we first undertake a systematic review of GIA and categorize existing methods into three types, i.e., optimization-based GIA (OP-GIA), generation-based GIA (GEN-GIA), and analytics-based GIA (ANA-GIA). Then, we comprehensively analyze and evaluate the three types of GIA in FL, providing insights into the factors that influence their performance, practicality, and potential threats. Our findings indicate that OP-GIA is the most practical attack setting despite its unsatisfactory performance, while GEN-GIA has many dependencies and ANA-GIA is easily detectable, making them both impractical. Finally, we offer a three-stage defense pipeline to users when designing FL frameworks and protocols for better privacy protection and share some future research directions from the perspectives of attackers and defenders that we believe should be pursued. We hope that our study can help researchers design more robust FL frameworks to defend against these attacks.', 'score': 13, 'issue_id': 2730, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'd31bf6f9bd4bc86b', 'authors': ['Pengxin Guo', 'Runxi Wang', 'Shuang Zeng', 'Jinjing Zhu', 'Haoning Jiang', 'Yanran Wang', 'Yuyin Zhou', 'Feifei Wang', 'Hui Xiong', 'Liangqiong Qu'], 'affiliations': ['Department of Biomedical Data Science, Stanford University, Stanford, CA 94305, USA', 'Department of Computer Science and Engineering, University of California, Santa Cruz, CA 95064, USA', 'Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong 999077, China', 'Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen 518055, China', 'Department of Mathematics, The University of Hong Kong, Hong Kong 999077, China', 'Materials Innovation Institute for Life Sciences and Energy (MILES), HKU-SIRI, Shenzhen 518055, China', 'School of Computing and Data Science, The University of Hong Kong, Hong Kong 999077, China', 'Thrust of Artificial Intelligence, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou 511458, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.11514.jpg', 'data': {'categories': ['#leakage', '#benchmark', '#security', '#survey', '#healthcare', '#data'], 'emoji': '🛡️', 'ru': {'title': 'Защита приватности в федеративном обучении: анализ атак с инверсией градиента', 'desc': 'Статья посвящена анализу атак с инверсией градиента (GIA) в контексте федеративного обучения (FL). Авторы классифицируют существующие методы GIA на три типа: оптимизационные, генеративные и аналитические. Проводится комплексная оценка эффективности и практичности каждого типа атак в FL. Исследование показывает, что оптимизационные GIA являются наиболее практичными, несмотря на их неудовлетворительную производительность.'}, 'en': {'title': 'Strengthening Privacy in Federated Learning Against Gradient Inversion Attacks', 'desc': "This paper focuses on the vulnerabilities of Federated Learning (FL) to Gradient Inversion Attacks (GIA), which can leak private information despite the model's privacy-preserving intentions. It categorizes existing GIA methods into three types: optimization-based, generation-based, and analytics-based, and provides a thorough analysis of their effectiveness and limitations. The study reveals that while optimization-based GIA is the most practical, it still has performance issues, whereas generation-based and analytics-based methods are less practical due to their dependencies and detectability. The authors propose a defense strategy to enhance privacy in FL frameworks and suggest future research directions to strengthen defenses against these attacks."}, 'zh': {'title': '提升联邦学习隐私保护的防御策略', 'desc': '联邦学习（FL）是一种保护隐私的协作模型训练方法，不需要共享原始数据。然而，最近的研究表明，通过共享梯度信息，私密信息仍然可能被泄露，并受到梯度反演攻击（GIA）的威胁。本文对现有的GIA方法进行了系统的回顾和分类，并分析了三种类型的GIA在FL中的表现和局限性。最后，我们提出了一个三阶段的防御方案，以帮助用户在设计FL框架时更好地保护隐私。'}}}, {'id': 'https://huggingface.co/papers/2503.10772', 'title': 'FlowTok: Flowing Seamlessly Across Text and Image Tokens', 'url': 'https://huggingface.co/papers/2503.10772', 'abstract': 'Bridging different modalities lies at the heart of cross-modality generation. While conventional approaches treat the text modality as a conditioning signal that gradually guides the denoising process from Gaussian noise to the target image modality, we explore a much simpler paradigm-directly evolving between text and image modalities through flow matching. This requires projecting both modalities into a shared latent space, which poses a significant challenge due to their inherently different representations: text is highly semantic and encoded as 1D tokens, whereas images are spatially redundant and represented as 2D latent embeddings. To address this, we introduce FlowTok, a minimal framework that seamlessly flows across text and images by encoding images into a compact 1D token representation. Compared to prior methods, this design reduces the latent space size by 3.3x at an image resolution of 256, eliminating the need for complex conditioning mechanisms or noise scheduling. Moreover, FlowTok naturally extends to image-to-text generation under the same formulation. With its streamlined architecture centered around compact 1D tokens, FlowTok is highly memory-efficient, requires significantly fewer training resources, and achieves much faster sampling speeds-all while delivering performance comparable to state-of-the-art models. Code will be available at https://github.com/bytedance/1d-tokenizer.', 'score': 11, 'issue_id': 2731, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '548255900cd1ec21', 'authors': ['Ju He', 'Qihang Yu', 'Qihao Liu', 'Liang-Chieh Chen'], 'affiliations': ['ByteDance Seed', 'Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10772.jpg', 'data': {'categories': ['#architecture', '#training', '#multimodal', '#diffusion'], 'emoji': '🌊', 'ru': {'title': 'FlowTok: эффективный переход между текстом и изображением через токены', 'desc': 'FlowTok - это новый подход к генерации изображений из текста и наоборот. Он использует метод согласования потоков для прямого перехода между модальностями текста и изображения в общем латентном пространстве. Ключевая идея заключается в кодировании изображений в компактное одномерное токенное представление, что значительно уменьшает размер латентного пространства. Это позволяет упростить архитектуру, сократить потребление памяти и ускорить обучение и генерацию по сравнению с существующими методами.'}, 'en': {'title': 'FlowTok: Simplifying Cross-Modality Generation with 1D Tokens', 'desc': 'This paper presents FlowTok, a novel framework for cross-modality generation that simplifies the process of transitioning between text and image modalities. Unlike traditional methods that rely on complex conditioning signals and denoising processes, FlowTok directly evolves between text and images by utilizing flow matching in a shared latent space. The framework encodes images into a compact 1D token representation, significantly reducing the latent space size and improving efficiency. FlowTok not only enhances memory usage and training resource requirements but also maintains competitive performance in generating images from text and vice versa.'}, 'zh': {'title': 'FlowTok：简化跨模态生成的高效框架', 'desc': '这篇论文探讨了跨模态生成中的不同模态之间的桥接问题。传统方法将文本模态视为引导信号，逐步引导去噪过程，而我们提出了一种更简单的方法，通过流匹配直接在文本和图像模态之间演变。我们引入了FlowTok框架，将图像编码为紧凑的1D标记表示，从而在共享潜在空间中流动，显著减少了潜在空间的大小。FlowTok不仅提高了内存效率和采样速度，还在图像到文本生成方面表现出色，性能与最先进的模型相当。'}}}, {'id': 'https://huggingface.co/papers/2503.10970', 'title': 'TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of\n  Tools', 'url': 'https://huggingface.co/papers/2503.10970', 'abstract': 'Precision therapeutics require multimodal adaptive models that generate personalized treatment recommendations. We introduce TxAgent, an AI agent that leverages multi-step reasoning and real-time biomedical knowledge retrieval across a toolbox of 211 tools to analyze drug interactions, contraindications, and patient-specific treatment strategies. TxAgent evaluates how drugs interact at molecular, pharmacokinetic, and clinical levels, identifies contraindications based on patient comorbidities and concurrent medications, and tailors treatment strategies to individual patient characteristics. It retrieves and synthesizes evidence from multiple biomedical sources, assesses interactions between drugs and patient conditions, and refines treatment recommendations through iterative reasoning. It selects tools based on task objectives and executes structured function calls to solve therapeutic tasks that require clinical reasoning and cross-source validation. The ToolUniverse consolidates 211 tools from trusted sources, including all US FDA-approved drugs since 1939 and validated clinical insights from Open Targets. TxAgent outperforms leading LLMs, tool-use models, and reasoning agents across five new benchmarks: DrugPC, BrandPC, GenericPC, TreatmentPC, and DescriptionPC, covering 3,168 drug reasoning tasks and 456 personalized treatment scenarios. It achieves 92.1% accuracy in open-ended drug reasoning tasks, surpassing GPT-4o and outperforming DeepSeek-R1 (671B) in structured multi-step reasoning. TxAgent generalizes across drug name variants and descriptions. By integrating multi-step inference, real-time knowledge grounding, and tool-assisted decision-making, TxAgent ensures that treatment recommendations align with established clinical guidelines and real-world evidence, reducing the risk of adverse events and improving therapeutic decision-making.', 'score': 10, 'issue_id': 2732, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': 'b7a03e6b34c3c0de', 'authors': ['Shanghua Gao', 'Richard Zhu', 'Zhenglun Kong', 'Ayush Noori', 'Xiaorui Su', 'Curtis Ginder', 'Theodoros Tsiligkaridis', 'Marinka Zitnik'], 'affiliations': ['Broad Institute of MIT and Harvard, Cambridge, MA', 'Cardiovascular Division, Department of Medicine, Brigham and Womens Hospital, Harvard Medical School, Boston, MA', 'Department of Biomedical Informatics, Harvard Medical School, Boston, MA', 'Harvard Data Science Initiative, Cambridge, MA', 'Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University, Cambridge, MA', 'MIT Lincoln Laboratory, Lexington, MA'], 'pdf_title_img': 'assets/pdf/title_img/2503.10970.jpg', 'data': {'categories': ['#alignment', '#healthcare', '#science', '#agents', '#reasoning', '#benchmark', '#multimodal'], 'emoji': '💊', 'ru': {'title': 'TxAgent: ИИ-помощник для точной и персонализированной фармакотерапии', 'desc': 'TxAgent - это ИИ-агент для персонализированной терапии, использующий многоэтапное рассуждение и извлечение биомедицинских знаний в реальном времени. Он анализирует взаимодействия лекарств, противопоказания и индивидуальные стратегии лечения, используя набор из 211 инструментов. TxAgent превосходит ведущие языковые модели и другие агенты в пяти новых бенчмарках, охватывающих 3168 задач по рассуждению о лекарствах и 456 персонализированных сценариев лечения. Интегрируя многоэтапный вывод, актуальные знания и инструментальное принятие решений, TxAgent обеспечивает соответствие рекомендаций клиническим руководствам и реальным данным.'}, 'en': {'title': 'TxAgent: Personalized Precision Therapeutics through Advanced AI Reasoning', 'desc': 'The paper presents TxAgent, an advanced AI agent designed for precision therapeutics that generates personalized treatment recommendations. It utilizes multi-step reasoning and real-time biomedical knowledge retrieval from a comprehensive toolbox of 211 tools to analyze drug interactions and contraindications. TxAgent evaluates drug interactions at various levels and tailors treatment strategies based on individual patient characteristics, ensuring recommendations are evidence-based. The system outperforms existing models in drug reasoning tasks, achieving high accuracy and improving therapeutic decision-making by integrating clinical guidelines and real-world evidence.'}, 'zh': {'title': '个性化治疗的智能助手TxAgent', 'desc': '精准治疗需要多模态自适应模型来生成个性化的治疗建议。我们介绍了TxAgent，这是一种利用多步推理和实时生物医学知识检索的人工智能代理，能够分析药物相互作用、禁忌症和患者特定的治疗策略。TxAgent在分子、药代动力学和临床层面评估药物相互作用，并根据患者的合并症和同时用药识别禁忌症，量身定制治疗策略。通过整合多步推理、实时知识基础和工具辅助决策，TxAgent确保治疗建议符合既定的临床指南和现实世界证据，从而降低不良事件的风险，改善治疗决策。'}}}, {'id': 'https://huggingface.co/papers/2503.10781', 'title': 'Large-scale Pre-training for Grounded Video Caption Generation', 'url': 'https://huggingface.co/papers/2503.10781', 'abstract': 'We propose a novel approach for captioning and object grounding in video, where the objects in the caption are grounded in the video via temporally dense bounding boxes. We introduce the following contributions. First, we present a large-scale automatic annotation method that aggregates captions grounded with bounding boxes across individual frames into temporally dense and consistent bounding box annotations. We apply this approach on the HowTo100M dataset to construct a large-scale pre-training dataset, named HowToGround1M. We also introduce a Grounded Video Caption Generation model, dubbed GROVE, and pre-train the model on HowToGround1M. Second, we introduce a new dataset, called iGround, of 3500 videos with manually annotated captions and dense spatio-temporally grounded bounding boxes. This allows us to measure progress on this challenging problem, as well as to fine-tune our model on this small-scale but high-quality data. Third, we demonstrate that our approach achieves state-of-the-art results on the proposed iGround dataset compared to a number of baselines, as well as on the VidSTG and ActivityNet-Entities datasets. We perform extensive ablations that demonstrate the importance of pre-training using our automatically annotated HowToGround1M dataset followed by fine-tuning on the manually annotated iGround dataset and validate the key technical contributions of our model.', 'score': 10, 'issue_id': 2737, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '842a44eb006952de', 'authors': ['Evangelos Kazakos', 'Cordelia Schmid', 'Josef Sivic'], 'affiliations': ['Czech Institute of Informatics, Robotics and Cybernetics at the Czech Technical University in Prague', 'Inria, Ecole normale superieure, CNRS, PSL Research University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10781.jpg', 'data': {'categories': ['#cv', '#dataset', '#training', '#video', '#data'], 'emoji': '🎥', 'ru': {'title': 'Революция в понимании видео: от автоматической разметки к точной локализации объектов', 'desc': 'Статья представляет новый подход к генерации подписей и локализации объектов в видео с использованием темпорально плотных ограничивающих рамок. Авторы создали большой датасет HowToGround1M для предобучения модели GROVE, а также набор данных iGround с ручной разметкой для точной настройки. Предложенный метод достигает передовых результатов на нескольких наборах данных, включая VidSTG и ActivityNet-Entities. Эксперименты подтверждают важность предобучения на автоматически размеченных данных с последующей точной настройкой на вручную аннотированном наборе.'}, 'en': {'title': 'Grounding Video Captions with Precision', 'desc': 'This paper presents a new method for video captioning and object grounding, where objects mentioned in captions are linked to specific locations in the video using detailed bounding boxes. The authors introduce a large-scale automatic annotation technique that creates consistent bounding box annotations across video frames, resulting in a dataset called HowToGround1M for pre-training. They also develop a model named GROVE, which is trained on this dataset and further fine-tuned on a smaller, high-quality dataset called iGround, containing manually annotated videos. The results show that their approach outperforms existing methods on several benchmark datasets, highlighting the effectiveness of their pre-training and fine-tuning strategy.'}, 'zh': {'title': '视频字幕生成与物体定位的新方法', 'desc': '本文提出了一种新颖的视频字幕生成和物体定位方法，通过时间密集的边界框将字幕中的物体与视频中的内容关联起来。我们介绍了一种大规模自动注释方法，将单帧的边界框注释聚合为时间上密集且一致的边界框注释，并在HowTo100M数据集上构建了一个名为HowToGround1M的大规模预训练数据集。我们还提出了一个名为GROVE的基于视频的字幕生成模型，并在HowToGround1M上进行了预训练。此外，我们创建了一个名为iGround的新数据集，包含3500个视频及其手动注释的字幕和密集的时空边界框，以便于评估模型的进展和进行微调。'}}}, {'id': 'https://huggingface.co/papers/2503.11576', 'title': 'SmolDocling: An ultra-compact vision-language model for end-to-end\n  multi-modal document conversion', 'url': 'https://huggingface.co/papers/2503.11576', 'abstract': 'We introduce SmolDocling, an ultra-compact vision-language model targeting end-to-end document conversion. Our model comprehensively processes entire pages by generating DocTags, a new universal markup format that captures all page elements in their full context with location. Unlike existing approaches that rely on large foundational models, or ensemble solutions that rely on handcrafted pipelines of multiple specialized models, SmolDocling offers an end-to-end conversion for accurately capturing content, structure and spatial location of document elements in a 256M parameters vision-language model. SmolDocling exhibits robust performance in correctly reproducing document features such as code listings, tables, equations, charts, lists, and more across a diverse range of document types including business documents, academic papers, technical reports, patents, and forms -- significantly extending beyond the commonly observed focus on scientific papers. Additionally, we contribute novel publicly sourced datasets for charts, tables, equations, and code recognition. Experimental results demonstrate that SmolDocling competes with other Vision Language Models that are up to 27 times larger in size, while reducing computational requirements substantially. The model is currently available, datasets will be publicly available soon.', 'score': 9, 'issue_id': 2744, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '5548a7c6526f8753', 'authors': ['Ahmed Nassar', 'Andres Marafioti', 'Matteo Omenetti', 'Maksym Lysak', 'Nikolaos Livathinos', 'Christoph Auer', 'Lucas Morin', 'Rafael Teixeira de Lima', 'Yusik Kim', 'A. Said Gurbuz', 'Michele Dolfi', 'Miquel Farré', 'Peter W. J. Staar'], 'affiliations': ['HuggingFace', 'IBM Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.11576.jpg', 'data': {'categories': ['#small_models', '#open_source', '#cv', '#dataset', '#science', '#multimodal'], 'emoji': '📄', 'ru': {'title': 'SmolDocling: компактная модель для комплексной обработки документов', 'desc': 'SmolDocling - это компактная модель обработки документов, сочетающая зрение и язык. Она генерирует универсальную разметку DocTags, захватывающую все элементы страницы с их расположением. Модель показывает надежную производительность в воспроизведении различных особенностей документов, включая листинги кода, таблицы, уравнения и диаграммы. SmolDocling конкурирует с моделями в 27 раз большего размера, существенно снижая вычислительные требования.'}, 'en': {'title': 'SmolDocling: Compact and Powerful Document Conversion', 'desc': 'SmolDocling is a compact vision-language model designed for end-to-end document conversion. It generates DocTags, a universal markup format that captures the content, structure, and spatial location of document elements. Unlike traditional methods that use large models or complex pipelines, SmolDocling achieves high accuracy with only 256 million parameters. It performs well across various document types, including business and academic papers, and introduces new datasets for recognizing charts, tables, equations, and code.'}, 'zh': {'title': 'SmolDocling：高效文档转换的新选择', 'desc': '我们介绍了SmolDocling，这是一种超紧凑的视觉-语言模型，旨在实现端到端的文档转换。该模型通过生成DocTags，一种新的通用标记格式，全面处理整个页面，捕捉所有页面元素的完整上下文和位置。与依赖大型基础模型或多个专用模型的手工管道的现有方法不同，SmolDocling提供了一种端到端的转换，准确捕捉文档元素的内容、结构和空间位置。实验结果表明，SmolDocling在性能上与其他高达27倍大小的视觉语言模型竞争，同时显著降低了计算需求。'}}}, {'id': 'https://huggingface.co/papers/2503.11579', 'title': 'Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers', 'url': 'https://huggingface.co/papers/2503.11579', 'abstract': 'State-of-the-art transformer-based large multimodal models (LMMs) struggle to handle hour-long video inputs due to the quadratic complexity of the causal self-attention operations, leading to high computational costs during training and inference. Existing token compression-based methods reduce the number of video tokens but often incur information loss and remain inefficient for extremely long sequences. In this paper, we explore an orthogonal direction to build a hybrid Mamba-Transformer model (VAMBA) that employs Mamba-2 blocks to encode video tokens with linear complexity. Without any token reduction, VAMBA can encode more than 1024 frames (640times360) on a single GPU, while transformer-based models can only encode 256 frames. On long video input, VAMBA achieves at least 50% reduction in GPU memory usage during training and inference, and nearly doubles the speed per training step compared to transformer-based LMMs. Our experimental results demonstrate that VAMBA improves accuracy by 4.3% on the challenging hour-long video understanding benchmark LVBench over prior efficient video LMMs, and maintains strong performance on a broad spectrum of long and short video understanding tasks.', 'score': 8, 'issue_id': 2744, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': 'e0a1f364990dbe23', 'authors': ['Weiming Ren', 'Wentao Ma', 'Huan Yang', 'Cong Wei', 'Ge Zhang', 'Wenhu Chen'], 'affiliations': ['1.AI', 'M-A-P', 'University of Toronto', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.11579.jpg', 'data': {'categories': ['#optimization', '#long_context', '#benchmark', '#architecture', '#video'], 'emoji': '🎥', 'ru': {'title': 'VAMBA: эффективная обработка длинных видео с линейной сложностью', 'desc': 'Статья представляет новую модель VAMBA, гибрид Mamba и трансформера, для обработки длинных видео. VAMBA использует блоки Mamba-2 для кодирования видеотокенов с линейной сложностью, что позволяет обрабатывать более 1024 кадров на одном GPU. Модель снижает использование памяти GPU на 50% и почти вдвое ускоряет обучение по сравнению с трансформерными моделями. VAMBA показывает улучшение точности на 4.3% на бенчмарке LVBench для понимания часовых видео.'}, 'en': {'title': 'VAMBA: Efficient Video Processing with Linear Complexity', 'desc': 'This paper introduces the Mamba-Transformer model (VAMBA), which addresses the limitations of existing transformer-based large multimodal models (LMMs) in processing long video inputs. Unlike traditional methods that reduce video tokens and often lose information, VAMBA uses Mamba-2 blocks to encode video tokens with linear complexity, allowing it to handle over 1024 frames efficiently. The model significantly reduces GPU memory usage by at least 50% during training and inference, while also increasing training speed nearly twofold compared to standard transformers. Experimental results show that VAMBA not only enhances accuracy on the LVBench benchmark but also performs well across various video understanding tasks.'}, 'zh': {'title': '高效视频理解的新突破：VAMBA模型', 'desc': '本论文提出了一种新的混合Mamba-Transformer模型（VAMBA），旨在解决现有多模态模型在处理长视频输入时的计算复杂性问题。VAMBA使用Mamba-2模块以线性复杂度编码视频标记，避免了信息损失，并且无需减少标记数量。与传统的变换器模型相比，VAMBA在单个GPU上能够编码超过1024帧的视频，显著提高了训练和推理的速度，并减少了GPU内存使用。实验结果表明，VAMBA在长视频理解基准测试中比之前的高效视频模型提高了4.3%的准确率，同时在多种视频理解任务中保持了强大的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.10632', 'title': 'Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision\n  Transformers?', 'url': 'https://huggingface.co/papers/2503.10632', 'abstract': "Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of learnable activation functions with the potential to capture more complex relationships from data. Although KANs are useful in finding symbolic representations and continual learning of one-dimensional functions, their effectiveness in diverse machine learning (ML) tasks, such as vision, remains questionable. Presently, KANs are deployed by replacing multilayer perceptrons (MLPs) in deep network architectures, including advanced architectures such as vision Transformers (ViTs). In this paper, we are the first to design a general learnable Kolmogorov-Arnold Attention (KArAt) for vanilla ViTs that can operate on any choice of basis. However, the computing and memory costs of training them motivated us to propose a more modular version, and we designed particular learnable attention, called Fourier-KArAt. Fourier-KArAt and its variants either outperform their ViT counterparts or show comparable performance on CIFAR-10, CIFAR-100, and ImageNet-1K datasets. We dissect these architectures' performance and generalization capacity by analyzing their loss landscapes, weight distributions, optimizer path, attention visualization, and spectral behavior, and contrast them with vanilla ViTs. The goal of this paper is not to produce parameter- and compute-efficient attention, but to encourage the community to explore KANs in conjunction with more advanced architectures that require a careful understanding of learnable activations. Our open-source code and implementation details are available on: https://subhajitmaity.me/KArAt", 'score': 8, 'issue_id': 2731, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '46504216bbce5b86', 'authors': ['Subhajit Maity', 'Killian Hitsman', 'Xin Li', 'Aritra Dutta'], 'affiliations': ['Department of Computer Science, University of Central Florida, Orlando, FL, USA', 'Department of Mathematics, University of Central Florida, Orlando, FL, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.10632.jpg', 'data': {'categories': ['#architecture', '#cv', '#optimization', '#open_source', '#training'], 'emoji': '🧠', 'ru': {'title': 'Новый взгляд на внимание: Колмогоров-Арнольд сети в Vision Transformers', 'desc': 'Статья представляет новый подход к архитектуре нейронных сетей - Колмогоров-Арнольд внимание (KArAt) для моделей Vision Transformer. Авторы разработали модульную версию KArAt на основе преобразования Фурье, которая показывает сопоставимую или превосходящую производительность по сравнению с обычными ViT на нескольких наборах данных. В работе проводится глубокий анализ свойств новой архитектуры, включая ландшафт функции потерь, распределение весов и визуализацию внимания. Исследование призывает сообщество изучать сети Колмогорова-Арнольда в сочетании с передовыми архитектурами глубокого обучения.'}, 'en': {'title': 'Unlocking Complex Relationships with Learnable Activations in Vision Transformers', 'desc': 'Kolmogorov-Arnold networks (KANs) introduce learnable activation functions that can model complex data relationships. This paper explores the application of KANs in vision tasks by integrating them into vision Transformers (ViTs) through a novel learnable attention mechanism called Kolmogorov-Arnold Attention (KArAt). The authors also present a more efficient variant, Fourier-KArAt, which shows competitive performance on popular image datasets like CIFAR-10 and ImageNet-1K. The study emphasizes the importance of understanding learnable activations in advanced architectures, rather than solely focusing on efficiency.'}, 'zh': {'title': '探索可学习激活函数的潜力', 'desc': 'Kolmogorov-Arnold网络（KANs）是一种创新的可学习激活函数，能够捕捉数据中的复杂关系。尽管KANs在一维函数的符号表示和持续学习中表现出色，但在视觉等多种机器学习任务中的有效性仍然存在疑问。本文首次为普通的视觉变换器（ViTs）设计了一种通用的可学习Kolmogorov-Arnold注意力（KArAt），并提出了更模块化的Fourier-KArAt版本。实验结果表明，Fourier-KArAt及其变体在CIFAR-10、CIFAR-100和ImageNet-1K数据集上表现优于或与ViT相当。'}}}, {'id': 'https://huggingface.co/papers/2503.11651', 'title': 'VGGT: Visual Geometry Grounded Transformer', 'url': 'https://huggingface.co/papers/2503.11651', 'abstract': 'We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views. This approach is a step forward in 3D computer vision, where models have typically been constrained to and specialized for single tasks. It is also simple and efficient, reconstructing images in under one second, and still outperforming alternatives that require post-processing with visual geometry optimization techniques. The network achieves state-of-the-art results in multiple 3D tasks, including camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking. We also show that using pretrained VGGT as a feature backbone significantly enhances downstream tasks, such as non-rigid point tracking and feed-forward novel view synthesis. Code and models are publicly available at https://github.com/facebookresearch/vggt.', 'score': 7, 'issue_id': 2740, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '701338c26ac42ac7', 'authors': ['Jianyuan Wang', 'Minghao Chen', 'Nikita Karaev', 'Andrea Vedaldi', 'Christian Rupprecht', 'David Novotny'], 'affiliations': ['Meta AI', 'Visual Geometry Group, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.11651.jpg', 'data': {'categories': ['#cv', '#open_source', '#3d', '#optimization'], 'emoji': '🔮', 'ru': {'title': 'VGGT: Универсальная нейросеть для комплексного 3D-анализа сцен', 'desc': 'VGGT - это нейронная сеть прямого распространения, которая напрямую выводит все ключевые 3D-атрибуты сцены из одного или нескольких её изображений. Модель эффективно реконструирует изображения менее чем за секунду, превосходя альтернативы, требующие постобработки. VGGT достигает передовых результатов в нескольких 3D-задачах, включая оценку параметров камеры и глубины, реконструкцию облака точек и 3D-трекинг. Использование предобученной VGGT в качестве основы значительно улучшает работу в задачах синтеза новых ракурсов и нежесткого трекинга точек.'}, 'en': {'title': 'VGGT: Revolutionizing 3D Scene Understanding with Speed and Efficiency', 'desc': 'VGGT is a feed-forward neural network designed to extract key 3D attributes from various views of a scene, such as camera parameters and depth maps. Unlike traditional models that focus on single tasks, VGGT efficiently handles multiple 3D computer vision tasks simultaneously. It operates quickly, reconstructing images in under one second while achieving superior results compared to methods that rely on post-processing. Additionally, VGGT can be used as a feature backbone to improve performance in related tasks like point tracking and novel view synthesis.'}, 'zh': {'title': 'VGGT：高效的3D场景推断网络', 'desc': '我们提出了VGGT，这是一种前馈神经网络，可以直接推断场景的所有关键3D属性，包括相机参数、点图、深度图和3D点轨迹。该方法在3D计算机视觉领域向前迈出了一步，克服了以往模型仅限于单一任务的局限性。VGGT简单高效，能够在不到一秒的时间内重建图像，并且在多个3D任务中表现优于需要后处理的替代方案。使用预训练的VGGT作为特征骨干显著提升了下游任务的性能，如非刚性点跟踪和前馈新视图合成。'}}}, {'id': 'https://huggingface.co/papers/2503.09279', 'title': 'Cockatiel: Ensembling Synthetic and Human Preferenced Training for\n  Detailed Video Caption', 'url': 'https://huggingface.co/papers/2503.09279', 'abstract': 'Video Detailed Captioning (VDC) is a crucial task for vision-language bridging, enabling fine-grained descriptions of complex video content. In this paper, we first comprehensively benchmark current state-of-the-art approaches and systematically identified two critical limitations: biased capability towards specific captioning aspect and misalignment with human preferences. To address these deficiencies, we propose Cockatiel, a novel three-stage training pipeline that ensembles synthetic and human-aligned training for improving VDC performance. In the first stage, we derive a scorer from a meticulously annotated dataset to select synthetic captions high-performing on certain fine-grained video-caption alignment and human-preferred while disregarding others. Then, we train Cockatiel-13B, using this curated dataset to infuse it with assembled model strengths and human preferences. Finally, we further distill Cockatiel-8B from Cockatiel-13B for the ease of usage. Extensive quantitative and qualitative experiments reflect the effectiveness of our method, as we not only set new state-of-the-art performance on VDCSCORE in a dimension-balanced way but also surpass leading alternatives on human preference by a large margin as depicted by the human evaluation results.', 'score': 5, 'issue_id': 2730, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': 'edf6712b564fd37a', 'authors': ['Luozheng Qin', 'Zhiyu Tan', 'Mengping Yang', 'Xiaomeng Yang', 'Hao Li'], 'affiliations': ['Fudan University', 'Shanghai Academy of Artificial Intelligence for Science'], 'pdf_title_img': 'assets/pdf/title_img/2503.09279.jpg', 'data': {'categories': ['#benchmark', '#training', '#multimodal', '#synthetic', '#alignment'], 'emoji': '🦜', 'ru': {'title': 'Cockatiel: Новый стандарт в детальном описании видео', 'desc': 'Статья представляет новый подход к детальному описанию видео (VDC) под названием Cockatiel. Авторы разработали трехэтапный процесс обучения, который объединяет синтетические и человеко-ориентированные данные для улучшения производительности. Метод включает отбор высококачественных синтетических подписей, обучение большой модели Cockatiel-13B и ее дистилляцию в меньшую Cockatiel-8B. Эксперименты показали, что Cockatiel превосходит существующие методы по метрике VDCSCORE и человеческим предпочтениям.'}, 'en': {'title': 'Bridging Vision and Language with Cockatiel for Enhanced Video Captioning', 'desc': 'This paper addresses the challenge of Video Detailed Captioning (VDC), which involves creating precise descriptions for complex video content. The authors identify two main issues with existing methods: a bias towards certain aspects of captioning and a lack of alignment with human preferences. To overcome these challenges, they introduce Cockatiel, a three-stage training pipeline that combines synthetic and human-aligned data to enhance VDC performance. Their experiments demonstrate that Cockatiel achieves state-of-the-art results on the VDCSCORE metric and significantly outperforms other methods in terms of human preference evaluations.'}, 'zh': {'title': '提升视频描述的智能化与人性化', 'desc': '视频详细描述（VDC）是连接视觉和语言的重要任务，能够对复杂视频内容进行细致的描述。本文首先对当前最先进的方法进行了全面评估，并系统地识别出两个关键限制：对特定描述方面的偏见能力和与人类偏好的不一致。为了解决这些问题，我们提出了Cockatiel，这是一种新颖的三阶段训练流程，结合了合成和人类对齐的训练，以提高VDC性能。通过大量的定量和定性实验，我们的方法在VDCSCORE上设定了新的最先进性能，并在与人类偏好的比较中大幅超越了领先的替代方案。'}}}, {'id': 'https://huggingface.co/papers/2503.10696', 'title': 'Neighboring Autoregressive Modeling for Efficient Visual Generation', 'url': 'https://huggingface.co/papers/2503.10696', 'abstract': 'Visual autoregressive models typically adhere to a raster-order ``next-token prediction" paradigm, which overlooks the spatial and temporal locality inherent in visual content. Specifically, visual tokens exhibit significantly stronger correlations with their spatially or temporally adjacent tokens compared to those that are distant. In this paper, we propose Neighboring Autoregressive Modeling (NAR), a novel paradigm that formulates autoregressive visual generation as a progressive outpainting procedure, following a near-to-far ``next-neighbor prediction" mechanism. Starting from an initial token, the remaining tokens are decoded in ascending order of their Manhattan distance from the initial token in the spatial-temporal space, progressively expanding the boundary of the decoded region. To enable parallel prediction of multiple adjacent tokens in the spatial-temporal space, we introduce a set of dimension-oriented decoding heads, each predicting the next token along a mutually orthogonal dimension. During inference, all tokens adjacent to the decoded tokens are processed in parallel, substantially reducing the model forward steps for generation. Experiments on ImageNet256times 256 and UCF101 demonstrate that NAR achieves 2.4times and 8.6times higher throughput respectively, while obtaining superior FID/FVD scores for both image and video generation tasks compared to the PAR-4X approach. When evaluating on text-to-image generation benchmark GenEval, NAR with 0.8B parameters outperforms Chameleon-7B while using merely 0.4 of the training data. Code is available at https://github.com/ThisisBillhe/NAR.', 'score': 5, 'issue_id': 2738, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '5adda6787d6613db', 'authors': ['Yefei He', 'Yuanyu He', 'Shaoxuan He', 'Feng Chen', 'Hong Zhou', 'Kaipeng Zhang', 'Bohan Zhuang'], 'affiliations': ['Shanghai AI Laboratory, China', 'The University of Adelaide, Australia', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.10696.jpg', 'data': {'categories': ['#cv', '#video', '#games', '#benchmark', '#optimization', '#architecture', '#training'], 'emoji': '🧩', 'ru': {'title': 'NAR: Революция в авторегрессионном моделировании визуального контента', 'desc': "Статья представляет новый подход к авторегрессионному моделированию визуального контента, названный Neighboring Autoregressive Modeling (NAR). В отличие от традиционных моделей, использующих растровый порядок предсказания, NAR применяет механизм 'предсказания следующего соседа', учитывающий пространственно-временную близость токенов. Модель начинает с одного токена и последовательно декодирует остальные, расширяя границы декодированной области. Для параллельного предсказания соседних токенов используются специальные декодирующие головки, что значительно ускоряет процесс генерации."}, 'en': {'title': 'Revolutionizing Visual Generation with Neighboring Autoregressive Modeling', 'desc': "This paper introduces Neighboring Autoregressive Modeling (NAR), a new approach to visual generation that improves upon traditional autoregressive models by focusing on the spatial and temporal relationships between visual tokens. Instead of predicting the next token in a raster order, NAR uses a 'next-neighbor prediction' method, decoding tokens based on their proximity to an initial token. This allows for parallel processing of adjacent tokens, significantly speeding up the generation process. Experiments show that NAR achieves much higher throughput and better quality scores in image and video generation compared to existing methods, while also being more efficient in terms of training data usage."}, 'zh': {'title': '邻近自回归建模：提升视觉生成效率的创新方法', 'desc': '本文提出了一种新的视觉自回归模型，称为邻近自回归建模（NAR），旨在改善传统的基于光栅顺序的“下一个标记预测”方法。NAR通过近邻预测机制，将自回归视觉生成视为一种逐步扩展的过程，从初始标记开始，按曼哈顿距离逐步解码剩余标记。该方法引入了一组面向维度的解码头，允许在空间-时间空间中并行预测多个相邻标记，从而显著减少生成所需的模型前向步骤。实验结果表明，NAR在图像和视频生成任务中均优于现有方法，显示出更高的吞吐量和更好的生成质量。'}}}, {'id': 'https://huggingface.co/papers/2503.06553', 'title': 'ProJudge: A Multi-Modal Multi-Discipline Benchmark and\n  Instruction-Tuning Dataset for MLLM-based Process Judges', 'url': 'https://huggingface.co/papers/2503.06553', 'abstract': "As multi-modal large language models (MLLMs) frequently exhibit errors when solving scientific problems, evaluating the validity of their reasoning processes is critical for ensuring reliability and uncovering fine-grained model weaknesses. Since human evaluation is laborious and costly, prompting MLLMs as automated process judges has become a common practice. However, the reliability of these model-based judges remains uncertain. To address this, we introduce ProJudgeBench, the first comprehensive benchmark specifically designed for evaluating abilities of MLLM-based process judges. ProJudgeBench comprises 2,400 test cases and 50,118 step-level labels, spanning four scientific disciplines with diverse difficulty levels and multi-modal content. In ProJudgeBench, each step is meticulously annotated by human experts for correctness, error type, and explanation, enabling a systematic evaluation of judges' capabilities to detect, classify and diagnose errors. Evaluation on ProJudgeBench reveals a significant performance gap between open-source and proprietary models. To bridge this gap, we further propose ProJudge-173k, a large-scale instruction-tuning dataset, and a Dynamic Dual-Phase fine-tuning strategy that encourages models to explicitly reason through problem-solving before assessing solutions. Both contributions significantly enhance the process evaluation capabilities of open-source models. All the resources will be released to foster future research of reliable multi-modal process evaluation.", 'score': 5, 'issue_id': 2737, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': '9546d0d9f897e116', 'authors': ['Jiaxin Ai', 'Pengfei Zhou', 'Zhaopan Xu', 'Ming Li', 'Fanrui Zhang', 'Zizhen Li', 'Jianwen Sun', 'Yukang Feng', 'Baojin Huang', 'Zhongyuan Wang', 'Kaipeng Zhang'], 'affiliations': ['HZAU', 'NKU', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'USTC', 'WHU'], 'pdf_title_img': 'assets/pdf/title_img/2503.06553.jpg', 'data': {'categories': ['#interpretability', '#reasoning', '#multimodal', '#science', '#open_source', '#dataset', '#training', '#benchmark'], 'emoji': '🔬', 'ru': {'title': 'Надежная оценка научных рассуждений мультимодальными ИИ-судьями', 'desc': 'В статье представлен ProJudgeBench - первый комплексный бенчмарк для оценки способностей мультимодальных языковых моделей (MLLM) в роли автоматизированных судей процессов решения научных задач. Бенчмарк включает 2400 тестовых случаев и более 50 000 пошаговых оценок в четырех научных дисциплинах. Авторы также предлагают набор данных ProJudge-173k и стратегию дообучения для улучшения возможностей оценки процессов у открытых моделей. Результаты показывают значительный разрыв в производительности между открытыми и проприетарными моделями в этой задаче.'}, 'en': {'title': 'Enhancing MLLM Reliability with ProJudgeBench', 'desc': 'This paper addresses the challenges faced by multi-modal large language models (MLLMs) in solving scientific problems accurately. It introduces ProJudgeBench, a benchmark designed to evaluate the reasoning abilities of MLLM-based judges through a comprehensive set of test cases and detailed annotations. The study highlights a performance gap between open-source and proprietary models, indicating the need for improvement in the evaluation process. To enhance this, the authors propose ProJudge-173k, a dataset for instruction-tuning and a new fine-tuning strategy that promotes better reasoning in problem-solving.'}, 'zh': {'title': '提升多模态模型的过程评估能力', 'desc': '本文介绍了ProJudgeBench，这是一个专门用于评估多模态大型语言模型（MLLM）过程判断能力的基准。该基准包含2400个测试案例和50118个步骤级标签，涵盖四个科学领域，具有不同的难度和多模态内容。每个步骤都由人类专家仔细注释，以便系统评估模型在检测、分类和诊断错误方面的能力。通过在ProJudgeBench上的评估，发现开源模型与专有模型之间存在显著的性能差距，并提出了ProJudge-173k数据集和动态双阶段微调策略，以提高开源模型的过程评估能力。'}}}, {'id': 'https://huggingface.co/papers/2503.06542', 'title': 'ARMOR v0.1: Empowering Autoregressive Multimodal Understanding Model\n  with Interleaved Multimodal Generation via Asymmetric Synergy', 'url': 'https://huggingface.co/papers/2503.06542', 'abstract': 'Unified models (UniMs) for multimodal understanding and generation have recently received much attention in the area of vision and language. Existing UniMs are designed to simultaneously learn both multimodal understanding and generation capabilities, demanding substantial computational resources, and often struggle to generate interleaved text-image. We present ARMOR, a resource-efficient and pure autoregressive framework that achieves both understanding and generation by fine-tuning existing multimodal large language models (MLLMs). Specifically, ARMOR extends existing MLLMs from three perspectives: (1) For model architecture, an asymmetric encoder-decoder architecture with a forward-switching mechanism is introduced to unify embedding space integrating textual and visual modalities for enabling natural text-image interleaved generation with minimal computational overhead. (2) For training data, a meticulously curated, high-quality interleaved dataset is collected for fine-tuning MLLMs. (3) For the training algorithm, we propose a ``what or how to generate" algorithm to empower existing MLLMs with multimodal generation capabilities while preserving their multimodal understanding capabilities, through three progressive training stages based on the collected dataset. Experimental results demonstrate that ARMOR upgrades existing MLLMs to UniMs with promising image generation capabilities, using limited training resources. Our code will be released soon at https://armor.github.io.', 'score': 5, 'issue_id': 2737, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': '4b19cfc1e459fb2f', 'authors': ['Jianwen Sun', 'Yukang Feng', 'Chuanhao Li', 'Fanrui Zhang', 'Zizhen Li', 'Jiaxin Ai', 'Sizhuo Zhou', 'Yu Dai', 'Shenglin Zhang', 'Kaipeng Zhang'], 'affiliations': ['Nankai University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'University of Science and Technology of China', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2503.06542.jpg', 'data': {'categories': ['#games', '#optimization', '#multimodal', '#dataset', '#training', '#architecture'], 'emoji': '🛡️', 'ru': {'title': 'ARMOR: Эффективное улучшение мультимодальных моделей', 'desc': 'ARMOR - это новый подход к созданию унифицированных моделей для мультимодального понимания и генерации. Он использует асимметричную архитектуру энкодер-декодер с механизмом переключения вперед для объединения текстовых и визуальных модальностей. ARMOR обучается на специально собранном наборе данных с чередующимися текстом и изображениями. Применяется алгоритм обучения в три этапа, позволяющий улучшить возможности генерации мультимодальных больших языковых моделей при сохранении их способностей к пониманию.'}, 'en': {'title': 'ARMOR: Efficient Multimodal Mastery with Minimal Resources', 'desc': 'The paper introduces ARMOR, a new framework designed to enhance multimodal understanding and generation in machine learning models. Unlike existing unified models that require extensive computational resources, ARMOR is resource-efficient and fine-tunes large language models to achieve its goals. It employs an asymmetric encoder-decoder architecture to seamlessly integrate text and image data, allowing for natural interleaved generation. Additionally, ARMOR utilizes a carefully curated dataset and a novel training algorithm to improve the multimodal capabilities of existing models while maintaining their understanding abilities.'}, 'zh': {'title': 'ARMOR：高效的多模态理解与生成框架', 'desc': '本文介绍了一种名为ARMOR的统一模型框架，旨在提高多模态理解和生成的效率。ARMOR通过微调现有的大型多模态语言模型（MLLMs），实现了文本和图像的自然交织生成。该框架采用了不对称的编码-解码架构，并引入了前向切换机制，以减少计算资源的消耗。实验结果表明，ARMOR能够在有限的训练资源下，显著提升现有模型的图像生成能力。'}}}, {'id': 'https://huggingface.co/papers/2503.06674', 'title': 'Learning Few-Step Diffusion Models by Trajectory Distribution Matching', 'url': 'https://huggingface.co/papers/2503.06674', 'abstract': "Accelerating diffusion model sampling is crucial for efficient AIGC deployment. While diffusion distillation methods -- based on distribution matching and trajectory matching -- reduce sampling to as few as one step, they fall short on complex tasks like text-to-image generation. Few-step generation offers a better balance between speed and quality, but existing approaches face a persistent trade-off: distribution matching lacks flexibility for multi-step sampling, while trajectory matching often yields suboptimal image quality. To bridge this gap, we propose learning few-step diffusion models by Trajectory Distribution Matching (TDM), a unified distillation paradigm that combines the strengths of distribution and trajectory matching. Our method introduces a data-free score distillation objective, aligning the student's trajectory with the teacher's at the distribution level. Further, we develop a sampling-steps-aware objective that decouples learning targets across different steps, enabling more adjustable sampling. This approach supports both deterministic sampling for superior image quality and flexible multi-step adaptation, achieving state-of-the-art performance with remarkable efficiency. Our model, TDM, outperforms existing methods on various backbones, such as SDXL and PixArt-alpha, delivering superior quality and significantly reduced training costs. In particular, our method distills PixArt-alpha into a 4-step generator that outperforms its teacher on real user preference at 1024 resolution. This is accomplished with 500 iterations and 2 A800 hours -- a mere 0.01% of the teacher's training cost. In addition, our proposed TDM can be extended to accelerate text-to-video diffusion. Notably, TDM can outperform its teacher model (CogVideoX-2B) by using only 4 NFE on VBench, improving the total score from 80.91 to 81.65. Project page: https://tdm-t2x.github.io/", 'score': 4, 'issue_id': 2732, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': '1ce5d8eb2086abfc', 'authors': ['Yihong Luo', 'Tianyang Hu', 'Jiacheng Sun', 'Yujun Cai', 'Jing Tang'], 'affiliations': ['Huawei Noahs Ark Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.06674.jpg', 'data': {'categories': ['#diffusion', '#training', '#cv', '#video'], 'emoji': '🚀', 'ru': {'title': 'TDM: Революция в ускорении диффузионных моделей', 'desc': 'Статья представляет новый метод ускорения генерации изображений с помощью диффузионных моделей, называемый Trajectory Distribution Matching (TDM). TDM объединяет преимущества методов сопоставления распределений и траекторий, позволяя создавать высококачественные изображения за меньшее количество шагов. Метод вводит безданную цель дистилляции оценки и цель, учитывающую количество шагов сэмплирования, что обеспечивает гибкость и эффективность. TDM превосходит существующие методы на различных архитектурах, таких как SDXL и PixArt-alpha, значительно сокращая время обучения и улучшая качество генерации.'}, 'en': {'title': 'Efficient Few-Step Diffusion with TDM', 'desc': 'This paper presents a new method called Trajectory Distribution Matching (TDM) to improve the efficiency of diffusion models in generating images and videos. TDM combines the benefits of distribution matching and trajectory matching, allowing for fewer sampling steps while maintaining high image quality. The method introduces a data-free score distillation objective that aligns the learning process of a smaller model with a larger, more complex model. By decoupling learning targets for different sampling steps, TDM achieves state-of-the-art performance with significantly reduced training costs, making it suitable for applications like text-to-image and text-to-video generation.'}, 'zh': {'title': '提升扩散模型采样效率的创新方法', 'desc': '加速扩散模型采样对于高效的AIGC部署至关重要。本文提出了一种新的学习少步扩散模型的方法，称为轨迹分布匹配（TDM），它结合了分布匹配和轨迹匹配的优点。通过引入无数据的分数蒸馏目标，我们的模型能够在不同采样步骤之间解耦学习目标，从而实现更灵活的采样。实验结果表明，TDM在多个基准上超越了现有方法，显著提高了图像质量并降低了训练成本。'}}}, {'id': 'https://huggingface.co/papers/2503.10624', 'title': 'ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant\n  Tightness', 'url': 'https://huggingface.co/papers/2503.10624', 'abstract': 'Fitting a body to a 3D clothed human point cloud is a common yet challenging task. Traditional optimization-based approaches use multi-stage pipelines that are sensitive to pose initialization, while recent learning-based methods often struggle with generalization across diverse poses and garment types. We propose Equivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline that estimates cloth-to-body surface mapping through locally approximate SE(3) equivariance, encoding tightness as displacement vectors from the cloth surface to the underlying body. Following this mapping, pose-invariant body features regress sparse body markers, simplifying clothed human fitting into an inner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show that ETCH significantly outperforms state-of-the-art methods -- both tightness-agnostic and tightness-aware -- in body fitting accuracy on loose clothing (16.7% ~ 69.5%) and shape accuracy (average 49.9%). Our equivariant tightness design can even reduce directional errors by (67.2% ~ 89.8%) in one-shot (or out-of-distribution) settings. Qualitative results demonstrate strong generalization of ETCH, regardless of challenging poses, unseen shapes, loose clothing, and non-rigid dynamics. We will release the code and models soon for research purposes at https://boqian-li.github.io/ETCH/.', 'score': 3, 'issue_id': 2738, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'c0aca4cafef8e621', 'authors': ['Boqian Li', 'Haiwen Feng', 'Zeyu Cai', 'Michael J. Black', 'Yuliang Xiu'], 'affiliations': ['Berkeley AI Research (BAIR)', 'Max Planck Institute for Intelligent Systems', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10624.jpg', 'data': {'categories': ['#3d', '#optimization', '#cv', '#open_source'], 'emoji': '👕', 'ru': {'title': 'Точная подгонка 3D-модели тела к одетому человеку с учетом плотности прилегания одежды', 'desc': 'Статья представляет новый метод ETCH для подгонки трехмерной модели тела к облаку точек одетого человека. ETCH использует локально приближенную SE(3)-эквивариантность для оценки соответствия между поверхностью одежды и телом, кодируя плотность прилегания как векторы смещения. Метод регрессирует разреженные маркеры тела с помощью инвариантных к позе признаков, упрощая задачу подгонки тела. Эксперименты показывают, что ETCH значительно превосходит современные методы по точности подгонки тела и формы, особенно для свободной одежды.'}, 'en': {'title': 'Revolutionizing Body Fitting with Equivariant Tightness!', 'desc': 'The paper introduces Equivariant Tightness Fitting for Clothed Humans (ETCH), a new method for fitting a body model to a 3D point cloud of a clothed human. ETCH improves upon traditional optimization methods by using a pipeline that leverages SE(3) equivariance to accurately map cloth surfaces to body shapes. This approach simplifies the fitting process by focusing on pose-invariant body features and regressing sparse body markers. Experimental results show that ETCH significantly enhances fitting accuracy for various clothing types and poses, outperforming existing methods in both tightness and shape accuracy.'}, 'zh': {'title': '等变紧致拟合：提升3D穿衣人类拟合精度的创新方法', 'desc': '本论文提出了一种新的方法，称为等变紧致拟合（ETCH），用于将身体与3D穿衣人类点云相匹配。传统的方法依赖于多阶段优化，容易受到姿势初始化的影响，而学习型方法在不同姿势和服装类型的泛化能力上存在困难。ETCH通过局部近似的SE(3)等变性来估计布料与身体表面的映射，并将紧致度编码为从布料表面到身体的位移向量。实验结果表明，ETCH在松散衣物的身体拟合精度和形状精度上显著优于现有的最先进方法，展示了其强大的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.11207', 'title': 'Can Large Reasoning Models do Analogical Reasoning under Perceptual\n  Uncertainty?', 'url': 'https://huggingface.co/papers/2503.11207', 'abstract': "This work presents a first evaluation of two state-of-the-art Large Reasoning Models (LRMs), OpenAI's o3-mini and DeepSeek R1, on analogical reasoning, focusing on well-established nonverbal human IQ tests based on Raven's progressive matrices. We benchmark with the I-RAVEN dataset and its more difficult extension, I-RAVEN-X, which tests the ability to generalize to longer reasoning rules and ranges of the attribute values. To assess the influence of visual uncertainties on these nonverbal analogical reasoning tests, we extend the I-RAVEN-X dataset, which otherwise assumes an oracle perception. We adopt a two-fold strategy to simulate this imperfect visual perception: 1) we introduce confounding attributes which, being sampled at random, do not contribute to the prediction of the correct answer of the puzzles and 2) smoothen the distributions of the input attributes' values. We observe a sharp decline in OpenAI's o3-mini task accuracy, dropping from 86.6% on the original I-RAVEN to just 17.0% -- approaching random chance -- on the more challenging I-RAVEN-X, which increases input length and range and emulates perceptual uncertainty. This drop occurred despite spending 3.4x more reasoning tokens. A similar trend is also observed for DeepSeek R1: from 80.6% to 23.2%. On the other hand, a neuro-symbolic probabilistic abductive model, ARLC, that achieves state-of-the-art performances on I-RAVEN, can robustly reason under all these out-of-distribution tests, maintaining strong accuracy with only a modest reduction from 98.6% to 88.0%. Our code is available at https://github.com/IBM/raven-large-language-models.", 'score': 2, 'issue_id': 2744, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '1910487e8b409ffb', 'authors': ['Giacomo Camposampiero', 'Michael Hersche', 'Roger Wattenhofer', 'Abu Sebastian', 'Abbas Rahimi'], 'affiliations': ['ETH Zürich', 'IBM Research - Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2503.11207.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#cv', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Крупные модели рассуждений уступают нейро-символическим подходам в аналогическом мышлении', 'desc': 'Эта работа представляет первую оценку двух современных моделей крупномасштабного рассуждения (LRM) - o3-mini от OpenAI и DeepSeek R1 - в задачах аналогического мышления на основе невербальных тестов IQ, использующих прогрессивные матрицы Равена. Исследователи провели бенчмаркинг на наборах данных I-RAVEN и его более сложном расширении I-RAVEN-X, которые проверяют способность обобщать более длинные правила рассуждения и диапазоны значений атрибутов. Для оценки влияния визуальной неопределенности авторы расширили набор данных I-RAVEN-X, введя случайные атрибуты и сглаживание распределений значений входных атрибутов. Результаты показали значительное снижение точности моделей LRM на более сложных тестах, в то время как нейро-символическая вероятностная модель ARLC продемонстрировала устойчивость к этим изменениям.'}, 'en': {'title': 'Evaluating Reasoning Under Uncertainty in Large Models', 'desc': "This paper evaluates two advanced Large Reasoning Models (LRMs), OpenAI's o3-mini and DeepSeek R1, on their ability to perform analogical reasoning using Raven's progressive matrices. The study uses the I-RAVEN dataset and its more challenging version, I-RAVEN-X, to test the models' generalization capabilities under visual uncertainties. The results show a significant drop in accuracy for both LRMs when faced with the more complex tasks, indicating their struggle with longer reasoning rules and perceptual noise. In contrast, a neuro-symbolic model, ARLC, demonstrates robust performance, maintaining high accuracy even under challenging conditions."}, 'zh': {'title': '大型推理模型在类比推理中的挑战与机遇', 'desc': '本文首次评估了两种先进的大型推理模型（LRMs），OpenAI的o3-mini和DeepSeek R1，在类比推理方面的表现，重点关注基于Raven渐进矩阵的非语言人类智商测试。我们使用I-RAVEN数据集及其更难的扩展版本I-RAVEN-X进行基准测试，后者测试模型对更长推理规则和属性值范围的泛化能力。为了评估视觉不确定性对这些非语言类比推理测试的影响，我们扩展了I-RAVEN-X数据集，并采用了两种策略来模拟不完美的视觉感知。结果显示，OpenAI的o3-mini在I-RAVEN-X上的任务准确率大幅下降，从86.6%降至仅17.0%，而ARLC模型在所有这些分布外测试中保持了强大的准确性。'}}}, {'id': 'https://huggingface.co/papers/2503.10684', 'title': 'Open-World Skill Discovery from Unsegmented Demonstrations', 'url': 'https://huggingface.co/papers/2503.10684', 'abstract': 'Learning skills in open-world environments is essential for developing agents capable of handling a variety of tasks by combining basic skills. Online demonstration videos are typically long but unsegmented, making them difficult to segment and label with skill identifiers. Unlike existing methods that rely on sequence sampling or human labeling, we have developed a self-supervised learning-based approach to segment these long videos into a series of semantic-aware and skill-consistent segments. Drawing inspiration from human cognitive event segmentation theory, we introduce Skill Boundary Detection (SBD), an annotation-free temporal video segmentation algorithm. SBD detects skill boundaries in a video by leveraging prediction errors from a pretrained unconditional action-prediction model. This approach is based on the assumption that a significant increase in prediction error indicates a shift in the skill being executed. We evaluated our method in Minecraft, a rich open-world simulator with extensive gameplay videos available online. Our SBD-generated segments improved the average performance of conditioned policies by 63.7% and 52.1% on short-term atomic skill tasks, and their corresponding hierarchical agents by 11.3% and 20.8% on long-horizon tasks. Our method can leverage the diverse YouTube videos to train instruction-following agents. The project page can be found in https://craftjarvis.github.io/SkillDiscovery.', 'score': 2, 'issue_id': 2740, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '004932028027606e', 'authors': ['Jingwen Deng', 'Zihao Wang', 'Shaofei Cai', 'Anji Liu', 'Yitao Liang'], 'affiliations': ['Peking University', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2503.10684.jpg', 'data': {'categories': ['#games', '#video', '#agents', '#open_source'], 'emoji': '🎮', 'ru': {'title': 'Автоматическая сегментация видео для обучения ИИ-агентов сложным навыкам', 'desc': 'Статья представляет новый метод самообучения для сегментации длинных видео на последовательности семантически связанных навыков. Авторы разработали алгоритм Skill Boundary Detection (SBD), который обнаруживает границы навыков в видео, используя ошибки предсказания предобученной модели. Метод был протестирован на видео игрового процесса Minecraft и показал значительное улучшение производительности обусловленных политик и иерархических агентов. Данный подход позволяет использовать разнообразные видео с YouTube для обучения агентов, выполняющих инструкции.'}, 'en': {'title': 'Segmenting Skills for Smarter Agents', 'desc': 'This paper presents a novel approach for segmenting long, unstructured demonstration videos into meaningful skill segments using self-supervised learning. The method, called Skill Boundary Detection (SBD), identifies transitions between skills by analyzing prediction errors from a pretrained action-prediction model. By applying this technique, the authors demonstrate significant improvements in the performance of agents trained on these segmented skills in the Minecraft environment. This approach allows for the effective use of diverse online video content to enhance the training of instruction-following agents without the need for manual labeling.'}, 'zh': {'title': '自监督学习助力技能边界检测', 'desc': '在开放世界环境中学习技能对于开发能够处理多种任务的智能体至关重要。我们提出了一种基于自监督学习的方法，可以将长视频分割成一系列语义明确且技能一致的片段，而无需人工标注。该方法通过检测预测误差来识别技能边界，假设预测误差的显著增加表明技能的转变。我们的实验表明，这种方法在Minecraft中显著提高了条件策略和层次代理的表现。'}}}, {'id': 'https://huggingface.co/papers/2503.05689', 'title': 'GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories\n  Generation in End-to-End Autonomous Driving', 'url': 'https://huggingface.co/papers/2503.05689', 'abstract': 'We propose GoalFlow, an end-to-end autonomous driving method for generating high-quality multimodal trajectories. In autonomous driving scenarios, there is rarely a single suitable trajectory. Recent methods have increasingly focused on modeling multimodal trajectory distributions. However, they suffer from trajectory selection complexity and reduced trajectory quality due to high trajectory divergence and inconsistencies between guidance and scene information. To address these issues, we introduce GoalFlow, a novel method that effectively constrains the generative process to produce high-quality, multimodal trajectories. To resolve the trajectory divergence problem inherent in diffusion-based methods, GoalFlow constrains the generated trajectories by introducing a goal point. GoalFlow establishes a novel scoring mechanism that selects the most appropriate goal point from the candidate points based on scene information. Furthermore, GoalFlow employs an efficient generative method, Flow Matching, to generate multimodal trajectories, and incorporates a refined scoring mechanism to select the optimal trajectory from the candidates. Our experimental results, validated on the NavsimDauner2024_navsim, demonstrate that GoalFlow achieves state-of-the-art performance, delivering robust multimodal trajectories for autonomous driving. GoalFlow achieved PDMS of 90.3, significantly surpassing other methods. Compared with other diffusion-policy-based methods, our approach requires only a single denoising step to obtain excellent performance. The code is available at https://github.com/YvanYin/GoalFlow.', 'score': 2, 'issue_id': 2731, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': 'eef61e2f2b4c0760', 'authors': ['Zebin Xing', 'Xingyu Zhang', 'Yang Hu', 'Bo Jiang', 'Tong He', 'Qian Zhang', 'Xiaoxiao Long', 'Wei Yin'], 'affiliations': ['Horizon Robotics', 'Huazhong University of Science & Technology', 'Nanjing University', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2503.05689.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#agents', '#multimodal'], 'emoji': '🚗', 'ru': {'title': 'GoalFlow: Точное планирование траекторий для автономного вождения', 'desc': 'GoalFlow - это новый метод автономного вождения для генерации высококачественных мультимодальных траекторий. Он решает проблему расхождения траекторий, вводя целевую точку и механизм оценки для выбора наиболее подходящей. GoalFlow использует эффективный генеративный метод Flow Matching для создания мультимодальных траекторий. Экспериментальные результаты показывают, что GoalFlow достигает передового уровня производительности, обеспечивая надежные мультимодальные траектории для автономного вождения.'}, 'en': {'title': 'GoalFlow: Driving the Future with High-Quality Multimodal Trajectories', 'desc': 'GoalFlow is a new method for generating high-quality multimodal trajectories in autonomous driving. It addresses the challenges of trajectory selection and quality by constraining the generative process with a goal point, which helps reduce trajectory divergence. The method uses a novel scoring mechanism to choose the best goal point based on the scene context, ensuring that the generated trajectories are relevant and effective. Experimental results show that GoalFlow outperforms existing methods, achieving state-of-the-art performance with fewer computational steps.'}, 'zh': {'title': 'GoalFlow：高质量多模态轨迹生成的创新方法', 'desc': '我们提出了GoalFlow，这是一种端到端的自动驾驶方法，用于生成高质量的多模态轨迹。在自动驾驶场景中，通常没有单一合适的轨迹，最近的方法越来越关注多模态轨迹分布的建模。为了克服轨迹选择的复杂性和轨迹质量下降的问题，GoalFlow通过引入目标点来有效约束生成过程，从而生成高质量的多模态轨迹。我们的实验结果表明，GoalFlow在NavsimDauner2024_navsim上实现了最先进的性能，提供了稳健的多模态轨迹。'}}}, {'id': 'https://huggingface.co/papers/2503.11629', 'title': 'TreeMeshGPT: Artistic Mesh Generation with Autoregressive Tree\n  Sequencing', 'url': 'https://huggingface.co/papers/2503.11629', 'abstract': 'We introduce TreeMeshGPT, an autoregressive Transformer designed to generate high-quality artistic meshes aligned with input point clouds. Instead of the conventional next-token prediction in autoregressive Transformer, we propose a novel Autoregressive Tree Sequencing where the next input token is retrieved from a dynamically growing tree structure that is built upon the triangle adjacency of faces within the mesh. Our sequencing enables the mesh to extend locally from the last generated triangular face at each step, and therefore reduces training difficulty and improves mesh quality. Our approach represents each triangular face with two tokens, achieving a compression rate of approximately 22% compared to the naive face tokenization. This efficient tokenization enables our model to generate highly detailed artistic meshes with strong point cloud conditioning, surpassing previous methods in both capacity and fidelity. Furthermore, our method generates mesh with strong normal orientation constraints, minimizing flipped normals commonly encountered in previous methods. Our experiments show that TreeMeshGPT enhances the mesh generation quality with refined details and normal orientation consistency.', 'score': 1, 'issue_id': 2744, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '5f56461a339a86bb', 'authors': ['Stefan Lionar', 'Jiabin Liang', 'Gim Hee Lee'], 'affiliations': ['Garena', 'National University of Singapore', 'Sea AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.11629.jpg', 'data': {'categories': ['#games', '#3d', '#optimization', '#architecture'], 'emoji': '🌳', 'ru': {'title': 'Генерация высококачественных 3D-сеток с помощью древовидной последовательности', 'desc': 'TreeMeshGPT - это авторегрессивный трансформер для генерации высококачественных художественных сеток, согласованных с входными облаками точек. Модель использует новый метод автоматической древовидной последовательности, где следующий токен ввода извлекается из динамически растущей древовидной структуры, построенной на основе смежности треугольников в сетке. Этот подход позволяет сетке локально расширяться от последней сгенерированной треугольной грани на каждом шаге, что снижает сложность обучения и улучшает качество сетки. TreeMeshGPT превосходит предыдущие методы по емкости и точности, генерируя детализированные сетки с сильным ограничением нормалей.'}, 'en': {'title': 'Revolutionizing Mesh Generation with TreeMeshGPT', 'desc': 'TreeMeshGPT is a novel autoregressive Transformer model that generates artistic meshes from point clouds. It introduces a unique Autoregressive Tree Sequencing method, which builds a dynamic tree structure based on the adjacency of triangular faces, allowing for local mesh extension during generation. This approach not only simplifies the training process but also enhances the quality of the generated meshes by achieving a 22% compression rate through efficient tokenization of triangular faces. Additionally, TreeMeshGPT ensures better normal orientation, reducing the occurrence of flipped normals and improving overall mesh fidelity and detail compared to previous techniques.'}, 'zh': {'title': 'TreeMeshGPT：高质量艺术网格生成的新方法', 'desc': '我们介绍了TreeMeshGPT，这是一种自回归Transformer，旨在生成与输入点云对齐的高质量艺术网格。与传统的自回归Transformer的下一个标记预测不同，我们提出了一种新颖的自回归树序列化方法，通过动态增长的树结构来检索下一个输入标记。我们的序列化方法使得网格能够在每一步从最后生成的三角面局部扩展，从而降低训练难度并提高网格质量。此外，我们的模型通过将每个三角面表示为两个标记，实现了约22%的压缩率，生成的网格在细节和法线方向一致性方面优于以往方法。'}}}, {'id': 'https://huggingface.co/papers/2503.10620', 'title': 'From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM', 'url': 'https://huggingface.co/papers/2503.10620', 'abstract': "Large language models (LLMs) have shown remarkable performance and generalization capabilities across multiple languages and tasks, making them very attractive targets for multi-modality integration (e.g., images or speech). In this work, we extend an existing LLM to the speech modality via speech discretization and continued pre-training. In particular, we are interested in multilingual LLMs, such as TOWER, as their pre-training setting allows us to treat discretized speech input as an additional translation language. The resulting open-source model, SPIRE, is able to transcribe and translate English speech input while maintaining TOWER's original performance on translation-related tasks, showcasing that discretized speech input integration as an additional language is feasible during LLM adaptation. We make our code and models available to the community.", 'score': 1, 'issue_id': 2739, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '6fc0a5f12205a8bd', 'authors': ['Kshitij Ambilduke', 'Ben Peters', 'Sonal Sannigrahi', 'Anil Keshwani', 'Tsz Kin Lam', 'Bruno Martins', 'Marcely Zanon Boito', 'André F. T. Martins'], 'affiliations': ['ELLIS Unit Lisbon', 'INESC-ID', 'Instituto Superior Técnico, Universidade de Lisboa', 'Instituto de Telecomunicações', 'NAVER LABS Europe', 'Paris-Saclay University', 'Sapienza University of Rome', 'Unbabel', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2503.10620.jpg', 'data': {'categories': ['#multilingual', '#machine_translation', '#open_source', '#multimodal', '#low_resource'], 'emoji': '🗣️', 'ru': {'title': 'Расширение языковых моделей на речевую модальность', 'desc': 'В этой статье описывается расширение возможностей большой языковой модели (LLM) для работы с речью путем дискретизации речевого сигнала и дополнительного предобучения. Авторы интегрируют дискретизированный речевой ввод как дополнительный язык в многоязычную модель TOWER. Полученная модель SPIRE способна транскрибировать и переводить английскую речь, сохраняя при этом исходную производительность TOWER в задачах перевода. Исследование демонстрирует возможность интеграции дискретизированного речевого ввода как дополнительного языка при адаптации LLM.'}, 'en': {'title': 'Integrating Speech into Multilingual LLMs for Enhanced Performance', 'desc': 'This paper discusses the enhancement of large language models (LLMs) by integrating speech as a new modality. The authors focus on multilingual LLMs, specifically TOWER, and propose a method to convert speech into a format that the model can understand. They introduce a new model called SPIRE, which can transcribe and translate English speech while preserving the original capabilities of TOWER. The research demonstrates that incorporating discretized speech as an additional language is a viable approach for adapting LLMs, and the authors provide their code and models for public use.'}, 'zh': {'title': '将语音融入大型语言模型的创新之路', 'desc': '大型语言模型（LLMs）在多种语言和任务中表现出色，具有很强的泛化能力，适合与多模态（如图像或语音）结合。本文将现有的LLM扩展到语音模态，通过语音离散化和持续预训练来实现。我们特别关注多语言LLM，例如TOWER，因为它的预训练设置允许我们将离散化的语音输入视为额外的翻译语言。最终生成的开源模型SPIRE能够转录和翻译英语语音输入，同时保持TOWER在翻译相关任务上的原始性能，证明了在LLM适应过程中将离散语音输入作为额外语言的可行性。'}}}, {'id': 'https://huggingface.co/papers/2503.08111', 'title': 'MaRI: Material Retrieval Integration across Domains', 'url': 'https://huggingface.co/papers/2503.08111', 'abstract': 'Accurate material retrieval is critical for creating realistic 3D assets. Existing methods rely on datasets that capture shape-invariant and lighting-varied representations of materials, which are scarce and face challenges due to limited diversity and inadequate real-world generalization. Most current approaches adopt traditional image search techniques. They fall short in capturing the unique properties of material spaces, leading to suboptimal performance in retrieval tasks. Addressing these challenges, we introduce MaRI, a framework designed to bridge the feature space gap between synthetic and real-world materials. MaRI constructs a shared embedding space that harmonizes visual and material attributes through a contrastive learning strategy by jointly training an image and a material encoder, bringing similar materials and images closer while separating dissimilar pairs within the feature space. To support this, we construct a comprehensive dataset comprising high-quality synthetic materials rendered with controlled shape variations and diverse lighting conditions, along with real-world materials processed and standardized using material transfer techniques. Extensive experiments demonstrate the superior performance, accuracy, and generalization capabilities of MaRI across diverse and complex material retrieval tasks, outperforming existing methods.', 'score': 1, 'issue_id': 2738, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': 'f3b97bb0031c6d57', 'authors': ['Jianhui Wang', 'Zhifei Yang', 'Yangfan He', 'Huixiong Zhang', 'Yuxuan Chen', 'Jingwei Huang'], 'affiliations': ['Fudan University', 'Peking University', 'Tencent Hunyuan3D', 'University of Electronic Science and Technology of China', 'University of Minnesota'], 'pdf_title_img': 'assets/pdf/title_img/2503.08111.jpg', 'data': {'categories': ['#synthetic', '#optimization', '#dataset', '#cv', '#3d'], 'emoji': '🔍', 'ru': {'title': 'MaRI: Революция в поиске 3D-материалов', 'desc': 'Статья представляет MaRI - фреймворк для улучшения поиска реалистичных 3D-материалов. Он создает общее пространство признаков для синтетических и реальных материалов с помощью контрастивного обучения. Авторы также собрали набор данных из высококачественных синтетических материалов и обработанных реальных материалов. Эксперименты показывают, что MaRI превосходит существующие методы в задачах поиска материалов.'}, 'en': {'title': 'Bridging the Gap in Material Retrieval with MaRI', 'desc': 'This paper presents MaRI, a novel framework aimed at improving material retrieval for realistic 3D asset creation. It addresses the limitations of existing methods that rely on traditional image search techniques, which often fail to capture the unique properties of materials. By utilizing a contrastive learning strategy, MaRI creates a shared embedding space that aligns visual and material attributes, enhancing the retrieval process. The framework is supported by a comprehensive dataset of synthetic and real-world materials, demonstrating superior performance in accuracy and generalization across various retrieval tasks.'}, 'zh': {'title': 'MaRI：提升材料检索的智能框架', 'desc': '准确的材料检索对于创建真实的3D资产至关重要。现有方法依赖于捕捉形状不变和光照变化的材料表示的数据集，这些数据集稀缺且面临多样性不足和现实世界泛化不良的挑战。我们提出了MaRI框架，旨在弥合合成材料和真实材料之间的特征空间差距。通过对比学习策略，MaRI构建了一个共享的嵌入空间，使得相似的材料和图像在特征空间中更接近，同时将不相似的对分开，从而提高了材料检索的性能和准确性。'}}}, {'id': 'https://huggingface.co/papers/2503.09330', 'title': 'Group-robust Machine Unlearning', 'url': 'https://huggingface.co/papers/2503.09330', 'abstract': 'Machine unlearning is an emerging paradigm to remove the influence of specific training data (i.e., the forget set) from a model while preserving its knowledge of the rest of the data (i.e., the retain set). Previous approaches assume the forget data to be uniformly distributed from all training datapoints. However, if the data to unlearn is dominant in one group, we empirically show that performance for this group degrades, leading to fairness issues. This work tackles the overlooked problem of non-uniformly distributed forget sets, which we call group-robust machine unlearning, by presenting a simple, effective strategy that mitigates the performance loss in dominant groups via sample distribution reweighting. Moreover, we present MIU (Mutual Information-aware Machine Unlearning), the first approach for group robustness in approximate machine unlearning. MIU minimizes the mutual information between model features and group information, achieving unlearning while reducing performance degradation in the dominant group of the forget set. Additionally, MIU exploits sample distribution reweighting and mutual information calibration with the original model to preserve group robustness. We conduct experiments on three datasets and show that MIU outperforms standard methods, achieving unlearning without compromising model robustness. Source code available at https://github.com/tdemin16/group-robust_machine_unlearning.', 'score': 0, 'issue_id': 2739, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '64a6c82be0db725d', 'authors': ['Thomas De Min', 'Subhankar Roy', 'Stéphane Lathuilière', 'Elisa Ricci', 'Massimiliano Mancini'], 'affiliations': ['Fondazione Bruno Kessler', 'Inria Grenoble, Univ. Grenoble Alpes', 'LTCI, Telecom Paris, Institut Polytechnique de Paris', 'University of Trento'], 'pdf_title_img': 'assets/pdf/title_img/2503.09330.jpg', 'data': {'categories': ['#training', '#dataset', '#ethics', '#data'], 'emoji': '🧠', 'ru': {'title': 'Групповое разобучение без потери устойчивости', 'desc': 'Статья посвящена проблеме группового машинного разобучения, когда данные для удаления из модели неравномерно распределены между группами. Авторы предлагают метод MIU, который минимизирует взаимную информацию между признаками модели и информацией о группах. MIU использует перевзвешивание распределения выборки и калибровку взаимной информации с исходной моделью для сохранения групповой устойчивости. Эксперименты на трех наборах данных показывают, что MIU превосходит стандартные методы, достигая разобучения без ущерба для устойчивости модели.'}, 'en': {'title': 'Fair and Effective Machine Unlearning for Diverse Data Groups', 'desc': 'This paper introduces the concept of group-robust machine unlearning, which addresses the challenge of removing specific training data from a model while maintaining its performance across different groups. It highlights the issue of fairness when the data to be unlearned is not uniformly distributed, leading to performance degradation in dominant groups. The authors propose a novel method called MIU (Mutual Information-aware Machine Unlearning) that minimizes the mutual information between model features and group information, thus enhancing unlearning effectiveness. Through experiments on various datasets, MIU demonstrates superior performance compared to traditional methods, ensuring robust model performance even after unlearning.'}, 'zh': {'title': '组鲁棒性机器遗忘：公平性与性能的平衡', 'desc': '机器遗忘是一种新兴的范式，旨在从模型中去除特定训练数据的影响，同时保留其对其他数据的知识。以往的方法假设遗忘数据均匀分布，但如果要遗忘的数据在某一组中占主导地位，模型在该组的性能会下降，导致公平性问题。本文提出了一种简单有效的策略，通过样本分布重加权来缓解主导组的性能损失，解决了非均匀分布遗忘集的问题。我们还提出了MIU（互信息感知机器遗忘），这是首个针对近似机器遗忘的组鲁棒性方法，能够在减少主导组性能下降的同时实现遗忘。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (4)', '#agents (5)', '#agi', '#alignment (2)', '#architecture (7)', '#audio', '#benchmark (8)', '#cv (10)', '#data (4)', '#dataset (9)', '#diffusion (4)', '#ethics (1)', '#games (5)', '#graphs', '#hallucinations', '#healthcare (2)', '#inference (1)', '#interpretability (1)', '#leakage (1)', '#long_context (2)', '#low_resource (1)', '#machine_translation (1)', '#math (1)', '#multilingual (1)', '#multimodal (9)', '#open_source (8)', '#optimization (11)', '#plp', '#rag', '#reasoning (3)', '#rl', '#rlhf', '#robotics (1)', '#science (3)', '#security (1)', '#small_models (1)', '#story_generation', '#survey (3)', '#synthetic (2)', '#training (13)', '#transfer_learning', '#video (6)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-03-17 20:12',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-17 20:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-17 20:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    