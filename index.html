
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 39 papers. June 5.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">5 июня</span> | <span id="title-articles-count">39 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-06-04.html">⬅️ <span id="prev-date">04.06</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-06-06.html">➡️ <span id="next-date">06.06</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-06.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'};
        let feedDateNext = {'ru': '06.06', 'en': '06/06', 'zh': '6月6日'};
        let feedDatePrev = {'ru': '04.06', 'en': '06/04', 'zh': '6月4日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.03569', 'title': 'MiMo-VL Technical Report', 'url': 'https://huggingface.co/papers/2506.03569', 'abstract': 'We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language models delivering state-of-the-art performance in both general visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing models with up to 78B parameters. For GUI grounding applications, it sets a new standard with 56.1 on OSWorld-G, even outperforming specialized models such as UI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens) with Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward signals. We identify the importance of incorporating high-quality reasoning data with long Chain-of-Thought into pre-training stages, and the benefits of mixed RL despite challenges in simultaneous multi-domain optimization. We also contribute a comprehensive evaluation suite covering 50+ tasks to promote reproducibility and advance the field. The model checkpoints and full evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.', 'score': 49, 'issue_id': 4133, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'cb568276c7e799cb', 'authors': ['Xiaomi LLM-Core Team', ':', 'Zihao Yue', 'Zhenru Lin', 'Yifan Song', 'Weikun Wang', 'Shuhuai Ren', 'Shuhao Gu', 'Shicheng Li', 'Peidian Li', 'Liang Zhao', 'Lei Li', 'Kainan Bao', 'Hao Tian', 'Hailin Zhang', 'Gang Wang', 'Dawei Zhu', 'Cici', 'Chenhong He', 'Bowen Ye', 'Bowen Shen', 'Zihan Zhang', 'Zihan Jiang', 'Zhixian Zheng', 'Zhichao Song', 'Zhenbo Luo', 'Yue Yu', 'Yudong Wang', 'Yuanyuan Tian', 'Yu Tu', 'Yihan Yan', 'Yi Huang', 'Xu Wang', 'Xinzhe Xu', 'Xingchen Song', 'Xing Zhang', 'Xing Yong', 'Xin Zhang', 'Xiangwei Deng', 'Wenyu Yang', 'Wenhan Ma', 'Weiwei Lv', 'Weiji Zhuang', 'Wei Liu', 'Sirui Deng', 'Shuo Liu', 'Shimao Chen', 'Shihua Yu', 'Shaohui Liu', 'Shande Wang', 'Rui Ma', 'Qiantong Wang', 'Peng Wang', 'Nuo Chen', 'Menghang Zhu', 'Kangyang Zhou', 'Kang Zhou', 'Kai Fang', 'Jun Shi', 'Jinhao Dong', 'Jiebao Xiao', 'Jiaming Xu', 'Huaqiu Liu', 'Hongshen Xu', 'Heng Qu', 'Haochen Zhao', 'Hanglong Lv', 'Guoan Wang', 'Duo Zhang', 'Dong Zhang', 'Di Zhang', 'Chong Ma', 'Chang Liu', 'Can Cai', 'Bingquan Xia'], 'affiliations': ['Xiaomi'], 'pdf_title_img': 'assets/pdf/title_img/2506.03569.jpg', 'data': {'categories': ['#training', '#rl', '#reasoning', '#multimodal', '#rlhf', '#benchmark', '#dataset', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Прорыв в мультимодальном ИИ: MiMo-VL устанавливает новые стандарты', 'desc': 'Исследователи представили две мощные мультимодальные модели MiMo-VL-7B-SFT и MiMo-VL-7B-RL, демонстрирующие передовые результаты в задачах визуального понимания и мультимодальных рассуждений. Модель MiMo-VL-7B-RL превосходит Qwen2.5-VL-7B в 35 из 40 оцениваемых задач и достигает 59.4 баллов на бенчмарке OlympiadBench. Обучение моделей включало четырехэтапное предобучение на 2.4 триллионах токенов и применение смешанного обучения с подкреплением (MORL). Авторы подчеркивают важность включения качественных данных для рассуждений с длинной цепочкой мыслей в этапы предобучения.'}, 'en': {'title': 'Revolutionizing Vision-Language Models with MiMo-VL', 'desc': 'The paper introduces two advanced vision-language models, MiMo-VL-7B-SFT and MiMo-VL-7B-RL, which excel in visual understanding and multimodal reasoning tasks. MiMo-VL-7B-RL demonstrates superior performance, outperforming other models on a majority of evaluated tasks and achieving high scores on benchmark datasets. The training methodology involves a four-stage pre-training process using a massive dataset and incorporates Mixed On-policy Reinforcement Learning to enhance model performance through diverse reward signals. Additionally, the authors emphasize the significance of high-quality reasoning data and provide a comprehensive evaluation suite to facilitate reproducibility in future research.'}, 'zh': {'title': '开创视觉-语言模型的新标准', 'desc': '我们开源了MiMo-VL-7B-SFT和MiMo-VL-7B-RL，这两个强大的视觉-语言模型在一般视觉理解和多模态推理方面表现出色。MiMo-VL-7B-RL在40个评估任务中有35个超越了Qwen2.5-VL-7B，并在OlympiadBench上得分59.4，超过了参数高达78B的模型。在GUI定位应用中，它在OSWorld-G上以56.1的分数设定了新标准，甚至超越了专门模型UI-TARS。我们的训练结合了四阶段的预训练（24万亿个标记）和混合在线强化学习（MORL），并强调了在预训练阶段融入高质量推理数据和长链思维的重要性。'}}}, {'id': 'https://huggingface.co/papers/2506.04207', 'title': 'Advancing Multimodal Reasoning: From Optimized Cold Start to Staged\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.04207', 'abstract': 'Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. In this paper, rather than examining multimodal RL in isolation, we delve into current training pipelines and identify three crucial phenomena: 1) Effective cold start initialization is critical for enhancing MLLM reasoning. Intriguingly, we find that initializing with carefully selected text data alone can lead to performance surpassing many recent multimodal reasoning models, even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers from gradient stagnation, which degrades training stability and performance. 3) Subsequent text-only RL training, following the multimodal RL phase, further enhances multimodal reasoning. This staged training approach effectively balances perceptual grounding and cognitive reasoning development. By incorporating the above insights and addressing multimodal RL issues, we introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath, LogicVista, DynaMath, and challenging AIME2024 and AIME2025.', 'score': 37, 'issue_id': 4135, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '61521f9ed974c930', 'authors': ['Shuang Chen', 'Yue Guo', 'Zhaochen Su', 'Yafu Li', 'Yulun Wu', 'Jiacheng Chen', 'Jiayu Chen', 'Weijie Wang', 'Xiaoye Qu', 'Yu Cheng'], 'affiliations': ['Fudan University', 'Shanghai AI Laboratory', 'Soochow University', 'The Chinese University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04207.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#multimodal', '#training', '#benchmark', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Улучшение рассуждений MLLM: от инициализации до многоэтапного RL', 'desc': 'Статья посвящена улучшению способностей мультимодальных больших языковых моделей (MLLM) к рассуждению с помощью обучения с подкреплением (RL). Авторы выявили три ключевых феномена в процессе обучения: важность правильной инициализации, проблему стагнации градиентов при стандартном GRPO и эффективность последующего текстового RL. На основе этих наблюдений была разработана модель ReVisual-R1, достигшая нового уровня производительности среди открытых 7B MLLM на сложных бенчмарках.'}, 'en': {'title': 'Unlocking Reasoning in MLLMs with Smart Training Strategies', 'desc': 'This paper explores how to improve reasoning in Multimodal Large Language Models (MLLMs) by analyzing their training processes. It identifies that starting with well-chosen text data can significantly boost reasoning capabilities, even before applying multimodal reinforcement learning (RL). The authors also highlight that traditional gradient-based methods in multimodal RL can lead to stagnation, negatively impacting training effectiveness. By implementing a staged training approach that combines text-only RL after multimodal RL, they introduce ReVisual-R1, which sets new performance records on various complex benchmarks.'}, 'zh': {'title': '提升多模态推理的新方法', 'desc': '本文探讨了如何通过强化学习（RL）提升多模态大型语言模型（MLLM）的推理能力。研究发现，良好的冷启动初始化对于增强MLLM的推理至关重要，单独使用精心选择的文本数据即可超越许多近期的多模态推理模型。标准的GRPO在多模态RL中存在梯度停滞的问题，影响了训练的稳定性和性能。通过分阶段的训练方法，结合文本和多模态RL，提出了ReVisual-R1，达到了开源7B MLLM在多个基准测试中的新状态。'}}}, {'id': 'https://huggingface.co/papers/2506.04089', 'title': 'AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment', 'url': 'https://huggingface.co/papers/2506.04089', 'abstract': 'AmbiK, a textual dataset of ambiguous instructions for kitchen robots, enables unified comparison of ambiguity detection methods.  \t\t\t\t\tAI-generated summary \t\t\t\t As a part of an embodied agent, Large Language Models (LLMs) are typically used for behavior planning given natural language instructions from the user. However, dealing with ambiguous instructions in real-world environments remains a challenge for LLMs. Various methods for task ambiguity detection have been proposed. However, it is difficult to compare them because they are tested on different datasets and there is no universal benchmark. For this reason, we propose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual dataset of ambiguous instructions addressed to a robot in a kitchen environment. AmbiK was collected with the assistance of LLMs and is human-validated. It comprises 1000 pairs of ambiguous tasks and their unambiguous counterparts, categorized by ambiguity type (Human Preferences, Common Sense Knowledge, Safety), with environment descriptions, clarifying questions and answers, user intents, and task plans, for a total of 2000 tasks. We hope that AmbiK will enable researchers to perform a unified comparison of ambiguity detection methods. AmbiK is available at https://github.com/cog-model/AmbiK-dataset.', 'score': 33, 'issue_id': 4140, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'e5facb42d11447ff', 'authors': ['Anastasiia Ivanova', 'Eva Bakaeva', 'Zoya Volovikova', 'Alexey K. Kovalev', 'Aleksandr I. Panov'], 'affiliations': ['AIRI, Moscow, Russia', 'LMU, Munich, Germany', 'MIPT, Dolgoprudny, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2506.04089.jpg', 'data': {'categories': ['#interpretability', '#agents', '#data', '#dataset', '#alignment'], 'emoji': '🍳', 'ru': {'title': 'AmbiK: унифицированный бенчмарк для обнаружения неоднозначности в инструкциях для роботов', 'desc': 'AmbiK - это текстовый датасет неоднозначных инструкций для кухонных роботов, созданный для унифицированного сравнения методов обнаружения неоднозначности. Датасет содержит 1000 пар неоднозначных задач и их однозначных аналогов, классифицированных по типу неоднозначности. AmbiK был собран с помощью больших языковых моделей (LLM) и проверен людьми. Он включает описания окружения, уточняющие вопросы и ответы, намерения пользователей и планы задач.'}, 'en': {'title': 'AmbiK: A Unified Benchmark for Ambiguity Detection in Kitchen Robotics', 'desc': 'The paper introduces AmbiK, a new dataset designed to help researchers evaluate how well different methods can detect ambiguous instructions for kitchen robots. AmbiK contains 1000 pairs of ambiguous and clear tasks, categorized by types of ambiguity such as human preferences and safety concerns. This dataset is unique because it allows for standardized testing of various ambiguity detection techniques, which have previously been difficult to compare due to differing datasets. By providing a common benchmark, AmbiK aims to advance the development of more effective language models in handling real-world instructions.'}, 'zh': {'title': '统一比较模糊性检测方法的AmbiK数据集', 'desc': 'AmbiK是一个针对厨房机器人模糊指令的文本数据集，旨在统一比较模糊性检测方法。该数据集包含1000对模糊任务及其明确对应任务，涵盖人类偏好、常识知识和安全等模糊性类型。AmbiK由大型语言模型（LLMs）协助收集，并经过人工验证，提供环境描述、澄清问题及答案、用户意图和任务计划等信息。我们希望AmbiK能帮助研究人员进行模糊性检测方法的统一比较。'}}}, {'id': 'https://huggingface.co/papers/2505.16968', 'title': 'CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark', 'url': 'https://huggingface.co/papers/2505.16968', 'abstract': 'CASS is a dataset and model suite for GPU code transpilation at both source and assembly levels, achieving high accuracy and performance matching with native code.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce CASS, the first large-scale dataset and model suite for cross-architecture GPU code transpilation, targeting both source-level (CUDA leftrightarrow HIP) and assembly-level (Nvidia SASS leftrightarrow AMD RDNA3) translation. The dataset comprises 70k verified code pairs across host and device, addressing a critical gap in low-level GPU code portability. Leveraging this resource, we train the CASS family of domain-specific language models, achieving 95% source translation accuracy and 37.5% assembly translation accuracy, substantially outperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our generated code matches native performance in over 85% of test cases, preserving runtime and memory behavior. To support rigorous evaluation, we introduce CASS-Bench, a curated benchmark spanning 16 GPU domains with ground-truth execution. All data, models, and evaluation tools are released as open source to foster progress in GPU compiler tooling, binary compatibility, and LLM-guided hardware translation. Dataset and benchmark are on https://huggingface.co/datasets/MBZUAI/cass{blue{HuggingFace}}, with code at https://github.com/GustavoStahl/CASS{blue{GitHub}}.', 'score': 27, 'issue_id': 4139, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': 'a069288c85761286', 'authors': ['Ahmed Heakl', 'Sarim Hashmi', 'Gustavo Bertolo Stahl', 'Seung Hun Eddie Han', 'Salman Khan', 'Abdulrahman Mahmoud'], 'affiliations': ['Australian National University', 'MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2505.16968.jpg', 'data': {'categories': ['#dataset', '#low_resource', '#benchmark', '#open_source'], 'emoji': '🔄', 'ru': {'title': 'CASS: Преодоление барьеров между GPU-архитектурами', 'desc': 'CASS представляет собой набор данных и набор моделей для транспиляции GPU-кода между архитектурами как на уровне исходного кода, так и на уровне ассемблера. Модели CASS достигают высокой точности перевода: 95% для исходного кода и 37.5% для ассемблера, превосходя коммерческие решения. Сгенерированный код соответствует производительности нативного кода в более чем 85% тестовых случаев. Авторы также представили CASS-Bench - набор тестов для оценки качества транспиляции GPU-кода.'}, 'en': {'title': 'CASS: Bridging GPU Code Portability with High Accuracy Transpilation', 'desc': 'CASS is a groundbreaking dataset and model suite designed for GPU code transpilation, focusing on both source-level and assembly-level translations. It includes 70,000 verified code pairs that facilitate the conversion of code between different GPU architectures, addressing the challenge of low-level code portability. The CASS models achieve impressive accuracy rates, with 95% for source translation and 37.5% for assembly translation, significantly surpassing existing commercial solutions. Additionally, the generated code maintains native performance in over 85% of cases, and the accompanying CASS-Bench provides a robust evaluation framework for various GPU domains.'}, 'zh': {'title': 'CASS：GPU代码转译的突破性进展', 'desc': 'CASS是一个用于GPU代码转译的数据集和模型套件，支持源代码和汇编级别的转译。它包含70,000对经过验证的代码对，解决了低级GPU代码可移植性的重要问题。通过训练CASS系列特定领域语言模型，我们在源代码转译中达到了95%的准确率，并在汇编转译中达到了37.5%的准确率。CASS生成的代码在超过85%的测试案例中与本地性能相匹配，保持了运行时和内存行为的一致性。'}}}, {'id': 'https://huggingface.co/papers/2506.02921', 'title': 'A Controllable Examination for Long-Context Language Models', 'url': 'https://huggingface.co/papers/2506.02921', 'abstract': 'LongBioBench is a new benchmark using artificially generated biographies to evaluate long-context language models across understanding, reasoning, and trustworthiness dimensions, addressing limitations in existing frameworks.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing frameworks for evaluating long-context language models (LCLM) can be broadly categorized into real-world and synthetic tasks. Despite their utility, both approaches are accompanied by certain intrinsic limitations. Real-world tasks are too complex to interpret or characterize and are susceptible to data contamination. In contrast, synthetic tasks often adopt the needle-in-the-haystack (NIAH) format, wherein a lack of coherence between the "needle" and the "haystack" compromises their validity as proxies for realistic applications. In response to these challenges, we posit that an ideal long-context evaluation framework should be characterized by three essential features: seamless context, controllable setting, and sound evaluation. This study introduces LongBioBench, a novel benchmark that utilizes artificially generated biographies as a controlled environment for assessing LCLMs across dimensions of understanding, reasoning, and trustworthiness. Our experimental evaluation, which includes 18 LCLMs in total, demonstrates that most models still exhibit deficiencies in semantic understanding and elementary reasoning over retrieved results and are less trustworthy as context length increases. Our further analysis indicates some design choices employed by existing synthetic benchmarks, such as contextual non-coherence, numerical needles, and the absence of distractors, rendering them vulnerable to test the model long-context capabilities. Moreover, we also reveal that long-context continual pretraining primarily adjusts RoPE embedding to accommodate extended context lengths. To sum up, compared to previous synthetic benchmarks, LongBioBench achieves a better trade-off between mirroring authentic language tasks and maintaining controllability, and is highly interpretable and configurable.', 'score': 26, 'issue_id': 4134, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '073ae66fedf9c141', 'authors': ['Yijun Yang', 'Zeyu Huang', 'Wenhao Zhu', 'Zihan Qiu', 'Fei Yuan', 'Jeff Z. Pan', 'Ivan Titov'], 'affiliations': ['Nanjing University', 'Qwen Team, Alibaba Group', 'Shanghai Artificial Intelligence Laboratory', 'University of Amsterdam', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2506.02921.jpg', 'data': {'categories': ['#benchmark', '#synthetic', '#long_context', '#reasoning', '#interpretability'], 'emoji': '📊', 'ru': {'title': 'LongBioBench: Новый стандарт оценки языковых моделей с длинным контекстом', 'desc': 'LongBioBench - это новый бенчмарк для оценки языковых моделей с длинным контекстом, использующий искусственно сгенерированные биографии. Он оценивает модели по трем аспектам: понимание, рассуждение и надежность. Бенчмарк создан для преодоления ограничений существующих методов оценки, таких как сложность интерпретации реальных задач и недостаток когерентности в синтетических тестах. Эксперименты показали, что большинство моделей все еще имеют проблемы с семантическим пониманием и элементарными рассуждениями, а также становятся менее надежными при увеличении длины контекста.'}, 'en': {'title': 'LongBioBench: A New Standard for Evaluating Long-Context Language Models', 'desc': 'LongBioBench is a new benchmark designed to evaluate long-context language models (LCLMs) using artificially generated biographies. It addresses the limitations of existing evaluation frameworks by providing a controlled environment that emphasizes understanding, reasoning, and trustworthiness. The study reveals that many LCLMs struggle with semantic understanding and reasoning as context length increases, highlighting the need for better evaluation methods. LongBioBench offers a more coherent and interpretable approach compared to previous synthetic benchmarks, making it a valuable tool for assessing LCLMs.'}, 'zh': {'title': 'LongBioBench：评估长上下文语言模型的新基准', 'desc': 'LongBioBench 是一个新的基准，利用人工生成的传记来评估长上下文语言模型（LCLM）在理解、推理和可信度方面的表现，解决了现有框架的局限性。现有的评估框架分为真实世界任务和合成任务，但两者都有内在的缺陷。真实世界任务复杂且易受数据污染，而合成任务常常缺乏连贯性，影响其作为现实应用的有效性。LongBioBench 提供了一个受控环境，能够更好地评估 LCLM 的能力，实验结果显示大多数模型在语义理解和基本推理上仍存在不足。'}}}, {'id': 'https://huggingface.co/papers/2506.04180', 'title': 'SuperWriter: Reflection-Driven Long-Form Generation with Large Language\n  Models', 'url': 'https://huggingface.co/papers/2506.04180', 'abstract': 'Long-form text generation remains a significant challenge for large language models (LLMs), particularly in maintaining coherence, ensuring logical consistency, and preserving text quality as sequence length increases. To address these limitations, we propose SuperWriter-Agent, an agent-based framework designed to enhance the quality and consistency of long-form text generation. SuperWriter-Agent introduces explicit structured thinking-through planning and refinement stages into the generation pipeline, guiding the model to follow a more deliberate and cognitively grounded process akin to that of a professional writer. Based on this framework, we construct a supervised fine-tuning dataset to train a 7B SuperWriter-LM. We further develop a hierarchical Direct Preference Optimization (DPO) procedure that uses Monte Carlo Tree Search (MCTS) to propagate final quality assessments and optimize each generation step accordingly. Empirical results across diverse benchmarks demonstrate that SuperWriter-LM achieves state-of-the-art performance, surpassing even larger-scale baseline models in both automatic evaluation and human evaluation. Furthermore, comprehensive ablation studies demonstrate the effectiveness of hierarchical DPO and underscore the value of incorporating structured thinking steps to improve the quality of long-form text generation.', 'score': 20, 'issue_id': 4133, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '3f52b337c5fa3683', 'authors': ['Yuhao Wu', 'Yushi Bai', 'Zhiqiang Hu', 'Juanzi Li', 'Roy Ka-Wei Lee'], 'affiliations': ['Singapore University of Technology and Design, Singapore', 'Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.04180.jpg', 'data': {'categories': ['#training', '#optimization', '#agents', '#long_context', '#rlhf', '#benchmark', '#dataset', '#story_generation'], 'emoji': '✍️', 'ru': {'title': 'Структурированное мышление для улучшения генерации длинных текстов', 'desc': 'SuperWriter-Agent - это новая система для улучшения качества генерации длинных текстов с помощью больших языковых моделей (LLM). Она вводит этапы планирования и уточнения в процесс генерации, имитируя подход профессионального писателя. Авторы обучили 7B-параметровую модель SuperWriter-LM на специально созданном наборе данных и разработали иерархическую процедуру оптимизации предпочтений (DPO) с использованием метода Монте-Карло. Эмпирические результаты показывают, что SuperWriter-LM превосходит более крупные базовые модели по автоматическим и человеческим оценкам.'}, 'en': {'title': 'Elevating Long-Form Text Generation with Structured Thinking', 'desc': 'This paper presents SuperWriter-Agent, a novel framework aimed at improving long-form text generation by large language models (LLMs). It introduces structured thinking through planning and refinement stages, which helps the model generate more coherent and logically consistent text. The framework is supported by a supervised fine-tuning dataset for training a 7B parameter model called SuperWriter-LM. Additionally, a hierarchical Direct Preference Optimization (DPO) method is employed, utilizing Monte Carlo Tree Search to enhance the quality of generated text, leading to superior performance on various benchmarks.'}, 'zh': {'title': '提升长文本生成质量的智能代理', 'desc': '长文本生成是大型语言模型（LLMs）面临的重要挑战，尤其是在保持连贯性、逻辑一致性和文本质量方面。为了解决这些问题，我们提出了SuperWriter-Agent，这是一个基于代理的框架，旨在提高长文本生成的质量和一致性。该框架通过规划和精炼阶段引入明确的结构化思维，指导模型遵循更有意识和认知基础的过程，类似于专业作家的写作方式。实验结果表明，SuperWriter-LM在多个基准测试中表现出色，超越了更大规模的基线模型，证明了分层直接偏好优化（DPO）和结构化思维步骤的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.01320', 'title': 'Ψ-Sampler: Initial Particle Sampling for SMC-Based Inference-Time\n  Reward Alignment in Score Models', 'url': 'https://huggingface.co/papers/2506.01320', 'abstract': 'We introduce Psi-Sampler, an SMC-based framework incorporating pCNL-based initial particle sampling for effective inference-time reward alignment with a score-based generative model. Inference-time reward alignment with score-based generative models has recently gained significant traction, following a broader paradigm shift from pre-training to post-training optimization. At the core of this trend is the application of Sequential Monte Carlo (SMC) to the denoising process. However, existing methods typically initialize particles from the Gaussian prior, which inadequately captures reward-relevant regions and results in reduced sampling efficiency. We demonstrate that initializing from the reward-aware posterior significantly improves alignment performance. To enable posterior sampling in high-dimensional latent spaces, we introduce the preconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines dimension-robust proposals with gradient-informed dynamics. This approach enables efficient and scalable posterior sampling and consistently improves performance across various reward alignment tasks, including layout-to-image generation, quantity-aware generation, and aesthetic-preference generation, as demonstrated in our experiments.', 'score': 15, 'issue_id': 4136, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '6441248200df695a', 'authors': ['Taehoon Yoon', 'Yunhong Min', 'Kyeongmin Yeo', 'Minhyuk Sung'], 'affiliations': ['KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2506.01320.jpg', 'data': {'categories': ['#inference', '#alignment', '#rlhf', '#optimization'], 'emoji': '🎨', 'ru': {'title': 'Эффективное согласование наград в генеративных моделях с помощью умной выборки', 'desc': 'Статья представляет Psi-Sampler - новый метод для улучшения согласования наград при инференсе в генеративных моделях на основе оценок. Авторы предлагают использовать последовательный метод Монте-Карло (SMC) с инициализацией частиц из апостериорного распределения, учитывающего награду. Для эффективной выборки из апостериорного распределения в высокоразмерных латентных пространствах вводится алгоритм preconditioned Crank-Nicolson Langevin (pCNL). Эксперименты показывают улучшение результатов на различных задачах генерации изображений с учетом наград.'}, 'en': {'title': 'Enhancing Reward Alignment with Psi-Sampler', 'desc': 'The paper presents Psi-Sampler, a framework that enhances reward alignment during inference by using Sequential Monte Carlo (SMC) methods. It addresses the limitations of traditional particle initialization from Gaussian priors, which often fail to capture important reward-related areas. By employing a reward-aware posterior for initialization, the framework significantly boosts sampling efficiency and alignment performance. Additionally, the introduction of the preconditioned Crank-Nicolson Langevin (pCNL) algorithm allows for effective sampling in complex, high-dimensional spaces, leading to improved results in various generative tasks.'}, 'zh': {'title': '高效的奖励对齐：Psi-Sampler框架', 'desc': '本文介绍了一种名为Psi-Sampler的框架，该框架基于序列蒙特卡洛（SMC）方法，并结合了基于奖励的初始粒子采样，以实现与基于分数的生成模型的有效推理时间奖励对齐。近年来，基于分数的生成模型在推理时间奖励对齐方面受到了广泛关注，标志着从预训练到后训练优化的范式转变。现有方法通常从高斯先验初始化粒子，这不足以捕捉与奖励相关的区域，导致采样效率降低。我们展示了从奖励感知后验初始化显著提高了对齐性能，并引入了预条件Crank-Nicolson Langevin（pCNL）算法，以实现高维潜在空间中的后验采样。'}}}, {'id': 'https://huggingface.co/papers/2506.04225', 'title': 'Voyager: Long-Range and World-Consistent Video Diffusion for Explorable\n  3D Scene Generation', 'url': 'https://huggingface.co/papers/2506.04225', 'abstract': 'Voyager is a video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image, enabling long-range, consistent 3D scene exploration with user-defined camera paths.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world applications like video gaming and virtual reality often demand the ability to model 3D scenes that users can explore along custom camera trajectories. While significant progress has been made in generating 3D objects from text or images, creating long-range, 3D-consistent, explorable 3D scenes remains a complex and challenging problem. In this work, we present Voyager, a novel video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image with user-defined camera path. Unlike existing approaches, Voyager achieves end-to-end scene generation and reconstruction with inherent consistency across frames, eliminating the need for 3D reconstruction pipelines (e.g., structure-from-motion or multi-view stereo). Our method integrates three key components: 1) World-Consistent Video Diffusion: A unified architecture that jointly generates aligned RGB and depth video sequences, conditioned on existing world observation to ensure global coherence 2) Long-Range World Exploration: An efficient world cache with point culling and an auto-regressive inference with smooth video sampling for iterative scene extension with context-aware consistency, and 3) Scalable Data Engine: A video reconstruction pipeline that automates camera pose estimation and metric depth prediction for arbitrary videos, enabling large-scale, diverse training data curation without manual 3D annotations. Collectively, these designs result in a clear improvement over existing methods in visual quality and geometric accuracy, with versatile applications.', 'score': 14, 'issue_id': 4139, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '99f491949aa412fd', 'authors': ['Tianyu Huang', 'Wangguandong Zheng', 'Tengfei Wang', 'Yuhao Liu', 'Zhenwei Wang', 'Junta Wu', 'Jie Jiang', 'Hui Li', 'Rynson W. H. Lau', 'Wangmeng Zuo', 'Chunchao Guo'], 'affiliations': ['City University of Hong Kong, China', 'Harbin Institute of Technology, China', 'Southeast University, China', 'Tencent Hunyuan, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.04225.jpg', 'data': {'categories': ['#games', '#dataset', '#diffusion', '#3d', '#video'], 'emoji': '🚀', 'ru': {'title': 'Исследуй 3D-миры из одного кадра', 'desc': 'Voyager - это система видеодиффузии, которая генерирует согласованные последовательности 3D-облаков точек из одного изображения. Она позволяет исследовать 3D-сцены на большие расстояния с пользовательскими траекториями камеры. Ключевые компоненты включают согласованную видеодиффузию, исследование мира на большие расстояния и масштабируемый механизм данных. Voyager превосходит существующие методы по визуальному качеству и геометрической точности, открывая новые возможности применения.'}, 'en': {'title': 'Voyager: Seamless 3D Scene Exploration from a Single Image', 'desc': 'Voyager is a video diffusion framework that creates 3D point-cloud sequences from a single image, allowing users to explore scenes along custom camera paths. It addresses the challenge of generating long-range, consistent 3D environments by integrating world-consistent video diffusion, which ensures alignment of RGB and depth sequences. The framework also features an efficient world cache for smooth scene exploration and a scalable data engine that automates camera pose estimation and depth prediction. Overall, Voyager enhances visual quality and geometric accuracy, making it suitable for applications in video gaming and virtual reality.'}, 'zh': {'title': 'Voyager：从单图像生成一致的3D场景探索', 'desc': 'Voyager是一种视频扩散框架，可以从单张图像生成世界一致的3D点云序列，支持用户定义的相机路径进行长距离的3D场景探索。该方法通过端到端的场景生成和重建，确保了帧间的一致性，避免了传统的3D重建流程。Voyager集成了三个关键组件：世界一致的视频扩散、长距离世界探索和可扩展的数据引擎，提升了视觉质量和几何精度。该框架在视频游戏和虚拟现实等应用中具有广泛的潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.04228', 'title': 'LayerFlow: A Unified Model for Layer-aware Video Generation', 'url': 'https://huggingface.co/papers/2506.04228', 'abstract': 'LayerFlow is a unified framework for generating layer-aware videos using a text-to-video diffusion transformer and layer embeddings, supporting various video generation tasks with a multi-stage training strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t We present LayerFlow, a unified solution for layer-aware video generation. Given per-layer prompts, LayerFlow generates videos for the transparent foreground, clean background, and blended scene. It also supports versatile variants like decomposing a blended video or generating the background for the given foreground and vice versa. Starting from a text-to-video diffusion transformer, we organize the videos for different layers as sub-clips, and leverage layer embeddings to distinguish each clip and the corresponding layer-wise prompts. In this way, we seamlessly support the aforementioned variants in one unified framework. For the lack of high-quality layer-wise training videos, we design a multi-stage training strategy to accommodate static images with high-quality layer annotations. Specifically, we first train the model with low-quality video data. Then, we tune a motion LoRA to make the model compatible with static frames. Afterward, we train the content LoRA on the mixture of image data with high-quality layered images along with copy-pasted video data. During inference, we remove the motion LoRA thus generating smooth videos with desired layers.', 'score': 12, 'issue_id': 4134, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '1e8f6532d8b54b21', 'authors': ['Sihui Ji', 'Hao Luo', 'Xi Chen', 'Yuanpeng Tu', 'Yiyang Wang', 'Hengshuang Zhao'], 'affiliations': ['DAMO Academy, Alibaba Group, China', 'Hupan Laboratory, China', 'The University of Hong Kong', 'The University of Hong Kong, Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.04228.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#synthetic', '#video', '#training'], 'emoji': '🎞️', 'ru': {'title': 'LayerFlow: Умная генерация многослойного видео по текстовым подсказкам', 'desc': 'LayerFlow - это унифицированная система для генерации видео с учетом слоев, использующая трансформер диффузии для преобразования текста в видео и встраивания слоев. Она поддерживает различные задачи генерации видео, включая создание прозрачного переднего плана, чистого фона и смешанной сцены. Система использует многоэтапную стратегию обучения для адаптации к статическим изображениям с высококачественными аннотациями слоев. LayerFlow применяет LoRA для настройки движения и содержания, что позволяет генерировать плавные видео с желаемыми слоями.'}, 'en': {'title': 'LayerFlow: Unified Layer-Aware Video Generation', 'desc': 'LayerFlow is a comprehensive framework designed for generating videos that are aware of different layers, such as foreground and background. It utilizes a text-to-video diffusion transformer to create videos based on specific prompts for each layer, allowing for various video generation tasks. The framework can decompose blended videos or generate backgrounds for given foregrounds, making it versatile. To address the challenge of limited high-quality training data, LayerFlow employs a multi-stage training strategy that begins with low-quality videos and progressively incorporates high-quality layered images.'}, 'zh': {'title': 'LayerFlow：统一的层感知视频生成框架', 'desc': 'LayerFlow是一个统一的框架，用于生成层感知的视频，利用文本到视频的扩散变换器和层嵌入。该框架支持多种视频生成任务，包括透明前景、干净背景和混合场景的视频生成。通过将视频按层组织为子剪辑，并利用层嵌入来区分每个剪辑及其对应的层级提示，LayerFlow实现了多种视频生成变体。为了克服高质量层级训练视频的缺乏，LayerFlow设计了多阶段训练策略，结合静态图像和高质量层注释进行训练。'}}}, {'id': 'https://huggingface.co/papers/2506.03139', 'title': 'SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation', 'url': 'https://huggingface.co/papers/2506.03139', 'abstract': 'SVGenius evaluates Large Language Models and Multimodal LLMs for SVG processing using a comprehensive benchmark across three dimensions: understanding, editing, and generation, revealing insights into model capabilities and limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) and Multimodal LLMs have shown promising capabilities for SVG processing, yet existing benchmarks suffer from limited real-world coverage, lack of complexity stratification, and fragmented evaluation paradigms. We introduce SVGenius, a comprehensive benchmark comprising 2,377 queries across three progressive dimensions: understanding, editing, and generation. Built on real-world data from 24 application domains with systematic complexity stratification, SVGenius evaluates models through 8 task categories and 18 metrics. We assess 22 mainstream models spanning different scales, architectures, training paradigms, and accessibility levels. Our analysis reveals that while proprietary models significantly outperform open-source counterparts, all models exhibit systematic performance degradation with increasing complexity, indicating fundamental limitations in current approaches; however, reasoning-enhanced training proves more effective than pure scaling for overcoming these limitations, though style transfer remains the most challenging capability across all model types. SVGenius establishes the first systematic evaluation framework for SVG processing, providing crucial insights for developing more capable vector graphics models and advancing automated graphic design applications. Appendix and supplementary materials (including all data and code) are available at https://zju-real.github.io/SVGenius.', 'score': 12, 'issue_id': 4137, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'c1fdc3559598aa68', 'authors': ['Siqi Chen', 'Xinyu Dong', 'Haolei Xu', 'Xingyu Wu', 'Fei Tang', 'Hang Zhang', 'Yuchen Yan', 'Linjuan Wu', 'Wenqi Zhang', 'Guiyang Hou', 'Yongliang Shen', 'Weiming Lu', 'Yueting Zhuang'], 'affiliations': ['Zhejiang University Hangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.03139.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#optimization', '#multimodal', '#benchmark', '#interpretability'], 'emoji': '📊', 'ru': {'title': 'SVGenius: комплексная оценка возможностей ИИ в работе с векторной графикой', 'desc': 'SVGenius - это комплексный бенчмарк для оценки способностей больших языковых моделей (LLM) и мультимодальных LLM в обработке SVG. Бенчмарк включает 2377 запросов в трех измерениях: понимание, редактирование и генерация SVG. Оценка проводится по 8 категориям задач и 18 метрикам на основе реальных данных из 24 прикладных областей. Результаты показывают, что проприетарные модели значительно превосходят открытые, но все модели демонстрируют снижение производительности с увеличением сложности задач.'}, 'en': {'title': 'SVGenius: Unveiling SVG Processing Potential in LLMs', 'desc': 'SVGenius is a benchmark designed to evaluate the performance of Large Language Models (LLMs) and Multimodal LLMs in processing Scalable Vector Graphics (SVG). It assesses models across three key dimensions: understanding, editing, and generation, using a total of 2,377 queries derived from real-world applications. The evaluation framework includes 8 task categories and 18 metrics, highlighting the strengths and weaknesses of 22 different models. Findings indicate that while proprietary models excel, all models struggle with complex tasks, suggesting a need for improved training methods, particularly in reasoning, to enhance their capabilities.'}, 'zh': {'title': 'SVGenius：全面评估 SVG 处理能力的基准工具', 'desc': 'SVGenius 是一个评估大型语言模型和多模态 LLM 在 SVG 处理能力的基准工具。它通过理解、编辑和生成三个维度，使用 2,377 个查询来全面评估模型的能力和局限性。研究发现，尽管专有模型在性能上优于开源模型，但所有模型在复杂性增加时表现普遍下降。SVGenius 提供了一个系统的评估框架，为开发更强大的矢量图形模型和推进自动化图形设计应用提供了重要见解。'}}}, {'id': 'https://huggingface.co/papers/2506.04158', 'title': 'Image Editing As Programs with Diffusion Models', 'url': 'https://huggingface.co/papers/2506.04158', 'abstract': 'While diffusion models have achieved remarkable success in text-to-image generation, they encounter significant challenges with instruction-driven image editing. Our research highlights a key challenge: these models particularly struggle with structurally inconsistent edits that involve substantial layout changes. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a unified image editing framework built upon the Diffusion Transformer (DiT) architecture. At its core, IEAP approaches instructional editing through a reductionist lens, decomposing complex editing instructions into sequences of atomic operations. Each operation is implemented via a lightweight adapter sharing the same DiT backbone and is specialized for a specific type of edit. Programmed by a vision-language model (VLM)-based agent, these operations collaboratively support arbitrary and structurally inconsistent transformations. By modularizing and sequencing edits in this way, IEAP generalizes robustly across a wide range of editing tasks, from simple adjustments to substantial structural changes. Extensive experiments demonstrate that IEAP significantly outperforms state-of-the-art methods on standard benchmarks across various editing scenarios. In these evaluations, our framework delivers superior accuracy and semantic fidelity, particularly for complex, multi-step instructions. Codes are available at https://github.com/YujiaHu1109/IEAP.', 'score': 11, 'issue_id': 4133, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'e5a32d484bb427f3', 'authors': ['Yujia Hu', 'Songhua Liu', 'Zhenxiong Tan', 'Xingyi Yang', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2506.04158.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#cv', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'Редактирование изображений как программирование: новый подход к ИИ-обработке визуального контента', 'desc': 'Исследователи представили новый подход к редактированию изображений с использованием искусственного интеллекта, названный IEAP (Image Editing As Programs). Эта система основана на архитектуре Diffusion Transformer и разбивает сложные инструкции по редактированию на последовательность простых операций. Каждая операция реализуется с помощью специализированного адаптера, использующего общий базовый DiT. IEAP значительно превосходит современные методы в различных задачах редактирования, особенно при сложных многоэтапных инструкциях.'}, 'en': {'title': 'Revolutionizing Image Editing with Programmatic Precision', 'desc': 'This paper addresses the limitations of diffusion models in instruction-driven image editing, particularly when it comes to making significant layout changes. The authors propose a new framework called Image Editing As Programs (IEAP), which utilizes the Diffusion Transformer (DiT) architecture to break down complex editing tasks into simpler, atomic operations. Each operation is executed by a lightweight adapter that specializes in a specific type of edit, allowing for more flexible and accurate transformations. The framework shows improved performance over existing methods, achieving higher accuracy and semantic fidelity in various editing scenarios, especially for complex instructions.'}, 'zh': {'title': '图像编辑的新方法：将复杂指令转化为简单操作', 'desc': '本研究提出了一种新的图像编辑框架，称为图像编辑作为程序（IEAP），旨在解决扩散模型在指令驱动的图像编辑中面临的挑战。IEAP基于扩散变换器（DiT）架构，通过将复杂的编辑指令分解为一系列原子操作来实现。每个操作由轻量级适配器实现，专门针对特定类型的编辑，能够支持任意和结构不一致的变换。实验结果表明，IEAP在各种编辑场景中显著优于现有的最先进方法，尤其在处理复杂的多步骤指令时表现出更高的准确性和语义保真度。'}}}, {'id': 'https://huggingface.co/papers/2506.03295', 'title': 'Unleashing the Reasoning Potential of Pre-trained LLMs by Critique\n  Fine-Tuning on One Problem', 'url': 'https://huggingface.co/papers/2506.03295', 'abstract': "Critique Fine-Tuning on a single problem can efficiently enhance the reasoning capabilities of large language models with significant performance gains and reduced computational cost compared to reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess immense reasoning potential inherited from the pre-training stage. With reinforcement learning (RL), these models can improve dramatically on reasoning tasks. Recent studies have shown that even RL on a single problem can unleash these models' reasoning capabilities. However, RL is not only expensive but also unstable. Even one-shot RL requires hundreds of GPU hours. This raises a critical question: Is there a more efficient way to unleash the reasoning potential of these powerful base LLMs? In this work, we demonstrate that Critique Fine-Tuning (CFT) on only one problem can effectively unleash the reasoning potential of LLMs. Our method constructs critique data by collecting diverse model-generated solutions to a single problem and using teacher LLMs to provide detailed critiques. We fine-tune Qwen and Llama family models, ranging from 1.5B to 14B parameters, on the CFT data and observe significant performance gains across diverse reasoning tasks. For example, with just 5 GPU hours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six math benchmarks and 16% on three logic reasoning benchmarks. These results are comparable to or even surpass the results from RL with 20x less compute. Ablation studies reveal the robustness of one-shot CFT across different prompt problems. These results highlight one-shot CFT as a simple, general, and compute-efficient approach to unleashing the reasoning capabilities of modern LLMs.", 'score': 11, 'issue_id': 4135, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '03df39687a4bdf5a', 'authors': ['Yubo Wang', 'Ping Nie', 'Kai Zou', 'Lijun Wu', 'Wenhu Chen'], 'affiliations': ['Independent', 'Netmind.AI', 'Shanghai AI Lab', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.03295.jpg', 'data': {'categories': ['#training', '#rl', '#optimization', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Эффективное раскрытие потенциала ИИ через обучение на критике', 'desc': 'Статья представляет метод Critique Fine-Tuning (CFT) для улучшения способностей рассуждения больших языковых моделей (LLM). CFT использует обучение на критических отзывах о решениях одной задачи, генерируемых моделью-учителем. Эксперименты показывают, что CFT значительно повышает производительность моделей на различных задачах рассуждения при меньших вычислительных затратах по сравнению с обучением с подкреплением. Результаты демонстрируют эффективность CFT как простого и общего подхода к раскрытию потенциала современных LLM.'}, 'en': {'title': 'Unlocking Reasoning Power with Efficient Critique Fine-Tuning', 'desc': 'This paper introduces Critique Fine-Tuning (CFT) as a method to enhance the reasoning abilities of large language models (LLMs) like Qwen and Llama. By focusing on a single problem, CFT generates critique data from various model-generated solutions, which are then used to fine-tune the models. The results show that this approach leads to significant performance improvements on reasoning tasks with much lower computational costs compared to traditional reinforcement learning methods. The findings suggest that CFT is a robust and efficient strategy for maximizing the reasoning potential of LLMs.'}, 'zh': {'title': '批评微调：高效释放语言模型推理潜力的利器', 'desc': '本文提出了一种名为批评微调（Critique Fine-Tuning, CFT）的方法，旨在高效提升大型语言模型（LLM）的推理能力。通过对单一问题进行微调，CFT能够显著提高模型在推理任务上的表现，同时减少计算成本。研究表明，使用CFT方法，模型在多个推理基准测试中平均提升了15%到16%。与传统的强化学习方法相比，CFT在计算资源上更加高效，展示了其作为一种简单且通用的推理能力提升策略的潜力。'}}}, {'id': 'https://huggingface.co/papers/2505.24500', 'title': "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning\n  for Enhancing LLMs' Social Intelligence", 'url': 'https://huggingface.co/papers/2505.24500', 'abstract': "Temporal-aware Hierarchical Cognitive Reinforcement Learning enhances LLMs' social intelligence by addressing the distinct cognitive demands of social domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, Large Language Models (LLMs) have made significant progress in IQ-related domains that require careful thinking, such as mathematics and coding. However, enhancing LLMs' cognitive development in social domains, particularly from a post-training perspective, remains underexplored. Recognizing that the social world follows a distinct timeline and requires a richer blend of cognitive modes (from intuitive reactions (System 1) and surface-level thinking to deliberate thinking (System 2)) than mathematics, which primarily relies on System 2 cognition (careful, step-by-step reasoning), we introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we systematically explore improving LLMs' social intelligence and validate the effectiveness of the TimeHC-RL method, through five other post-training paradigms and two test-time intervention paradigms on eight datasets with diverse data patterns. Experimental results reveal the superiority of our proposed TimeHC-RL method compared to the widely adopted System 2 RL method. It gives the 7B backbone model wings, enabling it to rival the performance of advanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic exploration from post-training and test-time interventions perspectives to improve LLMs' social intelligence has uncovered several valuable insights.", 'score': 11, 'issue_id': 4133, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'ee580986393d0b7e', 'authors': ['Guiyang Hou', 'Xing Gao', 'Yuchuan Wu', 'Xiang Huang', 'Wenqi Zhang', 'Zhe Zheng', 'Yongliang Shen', 'Jialu Du', 'Fei Huang', 'Yongbin Li', 'Weiming Lu'], 'affiliations': ['Nanjing University', 'Tongyi Lab, Alibaba Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24500.jpg', 'data': {'categories': ['#rlhf', '#rl', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Новый метод обучения для повышения социального интеллекта языковых моделей', 'desc': 'Исследователи представили метод Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) для улучшения социального интеллекта больших языковых моделей (LLM). Этот подход учитывает временную составляющую и иерархию когнитивных процессов, характерных для социальных взаимодействий. Метод TimeHC-RL показал превосходство над широко используемым методом обучения с подкреплением System 2 RL. Эксперименты продемонстрировали, что применение TimeHC-RL позволяет моделям с 7 миллиардами параметров достигать производительности передовых моделей в задачах социального интеллекта.'}, 'en': {'title': "Boosting LLMs' Social Intelligence with TimeHC-RL", 'desc': "This paper presents a new approach called Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) to improve the social intelligence of Large Language Models (LLMs). Unlike traditional methods that focus on logical reasoning, TimeHC-RL incorporates different cognitive processes, including intuitive and deliberate thinking, to better navigate social contexts. The authors conducted experiments across various datasets and compared TimeHC-RL with existing reinforcement learning methods, demonstrating its superior performance. The findings suggest that enhancing LLMs' cognitive abilities in social domains can significantly elevate their overall intelligence and effectiveness."}, 'zh': {'title': '提升社交智能的时间感知强化学习', 'desc': '本文提出了一种新的方法，称为时间感知层次认知强化学习（TimeHC-RL），旨在提升大型语言模型（LLMs）在社交领域的智能。与数学等依赖系统2认知的领域不同，社交领域需要更丰富的认知模式，包括直觉反应和表层思维。通过对八个不同数据集的实验，我们验证了TimeHC-RL方法的有效性，结果显示其在社交智能方面优于传统的系统2强化学习方法。该方法使得7B基础模型的表现接近于更先进的模型，如DeepSeek-R1和OpenAI-O3。'}}}, {'id': 'https://huggingface.co/papers/2506.03150', 'title': 'IllumiCraft: Unified Geometry and Illumination Diffusion for\n  Controllable Video Generation', 'url': 'https://huggingface.co/papers/2506.03150', 'abstract': 'IllumiCraft integrates geometric cues in a diffusion framework to generate high-fidelity, temporally coherent videos from textual or image inputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Although diffusion-based models can generate high-quality and high-resolution video sequences from textual or image inputs, they lack explicit integration of geometric cues when controlling scene lighting and visual appearance across frames. To address this limitation, we propose IllumiCraft, an end-to-end diffusion framework accepting three complementary inputs: (1) high-dynamic-range (HDR) video maps for detailed lighting control; (2) synthetically relit frames with randomized illumination changes (optionally paired with a static background reference image) to provide appearance cues; and (3) 3D point tracks that capture precise 3D geometry information. By integrating the lighting, appearance, and geometry cues within a unified diffusion architecture, IllumiCraft generates temporally coherent videos aligned with user-defined prompts. It supports background-conditioned and text-conditioned video relighting and provides better fidelity than existing controllable video generation methods. Project Page: https://yuanze-lin.me/IllumiCraft_page', 'score': 10, 'issue_id': 4142, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '524d1d7f47dfcf7f', 'authors': ['Yuanze Lin', 'Yi-Wen Chen', 'Yi-Hsuan Tsai', 'Ronald Clark', 'Ming-Hsuan Yang'], 'affiliations': ['Atmanity Inc.', 'Google DeepMind', 'NEC Labs America', 'UC Merced', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2506.03150.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#architecture', '#3d', '#video'], 'emoji': '🎥', 'ru': {'title': 'Геометрия света: новый уровень контроля в генерации видео', 'desc': 'IllumiCraft - это инновационная диффузионная модель для генерации видео, интегрирующая геометрические подсказки для улучшения качества и временной согласованности. Модель использует три типа входных данных: HDR-карты видео для контроля освещения, синтетически переосвещенные кадры для подсказок внешнего вида и 3D-треки точек для геометрической информации. IllumiCraft позволяет создавать видео с управляемым освещением на основе текстовых запросов или изображений, превосходя существующие методы по качеству и контролируемости.'}, 'en': {'title': 'IllumiCraft: Crafting Coherent Videos with Geometric Precision', 'desc': 'IllumiCraft is a novel diffusion framework designed to create high-quality videos from text or image inputs while incorporating geometric cues. It utilizes three main inputs: HDR video maps for precise lighting control, synthetically relit frames for appearance variations, and 3D point tracks for accurate geometry representation. By merging these elements, IllumiCraft ensures that the generated videos are not only visually appealing but also maintain temporal coherence across frames. This approach enhances the fidelity of video generation compared to existing methods that lack such integrated controls.'}, 'zh': {'title': 'IllumiCraft：高保真视频生成的新方法', 'desc': 'IllumiCraft 是一个集成几何线索的扩散框架，能够从文本或图像输入生成高保真、时间一致的视频。该方法通过接受高动态范围（HDR）视频图像、合成重新照明的帧和3D点轨迹，来控制场景的光照和视觉外观。通过将光照、外观和几何线索整合在一个统一的扩散架构中，IllumiCraft 生成与用户定义提示一致的时间连贯视频。与现有的可控视频生成方法相比，它提供了更好的保真度。'}}}, {'id': 'https://huggingface.co/papers/2506.03930', 'title': 'VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code\n  Generation', 'url': 'https://huggingface.co/papers/2506.03930', 'abstract': 'VisCode-200K, a large-scale dataset for visualization, improves plot generation performance by integrating execution-grounded supervision and iterative code correction, outperforming open-source models and rivaling proprietary ones.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) often struggle with visualization tasks like plotting diagrams, charts, where success depends on both code correctness and visual semantics. Existing instruction-tuning datasets lack execution-grounded supervision and offer limited support for iterative code correction, resulting in fragile and unreliable plot generation. We present VisCode-200K, a large-scale instruction tuning dataset for Python-based visualization and self-correction. It contains over 200K examples from two sources: (1) validated plotting code from open-source repositories, paired with natural language instructions and rendered plots; and (2) 45K multi-turn correction dialogues from Code-Feedback, enabling models to revise faulty code using runtime feedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create VisCoder, and evaluate it on PandasPlotBench. VisCoder significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4o-mini. We further adopt a self-debug evaluation protocol to assess iterative repair, demonstrating the benefits of feedback-driven learning for executable, visually accurate code generation.', 'score': 9, 'issue_id': 4139, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '56d407ae0e0e6b4a', 'authors': ['Yuansheng Ni', 'Ping Nie', 'Kai Zou', 'Xiang Yue', 'Wenhu Chen'], 'affiliations': ['Carnegie Mellon University', 'Independent Researcher', 'Netmind.ai', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2506.03930.jpg', 'data': {'categories': ['#story_generation', '#data', '#dataset', '#optimization', '#training'], 'emoji': '📊', 'ru': {'title': 'VisCode-200K: Большие данные для умного построения графиков', 'desc': 'VisCode-200K - это крупномасштабный набор данных для задач визуализации, который улучшает генерацию графиков с помощью обучения с подкреплением на основе выполнения кода и итеративной коррекции. Датасет содержит более 200 тысяч примеров из двух источников: проверенный код построения графиков из открытых репозиториев и диалоги по исправлению кода. На основе VisCode-200K была обучена модель VisCoder, которая превзошла открытые базовые модели и приблизилась к производительности проприетарных моделей. Исследование демонстрирует преимущества обучения на основе обратной связи для генерации исполняемого и визуально точного кода.'}, 'en': {'title': 'Empowering Visualization with VisCode-200K: A Leap in Plot Generation!', 'desc': 'The paper introduces VisCode-200K, a comprehensive dataset designed to enhance the performance of machine learning models in generating visualizations through improved code generation. It addresses the limitations of existing datasets by incorporating execution-grounded supervision and enabling iterative code correction, which helps models learn from their mistakes. The dataset consists of over 200,000 examples, including validated plotting code and multi-turn dialogues for code feedback. The authors demonstrate that their model, VisCoder, fine-tuned on this dataset, significantly outperforms existing open-source models and competes closely with proprietary ones in generating accurate visualizations.'}, 'zh': {'title': 'VisCode-200K：提升可视化生成的革命性数据集', 'desc': 'VisCode-200K是一个大规模的数据集，专注于可视化任务，旨在提高绘图生成的性能。该数据集结合了执行基础的监督和迭代代码修正，解决了现有模型在绘图时的脆弱性和不可靠性。它包含来自开源代码库的有效绘图代码和自然语言指令的配对，以及多轮修正对话，帮助模型修正错误代码。通过在VisCode-200K上微调模型，VisCoder在绘图生成方面显著超越了开源基线，接近商业模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.04141', 'title': "MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in\n  Videos", 'url': 'https://huggingface.co/papers/2506.04141', 'abstract': 'A new benchmark, MMR-V, is proposed to challenge multimodal large language models with long-range, multi-frame reasoning and hidden information processing in videos, revealing their limitations and inspiring further research.  \t\t\t\t\tAI-generated summary \t\t\t\t The sequential structure of videos poses a challenge to the ability of multimodal large language models (MLLMs) to locate multi-frame evidence and conduct multimodal reasoning. However, existing video benchmarks mainly focus on understanding tasks, which only require models to match frames mentioned in the question (hereafter referred to as "question frame") and perceive a few adjacent frames. To address this gap, we propose MMR-V: A Benchmark for Multimodal Deep Reasoning in Videos. The benchmark is characterized by the following features. (1) Long-range, multi-frame reasoning: Models are required to infer and analyze evidence frames that may be far from the question frame. (2) Beyond perception: Questions cannot be answered through direct perception alone but require reasoning over hidden information. (3) Reliability: All tasks are manually annotated, referencing extensive real-world user understanding to align with common perceptions. (4) Confusability: Carefully designed distractor annotation strategies to reduce model shortcuts. MMR-V consists of 317 videos and 1,257 tasks. Our experiments reveal that current models still struggle with multi-modal reasoning; even the best-performing model, o4-mini, achieves only 52.5% accuracy. Additionally, current reasoning enhancement strategies (Chain-of-Thought and scaling test-time compute) bring limited gains. Further analysis indicates that the CoT demanded for multi-modal reasoning differs from it in textual reasoning, which partly explains the limited performance gains. We hope that MMR-V can inspire further research into enhancing multi-modal reasoning capabilities.', 'score': 8, 'issue_id': 4137, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'a12411c7424fd2a7', 'authors': ['Kejian Zhu', 'Zhuoran Jin', 'Hongbang Yuan', 'Jiachun Li', 'Shangqing Tu', 'Pengfei Cao', 'Yubo Chen', 'Kang Liu', 'Jun Zhao'], 'affiliations': ['School of Artificial Intelligence, University of Chinese Academy of Sciences', 'The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04141.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#long_context', '#benchmark', '#video'], 'emoji': '🎥', 'ru': {'title': 'MMR-V: Новый рубеж в мультимодальных рассуждениях по видео', 'desc': 'Предложен новый бенчмарк MMR-V для оценки мультимодальных языковых моделей в задачах рассуждения по видео. Он требует анализа кадров, удаленных от упомянутых в вопросе, и выявления скрытой информации. Эксперименты показали, что современные модели испытывают трудности с такими задачами - лучшая достигла точности лишь 52.5%. Бенчмарк призван стимулировать исследования по улучшению навыков мультимодальных рассуждений у ИИ.'}, 'en': {'title': 'MMR-V: Elevating Multimodal Reasoning in Videos', 'desc': 'The paper introduces MMR-V, a new benchmark designed to test the capabilities of multimodal large language models (MLLMs) in video analysis. It emphasizes the need for long-range, multi-frame reasoning, where models must analyze evidence that is not immediately adjacent to the question frame. Unlike existing benchmarks that focus on simple understanding tasks, MMR-V requires models to reason about hidden information and avoid shortcuts through distractor annotations. The findings show that current models struggle with these challenges, achieving only modest accuracy, highlighting the need for further research in enhancing multimodal reasoning skills.'}, 'zh': {'title': 'MMR-V：推动多模态推理的新基准', 'desc': '本文提出了一个新的基准MMR-V，旨在挑战多模态大型语言模型在视频中的长距离、多帧推理和隐藏信息处理能力。现有的视频基准主要集中在理解任务上，而MMR-V要求模型进行更复杂的推理，分析与问题帧相距较远的证据帧。该基准包含317个视频和1257个任务，实验结果显示当前模型在多模态推理方面仍然存在困难，最佳模型的准确率仅为52.5%。我们希望MMR-V能够激发进一步研究，以提升多模态推理能力。'}}}, {'id': 'https://huggingface.co/papers/2506.04142', 'title': 'Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis', 'url': 'https://huggingface.co/papers/2506.04142', 'abstract': 'A method called shortcut neuron patching identifies and suppresses shortcut neurons in language models to mitigate data contamination issues in trustworthy evaluations.  \t\t\t\t\tAI-generated summary \t\t\t\t The development of large language models (LLMs) depends on trustworthy evaluation. However, most current evaluations rely on public benchmarks, which are prone to data contamination issues that significantly compromise fairness. Previous researches have focused on constructing dynamic benchmarks to address contamination. However, continuously building new benchmarks is costly and cyclical. In this work, we aim to tackle contamination by analyzing the mechanisms of contaminated models themselves. Through our experiments, we discover that the overestimation of contaminated models is likely due to parameters acquiring shortcut solutions in training. We further propose a novel method for identifying shortcut neurons through comparative and causal analysis. Building on this, we introduce an evaluation method called shortcut neuron patching to suppress shortcut neurons. Experiments validate the effectiveness of our approach in mitigating contamination. Additionally, our evaluation results exhibit a strong linear correlation with MixEval, a recently released trustworthy benchmark, achieving a Spearman coefficient (rho) exceeding 0.95. This high correlation indicates that our method closely reveals true capabilities of the models and is trustworthy. We conduct further experiments to demonstrate the generalizability of our method across various benchmarks and hyperparameter settings. Code: https://github.com/GaryStack/Trustworthy-Evaluation', 'score': 7, 'issue_id': 4137, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '4799bf9d7a1cb57f', 'authors': ['Kejian Zhu', 'Shangqing Tu', 'Zhuoran Jin', 'Lei Hou', 'Juanzi Li', 'Jun Zhao'], 'affiliations': ['School of Artificial Intelligence, University of Chinese Academy of Sciences', 'The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04142.jpg', 'data': {'categories': ['#data', '#hallucinations', '#benchmark', '#ethics', '#training'], 'emoji': '🧠', 'ru': {'title': 'Борьба с загрязнением данных в языковых моделях через патчинг нейронов-шорткатов', 'desc': "Метод 'shortcut neuron patching' идентифицирует и подавляет нейроны-шорткаты в языковых моделях для снижения проблем загрязнения данных при надежной оценке. Исследователи обнаружили, что переоценка загрязненных моделей вероятно связана с параметрами, приобретающими короткие пути решения при обучении. Предложенный метод показал высокую корреляцию с надежным бенчмарком MixEval, достигнув коэффициента Спирмена более 0,95. Эксперименты подтвердили эффективность подхода в снижении загрязнения данных и его обобщаемость на различные бенчмарки."}, 'en': {'title': 'Suppressing Shortcut Neurons for Trustworthy Evaluations', 'desc': 'This paper presents a method called shortcut neuron patching, which aims to identify and suppress shortcut neurons in language models to improve the reliability of evaluations. The authors highlight that current evaluation methods often suffer from data contamination, leading to unfair assessments of model performance. By analyzing the internal mechanisms of contaminated models, they find that shortcut solutions during training contribute to overestimation of model capabilities. Their proposed method effectively mitigates these issues, showing strong correlation with established trustworthy benchmarks, thus ensuring more accurate evaluations of language models.'}, 'zh': {'title': '抑制快捷神经元，提升评估可信度', 'desc': '本文提出了一种名为快捷神经元修补的方法，用于识别和抑制语言模型中的快捷神经元，以减轻数据污染问题，从而提高评估的可信度。当前的评估方法大多依赖公共基准，但这些基准容易受到数据污染的影响，导致评估结果不公平。我们通过比较和因果分析，发现训练过程中模型参数可能会获得快捷解决方案，从而导致对污染模型的过高估计。实验结果表明，我们的方法在减轻污染方面有效，并且与MixEval基准的评估结果具有很强的线性相关性，Spearman系数超过0.95，表明我们的方法能够真实反映模型的能力。'}}}, {'id': 'https://huggingface.co/papers/2506.04108', 'title': 'Rectified Sparse Attention', 'url': 'https://huggingface.co/papers/2506.04108', 'abstract': 'Rectified Sparse Attention (ReSA) improves the efficiency of long-sequence generation in Large Language Models by combining block-sparse attention with periodic dense rectification, maintaining high-quality generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Efficient long-sequence generation is a critical challenge for Large Language Models. While recent sparse decoding methods improve efficiency, they suffer from KV cache misalignment, where approximation errors accumulate and degrade generation quality. In this work, we propose Rectified Sparse Attention (ReSA), a simple yet effective method that combines block-sparse attention with periodic dense rectification. By refreshing the KV cache at fixed intervals using a dense forward pass, ReSA bounds error accumulation and preserves alignment with the pretraining distribution. Experiments across math reasoning, language modeling, and retrieval tasks demonstrate that ReSA achieves near-lossless generation quality with significantly improved efficiency. Notably, ReSA delivers up to 2.42times end-to-end speedup under decoding at 256K sequence length, making it a practical solution for scalable long-context inference. Code is available at https://aka.ms/ReSA-LM.', 'score': 7, 'issue_id': 4134, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'ff7222f16cd2bf28', 'authors': ['Yutao Sun', 'Tianzhu Ye', 'Li Dong', 'Yuqing Xia', 'Jian Chen', 'Yizhao Gao', 'Shijie Cao', 'Jianyong Wang', 'Furu Wei'], 'affiliations': ['Microsoft Research', 'The University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04108.jpg', 'data': {'categories': ['#architecture', '#inference', '#long_context', '#optimization', '#training'], 'emoji': '🚀', 'ru': {'title': 'Эффективная генерация длинных текстов без потери качества', 'desc': 'Метод Rectified Sparse Attention (ReSA) улучшает эффективность генерации длинных последовательностей в больших языковых моделях. Он сочетает блочно-разреженное внимание с периодической плотной ректификацией, что позволяет ограничить накопление ошибок. ReSA достигает качества генерации, близкого к безошибочному, при значительном повышении эффективности. Метод обеспечивает ускорение до 2,42 раза при декодировании последовательностей длиной 256 тысяч токенов.'}, 'en': {'title': 'Boosting Efficiency in Long-Sequence Generation with ReSA', 'desc': "Rectified Sparse Attention (ReSA) enhances the efficiency of generating long sequences in Large Language Models by integrating block-sparse attention with periodic dense rectification. This approach addresses the issue of KV cache misalignment that can lead to errors and reduced quality in generated outputs. By periodically refreshing the KV cache through a dense forward pass, ReSA minimizes error accumulation and maintains alignment with the model's pretraining data. Experimental results show that ReSA not only preserves high-quality generation but also achieves significant speed improvements, making it a viable option for long-context tasks."}, 'zh': {'title': '高效长序列生成的新方法：ReSA', 'desc': 'Rectified Sparse Attention（ReSA）是一种提高大型语言模型长序列生成效率的方法。它结合了块稀疏注意力和周期性密集整流，能够保持高质量的生成效果。通过在固定间隔内使用密集前向传递刷新KV缓存，ReSA限制了误差累积，并保持与预训练分布的对齐。实验表明，ReSA在数学推理、语言建模和检索任务中实现了接近无损的生成质量，并在256K序列长度下提供了高达2.42倍的端到端加速。'}}}, {'id': 'https://huggingface.co/papers/2506.03517', 'title': 'DenseDPO: Fine-Grained Temporal Preference Optimization for Video\n  Diffusion Models', 'url': 'https://huggingface.co/papers/2506.03517', 'abstract': 'Direct Preference Optimization (DPO) has recently been applied as a post-training technique for text-to-video diffusion models. To obtain training data, annotators are asked to provide preferences between two videos generated from independent noise. However, this approach prohibits fine-grained comparisons, and we point out that it biases the annotators towards low-motion clips as they often contain fewer visual artifacts. In this work, we introduce DenseDPO, a method that addresses these shortcomings by making three contributions. First, we create each video pair for DPO by denoising corrupted copies of a ground truth video. This results in aligned pairs with similar motion structures while differing in local details, effectively neutralizing the motion bias. Second, we leverage the resulting temporal alignment to label preferences on short segments rather than entire clips, yielding a denser and more precise learning signal. With only one-third of the labeled data, DenseDPO greatly improves motion generation over vanilla DPO, while matching it in text alignment, visual quality, and temporal consistency. Finally, we show that DenseDPO unlocks automatic preference annotation using off-the-shelf Vision Language Models (VLMs): GPT accurately predicts segment-level preferences similar to task-specifically fine-tuned video reward models, and DenseDPO trained on these labels achieves performance close to using human labels.', 'score': 7, 'issue_id': 4137, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '5be6b9aa57297fb6', 'authors': ['Ziyi Wu', 'Anil Kag', 'Ivan Skorokhodov', 'Willi Menapace', 'Ashkan Mirzaei', 'Igor Gilitschenski', 'Sergey Tulyakov', 'Aliaksandr Siarohin'], 'affiliations': ['Snap Research', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.03517.jpg', 'data': {'categories': ['#video', '#optimization', '#diffusion', '#rlhf'], 'emoji': '🎬', 'ru': {'title': 'DenseDPO: Точная оптимизация предпочтений для улучшения генерации видео', 'desc': 'DenseDPO - это новый метод для улучшения текст-в-видео диффузионных моделей. Он решает проблему смещения в сторону клипов с низкой подвижностью при аннотации предпочтений. DenseDPO создает пары видео путем очистки от шума искаженных копий исходного видео, что позволяет делать более точные сравнения. Метод также использует временное выравнивание для разметки предпочтений на коротких сегментах, а не на целых клипах. DenseDPO демонстрирует улучшенную генерацию движения по сравнению с обычным DPO, используя только треть размеченных данных.'}, 'en': {'title': 'Enhancing Video Generation with DenseDPO: Precision and Efficiency in Preference Learning', 'desc': 'This paper introduces DenseDPO, an improved method for Direct Preference Optimization (DPO) in text-to-video diffusion models. DenseDPO addresses the limitations of traditional DPO by creating video pairs from denoised versions of a ground truth video, allowing for better alignment and reducing bias towards low-motion clips. It also enables preference labeling on shorter video segments, which provides a more detailed learning signal while using less labeled data. Additionally, DenseDPO facilitates automatic preference annotation through Vision Language Models, achieving performance comparable to human-labeled data.'}, 'zh': {'title': 'DenseDPO：提升视频生成的偏好优化方法', 'desc': '直接偏好优化（DPO）最近被应用于文本到视频的扩散模型后训练技术。我们提出的DenseDPO方法通过三项贡献解决了DPO的不足之处。首先，我们通过去噪真实视频的损坏副本来创建视频对，从而消除了运动偏差。其次，我们利用时间对齐来标记短片段的偏好，使学习信号更加密集和精确，最终DenseDPO在运动生成方面显著优于传统DPO，同时在文本对齐、视觉质量和时间一致性方面表现相当。'}}}, {'id': 'https://huggingface.co/papers/2506.02592', 'title': 'Beyond the Surface: Measuring Self-Preference in LLM Judgments', 'url': 'https://huggingface.co/papers/2506.02592', 'abstract': 'The DBG score is introduced to measure self-preference bias in large language models by using gold judgments as proxies for response quality, addressing the confounding effect of response quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies show that large language models (LLMs) exhibit self-preference bias when serving as judges, meaning they tend to favor their own responses over those generated by other models. Existing methods typically measure this bias by calculating the difference between the scores a judge model assigns to its own responses and those it assigns to responses from other models. However, this approach conflates self-preference bias with response quality, as higher-quality responses from the judge model may also lead to positive score differences, even in the absence of bias. To address this issue, we introduce gold judgments as proxies for the actual quality of responses and propose the DBG score, which measures self-preference bias as the difference between the scores assigned by the judge model to its own responses and the corresponding gold judgments. Since gold judgments reflect true response quality, the DBG score mitigates the confounding effect of response quality on bias measurement. Using the DBG score, we conduct comprehensive experiments to assess self-preference bias across LLMs of varying versions, sizes, and reasoning abilities. Additionally, we investigate two factors that influence and help alleviate self-preference bias: response text style and the post-training data of judge models. Finally, we explore potential underlying mechanisms of self-preference bias from an attention-based perspective. Our code and data are available at https://github.com/zhiyuanc2001/self-preference.', 'score': 7, 'issue_id': 4134, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'ccdb2761a23fe0c8', 'authors': ['Zhi-Yuan Chen', 'Hao Wang', 'Xinyu Zhang', 'Enrui Hu', 'Yankai Lin'], 'affiliations': ['Beijing Key Laboratory of Research on Large Models and Intelligent Governance', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Huawei Poisson Lab'], 'pdf_title_img': 'assets/pdf/title_img/2506.02592.jpg', 'data': {'categories': ['#benchmark', '#data', '#hallucinations', '#interpretability', '#ethics', '#training'], 'emoji': '⚖️', 'ru': {'title': 'DBG: Новый способ измерения предвзятости в языковых моделях', 'desc': 'Статья представляет новый метод измерения предвзятости самопредпочтения в больших языковых моделях (LLM). Авторы вводят показатель DBG, который использует эталонные оценки в качестве прокси для качества ответов. Этот подход позволяет отделить влияние качества ответов от фактической предвзятости модели. Исследователи провели эксперименты с различными LLM и изучили факторы, влияющие на предвзятость самопредпочтения.'}, 'en': {'title': 'Measuring Self-Preference Bias with the DBG Score', 'desc': 'This paper introduces the DBG score, a new metric designed to measure self-preference bias in large language models (LLMs) while accounting for response quality. Traditional methods for assessing this bias often confuse it with the quality of the responses, as higher quality can lead to misleading score differences. By using gold judgments as benchmarks for response quality, the DBG score effectively isolates self-preference bias from quality effects. The authors conduct experiments across various LLMs and examine factors that influence bias, providing insights into the mechanisms behind self-preference in model responses.'}, 'zh': {'title': '引入DBG评分，精准测量自我偏好偏差', 'desc': '本文提出了DBG评分，用于测量大型语言模型中的自我偏好偏差。通过使用金标准判断作为响应质量的代理，DBG评分解决了响应质量对偏差测量的混淆效应。研究表明，现有方法在评估自我偏好偏差时，往往将其与响应质量混为一谈。我们通过实验评估了不同版本、规模和推理能力的语言模型的自我偏好偏差，并探讨了影响该偏差的因素。'}}}, {'id': 'https://huggingface.co/papers/2506.03106', 'title': 'Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback', 'url': 'https://huggingface.co/papers/2506.03106', 'abstract': 'Critique-GRPO, an RL framework combining numerical and natural language feedback, enhances LLM reasoning across tasks and outperforms existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose Critique-GRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided refinements simultaneously while maintaining exploration. Extensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that Critique-GRPO consistently outperforms supervised learning-based and RL-based fine-tuning approaches across eight challenging mathematical, STEM, and general reasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%, respectively. Notably, Critique-GRPO surpasses a strong baseline that incorporates expert demonstrations within online RL. Further analysis reveals two critical insights about policy exploration: (1) higher entropy does not always guarantee efficient learning from exploration, and (2) longer responses do not necessarily lead to more effective exploration.', 'score': 6, 'issue_id': 4134, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '99f5fb3b08ab4205', 'authors': ['Xiaoying Zhang', 'Hao Sun', 'Yipeng Zhang', 'Kaituo Feng', 'Chaochao Lu', 'Chao Yang', 'Helen Meng'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong, HCCL', 'The Chinese University of Hong Kong, MMLab', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2506.03106.jpg', 'data': {'categories': ['#math', '#rl', '#reasoning', '#optimization', '#rlhf', '#training'], 'emoji': '🧠', 'ru': {'title': 'Critique-GRPO: Улучшение рассуждений ИИ через комбинированную обратную связь', 'desc': 'Статья представляет Critique-GRPO - новую систему обучения с подкреплением для улучшения рассуждений больших языковых моделей. Эта система объединяет числовую и текстовую обратную связь, что позволяет преодолеть ограничения существующих методов. Эксперименты показывают, что Critique-GRPO превосходит другие подходы на основе обучения с учителем и обучения с подкреплением на различных задачах рассуждения. Исследование также выявляет важные аспекты исследовательского поведения модели в процессе обучения.'}, 'en': {'title': 'Enhancing LLM Reasoning with Critique-GRPO: A Dual Feedback Approach', 'desc': 'Critique-GRPO is a reinforcement learning (RL) framework that enhances the reasoning abilities of large language models (LLMs) by combining numerical and natural language feedback. It addresses challenges faced by traditional RL methods that rely solely on numerical feedback, such as performance plateaus and ineffective self-reflection. By incorporating critiques in natural language, Critique-GRPO allows models to refine their responses and improve their performance on difficult tasks. Experimental results show that this approach significantly outperforms existing fine-tuning methods, achieving better results in various reasoning tasks.'}, 'zh': {'title': 'Critique-GRPO：自然语言与数值反馈的完美结合', 'desc': 'Critique-GRPO是一种结合数值反馈和自然语言反馈的强化学习框架，旨在提升大型语言模型（LLM）的推理能力。该框架解决了仅依赖数值反馈时遇到的性能停滞、自我反思效果有限和持续失败等挑战。通过利用自然语言反馈，Critique-GRPO能够在模型表现停滞时，生成正确的改进建议。实验结果表明，Critique-GRPO在多个复杂任务中表现优于现有的监督学习和强化学习方法，显著提高了模型的平均通过率。'}}}, {'id': 'https://huggingface.co/papers/2506.03099', 'title': 'TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via\n  Autoregressive Diffusion Models', 'url': 'https://huggingface.co/papers/2506.03099', 'abstract': 'TalkingMachines transforms a pretrained image-to-video model into an audio-driven avatar generator, supports infinite video streaming, and uses engineering optimizations for real-time performance.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present TalkingMachines -- an efficient framework that transforms pretrained video generation models into real-time, audio-driven character animators. TalkingMachines enables natural conversational experiences by integrating an audio large language model (LLM) with our video generation foundation model. Our primary contributions include: (1) We adapt a pretrained SOTA image-to-video DiT into an audio-driven avatar generation model of 18 billion parameters; (2) We enable infinite video streaming without error accumulation through asymmetric knowledge distillation from a bidirectional teacher model into a sparse causal, autoregressive student model; (3) We design a high-throughput, low-latency inference pipeline incorporating several key engineering optimizations such as: (a) disaggregation of the DiT and VAE decoder across separate devices, (b) efficient overlap of inter-device communication and computation using CUDA streams, (c) elimination of redundant recomputations to maximize frame-generation throughput. Please see demo videos here - https://aaxwaz.github.io/TalkingMachines/', 'score': 6, 'issue_id': 4134, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'eff27ca5fef5cdcf', 'authors': ['Chetwin Low', 'Weimin Wang'], 'affiliations': ['Character AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.03099.jpg', 'data': {'categories': ['#architecture', '#multimodal', '#inference', '#games', '#audio', '#video', '#optimization'], 'emoji': '🗣️', 'ru': {'title': 'Оживляем аватары: аудио-управляемая генерация видео в реальном времени', 'desc': 'TalkingMachines - это эффективная система, преобразующая предобученные модели генерации видео в аниматоры персонажей, управляемые аудио в реальном времени. Она адаптирует современную модель преобразования изображений в видео DiT для генерации аватаров на основе аудио с 18 миллиардами параметров. Система обеспечивает бесконечную потоковую передачу видео без накопления ошибок с помощью асимметричной дистилляции знаний. TalkingMachines также включает ряд инженерных оптимизаций для высокопроизводительного вывода с низкой задержкой.'}, 'en': {'title': 'Transforming Audio into Real-Time Avatar Animation', 'desc': 'TalkingMachines is a novel framework that converts existing image-to-video models into real-time, audio-responsive avatar generators. It combines a large language model (LLM) with a video generation foundation model to create engaging conversational avatars. The framework features a significant adaptation of a state-of-the-art (SOTA) image-to-video model, allowing for efficient infinite video streaming through advanced knowledge distillation techniques. Additionally, it incorporates engineering optimizations to enhance performance, such as distributing processing across devices and minimizing computation delays.'}, 'zh': {'title': '实时音频驱动的角色动画生成器', 'desc': '本文介绍了TalkingMachines，这是一个高效的框架，将预训练的视频生成模型转变为实时的音频驱动角色动画生成器。通过将音频大型语言模型（LLM）与视频生成基础模型结合，TalkingMachines能够实现自然的对话体验。我们的主要贡献包括：将一个预训练的最先进的图像到视频模型适配为一个具有180亿参数的音频驱动头像生成模型，以及通过不对称知识蒸馏实现无限视频流的生成。我们还设计了一个高吞吐量、低延迟的推理管道，结合了多项关键的工程优化。'}}}, {'id': 'https://huggingface.co/papers/2506.03355', 'title': 'Robustness in Both Domains: CLIP Needs a Robust Text Encoder', 'url': 'https://huggingface.co/papers/2506.03355', 'abstract': 'LEAF, an adversarial finetuning method, enhances the robustness of CLIP text encoders, improving zero-shot accuracy and multimodal retrieval performance under adversarial noise.  \t\t\t\t\tAI-generated summary \t\t\t\t Adversarial input attacks can cause a significant shift of CLIP embeddings. This can affect the downstream robustness of models incorporating CLIP in the pipeline, such as text-to-image generative models or large vision language models. While some efforts have been done towards making the CLIP image encoders robust, the robustness of text encoders remains unexplored. In this work, we cover this gap in the literature. We propose LEAF: an efficient adversarial finetuning method for the text domain, with the ability to scale to large CLIP models. Our models significantly improve the zero-shot adversarial accuracy in the text domain, while maintaining the vision performance provided by robust image encoders. When combined with text-to-image diffusion models, we can improve the generation quality under adversarial noise. When employing our robust CLIP encoders in multimodal retrieval tasks, we improve the recall under adversarial noise over standard CLIP models. Finally, we show that robust text encoders facilitate better reconstruction of input text from its embedding via direct optimization.', 'score': 5, 'issue_id': 4140, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'd9088aaea42f6fff', 'authors': ['Elias Abad Rocamora', 'Christian Schlarmann', 'Naman Deep Singh', 'Yongtao Wu', 'Matthias Hein', 'Volkan Cevher'], 'affiliations': ['LIONS - Ecole Polytechnique Federale de Lausanne, Switzerland', 'Tubingen AI center, University of Tubingen, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2506.03355.jpg', 'data': {'categories': ['#multimodal', '#training', '#optimization', '#diffusion', '#rlhf', '#security'], 'emoji': '🛡️', 'ru': {'title': 'LEAF: Повышение устойчивости CLIP к состязательным атакам', 'desc': 'LEAF - это метод состязательной доводки (adversarial finetuning), который повышает устойчивость текстовых энкодеров CLIP. Он улучшает точность классификации с нулевым обучением (zero-shot accuracy) и производительность мультимодального поиска в условиях состязательного шума. LEAF эффективно масштабируется на большие модели CLIP и сохраняет исходную производительность для визуальных задач. Метод также улучшает качество генерации изображений по тексту и реконструкцию текста из эмбеддингов в условиях шума.'}, 'en': {'title': 'Enhancing Text Encoder Robustness with LEAF', 'desc': 'This paper introduces LEAF, a novel adversarial finetuning method designed to enhance the robustness of CLIP text encoders against adversarial attacks. By addressing the vulnerability of text embeddings, LEAF significantly boosts zero-shot accuracy and improves performance in multimodal retrieval tasks, even under adversarial noise. The method not only preserves the strong performance of image encoders but also enhances the quality of text-to-image generation. Overall, LEAF fills a critical gap in the literature by ensuring that text encoders are as robust as their image counterparts, leading to better model performance in various applications.'}, 'zh': {'title': 'LEAF：提升CLIP文本编码器鲁棒性的对抗微调方法', 'desc': 'LEAF是一种对抗微调方法，旨在增强CLIP文本编码器的鲁棒性。通过对抗噪声的训练，LEAF显著提高了文本领域的零-shot准确率和多模态检索性能。该方法不仅保持了图像编码器的视觉性能，还能在文本到图像生成模型中提升生成质量。我们的研究填补了文本编码器鲁棒性研究的空白，展示了其在多模态任务中的优势。'}}}, {'id': 'https://huggingface.co/papers/2505.21541', 'title': 'DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via\n  Diffusion Transformers', 'url': 'https://huggingface.co/papers/2505.21541', 'abstract': 'DiffDecompose, a diffusion Transformer-based framework, effectively decomposes images into constituent layers with semantic prompts, addressing challenges in transparent layer decomposition.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have recently motivated great success in many generation tasks like object removal. Nevertheless, existing image decomposition methods struggle to disentangle semi-transparent or transparent layer occlusions due to mask prior dependencies, static object assumptions, and the lack of datasets. In this paper, we delve into a novel task: Layer-Wise Decomposition of Alpha-Composited Images, aiming to recover constituent layers from single overlapped images under the condition of semi-transparent/transparent alpha layer non-linear occlusion. To address challenges in layer ambiguity, generalization, and data scarcity, we first introduce AlphaBlend, the first large-scale and high-quality dataset for transparent and semi-transparent layer decomposition, supporting six real-world subtasks (e.g., translucent flare removal, semi-transparent cell decomposition, glassware decomposition). Building on this dataset, we present DiffDecompose, a diffusion Transformer-based framework that learns the posterior over possible layer decompositions conditioned on the input image, semantic prompts, and blending type. Rather than regressing alpha mattes directly, DiffDecompose performs In-Context Decomposition, enabling the model to predict one or multiple layers without per-layer supervision, and introduces Layer Position Encoding Cloning to maintain pixel-level correspondence across layers. Extensive experiments on the proposed AlphaBlend dataset and public LOGO dataset verify the effectiveness of DiffDecompose. The code and dataset will be available upon paper acceptance. Our code will be available at: https://github.com/Wangzt1121/DiffDecompose.', 'score': 5, 'issue_id': 4133, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 мая', 'en': 'May 24', 'zh': '5月24日'}, 'hash': 'cda6015909393ad0', 'authors': ['Zitong Wang', 'Hang Zhao', 'Qianyu Zhou', 'Xuequan Lu', 'Xiangtai Li', 'Yiren Song'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.21541.jpg', 'data': {'categories': ['#diffusion', '#open_source', '#cv', '#dataset'], 'emoji': '🖼️', 'ru': {'title': 'Умное разделение изображений на слои с помощью ИИ', 'desc': 'DiffDecompose - это новая система на основе диффузионного трансформера для декомпозиции изображений на семантические слои. Она решает проблемы разделения полупрозрачных и прозрачных наложений, с которыми не справлялись предыдущие методы. Авторы создали датасет AlphaBlend для обучения модели работе с различными типами прозрачности. DiffDecompose использует условное генерирование и позиционное кодирование слоев для точного восстановления составляющих изображения.'}, 'en': {'title': 'Revolutionizing Image Layer Decomposition with DiffDecompose', 'desc': "This paper introduces DiffDecompose, a novel framework that uses diffusion Transformers to decompose images into their individual layers, particularly focusing on transparent and semi-transparent layers. The authors highlight the limitations of existing methods in handling complex occlusions and propose a new dataset called AlphaBlend, which is designed to support various real-world image decomposition tasks. DiffDecompose employs In-Context Decomposition to predict multiple layers without needing direct supervision for each layer, enhancing its ability to generalize across different scenarios. The framework's effectiveness is validated through extensive experiments on the AlphaBlend dataset and the public LOGO dataset, showcasing its potential in image processing applications."}, 'zh': {'title': '透明层分解的新突破：DiffDecompose', 'desc': 'DiffDecompose 是一个基于扩散 Transformer 的框架，能够有效地将图像分解为组成层，并使用语义提示来解决透明层分解中的挑战。该方法针对半透明和透明图层的非线性遮挡问题，提出了一种新的任务：逐层分解 alpha 合成图像。为了解决层模糊、泛化能力和数据稀缺的问题，研究者们首次引入了 AlphaBlend 数据集，支持多种实际应用场景。DiffDecompose 通过上下文分解的方法，能够在没有逐层监督的情况下预测一个或多个层，展示了其在图像分解任务中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.03956', 'title': 'Adapt before Continual Learning', 'url': 'https://huggingface.co/papers/2506.03956', 'abstract': 'Adapting Pre-trained Models before the core CL process (ACL) improves Continual Learning by enhancing plasticity while maintaining stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Continual Learning (CL) seeks to enable neural networks to incrementally acquire new knowledge (plasticity) while retaining existing knowledge (stability). While pre-trained models (PTMs) have become pivotal in CL, prevailing approaches freeze the PTM backbone to preserve stability, limiting their plasticity, particularly when encountering significant domain gaps in incremental tasks. Conversely, sequentially finetuning the entire PTM risks catastrophic forgetting of generalizable knowledge, exposing a critical stability-plasticity trade-off. To address this challenge, we propose Adapting PTMs before the core CL process (ACL), a novel framework that refines the PTM backbone through a plug-and-play adaptation phase before learning each new task with existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by aligning embeddings with their original class prototypes while distancing them from others, theoretically and empirically shown to balance stability and plasticity. Extensive experiments demonstrate that ACL significantly improves CL performance across benchmarks and integrated methods, offering a versatile solution for PTM-based CL.', 'score': 4, 'issue_id': 4138, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '33f8cb4049923c1f', 'authors': ['Aojun Lu', 'Tao Feng', 'Hangjie Yuan', 'Chunhui Ding', 'Yanan Sun'], 'affiliations': ['College of Computer Science Sichuan University Chengdu, China', 'College of Computer Science and Technology Zhejiang University Hangzhou, China', 'Department of Computer Science and Technology Tsinghua University Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.03956.jpg', 'data': {'categories': ['#benchmark', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Балансировка стабильности и пластичности в непрерывном обучении', 'desc': 'Статья представляет новый подход к непрерывному обучению (Continual Learning) с использованием предобученных моделей. Авторы предлагают метод ACL (Adapting Pre-trained Models before the core CL process), который адаптирует основу предобученной модели перед обучением каждой новой задачи. ACL улучшает пластичность модели, выравнивая эмбеддинги с их исходными прототипами классов, при этом сохраняя стабильность. Эксперименты показывают, что ACL значительно повышает производительность непрерывного обучения на различных бенчмарках.'}, 'en': {'title': 'Enhancing Learning Flexibility with Pre-trained Models', 'desc': 'This paper introduces a new method called Adapting Pre-trained Models before the core Continual Learning (CL) process, which aims to improve how neural networks learn new information while keeping what they already know. The authors highlight the common issue where pre-trained models are often frozen to maintain stability, which limits their ability to adapt to new tasks. Their approach involves refining the pre-trained model before learning new tasks, allowing for better alignment of knowledge and reducing the risk of forgetting previous information. The results show that this method enhances the performance of CL systems, making it a promising solution for integrating pre-trained models in continual learning scenarios.'}, 'zh': {'title': '提升持续学习的可塑性与稳定性', 'desc': '这篇论文提出了一种新的框架，称为在核心持续学习过程之前调整预训练模型（ACL）。该方法旨在提高神经网络的可塑性，同时保持其稳定性，以便在增量学习中更好地适应新知识。通过在学习每个新任务之前对预训练模型进行适应性调整，ACL能够有效地对齐嵌入与原始类别原型，从而减少灾难性遗忘。实验结果表明，ACL在多个基准测试中显著提升了持续学习的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.03448', 'title': 'RefEdit: A Benchmark and Method for Improving Instruction-based Image\n  Editing Model on Referring Expressions', 'url': 'https://huggingface.co/papers/2506.03448', 'abstract': 'RefEdit, an instruction-based editing model trained on synthetic data, outperforms baselines in complex scene editing and referring expression tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent advances in inversion and instruction-based image editing, existing approaches primarily excel at editing single, prominent objects but significantly struggle when applied to complex scenes containing multiple entities. To quantify this gap, we first introduce RefEdit-Bench, a rigorous real-world benchmark rooted in RefCOCO, where even baselines trained on millions of samples perform poorly. To overcome this limitation, we introduce RefEdit -- an instruction-based editing model trained on our scalable synthetic data generation pipeline. Our RefEdit, trained on only 20,000 editing triplets, outperforms the Flux/SD3 model-based baselines trained on millions of data. Extensive evaluations across various benchmarks demonstrate that our model not only excels in referring expression tasks but also enhances performance on traditional benchmarks, achieving state-of-the-art results comparable to closed-source methods. We release data \\& checkpoint for reproducibility.', 'score': 4, 'issue_id': 4134, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '717f877ff02ce882', 'authors': ['Bimsara Pathiraja', 'Maitreya Patel', 'Shivam Singh', 'Yezhou Yang', 'Chitta Baral'], 'affiliations': ['Arizona State University'], 'pdf_title_img': 'assets/pdf/title_img/2506.03448.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#synthetic', '#cv', '#optimization', '#open_source'], 'emoji': '🖼️', 'ru': {'title': 'RefEdit: Прорыв в редактировании сложных изображений с помощью ИИ', 'desc': 'RefEdit - это модель редактирования изображений на основе инструкций, обученная на синтетических данных. Она превосходит базовые модели в задачах редактирования сложных сцен и работы с референсными выражениями. Авторы представили новый бенчмарк RefEdit-Bench для оценки таких моделей. RefEdit, обученная всего на 20 000 примерах, превзошла модели, обученные на миллионах образцов.'}, 'en': {'title': 'Revolutionizing Image Editing with Instruction-Based Learning', 'desc': 'RefEdit is a new model designed for editing images based on instructions, specifically focusing on complex scenes with multiple objects. Unlike previous models that struggle with such tasks, RefEdit is trained on a unique synthetic data generation pipeline, allowing it to learn effectively from a smaller dataset of 20,000 editing examples. The model significantly outperforms existing baselines, which were trained on millions of samples, in both referring expression tasks and traditional editing benchmarks. This advancement highlights the potential of instruction-based editing in achieving high performance in challenging image editing scenarios.'}, 'zh': {'title': 'RefEdit：复杂场景编辑的新突破', 'desc': 'RefEdit是一种基于指令的编辑模型，专门针对复杂场景中的编辑任务进行训练。与传统方法相比，RefEdit在处理多个实体的复杂场景时表现更为出色。我们还引入了RefEdit-Bench，这是一个基于RefCOCO的真实世界基准，用于量化现有方法的不足。通过使用合成数据生成管道，RefEdit在仅使用20,000个编辑三元组的情况下，超越了基于Flux/SD3模型的基线，展示了其在指代表达任务和传统基准上的优越性能。'}}}, {'id': 'https://huggingface.co/papers/2506.02945', 'title': 'Quantitative LLM Judges', 'url': 'https://huggingface.co/papers/2506.02945', 'abstract': "LLM-as-a-judge is a framework in which a large language model (LLM) automatically evaluates the output of another LLM. We propose quantitative LLM judges, which align evaluation scores of existing LLM judges to human scores in a given domain using regression models. The models are trained to improve the score of the original judge by using the judge's textual evaluation and score. We present four quantitative judges for different types of absolute and relative feedback, which showcases the generality and versatility of our framework. Our framework is more computationally efficient than supervised fine-tuning and can be more statistically efficient when human feedback is limited, which is expected in most applications of our work. We validate these claims empirically on four datasets using two base judges. Our experiments show that quantitative judges can effectively improve the predictive power of existing judges through post-hoc modeling.", 'score': 4, 'issue_id': 4133, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'de4ea9c8e4abb76a', 'authors': ['Aishwarya Sahoo', 'Jeevana Kruthi Karnuthala', 'Tushar Parmanand Budhwani', 'Pranchal Agarwal', 'Sankaran Vaidyanathan', 'Alexa Siu', 'Franck Dernoncourt', 'Jennifer Healey', 'Nedim Lipka', 'Ryan Rossi', 'Uttaran Bhattacharya', 'Branislav Kveton'], 'affiliations': ['Adobe Research', 'University of Massachusetts Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2506.02945.jpg', 'data': {'categories': ['#training', '#optimization', '#alignment', '#rlhf', '#dataset'], 'emoji': '⚖️', 'ru': {'title': 'LLM-судьи: автоматическая оценка языковых моделей с помощью регрессии', 'desc': 'Статья представляет фреймворк LLM-as-a-judge, где большая языковая модель оценивает результаты другой модели. Авторы предлагают количественных LLM-судей, которые согласуют оценки существующих судей с человеческими оценками в заданной области с помощью регрессионных моделей. Представлены четыре количественных судьи для различных типов абсолютной и относительной обратной связи. Эксперименты показывают, что количественные судьи могут эффективно улучшить предсказательную силу существующих судей через постобработку.'}, 'en': {'title': 'Enhancing LLM Evaluation with Quantitative Judges', 'desc': "The paper introduces a framework called LLM-as-a-judge, where a large language model (LLM) assesses the outputs of another LLM. It focuses on creating quantitative LLM judges that align their evaluation scores with human assessments using regression models. These models enhance the original judge's scoring by leveraging its textual evaluations and scores. The framework is shown to be more computationally and statistically efficient than traditional supervised fine-tuning, especially when human feedback is scarce, and is validated through experiments on multiple datasets."}, 'zh': {'title': '利用LLM提升评估效率的创新框架', 'desc': '本文提出了一种名为LLM-as-a-judge的框架，利用大型语言模型（LLM）自动评估另一个LLM的输出。我们引入了定量LLM评估者，通过回归模型将现有评估者的评分与人类评分对齐。该模型通过使用评估者的文本评价和评分来提高原始评估者的评分。我们的框架在计算效率上优于监督微调，并且在人工反馈有限的情况下，统计效率更高，适用于大多数应用场景。'}}}, {'id': 'https://huggingface.co/papers/2506.00482', 'title': 'BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM\n  Evaluation', 'url': 'https://huggingface.co/papers/2506.00482', 'abstract': 'BenchHub is a dynamic benchmark repository that aggregates and classifies datasets for large language models, facilitating domain-specific evaluations and improving model comparisons.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) continue to advance, the need for up-to-date and well-organized benchmarks becomes increasingly critical. However, many existing datasets are scattered, difficult to manage, and make it challenging to perform evaluations tailored to specific needs or domains, despite the growing importance of domain-specific models in areas such as math or code. In this paper, we introduce BenchHub, a dynamic benchmark repository that empowers researchers and developers to evaluate LLMs more effectively. BenchHub aggregates and automatically classifies benchmark datasets from diverse domains, integrating 303K questions across 38 benchmarks. It is designed to support continuous updates and scalable data management, enabling flexible and customizable evaluation tailored to various domains or use cases. Through extensive experiments with various LLM families, we demonstrate that model performance varies significantly across domain-specific subsets, emphasizing the importance of domain-aware benchmarking. We believe BenchHub can encourage better dataset reuse, more transparent model comparisons, and easier identification of underrepresented areas in existing benchmarks, offering a critical infrastructure for advancing LLM evaluation research.', 'score': 3, 'issue_id': 4137, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '0f7e970118d80f26', 'authors': ['Eunsu Kim', 'Haneul Yoo', 'Guijin Son', 'Hitesh Patel', 'Amit Agarwal', 'Alice Oh'], 'affiliations': ['KAIST', 'OnelineAI', 'Oracle', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00482.jpg', 'data': {'categories': ['#optimization', '#dataset', '#survey', '#benchmark'], 'emoji': '📊', 'ru': {'title': 'BenchHub: Универсальный инструмент для оценки языковых моделей', 'desc': 'BenchHub - это динамическое хранилище бенчмарков, которое агрегирует и классифицирует наборы данных для больших языковых моделей (LLM). Оно включает в себя 303 тысячи вопросов из 38 бенчмарков различных предметных областей. BenchHub позволяет проводить гибкую и настраиваемую оценку моделей, адаптированную под конкретные домены или сценарии использования. Эксперименты показали, что производительность моделей значительно варьируется в зависимости от предметной области, подчеркивая важность домен-ориентированного бенчмаркинга.'}, 'en': {'title': 'BenchHub: Streamlining Domain-Specific Evaluations for LLMs', 'desc': 'BenchHub is a repository designed to organize and classify datasets specifically for evaluating large language models (LLMs). It addresses the challenge of scattered and hard-to-manage datasets, which complicate domain-specific evaluations. By aggregating 303K questions across 38 benchmarks, BenchHub allows for flexible and customizable assessments tailored to various domains. The paper highlights the importance of domain-aware benchmarking, showing that model performance can vary significantly based on the specific dataset used.'}, 'zh': {'title': 'BenchHub：提升语言模型评估的动态基准库', 'desc': 'BenchHub是一个动态基准库，专门用于聚合和分类大型语言模型的数据集，旨在促进特定领域的评估并改善模型比较。随着大型语言模型的不断进步，更新和组织良好的基准变得越来越重要。BenchHub集成了来自38个基准的303K问题，支持持续更新和可扩展的数据管理，允许根据不同领域或用例进行灵活的评估。通过对不同语言模型的广泛实验，我们展示了模型性能在特定领域子集之间的显著差异，强调了领域感知基准的重要性。'}}}, {'id': 'https://huggingface.co/papers/2505.23807', 'title': 'DLP: Dynamic Layerwise Pruning in Large Language Models', 'url': 'https://huggingface.co/papers/2505.23807', 'abstract': 'A dynamic layerwise pruning method adaptively determines layer importance by combining model weights and activation information to maintain performance in large language models at high sparsity.  \t\t\t\t\tAI-generated summary \t\t\t\t Pruning has recently been widely adopted to reduce the parameter scale and improve the inference efficiency of Large Language Models (LLMs). Mainstream pruning techniques often rely on uniform layerwise pruning strategies, which can lead to severe performance degradation at high sparsity levels. Recognizing the varying contributions of different layers in LLMs, recent studies have shifted their focus toward non-uniform layerwise pruning. However, these approaches often rely on pre-defined values, which can result in suboptimal performance. To overcome these limitations, we propose a novel method called Dynamic Layerwise Pruning (DLP). This approach adaptively determines the relative importance of each layer by integrating model weights with input activation information, assigning pruning rates accordingly. Experimental results show that DLP effectively preserves model performance at high sparsity levels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the perplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7% compared to state-of-the-art methods. Moreover, DLP is compatible with various existing LLM compression techniques and can be seamlessly integrated into Parameter-Efficient Fine-Tuning (PEFT). We release the code at https://github.com/ironartisan/DLP to facilitate future research.', 'score': 3, 'issue_id': 4133, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'a817afc0cdd35d8d', 'authors': ['Yuli Chen', 'Bo Cheng', 'Jiale Han', 'Yingying Zhang', 'Yingting Li', 'Shuhao Zhang'], 'affiliations': ['Hong Kong University of Science and Technology, Hong Kong, China', 'State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.23807.jpg', 'data': {'categories': ['#optimization', '#inference', '#training'], 'emoji': '✂️', 'ru': {'title': 'Умная обрезка слоев для эффективных языковых моделей', 'desc': 'Предложен новый метод динамической послойной обрезки (DLP) для больших языковых моделей. DLP адаптивно определяет важность каждого слоя, комбинируя информацию о весах модели и активациях. Это позволяет сохранить производительность модели при высоком уровне разреженности. Эксперименты показали, что DLP превосходит существующие методы обрезки для различных языковых моделей.'}, 'en': {'title': 'Dynamic Layerwise Pruning: Smart Sparsity for Language Models', 'desc': 'This paper introduces a new method called Dynamic Layerwise Pruning (DLP) that improves the efficiency of large language models (LLMs) by adaptively determining the importance of each layer. Unlike traditional pruning methods that apply uniform strategies, DLP combines model weights and activation data to assign specific pruning rates to different layers. This approach helps maintain model performance even at high levels of sparsity, which is crucial for effective model compression. Experimental results demonstrate that DLP significantly enhances accuracy and reduces perplexity in LLMs compared to existing techniques.'}, 'zh': {'title': '动态剪枝，智能保持性能！', 'desc': '动态层级剪枝方法通过结合模型权重和激活信息，自适应地确定每一层的重要性，从而在高稀疏性下保持大型语言模型的性能。传统的剪枝技术通常采用均匀层级剪枝策略，这可能导致在高稀疏性水平下性能显著下降。动态层级剪枝（DLP）方法克服了这一限制，能够根据输入激活信息动态调整剪枝率。实验结果表明，DLP在多个大型语言模型中有效地保持了高稀疏性下的模型性能。'}}}, {'id': 'https://huggingface.co/papers/2506.04133', 'title': 'TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management\n  in LLM-based Agentic Multi-Agent Systems', 'url': 'https://huggingface.co/papers/2506.04133', 'abstract': 'A review of trust, risk, and security management in LLM-based agentic multi-agent systems, examining governance, explainability, ModelOps, and privacy/security.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic AI systems, built on large language models (LLMs) and deployed in multi-agent configurations, are redefining intelligent autonomy, collaboration and decision-making across enterprise and societal domains. This review presents a structured analysis of Trust, Risk, and Security Management (TRiSM) in the context of LLM-based agentic multi-agent systems (AMAS). We begin by examining the conceptual foundations of agentic AI, its architectural differences from traditional AI agents, and the emerging system designs that enable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is then detailed through four pillars governance, explainability, ModelOps, and privacy/security each contextualized for agentic LLMs. We identify unique threat vectors and introduce a comprehensive risk taxonomy for the agentic AI applications, supported by case studies illustrating real-world vulnerabilities. Furthermore, the paper also surveys trust-building mechanisms, transparency and oversight techniques, and state-of-the-art explainability strategies in distributed LLM agent systems. Additionally, metrics for evaluating trust, interpretability, and human-centered performance are reviewed alongside open benchmarking challenges. Security and privacy are addressed through encryption, adversarial defense, and compliance with evolving AI regulations. The paper concludes with a roadmap for responsible agentic AI, proposing research directions to align emerging multi-agent systems with robust TRiSM principles for safe, accountable, and transparent deployment.', 'score': 2, 'issue_id': 4133, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'a0a258935ed39508', 'authors': ['Shaina Raza', 'Ranjan Sapkota', 'Manoj Karkee', 'Christos Emmanouilidis'], 'affiliations': ['Cornell University, USA', 'University of Groningen, Netherlands', 'Vector Institute, Toronto, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2506.04133.jpg', 'data': {'categories': ['#training', '#architecture', '#survey', '#agents', '#multimodal', '#security', '#alignment', '#benchmark', '#interpretability'], 'emoji': '🤖', 'ru': {'title': 'Безопасность и доверие в эпоху агентного ИИ', 'desc': 'Статья представляет структурированный анализ управления доверием, рисками и безопасностью (TRiSM) в контексте агентных мультиагентных систем на основе больших языковых моделей (LLM). Рассматриваются четыре основных аспекта: управление, объяснимость, ModelOps и конфиденциальность/безопасность. Авторы идентифицируют уникальные векторы угроз и представляют комплексную таксономию рисков для приложений агентного ИИ. Статья также исследует механизмы построения доверия, методы обеспечения прозрачности и надзора, а также современные стратегии объяснимости в распределенных системах агентов LLM.'}, 'en': {'title': 'Navigating Trust and Security in Agentic AI Systems', 'desc': 'This paper reviews the management of trust, risk, and security in multi-agent systems that use large language models (LLMs). It discusses how these agentic AI systems differ from traditional AI, focusing on their ability to operate autonomously and collaboratively. The authors outline four key areas of Trust, Risk, and Security Management (TRiSM): governance, explainability, ModelOps, and privacy/security, providing a framework for understanding the unique challenges these systems face. The paper also highlights the importance of building trust and ensuring transparency in these systems, while proposing future research directions for responsible deployment.'}, 'zh': {'title': '构建安全透明的代理人工智能系统', 'desc': '本文回顾了基于大型语言模型（LLM）的代理多智能体系统中的信任、风险和安全管理（TRiSM）。我们分析了代理人工智能的概念基础及其与传统人工智能代理的架构差异，并探讨了支持可扩展自主性的系统设计。文章详细阐述了TRiSM的四个支柱：治理、可解释性、模型操作和隐私/安全，并为代理LLM提供了具体的背景。最后，提出了负责任的代理人工智能的路线图，建议研究方向以确保新兴多智能体系统的安全、透明和负责任的部署。'}}}, {'id': 'https://huggingface.co/papers/2506.04034', 'title': 'Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning', 'url': 'https://huggingface.co/papers/2506.04034', 'abstract': 'Object referring aims to detect all objects in an image that match a given natural language description. We argue that a robust object referring model should be grounded, meaning its predictions should be both explainable and faithful to the visual content. Specifically, it should satisfy two key properties: 1) Verifiable, by producing interpretable reasoning that justifies its predictions and clearly links them to visual evidence; and 2) Trustworthy, by learning to abstain when no object in the image satisfies the given expression. However, most methods treat referring as a direct bounding box prediction task, offering limited interpretability and struggling to reject expressions with no matching object. In this work, we propose Rex-Thinker, a model that formulates object referring as an explicit CoT reasoning task. Given a referring expression, we first identify all candidate object instances corresponding to the referred object category. Rex-Thinker then performs step-by-step reasoning over each candidate to assess whether it matches the given expression, before making a final prediction. To support this paradigm, we construct a large-scale CoT-style referring dataset named HumanRef-CoT by prompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a structured planning, action, and summarization format, enabling the model to learn decomposed, interpretable reasoning over object candidates. We then train Rex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach the model how to perform structured reasoning, followed by GRPO-based RL learning to improve accuracy and generalization. Experiments show that our approach outperforms standard baselines in both precision and interpretability on in-domain evaluation, while also demonstrating improved ability to reject hallucinated outputs and strong generalization in out-of-domain settings.', 'score': 2, 'issue_id': 4133, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '9d3dcbdd5158f101', 'authors': ['Qing Jiang', 'Xingyu Chen', 'Zhaoyang Zeng', 'Junzhi Yu', 'Lei Zhang'], 'affiliations': ['International Digital Economy Academy (IDEA)', 'Peking University', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.04034.jpg', 'data': {'categories': ['#cv', '#rl', '#training', '#reasoning', '#hallucinations', '#interpretability', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Интерпретируемое объектное реферирование через пошаговые рассуждения', 'desc': 'Статья представляет новый подход к задаче объектного реферирования в компьютерном зрении, названный Rex-Thinker. Модель использует пошаговое рассуждение для оценки соответствия объектов заданному описанию, что повышает интерпретируемость и надежность предсказаний. Авторы создали датасет HumanRef-CoT для обучения модели структурированным рассуждениям. Rex-Thinker обучается в два этапа: контролируемая тонкая настройка и обучение с подкреплением, что улучшает точность и обобщающую способность модели.'}, 'en': {'title': 'Rex-Thinker: Grounded Object Referring with Explainable Reasoning', 'desc': 'This paper introduces Rex-Thinker, a model designed to enhance object referring in images by incorporating explainable and trustworthy reasoning. Unlike traditional methods that focus solely on bounding box predictions, Rex-Thinker employs a Chain of Thought (CoT) reasoning approach to evaluate candidate objects against natural language descriptions. The model is trained on a new dataset, HumanRef-CoT, which facilitates structured reasoning through a systematic planning and summarization process. Results indicate that Rex-Thinker not only improves precision and interpretability but also effectively rejects irrelevant predictions, showcasing its robustness in various scenarios.'}, 'zh': {'title': 'Rex-Thinker：可解释的物体指代模型', 'desc': '本文提出了一种新的物体指代模型Rex-Thinker，旨在通过明确的链式推理任务来检测与自然语言描述匹配的图像中的所有物体。该模型强调可验证性和可信性，确保其预测能够解释并与视觉证据相连。Rex-Thinker通过逐步推理候选物体实例，判断其是否符合给定的描述，从而做出最终预测。实验结果表明，该方法在精确度和可解释性方面优于传统基线，并在拒绝虚假输出和跨领域泛化能力上表现出色。'}}}, {'id': 'https://huggingface.co/papers/2506.03951', 'title': 'Rethinking the Stability-Plasticity Trade-off in Continual Learning from\n  an Architectural Perspective', 'url': 'https://huggingface.co/papers/2506.03951', 'abstract': 'A novel framework, Dual-Arch, enhances Continual Learning by addressing the stability-plasticity dilemma at the architectural level using two specialized networks.  \t\t\t\t\tAI-generated summary \t\t\t\t The quest for Continual Learning (CL) seeks to empower neural networks with the ability to learn and adapt incrementally. Central to this pursuit is addressing the stability-plasticity dilemma, which involves striking a balance between two conflicting objectives: preserving previously learned knowledge and acquiring new knowledge. While numerous CL methods aim to achieve this trade-off, they often overlook the impact of network architecture on stability and plasticity, restricting the trade-off to the parameter level. In this paper, we delve into the conflict between stability and plasticity at the architectural level. We reveal that under an equal parameter constraint, deeper networks exhibit better plasticity, while wider networks are characterized by superior stability. To address this architectural-level dilemma, we introduce a novel framework denoted Dual-Arch, which serves as a plug-in component for CL. This framework leverages the complementary strengths of two distinct and independent networks: one dedicated to plasticity and the other to stability. Each network is designed with a specialized and lightweight architecture, tailored to its respective objective. Extensive experiments demonstrate that Dual-Arch enhances the performance of existing CL methods while being up to 87% more compact in terms of parameters.', 'score': 2, 'issue_id': 4140, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '19d2cecb53fd6998', 'authors': ['Aojun Lu', 'Hangjie Yuan', 'Tao Feng', 'Yanan Sun'], 'affiliations': ['College of Computer Science and Technology, Zhejiang University, Hangzhou, China', 'College of Computer Science, Sichuan University, Chengdu, China', 'Department of Computer Science and Technology, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.03951.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Двойная архитектура для эффективного непрерывного обучения', 'desc': 'Статья представляет новую архитектуру Dual-Arch для непрерывного обучения, которая решает дилемму стабильности-пластичности на архитектурном уровне. Авторы обнаружили, что при равном количестве параметров более глубокие сети обладают лучшей пластичностью, а более широкие - лучшей стабильностью. Dual-Arch использует две отдельные сети: одну для пластичности, другую для стабильности, каждая со специализированной архитектурой. Эксперименты показали, что Dual-Arch улучшает производительность существующих методов непрерывного обучения, при этом используя до 87% меньше параметров.'}, 'en': {'title': 'Dual-Arch: Balancing Stability and Plasticity in Continual Learning', 'desc': 'The paper presents a new framework called Dual-Arch that improves Continual Learning (CL) by tackling the stability-plasticity dilemma through architectural innovations. It highlights that deeper networks are better at learning new information (plasticity), while wider networks excel at retaining old knowledge (stability). By utilizing two specialized networks—one focused on plasticity and the other on stability—Dual-Arch effectively balances these competing needs. Experimental results show that this approach not only enhances the performance of existing CL methods but also reduces the model size by up to 87%.'}, 'zh': {'title': '双网络架构，平衡学习稳定性与可塑性', 'desc': '本文提出了一种新框架Dual-Arch，旨在通过在架构层面解决稳定性与可塑性之间的矛盾来增强持续学习。持续学习的目标是使神经网络能够逐步学习和适应新知识，同时保持对旧知识的记忆。研究表明，在相同参数约束下，深层网络具有更好的可塑性，而宽层网络则表现出更高的稳定性。Dual-Arch框架结合了两个独立网络的优势，一个专注于可塑性，另一个专注于稳定性，从而提高了现有持续学习方法的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.03614', 'title': 'VLMs Can Aggregate Scattered Training Patches', 'url': 'https://huggingface.co/papers/2506.03614', 'abstract': 'VLMs exhibit visual stitching, an ability to integrate fragmented visual information, which enables harmful content to evade data moderation and be reconstructed during inference.  \t\t\t\t\tAI-generated summary \t\t\t\t One way to mitigate risks in vision-language models (VLMs) is to remove dangerous samples in their training data. However, such data moderation can be easily bypassed when harmful images are split into small, benign-looking patches, scattered across many training samples. VLMs may then learn to piece these fragments together during training and generate harmful responses at inference, either from full images or text references. For instance, if trained on image patches from a bloody scene paired with the descriptions "safe," VLMs may later describe, the full image or a text reference to the scene, as "safe." We define the core ability of VLMs enabling this attack as visual stitching -- the ability to integrate visual information spread across multiple training samples that share the same textual descriptions. In our work, we first demonstrate visual stitching abilities in common open-source VLMs on three datasets where each image is labeled with a unique synthetic ID: we split each (image, ID) pair into {(patch, ID)} pairs at different granularity for finetuning, and we find that tuned models can verbalize the correct IDs from full images or text reference. Building on this, we simulate the adversarial data poisoning scenario mentioned above by using patches from dangerous images and replacing IDs with text descriptions like ``safe\'\' or ``unsafe\'\', demonstrating how harmful content can evade moderation in patches and later be reconstructed through visual stitching, posing serious VLM safety risks. Code is available at https://github.com/ZHZisZZ/visual-stitching.', 'score': 2, 'issue_id': 4137, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '180b48fc19d50b80', 'authors': ['Zhanhui Zhou', 'Lingjie Chen', 'Chao Yang', 'Chaochao Lu'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2506.03614.jpg', 'data': {'categories': ['#open_source', '#data', '#dataset', '#multimodal', '#cv', '#benchmark', '#security', '#ethics'], 'emoji': '🧩', 'ru': {'title': 'Визуальное сшивание: скрытая угроза в моделях компьютерного зрения', 'desc': "Это исследование раскрывает феномен 'визуального сшивания' в моделях компьютерного зрения и обработки естественного языка (VLM). Авторы демонстрируют, как VLM способны интегрировать фрагментированную визуальную информацию, что позволяет обходить модерацию данных и реконструировать нежелательный контент во время вывода. Эксперименты показывают, что модели могут собирать полные изображения из небольших, безобидных на вид фрагментов, разбросанных по множеству обучающих образцов. Это открытие подчеркивает серьезные риски безопасности при использовании VLM и необходимость разработки новых методов защиты."}, 'en': {'title': 'Visual Stitching: A Hidden Risk in Vision-Language Models', 'desc': 'This paper discusses a vulnerability in vision-language models (VLMs) known as visual stitching, which allows these models to reconstruct harmful content from fragmented visual information. The authors show that when dangerous images are divided into small patches and mixed with benign data, VLMs can still learn to piece them together during training. This leads to a situation where the models can generate harmful outputs by associating safe descriptions with dangerous images. The study highlights the risks of data moderation being bypassed and emphasizes the need for improved safety measures in VLMs.'}, 'zh': {'title': '视觉拼接：VLMs的安全隐患', 'desc': '本论文探讨了视觉语言模型（VLMs）中的视觉拼接能力，这种能力使得模型能够整合分散的视觉信息。研究表明，当有害图像被分割成小的、看似无害的片段时，数据的审查可以被轻易绕过。VLMs在训练过程中可能会学习将这些片段拼接在一起，从而在推理时生成有害的响应。我们通过实验展示了这一现象，并模拟了对抗性数据中毒的场景，揭示了VLMs在安全性方面的潜在风险。'}}}, {'id': 'https://huggingface.co/papers/2506.02294', 'title': 'Improving Knowledge Distillation Under Unknown Covariate Shift Through\n  Confidence-Guided Data Augmentation', 'url': 'https://huggingface.co/papers/2506.02294', 'abstract': 'A diffusion-based data augmentation strategy improves robustness in knowledge distillation by generating challenging samples, enhancing accuracy and spurious feature resilience.  \t\t\t\t\tAI-generated summary \t\t\t\t Large foundation models trained on extensive datasets demonstrate strong zero-shot capabilities in various domains. To replicate their success when data and model size are constrained, knowledge distillation has become an established tool for transferring knowledge from foundation models to small student networks. However, the effectiveness of distillation is critically limited by the available training data. This work addresses the common practical issue of covariate shift in knowledge distillation, where spurious features appear during training but not at test time. We ask the question: when these spurious features are unknown, yet a robust teacher is available, is it possible for a student to also become robust to them? We address this problem by introducing a novel diffusion-based data augmentation strategy that generates images by maximizing the disagreement between the teacher and the student, effectively creating challenging samples that the student struggles with. Experiments demonstrate that our approach significantly improves worst group and mean group accuracy on CelebA and SpuCo Birds as well as the spurious mAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art diffusion-based data augmentation baselines', 'score': 2, 'issue_id': 4133, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '740d99ccc158d514', 'authors': ['Niclas Popp', 'Kevin Alexander Laube', 'Matthias Hein', 'Lukas Schott'], 'affiliations': ['Bosch Center for Artificial Intelligence', 'University of Tübingen'], 'pdf_title_img': 'assets/pdf/title_img/2506.02294.jpg', 'data': {'categories': ['#data', '#transfer_learning', '#training', '#optimization', '#diffusion', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Повышение устойчивости моделей через генерацию сложных примеров', 'desc': 'Статья представляет новую стратегию аугментации данных на основе диффузии для улучшения робастности в процессе дистилляции знаний. Метод генерирует сложные образцы, максимизируя разногласие между учителем и учеником, что помогает преодолеть проблему ковариационного сдвига. Эксперименты показывают значительное улучшение точности на наихудших группах и средней точности по группам на датасетах CelebA и SpuCo Birds. Подход превосходит современные методы аугментации данных на основе диффузии.'}, 'en': {'title': 'Boosting Student Robustness with Diffusion Data Augmentation', 'desc': 'This paper presents a new data augmentation method using diffusion processes to enhance knowledge distillation. The approach generates challenging samples that help student networks learn to be more robust against spurious features that may not appear during testing. By maximizing the disagreement between a robust teacher model and the student model, the method effectively prepares the student for real-world scenarios where data may shift. Experiments show that this strategy improves accuracy and resilience against spurious features in various datasets, outperforming existing methods.'}, 'zh': {'title': '基于扩散的数据增强提升知识蒸馏鲁棒性', 'desc': '本文提出了一种基于扩散的数据增强策略，以提高知识蒸馏中的鲁棒性。该方法通过生成具有挑战性的样本，增强了学生网络对虚假特征的抵抗力。实验结果表明，在CelebA和SpuCo Birds数据集上，该策略显著提高了最差组和平均组的准确率。通过最大化教师和学生之间的分歧，本文有效地解决了知识蒸馏中的协变量偏移问题。'}}}, {'id': 'https://huggingface.co/papers/2506.01344', 'title': 'Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic\n  Agents', 'url': 'https://huggingface.co/papers/2506.01344', 'abstract': "Flowcharts are a critical tool for visualizing decision-making processes. However, their non-linear structure and complex visual-textual relationships make it challenging to interpret them using LLMs, as vision-language models frequently hallucinate nonexistent connections and decision paths when analyzing these diagrams. This leads to compromised reliability for automated flowchart processing in critical domains such as logistics, health, and engineering. We introduce the task of Fine-grained Flowchart Attribution, which traces specific components grounding a flowchart referring LLM response. Flowchart Attribution ensures the verifiability of LLM predictions and improves explainability by linking generated responses to the flowchart's structure. We propose FlowPathAgent, a neurosymbolic agent that performs fine-grained post hoc attribution through graph-based reasoning. It first segments the flowchart, then converts it into a structured symbolic graph, and then employs an agentic approach to dynamically interact with the graph, to generate attribution paths. Additionally, we present FlowExplainBench, a novel benchmark for evaluating flowchart attributions across diverse styles, domains, and question types. Experimental results show that FlowPathAgent mitigates visual hallucinations in LLM answers over flowchart QA, outperforming strong baselines by 10-14% on our proposed FlowExplainBench dataset.", 'score': 2, 'issue_id': 4133, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '788495117e4bf1d7', 'authors': ['Manan Suri', 'Puneet Mathur', 'Nedim Lipka', 'Franck Dernoncourt', 'Ryan A. Rossi', 'Vivek Gupta', 'Dinesh Manocha'], 'affiliations': ['Adobe', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2506.01344.jpg', 'data': {'categories': ['#graphs', '#cv', '#reasoning', '#agents', '#hallucinations', '#multimodal', '#benchmark', '#interpretability'], 'emoji': '🔀', 'ru': {'title': 'Точная интерпретация блок-схем с помощью нейросимволического агента', 'desc': 'Статья представляет задачу точной атрибуции блок-схем и агента FlowPathAgent для ее решения. Авторы разработали нейросимволический подход, который сегментирует блок-схему, преобразует ее в структурированный символьный граф и использует агентный метод для генерации путей атрибуции. Также представлен новый бенчмарк FlowExplainBench для оценки атрибуций блок-схем. Результаты показывают, что FlowPathAgent снижает визуальные галлюцинации в ответах языковых моделей на вопросы по блок-схемам, превосходя базовые методы на 10-14%.'}, 'en': {'title': 'Enhancing Flowchart Interpretation with Fine-grained Attribution', 'desc': 'This paper addresses the challenges of interpreting flowcharts using large language models (LLMs) due to their complex structures and potential for hallucination. It introduces Fine-grained Flowchart Attribution, a method that links LLM responses to specific components of flowcharts, enhancing the reliability and explainability of automated processing. The authors present FlowPathAgent, a neurosymbolic agent that utilizes graph-based reasoning to segment flowcharts and create structured symbolic graphs for dynamic interaction. Experimental results demonstrate that FlowPathAgent significantly reduces hallucinations in LLM outputs, achieving improved performance on the newly introduced FlowExplainBench benchmark.'}, 'zh': {'title': '提升流程图解析的可靠性与可解释性', 'desc': '本论文介绍了一种新的任务，称为细粒度流程图归因，旨在提高大型语言模型（LLM）在处理流程图时的可靠性和可解释性。我们提出了FlowPathAgent，这是一种神经符号代理，通过图形推理进行细粒度的后期归因。该代理首先对流程图进行分割，然后将其转换为结构化的符号图，并动态与图进行交互，以生成归因路径。实验结果表明，FlowPathAgent在流程图问答中减少了视觉幻觉，相较于强基线提高了10-14%的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.03817', 'title': 'Survey of Active Learning Hyperparameters: Insights from a Large-Scale\n  Experimental Grid', 'url': 'https://huggingface.co/papers/2506.03817', 'abstract': 'Annotating data is a time-consuming and costly task, but it is inherently required for supervised machine learning. Active Learning (AL) is an established method that minimizes human labeling effort by iteratively selecting the most informative unlabeled samples for expert annotation, thereby improving the overall classification performance. Even though AL has been known for decades, AL is still rarely used in real-world applications. As indicated in the two community web surveys among the NLP community about AL, two main reasons continue to hold practitioners back from using AL: first, the complexity of setting AL up, and second, a lack of trust in its effectiveness. We hypothesize that both reasons share the same culprit: the large hyperparameter space of AL. This mostly unexplored hyperparameter space often leads to misleading and irreproducible AL experiment results. In this study, we first compiled a large hyperparameter grid of over 4.6 million hyperparameter combinations, second, recorded the performance of all combinations in the so-far biggest conducted AL study, and third, analyzed the impact of each hyperparameter in the experiment results. In the end, we give recommendations about the influence of each hyperparameter, demonstrate the surprising influence of the concrete AL strategy implementation, and outline an experimental study design for reproducible AL experiments with minimal computational effort, thus contributing to more reproducible and trustworthy AL research in the future.', 'score': 1, 'issue_id': 4140, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '24782a46b48333d1', 'authors': ['Julius Gonsior', 'Tim Rieß', 'Anja Reusch', 'Claudio Hartmann', 'Maik Thiele', 'Wolfgang Lehner'], 'affiliations': ['Hochschule fur Technik und Wirtschaft Dresden', 'Technion - Israeli Institute of Technology', 'Technische Universitat Dresden'], 'pdf_title_img': 'assets/pdf/title_img/2506.03817.jpg', 'data': {'categories': ['#training', '#data', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Раскрывая секреты гиперпараметров в активном обучении', 'desc': 'Это исследование посвящено активному обучению (Active Learning, AL) в машинном обучении, которое помогает минимизировать усилия по разметке данных. Авторы провели масштабный эксперимент, изучив более 4,6 миллионов комбинаций гиперпараметров AL. Они проанализировали влияние каждого гиперпараметра на результаты и предложили рекомендации по настройке AL. Исследование направлено на повышение воспроизводимости и надежности экспериментов с AL в будущем.'}, 'en': {'title': 'Unlocking Active Learning: Simplifying Setup for Trustworthy Results', 'desc': 'This paper addresses the challenges of using Active Learning (AL) in supervised machine learning, particularly the complexities and trust issues that hinder its adoption. The authors compiled a vast hyperparameter grid with over 4.6 million combinations to analyze how different settings affect AL performance. They conducted the largest AL study to date, recording the results and examining the impact of each hyperparameter on the outcomes. The findings provide insights and recommendations for setting up reproducible AL experiments, aiming to enhance the reliability and effectiveness of AL in real-world applications.'}, 'zh': {'title': '优化主动学习，提升标注效率', 'desc': '标注数据是一个耗时且成本高昂的任务，但这是监督学习所必需的。主动学习（Active Learning, AL）是一种通过迭代选择最具信息量的未标记样本来减少人工标注工作的方法，从而提高分类性能。尽管AL已经存在了几十年，但在实际应用中仍然很少被使用。本文研究了AL中超参数空间的复杂性，提出了一个包含460万种超参数组合的大型网格，并分析了每个超参数对实验结果的影响，以促进更可靠的AL研究。'}}}, {'id': 'https://huggingface.co/papers/2506.03538', 'title': 'Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian\n  Splatting', 'url': 'https://huggingface.co/papers/2506.03538', 'abstract': 'A novel Asymmetric Dual 3DGS framework improves 3D reconstruction by training dual models with consistency constraints and divergent masking, outperforming existing methods with high efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D reconstruction from in-the-wild images remains a challenging task due to inconsistent lighting conditions and transient distractors. Existing methods typically rely on heuristic strategies to handle the low-quality training data, which often struggle to produce stable and consistent reconstructions, frequently resulting in visual artifacts. In this work, we propose Asymmetric Dual 3DGS, a novel framework that leverages the stochastic nature of these artifacts: they tend to vary across different training runs due to minor randomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS) models in parallel, enforcing a consistency constraint that encourages convergence on reliable scene geometry while suppressing inconsistent artifacts. To prevent the two models from collapsing into similar failure modes due to confirmation bias, we introduce a divergent masking strategy that applies two complementary masks: a multi-cue adaptive mask and a self-supervised soft mask, which leads to an asymmetric training process of the two models, reducing shared error modes. In addition, to improve the efficiency of model training, we introduce a lightweight variant called Dynamic EMA Proxy, which replaces one of the two models with a dynamically updated Exponential Moving Average (EMA) proxy, and employs an alternating masking strategy to preserve divergence. Extensive experiments on challenging real-world datasets demonstrate that our method consistently outperforms existing approaches while achieving high efficiency. Codes and trained models will be released.', 'score': 1, 'issue_id': 4140, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'c9dadfe8cdfe5d4c', 'authors': ['Chengqi Li', 'Zhihao Shi', 'Yangdi Lu', 'Wenbo He', 'Xiangyu Xu'], 'affiliations': ['Department of Computing and Software McMaster University', 'Department of Electrical and Computer Engineering McMaster University', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.03538.jpg', 'data': {'categories': ['#training', '#3d', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Двойное зрение для точной 3D-реконструкции', 'desc': 'Предложен новый метод Asymmetric Dual 3DGS для улучшения 3D-реконструкции изображений. Он основан на обучении двух параллельных моделей 3D Gaussian Splatting с ограничением согласованности и дивергентным маскированием. Метод превосходит существующие подходы, подавляя артефакты и выделяя надежную геометрию сцены. Также представлен облегченный вариант Dynamic EMA Proxy для повышения эффективности обучения.'}, 'en': {'title': 'Revolutionizing 3D Reconstruction with Asymmetric Dual Models', 'desc': 'The paper presents the Asymmetric Dual 3DGS framework, which enhances 3D reconstruction from images taken in varied conditions. It addresses the challenges of inconsistent lighting and distracting elements by training two models simultaneously with a focus on consistency and divergence. The framework uses a unique masking strategy to prevent the models from converging on the same errors, thus improving the quality of the reconstructions. Experimental results show that this approach is more efficient and effective than current methods, leading to better performance in real-world scenarios.'}, 'zh': {'title': '非对称双重3DGS框架：高效的3D重建新方法', 'desc': '本文提出了一种新颖的非对称双重3DGS框架，旨在提高3D重建的效果。该方法通过训练两个模型并施加一致性约束，来减少不一致的视觉伪影。我们引入了多线索自适应掩码和自监督软掩码，确保两个模型在训练过程中保持差异，从而降低共享错误模式。实验结果表明，该方法在处理真实世界数据集时，表现出更高的效率和更好的重建质量。'}}}, {'id': 'https://huggingface.co/papers/2506.02680', 'title': 'Solving Inverse Problems with FLAIR', 'url': 'https://huggingface.co/papers/2506.02680', 'abstract': 'FLAIR, a novel training-free variational framework, leverages flow-based generative models to enhance inverse problem solutions, achieving superior reconstruction quality and sample diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Flow-based latent generative models such as Stable Diffusion 3 are able to generate images with remarkable quality, even enabling photorealistic text-to-image generation. Their impressive performance suggests that these models should also constitute powerful priors for inverse imaging problems, but that approach has not yet led to comparable fidelity. There are several key obstacles: (i) the encoding into a lower-dimensional latent space makes the underlying (forward) mapping non-linear; (ii) the data likelihood term is usually intractable; and (iii) learned generative models struggle to recover rare, atypical data modes during inference. We present FLAIR, a novel training free variational framework that leverages flow-based generative models as a prior for inverse problems. To that end, we introduce a variational objective for flow matching that is agnostic to the type of degradation, and combine it with deterministic trajectory adjustments to recover atypical modes. To enforce exact consistency with the observed data, we decouple the optimization of the data fidelity and regularization terms. Moreover, we introduce a time-dependent calibration scheme in which the strength of the regularization is modulated according to off-line accuracy estimates. Results on standard imaging benchmarks demonstrate that FLAIR consistently outperforms existing diffusion- and flow-based methods in terms of reconstruction quality and sample diversity.', 'score': 1, 'issue_id': 4143, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '82a645cb87d64a3f', 'authors': ['Julius Erbach', 'Dominik Narnhofer', 'Andreas Dombos', 'Bernt Schiele', 'Jan Eric Lenssen', 'Konrad Schindler'], 'affiliations': ['ETH Zürich', 'Max Planck Institute for Informatics, Saarland Informatics Campus'], 'pdf_title_img': 'assets/pdf/title_img/2506.02680.jpg', 'data': {'categories': ['#cv', '#benchmark', '#diffusion', '#data', '#training', '#optimization'], 'emoji': '🔄', 'ru': {'title': 'FLAIR: Революция в решении обратных задач с помощью потоковых генеративных моделей', 'desc': 'FLAIR - это новая вариационная система, использующая генеративные модели на основе потоков для улучшения решений обратных задач. Она не требует дополнительного обучения и преодолевает ключевые препятствия, связанные с нелинейностью отображения и трудностями в восстановлении редких режимов данных. FLAIR вводит вариационную целевую функцию для сопоставления потоков, независимую от типа деградации, и комбинирует ее с детерминированными корректировками траектории. Система превосходит существующие методы на основе диффузии и потоков по качеству реконструкции и разнообразию выборки.'}, 'en': {'title': 'FLAIR: Enhancing Inverse Problems with Flow-Based Generative Models', 'desc': 'FLAIR is a new framework that improves solutions to inverse problems using flow-based generative models without requiring extensive training. It addresses challenges like non-linear mappings and intractable data likelihoods by introducing a variational objective that is flexible to different types of data degradation. The framework also includes techniques to recover rare data patterns and ensures consistency with observed data by separating optimization processes. Experimental results show that FLAIR achieves better image reconstruction quality and greater diversity in samples compared to existing methods.'}, 'zh': {'title': 'FLAIR：提升逆问题解决的新方法', 'desc': 'FLAIR是一种新颖的无训练变分框架，利用基于流的生成模型来增强逆问题的解决方案。该方法通过引入变分目标和确定性轨迹调整，克服了在低维潜在空间编码带来的非线性映射问题。FLAIR能够有效恢复稀有和非典型的数据模式，并确保与观测数据的一致性。实验结果表明，FLAIR在重建质量和样本多样性方面优于现有的扩散和流模型方法。'}}}, {'id': 'https://huggingface.co/papers/2506.02515', 'title': 'FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial\n  Reasoning', 'url': 'https://huggingface.co/papers/2506.02515', 'abstract': 'A new benchmark called FinChain evaluates multi-step symbolic reasoning in financial tasks with a focus on intermediate reasoning steps, introducing ChainEval as a metric for assessing both final answers and reasoning processes.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-step symbolic reasoning is critical for advancing downstream performance on financial tasks. Yet, benchmarks for systematically evaluating this capability are lacking. Existing datasets like FinQA and ConvFinQA supervise only final numerical answers, without assessing intermediate reasoning steps. To address this, we introduce FinChain, the first symbolic benchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning. Spanning 54 topics across 12 financial domains, Fin- Chain offers five parameterized templates per topic, each varying in reasoning complexity and domain expertise required. Each dataset instance includes an executable Python trace, enabling automatic generation of extensive training data and easy adaptation to other domains. We also introduce ChainEval, a new metric for automatic evaluation of both final answers and intermediate reasoning. Benchmarking 30 LLMs on our dataset, we find that even state-of-the-art models have considerable room for improvement in multi-step financial reasoning. All templates and evaluation metrics for FinChain are available at https: //github.com/mbzuai-nlp/finchain.', 'score': 1, 'issue_id': 4142, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '0b49cede5613cc2d', 'authors': ['Zhuohan Xie', 'Dhruv Sahnan', 'Debopriyo Banerjee', 'Georgi Georgiev', 'Rushil Thareja', 'Hachem Madmoun', 'Jinyan Su', 'Aaryamonvikram Singh', 'Yuxia Wang', 'Rui Xing', 'Fajri Koto', 'Haonan Li', 'Ivan Koychev', 'Tanmoy Chakraborty', 'Salem Lahlou', 'Veselin Stoyanov', 'Preslav Nakov'], 'affiliations': ['Cornell University, USA', 'FMI, Sofia University, Bulgaria', 'IIT Delhi, India', 'MBZUAI, UAE', 'Quantsquare, France'], 'pdf_title_img': 'assets/pdf/title_img/2506.02515.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#dataset', '#benchmark'], 'emoji': '💹', 'ru': {'title': 'FinChain: новый рубеж в оценке финансовых рассуждений ИИ', 'desc': 'FinChain - это новый бенчмарк для оценки многоэтапных символьных рассуждений в финансовых задачах, фокусирующийся на промежуточных шагах рассуждений. Он включает 54 темы из 12 финансовых областей, с пятью параметризованными шаблонами для каждой темы. Авторы также представили метрику ChainEval для автоматической оценки как конечных ответов, так и промежуточных рассуждений. Тестирование 30 различных языковых моделей на этом датасете показало, что даже современные модели имеют значительный потенциал для улучшения в области многоэтапных финансовых рассуждений.'}, 'en': {'title': 'FinChain: Advancing Financial Reasoning with Intermediate Steps', 'desc': 'The paper introduces FinChain, a new benchmark designed to evaluate multi-step symbolic reasoning specifically in financial tasks. Unlike existing datasets that only focus on final answers, FinChain emphasizes the importance of intermediate reasoning steps through its novel metric, ChainEval. This benchmark covers a wide range of financial topics and provides parameterized templates to assess varying levels of reasoning complexity. The findings reveal that even advanced language models struggle with multi-step reasoning in finance, highlighting the need for improved capabilities in this area.'}, 'zh': {'title': 'FinChain：金融推理的新基准', 'desc': 'FinChain是一个新的基准，旨在评估金融任务中的多步骤符号推理，特别关注中间推理步骤。它引入了ChainEval作为评估最终答案和推理过程的新指标。现有的数据集如FinQA和ConvFinQA仅监督最终的数值答案，而不评估中间推理步骤。FinChain覆盖12个金融领域的54个主题，为每个主题提供五个不同推理复杂度和领域专业知识要求的模板。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (4)', '#agi', '#alignment (4)', '#architecture (6)', '#audio (1)', '#benchmark (18)', '#cv (7)', '#data (8)', '#dataset (14)', '#diffusion (9)', '#ethics (3)', '#games (2)', '#graphs (1)', '#hallucinations (4)', '#healthcare', '#inference (4)', '#interpretability (7)', '#leakage', '#long_context (4)', '#low_resource (1)', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (11)', '#open_source (7)', '#optimization (21)', '#plp', '#rag', '#reasoning (11)', '#rl (6)', '#rlhf (8)', '#robotics', '#science', '#security (3)', '#small_models', '#story_generation (2)', '#survey (2)', '#synthetic (3)', '#training (22)', '#transfer_learning (1)', '#video (6)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-06-05 12:22',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-05 12:22')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-05 12:22')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    