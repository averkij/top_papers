
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 19 papers. July 1.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">1 июля</span> | <span id="title-articles-count">19 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-06-30.html">⬅️ <span id="prev-date">30.06</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-07-02.html">➡️ <span id="next-date">02.07</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-07.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '1 июля', 'en': 'July 1', 'zh': '7月1日'};
        let feedDateNext = {'ru': '02.07', 'en': '07/02', 'zh': '7月2日'};
        let feedDatePrev = {'ru': '30.06', 'en': '06/30', 'zh': '6月30日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.23044', 'title': 'Ovis-U1 Technical Report', 'url': 'https://huggingface.co/papers/2506.23044', 'abstract': 'Ovis-U1, a 3-billion-parameter model, combines multimodal understanding, text-to-image generation, and image editing, achieving state-of-the-art performance in various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we introduce Ovis-U1, a 3-billion-parameter unified model that integrates multimodal understanding, text-to-image generation, and image editing capabilities. Building on the foundation of the Ovis series, Ovis-U1 incorporates a diffusion-based visual decoder paired with a bidirectional token refiner, enabling image generation tasks comparable to leading models like GPT-4o. Unlike some previous models that use a frozen MLLM for generation tasks, Ovis-U1 utilizes a new unified training approach starting from a language model. Compared to training solely on understanding or generation tasks, unified training yields better performance, demonstrating the enhancement achieved by integrating these two tasks. Ovis-U1 achieves a score of 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent state-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In text-to-image generation, it excels with scores of 83.72 and 0.89 on the DPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves 4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the initial version of the Ovis unified model series, Ovis-U1 pushes the boundaries of multimodal understanding, generation, and editing.', 'score': 44, 'issue_id': 4573, 'pub_date': '2025-06-29', 'pub_date_card': {'ru': '29 июня', 'en': 'June 29', 'zh': '6月29日'}, 'hash': 'caa82e446dde84a7', 'authors': ['Guo-Hua Wang', 'Shanshan Zhao', 'Xinjie Zhang', 'Liangfu Cao', 'Pengxin Zhan', 'Lunhao Duan', 'Shiyin Lu', 'Minghao Fu', 'Xiaohao Chen', 'Jianshan Zhao', 'Yang Li', 'Qing-Guo Chen'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2506.23044.jpg', 'data': {'categories': ['#cv', '#diffusion', '#architecture', '#multimodal', '#benchmark'], 'emoji': '🦾', 'ru': {'title': 'Ovis-U1: Новый уровень мультимодального ИИ', 'desc': 'Ovis-U1 - это мультимодальная языковая модель с 3 миллиардами параметров, объединяющая понимание разных типов данных, генерацию изображений по тексту и редактирование изображений. Модель использует диффузионный визуальный декодер и двунаправленный уточнитель токенов для задач генерации изображений. Ovis-U1 применяет новый унифицированный подход к обучению, начиная с языковой модели, что позволяет достичь лучших результатов по сравнению с обучением только на задачах понимания или генерации. Модель демонстрирует высокие результаты в различных бенчмарках, превосходя современные аналоги в мультимодальном понимании, генерации и редактировании изображений.'}, 'en': {'title': 'Ovis-U1: Unifying Text and Image Mastery', 'desc': 'Ovis-U1 is a powerful machine learning model with 3 billion parameters that excels in understanding and generating images from text, as well as editing images. It uses a unique training method that combines language understanding and image generation, leading to improved performance over models that focus on just one of these tasks. The model features a diffusion-based visual decoder and a bidirectional token refiner, which enhance its capabilities in generating high-quality images. Ovis-U1 has achieved impressive scores on various benchmarks, outperforming other state-of-the-art models in multimodal tasks.'}, 'zh': {'title': 'Ovis-U1：多模态统一模型的突破', 'desc': 'Ovis-U1是一个拥有30亿参数的统一模型，结合了多模态理解、文本到图像生成和图像编辑的能力。它采用基于扩散的视觉解码器和双向令牌精炼器，使得图像生成任务的表现与领先模型如GPT-4o相当。与一些使用固定多语言模型进行生成任务的模型不同，Ovis-U1采用了一种新的统一训练方法，从语言模型开始训练。通过将理解和生成任务结合，Ovis-U1在多个基准测试中表现出色，推动了多模态理解、生成和编辑的边界。'}}}, {'id': 'https://huggingface.co/papers/2506.24119', 'title': 'SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via\n  Multi-Agent Multi-Turn Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.24119', 'abstract': 'Self-play in zero-sum games using SPIRAL enhances reasoning capabilities in language models through self-improvement and transfer learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reinforcement learning have shown that language models can develop sophisticated reasoning through training on tasks with verifiable rewards, but these approaches depend on human-curated problem-answer pairs and domain-specific reward engineering. We introduce SPIRAL, a self-play framework where models learn by playing multi-turn, zero-sum games against continuously improving versions of themselves, eliminating the need for human supervision. Through self-play, SPIRAL generates an infinite curriculum of progressively challenging problems as models must constantly adapt to stronger opponents. To enable this self-play training at scale, We implement a fully online, multi-turn, multi-agent reinforcement learning system for LLMs and propose role-conditioned advantage estimation (RAE) to stabilize multi-agent training. Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6% improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000 expert game trajectories. Analysis reveals that this transfer occurs through three cognitive patterns: systematic decomposition, expected value calculation, and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple Negotiation) further enhances performance as each game develops distinct reasoning strengths. Applying SPIRAL to a strong reasoning model (DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These results demonstrate that zero-sum games naturally develop transferable reasoning capabilities, highlighting a promising direction for autonomous reasoning development.', 'score': 25, 'issue_id': 4570, 'pub_date': '2025-06-30', 'pub_date_card': {'ru': '30 июня', 'en': 'June 30', 'zh': '6月30日'}, 'hash': 'fbb4b5b047892d14', 'authors': ['Bo Liu', 'Leon Guertler', 'Simon Yu', 'Zichen Liu', 'Penghui Qi', 'Daniel Balcells', 'Mickel Liu', 'Cheston Tan', 'Weiyan Shi', 'Min Lin', 'Wee Sun Lee', 'Natasha Jaques'], 'affiliations': ['Centre for Frontier AI Research (CFAR), A*STAR', 'National University of Singapore', 'Northeastern University', 'Plastic Labs', 'Sea AI Lab', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.24119.jpg', 'data': {'categories': ['#rl', '#reasoning', '#games', '#transfer_learning', '#rlhf', '#training'], 'emoji': '🧠', 'ru': {'title': 'Самосовершенствование ИИ через игры с нулевой суммой', 'desc': 'Статья представляет SPIRAL - фреймворк для самообучения языковых моделей через игру в игры с нулевой суммой против улучшающихся версий самих себя. Этот подход устраняет необходимость в человеческом кураторстве, создавая бесконечный курс прогрессивно усложняющихся задач. Обучение на одной только игре Кун-покер улучшило результаты модели Qwen3-4B-Base на 8.6% в математике и 8.4% в общих рассуждениях. Анализ показал, что перенос навыков происходит через три когнитивных паттерна: систематическую декомпозицию, расчет ожидаемой ценности и анализ по отдельным случаям.'}, 'en': {'title': 'Empowering AI Reasoning through Self-Play in Zero-Sum Games', 'desc': 'The paper presents SPIRAL, a self-play framework that enhances reasoning abilities in language models by allowing them to compete against improved versions of themselves in zero-sum games. This approach eliminates the need for human-generated problem-answer pairs and domain-specific rewards, enabling models to learn through an infinite curriculum of challenges. By implementing a multi-agent reinforcement learning system, SPIRAL stabilizes training and facilitates the development of reasoning skills that can be transferred across different tasks. The results show significant improvements in reasoning performance, demonstrating the effectiveness of self-play in fostering cognitive skills in AI models.'}, 'zh': {'title': '自我对弈：提升语言模型推理能力的新方法', 'desc': '本文介绍了一种名为SPIRAL的自我对弈框架，旨在通过零和游戏提升语言模型的推理能力。该方法通过模型与自身不断改进的版本进行多轮对弈，消除了对人工监督的需求。SPIRAL能够生成无限的逐步挑战问题，使模型必须适应更强的对手，从而实现自我提升。实验结果表明，使用SPIRAL进行训练的模型在数学和一般推理方面均有显著提升，展示了零和游戏在自主推理发展中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.23858', 'title': 'VMoBA: Mixture-of-Block Attention for Video Diffusion Models', 'url': 'https://huggingface.co/papers/2506.23858', 'abstract': 'VMoBA, a novel sparse attention mechanism for Video Diffusion Models (VDMs), accelerates training and inference by addressing the quadratic complexity of full attention mechanisms while maintaining or improving video generation quality.  \t\t\t\t\tAI-generated summary \t\t\t\t The quadratic complexity of full attention mechanisms poses a significant bottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration, high-resolution videos. While various sparse attention methods have been proposed, many are designed as training-free inference accelerators or do not optimally capture the unique spatio-temporal characteristics inherent in video data when trained natively. This paper introduces Video Mixture of Block Attention (VMoBA), a novel sparse attention mechanism specifically adapted for VDMs. Motivated by an in-depth analysis of attention patterns within pre-trained video transformers, which revealed strong spatio-temporal locality, varying query importance, and head-specific concentration levels, VMoBA enhances the original MoBA framework with three key modifications: (1) a layer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to diverse spatio-temporal attention patterns and improve efficiency; (2) global block selection to prioritize the most salient query-key block interactions across an entire attention head; and (3) threshold-based block selection to dynamically determine the number of attended blocks based on their cumulative similarity. Extensive experiments demonstrate that VMoBA significantly accelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and 1.48x latency speedup, while attaining comparable or even superior generation quality to full attention. Furthermore, VMoBA exhibits competitive performance in training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for high-res video generation.', 'score': 25, 'issue_id': 4570, 'pub_date': '2025-06-30', 'pub_date_card': {'ru': '30 июня', 'en': 'June 30', 'zh': '6月30日'}, 'hash': '684efafe36e7bbc8', 'authors': ['Jianzong Wu', 'Liang Hou', 'Haotian Yang', 'Xin Tao', 'Ye Tian', 'Pengfei Wan', 'Di Zhang', 'Yunhai Tong'], 'affiliations': ['Kling Team, Kuaishou Technology', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2506.23858.jpg', 'data': {'categories': ['#optimization', '#architecture', '#video', '#diffusion', '#training'], 'emoji': '🎬', 'ru': {'title': 'VMoBA: Эффективное внимание для генерации видео', 'desc': 'VMoBA - это новый механизм разреженного внимания для видео-диффузионных моделей (VDM). Он решает проблему квадратичной сложности полных механизмов внимания, ускоряя обучение и инференс. VMoBA использует схему разбиения на блоки 1D-2D-3D, глобальный выбор блоков и выбор на основе порога для адаптации к пространственно-временным паттернам внимания. Эксперименты показывают значительное ускорение при сохранении или улучшении качества генерации видео.'}, 'en': {'title': 'Accelerating Video Generation with Sparse Attention', 'desc': 'VMoBA is a new sparse attention mechanism designed to improve Video Diffusion Models (VDMs) by reducing the computational complexity associated with full attention mechanisms. It addresses the challenges of generating long-duration, high-resolution videos while maintaining quality. The method incorporates a layer-wise recurrent block partition scheme, global block selection, and threshold-based block selection to optimize attention patterns specific to video data. Experimental results show that VMoBA significantly speeds up training and inference times while achieving comparable or superior video generation quality.'}, 'zh': {'title': 'VMoBA：加速视频生成的新稀疏注意力机制', 'desc': 'VMoBA是一种新颖的稀疏注意力机制，专为视频扩散模型（VDMs）设计，旨在解决全注意力机制的平方复杂度问题，从而加速训练和推理。通过对预训练视频变换器的注意力模式进行深入分析，VMoBA能够有效捕捉视频数据的时空特性。该机制通过三项关键改进提升了原有的MoBA框架，包括动态适应时空注意力模式的分层递归块划分方案、优先选择最显著的查询-键块交互的全局块选择，以及基于阈值的块选择来动态确定关注块的数量。实验结果表明，VMoBA在长序列训练中显著加速VDMs的训练，同时在生成质量上与全注意力机制相当或更优。'}}}, {'id': 'https://huggingface.co/papers/2506.24123', 'title': 'Calligrapher: Freestyle Text Image Customization', 'url': 'https://huggingface.co/papers/2506.24123', 'abstract': "Calligrapher uses a diffusion-based framework with self-distillation and localized style injection to generate high-quality, stylistically consistent digital typography.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Calligrapher, a novel diffusion-based framework that innovatively integrates advanced text customization with artistic typography for digital calligraphy and design applications. Addressing the challenges of precise style control and data dependency in typographic customization, our framework incorporates three key technical contributions. First, we develop a self-distillation mechanism that leverages the pre-trained text-to-image generative model itself alongside the large language model to automatically construct a style-centric typography benchmark. Second, we introduce a localized style injection framework via a trainable style encoder, which comprises both Qformer and linear layers, to extract robust style features from reference images. An in-context generation mechanism is also employed to directly embed reference images into the denoising process, further enhancing the refined alignment of target styles. Extensive quantitative and qualitative evaluations across diverse fonts and design contexts confirm Calligrapher's accurate reproduction of intricate stylistic details and precise glyph positioning. By automating high-quality, visually consistent typography, Calligrapher surpasses traditional models, empowering creative practitioners in digital art, branding, and contextual typographic design.", 'score': 24, 'issue_id': 4570, 'pub_date': '2025-06-30', 'pub_date_card': {'ru': '30 июня', 'en': 'June 30', 'zh': '6月30日'}, 'hash': 'ee3cd26fec757450', 'authors': ['Yue Ma', 'Qingyan Bai', 'Hao Ouyang', 'Ka Leong Cheng', 'Qiuyu Wang', 'Hongyu Liu', 'Zichen Liu', 'Haofan Wang', 'Jingye Chen', 'Yujun Shen', 'Qifeng Chen'], 'affiliations': ['Ant Group, China', 'Hong Kong University of Science and Technology, China', 'InstantX, Independent Research Team'], 'pdf_title_img': 'assets/pdf/title_img/2506.24123.jpg', 'data': {'categories': ['#architecture', '#benchmark', '#diffusion', '#dataset'], 'emoji': '✒️', 'ru': {'title': 'Искусственный каллиграф: новый уровень цифровой типографики', 'desc': 'Статья представляет Calligrapher - новую систему для генерации цифровой типографики на основе диффузионных моделей. Ключевые особенности включают самодистилляцию для создания стилистического эталона и локализованное внедрение стиля через обучаемый энкодер. Система использует предобученную генеративную модель текст-изображение и большую языковую модель для автоматического конструирования типографических образцов. Calligrapher превосходит традиционные модели в точности воспроизведения стилистических деталей и позиционирования глифов.'}, 'en': {'title': 'Empowering Digital Typography with Style Consistency', 'desc': "Calligrapher is a new framework that uses diffusion processes to create high-quality digital typography with a focus on style consistency. It addresses challenges in customizing typography by employing a self-distillation method that utilizes a pre-trained text-to-image model and a large language model to create a style-focused benchmark. Additionally, it features a localized style injection system that extracts style features from reference images using a trainable style encoder. The framework's in-context generation mechanism enhances the alignment of styles, allowing for precise and intricate typography suitable for various design applications."}, 'zh': {'title': 'Calligrapher：自动化高质量数字排版的创新框架', 'desc': 'Calligrapher 是一个基于扩散的框架，结合了自蒸馏和局部风格注入技术，旨在生成高质量且风格一致的数字排版。该框架解决了排版定制中风格控制和数据依赖的挑战，提出了三项关键技术贡献。首先，开发了一种自蒸馏机制，利用预训练的文本到图像生成模型和大型语言模型，自动构建以风格为中心的排版基准。其次，通过可训练的风格编码器引入局部风格注入框架，提取参考图像中的强健风格特征，从而提升目标风格的对齐精度。'}}}, {'id': 'https://huggingface.co/papers/2506.22832', 'title': 'Listener-Rewarded Thinking in VLMs for Image Preferences', 'url': 'https://huggingface.co/papers/2506.22832', 'abstract': 'A listener-augmented Group Relative Policy Optimization framework improves reward models by re-evaluating reasoning processes, leading to enhanced accuracy and out-of-distribution performance in aligning vision-language models with human preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Training robust and generalizable reward models for human visual preferences is essential for aligning text-to-image and text-to-video generative models with human intent. However, current reward models often fail to generalize, and supervised fine-tuning leads to memorization, demanding complex annotation pipelines. While reinforcement learning (RL), specifically Group Relative Policy Optimization (GRPO), improves generalization, we uncover a key failure mode: a significant drop in reasoning accuracy occurs when a model\'s reasoning trace contradicts that of an independent, frozen vision-language model ("listener") evaluating the same output. To address this, we introduce a listener-augmented GRPO framework. Here, the listener re-evaluates the reasoner\'s chain-of-thought to provide a dense, calibrated confidence score, shaping the RL reward signal. This encourages the reasoner not only to answer correctly, but to produce explanations that are persuasive to an independent model. Our listener-shaped reward scheme achieves best accuracy on the ImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD) performance on a large-scale human preference dataset (1.2M votes, up to +6% over naive reasoner), and reduces reasoning contradictions compared to strong GRPO and SFT baselines. These results demonstrate that listener-based rewards provide a scalable, data-efficient path to aligning vision-language models with nuanced human preferences. We will release our reasoning model here: https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.', 'score': 18, 'issue_id': 4575, 'pub_date': '2025-06-28', 'pub_date_card': {'ru': '28 июня', 'en': 'June 28', 'zh': '6月28日'}, 'hash': '70be97df80ce7e08', 'authors': ['Alexander Gambashidze', 'Li Pengyi', 'Matvey Skripkin', 'Andrey Galichin', 'Anton Gusarov', 'Konstantin Sobolev', 'Andrey Kuznetsov', 'Ivan Oseledets'], 'affiliations': ['Artificial Intelligence Research Institute, Moscow, Russia', 'Skolkovo Institute of Science and Technology, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2506.22832.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#alignment', '#rl', '#reasoning', '#rlhf'], 'emoji': '👂', 'ru': {'title': 'Слушай и учись: новый метод обучения моделей вознаграждения', 'desc': "Статья представляет новый подход к улучшению моделей вознаграждения для выравнивания генеративных моделей изображений и видео с человеческими предпочтениями. Авторы предлагают метод обучения с подкреплением, называемый listener-augmented Group Relative Policy Optimization (GRPO), который использует независимую модель-'слушателя' для оценки объяснений основной модели. Этот подход позволяет значительно повысить точность и обобщающую способность модели вознаграждения на бенчмарке ImageReward и крупномасштабном наборе данных о человеческих предпочтениях. Результаты демонстрируют эффективность метода для создания моделей, лучше соответствующих нюансам человеческих предпочтений."}, 'en': {'title': 'Enhancing Model Reasoning with Listener-Augmented Rewards', 'desc': "This paper presents a new framework called listener-augmented Group Relative Policy Optimization (GRPO) to improve reward models for aligning vision-language models with human preferences. The framework addresses the issue of reasoning accuracy by incorporating a 'listener' model that re-evaluates the reasoning process of the main model, providing calibrated confidence scores. This approach not only enhances the accuracy of the model's outputs but also reduces contradictions in reasoning when compared to traditional methods. The results show significant improvements in both accuracy and out-of-distribution performance, demonstrating the effectiveness of listener-based rewards in training robust models."}, 'zh': {'title': '增强推理，提升人类偏好对齐的准确性', 'desc': '本文提出了一种增强型的群体相对策略优化框架，通过重新评估推理过程来改善奖励模型，从而提高视觉-语言模型与人类偏好的对齐精度和在分布外的表现。当前的奖励模型在泛化能力上存在不足，且监督微调容易导致记忆化。我们引入了一个“听众”模型，它独立于推理模型，能够对推理过程进行重新评估，并提供更为准确的置信度评分。通过这种方式，我们的奖励机制不仅鼓励推理模型给出正确答案，还促使其生成对独立模型具有说服力的解释。'}}}, {'id': 'https://huggingface.co/papers/2506.17930', 'title': 'Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective', 'url': 'https://huggingface.co/papers/2506.17930', 'abstract': 'A novel prompt design paradigm, PromptQuine, shows that pruning random demonstrations into "gibberish" can improve large language model performance across various tasks, surpassing state-of-the-art methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a novel prompt design paradigm that challenges conventional wisdom in large language model (LLM) prompting. While conventional wisdom prioritizes well-crafted instructions and demonstrations for in-context learning (ICL), we show that pruning random demonstrations into seemingly incoherent "gibberish" can remarkably improve performance across diverse tasks. Notably, the "gibberish" always matches or surpasses state-of-the-art automatic prompt optimization techniques, achieving substantial gains regardless of LLM alignment. Nevertheless, discovering an effective pruning strategy is non-trivial, as existing attribution methods and prompt compression algorithms fail to deliver robust results, let alone human intuition. In terms of this, we propose a self-discover prompt optimization framework, PromptQuine, an evolutionary search framework that automatically searches for the pruning strategy by itself using only low-data regimes. Much like the emergent complexity in nature--such as symbiosis and self-organization--arising in response to resource constraints, our framework evolves and refines unconventional yet highly effective prompts by leveraging only the tokens present within the context. We demonstrate its effectiveness across classification, multi-choice question answering, generation and math reasoning tasks across LLMs, while achieving decent runtime efficiency. We hope our findings can guide mechanistic studies on in-context learning, and provide a call to action, to pave the way for more open-ended search algorithms for more effective LLM prompting.', 'score': 16, 'issue_id': 4570, 'pub_date': '2025-06-22', 'pub_date_card': {'ru': '22 июня', 'en': 'June 22', 'zh': '6月22日'}, 'hash': '346a389b3fbfb2bd', 'authors': ['Jianyu Wang', 'Zhiqiang Hu', 'Lidong Bing'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab', 'MiroMind'], 'pdf_title_img': 'assets/pdf/title_img/2506.17930.jpg', 'data': {'categories': ['#optimization', '#alignment', '#multimodal', '#training'], 'emoji': '✂️', 'ru': {'title': 'Бессмыслица в промптах улучшает работу языковых моделей', 'desc': "В статье представлена новая парадигма проектирования промптов под названием PromptQuine. Исследование показывает, что обрезание случайных демонстраций до 'бессмыслицы' может улучшить производительность больших языковых моделей (LLM) в различных задачах. Этот метод превосходит современные методы автоматической оптимизации промптов. PromptQuine использует эволюционный поиск для автоматического обнаружения стратегии обрезки, используя только небольшие наборы данных."}, 'en': {'title': "Unlocking LLM Potential with 'Gibberish' Prompts!", 'desc': "This paper introduces PromptQuine, a new approach to designing prompts for large language models (LLMs). Instead of using well-structured instructions, it shows that transforming random examples into 'gibberish' can enhance model performance on various tasks. The authors propose an evolutionary search framework that autonomously finds effective pruning strategies, even with limited data. Their results indicate that this unconventional method can outperform traditional prompt optimization techniques, suggesting new directions for improving in-context learning."}, 'zh': {'title': '胡言乱语，提升模型表现的秘密', 'desc': '本文提出了一种新的提示设计范式，称为PromptQuine，挑战了传统的大型语言模型（LLM）提示方法。研究表明，将随机示例修剪成看似无意义的“胡言乱语”可以显著提高模型在多种任务上的表现，甚至超越了现有的最佳方法。尽管发现有效的修剪策略并不简单，但PromptQuine框架通过自我发现的方式，自动搜索修剪策略，利用低数据环境进行优化。我们的研究结果希望能为在上下文学习中的机制研究提供指导，并推动更开放的搜索算法的发展，以实现更有效的LLM提示。'}}}, {'id': 'https://huggingface.co/papers/2506.23542', 'title': 'Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric\n  Attention', 'url': 'https://huggingface.co/papers/2506.23542', 'abstract': 'A novel ToF depth denoising network uses motion-invariant graph fusion and adaptive filters to improve temporal stability and spatial sharpness, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Depth images captured by Time-of-Flight (ToF) sensors are prone to noise, requiring denoising for reliable downstream applications. Previous works either focus on single-frame processing, or perform multi-frame processing without considering depth variations at corresponding pixels across frames, leading to undesirable temporal inconsistency and spatial ambiguity. In this paper, we propose a novel ToF depth denoising network leveraging motion-invariant graph fusion to simultaneously enhance temporal stability and spatial sharpness. Specifically, despite depth shifts across frames, graph structures exhibit temporal self-similarity, enabling cross-frame geometric attention for graph fusion. Then, by incorporating an image smoothness prior on the fused graph and data fidelity term derived from ToF noise distribution, we formulate a maximum a posterior problem for ToF denoising. Finally, the solution is unrolled into iterative filters whose weights are adaptively learned from the graph-informed geometric attention, producing a high-performance yet interpretable network. Experimental results demonstrate that the proposed scheme achieves state-of-the-art performance in terms of accuracy and consistency on synthetic DVToF dataset and exhibits robust generalization on the real Kinectv2 dataset. Source code will be released at https://github.com/davidweidawang/GIGA-ToF{https://github.com/davidweidawang/GIGA-ToF}.', 'score': 12, 'issue_id': 4570, 'pub_date': '2025-06-30', 'pub_date_card': {'ru': '30 июня', 'en': 'June 30', 'zh': '6月30日'}, 'hash': '9c9286ea4d796818', 'authors': ['Weida Wang', 'Changyong He', 'Jin Zeng', 'Di Qiu'], 'affiliations': ['Google', 'School of Computer Science and Technology, Tongji University'], 'pdf_title_img': 'assets/pdf/title_img/2506.23542.jpg', 'data': {'categories': ['#cv', '#optimization', '#dataset', '#graphs', '#synthetic', '#open_source'], 'emoji': '🔍', 'ru': {'title': 'Революционное шумоподавление ToF: стабильность и четкость через графовое слияние', 'desc': 'Предложена новая нейронная сеть для шумоподавления в датчиках глубины Time-of-Flight (ToF), использующая инвариантное к движению слияние графов и адаптивные фильтры. Сеть одновременно улучшает временную стабильность и пространственную четкость изображений глубины. Метод основан на самоподобии графовых структур во времени, что позволяет применять межкадровое геометрическое внимание для слияния графов. Экспериментальные результаты показывают, что предложенный подход достигает наилучших показателей точности и согласованности на синтетических и реальных наборах данных.'}, 'en': {'title': 'Enhancing ToF Depth Images with Motion-Invariant Graph Fusion', 'desc': 'This paper presents a new method for denoising depth images captured by Time-of-Flight (ToF) sensors, which often suffer from noise. The proposed network utilizes motion-invariant graph fusion to improve both the temporal stability and spatial sharpness of the images. By leveraging the self-similarity of graph structures across frames, the method effectively addresses depth variations while maintaining consistency. The approach is formulated as a maximum a posterior problem, resulting in an interpretable network that outperforms existing methods on benchmark datasets.'}, 'zh': {'title': '提升ToF深度图像的去噪效果', 'desc': '本文提出了一种新颖的时间飞行（ToF）深度去噪网络，利用运动不变图融合和自适应滤波器来提高时间稳定性和空间清晰度。以往的研究主要集中在单帧处理或多帧处理，但未考虑不同帧中对应像素的深度变化，导致时间不一致和空间模糊。我们的方法通过图结构的时间自相似性，实现跨帧几何注意力的图融合，并结合图的平滑性先验和ToF噪声分布的数据信度项，构建最大后验去噪问题。实验结果表明，该方案在合成DVToF数据集上实现了最先进的性能，并在真实Kinectv2数据集上展现了良好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2506.23151', 'title': 'MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame\n  Optical Flow Estimation', 'url': 'https://huggingface.co/papers/2506.23151', 'abstract': 'MEMFOF is a memory-efficient multi-frame optical flow method that achieves state-of-the-art performance on high-resolution inputs with reduced GPU memory usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in optical flow estimation have prioritized accuracy at the cost of growing GPU memory consumption, particularly for high-resolution (FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical flow method that identifies a favorable trade-off between multi-frame estimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU memory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely positions our method to be trained at native 1080p without the need for cropping or downsampling. We systematically revisit design choices from RAFT-like architectures, integrating reduced correlation volumes and high-resolution training protocols alongside multi-frame estimation, to achieve state-of-the-art performance across multiple benchmarks while substantially reducing memory overhead. Our method outperforms more resource-intensive alternatives in both accuracy and runtime efficiency, validating its robustness for flow estimation at high resolutions. At the time of submission, our method ranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289, leads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the best Fl-all error on KITTI-2015 at 2.94%. The code is available at https://github.com/msu-video-group/memfof.', 'score': 12, 'issue_id': 4578, 'pub_date': '2025-06-29', 'pub_date_card': {'ru': '29 июня', 'en': 'June 29', 'zh': '6月29日'}, 'hash': 'ad8371e7e40b14b4', 'authors': ['Vladislav Bargatin', 'Egor Chistov', 'Alexander Yakovenko', 'Dmitriy Vatolin'], 'affiliations': ['Lomonosov Moscow State University, Moscow, Russia', 'MSU Institute for Artificial Intelligence, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2506.23151.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#architecture', '#cv', '#inference'], 'emoji': '🔍', 'ru': {'title': 'Эффективная оценка оптического потока с минимальным использованием памяти', 'desc': 'MEMFOF - это метод оптического потока, который эффективно использует память GPU для высокоразрешающих входных данных. Он достигает современного уровня производительности на нескольких эталонных тестах, включая Spring, Sintel и KITTI-2015. MEMFOF использует многокадровую оценку и пересмотренную архитектуру RAFT для оптимизации соотношения точности и потребления памяти. Метод требует всего 2,09 ГБ памяти GPU во время выполнения для входных данных 1080p, что позволяет обучать его на нативном разрешении без обрезки или понижения дискретизации.'}, 'en': {'title': 'MEMFOF: Efficient Optical Flow for High-Resolution Images', 'desc': 'MEMFOF is a novel optical flow estimation method designed to be memory-efficient while maintaining high accuracy for high-resolution images. It significantly reduces GPU memory usage during both training and inference, allowing for full 1080p processing without the need for downsampling. By optimizing design choices from existing architectures and incorporating multi-frame estimation, MEMFOF achieves state-of-the-art performance across various benchmarks. This method not only excels in accuracy but also enhances runtime efficiency, making it a robust solution for real-time applications in optical flow estimation.'}, 'zh': {'title': 'MEMFOF：高效光流估计的新选择', 'desc': 'MEMFOF是一种内存高效的多帧光流估计方法，能够在高分辨率输入下实现最先进的性能，同时减少GPU内存使用。该方法在1080p输入时仅需2.09 GB的GPU内存，训练时为28.5 GB，使其能够在原生1080p下进行训练，而无需裁剪或下采样。MEMFOF通过整合减少的相关体积和高分辨率训练协议，优化了RAFT类架构的设计选择，从而在多个基准测试中实现了卓越的性能。与其他资源密集型方法相比，MEMFOF在准确性和运行效率上均表现出色，验证了其在高分辨率光流估计中的稳健性。'}}}, {'id': 'https://huggingface.co/papers/2506.16500', 'title': 'SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity', 'url': 'https://huggingface.co/papers/2506.16500', 'abstract': 'SparseLoRA reduces computational cost and speeds up fine-tuning of LLMs by dynamically selecting a sparse subset of weights for loss and gradient computation.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-tuning LLMs is both computationally and memory-intensive. While parameter-efficient fine-tuning methods, such as QLoRA and DoRA, reduce the number of trainable parameters and lower memory usage, they do not decrease computational cost. In some cases, they may even slow down fine-tuning. In this paper, we introduce SparseLoRA, a method that accelerates LLM fine-tuning through contextual sparsity. We propose a lightweight, training-free SVD sparsity estimator that dynamically selects a sparse subset of weights for loss and gradient computation. Also, we systematically analyze and address sensitivity across layers, tokens, and training steps. Our experimental results show that SparseLoRA reduces computational cost by up to 2.2 times and a measured speedup of up to 1.6 times while maintaining accuracy across various downstream tasks, including commonsense and arithmetic reasoning, code generation, and instruction following.', 'score': 11, 'issue_id': 4572, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 июня', 'en': 'June 19', 'zh': '6月19日'}, 'hash': '8a6cd2aa2a56bf51', 'authors': ['Samir Khaki', 'Xiuyu Li', 'Junxian Guo', 'Ligeng Zhu', 'Chenfeng Xu', 'Konstantinos N. Plataniotis', 'Amir Yazdanbakhsh', 'Kurt Keutzer', 'Song Han', 'Zhijian Liu'], 'affiliations': ['Google DeepMind', 'MIT', 'UC Berkeley', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2506.16500.jpg', 'data': {'categories': ['#training', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'SparseLoRA: Ускорение тонкой настройки LLM без потери качества', 'desc': 'SparseLoRA - это новый метод ускорения тонкой настройки больших языковых моделей (LLM). Он использует контекстную разреженность для динамического выбора подмножества весов для вычисления потерь и градиентов. Метод основан на легковесном SVD-оценщике разреженности, не требующем обучения. Эксперименты показывают, что SparseLoRA снижает вычислительные затраты до 2,2 раз и ускоряет процесс до 1,6 раз, сохраняя точность на различных задачах.'}, 'en': {'title': 'SparseLoRA: Speeding Up LLM Fine-Tuning with Smart Sparsity', 'desc': 'SparseLoRA is a novel method designed to enhance the efficiency of fine-tuning large language models (LLMs) by utilizing a sparse subset of weights for loss and gradient calculations. Unlike previous methods that only reduce the number of trainable parameters, SparseLoRA significantly cuts down on computational costs and speeds up the fine-tuning process. It employs a lightweight, training-free singular value decomposition (SVD) estimator to dynamically select which weights to use, ensuring optimal performance across different layers and training steps. Experimental results demonstrate that SparseLoRA can reduce computational costs by up to 2.2 times and improve speed by up to 1.6 times, all while maintaining high accuracy on various tasks.'}, 'zh': {'title': 'SparseLoRA：加速大规模语言模型微调的稀疏方法', 'desc': 'SparseLoRA是一种新方法，通过动态选择稀疏权重子集来减少大规模语言模型（LLM）微调的计算成本和加速过程。与其他参数高效的微调方法相比，SparseLoRA不仅降低了内存使用，还显著提高了计算效率。该方法使用轻量级的训练无关奇异值分解（SVD）稀疏性估计器，能够在训练过程中实时选择需要计算的权重。实验结果表明，SparseLoRA在保持准确性的同时，计算成本降低了最多2.2倍，速度提升了最多1.6倍。'}}}, {'id': 'https://huggingface.co/papers/2506.17417', 'title': 'Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in\n  Inference-time Scaling?', 'url': 'https://huggingface.co/papers/2506.17417', 'abstract': 'Inference-time techniques like decoding-time scaling and self-refinement enhance reasoning in vision-language models, with generation-based methods providing greater improvement than verification-based methods, despite RL-trained models not showing self-correction benefits.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have demonstrated that inference-time computation techniques, such as decoding-time scaling and self-refinement, can significantly enhance reasoning capabilities without relying on external knowledge. A key driver of this success is the emergence of self-correction and self-verification behaviors, often elicited through reinforcement learning (RL). In this paper, we investigate whether these inference-time techniques extend effectively to vision-language models (VLMs), particularly those trained with RL. We find that while decoding strategies such as majority voting and best-of-N selection with self-verification all improve VLM reasoning performance, generation-reliant methods such as the former achieve significantly higher gains versus verification-reliant methods such as the latter. Additionally, the self-correction behavior often associated with RL-tuned models, such as aha moment, does not lead to measurable gains. We show via extensive experimentation within the inference-time scaling framework to identify a key root cause: RL-trained VLMs still lack robust self-verification capabilities across both visual and textual modalities.', 'score': 9, 'issue_id': 4570, 'pub_date': '2025-06-20', 'pub_date_card': {'ru': '20 июня', 'en': 'June 20', 'zh': '6月20日'}, 'hash': '57576e8287f4515e', 'authors': ['Mingyuan Wu', 'Meitang Li', 'Jingcheng Yang', 'Jize Jiang', 'Kaizhuo Yan', 'Zhaoheng Li', 'Minjia Zhang', 'Klara Nahrstedt'], 'affiliations': ['University of Illinois Urbana Champaign', 'University of Michigan Ann Arbor'], 'pdf_title_img': 'assets/pdf/title_img/2506.17417.jpg', 'data': {'categories': ['#rl', '#reasoning', '#inference', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Улучшение рассуждений в визуально-языковых моделях: победа генерации над верификацией', 'desc': 'Исследование показывает, что методы вывода, такие как масштабирование во время декодирования и самоуточнение, улучшают способности рассуждения в визуально-языковых моделях. Генеративные методы, например мажоритарное голосование, оказались эффективнее методов верификации. Интересно, что модели, обученные с помощью обучения с подкреплением, не продемонстрировали ожидаемых преимуществ в самокоррекции. Основной причиной этого является отсутствие у таких моделей надежных возможностей самопроверки в визуальной и текстовой модальностях.'}, 'en': {'title': 'Boosting VLM Reasoning with Inference-Time Techniques', 'desc': 'This paper explores how inference-time techniques can improve reasoning in vision-language models (VLMs). It highlights that methods based on generation, like decoding-time scaling, provide better performance than those based on verification. The study also reveals that reinforcement learning (RL) trained models do not exhibit the expected self-correction benefits. Ultimately, the findings suggest that RL-trained VLMs struggle with self-verification, which limits their reasoning capabilities.'}, 'zh': {'title': '推理时技术提升视觉-语言模型的推理能力', 'desc': '本文探讨了推理时技术如何提升视觉-语言模型（VLM）的推理能力。研究发现，解码时间缩放和自我修正等技术在没有外部知识的情况下显著改善了模型的表现。尽管强化学习（RL）训练的模型未能显示出自我修正的优势，但基于生成的方法在提升性能方面明显优于基于验证的方法。实验结果表明，RL训练的VLM在视觉和文本模态上仍缺乏强大的自我验证能力。'}}}, {'id': 'https://huggingface.co/papers/2506.22598', 'title': 'RExBench: Can coding agents autonomously implement AI research\n  extensions?', 'url': 'https://huggingface.co/papers/2506.22598', 'abstract': 'RExBench evaluates the ability of LLM agents to autonomously implement research extensions, failing to achieve significant success without human hints.  \t\t\t\t\tAI-generated summary \t\t\t\t Agents based on Large Language Models (LLMs) have shown promise for performing sophisticated software engineering tasks autonomously. In addition, there has been progress towards developing agents that can perform parts of the research pipeline in machine learning and the natural sciences. We argue that research extension and its implementation is a critical capability for such systems, and introduce RExBench to support the evaluation of this capability. RExBench is a benchmark consisting of 12 realistic research experiment implementation tasks that aim to investigate research hypotheses that have not previously been implemented. Each task is set up as an extension to an existing research paper and codebase, accompanied by domain expert-written instructions. RExBench is robust to data contamination, and supports an automatic evaluation infrastructure that executes agent outputs to determine whether the success criteria are met. We use this benchmark to evaluate nine LLM agents implemented using three different frameworks: aider, Claude Code, and OpenHands. We find that all agents evaluated fail to autonomously implement the majority of the extensions. Although the success rate improves with additional human-written hints, the best performance under this setting remains below 40%. This indicates that current agents are still short of being able to handle realistic research extension tasks without substantial human guidance.', 'score': 6, 'issue_id': 4582, 'pub_date': '2025-06-27', 'pub_date_card': {'ru': '27 июня', 'en': 'June 27', 'zh': '6月27日'}, 'hash': '236eb718627b6f97', 'authors': ['Nicholas Edwards', 'Yukyung Lee', 'Yujun', 'Mao', 'Yulu Qin', 'Sebastian Schuster', 'Najoung Kim'], 'affiliations': ['Boston University', 'University College London', 'University of Vienna'], 'pdf_title_img': 'assets/pdf/title_img/2506.22598.jpg', 'data': {'categories': ['#benchmark', '#science', '#agents'], 'emoji': '🧪', 'ru': {'title': 'LLM-агенты пока не готовы к самостоятельным научным исследованиям', 'desc': 'RExBench - это новый бенчмарк для оценки способности агентов на основе больших языковых моделей (LLM) автономно реализовывать расширения исследований. Он включает 12 реалистичных задач по реализации экспериментов, основанных на существующих научных работах и кодовых базах. Результаты показывают, что современные агенты не способны успешно выполнять большинство задач без значительной помощи человека. Даже с дополнительными подсказками лучший результат не превышает 40% успешных реализаций.'}, 'en': {'title': 'Evaluating LLM Agents: The Challenge of Autonomous Research Extensions', 'desc': 'RExBench is a benchmark designed to assess the capability of Large Language Model (LLM) agents in autonomously implementing research extensions. The benchmark includes 12 tasks that require agents to extend existing research papers and codebases, with expert-written instructions provided for guidance. Despite the potential of LLMs in software engineering, the study found that these agents struggled to successfully complete the tasks without significant human assistance. The results highlight the limitations of current LLM agents in handling complex research tasks independently, achieving less than 40% success even with hints.'}, 'zh': {'title': '评估LLM代理的研究扩展能力', 'desc': 'RExBench是一个评估大型语言模型（LLM）代理在自主实现研究扩展能力的基准工具。研究表明，现有的LLM代理在没有人类提示的情况下，无法成功完成大多数研究扩展任务。尽管在提供额外的人类提示后，成功率有所提高，但仍然低于40%。这表明当前的代理在处理现实研究扩展任务时，仍然需要大量的人类指导。'}}}, {'id': 'https://huggingface.co/papers/2506.23219', 'title': 'UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence\n  with Spatial Reasoning and Understanding', 'url': 'https://huggingface.co/papers/2506.23219', 'abstract': 'UrbanLLaVA, a multi-modal large language model, effectively processes urban datasets for various tasks, outperforming existing models in both single-modal and complex cross-modal scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Urban research involves a wide range of scenarios and tasks that require the understanding of multi-modal data. Current methods often focus on specific data types and lack a unified framework in urban field for processing them comprehensively. The recent success of multi-modal large language models (MLLMs) presents a promising opportunity to overcome this limitation. In this paper, we introduce UrbanLLaVA, a multi-modal large language model designed to process these four types of data simultaneously and achieve strong performance across diverse urban tasks compared with general MLLMs. In UrbanLLaVA, we first curate a diverse urban instruction dataset encompassing both single-modal and cross-modal urban data, spanning from location view to global view of urban environment. Additionally, we propose a multi-stage training framework that decouples spatial reasoning enhancement from domain knowledge learning, thereby improving the compatibility and downstream performance of UrbanLLaVA across diverse urban tasks. Finally, we also extend existing benchmark for urban research to assess the performance of MLLMs across a wide range of urban tasks. Experimental results from three cities demonstrate that UrbanLLaVA outperforms open-source and proprietary MLLMs in both single-modal tasks and complex cross-modal tasks and shows robust generalization abilities across cities. Source codes and data are openly accessible to the research community via https://github.com/tsinghua-fib-lab/UrbanLLaVA.', 'score': 5, 'issue_id': 4573, 'pub_date': '2025-06-29', 'pub_date_card': {'ru': '29 июня', 'en': 'June 29', 'zh': '6月29日'}, 'hash': '86191833a27a1741', 'authors': ['Jie Feng', 'Shengyuan Wang', 'Tianhui Liu', 'Yanxin Xi', 'Yong Li'], 'affiliations': ['Department of Computer Science and Technology, Tsinghua University, Beijing, China', 'Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China', 'School of Electronic and Information Engineering, Beijing Jiaotong University, China', 'University of Helsinki, Finland'], 'pdf_title_img': 'assets/pdf/title_img/2506.23219.jpg', 'data': {'categories': ['#dataset', '#science', '#training', '#multimodal', '#benchmark', '#open_source'], 'emoji': '🏙️', 'ru': {'title': 'UrbanLLaVA: Революция в анализе городских данных с помощью мультимодального ИИ', 'desc': 'UrbanLLaVA - это мультимодальная большая языковая модель, разработанная для обработки городских данных. Модель способна эффективно работать с четырьмя типами данных одновременно, превосходя существующие модели как в одномодальных, так и в сложных кросс-модальных сценариях. Авторы предложили многоэтапную структуру обучения, которая разделяет улучшение пространственного рассуждения и изучение предметных знаний. Экспериментальные результаты показывают, что UrbanLLaVA превосходит открытые и проприетарные мультимодальные языковые модели в различных городских задачах.'}, 'en': {'title': 'UrbanLLaVA: Revolutionizing Urban Data Processing with Multi-Modal AI', 'desc': 'UrbanLLaVA is a multi-modal large language model specifically designed to handle various urban datasets, excelling in both single-modal and complex cross-modal tasks. It addresses the limitations of existing models by providing a unified framework that processes multiple types of urban data simultaneously. The model is trained using a diverse urban instruction dataset and employs a multi-stage training approach to enhance spatial reasoning and domain knowledge. Experimental results indicate that UrbanLLaVA significantly outperforms other models in urban research, demonstrating strong generalization across different cities.'}, 'zh': {'title': 'UrbanLLaVA：城市数据处理的新突破', 'desc': 'UrbanLLaVA是一种多模态大型语言模型，能够有效处理城市数据集，适用于多种任务。与现有模型相比，它在单模态和复杂跨模态场景中表现更佳。该模型通过构建多样化的城市指令数据集，并采用多阶段训练框架，提升了空间推理和领域知识的学习能力。实验结果表明，UrbanLLaVA在多个城市的任务中均优于其他开源和专有的多模态大型语言模型。'}}}, {'id': 'https://huggingface.co/papers/2506.22992', 'title': 'MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning', 'url': 'https://huggingface.co/papers/2506.22992', 'abstract': 'MARBLE is a challenging multimodal reasoning benchmark that highlights the limitations of existing multimodal language models in step-by-step reasoning and plan crafting under spatial and visual constraints.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability to process information from multiple modalities and to reason through it step-by-step remains a critical challenge in advancing artificial intelligence. However, existing reasoning benchmarks focus on text-only reasoning, or employ multimodal questions that can be answered by directly retrieving information from a non-text modality. Thus, complex reasoning remains poorly understood in multimodal domains. Here, we present MARBLE, a challenging multimodal reasoning benchmark that is designed to scrutinize multimodal language models (MLLMs) in their ability to carefully reason step-by-step through complex multimodal problems and environments. MARBLE is composed of two highly challenging tasks, M-Portal and M-Cube, that require the crafting and understanding of multistep plans under spatial, visual, and physical constraints. We find that current MLLMs perform poorly on MARBLE -- all the 12 advanced models obtain near-random performance on M-Portal and 0% accuracy on M-Cube. Only in simplified subtasks some models outperform the random baseline, indicating that complex reasoning is still a challenge for existing MLLMs. Moreover, we show that perception remains a bottleneck, where MLLMs occasionally fail to extract information from the visual inputs. By shedding a light on the limitations of MLLMs, we hope that MARBLE will spur the development of the next generation of models with the ability to reason and plan across many, multimodal reasoning steps.', 'score': 5, 'issue_id': 4574, 'pub_date': '2025-06-28', 'pub_date_card': {'ru': '28 июня', 'en': 'June 28', 'zh': '6月28日'}, 'hash': '19689e84c5482c65', 'authors': ['Yulun Jiang', 'Yekun Chai', 'Maria Brbić', 'Michael Moor'], 'affiliations': ['EPFL', 'ETH Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2506.22992.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'MARBLE: Вызов мультимодальным языковым моделям в сложных рассуждениях', 'desc': 'MARBLE - это сложный мультимодальный бенчмарк для оценки рассуждений, который выявляет ограничения существующих мультимодальных языковых моделей в пошаговом рассуждении и создании планов с учетом пространственных и визуальных ограничений. Бенчмарк состоит из двух задач: M-Portal и M-Cube, требующих составления и понимания многоэтапных планов. Текущие мультимодальные языковые модели показывают низкую производительность на MARBLE, демонстрируя случайные результаты на M-Portal и 0% точности на M-Cube. MARBLE призван стимулировать разработку нового поколения моделей, способных рассуждать и планировать в мультимодальных задачах.'}, 'en': {'title': 'MARBLE: Unveiling the Limits of Multimodal Reasoning in AI', 'desc': 'MARBLE is a new benchmark designed to test multimodal language models (MLLMs) on their ability to perform complex reasoning tasks that involve both visual and spatial elements. It includes two main tasks, M-Portal and M-Cube, which require models to create and understand multistep plans while adhering to various constraints. Current MLLMs struggle significantly with these tasks, showing near-random performance on M-Portal and failing completely on M-Cube, highlighting their limitations in complex reasoning. By identifying these challenges, MARBLE aims to encourage the development of more advanced models capable of effective multimodal reasoning and planning.'}, 'zh': {'title': 'MARBLE：揭示多模态推理的挑战', 'desc': 'MARBLE是一个具有挑战性的多模态推理基准，旨在揭示现有多模态语言模型在逐步推理和计划制定方面的局限性。该基准包含两个高度挑战性的任务，M-Portal和M-Cube，要求在空间、视觉和物理约束下进行多步骤计划的制定和理解。研究发现，当前的多模态语言模型在MARBLE上的表现不佳，所有12个先进模型在M-Portal上的表现接近随机，而在M-Cube上的准确率为0%。通过揭示多模态语言模型的局限性，MARBLE希望推动下一代模型的发展，使其能够在多模态推理步骤中进行有效的推理和计划。'}}}, {'id': 'https://huggingface.co/papers/2506.23394', 'title': 'Teaching a Language Model to Speak the Language of Tools', 'url': 'https://huggingface.co/papers/2506.23394', 'abstract': 'A methodology is presented to adapt language models for robust tool use across languages, specifically improving function-calling accuracy in Bulgarian.  \t\t\t\t\tAI-generated summary \t\t\t\t External tool integration through function-calling is essential for practical language model applications, yet most multilingual models lack reliable tool-use capabilities in non-English languages. Even state-of-the-art multilingual models struggle with determining when to use tools and generating the structured outputs required for function calls, often exhibiting language confusion when prompted in lower-resource languages. This work presents a methodology for adapting existing language models to enable robust tool use in any target language, using Bulgarian as a case study. The approach involves continued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a novel bilingual dataset of 10,035 function-calling examples designed to support standardized protocols like MCP (Model Context Protocol). The research introduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to 28.75% improvement in function-calling accuracy over base models while preserving core language understanding, as verified on established Bulgarian benchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready response formatting with clean, parsable function calls, contrasting with the verbose and inconsistent outputs of base models. The models, evaluation framework, and dataset are released to enable replication for other languages. This work demonstrates a practical approach for extending tool-augmented capabilities beyond English-centric systems.', 'score': 4, 'issue_id': 4570, 'pub_date': '2025-06-29', 'pub_date_card': {'ru': '29 июня', 'en': 'June 29', 'zh': '6月29日'}, 'hash': '19b24559e749a260', 'authors': ['Simeon Emanuilov'], 'affiliations': ['Department of Software Technologies, Faculty of Mathematics and Informatics, Sofia University St. Kliment Ohridski'], 'pdf_title_img': 'assets/pdf/title_img/2506.23394.jpg', 'data': {'categories': ['#multilingual', '#dataset', '#low_resource', '#benchmark', '#open_source', '#training'], 'emoji': '🔧', 'ru': {'title': 'Многоязычное расширение возможностей ИИ: точные вызовы функций на любом языке', 'desc': 'Представлена методология адаптации языковых моделей для надежного использования инструментов на разных языках, в частности для повышения точности вызова функций на болгарском языке. Исследование включает дообучение серии моделей BgGPT на двуязычном наборе данных с примерами вызовов функций. Разработанная система TUCAN достигает значительного улучшения точности вызова функций по сравнению с базовыми моделями. Результаты демонстрируют практический подход к расширению возможностей использования инструментов за пределами англоязычных систем.'}, 'en': {'title': 'Empowering Multilingual Models for Effective Tool Use', 'desc': 'This paper presents a new method to enhance language models for better tool usage in various languages, focusing on Bulgarian. It addresses the challenges faced by multilingual models in accurately using functions and generating structured outputs, especially in lower-resource languages. The authors introduce TUCAN, a model that significantly improves function-calling accuracy by training on a specialized bilingual dataset. This approach not only boosts performance but also ensures that the outputs are clean and usable, paving the way for better multilingual applications.'}, 'zh': {'title': '提升多语言工具使用能力的创新方法', 'desc': '本文提出了一种方法，旨在提高语言模型在多语言环境中工具使用的准确性，特别是保加利亚语的功能调用准确性。大多数多语言模型在非英语语言中缺乏可靠的工具使用能力，尤其是在低资源语言中表现不佳。研究通过对BgGPT模型系列进行持续训练，使用一个包含10,035个功能调用示例的双语数据集，来增强模型的工具使用能力。最终，研究推出的TUCAN模型在功能调用准确性上比基础模型提高了28.75%，并且在响应格式上也表现出色。'}}}, {'id': 'https://huggingface.co/papers/2506.21448', 'title': 'ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language\n  Models for Audio Generation and Editing', 'url': 'https://huggingface.co/papers/2506.21448', 'abstract': 'ThinkSound, a novel framework, uses Chain-of-Thought reasoning with a multimodal large language model to generate high-quality audio from videos, achieving state-of-the-art results in various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t While end-to-end video-to-audio generation has greatly improved, producing high-fidelity audio that authentically captures the nuances of visual content remains challenging. Like professionals in the creative industries, such generation requires sophisticated reasoning about items such as visual dynamics, acoustic environments, and temporal relationships. We present ThinkSound, a novel framework that leverages Chain-of-Thought (CoT) reasoning to enable stepwise, interactive audio generation and editing for videos. Our approach decomposes the process into three complementary stages: foundational foley generation that creates semantically coherent soundscapes, interactive object-centric refinement through precise user interactions, and targeted editing guided by natural language instructions. At each stage, a multimodal large language model generates contextually aligned CoT reasoning that guides a unified audio foundation model. Furthermore, we introduce AudioCoT, a comprehensive dataset with structured reasoning annotations that establishes connections between visual content, textual descriptions, and sound synthesis. Experiments demonstrate that ThinkSound achieves state-of-the-art performance in video-to-audio generation across both audio metrics and CoT metrics and excels in out-of-distribution Movie Gen Audio benchmark. The demo page is available at https://ThinkSound-Project.github.io.', 'score': 4, 'issue_id': 4573, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 июня', 'en': 'June 26', 'zh': '6月26日'}, 'hash': 'fefdbdbfb0394a3c', 'authors': ['Huadai Liu', 'Jialei Wang', 'Kaicheng Luo', 'Wen Wang', 'Qian Chen', 'Zhou Zhao', 'Wei Xue'], 'affiliations': ['Hong Kong University of Science and Technology (HKUST)', 'Tongyi Lab, Alibaba Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.21448.jpg', 'data': {'categories': ['#dataset', '#games', '#multimodal', '#benchmark', '#reasoning', '#audio'], 'emoji': '🎬', 'ru': {'title': 'Думай как звукорежиссер: ИИ создает аудио для видео с помощью рассуждений', 'desc': 'ThinkSound - это новая система для генерации высококачественного аудио из видео с использованием рассуждений по цепочке мыслей (Chain-of-Thought) и мультимодальной большой языковой модели. Система работает в три этапа: создание базового звукового ландшафта, интерактивное уточнение звуков отдельных объектов и целенаправленное редактирование с помощью естественно-языковых инструкций. Авторы также представили датасет AudioCoT с аннотациями структурированных рассуждений, связывающих визуальный контент, текстовые описания и синтез звука. Эксперименты показали, что ThinkSound достигает наилучших результатов в генерации аудио из видео по различным метрикам.'}, 'en': {'title': 'ThinkSound: Revolutionizing Video-to-Audio Generation with Chain-of-Thought Reasoning', 'desc': 'ThinkSound is a new framework that enhances video-to-audio generation by using Chain-of-Thought reasoning with a multimodal large language model. It addresses the challenge of creating high-quality audio that accurately reflects the visual content by breaking the process into three stages: foundational foley generation, interactive object-centric refinement, and targeted editing. Each stage utilizes contextually aligned reasoning to guide the audio generation, ensuring that the soundscapes are semantically coherent and tailored to user inputs. The framework also introduces the AudioCoT dataset, which connects visual elements, text descriptions, and sound synthesis, leading to state-of-the-art performance in various benchmarks.'}, 'zh': {'title': 'ThinkSound：视频生成高保真音频的新方法', 'desc': 'ThinkSound是一个新颖的框架，利用链式思维推理与多模态大语言模型，从视频生成高质量音频，达到了各项基准测试的最先进结果。尽管端到端的视频到音频生成技术已有显著进步，但生成真实捕捉视觉内容细微差别的高保真音频仍然具有挑战性。该框架将生成过程分为三个互补阶段：基础音效生成、交互式对象中心细化和基于自然语言指令的目标编辑。通过引入AudioCoT数据集，ThinkSound在视频到音频生成的实验中表现出色，尤其在音频指标和链式思维指标上均取得了领先成绩。'}}}, {'id': 'https://huggingface.co/papers/2506.22694', 'title': 'VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs', 'url': 'https://huggingface.co/papers/2506.22694', 'abstract': 'A technique called VocabTrim improves drafter-based speculative decoding by reducing the vocabulary of the drafter language model, thus decreasing drafting latency in memory-bound environments.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce a simple training-free technique to improve the performance of drafter-based speculative decoding (SpD) methods that incorporates language modeling head (LM head) during drafting process. A drafter-based speculative decoding leverages one or more smaller language models, a.k.a. drafters or draft models, to sample a draft sequence or tree consisting of multiple tokens, followed by verification by a base LLM, a target model, accepting a subset as its valid generation. As it is usually considered that the speculative decoding requires one-to-one mapping between vocabularies of the target model and the draft model, it has been natural to share the vocabulary between them, or even share the LM head as in EAGLE or Medusa. We first identify that this draft token sampling scheme inherently contains an unnecessary inference overhead in drafting, especially for some target LLMs with very large vocabularies. Then, we propose a simple technique, VocabTrim, to mitigate the drafting overhead to improve the generation speed in memory-bound environment. VocabTrim reconstructs the drafter LM head to contain only a limited set of tokens, selected by the most frequently sampled from the vocabulary of the target model. While limiting the vocabulary in drafting slightly degrades the acceptance rate, it significantly reduces the drafting latency in memory-bound process which is often the case on edge devices, resulting in higher memory-bound speed up (MBSU). We show that our method can boost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically by 16% for Llama-3.2-3B-Instruct.', 'score': 3, 'issue_id': 4573, 'pub_date': '2025-06-28', 'pub_date_card': {'ru': '28 июня', 'en': 'June 28', 'zh': '6月28日'}, 'hash': 'a9c9ccf73356a002', 'authors': ['Raghavv Goel', 'Sudhanshu Agrawal', 'Mukul Gagrani', 'Junyoung Park', 'Yifan Zao', 'He Zhang', 'Tian Liu', 'Yiping Yang', 'Xin Yuan', 'Jiuyan Lu', 'Chris Lott', 'Mingu Lee'], 'affiliations': ['Qualcomm AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2506.22694.jpg', 'data': {'categories': ['#inference', '#training', '#optimization', '#small_models'], 'emoji': '✂️', 'ru': {'title': 'Ускорение генерации текста путем обрезки словаря', 'desc': 'Эта статья представляет технику VocabTrim, которая улучшает спекулятивное декодирование на основе драфтера путем уменьшения словаря языковой модели-драфтера. VocabTrim реконструирует head-часть языковой модели-драфтера, оставляя только ограниченный набор наиболее часто используемых токенов из словаря целевой модели. Хотя это немного снижает коэффициент принятия, значительно уменьшается задержка при создании черновика в условиях ограниченной памяти. Метод показал увеличение скорости генерации на 16% для модели Llama-3.2-3B-Instruct в условиях ограниченной памяти.'}, 'en': {'title': 'Speed Up Drafting with VocabTrim!', 'desc': 'This paper presents VocabTrim, a technique designed to enhance drafter-based speculative decoding by optimizing the vocabulary used in the drafting process. By limiting the vocabulary to only the most frequently sampled tokens from the target model, VocabTrim reduces the inference overhead associated with drafting, particularly in memory-constrained environments. Although this approach may slightly lower the acceptance rate of generated sequences, it significantly accelerates drafting speed, especially on edge devices. The results demonstrate a notable 16% improvement in memory-bound speed-up for Llama-3 models on Spec-Bench, showcasing the effectiveness of this method.'}, 'zh': {'title': 'VocabTrim：提升推测解码速度的关键技术', 'desc': '本文提出了一种名为VocabTrim的技术，旨在通过减少草拟语言模型的词汇量来改善基于草拟的推测解码性能。这种方法可以降低在内存受限环境中的草拟延迟，特别是对于具有大词汇量的目标语言模型。VocabTrim通过重构草拟语言模型的头部，仅保留目标模型中最常被采样的有限词汇，从而提高生成速度。尽管限制词汇会略微降低接受率，但在边缘设备上显著提高了内存受限速度。'}}}, {'id': 'https://huggingface.co/papers/2506.23135', 'title': 'RoboScape: Physics-informed Embodied World Model', 'url': 'https://huggingface.co/papers/2506.23135', 'abstract': 'RoboScape is a unified physics-informed world model that enhances visual fidelity and physical plausibility in robotic video generation by integrating temporal depth prediction and keypoint dynamics learning.  \t\t\t\t\tAI-generated summary \t\t\t\t World models have become indispensable tools for embodied intelligence, serving as powerful simulators capable of generating realistic robotic videos while addressing critical data scarcity challenges. However, current embodied world models exhibit limited physical awareness, particularly in modeling 3D geometry and motion dynamics, resulting in unrealistic video generation for contact-rich robotic scenarios. In this paper, we present RoboScape, a unified physics-informed world model that jointly learns RGB video generation and physics knowledge within an integrated framework. We introduce two key physics-informed joint training tasks: temporal depth prediction that enhances 3D geometric consistency in video rendering, and keypoint dynamics learning that implicitly encodes physical properties (e.g., object shape and material characteristics) while improving complex motion modeling. Extensive experiments demonstrate that RoboScape generates videos with superior visual fidelity and physical plausibility across diverse robotic scenarios. We further validate its practical utility through downstream applications including robotic policy training with generated data and policy evaluation. Our work provides new insights for building efficient physics-informed world models to advance embodied intelligence research. The code is available at: https://github.com/tsinghua-fib-lab/RoboScape.', 'score': 2, 'issue_id': 4574, 'pub_date': '2025-06-29', 'pub_date_card': {'ru': '29 июня', 'en': 'June 29', 'zh': '6月29日'}, 'hash': '421522bfdd825b96', 'authors': ['Yu Shang', 'Xin Zhang', 'Yinzhou Tang', 'Lei Jin', 'Chen Gao', 'Wei Wu', 'Yong Li'], 'affiliations': ['Manifold AI', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.23135.jpg', 'data': {'categories': ['#optimization', '#science', '#open_source', '#robotics', '#video', '#games', '#3d'], 'emoji': '🤖', 'ru': {'title': 'RoboScape: физически достоверная генерация видео для продвижения воплощенного ИИ', 'desc': 'RoboScape - это унифицированная физически-информированная мировая модель для генерации видео роботов. Она объединяет предсказание временной глубины и обучение динамике ключевых точек для улучшения визуальной достоверности и физической правдоподобности. Модель совместно обучается генерации RGB-видео и физическим знаниям в интегрированной структуре. Эксперименты показывают превосходное качество генерируемых видео и практическую пользу для обучения и оценки политик роботов.'}, 'en': {'title': 'RoboScape: Realistic Robotic Video Generation with Physics Awareness', 'desc': 'RoboScape is a new model that improves how robots generate videos by making them look more realistic and physically accurate. It combines two important tasks: predicting depth over time to ensure 3D shapes are consistent, and learning how key points move to capture the physical properties of objects. This model helps robots create videos that are not only visually appealing but also reflect real-world physics, especially in complex interactions. The research shows that RoboScape can be used effectively for training robotic policies and evaluating their performance using the generated videos.'}, 'zh': {'title': 'RoboScape：提升机器人视频生成的物理真实感', 'desc': 'RoboScape 是一个统一的物理信息世界模型，通过整合时间深度预测和关键点动态学习，提升了机器人视频生成的视觉真实感和物理合理性。当前的世界模型在建模三维几何和运动动态方面存在局限，导致生成的视频在接触丰富的机器人场景中不够真实。本文提出的 RoboScape 通过联合学习 RGB 视频生成和物理知识，采用了两个关键的物理信息联合训练任务，增强了视频渲染中的三维几何一致性。实验结果表明，RoboScape 在多种机器人场景中生成的视频具有更高的视觉真实感和物理合理性，并在下游应用中验证了其实用性。'}}}, {'id': 'https://huggingface.co/papers/2506.22753', 'title': 'Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography', 'url': 'https://huggingface.co/papers/2506.22753', 'abstract': 'The proposed Degradation-Modeled Multipath Diffusion framework improves metalens image quality by using natural image priors and specific modules to balance detail, fidelity, and perceptual quality while addressing optical degradation.  \t\t\t\t\tAI-generated summary \t\t\t\t Metalenses offer significant potential for ultra-compact computational imaging but face challenges from complex optical degradation and computational restoration difficulties. Existing methods typically rely on precise optical calibration or massive paired datasets, which are non-trivial for real-world imaging systems. Furthermore, a lack of control over the inference process often results in undesirable hallucinated artifacts. We introduce Degradation-Modeled Multipath Diffusion for tunable metalens photography, leveraging powerful natural image priors from pretrained models instead of large datasets. Our framework uses positive, neutral, and negative-prompt paths to balance high-frequency detail generation, structural fidelity, and suppression of metalens-specific degradation, alongside pseudo data augmentation. A tunable decoder enables controlled trade-offs between fidelity and perceptual quality. Additionally, a spatially varying degradation-aware attention (SVDA) module adaptively models complex optical and sensor-induced degradation. Finally, we design and build a millimeter-scale MetaCamera for real-world validation. Extensive results show that our approach outperforms state-of-the-art methods, achieving high-fidelity and sharp image reconstruction. More materials: https://dmdiff.github.io/.', 'score': 2, 'issue_id': 4578, 'pub_date': '2025-06-28', 'pub_date_card': {'ru': '28 июня', 'en': 'June 28', 'zh': '6月28日'}, 'hash': 'a52ba639d6bd763f', 'authors': ['Jianing Zhang', 'Jiayi Zhu', 'Feiyu Ji', 'Xiaokang Yang', 'Xiaoyun Yuan'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.22753.jpg', 'data': {'categories': ['#hallucinations', '#cv', '#data', '#inference', '#diffusion'], 'emoji': '🔍', 'ru': {'title': 'Улучшение качества изображений с металинз без масштабных датасетов', 'desc': 'Предложена новая модель Degradation-Modeled Multipath Diffusion для улучшения качества изображений, получаемых с помощью металинз. Модель использует априорную информацию о естественных изображениях и специальные модули для балансировки детализации, точности и визуального качества. Особое внимание уделяется компенсации оптических искажений, характерных для металинз. Система не требует большого набора парных данных или точной оптической калибровки, что упрощает ее применение в реальных условиях.'}, 'en': {'title': 'Enhancing Metalens Imaging with Adaptive Degradation Modeling', 'desc': 'The Degradation-Modeled Multipath Diffusion framework enhances the image quality of metalenses by integrating natural image priors and specialized modules. It effectively addresses the challenges of optical degradation while balancing detail, fidelity, and perceptual quality. Unlike traditional methods that require extensive calibration or large datasets, this approach utilizes pretrained models and a tunable decoder for better control over the inference process. The introduction of a spatially varying degradation-aware attention module allows for adaptive modeling of complex degradations, leading to superior image reconstruction results.'}, 'zh': {'title': '降解建模提升金属透镜图像质量', 'desc': '本文提出了一种降解建模的多路径扩散框架，旨在通过利用自然图像先验来改善金属透镜的图像质量。该框架通过特定模块平衡细节、保真度和感知质量，同时解决光学降解问题。与传统方法不同，我们的方法不依赖于精确的光学校准或庞大的配对数据集，而是使用预训练模型的强大自然图像先验。最终，我们的实验结果表明，该方法在高保真和清晰图像重建方面优于现有的最先进技术。'}}}, {'id': 'https://huggingface.co/papers/2506.17080', 'title': 'Tower+: Bridging Generality and Translation Specialization in\n  Multilingual LLMs', 'url': 'https://huggingface.co/papers/2506.17080', 'abstract': 'Tower+, a suite of fine-tuned language models, achieves strong performance in both translation and multilingual general-purpose text tasks through a novel training recipe that includes continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-tuning pretrained LLMs has been shown to be an effective strategy for reaching state-of-the-art performance on specific tasks like machine translation. However, this process of adaptation often implies sacrificing general-purpose capabilities, such as conversational reasoning and instruction-following, hampering the utility of the system in real-world applications that require a mixture of skills. In this paper, we introduce Tower+, a suite of models designed to deliver strong performance across both translation and multilingual general-purpose text capabilities. We achieve a Pareto frontier between translation specialization and multilingual general-purpose capabilities by introducing a novel training recipe that builds on Tower (Alves et al., 2024), comprising continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning with verifiable rewards. At each stage of training, we carefully generate and curate data to strengthen performance on translation as well as general-purpose tasks involving code generation, mathematics problem solving, and general instruction-following. We develop models at multiple scales: 2B, 9B, and 72B. Our smaller models often outperform larger general-purpose open-weight and proprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers best-in-class translation performance for high-resource languages and top results in multilingual Arena Hard evaluations and in IF-MT, a benchmark we introduce for evaluating both translation and instruction-following. Our findings highlight that it is possible to rival frontier models in general capabilities, while optimizing for specific business domains, such as translation and localization.', 'score': 2, 'issue_id': 4578, 'pub_date': '2025-06-20', 'pub_date_card': {'ru': '20 июня', 'en': 'June 20', 'zh': '6月20日'}, 'hash': 'f35759eeab0ef755', 'authors': ['Ricardo Rei', 'Nuno M. Guerreiro', 'José Pombal', 'João Alves', 'Pedro Teixeirinha', 'Amin Farajian', 'André F. T. Martins'], 'affiliations': ['Instituto Superior Técnico & Universidade de Lisboa (Lisbon ELLIS Unit)', 'Instituto de Telecomunicações', 'MICS, CentraleSupélec, Université Paris-Saclay', 'Unbabel'], 'pdf_title_img': 'assets/pdf/title_img/2506.17080.jpg', 'data': {'categories': ['#multilingual', '#benchmark', '#optimization', '#dataset', '#training', '#machine_translation'], 'emoji': '🗼', 'ru': {'title': 'Tower+: Языковые модели, объединяющие перевод и многоязычные задачи', 'desc': 'Tower+ представляет собой набор языковых моделей, достигающих высокой производительности как в задачах перевода, так и в многоязычных задачах общего назначения. Модели используют новую методику обучения, включающую дополнительное предварительное обучение, контролируемую настройку, оптимизацию предпочтений и обучение с подкреплением. Tower+ демонстрирует, что возможно достичь баланса между специализацией на переводе и многоязычными возможностями общего назначения. Модели Tower+ часто превосходят более крупные языковые модели общего назначения в различных задачах.'}, 'en': {'title': 'Tower+: Bridging Translation and General-Purpose Language Tasks', 'desc': 'Tower+ is a suite of fine-tuned language models that excels in both translation and multilingual text tasks. The models are trained using a unique recipe that includes continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning, allowing them to maintain strong general-purpose capabilities while specializing in translation. By carefully curating data at each training stage, Tower+ achieves a balance between translation performance and general tasks like code generation and problem-solving. The results show that even smaller models can outperform larger existing models, demonstrating the effectiveness of this training approach.'}, 'zh': {'title': 'Tower+: 翻译与多语言能力的完美平衡', 'desc': 'Tower+是一套经过精细调优的语言模型，能够在翻译和多语言通用文本任务中表现出色。通过一种新颖的训练方法，包括持续预训练、监督微调、偏好优化和强化学习，Tower+实现了翻译专业化与多语言通用能力之间的平衡。我们在训练的每个阶段精心生成和整理数据，以增强翻译和通用任务的表现。我们的模型在多个规模上开发，较小的模型在特定任务上常常超越更大的通用模型。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (1)', '#agi', '#alignment (2)', '#architecture (4)', '#audio (1)', '#benchmark (10)', '#cv (4)', '#data (1)', '#dataset (7)', '#diffusion (4)', '#ethics', '#games (3)', '#graphs (1)', '#hallucinations (1)', '#healthcare', '#inference (4)', '#interpretability', '#leakage', '#long_context', '#low_resource (1)', '#machine_translation (1)', '#math', '#multilingual (2)', '#multimodal (6)', '#open_source (4)', '#optimization (8)', '#plp', '#rag', '#reasoning (5)', '#rl (3)', '#rlhf (2)', '#robotics (1)', '#science (3)', '#security', '#small_models (1)', '#story_generation', '#survey', '#synthetic (1)', '#training (8)', '#transfer_learning (1)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-07-01 19:09',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-07-01 19:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-07-01 19:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    