
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 14 papers. December 10.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">10 декабря</span> | <span id="title-articles-count">14 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2024-12-09.html">⬅️ <span id="prev-date">09.12</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-12-11.html">➡️ <span id="next-date">11.12</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-12.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '10 декабря', 'en': 'December 10', 'zh': '12月10日'};
        let feedDateNext = {'ru': '11.12', 'en': '12/11', 'zh': '12月11日'};
        let feedDatePrev = {'ru': '09.12', 'en': '12/09', 'zh': '12月9日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2412.06531', 'title': 'Unraveling the Complexity of Memory in RL Agents: an Approach for Classification and Evaluation', 'url': 'https://huggingface.co/papers/2412.06531', 'abstract': "The incorporation of memory into agents is essential for numerous tasks within the domain of Reinforcement Learning (RL). In particular, memory is paramount for tasks that require the utilization of past information, adaptation to novel environments, and improved sample efficiency. However, the term ``memory'' encompasses a wide range of concepts, which, coupled with the lack of a unified methodology for validating an agent's memory, leads to erroneous judgments about agents' memory capabilities and prevents objective comparison with other memory-enhanced agents. This paper aims to streamline the concept of memory in RL by providing practical precise definitions of agent memory types, such as long-term versus short-term memory and declarative versus procedural memory, inspired by cognitive science. Using these definitions, we categorize different classes of agent memory, propose a robust experimental methodology for evaluating the memory capabilities of RL agents, and standardize evaluations. Furthermore, we empirically demonstrate the importance of adhering to the proposed methodology when evaluating different types of agent memory by conducting experiments with different RL agents and what its violation leads to.", 'score': 53, 'issue_id': 1045, 'pub_date': '2024-12-09', 'pub_date_card': {'ru': '9 декабря', 'en': 'December 9', 'zh': '12月9日'}, 'hash': '7ddb66a515f8803e', 'authors': ['Egor Cherepanov', 'Nikita Kachaev', 'Artem Zholus', 'Alexey K. Kovalev', 'Aleksandr I. Panov'], 'affiliations': ['AIRI, Moscow, Russia', 'Chandar Research Lab', 'MIPT, Dolgoprudny, Russia', 'Mila Quebec AI Institute', 'Polytechnique Montréal'], 'pdf_title_img': 'assets/pdf/title_img/2412.06531.jpg', 'data': {'categories': ['#benchmark', '#rl', '#reasoning', '#agents', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Унификация концепции памяти в обучении с подкреплением', 'desc': 'Эта статья посвящена важности памяти в обучении с подкреплением (RL). Авторы предлагают четкие определения различных типов памяти агентов, вдохновленные когнитивной наукой. Они разрабатывают методологию для оценки возможностей памяти RL-агентов и стандартизации оценок. Экспериментально демонстрируется важность следования предложенной методологии при оценке различных типов памяти агентов.'}, 'en': {'title': 'Streamlining Memory Evaluation in Reinforcement Learning', 'desc': 'This paper focuses on the role of memory in Reinforcement Learning (RL) agents, highlighting its importance for tasks that need past information and adaptability. It clarifies the various types of memory, such as long-term and short-term, as well as declarative and procedural memory, drawing from cognitive science. The authors propose a standardized methodology for evaluating the memory capabilities of RL agents, which is currently lacking in the field. Through experiments, they show that following this methodology is crucial for accurate assessments of memory in RL agents.'}, 'zh': {'title': '强化学习中的记忆：定义与评估的重要性', 'desc': '在强化学习（RL）中，将记忆融入智能体是许多任务的关键。记忆对于利用过去信息、适应新环境和提高样本效率至关重要。本文旨在通过提供智能体记忆类型的明确定义，简化RL中记忆的概念，并提出一种评估智能体记忆能力的实验方法。我们通过实验验证了遵循该方法的重要性，并展示了不遵循时可能导致的错误判断。'}}}, {'id': 'https://huggingface.co/papers/2412.06559', 'title': 'ProcessBench: Identifying Process Errors in Mathematical Reasoning', 'url': 'https://huggingface.co/papers/2412.06559', 'abstract': 'As language models regularly make mistakes when solving math problems, automated identification of errors in the reasoning process becomes increasingly significant for their scalable oversight. In this paper, we introduce ProcessBench for measuring the ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems. Each test case contains a step-by-step solution with error location annotated by human experts. Models are required to identify the earliest step that contains an error, or conclude that all steps are correct. We conduct extensive evaluation on ProcessBench, involving two types of models: process reward models (PRMs) and critic models, where for the latter we prompt general language models to critique each solution step by step. We draw two main observations: (1) Existing PRMs typically fail to generalize to more challenging math problems beyond GSM8K and MATH. They underperform both critic models (i.e., prompted general language models) and our own trained PRM that is straightforwardly fine-tuned on the PRM800K dataset. (2) The best open-source model, QwQ-32B-Preview, has demonstrated the critique capability competitive with the proprietary model GPT-4o, despite that it still lags behind the reasoning-specialized o1-mini. We hope ProcessBench can foster future research in reasoning process assessment, paving the way toward scalable oversight of language models.', 'score': 36, 'issue_id': 1039, 'pub_date': '2024-12-09', 'pub_date_card': {'ru': '9 декабря', 'en': 'December 9', 'zh': '12月9日'}, 'hash': '02f9abef0bc10297', 'authors': ['Chujie Zheng', 'Zhenru Zhang', 'Beichen Zhang', 'Runji Lin', 'Keming Lu', 'Bowen Yu', 'Dayiheng Liu', 'Jingren Zhou', 'Junyang Lin'], 'affiliations': ['Qwen Team, Alibaba Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2412.06559.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#math', '#benchmark', '#open_source', '#training'], 'emoji': '🧮', 'ru': {'title': 'ProcessBench: новый бенчмарк для оценки выявления ошибок в математических рассуждениях', 'desc': 'Статья представляет ProcessBench - набор данных для оценки способности моделей машинного обучения идентифицировать ошибки в математических рассуждениях. ProcessBench содержит 3400 тестовых примеров, в основном олимпиадного уровня, с пошаговыми решениями и аннотациями ошибок от экспертов. Авторы провели обширное тестирование различных моделей, включая Process Reward Models (PRM) и критические модели на основе больших языковых моделей. Результаты показывают, что существующие PRM плохо обобщаются на сложные задачи, а лучшая открытая модель QwQ-32B-Preview демонстрирует конкурентоспособность с проприетарной GPT-4.'}, 'en': {'title': 'Enhancing Error Detection in Math Reasoning with ProcessBench', 'desc': 'This paper presents ProcessBench, a benchmark designed to evaluate how well language models can identify errors in mathematical reasoning. It includes 3,400 test cases that focus on advanced math problems, with each case providing a detailed solution and expert-annotated error locations. The study compares the performance of process reward models (PRMs) and critic models, revealing that existing PRMs struggle with complex problems while critic models, particularly those based on general language models, perform better. The findings suggest that ProcessBench can enhance research on assessing reasoning processes in language models, contributing to their effective oversight.'}, 'zh': {'title': '提升语言模型数学推理的错误识别能力', 'desc': '本文介绍了一个名为ProcessBench的工具，用于评估语言模型在数学推理中识别错误步骤的能力。该工具包含3400个测试案例，主要集中在竞赛和奥林匹克级别的数学问题上。每个案例都提供了逐步解决方案，并由人类专家标注了错误位置。研究发现，现有的过程奖励模型在更具挑战性的数学问题上表现不佳，而经过微调的PRM在识别错误方面优于一般语言模型的批评能力。'}}}, {'id': 'https://huggingface.co/papers/2412.06769', 'title': 'Training Large Language Models to Reason in a Continuous Latent Space', 'url': 'https://huggingface.co/papers/2412.06769', 'abstract': 'Large language models (LLMs) are restricted to reason in the "language space", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may not always be optimal for reasoning. For example, most word tokens are primarily for textual coherence and not essential for reasoning, while some critical tokens require complex planning and pose huge challenges to LLMs. To explore the potential of LLM reasoning in an unrestricted latent space instead of using natural language, we introduce a new paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden state of the LLM as a representation of the reasoning state (termed "continuous thought"). Rather than decoding this into a word token, we feed it back to the LLM as the subsequent input embedding directly in the continuous space. Experiments show that Coconut can effectively augment the LLM on several reasoning tasks. This novel latent reasoning paradigm leads to emergent advanced reasoning patterns: the continuous thought can encode multiple alternative next reasoning steps, allowing the model to perform a breadth-first search (BFS) to solve the problem, rather than prematurely committing to a single deterministic path like CoT. Coconut outperforms CoT in certain logical reasoning tasks that require substantial backtracking during planning, with fewer thinking tokens during inference. These findings demonstrate the promise of latent reasoning and offer valuable insights for future research.', 'score': 23, 'issue_id': 1039, 'pub_date': '2024-12-09', 'pub_date_card': {'ru': '9 декабря', 'en': 'December 9', 'zh': '12月9日'}, 'hash': '0ab7afee5f208244', 'authors': ['Shibo Hao', 'Sainbayar Sukhbaatar', 'DiJia Su', 'Xian Li', 'Zhiting Hu', 'Jason Weston', 'Yuandong Tian'], 'affiliations': ['FAIR at Meta', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2412.06769.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#rl', '#training'], 'emoji': '🥥', 'ru': {'title': 'Coconut: непрерывные рассуждения в скрытом пространстве для больших языковых моделей', 'desc': 'Статья представляет новую парадигму рассуждений для больших языковых моделей под названием Coconut (Chain of Continuous Thought). В отличие от традиционного подхода цепочки размышлений (CoT), Coconut использует скрытое состояние модели как непрерывное представление хода рассуждений. Эксперименты показывают, что Coconut может эффективно усиливать способности языковых моделей в задачах рассуждения, позволяя им выполнять поиск в ширину вместо детерминированного пути. Результаты демонстрируют преимущества Coconut над CoT в некоторых задачах логического вывода, требующих значительного бэктрекинга при планировании.'}, 'en': {'title': 'Unlocking Reasoning Potential with Continuous Thought', 'desc': 'This paper introduces a new reasoning approach for large language models (LLMs) called Coconut, which operates in a continuous latent space rather than the traditional language space. The authors argue that the language space can limit reasoning capabilities, as many tokens are not essential for reasoning tasks. By using the last hidden state of the LLM as a representation of reasoning, Coconut allows for more flexible exploration of reasoning paths, enabling the model to consider multiple alternatives simultaneously. Experimental results show that Coconut outperforms the conventional chain-of-thought method in logical reasoning tasks that require backtracking, demonstrating the effectiveness of this novel paradigm.'}, 'zh': {'title': 'Coconut：超越语言空间的推理新范式', 'desc': '大型语言模型（LLMs）通常在“语言空间”中进行推理，使用链式思维（CoT）来解决复杂问题。然而，语言空间并不总是最优的推理方式，因为许多词汇主要用于文本连贯性，而非推理本身。本文提出了一种新范式Coconut（连续思维链），利用LLM的最后隐藏状态作为推理状态的表示，并直接在连续空间中进行输入嵌入。实验表明，Coconut在多个推理任务中有效增强了LLM的表现，尤其在需要大量回溯的逻辑推理任务中表现优于传统的CoT。'}}}, {'id': 'https://huggingface.co/papers/2412.06781', 'title': 'Around the World in 80 Timesteps: A Generative Approach to Global Visual Geolocation', 'url': 'https://huggingface.co/papers/2412.06781', 'abstract': "Global visual geolocation predicts where an image was captured on Earth. Since images vary in how precisely they can be localized, this task inherently involves a significant degree of ambiguity. However, existing approaches are deterministic and overlook this aspect. In this paper, we aim to close the gap between traditional geolocalization and modern generative methods. We propose the first generative geolocation approach based on diffusion and Riemannian flow matching, where the denoising process operates directly on the Earth's surface. Our model achieves state-of-the-art performance on three visual geolocation benchmarks: OpenStreetView-5M, YFCC-100M, and iNat21. In addition, we introduce the task of probabilistic visual geolocation, where the model predicts a probability distribution over all possible locations instead of a single point. We introduce new metrics and baselines for this task, demonstrating the advantages of our diffusion-based approach. Codes and models will be made available.", 'score': 9, 'issue_id': 1047, 'pub_date': '2024-12-09', 'pub_date_card': {'ru': '9 декабря', 'en': 'December 9', 'zh': '12月9日'}, 'hash': '70c292a63437d52a', 'authors': ['Nicolas Dufour', 'David Picard', 'Vicky Kalogeiton', 'Loic Landrieu'], 'affiliations': ['LIGM, Ecole des Ponts, IP Paris, CNRS, UGE', 'LIX, Ecole Polytechnique, IP Paris'], 'pdf_title_img': 'assets/pdf/title_img/2412.06781.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#cv', '#games', '#dataset'], 'emoji': '🌍', 'ru': {'title': 'Генеративная геолокация: от точки к распределению', 'desc': 'Статья представляет новый подход к глобальной визуальной геолокации изображений с использованием генеративных методов. Авторы предлагают первую генеративную модель геолокации на основе диффузии и римановского потокового сопоставления, работающую непосредственно на поверхности Земли. Модель достигает наилучших результатов на трех эталонных наборах данных по визуальной геолокации. Кроме того, вводится задача вероятностной визуальной геолокации, где модель предсказывает распределение вероятностей по всем возможным местоположениям.'}, 'en': {'title': 'Revolutionizing Geolocation with Generative Models', 'desc': "This paper presents a novel approach to global visual geolocation, which determines where an image was taken on Earth. Unlike traditional methods that provide a single location, this research introduces a generative model that predicts a probability distribution over possible locations, addressing the inherent ambiguity in localization. The proposed method utilizes diffusion and Riemannian flow matching to enhance the denoising process directly on the Earth's surface. The model outperforms existing techniques on multiple benchmarks, showcasing the effectiveness of integrating generative methods into geolocation tasks."}, 'zh': {'title': '生成性地理定位：超越传统方法的创新', 'desc': '全球视觉地理定位旨在预测图像在地球上的拍摄位置。由于图像的定位精度各不相同，这一任务本质上存在很大的模糊性。现有的方法往往是确定性的，未能考虑这一点。本文提出了一种基于扩散和黎曼流匹配的生成性地理定位方法，能够在地球表面直接进行去噪处理，并在多个基准测试中取得了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2412.04432', 'title': 'Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation', 'url': 'https://huggingface.co/papers/2412.04432', 'abstract': 'In recent years, there has been a significant surge of interest in unifying image comprehension and generation within Large Language Models (LLMs). This growing interest has prompted us to explore extending this unification to videos. The core challenge lies in developing a versatile video tokenizer that captures both the spatial characteristics and temporal dynamics of videos to obtain representations for LLMs, and the representations can be further decoded into realistic video clips to enable video generation. In this work, we introduce Divot, a Diffusion-Powered Video Tokenizer, which leverages the diffusion process for self-supervised video representation learning. We posit that if a video diffusion model can effectively de-noise video clips by taking the features of a video tokenizer as the condition, then the tokenizer has successfully captured robust spatial and temporal information. Additionally, the video diffusion model inherently functions as a de-tokenizer, decoding videos from their representations. Building upon the Divot tokenizer, we present Divot-Vicuna through video-to-text autoregression and text-to-video generation by modeling the distributions of continuous-valued Divot features with a Gaussian Mixture Model. Experimental results demonstrate that our diffusion-based video tokenizer, when integrated with a pre-trained LLM, achieves competitive performance across various video comprehension and generation benchmarks. The instruction tuned Divot-Vicuna also excels in video storytelling, generating interleaved narratives and corresponding videos.', 'score': 9, 'issue_id': 1044, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 декабря', 'en': 'December 5', 'zh': '12月5日'}, 'hash': 'f3480cfa7666bb51', 'authors': ['Yuying Ge', 'Yizhuo Li', 'Yixiao Ge', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG'], 'pdf_title_img': 'assets/pdf/title_img/2412.04432.jpg', 'data': {'categories': ['#benchmark', '#video', '#architecture', '#story_generation', '#multimodal', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Единая модель для понимания и генерации видео на основе диффузии', 'desc': 'Исследователи представили Divot - токенизатор видео на основе диффузионных моделей. Divot использует процесс диффузии для самообучаемого представления видео, эффективно захватывая пространственно-временную информацию. На основе Divot создана модель Divot-Vicuna, способная к авторегрессии видео-в-текст и генерации видео по тексту. Эксперименты показывают, что интеграция Divot с предобученной языковой моделью позволяет достичь высоких результатов в задачах понимания и генерации видео.'}, 'en': {'title': 'Unifying Video Understanding and Generation with Divot', 'desc': "This paper discusses the integration of image understanding and creation within Large Language Models (LLMs) and extends this concept to video processing. The authors introduce Divot, a video tokenizer that uses a diffusion process for self-supervised learning, capturing both spatial and temporal features of videos. By conditioning a video diffusion model on the tokenizer's features, they demonstrate effective video representation and generation. The results show that Divot, when combined with a pre-trained LLM, performs well in video comprehension and storytelling tasks, generating coherent narratives and videos."}, 'zh': {'title': '视频理解与生成的统一新突破', 'desc': '近年来，图像理解与生成在大型语言模型（LLMs）中的统一引起了广泛关注。我们探索将这种统一扩展到视频领域，面临的核心挑战是开发一个多功能的视频标记器，以捕捉视频的空间特征和时间动态。我们提出了Divot，这是一种基于扩散过程的视频标记器，能够进行自监督的视频表示学习。通过将Divot与预训练的LLM结合，我们的实验结果表明，该方法在视频理解和生成的多个基准测试中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2412.05939', 'title': 'Exploring Multi-Grained Concept Annotations for Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2412.05939', 'abstract': 'Multimodal Large Language Models (MLLMs) excel in vision--language tasks by pre-training solely on coarse-grained concept annotations (e.g., image captions). We hypothesize that integrating fine-grained concept annotations (e.g., object labels and object regions) will further improve performance, as both data granularities complement each other in terms of breadth and depth in concept representation. We introduce a new dataset featuring Multimodal Multi-Grained Concept annotations (MMGiC) for MLLMs. In constructing MMGiC, we explore the impact of different data recipes on multimodal comprehension and generation. Our analyses reveal that multi-grained concept annotations integrate and complement each other, under our structured template and a general MLLM framework. We clearly explore and demonstrate the potential of MMGiC to help MLLMs better locate and learn concepts, aligning vision and language at multiple granularities. We further validate our hypothesis by investigating the fair comparison and effective collaboration between MMGiC and image--caption data on 12 multimodal comprehension and generation benchmarks, e.g., their appropriate combination achieve 3.95% and 2.34% absolute improvements over image--caption data alone on POPE and SEED-Bench. Code, data and models will be available at https://github.com/LooperXX/MMGiC.', 'score': 8, 'issue_id': 1042, 'pub_date': '2024-12-08', 'pub_date_card': {'ru': '8 декабря', 'en': 'December 8', 'zh': '12月8日'}, 'hash': 'a036a456409ed492', 'authors': ['Xiao Xu', 'Tianhao Niu', 'Yuxi Xie', 'Libo Qin', 'Wanxiang Che', 'Min-Yen Kan'], 'affiliations': ['Central South University', 'Harbin Institute of Technology', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2412.05939.jpg', 'data': {'categories': ['#data', '#multimodal', '#alignment', '#dataset', '#benchmark', '#open_source'], 'emoji': '🔬', 'ru': {'title': 'Многозернистые аннотации улучшают мультимодальное обучение', 'desc': 'Статья представляет новый набор данных MMGiC, содержащий мультимодальные многозернистые концептуальные аннотации для обучения мультимодальных больших языковых моделей (MLLM). Авторы исследуют влияние различных комбинаций данных на понимание и генерацию мультимодального контента. Результаты показывают, что многозернистые аннотации концептов дополняют друг друга и помогают MLLM лучше локализовать и изучать концепты, улучшая согласование зрения и языка на нескольких уровнях детализации. Эксперименты на 12 бенчмарках демонстрируют значительное улучшение производительности при комбинировании MMGiC с данными подписей к изображениям.'}, 'en': {'title': 'Enhancing MLLMs with Multi-Grained Concept Annotations', 'desc': "This paper discusses the enhancement of Multimodal Large Language Models (MLLMs) by incorporating fine-grained concept annotations alongside coarse-grained annotations like image captions. The authors introduce a new dataset called MMGiC, which includes both types of annotations to improve the models' understanding and generation of multimodal content. Their experiments show that using multi-grained annotations leads to better alignment between vision and language, resulting in significant performance improvements on various benchmarks. The findings suggest that combining different levels of data granularity can effectively enhance the capabilities of MLLMs in vision-language tasks."}, 'zh': {'title': '多粒度概念注释提升多模态学习效果', 'desc': '多模态大型语言模型（MLLMs）在视觉-语言任务中表现出色，主要依赖于粗粒度的概念注释（如图像标题）。我们假设，整合细粒度的概念注释（如物体标签和物体区域）将进一步提升性能，因为这两种数据粒度在概念表示的广度和深度上相辅相成。我们引入了一个新的数据集，包含多模态多粒度概念注释（MMGiC），并探讨不同数据组合对多模态理解和生成的影响。我们的分析表明，MMGiC能够帮助MLLMs更好地定位和学习概念，在多个粒度上对齐视觉和语言。'}}}, {'id': 'https://huggingface.co/papers/2412.06699', 'title': 'You See it, You Got it: Learning 3D Creation on Pose-Free Videos at Scale', 'url': 'https://huggingface.co/papers/2412.06699', 'abstract': "Recent 3D generation models typically rely on limited-scale 3D `gold-labels' or 2D diffusion priors for 3D content creation. However, their performance is upper-bounded by constrained 3D priors due to the lack of scalable learning paradigms. In this work, we present See3D, a visual-conditional multi-view diffusion model trained on large-scale Internet videos for open-world 3D creation. The model aims to Get 3D knowledge by solely Seeing the visual contents from the vast and rapidly growing video data -- You See it, You Got it. To achieve this, we first scale up the training data using a proposed data curation pipeline that automatically filters out multi-view inconsistencies and insufficient observations from source videos. This results in a high-quality, richly diverse, large-scale dataset of multi-view images, termed WebVi3D, containing 320M frames from 16M video clips. Nevertheless, learning generic 3D priors from videos without explicit 3D geometry or camera pose annotations is nontrivial, and annotating poses for web-scale videos is prohibitively expensive. To eliminate the need for pose conditions, we introduce an innovative visual-condition - a purely 2D-inductive visual signal generated by adding time-dependent noise to the masked video data. Finally, we introduce a novel visual-conditional 3D generation framework by integrating See3D into a warping-based pipeline for high-fidelity 3D generation. Our numerical and visual comparisons on single and sparse reconstruction benchmarks show that See3D, trained on cost-effective and scalable video data, achieves notable zero-shot and open-world generation capabilities, markedly outperforming models trained on costly and constrained 3D datasets. Please refer to our project page at: https://vision.baai.ac.cn/see3d", 'score': 7, 'issue_id': 1041, 'pub_date': '2024-12-09', 'pub_date_card': {'ru': '9 декабря', 'en': 'December 9', 'zh': '12月9日'}, 'hash': '1d5d0c1aa060a03f', 'authors': ['Baorui Ma', 'Huachen Gao', 'Haoge Deng', 'Zhengxiong Luo', 'Tiejun Huang', 'Lulu Tang', 'Xinlong Wang'], 'affiliations': ['Beijing Academy of Artificial Intelligence (BAAI)'], 'pdf_title_img': 'assets/pdf/title_img/2412.06699.jpg', 'data': {'categories': ['#dataset', '#open_source', '#3d', '#diffusion', '#data', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'Вы видите - вы понимаете 3D', 'desc': 'См3D - это модель мультиракурсной диффузии, обученная на масштабных интернет-видео для создания 3D-контента. Она получает знания о 3D, анализируя только визуальное содержание огромных объемов видеоданных. Для обучения был создан высококачественный набор данных WebVi3D, содержащий 320 млн кадров из 16 млн видеоклипов. См3D использует новаторский визуальный условный сигнал и интегрируется в конвейер на основе деформации для высококачественной 3D-генерации.'}, 'en': {'title': 'You See It, You Got It: 3D Generation from Video Data', 'desc': 'This paper introduces See3D, a novel multi-view diffusion model designed for generating 3D content from large-scale Internet videos. Unlike traditional methods that depend on limited 3D labels or 2D priors, See3D leverages a vast dataset of multi-view images, called WebVi3D, which is curated from 320 million frames across 16 million video clips. The model innovatively uses a visual-condition approach that eliminates the need for explicit 3D geometry or camera pose annotations, allowing it to learn generic 3D priors effectively. The results demonstrate that See3D excels in zero-shot and open-world generation tasks, outperforming existing models that rely on expensive 3D datasets.'}, 'zh': {'title': '通过视觉内容实现开放世界3D生成', 'desc': '本文介绍了一种名为See3D的视觉条件多视角扩散模型，旨在通过大规模互联网视频进行开放世界的3D内容生成。该模型通过自动过滤多视角不一致性和不足观察，构建了一个包含320M帧的高质量多视角图像数据集WebVi3D。为了消除对相机姿态的依赖，See3D引入了一种创新的视觉条件，通过在掩蔽视频数据中添加时间相关噪声生成纯2D诱导视觉信号。最终，See3D在单一和稀疏重建基准测试中表现出显著的零样本和开放世界生成能力，超越了基于昂贵3D数据集训练的模型。'}}}, {'id': 'https://huggingface.co/papers/2412.03123', 'title': 'Robust Multi-bit Text Watermark with LLM-based Paraphrasers', 'url': 'https://huggingface.co/papers/2412.03123', 'abstract': 'We propose an imperceptible multi-bit text watermark embedded by paraphrasing with LLMs. We fine-tune a pair of LLM paraphrasers that are designed to behave differently so that their paraphrasing difference reflected in the text semantics can be identified by a trained decoder. To embed our multi-bit watermark, we use two paraphrasers alternatively to encode the pre-defined binary code at the sentence level. Then we use a text classifier as the decoder to decode each bit of the watermark. Through extensive experiments, we show that our watermarks can achieve over 99.99\\% detection AUC with small (1.1B) text paraphrasers while keeping the semantic information of the original sentence. More importantly, our pipeline is robust under word substitution and sentence paraphrasing perturbations and generalizes well to out-of-distributional data. We also show the stealthiness of our watermark with LLM-based evaluation. We open-source the code: https://github.com/xiaojunxu/multi-bit-text-watermark.', 'score': 5, 'issue_id': 1040, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 декабря', 'en': 'December 4', 'zh': '12月4日'}, 'hash': 'bfad93a82eaec473', 'authors': ['Xiaojun Xu', 'Jinghan Jia', 'Yuanshun Yao', 'Yang Liu', 'Hang Li'], 'affiliations': ['ByteDance Research', 'Michigan State University', 'University of California, Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2412.03123.jpg', 'data': {'categories': ['#data', '#multimodal', '#dataset', '#open_source', '#small_models'], 'emoji': '💧', 'ru': {'title': 'Невидимые водяные знаки в тексте с помощью ИИ', 'desc': 'Исследователи предлагают метод встраивания незаметного многобитного водяного знака в текст с помощью перефразирования, используя большие языковые модели (LLM). Они обучают пару LLM-парафразеров, которые ведут себя по-разному, чтобы их различия в перефразировании могли быть идентифицированы специальным декодером. Для встраивания водяного знака используются два парафразера, которые кодируют заданный бинарный код на уровне предложений. Эксперименты показывают высокую эффективность метода, его устойчивость к искажениям и хорошую обобщаемость на новые данные.'}, 'en': {'title': 'Stealthy Multi-Bit Watermarking via Paraphrasing with LLMs', 'desc': "This paper presents a method for embedding a multi-bit watermark in text using paraphrasing techniques with large language models (LLMs). The authors fine-tune two distinct LLM paraphrasers that create different paraphrases, allowing a trained decoder to identify the semantic differences and extract the embedded watermark. The watermark is encoded at the sentence level by alternating between the two paraphrasers, achieving high detection accuracy while preserving the original text's meaning. The proposed method demonstrates robustness against various text perturbations and maintains effectiveness even with out-of-distribution data."}, 'zh': {'title': '隐形水印，语义保留！', 'desc': '本文提出了一种通过大语言模型（LLMs）进行的不可察觉的多比特文本水印嵌入方法。我们微调了一对表现不同的LLM改写器，以便通过文本语义的差异来识别其改写结果。为了嵌入多比特水印，我们交替使用两个改写器在句子级别编码预定义的二进制代码，并使用文本分类器作为解码器来解码每一位水印。实验结果表明，我们的水印在保持原句语义信息的同时，检测AUC超过99.99%，并且在词语替换和句子改写扰动下表现出良好的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2412.05600', 'title': 'Global and Dense Embeddings of Earth: Major TOM Floating in the Latent Space', 'url': 'https://huggingface.co/papers/2412.05600', 'abstract': "With the ever-increasing volumes of the Earth observation data present in the archives of large programmes such as Copernicus, there is a growing need for efficient vector representations of the underlying raw data. The approach of extracting feature representations from pretrained deep neural networks is a powerful approach that can provide semantic abstractions of the input data. However, the way this is done for imagery archives containing geospatial data has not yet been defined. In this work, an extension is proposed to an existing community project, Major TOM, focused on the provision and standardization of open and free AI-ready datasets for Earth observation. Furthermore, four global and dense embedding datasets are released openly and for free along with the publication of this manuscript, resulting in the most comprehensive global open dataset of geospatial visual embeddings in terms of covered Earth's surface.", 'score': 4, 'issue_id': 1046, 'pub_date': '2024-12-07', 'pub_date_card': {'ru': '7 декабря', 'en': 'December 7', 'zh': '12月7日'}, 'hash': 'a30334645404dda6', 'authors': ['Mikolaj Czerkawski', 'Marcin Kluczek', 'Jędrzej S. Bojanowski'], 'affiliations': ['CloudFerro, Warsaw, Poland', 'Φ-lab, European Space Agency, Frascati, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2412.05600.jpg', 'data': {'categories': ['#data', '#dataset', '#open_source'], 'emoji': '🛰️', 'ru': {'title': 'Глобальные визуальные эмбеддинги для эффективного анализа спутниковых снимков', 'desc': 'В статье предлагается метод эффективного векторного представления данных дистанционного зондирования Земли с использованием предобученных глубоких нейронных сетей. Авторы расширяют существующий проект Major TOM для стандартизации открытых наборов данных для наблюдения Земли. Они выпускают четыре глобальных набора данных плотных эмбеддингов, полученных из спутниковых снимков. Это самый полный открытый набор геопространственных визуальных эмбеддингов с точки зрения охвата поверхности Земли.'}, 'en': {'title': 'Unlocking Earth Data: Semantic Embeddings for Geospatial Insights', 'desc': 'This paper addresses the challenge of efficiently representing large volumes of Earth observation data, particularly from programs like Copernicus. It proposes using pretrained deep neural networks to extract feature representations that provide semantic insights into geospatial imagery. The authors extend the Major TOM project, which aims to standardize open AI-ready datasets for Earth observation. Additionally, they release four global embedding datasets, creating a comprehensive resource for researchers working with geospatial visual embeddings.'}, 'zh': {'title': '高效地球观测数据表示的新方法', 'desc': '随着地球观测数据量的不断增加，如何有效地表示这些原始数据变得越来越重要。本文提出了一种从预训练深度神经网络中提取特征表示的方法，可以为输入数据提供语义抽象。针对包含地理空间数据的图像档案，本文定义了一种新的处理方式，并扩展了现有的社区项目Major TOM。最后，本文公开发布了四个全球密集嵌入数据集，成为覆盖地球表面的最全面的开放地理空间视觉嵌入数据集。'}}}, {'id': 'https://huggingface.co/papers/2412.06767', 'title': 'MAtCha Gaussians: Atlas of Charts for High-Quality Geometry and Photorealism From Sparse Views', 'url': 'https://huggingface.co/papers/2412.06767', 'abstract': "We present a novel appearance model that simultaneously realizes explicit high-quality 3D surface mesh recovery and photorealistic novel view synthesis from sparse view samples. Our key idea is to model the underlying scene geometry Mesh as an Atlas of Charts which we render with 2D Gaussian surfels (MAtCha Gaussians). MAtCha distills high-frequency scene surface details from an off-the-shelf monocular depth estimator and refines it through Gaussian surfel rendering. The Gaussian surfels are attached to the charts on the fly, satisfying photorealism of neural volumetric rendering and crisp geometry of a mesh model, i.e., two seemingly contradicting goals in a single model. At the core of MAtCha lies a novel neural deformation model and a structure loss that preserve the fine surface details distilled from learned monocular depths while addressing their fundamental scale ambiguities. Results of extensive experimental validation demonstrate MAtCha's state-of-the-art quality of surface reconstruction and photorealism on-par with top contenders but with dramatic reduction in the number of input views and computational time. We believe MAtCha will serve as a foundational tool for any visual application in vision, graphics, and robotics that require explicit geometry in addition to photorealism. Our project page is the following: https://anttwo.github.io/matcha/", 'score': 3, 'issue_id': 1050, 'pub_date': '2024-12-09', 'pub_date_card': {'ru': '9 декабря', 'en': 'December 9', 'zh': '12月9日'}, 'hash': '3f286ad9a4744a4d', 'authors': ['Antoine Guédon', 'Tomoki Ichikawa', 'Kohei Yamashita', 'Ko Nishino'], 'affiliations': ['Graduate School of Informatics, Kyoto University, Japan', 'LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, France'], 'pdf_title_img': 'assets/pdf/title_img/2412.06767.jpg', 'data': {'categories': ['#3d', '#cv', '#robotics'], 'emoji': '🔍', 'ru': {'title': 'Фотореалистичная 3D-реконструкция из небольшого числа изображений', 'desc': 'Статья представляет новую модель внешнего вида, которая одновременно восстанавливает высококачественную 3D-сетку поверхности и синтезирует фотореалистичные новые ракурсы из небольшого набора исходных видов. Ключевая идея заключается в моделировании геометрии сцены как атласа графиков, которые отрисовываются с помощью 2D гауссовых сёрфелов (MAtCha Gaussians). MAtCha извлекает детали поверхности сцены высокой частоты из монокулярного оценщика глубины и уточняет их через рендеринг гауссовых сёрфелов. В основе MAtCha лежит новая нейронная модель деформации и структурная функция потерь, которые сохраняют мелкие детали поверхности, извлеченные из оценок монокулярной глубины.'}, 'en': {'title': 'MAtCha: Merging Geometry and Photorealism in 3D Reconstruction', 'desc': 'This paper introduces MAtCha, a new model that effectively combines high-quality 3D surface mesh recovery with photorealistic image generation from limited view samples. The model uses an innovative approach called an Atlas of Charts, which employs 2D Gaussian surfels to enhance the visual quality of the rendered scenes. By integrating a neural deformation model and a structure loss, MAtCha preserves intricate surface details while resolving scale ambiguities from monocular depth estimations. Experimental results show that MAtCha achieves top-tier performance in both surface reconstruction and photorealism, requiring fewer input views and less computational power than existing methods.'}, 'zh': {'title': 'MAtCha：高效的3D表面重建与逼真渲染结合', 'desc': '我们提出了一种新颖的外观模型，能够同时实现高质量的3D表面网格恢复和逼真的新视图合成。我们的关键思想是将场景几何建模为一个图集，通过2D高斯表面点进行渲染。MAtCha从现成的单目深度估计器中提取高频场景表面细节，并通过高斯表面点渲染进行精细化。实验结果表明，MAtCha在表面重建和逼真度方面达到了最先进的质量，同时显著减少了输入视图数量和计算时间。'}}}, {'id': 'https://huggingface.co/papers/2412.06782', 'title': 'CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive Prediction', 'url': 'https://huggingface.co/papers/2412.06782', 'abstract': 'In robotic visuomotor policy learning, diffusion-based models have achieved significant success in improving the accuracy of action trajectory generation compared to traditional autoregressive models. However, they suffer from inefficiency due to multiple denoising steps and limited flexibility from complex constraints. In this paper, we introduce Coarse-to-Fine AutoRegressive Policy (CARP), a novel paradigm for visuomotor policy learning that redefines the autoregressive action generation process as a coarse-to-fine, next-scale approach. CARP decouples action generation into two stages: first, an action autoencoder learns multi-scale representations of the entire action sequence; then, a GPT-style transformer refines the sequence prediction through a coarse-to-fine autoregressive process. This straightforward and intuitive approach produces highly accurate and smooth actions, matching or even surpassing the performance of diffusion-based policies while maintaining efficiency on par with autoregressive policies. We conduct extensive evaluations across diverse settings, including single-task and multi-task scenarios on state-based and image-based simulation benchmarks, as well as real-world tasks. CARP achieves competitive success rates, with up to a 10% improvement, and delivers 10x faster inference compared to state-of-the-art policies, establishing a high-performance, efficient, and flexible paradigm for action generation in robotic tasks.', 'score': 3, 'issue_id': 1039, 'pub_date': '2024-12-09', 'pub_date_card': {'ru': '9 декабря', 'en': 'December 9', 'zh': '12月9日'}, 'hash': '584dec780be05e2d', 'authors': ['Zhefei Gong', 'Pengxiang Ding', 'Shangke Lyu', 'Siteng Huang', 'Mingyang Sun', 'Wei Zhao', 'Zhaoxin Fan', 'Donglin Wang'], 'affiliations': ['Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2412.06782.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#agents', '#training', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'CARP: эффективное и точное обучение роботов через поэтапное уточнение действий', 'desc': 'В этой статье представлен новый подход к обучению визуомоторной политики роботов, называемый CARP. Он использует двухэтапный процесс: сначала автоэнкодер действий обучается многомасштабным представлениям последовательности действий, а затем трансформер в стиле GPT уточняет предсказание последовательности через поэтапный авторегрессивный процесс. CARP показывает высокую точность и плавность действий, сравнимую или превосходящую диффузионные модели, при этом сохраняя эффективность авторегрессивных подходов. Метод демонстрирует конкурентоспособные показатели успешности и в 10 раз более быстрый вывод по сравнению с современными методами.'}, 'en': {'title': 'Efficient and Accurate Action Generation with CARP', 'desc': 'This paper presents the Coarse-to-Fine AutoRegressive Policy (CARP), a new method for robotic visuomotor policy learning that enhances action trajectory generation. CARP improves upon traditional autoregressive models by breaking down the action generation into two stages: first, it uses an action autoencoder to create multi-scale representations, and then a GPT-style transformer refines these predictions. This approach not only increases the accuracy and smoothness of actions but also maintains efficiency, achieving up to 10x faster inference than existing methods. Extensive evaluations show that CARP outperforms diffusion-based models and achieves competitive success rates in various robotic tasks.'}, 'zh': {'title': '高效灵活的机器人动作生成新范式', 'desc': '在机器人视觉运动策略学习中，基于扩散模型的技术在动作轨迹生成的准确性上取得了显著成功，但由于多次去噪步骤和复杂约束，效率较低。本文提出了一种新颖的粗到细自回归策略（CARP），将自回归动作生成过程重新定义为粗到细的下一尺度方法。CARP将动作生成分为两个阶段：首先，动作自编码器学习整个动作序列的多尺度表示；然后，GPT风格的变换器通过粗到细的自回归过程精炼序列预测。该方法在效率上与自回归策略相当，同时在准确性和流畅性上与基于扩散的策略相匹配或超越，展示了高性能、高效和灵活的机器人任务动作生成新范式。'}}}, {'id': 'https://huggingface.co/papers/2412.04470', 'title': 'Turbo3D: Ultra-fast Text-to-3D Generation', 'url': 'https://huggingface.co/papers/2412.04470', 'abstract': "We present Turbo3D, an ultra-fast text-to-3D system capable of generating high-quality Gaussian splatting assets in under one second. Turbo3D employs a rapid 4-step, 4-view diffusion generator and an efficient feed-forward Gaussian reconstructor, both operating in latent space. The 4-step, 4-view generator is a student model distilled through a novel Dual-Teacher approach, which encourages the student to learn view consistency from a multi-view teacher and photo-realism from a single-view teacher. By shifting the Gaussian reconstructor's inputs from pixel space to latent space, we eliminate the extra image decoding time and halve the transformer sequence length for maximum efficiency. Our method demonstrates superior 3D generation results compared to previous baselines, while operating in a fraction of their runtime.", 'score': 1, 'issue_id': 1053, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 декабря', 'en': 'December 5', 'zh': '12月5日'}, 'hash': '426824b2e09af1a0', 'authors': ['Hanzhe Hu', 'Tianwei Yin', 'Fujun Luan', 'Yiwei Hu', 'Hao Tan', 'Zexiang Xu', 'Sai Bi', 'Shubham Tulsiani', 'Kai Zhang'], 'affiliations': ['Adobe Research', 'Carnegie Mellon University', 'Massachusetts Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2412.04470.jpg', 'data': {'categories': ['#3d', '#architecture', '#diffusion', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Революция в 3D-генерации: от текста к модели за доли секунды', 'desc': 'Turbo3D - это сверхбыстрая система генерации трехмерных объектов из текстового описания, создающая высококачественные модели на основе гауссова сплаттинга менее чем за секунду. Система использует быстрый 4-шаговый, 4-ракурсный генератор диффузии и эффективный реконструктор гауссовых сплатов, работающие в латентном пространстве. Генератор обучается с помощью нового подхода Dual-Teacher, который поощряет согласованность ракурсов и фотореалистичность. Благодаря переносу входных данных реконструктора в латентное пространство, Turbo3D достигает максимальной эффективности, превосходя существующие методы по качеству и скорости генерации 3D-объектов.'}, 'en': {'title': 'Turbo3D: Lightning-Fast Text-to-3D Generation!', 'desc': 'Turbo3D is a cutting-edge system that quickly converts text descriptions into 3D models using Gaussian splatting techniques. It utilizes a unique 4-step, 4-view diffusion generator, which is trained through a Dual-Teacher method to ensure both view consistency and photo-realism. By processing data in latent space instead of pixel space, Turbo3D significantly reduces the time needed for image decoding and optimizes the transformer sequence length. This innovative approach results in faster and higher-quality 3D asset generation compared to existing methods.'}, 'zh': {'title': 'Turbo3D：超快速文本到3D生成的革命', 'desc': 'Turbo3D是一种超快速的文本到3D生成系统，能够在不到一秒的时间内生成高质量的高斯点云资产。该系统采用快速的四步、四视图扩散生成器和高效的前馈高斯重构器，均在潜在空间中运行。通过新颖的双教师方法，四步、四视图生成器的学生模型能够从多视图教师那里学习视图一致性，并从单视图教师那里学习照片真实感。我们的研究表明，Turbo3D在3D生成结果上优于之前的基线，同时运行时间仅为其一小部分。'}}}, {'id': 'https://huggingface.co/papers/2412.04144', 'title': "If You Can't Use Them, Recycle Them: Optimizing Merging at Scale Mitigates Performance Tradeoffs", 'url': 'https://huggingface.co/papers/2412.04144', 'abstract': "Model merging has shown great promise at combining expert models, but the benefit of merging is unclear when merging ``generalist'' models trained on many tasks. We explore merging in the context of large (sim100B) models, by recycling checkpoints that exhibit tradeoffs among different tasks. Such checkpoints are often created in the process of developing a frontier model, and many suboptimal ones are usually discarded. Given a pool of model checkpoints obtained from different training runs (e.g., different stages, objectives, hyperparameters, and data mixtures), which naturally show tradeoffs across different language capabilities (e.g., instruction following vs. code generation), we investigate whether merging can recycle such suboptimal models into a Pareto-optimal one. Our optimization algorithm tunes the weight of each checkpoint in a linear combination, resulting in a Pareto-optimal models that outperforms both individual models and merge-based baselines. Further analysis shows that good merges tend to include almost all checkpoints with with non-zero weights, indicating that even seemingly bad initial checkpoints can contribute to good final merges.", 'score': 1, 'issue_id': 1049, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 декабря', 'en': 'December 5', 'zh': '12月5日'}, 'hash': '405fa76c78968872', 'authors': ['Muhammad Khalifa', 'Yi-Chern Tan', 'Arash Ahmadian', 'Tom Hosking', 'Honglak Lee', 'Lu Wang', 'Ahmet Üstün', 'Tom Sherborne', 'Matthias Gallé'], 'affiliations': ['Cohere', 'Cohere For AI', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2412.04144.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization'], 'emoji': '🧩', 'ru': {'title': 'Оптимальное слияние языковых моделей для достижения Парето-оптимальности', 'desc': 'Статья исследует объединение больших языковых моделей (около 100 млрд параметров) для улучшения их производительности. Авторы предлагают алгоритм оптимизации, который настраивает веса отдельных чекпойнтов в линейной комбинации. Результаты показывают, что объединенные модели превосходят как отдельные модели, так и базовые методы слияния. Анализ демонстрирует, что даже изначально неоптимальные чекпойнты могут внести вклад в итоговое успешное объединение.'}, 'en': {'title': 'Unlocking Potential: Merging Suboptimal Models for Optimal Performance', 'desc': 'This paper investigates the process of merging large language models, specifically those with around 100 billion parameters, to enhance their performance across various tasks. The authors focus on recycling suboptimal model checkpoints, which are often discarded during model development, to create a new model that is Pareto-optimal. By employing an optimization algorithm that adjusts the weights of these checkpoints, the resulting merged model surpasses both individual models and traditional merging methods. The findings suggest that even checkpoints that appear to be poor can play a significant role in achieving better overall performance when merged effectively.'}, 'zh': {'title': '合并模型，优化性能的关键', 'desc': '模型合并在结合专家模型方面显示出很大潜力，但在合并训练了多种任务的“通用”模型时，其好处并不明确。我们在大型模型（如1000亿参数）中探索合并，通过回收在不同任务之间存在权衡的检查点。我们的方法通过调整每个检查点的权重，生成一个帕累托最优模型，超越了单个模型和基于合并的基准。进一步分析表明，好的合并通常包括几乎所有具有非零权重的检查点，表明即使是看似不佳的初始检查点也能为最终合并做出贡献。'}}}, {'id': 'https://huggingface.co/papers/2412.05355', 'title': 'MotionShop: Zero-Shot Motion Transfer in Video Diffusion Models with Mixture of Score Guidance', 'url': 'https://huggingface.co/papers/2412.05355', 'abstract': 'In this work, we propose the first motion transfer approach in diffusion transformer through Mixture of Score Guidance (MSG), a theoretically-grounded framework for motion transfer in diffusion models. Our key theoretical contribution lies in reformulating conditional score to decompose motion score and content score in diffusion models. By formulating motion transfer as a mixture of potential energies, MSG naturally preserves scene composition and enables creative scene transformations while maintaining the integrity of transferred motion patterns. This novel sampling operates directly on pre-trained video diffusion models without additional training or fine-tuning. Through extensive experiments, MSG demonstrates successful handling of diverse scenarios including single object, multiple objects, and cross-object motion transfer as well as complex camera motion transfer. Additionally, we introduce MotionBench, the first motion transfer dataset consisting of 200 source videos and 1000 transferred motions, covering single/multi-object transfers, and complex camera motions.', 'score': 0, 'issue_id': 1047, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 декабря', 'en': 'December 6', 'zh': '12月6日'}, 'hash': 'fd2b5f6636c2d2af', 'authors': ['Hidir Yesiltepe', 'Tuna Han Salih Meral', 'Connor Dunlop', 'Pinar Yanardag'], 'affiliations': ['Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2412.05355.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#video', '#dataset'], 'emoji': '🎭', 'ru': {'title': 'Революционный подход к переносу движения в генеративных видеомоделях', 'desc': 'Исследователи предложили новый подход к переносу движения в диффузионных трансформерах, названный Mixture of Score Guidance (MSG). Ключевой теоретический вклад заключается в переформулировке условного скора для разложения скора движения и содержания в диффузионных моделях. MSG позволяет сохранять композицию сцены и обеспечивает творческие преобразования сцены, сохраняя целостность перенесенных паттернов движения. Метод работает напрямую с предобученными видео-диффузионными моделями без дополнительного обучения.'}, 'en': {'title': 'Revolutionizing Motion Transfer with Mixture of Score Guidance', 'desc': 'This paper introduces a new method called Mixture of Score Guidance (MSG) for transferring motion in video using diffusion transformers. The authors reformulate the conditional score to separate motion and content scores, allowing for better control over how motion is applied to scenes. MSG enables creative transformations while keeping the original motion patterns intact, and it works with existing pre-trained video diffusion models without needing extra training. The paper also presents MotionBench, a dataset designed for evaluating motion transfer techniques, featuring a variety of scenarios including single and multiple object transfers and complex camera movements.'}, 'zh': {'title': '创新运动转移：混合评分引导的应用', 'desc': '本文提出了一种在扩散变换器中进行运动转移的首个方法，称为混合评分引导（MSG）。我们通过重新构建条件评分，将运动评分和内容评分分解，从而为扩散模型中的运动转移提供了理论基础。MSG方法能够自然地保持场景构图，并在保持转移运动模式完整性的同时，实现创意场景变换。通过大量实验，MSG成功处理了多种场景，包括单个物体、多物体和复杂相机运动转移，并引入了MotionBench数据集，包含200个源视频和1000个转移运动。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (2)', '#agi', '#alignment (1)', '#architecture (4)', '#audio', '#benchmark (7)', '#cv (2)', '#data (4)', '#dataset (7)', '#diffusion (6)', '#ethics', '#games (1)', '#graphs', '#hallucinations', '#healthcare', '#inference', '#interpretability', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (3)', '#open_source (5)', '#optimization (4)', '#plp', '#rag', '#reasoning (3)', '#rl (2)', '#rlhf', '#robotics (2)', '#science', '#security', '#small_models (1)', '#story_generation (1)', '#survey', '#synthetic', '#training (4)', '#transfer_learning', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-12-10 21:10',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-12-10 21:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-12-10 21:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    