
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 21 papers. February 11.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">11 февраля</span> | <span id="title-articles-count">21 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-02-10.html">⬅️ <span id="prev-date">10.02</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-02-12.html">➡️ <span id="next-date">12.02</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-02.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'};
        let feedDateNext = {'ru': '12.02', 'en': '02/12', 'zh': '2月12日'};
        let feedDatePrev = {'ru': '10.02', 'en': '02/10', 'zh': '2月10日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2502.06394', 'title': 'SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data Annotators', 'url': 'https://huggingface.co/papers/2502.06394', 'abstract': 'Existing approaches to multilingual text detoxification are hampered by the scarcity of parallel multilingual datasets. In this work, we introduce a pipeline for the generation of multilingual parallel detoxification data. We also introduce SynthDetoxM, a manually collected and synthetically generated multilingual parallel text detoxification dataset comprising 16,000 high-quality detoxification sentence pairs across German, French, Spanish and Russian. The data was sourced from different toxicity evaluation datasets and then rewritten with nine modern open-source LLMs in few-shot setting. Our experiments demonstrate that models trained on the produced synthetic datasets have superior performance to those trained on the human-annotated MultiParaDetox dataset even in data limited setting. Models trained on SynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our dataset and code to help further research in multilingual text detoxification.', 'score': 62, 'issue_id': 2145, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '86b7da795fcf943b', 'authors': ['Daniil Moskovskiy', 'Nikita Sushko', 'Sergey Pletenev', 'Elena Tutubalina', 'Alexander Panchenko'], 'affiliations': ['AIRI', 'ISP RAS Research Center for Trusted AI', 'Sber AI', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2502.06394.jpg', 'data': {'categories': ['#low_resource', '#multilingual', '#dataset', '#data', '#synthetic', '#open_source'], 'emoji': '🧼', 'ru': {'title': 'Синтетические данные улучшают многоязычную детоксификацию текста', 'desc': 'Статья представляет новый подход к многоязычной детоксификации текста. Авторы разработали конвейер для генерации параллельных многоязычных данных для детоксификации и создали датасет SynthDetoxM, содержащий 16,000 высококачественных пар предложений на немецком, французском, испанском и русском языках. Эксперименты показали, что модели, обученные на синтетических данных, превосходят модели, обученные на аннотированном людьми датасете MultiParaDetox. Авторы опубликовали свой датасет и код для дальнейших исследований в области многоязычной детоксификации текста.'}, 'en': {'title': 'Enhancing Multilingual Detoxification with SynthDetoxM', 'desc': 'This paper addresses the challenge of multilingual text detoxification, which is limited by the lack of parallel datasets in multiple languages. The authors present a new pipeline for generating such datasets, introducing SynthDetoxM, a collection of 16,000 detoxified sentence pairs in German, French, Spanish, and Russian. These pairs were created by rewriting existing toxicity evaluation data using modern open-source large language models (LLMs) in a few-shot learning context. The results show that models trained on this synthetic dataset outperform those trained on existing human-annotated datasets, demonstrating the effectiveness of the proposed approach in enhancing multilingual detoxification efforts.'}, 'zh': {'title': '多语言文本去毒化的新突破', 'desc': '本研究提出了一种生成多语言平行去毒化数据的流程，以解决现有多语言文本去毒化方法中平行多语言数据集稀缺的问题。我们介绍了SynthDetoxM，这是一个手动收集和合成生成的多语言平行文本去毒化数据集，包含来自德语、法语、西班牙语和俄语的16,000对高质量去毒化句子。数据来源于不同的毒性评估数据集，并通过九种现代开源大语言模型在少量样本设置下进行重写。实验结果表明，基于合成数据集训练的模型在数据有限的情况下表现优于基于人工标注的MultiParaDetox数据集训练的模型。'}}}, {'id': 'https://huggingface.co/papers/2502.06781', 'title': 'Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning', 'url': 'https://huggingface.co/papers/2502.06781', 'abstract': 'Reasoning abilities, especially those for solving complex math problems, are crucial components of general intelligence. Recent advances by proprietary companies, such as o-series models of OpenAI, have made remarkable progress on reasoning tasks. However, the complete technical details remain unrevealed, and the techniques that are believed certainly to be adopted are only reinforcement learning (RL) and the long chain of thoughts. This paper proposes a new RL framework, termed OREAL, to pursue the performance limit that can be achieved through Outcome REwArd-based reinforcement Learning for mathematical reasoning tasks, where only binary outcome rewards are easily accessible. We theoretically prove that behavior cloning on positive trajectories from best-of-N (BoN) sampling is sufficient to learn the KL-regularized optimal policy in binary feedback environments. This formulation further implies that the rewards of negative samples should be reshaped to ensure the gradient consistency between positive and negative samples. To alleviate the long-existing difficulties brought by sparse rewards in RL, which are even exacerbated by the partial correctness of the long chain of thought for reasoning tasks, we further apply a token-level reward model to sample important tokens in reasoning trajectories for learning. With OREAL, for the first time, a 7B model can obtain 94.0 pass@1 accuracy on MATH-500 through RL, being on par with 32B models. OREAL-32B also surpasses previous 32B models trained by distillation with 95.0 pass@1 accuracy on MATH-500. Our investigation also indicates the importance of initial policy models and training queries for RL. Code, models, and data will be released to benefit future researchhttps://github.com/InternLM/OREAL.', 'score': 31, 'issue_id': 2142, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '9cd2694b7c865b94', 'authors': ['Chengqi Lyu', 'Songyang Gao', 'Yuzhe Gu', 'Wenwei Zhang', 'Jianfei Gao', 'Kuikun Liu', 'Ziyi Wang', 'Shuaibin Li', 'Qian Zhao', 'Haian Huang', 'Weihan Cao', 'Jiangning Liu', 'Hongwei Liu', 'Junnan Liu', 'Songyang Zhang', 'Dahua Lin', 'Kai Chen'], 'affiliations': ['HKGAI under InnoHK', 'MMLab, The Chinese University of Hong Kong', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06781.jpg', 'data': {'categories': ['#training', '#open_source', '#rl', '#reasoning', '#optimization', '#math'], 'emoji': '🧮', 'ru': {'title': 'OREAL: Прорыв в обучении с подкреплением для математических рассуждений', 'desc': 'Статья представляет новый фреймворк обучения с подкреплением под названием OREAL для решения математических задач. Авторы теоретически доказывают, что клонирование поведения на положительных траекториях из выборки best-of-N достаточно для обучения оптимальной политики в средах с бинарной обратной связью. Для преодоления проблемы разреженных наград применяется модель вознаграждения на уровне токенов. С помощью OREAL модель размером 7B достигает точности 94.0% pass@1 на датасете MATH-500, что сопоставимо с результатами 32B моделей.'}, 'en': {'title': 'OREAL: Advancing AI Reasoning with Outcome-Based Reinforcement Learning', 'desc': 'This paper introduces a new reinforcement learning framework called OREAL, designed to enhance mathematical reasoning capabilities in AI models. OREAL focuses on using binary outcome rewards to improve learning efficiency, particularly in environments where feedback is sparse. The authors demonstrate that behavior cloning from positive examples can effectively learn optimal policies, while also reshaping negative rewards to maintain gradient consistency. The results show that OREAL achieves high accuracy on mathematical tasks, outperforming larger models and highlighting the significance of initial policy models in the training process.'}, 'zh': {'title': 'OREAL：数学推理的新突破', 'desc': '这篇论文提出了一种新的强化学习框架，称为OREAL，旨在提高数学推理任务的性能。OREAL使用基于结果的奖励机制，专注于二元结果奖励，以解决强化学习中的稀疏奖励问题。研究表明，通过对最佳样本进行行为克隆，可以有效学习最优策略，并且需要对负样本的奖励进行重塑以保持梯度一致性。实验结果显示，使用OREAL的7B模型在MATH-500上达到了94.0的准确率，表现与32B模型相当，且OREAL-32B在同一任务上超越了之前的32B模型。'}}}, {'id': 'https://huggingface.co/papers/2502.06703', 'title': 'Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling', 'url': 'https://huggingface.co/papers/2502.06703', 'abstract': 'Test-Time Scaling (TTS) is an important method for improving the performance of Large Language Models (LLMs) by using additional computation during the inference phase. However, current studies do not systematically analyze how policy models, Process Reward Models (PRMs), and problem difficulty influence TTS. This lack of analysis limits the understanding and practical use of TTS methods. In this paper, we focus on two core questions: (1) What is the optimal approach to scale test-time computation across different policy models, PRMs, and problem difficulty levels? (2) To what extent can extended computation improve the performance of LLMs on complex tasks, and can smaller language models outperform larger ones through this approach? Through comprehensive experiments on MATH-500 and challenging AIME24 tasks, we have the following observations: (1) The compute-optimal TTS strategy is highly dependent on the choice of policy model, PRM, and problem difficulty. (2) With our compute-optimal TTS strategy, extremely small policy models can outperform larger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500. Moreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM surpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher inference efficiency. These findings show the significance of adapting TTS strategies to the specific characteristics of each task and model and indicate that TTS is a promising approach for enhancing the reasoning abilities of LLMs.', 'score': 27, 'issue_id': 2143, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '2129c5ac1750f3cc', 'authors': ['Runze Liu', 'Junqi Gao', 'Jian Zhao', 'Kaiyan Zhang', 'Xiu Li', 'Biqing Qi', 'Wanli Ouyang', 'Bowen Zhou'], 'affiliations': ['BUPT', 'Harbin Institute of Technology', 'Shanghai AI Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06703.jpg', 'data': {'categories': ['#inference', '#reasoning', '#small_models', '#training', '#optimization', '#math'], 'emoji': '🧮', 'ru': {'title': 'Маленькие модели побеждают гигантов: сила масштабирования во время теста', 'desc': 'Это исследование анализирует влияние метода Test-Time Scaling (TTS) на производительность больших языковых моделей (LLM) при решении сложных математических задач. Авторы изучают, как выбор политики модели, модели вознаграждения процесса (PRM) и сложность задачи влияют на оптимальную стратегию TTS. Результаты показывают, что даже небольшие модели могут превзойти более крупные при использовании оптимальной стратегии TTS. Исследование демонстрирует потенциал TTS для улучшения способностей LLM к рассуждению и решению сложных задач.'}, 'en': {'title': 'Unlocking LLM Potential: Small Models, Big Gains with TTS!', 'desc': 'This paper investigates Test-Time Scaling (TTS), a technique that enhances the performance of Large Language Models (LLMs) by adjusting computation during inference. It addresses how different policy models, Process Reward Models (PRMs), and the difficulty of problems affect the effectiveness of TTS. The authors conduct experiments on MATH-500 and AIME24 tasks, revealing that smaller models can outperform larger ones when using an optimal TTS strategy. The results emphasize the importance of tailoring TTS methods to specific models and tasks to improve reasoning capabilities in LLMs.'}, 'zh': {'title': '优化测试时间扩展，提升小型模型性能！', 'desc': '测试时间扩展（TTS）是一种通过在推理阶段增加计算量来提高大型语言模型（LLMs）性能的方法。本文系统分析了策略模型、过程奖励模型（PRMs）和问题难度如何影响TTS的效果。研究表明，计算最优的TTS策略依赖于所选的策略模型、PRM和问题难度，且小型模型在某些情况下可以超越大型模型。通过在MATH-500和AIME24任务上的实验，我们发现适应特定任务和模型的TTS策略对于提升LLMs的推理能力至关重要。'}}}, {'id': 'https://huggingface.co/papers/2502.05609', 'title': 'Lossless Acceleration of Large Language Models with Hierarchical Drafting based on Temporal Locality in Speculative Decoding', 'url': 'https://huggingface.co/papers/2502.05609', 'abstract': 'Accelerating inference in Large Language Models (LLMs) is critical for real-time interactions, as they have been widely incorporated into real-world services. Speculative decoding, a fully algorithmic solution, has gained attention for improving inference speed by drafting and verifying tokens, thereby generating multiple tokens in a single forward pass. However, current drafting strategies usually require significant fine-tuning or have inconsistent performance across tasks. To address these challenges, we propose Hierarchy Drafting (HD), a novel lossless drafting approach that organizes various token sources into multiple databases in a hierarchical framework based on temporal locality. In the drafting step, HD sequentially accesses multiple databases to obtain draft tokens from the highest to the lowest locality, ensuring consistent acceleration across diverse tasks and minimizing drafting latency. Our experiments on Spec-Bench using LLMs with 7B and 13B parameters demonstrate that HD outperforms existing database drafting methods, achieving robust inference speedups across model sizes, tasks, and temperatures.', 'score': 12, 'issue_id': 2141, 'pub_date': '2025-02-08', 'pub_date_card': {'ru': '8 февраля', 'en': 'February 8', 'zh': '2月8日'}, 'hash': '6f559083c224138c', 'authors': ['Sukmin Cho', 'Sangjin Choi', 'Taeho Hwang', 'Jeongyeon Seo', 'Soyeong Jeong', 'Huije Lee', 'Hoyun Song', 'Jong C. Park', 'Youngjin Kwon'], 'affiliations': ['School of Computing, Graduate School of AI, Korea Advanced Institute of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.05609.jpg', 'data': {'categories': ['#training', '#architecture', '#inference', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Иерархическая черновая генерация: новый подход к ускорению вывода в LLM', 'desc': 'Статья представляет новый метод ускорения вывода в больших языковых моделях (LLM) под названием Hierarchy Drafting (HD). HD организует различные источники токенов в иерархические базы данных, основываясь на временной локальности. Метод последовательно обращается к базам данных для получения черновых токенов, обеспечивая стабильное ускорение на различных задачах. Эксперименты показали, что HD превосходит существующие методы черновой генерации, демонстрируя надежное ускорение вывода для моделей разного размера, задач и температур.'}, 'en': {'title': 'Boosting Inference Speed with Hierarchy Drafting in LLMs', 'desc': 'This paper focuses on improving the speed of inference in Large Language Models (LLMs) for real-time applications. It introduces a new method called Hierarchy Drafting (HD), which organizes token sources into a hierarchical structure to enhance the drafting process. By accessing these token databases based on their temporal locality, HD ensures faster and more consistent token generation across various tasks. Experimental results show that HD significantly outperforms existing methods, providing robust speed improvements for LLMs of different sizes and tasks.'}, 'zh': {'title': '层次草拟：加速大型语言模型推理的新方法', 'desc': '加速大型语言模型（LLMs）的推理对于实时交互至关重要。本文提出了一种新的无损草拟方法，称为层次草拟（HD），它通过基于时间局部性的层次框架组织多种令牌源。HD在草拟步骤中依次访问多个数据库，从最高到最低的局部性获取草拟令牌，从而确保在不同任务中一致的加速效果。实验结果表明，HD在推理速度上优于现有的数据库草拟方法，适用于不同规模的模型和任务。'}}}, {'id': 'https://huggingface.co/papers/2502.05415', 'title': 'Show-o Turbo: Towards Accelerated Unified Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2502.05415', 'abstract': 'There has been increasing research interest in building unified multimodal understanding and generation models, among which Show-o stands as a notable representative, demonstrating great promise for both text-to-image and image-to-text generation. The inference of Show-o involves progressively denoising image tokens and autoregressively decoding text tokens, and hence, unfortunately, suffers from inefficiency issues from both sides. This paper introduces Show-o Turbo to bridge the gap. We first identify a unified denoising perspective for the generation of images and text in Show-o based on the parallel decoding of text tokens. We then propose to extend consistency distillation (CD), a qualified approach for shortening the denoising process of diffusion models, to the multimodal denoising trajectories of Show-o. We introduce a trajectory segmentation strategy and a curriculum learning procedure to improve the training convergence. Empirically, in text-to-image generation, Show-o Turbo displays a GenEval score of 0.625 at 4 sampling steps without using classifier-free guidance (CFG), outperforming that of the original Show-o with 8 steps and CFG; in image-to-text generation, Show-o Turbo exhibits a 1.5x speedup without significantly sacrificing performance. The code is available at https://github.com/zhijie-group/Show-o-Turbo.', 'score': 11, 'issue_id': 2144, 'pub_date': '2025-02-08', 'pub_date_card': {'ru': '8 февраля', 'en': 'February 8', 'zh': '2月8日'}, 'hash': '521adeebda96668f', 'authors': ['Chenkai Xu', 'Xu Wang', 'Zhenyi Liao', 'Yishun Li', 'Tianqi Hou', 'Zhijie Deng'], 'affiliations': ['Huawei', 'Shanghai Jiao Tong University', 'Tongji University'], 'pdf_title_img': 'assets/pdf/title_img/2502.05415.jpg', 'data': {'categories': ['#training', '#optimization', '#multimodal', '#diffusion'], 'emoji': '🚀', 'ru': {'title': 'Show-o Turbo: Ускоренная мультимодальная генерация без потери качества', 'desc': 'Статья представляет Show-o Turbo - улучшенную версию мультимодальной модели Show-o для генерации изображений по тексту и текста по изображениям. Авторы предлагают унифицированный подход к шумоподавлению для обоих типов генерации, основанный на параллельном декодировании текстовых токенов. Они применяют метод consistency distillation для ускорения процесса шумоподавления, а также вводят стратегию сегментации траектории и процедуру курикулярного обучения. Эксперименты показывают, что Show-o Turbo превосходит оригинальную модель по скорости и качеству генерации.'}, 'en': {'title': 'Show-o Turbo: Accelerating Multimodal Generation with Unified Denoising', 'desc': 'This paper presents Show-o Turbo, an advanced model for multimodal understanding and generation that improves upon the original Show-o framework. It addresses inefficiencies in the generation process by introducing a unified denoising approach that allows for parallel decoding of text tokens. The authors enhance the training process using consistency distillation and a new trajectory segmentation strategy, which leads to faster convergence. Empirical results show that Show-o Turbo achieves better performance in both text-to-image and image-to-text tasks, significantly reducing the number of sampling steps required for generation.'}, 'zh': {'title': '提升多模态生成效率的Show-o Turbo', 'desc': '本论文介绍了一种新的多模态生成模型Show-o Turbo，旨在提高文本到图像和图像到文本生成的效率。通过并行解码文本标记，Show-o Turbo采用统一的去噪视角，缩短了去噪过程。我们还引入了一种轨迹分割策略和课程学习程序，以改善训练收敛性。实验结果表明，Show-o Turbo在生成图像时的效率显著提高，同时在生成文本时也实现了1.5倍的速度提升。'}}}, {'id': 'https://huggingface.co/papers/2502.06060', 'title': 'Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning', 'url': 'https://huggingface.co/papers/2502.06060', 'abstract': "Communicating in natural language is a powerful tool in multi-agent settings, as it enables independent agents to share information in partially observable settings and allows zero-shot coordination with humans. However, most prior works are limited as they either rely on training with large amounts of human demonstrations or lack the ability to generate natural and useful communication strategies. In this work, we train language models to have productive discussions about their environment in natural language without any human demonstrations. We decompose the communication problem into listening and speaking. Our key idea is to leverage the agent's goal to predict useful information about the world as a dense reward signal that guides communication. Specifically, we improve a model's listening skills by training them to predict information about the environment based on discussions, and we simultaneously improve a model's speaking skills with multi-agent reinforcement learning by rewarding messages based on their influence on other agents. To investigate the role and necessity of communication in complex social settings, we study an embodied social deduction game based on Among Us, where the key question to answer is the identity of an adversarial imposter. We analyze emergent behaviors due to our technique, such as accusing suspects and providing evidence, and find that it enables strong discussions, doubling the win rates compared to standard RL. We release our code and models at https://socialdeductionllm.github.io/", 'score': 10, 'issue_id': 2146, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': 'd235746154e72f16', 'authors': ['Bidipta Sarkar', 'Warren Xia', 'C. Karen Liu', 'Dorsa Sadigh'], 'affiliations': ['Stanford University, Stanford, United States of America'], 'pdf_title_img': 'assets/pdf/title_img/2502.06060.jpg', 'data': {'categories': ['#alignment', '#games', '#rlhf', '#agents', '#open_source', '#rl'], 'emoji': '🕵️', 'ru': {'title': 'Естественный язык как инструмент координации ИИ-агентов', 'desc': 'В статье представлен метод обучения языковых моделей вести продуктивные дискуссии в естественной среде без использования демонстраций от людей. Авторы разделяют проблему коммуникации на навыки слушания и говорения, используя цель агента для предсказания полезной информации об окружающей среде в качестве сигнала награды. Метод применяется к социальной игре на дедукцию, основанной на Among Us, где ключевой вопрос - определение личности противника. Результаты показывают, что техника позволяет вести эффективные обсуждения, удваивая показатели выигрыша по сравнению со стандартным обучением с подкреплением.'}, 'en': {'title': 'Empowering Agents with Natural Language Communication for Enhanced Coordination', 'desc': "This paper explores how language models can be trained to communicate effectively in multi-agent environments without relying on human demonstrations. The authors break down communication into two parts: listening and speaking, using the agents' goals as a reward signal to enhance their communication skills. By applying multi-agent reinforcement learning, they improve how agents generate and interpret messages, leading to more productive discussions. The study demonstrates that these enhanced communication strategies significantly increase success rates in a social deduction game, showcasing the importance of effective communication in complex scenarios."}, 'zh': {'title': '自然语言沟通提升多智能体协作', 'desc': '本研究探讨了在多智能体环境中，如何通过自然语言进行有效沟通。我们提出了一种方法，训练语言模型在没有人类示范的情况下，进行关于环境的讨论。通过将沟通问题分解为倾听和发言，我们利用智能体的目标来预测有用的信息，从而引导沟通。实验表明，这种方法在复杂社交场景中显著提高了智能体的胜率，促进了更强的讨论能力。'}}}, {'id': 'https://huggingface.co/papers/2502.06772', 'title': 'ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates', 'url': 'https://huggingface.co/papers/2502.06772', 'abstract': 'We present that hierarchical LLM reasoning via scaling thought templates can effectively optimize the reasoning search space and outperform the mathematical reasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3. We train our ReasonFlux-32B model with only 8 GPUs and introduces three innovations: (i) a structured and generic thought template library, containing around 500 high-level thought templates capable of generalizing to similar or relevant reasoning problems; (ii) performing hierarchical reinforcement learning on a sequence of thought templates instead of long CoTs, optimizing a base LLM to plan out an optimal template trajectory for gradually handling complex problems; (iii) a brand new inference scaling system that enables hierarchical LLM reasoning by adaptively scaling thought templates at inference time. With a template trajectory containing sequential thought templates, our ReasonFlux-32B significantly advances math reasoning capabilities to state-of-the-art levels. Notably, on the MATH benchmark, it achieves an accuracy of 91.2% and surpasses o1-preview by 6.7%. On the USA Math Olympiad (AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems, surpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code: https://github.com/Gen-Verse/ReasonFlux', 'score': 9, 'issue_id': 2141, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '1ac59597c9610fb2', 'authors': ['Ling Yang', 'Zhaochen Yu', 'Bin Cui', 'Mengdi Wang'], 'affiliations': ['Peking University', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06772.jpg', 'data': {'categories': ['#rl', '#optimization', '#training', '#reasoning', '#math', '#benchmark', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Революция в математическом мышлении ИИ: иерархические рассуждения на новом уровне', 'desc': 'Представлена модель ReasonFlux-32B, использующая иерархическое рассуждение с масштабированием шаблонов мышления для оптимизации пространства поиска решений. Модель превосходит математические способности мощных языковых моделей, таких как OpenAI o1-preview и DeepSeek V3. ReasonFlux-32B использует структурированную библиотеку шаблонов мышления и иерархическое обучение с подкреплением для планирования оптимальной траектории шаблонов. На бенчмарке MATH модель достигает точности 91.2%, превосходя o1-preview на 6.7%.'}, 'en': {'title': 'Revolutionizing Math Reasoning with Hierarchical Thought Templates', 'desc': 'This paper introduces ReasonFlux-32B, a model that enhances mathematical reasoning in large language models (LLMs) by using hierarchical reasoning with thought templates. It features a library of 500 structured thought templates that help generalize reasoning across similar problems. The model employs hierarchical reinforcement learning to optimize the sequence of thought templates, allowing it to tackle complex problems more effectively. With these innovations, ReasonFlux-32B achieves state-of-the-art performance on math benchmarks, significantly outperforming existing models like OpenAI o1-preview and DeepSeek V3.'}, 'zh': {'title': '层次化推理，数学能力新突破', 'desc': '本文提出通过扩展思维模板的层次化大语言模型（LLM）推理，可以有效优化推理搜索空间，并超越强大的LLM如OpenAI o1-preview和DeepSeek V3的数学推理能力。我们训练的ReasonFlux-32B模型仅使用8个GPU，并引入了三项创新：一是构建了一个包含约500个高层次思维模板的结构化通用模板库，能够推广到类似的推理问题；二是对思维模板序列进行层次化强化学习，而不是长链的思维（CoTs），优化基础LLM以规划出处理复杂问题的最佳模板轨迹；三是全新的推理扩展系统，通过在推理时自适应扩展思维模板，实现层次化LLM推理。通过包含顺序思维模板的模板轨迹，ReasonFlux-32B在数学推理能力上显著提升，达到了最先进的水平。'}}}, {'id': 'https://huggingface.co/papers/2502.03628', 'title': 'The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering', 'url': 'https://huggingface.co/papers/2502.03628', 'abstract': 'Large Vision-Language Models (LVLMs) can reason effectively over both textual and visual inputs, but they tend to hallucinate syntactically coherent yet visually ungrounded contents. In this paper, we investigate the internal dynamics of hallucination by examining the tokens logits rankings throughout the generation process, revealing three key patterns in how LVLMs process information: (1) gradual visual information loss -- visually grounded tokens gradually become less favored throughout generation, and (2) early excitation -- semantically meaningful tokens achieve peak activation in the layers earlier than the final layer. (3) hidden genuine information -- visually grounded tokens though not being eventually decided still retain relatively high rankings at inference. Based on these insights, we propose VISTA (Visual Information Steering with Token-logit Augmentation), a training-free inference-time intervention framework that reduces hallucination while promoting genuine information. VISTA works by combining two complementary approaches: reinforcing visual information in activation space and leveraging early layer activations to promote semantically meaningful decoding. Compared to existing methods, VISTA requires no external supervision and is applicable to various decoding strategies. Extensive experiments show that VISTA on average reduces hallucination by abount 40% on evaluated open-ended generation task, and it consistently outperforms existing methods on four benchmarks across four architectures under three decoding strategies.', 'score': 9, 'issue_id': 2140, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '04b182d80abf9219', 'authors': ['Zhuowei Li', 'Haizhou Shi', 'Yunhe Gao', 'Di Liu', 'Zhenting Wang', 'Yuxiao Chen', 'Ting Liu', 'Long Zhao', 'Hao Wang', 'Dimitris N. Metaxas'], 'affiliations': ['Google DeepMind', 'Rutgers University', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2502.03628.jpg', 'data': {'categories': ['#cv', '#training', '#hallucinations', '#benchmark', '#inference', '#interpretability', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Борьба с галлюцинациями в визуально-языковых моделях: метод VISTA', 'desc': 'Эта статья исследует проблему галлюцинаций в крупных визуально-языковых моделях (LVLM) при обработке текстовых и визуальных входных данных. Авторы анализируют внутренние механизмы возникновения галлюцинаций, изучая ранжирование логитов токенов в процессе генерации. На основе выявленных закономерностей предлагается метод VISTA для уменьшения галлюцинаций и усиления достоверной информации во время вывода. Эксперименты показывают, что VISTA в среднем снижает уровень галлюцинаций на 40% в задачах открытой генерации и превосходит существующие методы на четырех бенчмарках.'}, 'en': {'title': 'VISTA: Reducing Hallucination in Vision-Language Models', 'desc': 'This paper explores the issue of hallucination in Large Vision-Language Models (LVLMs), where the models generate plausible text that does not correspond to visual inputs. The authors identify three patterns in the generation process: a gradual loss of visual information, early activation of semantically meaningful tokens, and the presence of high-ranking visually grounded tokens that are not ultimately chosen. To address these issues, they introduce VISTA, a framework that enhances visual information during inference without requiring additional training. VISTA effectively reduces hallucination by about 40% and outperforms existing methods across multiple benchmarks and architectures.'}, 'zh': {'title': '减少幻觉，提升真实信息的VISTA框架', 'desc': '大型视觉语言模型（LVLMs）能够有效地处理文本和视觉输入，但它们往往会产生语法上连贯但视觉上不真实的内容。本文研究了幻觉的内部动态，发现LVLMs在生成过程中处理信息的三种关键模式：逐渐丧失视觉信息、早期激活和隐藏的真实信息。基于这些发现，我们提出了VISTA（视觉信息引导与标记逻辑增强），这是一种无需训练的推理时干预框架，旨在减少幻觉并促进真实信息的生成。实验表明，VISTA在开放式生成任务中平均减少了约40%的幻觉，并在四个基准测试中在三种解码策略下始终优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2502.06786', 'title': 'Matryoshka Quantization', 'url': 'https://huggingface.co/papers/2502.06786', 'abstract': 'Quantizing model weights is critical for reducing the communication and inference costs of large models. However, quantizing models -- especially to low precisions like int4 or int2 -- requires a trade-off in model quality; int2, in particular, is known to severely degrade model quality. Consequently, practitioners are often forced to maintain multiple models with different quantization levels or serve a single model that best satisfies the quality-latency trade-off. On the other hand, integer data types, such as int8, inherently possess a nested (Matryoshka) structure where smaller bit-width integers, like int4 or int2, are nested within the most significant bits. This paper proposes Matryoshka Quantization (MatQuant), a novel multi-scale quantization technique that addresses the challenge of needing multiple quantized models. It allows training and maintaining just one model, which can then be served at different precision levels. Furthermore, due to the co-training and co-distillation regularization provided by MatQuant, the int2 precision models extracted by MatQuant can be up to 10% more accurate than standard int2 quantization (using techniques like QAT or OmniQuant). This represents significant progress in model quantization, demonstrated by the fact that, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more accurate than an int8 FFN-quantized Gemma-2 2B model.', 'score': 8, 'issue_id': 2143, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '1126a5fe83c7422d', 'authors': ['Pranav Nair', 'Puranjay Datta', 'Jeff Dean', 'Prateek Jain', 'Aditya Kusupati'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2502.06786.jpg', 'data': {'categories': ['#training', '#optimization', '#inference'], 'emoji': '🪆', 'ru': {'title': 'MatQuant: Одна модель - множество уровней точности', 'desc': 'MatQuant - это новый метод многомасштабной квантизации для моделей машинного обучения. Он позволяет обучать и обслуживать одну модель, которую затем можно использовать с разными уровнями точности. Благодаря совместному обучению и ко-дистилляции, модели int2, полученные с помощью MatQuant, могут быть до 10% точнее, чем при стандартной квантизации int2. Этот метод значительно улучшает квантизацию моделей, что демонстрируется тем, что модель Gemma-2 9B с квантизацией FFN до int2 оказывается точнее, чем модель Gemma-2 2B с квантизацией FFN до int8.'}, 'en': {'title': 'One Model, Multiple Precision: Revolutionizing Quantization with MatQuant', 'desc': 'This paper introduces Matryoshka Quantization (MatQuant), a new technique for quantizing model weights that allows for multiple precision levels without sacrificing model quality. Traditional low-precision quantization, especially to int2, often leads to significant degradation in performance, forcing practitioners to manage several models. MatQuant leverages the nested structure of integer data types to enable a single model to be trained and served at various precision levels. The method also enhances the accuracy of int2 models by up to 10% compared to standard quantization techniques, showcasing its effectiveness in reducing the need for multiple quantized models.'}, 'zh': {'title': 'Matryoshka量化：单模型多精度服务的创新', 'desc': '量化模型权重对于减少大型模型的通信和推理成本至关重要。然而，将模型量化到低精度（如int4或int2）时，模型质量会受到影响，尤其是int2会显著降低模型性能。为了解决这个问题，本文提出了一种新的多尺度量化技术——Matryoshka量化（MatQuant），它允许只训练和维护一个模型，并在不同精度级别下进行服务。通过MatQuant的共同训练和共同蒸馏正则化，提取的int2精度模型的准确性比标准的int2量化高出10%。'}}}, {'id': 'https://huggingface.co/papers/2502.06788', 'title': 'EVEv2: Improved Baselines for Encoder-Free Vision-Language Models', 'url': 'https://huggingface.co/papers/2502.06788', 'abstract': 'Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient deployment. We systematically clarify the performance gap between VLMs using pre-trained vision encoders, discrete tokenizers, and minimalist visual layers from scratch, deeply excavating the under-examined characteristics of encoder-free VLMs. We develop efficient strategies for encoder-free VLMs that rival mainstream encoder-based ones. After an in-depth investigation, we launch EVEv2.0, a new and improved family of encoder-free VLMs. We show that: (i) Properly decomposing and hierarchically associating vision and language within a unified model reduces interference between modalities. (ii) A well-designed training strategy enables effective optimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0 represents a thorough study for developing a decoder-only architecture across modalities, demonstrating superior data efficiency and strong vision-reasoning capability. Code is publicly available at: https://github.com/baaivision/EVE.', 'score': 8, 'issue_id': 2141, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '4374edb93ca102c6', 'authors': ['Haiwen Diao', 'Xiaotong Li', 'Yufeng Cui', 'Yueze Wang', 'Haoge Deng', 'Ting Pan', 'Wenxuan Wang', 'Huchuan Lu', 'Xinlong Wang'], 'affiliations': ['BAAI', 'BUPT', 'CASIA', 'DLUT', 'PKU', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2502.06788.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture', '#agi', '#multimodal'], 'emoji': '🔬', 'ru': {'title': 'Революция в мультимодальном обучении: EVEv2.0 - эффективность без энкодеров', 'desc': 'Статья представляет новое семейство моделей машинного обучения EVEv2.0, работающих с изображениями и текстом без использования энкодеров. Авторы систематически исследуют разрыв в производительности между моделями с энкодерами и без них, разрабатывая эффективные стратегии для последних. Они демонстрируют, что правильное разложение и иерархическая ассоциация зрения и языка в единой модели снижает интерференцию между модальностями. EVEv2.0 показывает превосходную эффективность использования данных и сильные способности к визуальному рассуждению.'}, 'en': {'title': 'EVEv2.0: Bridging the Gap in Vision-Language Models Without Encoders', 'desc': 'This paper discusses the advancements in encoder-free vision-language models (VLMs) that are closing the performance gap with traditional encoder-based models. The authors explore the characteristics of these encoder-free VLMs and propose efficient strategies to enhance their performance. They introduce EVEv2.0, a new family of encoder-free VLMs that effectively integrates vision and language while minimizing interference. The study demonstrates that a well-structured training approach and hierarchical association of modalities lead to improved data efficiency and vision-reasoning capabilities.'}, 'zh': {'title': '无编码器VLM的潜力与创新', 'desc': '本论文探讨了无编码器的视觉-语言模型（VLMs）在性能上与基于编码器的模型之间的差距。我们系统性地分析了使用预训练视觉编码器和简约视觉层的无编码器VLMs的特性。通过开发高效的策略，我们推出了EVEv2.0，一个改进的无编码器VLM系列，展示了其在数据效率和视觉推理能力上的优势。我们的研究表明，合理分解和层次关联视觉与语言可以减少模态之间的干扰，并通过良好的训练策略实现有效优化。'}}}, {'id': 'https://huggingface.co/papers/2502.06049', 'title': 'LM2: Large Memory Models', 'url': 'https://huggingface.co/papers/2502.06049', 'abstract': 'This paper introduces the Large Memory Model (LM2), a decoder-only Transformer architecture enhanced with an auxiliary memory module that aims to address the limitations of standard Transformers in multi-step reasoning, relational argumentation, and synthesizing information distributed over long contexts. The proposed LM2 incorporates a memory module that acts as a contextual representation repository, interacting with input tokens via cross attention and updating through gating mechanisms. To preserve the Transformers general-purpose capabilities, LM2 maintains the original information flow while integrating a complementary memory pathway. Experimental results on the BABILong benchmark demonstrate that the LM2model outperforms both the memory-augmented RMT model by 37.1% and the baseline Llama-3.2 model by 86.3% on average across tasks. LM2 exhibits exceptional capabilities in multi-hop inference, numerical reasoning, and large-context question-answering. On the MMLU dataset, it achieves a 5.0% improvement over a pre-trained vanilla model, demonstrating that its memory module does not degrade performance on general tasks. Further, in our analysis, we explore the memory interpretability, effectiveness of memory modules, and test-time behavior. Our findings emphasize the importance of explicit memory in enhancing Transformer architectures.', 'score': 8, 'issue_id': 2140, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': '5f62d7e814a6918f', 'authors': ['Jikun Kang', 'Wenqi Wu', 'Filippos Christianos', 'Alex J. Chan', 'Fraser Greenlee', 'George Thomas', 'Marvin Purtorab', 'Andy Toulis'], 'affiliations': ['Convergence Labs Ltd.'], 'pdf_title_img': 'assets/pdf/title_img/2502.06049.jpg', 'data': {'categories': ['#dataset', '#architecture', '#benchmark', '#interpretability', '#long_context', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'LM2: Трансформер с памятью для улучшенных рассуждений', 'desc': 'В статье представлена модель Large Memory Model (LM2), архитектура декодер-трансформер с дополнительным модулем памяти. LM2 решает проблемы стандартных трансформеров в многошаговых рассуждениях и обработке длинных контекстов. Модель показала значительное улучшение производительности на бенчмарке BABILong по сравнению с базовыми моделями. LM2 также продемонстрировала улучшенные возможности в многоходовых выводах, числовых вычислениях и ответах на вопросы с большим контекстом.'}, 'en': {'title': 'Enhancing Transformers with Memory for Superior Reasoning', 'desc': 'The paper presents the Large Memory Model (LM2), a new type of Transformer designed to improve multi-step reasoning and information synthesis over long contexts. LM2 features an auxiliary memory module that stores contextual information and interacts with input data through cross attention, allowing it to update its memory dynamically. This model retains the original capabilities of Transformers while adding a memory pathway that enhances performance on complex tasks. Experimental results show that LM2 significantly outperforms existing models in reasoning tasks and maintains strong performance on general tasks, highlighting the value of integrating explicit memory into Transformer architectures.'}, 'zh': {'title': '大型记忆模型：提升Transformer推理能力的关键', 'desc': '本文介绍了一种名为大型记忆模型（LM2）的解码器仅Transformer架构，旨在解决标准Transformer在多步推理、关系论证和长上下文信息综合方面的局限性。LM2引入了一个辅助记忆模块，作为上下文表示的存储库，通过交叉注意力与输入标记交互，并通过门控机制进行更新。实验结果表明，LM2在BABILong基准测试中，平均性能比记忆增强的RMT模型提高了37.1%，比基线Llama-3.2模型提高了86.3%。LM2在多跳推理、数值推理和大上下文问答方面表现出色，证明了显式记忆在增强Transformer架构中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2502.06782', 'title': 'Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT', 'url': 'https://huggingface.co/papers/2502.06782', 'abstract': "Recent advancements have established Diffusion Transformers (DiTs) as a dominant framework in generative modeling. Building on this success, Lumina-Next achieves exceptional performance in the generation of photorealistic images with Next-DiT. However, its potential for video generation remains largely untapped, with significant challenges in modeling the spatiotemporal complexity inherent to video data. To address this, we introduce Lumina-Video, a framework that leverages the strengths of Next-DiT while introducing tailored solutions for video synthesis. Lumina-Video incorporates a Multi-scale Next-DiT architecture, which jointly learns multiple patchifications to enhance both efficiency and flexibility. By incorporating the motion score as an explicit condition, Lumina-Video also enables direct control of generated videos' dynamic degree. Combined with a progressive training scheme with increasingly higher resolution and FPS, and a multi-source training scheme with mixed natural and synthetic data, Lumina-Video achieves remarkable aesthetic quality and motion smoothness at high training and inference efficiency. We additionally propose Lumina-V2A, a video-to-audio model based on Next-DiT, to create synchronized sounds for generated videos. Codes are released at https://www.github.com/Alpha-VLLM/Lumina-Video.", 'score': 6, 'issue_id': 2143, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '3b903654a6ff6710', 'authors': ['Dongyang Liu', 'Shicheng Li', 'Yutong Liu', 'Zhen Li', 'Kai Wang', 'Xinyue Li', 'Qi Qin', 'Yufei Liu', 'Yi Xin', 'Zhongyu Li', 'Bin Fu', 'Chenyang Si', 'Yuewen Cao', 'Conghui He', 'Ziwei Liu', 'Yu Qiao', 'Qibin Hou', 'Hongsheng Li', 'Peng Gao'], 'affiliations': ['Nankai University', 'Shanghai Correspondence AI Laboratory', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.06782.jpg', 'data': {'categories': ['#video', '#architecture', '#synthetic', '#diffusion', '#audio', '#training'], 'emoji': '🎬', 'ru': {'title': 'Lumina-Video: новый уровень генерации видео с помощью диффузионных трансформеров', 'desc': 'Статья представляет Lumina-Video - новую архитектуру для генерации видео, основанную на Diffusion Transformers (DiT). Авторы предлагают мультимасштабную архитектуру Next-DiT, которая обучается на нескольких уровнях детализации одновременно. Модель использует условие движения для контроля динамики генерируемого видео. Благодаря прогрессивной схеме обучения и использованию смешанных данных, Lumina-Video достигает высокого качества и плавности движения при эффективном обучении и инференсе.'}, 'en': {'title': 'Revolutionizing Video Generation with Lumina-Video', 'desc': 'This paper introduces Lumina-Video, a new framework for generating videos using the strengths of Diffusion Transformers, specifically the Next-DiT model. It addresses the challenges of capturing the complex movements and changes in video data by using a Multi-scale Next-DiT architecture that learns from different segments of the video. The framework also incorporates a motion score to allow for better control over the dynamics of the generated videos. Additionally, Lumina-Video employs a progressive training approach to improve the quality and smoothness of the output while maintaining efficiency in both training and inference.'}, 'zh': {'title': 'Lumina-Video：高效生成视频的新框架', 'desc': '最近的研究表明，扩散变换器（DiTs）在生成建模中表现出色。基于这一成功，Lumina-Next在生成逼真图像方面取得了卓越的性能，但在视频生成方面仍面临挑战。为了解决这一问题，我们提出了Lumina-Video框架，它结合了Next-DiT的优势，并针对视频合成引入了定制化的解决方案。通过多尺度Next-DiT架构和运动评分的引入，Lumina-Video实现了高效、灵活的视频生成，并在训练和推理效率上表现出色。'}}}, {'id': 'https://huggingface.co/papers/2502.06764', 'title': 'History-Guided Video Diffusion', 'url': 'https://huggingface.co/papers/2502.06764', 'abstract': 'Classifier-free guidance (CFG) is a key technique for improving conditional generation in diffusion models, enabling more accurate control while enhancing sample quality. It is natural to extend this technique to video diffusion, which generates video conditioned on a variable number of context frames, collectively referred to as history. However, we find two key challenges to guiding with variable-length history: architectures that only support fixed-size conditioning, and the empirical observation that CFG-style history dropout performs poorly. To address this, we propose the Diffusion Forcing Transformer (DFoT), a video diffusion architecture and theoretically grounded training objective that jointly enable conditioning on a flexible number of history frames. We then introduce History Guidance, a family of guidance methods uniquely enabled by DFoT. We show that its simplest form, vanilla history guidance, already significantly improves video generation quality and temporal consistency. A more advanced method, history guidance across time and frequency further enhances motion dynamics, enables compositional generalization to out-of-distribution history, and can stably roll out extremely long videos. Website: https://boyuan.space/history-guidance', 'score': 5, 'issue_id': 2143, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '66644a3e757a5d21', 'authors': ['Kiwhan Song', 'Boyuan Chen', 'Max Simchowitz', 'Yilun Du', 'Russ Tedrake', 'Vincent Sitzmann'], 'affiliations': ['MIT'], 'pdf_title_img': 'assets/pdf/title_img/2502.06764.jpg', 'data': {'categories': ['#diffusion', '#training', '#video', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'Улучшение генерации видео с помощью гибкого обусловливания историей', 'desc': 'Статья представляет новый подход к улучшению генерации видео с помощью диффузионных моделей. Авторы предлагают архитектуру Diffusion Forcing Transformer (DFoT), которая позволяет использовать переменное количество кадров истории для обусловливания генерации. Они также вводят концепцию History Guidance - семейство методов, которые улучшают качество и временную согласованность генерируемого видео. Эксперименты показывают, что предложенный подход значительно улучшает динамику движения и позволяет генерировать очень длинные видео.'}, 'en': {'title': 'Enhancing Video Diffusion with Flexible History Guidance', 'desc': 'This paper introduces the Diffusion Forcing Transformer (DFoT), a novel architecture designed for video diffusion that allows for flexible conditioning on a variable number of context frames. The authors address challenges related to fixed-size conditioning architectures and the inefficacy of traditional classifier-free guidance (CFG) methods when applied to variable-length history. They propose History Guidance, a set of techniques that leverage DFoT to improve video generation quality and temporal consistency. The results demonstrate that these methods enhance motion dynamics and enable the generation of longer videos with better compositional generalization.'}, 'zh': {'title': '灵活历史引导，提升视频生成质量', 'desc': '本论文提出了一种新的视频扩散模型架构，称为Diffusion Forcing Transformer（DFoT），旨在解决在可变长度历史帧条件下进行视频生成的挑战。我们发现，传统的分类器无关引导（CFG）方法在处理可变长度历史时效果不佳，因此我们设计了新的引导方法，称为历史引导。DFoT允许灵活地使用历史帧进行条件生成，从而显著提高视频生成的质量和时间一致性。通过引入更高级的历史引导方法，我们进一步增强了运动动态，并实现了对超出分布历史的组合泛化。'}}}, {'id': 'https://huggingface.co/papers/2502.06527', 'title': 'CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers', 'url': 'https://huggingface.co/papers/2502.06527', 'abstract': 'Customized generation has achieved significant progress in image synthesis, yet personalized video generation remains challenging due to temporal inconsistencies and quality degradation. In this paper, we introduce CustomVideoX, an innovative framework leveraging the video diffusion transformer for personalized video generation from a reference image. CustomVideoX capitalizes on pre-trained video networks by exclusively training the LoRA parameters to extract reference features, ensuring both efficiency and adaptability. To facilitate seamless interaction between the reference image and video content, we propose 3D Reference Attention, which enables direct and simultaneous engagement of reference image features with all video frames across spatial and temporal dimensions. To mitigate the excessive influence of reference image features and textual guidance on generated video content during inference, we implement the Time-Aware Reference Attention Bias (TAB) strategy, dynamically modulating reference bias over different time steps. Additionally, we introduce the Entity Region-Aware Enhancement (ERAE) module, aligning highly activated regions of key entity tokens with reference feature injection by adjusting attention bias. To thoroughly evaluate personalized video generation, we establish a new benchmark, VideoBench, comprising over 50 objects and 100 prompts for extensive assessment. Experimental results show that CustomVideoX significantly outperforms existing methods in terms of video consistency and quality.', 'score': 5, 'issue_id': 2143, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '44b3a6931980556a', 'authors': ['D. She', 'Mushui Liu', 'Jingxuan Pang', 'Jin Wang', 'Zhen Yang', 'Wanggui He', 'Guanghao Zhang', 'Yi Wang', 'Qihan Huang', 'Haobin Tang', 'Yunlong Yu', 'Siming Fu'], 'affiliations': ['Hong Kong University of Science and Technology (Guangzhou)', 'University of Science and Technology of China', 'Zhejiang Univerisity'], 'pdf_title_img': 'assets/pdf/title_img/2502.06527.jpg', 'data': {'categories': ['#diffusion', '#video', '#benchmark', '#3d'], 'emoji': '🎬', 'ru': {'title': 'CustomVideoX: Персонализированная генерация видео нового уровня', 'desc': 'CustomVideoX - это инновационная система для персонализированной генерации видео на основе референсного изображения, использующая видео-диффузионный трансформер. Система применяет предобученные видеосети и обучает только параметры LoRA для извлечения признаков из референса, что обеспечивает эффективность и адаптивность. Предложенное 3D Reference Attention позволяет взаимодействовать признакам референсного изображения со всеми кадрами видео в пространственном и временном измерениях. Для улучшения качества генерации используются стратегии Time-Aware Reference Attention Bias и Entity Region-Aware Enhancement.'}, 'en': {'title': 'Revolutionizing Personalized Video Generation with CustomVideoX', 'desc': 'This paper presents CustomVideoX, a new framework designed for generating personalized videos from a reference image. It utilizes a video diffusion transformer and focuses on training LoRA parameters to efficiently extract features from the reference image. The framework introduces 3D Reference Attention to enhance the interaction between the reference image and video frames, addressing temporal inconsistencies. Additionally, it employs the Time-Aware Reference Attention Bias and the Entity Region-Aware Enhancement modules to improve video quality and consistency, validated by a new benchmark called VideoBench.'}, 'zh': {'title': '个性化视频生成的新突破', 'desc': '个性化视频生成在图像合成领域取得了显著进展，但由于时间不一致性和质量下降，仍然面临挑战。本文提出了CustomVideoX，一个创新框架，利用视频扩散变换器从参考图像生成个性化视频。CustomVideoX通过专门训练LoRA参数来提取参考特征，确保了效率和适应性。我们还提出了3D参考注意力机制，以便在空间和时间维度上直接和同时地将参考图像特征与所有视频帧进行交互。'}}}, {'id': 'https://huggingface.co/papers/2502.05431', 'title': 'APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding', 'url': 'https://huggingface.co/papers/2502.05431', 'abstract': "Context-augmented generation (CAG) techniques, including RAG and ICL, require the efficient combination of multiple contexts to generate responses to user queries. Directly inputting these contexts as a sequence introduces a considerable computational burden by re-encoding the combined selection of contexts for every request. To address this, we explore the promising potential of parallel encoding to independently pre-compute and cache each context's KV states. This approach enables the direct loading of cached states during inference while accommodating more contexts through position reuse across contexts. However, due to misalignments in attention distribution, directly applying parallel encoding results in a significant performance drop. To enable effective and efficient CAG, we propose Adaptive Parallel Encoding (APE), which brings shared prefix, attention temperature, and scaling factor to align the distribution of parallel encoding with sequential encoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98% and 93% sequential encoding performance using the same inputs while outperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales to many-shot CAG, effectively encoding hundreds of contexts in parallel. Efficiency evaluation shows that APE can achieve an end-to-end 4.5times speedup by reducing 28times prefilling time for a 128K-length context.", 'score': 5, 'issue_id': 2141, 'pub_date': '2025-02-08', 'pub_date_card': {'ru': '8 февраля', 'en': 'February 8', 'zh': '2月8日'}, 'hash': '7bc5b7aeb3716893', 'authors': ['Xinyu Yang', 'Tianqi Chen', 'Beidi Chen'], 'affiliations': ['Carnegie Mellon University', 'Nvidia'], 'pdf_title_img': 'assets/pdf/title_img/2502.05431.jpg', 'data': {'categories': ['#rag', '#optimization', '#inference', '#long_context'], 'emoji': '⚡', 'ru': {'title': 'Ускорение генерации с контекстом: адаптивное параллельное кодирование', 'desc': 'Статья представляет новый метод адаптивного параллельного кодирования (APE) для эффективной обработки множественных контекстов в задачах генерации с использованием контекста. APE позволяет предварительно вычислять и кэшировать KV-состояния каждого контекста независимо, что значительно ускоряет процесс обработки запросов. Метод решает проблему несоответствия распределения внимания при параллельном кодировании, используя общий префикс, температуру внимания и масштабирующий фактор. Эксперименты показывают, что APE сохраняет до 98% производительности последовательного кодирования, превосходя обычное параллельное кодирование на 3.6-7.9% в задачах RAG и ICL.'}, 'en': {'title': 'Boosting Efficiency in Context-Augmented Generation with APE', 'desc': 'This paper introduces Adaptive Parallel Encoding (APE) as a solution to improve the efficiency of context-augmented generation (CAG) techniques like RAG and ICL. Traditional methods face high computational costs when combining multiple contexts for generating responses, as they require re-encoding for each request. APE allows for the pre-computation and caching of key-value (KV) states for each context, which can then be loaded during inference, significantly speeding up the process. The proposed method aligns the attention distribution of parallel encoding with that of sequential encoding, achieving high performance while handling many contexts efficiently.'}, 'zh': {'title': '自适应并行编码：提升上下文生成效率的关键', 'desc': '本文探讨了上下文增强生成（CAG）技术中的并行编码方法，以提高生成用户查询响应的效率。传统方法在每次请求时都需要重新编码多个上下文，导致计算负担过重。我们提出了自适应并行编码（APE），通过共享前缀、注意力温度和缩放因子来调整并行编码与顺序编码的注意力分布，从而提高性能。实验结果表明，APE在保持高性能的同时，能够显著加快处理速度，适用于处理大量上下文。'}}}, {'id': 'https://huggingface.co/papers/2502.06155', 'title': 'Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile', 'url': 'https://huggingface.co/papers/2502.06155', 'abstract': 'Despite the promise of synthesizing high-fidelity videos, Diffusion Transformers (DiTs) with 3D full attention suffer from expensive inference due to the complexity of attention computation and numerous sampling steps. For example, the popular Open-Sora-Plan model consumes more than 9 minutes for generating a single video of 29 frames. This paper addresses the inefficiency issue from two aspects: 1) Prune the 3D full attention based on the redundancy within video data; We identify a prevalent tile-style repetitive pattern in the 3D attention maps for video data, and advocate a new family of sparse 3D attention that holds a linear complexity w.r.t. the number of video frames. 2) Shorten the sampling process by adopting existing multi-step consistency distillation; We split the entire sampling trajectory into several segments and perform consistency distillation within each one to activate few-step generation capacities. We further devise a three-stage training pipeline to conjoin the low-complexity attention and few-step generation capacities. Notably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into an efficient one that is 7.4x -7.8x faster for 29 and 93 frames 720p video generation with a marginal performance trade-off in VBench. In addition, we demonstrate that our approach is amenable to distributed inference, achieving an additional 3.91x speedup when running on 4 GPUs with sequence parallelism.', 'score': 5, 'issue_id': 2140, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '2f8d5e54db328d39', 'authors': ['Hangliang Ding', 'Dacheng Li', 'Runlong Su', 'Peiyuan Zhang', 'Zhijie Deng', 'Ion Stoica', 'Hao Zhang'], 'affiliations': ['Shanghai Jiao Tong University', 'Tsinghua University', 'University of California, Berkeley', 'University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2502.06155.jpg', 'data': {'categories': ['#training', '#diffusion', '#optimization', '#inference', '#video'], 'emoji': '🎬', 'ru': {'title': 'Ускорение генерации видео: эффективность без потери качества', 'desc': 'Статья представляет новый подход к ускорению генерации видео с помощью Диффузионных Трансформеров (DiTs). Авторы предлагают прореживание 3D-внимания на основе избыточности видеоданных и сокращение процесса сэмплирования с помощью многошаговой дистилляции согласованности. Разработан трехэтапный процесс обучения для объединения внимания с низкой сложностью и возможностей генерации за несколько шагов. Результаты показывают ускорение в 7.4-7.8 раз для генерации видео 720p с 29 и 93 кадрами при использовании всего 0.1% данных предобучения.'}, 'en': {'title': 'Speeding Up Video Generation with Efficient Attention Mechanisms', 'desc': 'This paper presents a solution to the inefficiency of Diffusion Transformers (DiTs) in generating high-fidelity videos. It introduces a method to prune 3D full attention by recognizing repetitive patterns in video data, leading to a sparse attention mechanism that reduces computational complexity. Additionally, the authors propose a multi-step consistency distillation approach to shorten the sampling process, allowing for faster video generation. The resulting model, Open-Sora-Plan-1.2, achieves significant speed improvements while maintaining performance, especially when utilizing distributed inference across multiple GPUs.'}, 'zh': {'title': '高效视频生成的新方法', 'desc': '本论文提出了一种改进的Diffusion Transformers（DiTs）模型，以解决生成高保真视频时的效率问题。我们通过识别视频数据中的冗余，提出了一种稀疏的3D注意力机制，使其在视频帧数量上具有线性复杂度。其次，我们采用多步一致性蒸馏技术，缩短了采样过程，从而实现了更快速的视频生成。最终，我们的模型在使用极少的预训练数据时，生成速度提高了7.4到7.8倍，同时保持了良好的性能。'}}}, {'id': 'https://huggingface.co/papers/2502.05957', 'title': 'MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents', 'url': 'https://huggingface.co/papers/2502.05957', 'abstract': "Large Language Model (LLM) Agents have demonstrated remarkable capabilities in task automation and intelligent decision-making, driving the widespread adoption of agent development frameworks such as LangChain and AutoGen. However, these frameworks predominantly serve developers with extensive technical expertise - a significant limitation considering that only 0.03 % of the global population possesses the necessary programming skills. This stark accessibility gap raises a fundamental question: Can we enable everyone, regardless of technical background, to build their own LLM agents using natural language alone? To address this challenge, we introduce MetaChain-a Fully-Automated and highly Self-Developing framework that enables users to create and deploy LLM agents through Natural Language Alone. Operating as an autonomous Agent Operating System, MetaChain comprises four key components: i) Agentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing File System, and iv) Self-Play Agent Customization module. This lightweight yet powerful system enables efficient and dynamic creation and modification of tools, agents, and workflows without coding requirements or manual intervention. Beyond its code-free agent development capabilities, MetaChain also serves as a versatile multi-agent system for General AI Assistants. Comprehensive evaluations on the GAIA benchmark demonstrate MetaChain's effectiveness in generalist multi-agent tasks, surpassing existing state-of-the-art methods. Furthermore, MetaChain's Retrieval-Augmented Generation (RAG)-related capabilities have shown consistently superior performance compared to many alternative LLM-based solutions.", 'score': 4, 'issue_id': 2144, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': 'f3a18de353dcfad8', 'authors': ['Jiabin Tang', 'Tianyu Fan', 'Chao Huang'], 'affiliations': ['The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.05957.jpg', 'data': {'categories': ['#rag', '#games', '#agents', '#benchmark', '#optimization', '#agi'], 'emoji': '🤖', 'ru': {'title': 'MetaChain: ИИ-агенты для всех без кода', 'desc': 'MetaChain - это полностью автоматизированная и самоуправляемая система для создания агентов на основе больших языковых моделей (LLM) без необходимости программирования. Она состоит из четырех ключевых компонентов: агентных системных утилит, движка действий на основе LLM, самоуправляемой файловой системы и модуля самонастройки агентов. MetaChain позволяет эффективно создавать и модифицировать инструменты, агенты и рабочие процессы без кодирования. Система показала превосходные результаты в задачах многоагентного взаимодействия и генерации с извлечением информации (RAG).'}, 'en': {'title': 'Empowering Everyone to Build LLM Agents with MetaChain', 'desc': 'This paper introduces MetaChain, a framework designed to allow users to create and deploy Large Language Model (LLM) agents using only natural language, eliminating the need for programming skills. MetaChain operates as an autonomous Agent Operating System, featuring components like an Actionable Engine and a Self-Managing File System to facilitate dynamic agent development. The framework addresses the accessibility gap in LLM agent creation, enabling a broader audience to leverage AI technology. Evaluations on the GAIA benchmark indicate that MetaChain outperforms existing methods in multi-agent tasks and demonstrates superior capabilities in Retrieval-Augmented Generation (RAG).'}, 'zh': {'title': '让每个人都能用自然语言构建智能代理', 'desc': '大型语言模型（LLM）代理在任务自动化和智能决策方面表现出色，但现有的开发框架主要面向技术背景深厚的开发者，限制了普通用户的使用。为了解决这一问题，我们提出了MetaChain，一个完全自动化且高度自我发展的框架，允许用户仅通过自然语言创建和部署LLM代理。MetaChain作为一个自主代理操作系统，包含四个关键组件，能够高效动态地创建和修改工具、代理和工作流程，而无需编写代码。经过GAIA基准的全面评估，MetaChain在通用多代理任务中表现优于现有的最先进方法，展现了其强大的能力。'}}}, {'id': 'https://huggingface.co/papers/2502.06635', 'title': 'Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM', 'url': 'https://huggingface.co/papers/2502.06635', 'abstract': "Steel-LLM is a Chinese-centric language model developed from scratch with the goal of creating a high-quality, open-source model despite limited computational resources. Launched in March 2024, the project aimed to train a 1-billion-parameter model on a large-scale dataset, prioritizing transparency and the sharing of practical insights to assist others in the community. The training process primarily focused on Chinese data, with a small proportion of English data included, addressing gaps in existing open-source LLMs by providing a more detailed and practical account of the model-building journey. Steel-LLM has demonstrated competitive performance on benchmarks such as CEVAL and CMMLU, outperforming early models from larger institutions. This paper provides a comprehensive summary of the project's key contributions, including data collection, model design, training methodologies, and the challenges encountered along the way, offering a valuable resource for researchers and practitioners looking to develop their own LLMs. The model checkpoints and training script are available at https://github.com/zhanshijinwat/Steel-LLM.", 'score': 4, 'issue_id': 2141, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '2deb5075264d7660', 'authors': ['Qingshui Gu', 'Shu Li', 'Tianyu Zheng', 'Zhaoxiang Zhang'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'Institute of Automation, Chinese Academy of Sciences', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06635.jpg', 'data': {'categories': ['#small_models', '#multilingual', '#training', '#data', '#benchmark', '#open_source', '#dataset', '#low_resource'], 'emoji': '🇨🇳', 'ru': {'title': 'Создание эффективной китайскоязычной LLM с открытым исходным кодом', 'desc': 'Steel-LLM - это языковая модель, ориентированная на китайский язык, разработанная с нуля при ограниченных вычислительных ресурсах. Модель с 1 миллиардом параметров была обучена на крупномасштабном наборе данных, в основном на китайском языке. Steel-LLM показала конкурентоспособную производительность на бенчмарках CEVAL и CMMLU, превзойдя ранние модели от более крупных институтов. Статья предоставляет подробный отчет о процессе разработки, включая сбор данных, дизайн модели и методологии обучения.'}, 'en': {'title': 'Empowering Chinese NLP with Steel-LLM: Open-Source Innovation', 'desc': "Steel-LLM is a language model specifically designed for the Chinese language, built from the ground up to be open-source and accessible. It features 1 billion parameters and was trained on a large dataset primarily consisting of Chinese text, with some English data to fill existing gaps. The model has shown strong performance on various benchmarks, surpassing earlier models from larger organizations. This paper details the project's contributions, including data collection, model architecture, training techniques, and the challenges faced, serving as a guide for others in the field of language model development."}, 'zh': {'title': '打造中文优质开源语言模型的探索', 'desc': 'Steel-LLM是一个以中文为中心的语言模型，旨在在有限的计算资源下开发出高质量的开源模型。该项目于2024年3月启动，训练了一个拥有10亿参数的大规模模型，重点关注透明度和实用见解的分享。训练过程中主要使用中文数据，并适量包含英文数据，填补了现有开源大语言模型的空白。Steel-LLM在CEVAL和CMMLU等基准测试中表现出色，超越了大型机构的早期模型，为研究人员和实践者提供了宝贵的资源。'}}}, {'id': 'https://huggingface.co/papers/2502.05795', 'title': 'The Curse of Depth in Large Language Models', 'url': 'https://huggingface.co/papers/2502.05795', 'abstract': 'In this paper, we introduce the Curse of Depth, a concept that highlights, explains, and addresses the recent observation in modern Large Language Models(LLMs) where nearly half of the layers are less effective than expected. We first confirm the wide existence of this phenomenon across the most popular families of LLMs such as Llama, Mistral, DeepSeek, and Qwen. Our analysis, theoretically and empirically, identifies that the underlying reason for the ineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer Normalization (Pre-LN). While Pre-LN stabilizes the training of Transformer LLMs, its output variance exponentially grows with the model depth, which undesirably causes the derivative of the deep Transformer blocks to be an identity matrix, and therefore barely contributes to the training. To resolve this training pitfall, we propose LayerNorm Scaling, which scales the variance of output of the layer normalization inversely by the square root of its depth. This simple modification mitigates the output variance explosion of deeper Transformer layers, improving their contribution. Our experimental results, spanning model sizes from 130M to 1B, demonstrate that LayerNorm Scaling significantly enhances LLM pre-training performance compared to Pre-LN. Moreover, this improvement seamlessly carries over to supervised fine-tuning. All these gains can be attributed to the fact that LayerNorm Scaling enables deeper layers to contribute more effectively during training.', 'score': 3, 'issue_id': 2147, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': '3b1a3626926ac2f4', 'authors': ['Wenfang Sun', 'Xinyuan Song', 'Pengxiang Li', 'Lu Yin', 'Yefeng Zheng', 'Shiwei Liu'], 'affiliations': ['Dalian University of Technology, China', 'Emory University, USA', 'Medical Artificial Intelligence Laboratory, Westlake University, China', 'University of Surrey, UK'], 'pdf_title_img': 'assets/pdf/title_img/2502.05795.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training'], 'emoji': '🧠', 'ru': {'title': "Преодоление 'Проклятия глубины' в больших языковых моделях", 'desc': "В статье представлена концепция 'Проклятия глубины', объясняющая низкую эффективность почти половины слоев в современных больших языковых моделях (LLM). Авторы подтверждают широкое распространение этого явления среди популярных семейств LLM, таких как Llama, Mistral, DeepSeek и Qwen. Анализ показывает, что причиной неэффективности глубоких слоев является использование предварительной нормализации слоев (Pre-LN). Для решения этой проблемы предлагается метод масштабирования LayerNorm, который улучшает вклад глубоких слоев в обучение модели."}, 'en': {'title': 'Unlocking the Power of Deep Layers in LLMs', 'desc': "This paper introduces the 'Curse of Depth', which describes a problem in Large Language Models (LLMs) where many layers do not perform as well as expected. The authors find that this issue is common in popular LLMs like Llama and Mistral, and it stems from the use of Pre-Layer Normalization (Pre-LN). Pre-LN helps stabilize training but leads to increased output variance in deeper layers, making them less effective. To address this, the authors propose LayerNorm Scaling, which reduces the output variance of deeper layers, resulting in improved training performance and better contributions from these layers."}, 'zh': {'title': '解决深度模型的训练困境', 'desc': '本文介绍了深度诅咒的概念，强调了现代大型语言模型（LLMs）中近一半层的效果低于预期的现象。我们确认了这一现象在流行的LLM家族中普遍存在，如Llama、Mistral、DeepSeek和Qwen。分析表明，深层无效的根本原因是广泛使用的预层归一化（Pre-LN），它导致输出方差随着模型深度的增加而指数增长。为了解决这个问题，我们提出了层归一化缩放（LayerNorm Scaling），通过对层归一化的输出方差进行缩放，显著提高了深层的贡献。'}}}, {'id': 'https://huggingface.co/papers/2502.04370', 'title': 'DreamDPO: Aligning Text-to-3D Generation with Human Preferences via Direct Preference Optimization', 'url': 'https://huggingface.co/papers/2502.04370', 'abstract': 'Text-to-3D generation automates 3D content creation from textual descriptions, which offers transformative potential across various fields. However, existing methods often struggle to align generated content with human preferences, limiting their applicability and flexibility. To address these limitations, in this paper, we propose DreamDPO, an optimization-based framework that integrates human preferences into the 3D generation process, through direct preference optimization. Practically, DreamDPO first constructs pairwise examples, then compare their alignment with human preferences using reward or large multimodal models, and lastly optimizes the 3D representation with a preference-driven loss function. By leveraging pairwise comparison to reflect preferences, DreamDPO reduces reliance on precise pointwise quality evaluations while enabling fine-grained controllability through preference-guided optimization. Experiments demonstrate that DreamDPO achieves competitive results, and provides higher-quality and more controllable 3D content compared to existing methods. The code and models will be open-sourced.', 'score': 3, 'issue_id': 2145, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': 'a475f5281f318a1e', 'authors': ['Zhenglin Zhou', 'Xiaobo Xia', 'Fan Ma', 'Hehe Fan', 'Yi Yang', 'Tat-Seng Chua'], 'affiliations': ['National University of Singapore', 'Yale University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.04370.jpg', 'data': {'categories': ['#training', '#optimization', '#3d', '#alignment', '#open_source', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Создание 3D-контента с учетом человеческих предпочтений', 'desc': 'DreamDPO - это новый подход к генерации 3D-контента на основе текстовых описаний, который учитывает предпочтения человека. Метод использует попарное сравнение сгенерированных образцов для оценки их соответствия предпочтениям с помощью моделей вознаграждения или мультимодальных языковых моделей. DreamDPO оптимизирует 3D-представление с использованием функции потерь, ориентированной на предпочтения. Эксперименты показывают, что DreamDPO обеспечивает более качественный и контролируемый 3D-контент по сравнению с существующими методами.'}, 'en': {'title': 'DreamDPO: Aligning 3D Generation with Human Preferences', 'desc': 'This paper introduces DreamDPO, a new framework for generating 3D content from text that incorporates human preferences. It uses an optimization approach that focuses on pairwise comparisons to better align the generated 3D models with what people actually want. By employing a preference-driven loss function, DreamDPO enhances the quality and control of the generated content without needing exact quality scores. The results show that DreamDPO outperforms existing methods, making it a significant advancement in text-to-3D generation.'}, 'zh': {'title': 'DreamDPO：将人类偏好融入3D生成的创新框架', 'desc': '文本到3D生成技术可以根据文本描述自动创建3D内容，具有广泛的应用潜力。然而，现有方法在生成内容与人类偏好之间的对齐上存在困难，限制了其适用性和灵活性。为了解决这些问题，本文提出了DreamDPO，一个基于优化的框架，通过直接偏好优化将人类偏好融入3D生成过程中。实验表明，DreamDPO在生成高质量和可控的3D内容方面优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2502.06023', 'title': 'Dual Caption Preference Optimization for Diffusion Models', 'url': 'https://huggingface.co/papers/2502.06023', 'abstract': "Recent advancements in human preference optimization, originally developed for Large Language Models (LLMs), have shown significant potential in improving text-to-image diffusion models. These methods aim to learn the distribution of preferred samples while distinguishing them from less preferred ones. However, existing preference datasets often exhibit overlap between these distributions, leading to a conflict distribution. Additionally, we identified that input prompts contain irrelevant information for less preferred images, limiting the denoising network's ability to accurately predict noise in preference optimization methods, known as the irrelevant prompt issue. To address these challenges, we propose Dual Caption Preference Optimization (DCPO), a novel approach that utilizes two distinct captions to mitigate irrelevant prompts. To tackle conflict distribution, we introduce the Pick-Double Caption dataset, a modified version of Pick-a-Pic v2 with separate captions for preferred and less preferred images. We further propose three different strategies for generating distinct captions: captioning, perturbation, and hybrid methods. Our experiments show that DCPO significantly improves image quality and relevance to prompts, outperforming Stable Diffusion (SD) 2.1, SFT_Chosen, Diffusion-DPO, and MaPO across multiple metrics, including Pickscore, HPSv2.1, GenEval, CLIPscore, and ImageReward, fine-tuned on SD 2.1 as the backbone.", 'score': 3, 'issue_id': 2141, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': '07782353b3b8b697', 'authors': ['Amir Saeidi', 'Yiran Luo', 'Agneet Chatterjee', 'Shamanthak Hegde', 'Bimsara Pathiraja', 'Yezhou Yang', 'Chitta Baral'], 'affiliations': ['Arizona State University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06023.jpg', 'data': {'categories': ['#optimization', '#training', '#rlhf', '#dataset', '#diffusion', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Двойные подписи для улучшения генерации изображений по текстовым описаниям', 'desc': 'Эта статья описывает новый метод под названием Dual Caption Preference Optimization (DCPO) для улучшения моделей диффузии текст-в-изображение. DCPO использует два отдельных описания для решения проблемы нерелевантных промптов и конфликтующих распределений в наборах данных предпочтений. Авторы также представляют новый датасет Pick-Double Caption с отдельными подписями для предпочтительных и менее предпочтительных изображений. Эксперименты показывают, что DCPO значительно улучшает качество изображений и их соответствие промптам по сравнению с существующими методами.'}, 'en': {'title': 'Enhancing Image Generation with Dual Captions!', 'desc': 'This paper presents Dual Caption Preference Optimization (DCPO), a new method to enhance text-to-image diffusion models by addressing issues in human preference optimization. It identifies problems with existing preference datasets, such as overlapping distributions and irrelevant prompts that hinder the denoising process. To overcome these challenges, DCPO employs two distinct captions for preferred and less preferred images, utilizing a modified dataset called Pick-Double Caption. The results demonstrate that DCPO significantly improves image quality and relevance, outperforming several existing models across various evaluation metrics.'}, 'zh': {'title': '双重标题优化，提升图像质量！', 'desc': '最近在大型语言模型（LLMs）中发展的人类偏好优化技术，显示出在改进文本到图像扩散模型方面的巨大潜力。这些方法旨在学习偏好样本的分布，并将其与不太偏好的样本区分开来。然而，现有的偏好数据集通常存在分布重叠的问题，导致冲突分布。此外，我们发现输入提示中包含与不太偏好的图像无关的信息，这限制了去噪网络在偏好优化方法中的准确预测能力。为了解决这些挑战，我们提出了双重标题偏好优化（DCPO），利用两个不同的标题来减轻无关提示的问题。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (2)', '#agi (2)', '#alignment (2)', '#architecture (6)', '#audio (1)', '#benchmark (6)', '#cv (2)', '#data (2)', '#dataset (4)', '#diffusion (6)', '#ethics', '#games (2)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (7)', '#interpretability (2)', '#leakage', '#long_context (2)', '#low_resource (2)', '#machine_translation', '#math (3)', '#multilingual (2)', '#multimodal (4)', '#open_source (5)', '#optimization (13)', '#plp', '#rag (2)', '#reasoning (4)', '#rl (3)', '#rlhf (2)', '#robotics', '#science', '#security', '#small_models (2)', '#story_generation', '#survey', '#synthetic (2)', '#training (15)', '#transfer_learning', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-02-11 12:19',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-02-11 12:19')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-02-11 12:19')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    