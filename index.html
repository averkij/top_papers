
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF (19 статей)</title>
    <link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #03dac6;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        header {
            padding: 1.6em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.5em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .background-digit {
            position: absolute;
            bottom: -20px;
            right: -10px;
            font-size: 12em;
            font-weight: bold;
            color: rgba(0, 0, 0, 0.03);
            z-index: 0;
            line-height: 1;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: var(--secondary-color);
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: fixed;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
        }
        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .update-info-container {
            flex: 1;
        }
        .sort-container {
            flex: 2;
        }
        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .category-toggle {
            display: none;
            margin-bottom: 10px;
            margin-top: 15px;
            cursor: pointer;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }

        .svg-container span {
            position: relative;
            z-index: 1;
        }

        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .sort-container {
                margin-top: 0px;
                text-align: left;
                width: 100%;
            .sort-dropdown {
                float: right;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiffRu(dateString) {
        const timeUnits = {
            minute: ["минуту", "минуты", "минут"],
            hour: ["час", "часа", "часов"],
            day: ["день", "дня", "дней"]
        };

        function getRussianPlural(number, words) {
            if (number % 10 === 1 && number % 100 !== 11) {
                return words[0];
            } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                return words[1];
            } else {
                return words[2];
            }
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);

        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes == 0) {
            return 'только что';
        }
        else if (minutes < 60) {
            return `${minutes} ${getRussianPlural(minutes, timeUnits.minute)} назад`;
        } else if (hours < 24) {
            return `${hours} ${getRussianPlural(hours, timeUnits.hour)} назад`;
        } else {
            return `${days} ${getRussianPlural(days, timeUnits.day)} назад`;
        }
    }
    function formatArticlesTitle(number) {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;

        let word;

        if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
            word = "статей";
        } else if (lastDigit === 1) {
            word = "статья";
        } else if (lastDigit >= 2 && lastDigit <= 4) {
            word = "статьи";
        } else {
            word = "статей";
        }

        return `${number} ${word}`;
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">
            <h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">хф дэйли</h1>
            <p>15 октября | 19 статей</p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 Сортировка по</label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="category-toggle">
            <div class="svg-container">
                <span id="category-toggle">🔍 Фильтр</span>
                <svg height="3" width="200">
                    <line x1="0" y1="0" x2="200" y2="0" 
                        stroke="black" 
                        stroke-width="2" 
                        stroke-dasharray="3, 3" />
                </svg>
            </div>
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">градиент обреченный</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>    
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "хф найтли";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "хф дэйли";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.09732', 'title': 'LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models', 'url': 'https://huggingface.co/papers/2410.09732', 'abstract': 'With the rapid development of AI-generated content, the future internet may be inundated with synthetic data, making the discrimination of authentic and credible multimodal data increasingly challenging. Synthetic data detection has thus garnered widespread attention, and the performance of large multimodal models (LMMs) in this task has attracted significant interest. LMMs can provide natural language explanations for their authenticity judgments, enhancing the explainability of synthetic content detection. Simultaneously, the task of distinguishing between real and synthetic data effectively tests the perception, knowledge, and reasoning capabilities of LMMs. In response, we introduce LOKI, a novel benchmark designed to evaluate the ability of LMMs to detect synthetic data across multiple modalities. LOKI encompasses video, image, 3D, text, and audio modalities, comprising 18K carefully curated questions across 26 subcategories with clear difficulty levels. The benchmark includes coarse-grained judgment and multiple-choice questions, as well as fine-grained anomaly selection and explanation tasks, allowing for a comprehensive analysis of LMMs. We evaluated 22 open-source LMMs and 6 closed-source models on LOKI, highlighting their potential as synthetic data detectors and also revealing some limitations in the development of LMM capabilities. More information about LOKI can be found at https://opendatalab.github.io/LOKI/', 'score': 44, 'issue_id': 107, 'pub_date': '2024-10-13', 'pub_date_ru': '13 октября', 'data': {'desc': 'LOKI - это новый бенчмарк для оценки способности больших мультимодальных моделей (LMM) обнаруживать синтетические данные в различных модальностях. Он включает в себя 18 тысяч вопросов по видео, изображениям, 3D, тексту и аудио, разделенных на 26 подкатегорий с четкими уровнями сложности. LOKI позволяет проводить комплексный анализ LMM через задачи грубой классификации, выбора из нескольких вариантов, а также выявления и объяснения аномалий. Авторы оценили 28 моделей LMM на этом бенчмарке, выявив их потенциал и ограничения в обнаружении синтетических данных.', 'tags': ['#syntheticDataDetection', '#multimodalBenchmark', '#LMMevaluation'], 'categories': ['#multimodal', '#benchmark', '#dataset'], 'emoji': '🕵️', 'title': 'LOKI: Мультимодальный детектив для искусственного интеллекта'}}, {'id': 'https://huggingface.co/papers/2410.10139', 'title': 'MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models', 'url': 'https://huggingface.co/papers/2410.10139', 'abstract': 'Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this capability remains insufficient. Existing benchmarks suffer from limitations in data scale, scope, and evaluation depth, while current evaluation metrics are often costly or biased, lacking in reliability for practical applications. To address these challenges, we introduce MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs). MMIE comprises 20K meticulously curated multimodal queries, spanning 3 categories, 12 fields, and 102 subfields, including mathematics, coding, physics, literature, health, and arts. It supports both interleaved inputs and outputs, offering a mix of multiple-choice and open-ended question formats to evaluate diverse competencies. Moreover, we propose a reliable automated evaluation metric, leveraging a scoring model fine-tuned with human-annotated data and systematic evaluation criteria, aimed at reducing bias and improving evaluation accuracy. Extensive experiments demonstrate the effectiveness of our benchmark and metrics in providing a comprehensive evaluation of interleaved LVLMs. Specifically, we evaluate eight LVLMs, revealing that even the best models show significant room for improvement, with most achieving only moderate results. We believe MMIE will drive further advancements in the development of interleaved LVLMs. We publicly release our benchmark and code in https://mmie-bench.github.io/.', 'score': 42, 'issue_id': 107, 'pub_date': '2024-10-14', 'pub_date_ru': '14 октября', 'data': {'desc': 'MMIE - это новый масштабный бенчмарк для оценки моделей с чередующимся мультимодальным пониманием и генерацией (LVLMs). Он содержит 20 000 тщательно отобранных мультимодальных запросов из 12 областей знаний. MMIE поддерживает как чередующиеся входные данные, так и выходные, предлагая различные форматы вопросов. Авторы также предлагают автоматизированную метрику оценки, основанную на модели, дообученной на размеченных человеком данных.', 'tags': ['#LVLMs', '#интерливингМультимодальность', '#оценкаМоделей'], 'categories': ['#multimodal', '#benchmark', '#dataset'], 'emoji': '🔄', 'title': 'MMIE: Новый стандарт для оценки чередующихся мультимодальных моделей'}}, {'id': 'https://huggingface.co/papers/2410.09584', 'title': 'Toward General Instruction-Following Alignment for Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2410.09584', 'abstract': 'Following natural instructions is crucial for the effective application of Retrieval-Augmented Generation (RAG) systems. Despite recent advancements in Large Language Models (LLMs), research on assessing and improving instruction-following (IF) alignment within the RAG domain remains limited. To address this issue, we propose VIF-RAG, the first automated, scalable, and verifiable synthetic pipeline for instruction-following alignment in RAG systems. We start by manually crafting a minimal set of atomic instructions (<100) and developing combination rules to synthesize and verify complex instructions for a seed set. We then use supervised models for instruction rewriting while simultaneously generating code to automate the verification of instruction quality via a Python executor. Finally, we integrate these instructions with extensive RAG and general data samples, scaling up to a high-quality VIF-RAG-QA dataset (>100k) through automated processes. To further bridge the gap in instruction-following auto-evaluation for RAG systems, we introduce FollowRAG Benchmark, which includes approximately 3K test samples, covering 22 categories of general instruction constraints and four knowledge-intensive QA datasets. Due to its robust pipeline design, FollowRAG can seamlessly integrate with different RAG benchmarks. Using FollowRAG and eight widely-used IF and foundational abilities benchmarks for LLMs, we demonstrate that VIF-RAG markedly enhances LLM performance across a broad range of general instruction constraints while effectively leveraging its capabilities in RAG scenarios. Further analysis offers practical insights for achieving IF alignment in RAG systems. Our code and datasets are released at https://FollowRAG.github.io.', 'score': 32, 'issue_id': 107, 'pub_date': '2024-10-12', 'pub_date_ru': '12 октября', 'data': {'desc': 'VIF-RAG - это первый автоматизированный и масштабируемый конвейер для улучшения следования инструкциям в системах RAG. Авторы создали набор атомарных инструкций, правила их комбинирования и верификации, а также модели для перефразирования инструкций. На основе этого был сгенерирован большой датасет VIF-RAG-QA и создан бенчмарк FollowRAG для оценки RAG-систем. Эксперименты показали, что VIF-RAG значительно улучшает способность языковых моделей следовать инструкциям в различных сценариях.', 'tags': ['#instruction-following', '#synthetic-data-generation', '#rag-evaluation'], 'categories': ['#nlp', '#rag', '#benchmark', '#dataset'], 'emoji': '🧠', 'title': 'VIF-RAG: Новый подход к обучению RAG-систем следованию инструкциям'}}, {'id': 'https://huggingface.co/papers/2410.10306', 'title': 'Animate-X: Universal Character Image Animation with Enhanced Motion Representation', 'url': 'https://huggingface.co/papers/2410.10306', 'abstract': 'Character image animation, which generates high-quality videos from a reference image and target pose sequence, has seen significant progress in recent years. However, most existing methods only apply to human figures, which usually do not generalize well on anthropomorphic characters commonly used in industries like gaming and entertainment. Our in-depth analysis suggests to attribute this limitation to their insufficient modeling of motion, which is unable to comprehend the movement pattern of the driving video, thus imposing a pose sequence rigidly onto the target character. To this end, this paper proposes Animate-X, a universal animation framework based on LDM for various character types (collectively named X), including anthropomorphic characters. To enhance motion representation, we introduce the Pose Indicator, which captures comprehensive motion pattern from the driving video through both implicit and explicit manner. The former leverages CLIP visual features of a driving video to extract its gist of motion, like the overall movement pattern and temporal relations among motions, while the latter strengthens the generalization of LDM by simulating possible inputs in advance that may arise during inference. Moreover, we introduce a new Animated Anthropomorphic Benchmark (A^2Bench) to evaluate the performance of Animate-X on universal and widely applicable animation images. Extensive experiments demonstrate the superiority and effectiveness of Animate-X compared to state-of-the-art methods.', 'score': 23, 'issue_id': 108, 'pub_date': '2024-10-14', 'pub_date_ru': '14 октября', 'data': {'desc': 'Статья представляет Animate-X - универсальную систему анимации персонажей на основе LDM, включая антропоморфных. Авторы вводят Pose Indicator для улучшенного представления движений, используя как неявные, так и явные методы. Система использует визуальные признаки CLIP для извлечения сути движения из управляющего видео. Также представлен новый бенчмарк A^2Bench для оценки производительности на универсальных анимационных изображениях.', 'tags': ['#characterAnimation', '#poseIndicator', '#anthropomorphicCharacters'], 'categories': ['#cv', '#video', '#benchmark', '#multimodal'], 'emoji': '🎭', 'title': 'Универсальная анимация персонажей с улучшенным представлением движений'}}, {'id': 'https://huggingface.co/papers/2410.10563', 'title': 'MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks', 'url': 'https://huggingface.co/papers/2410.10563', 'abstract': 'We present MEGA-Bench, an evaluation suite that scales multimodal evaluation to over 500 real-world tasks, to address the highly heterogeneous daily use cases of end users. Our objective is to optimize for a set of high-quality data samples that cover a highly diverse and rich set of multimodal tasks, while enabling cost-effective and accurate model evaluation. In particular, we collected 505 realistic tasks encompassing over 8,000 samples from 16 expert annotators to extensively cover the multimodal task space. Instead of unifying these problems into standard multi-choice questions (like MMMU, MMBench, and MMT-Bench), we embrace a wide range of output formats like numbers, phrases, code, \\LaTeX, coordinates, JSON, free-form, etc. To accommodate these formats, we developed over 40 metrics to evaluate these tasks. Unlike existing benchmarks, MEGA-Bench offers a fine-grained capability report across multiple dimensions (e.g., application, input type, output format, skill), allowing users to interact with and visualize model capabilities in depth. We evaluate a wide variety of frontier vision-language models on MEGA-Bench to understand their capabilities across these dimensions.', 'score': 23, 'issue_id': 108, 'pub_date': '2024-10-14', 'pub_date_ru': '14 октября', 'data': {'desc': 'MEGA-Bench - это новый набор данных для оценки мультимодальных моделей, включающий более 500 реальных задач. Он охватывает широкий спектр форматов вывода и использует более 40 метрик для оценки. MEGA-Bench предоставляет подробный отчет о возможностях моделей по различным измерениям. Авторы провели оценку различных современных мультимодальных моделей на этом наборе данных.', 'tags': ['#мультимодальнаяОценка', '#реалистичныеЗадачи', '#многоформатныйВывод'], 'categories': ['#multimodal', '#benchmark', '#dataset'], 'emoji': '🧠', 'title': 'MEGA-Bench: Масштабная оценка мультимодальных моделей на реальных задачах'}}, {'id': 'https://huggingface.co/papers/2410.10783', 'title': 'LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content', 'url': 'https://huggingface.co/papers/2410.10783', 'abstract': 'The large-scale training of multi-modal models on data scraped from the web has shown outstanding utility in infusing these models with the required world knowledge to perform effectively on multiple downstream tasks. However, one downside of scraping data from the web can be the potential sacrifice of the benchmarks on which the abilities of these models are often evaluated. To safeguard against test data contamination and to truly test the abilities of these foundation models we propose LiveXiv: A scalable evolving live benchmark based on scientific ArXiv papers. LiveXiv accesses domain-specific manuscripts at any given timestamp and proposes to automatically generate visual question-answer pairs (VQA). This is done without any human-in-the-loop, using the multi-modal content in the manuscripts, like graphs, charts, and tables. Moreover, we introduce an efficient evaluation approach that estimates the performance of all models on the evolving benchmark using evaluations of only a subset of models. This significantly reduces the overall evaluation cost. We benchmark multiple open and proprietary Large Multi-modal Models (LMMs) on the first version of our benchmark, showing its challenging nature and exposing the models true abilities, avoiding contamination. Lastly, in our commitment to high quality, we have collected and evaluated a manually verified subset. By comparing its overall results to our automatic annotations, we have found that the performance variance is indeed minimal (<2.5%). Our dataset is available online on HuggingFace, and our code will be available here.', 'score': 22, 'issue_id': 109, 'pub_date': '2024-10-14', 'pub_date_ru': '14 октября', 'data': {'desc': 'LiveXiv - это масштабируемый развивающийся бенчмарк, основанный на научных статьях ArXiv. Он автоматически генерирует пары вопрос-ответ по визуальному контенту (VQA) из рукописей, используя графики, диаграммы и таблицы. Бенчмарк предлагает эффективный подход к оценке производительности моделей, снижая общие затраты на оценку. LiveXiv позволяет оценить истинные способности мультимодальных моделей, избегая загрязнения тестовых данных.', 'tags': ['#LiveXiv', '#VQA', '#ArXiv'], 'categories': ['#multimodal', '#benchmark', '#dataset'], 'emoji': '📊', 'title': 'LiveXiv: Эволюционирующий бенчмарк для оценки мультимодальных моделей'}}, {'id': 'https://huggingface.co/papers/2410.07985', 'title': 'Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models', 'url': 'https://huggingface.co/papers/2410.07985', 'abstract': "Recent advancements in large language models (LLMs) have led to significant breakthroughs in mathematical reasoning capabilities. However, existing benchmarks like GSM8K or MATH are now being solved with high accuracy (e.g., OpenAI o1 achieves 94.8% on MATH dataset), indicating their inadequacy for truly challenging these models. To bridge this gap, we propose a comprehensive and challenging benchmark specifically designed to assess LLMs' mathematical reasoning at the Olympiad level. Unlike existing Olympiad-related benchmarks, our dataset focuses exclusively on mathematics and comprises a vast collection of 4428 competition-level problems with rigorous human annotation. These problems are meticulously categorized into over 33 sub-domains and span more than 10 distinct difficulty levels, enabling a holistic assessment of model performance in Olympiad-mathematical reasoning. Furthermore, we conducted an in-depth analysis based on this benchmark. Our experimental results show that even the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle with highly challenging Olympiad-level problems, with 60.54% and 52.55% accuracy, highlighting significant challenges in Olympiad-level mathematical reasoning.", 'score': 20, 'issue_id': 107, 'pub_date': '2024-10-10', 'pub_date_ru': '10 октября', 'data': {'desc': 'Статья представляет новый эталонный набор данных для оценки математических способностей больших языковых моделей на уровне олимпиад. Набор данных содержит 4428 задач олимпиадного уровня с тщательной человеческой аннотацией, разделенных на 33 поддомена и 10 уровней сложности. Эксперименты показали, что даже самые продвинутые модели, такие как OpenAI o1-mini и OpenAI o1-preview, испытывают трудности с решением сложных олимпиадных задач, достигая точности 60.54% и 52.55% соответственно. Это исследование выявляет значительные проблемы в области математических рассуждений на олимпиадном уровне для современных языковых моделей.', 'tags': ['#olympiad-math', '#benchmark-dataset', '#llm-evaluation'], 'categories': ['#benchmark', '#dataset', '#nlp'], 'emoji': '🧮', 'title': 'Новый вызов для ИИ: олимпиадная математика как тест на интеллект'}}, {'id': 'https://huggingface.co/papers/2410.10774', 'title': 'Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention', 'url': 'https://huggingface.co/papers/2410.10774', 'abstract': 'In recent years there have been remarkable breakthroughs in image-to-video generation. However, the 3D consistency and camera controllability of generated frames have remained unsolved. Recent studies have attempted to incorporate camera control into the generation process, but their results are often limited to simple trajectories or lack the ability to generate consistent videos from multiple distinct camera paths for the same scene. To address these limitations, we introduce Cavia, a novel framework for camera-controllable, multi-view video generation, capable of converting an input image into multiple spatiotemporally consistent videos. Our framework extends the spatial and temporal attention modules into view-integrated attention modules, improving both viewpoint and temporal consistency. This flexible design allows for joint training with diverse curated data sources, including scene-level static videos, object-level synthetic multi-view dynamic videos, and real-world monocular dynamic videos. To our best knowledge, Cavia is the first of its kind that allows the user to precisely specify camera motion while obtaining object motion. Extensive experiments demonstrate that Cavia surpasses state-of-the-art methods in terms of geometric consistency and perceptual quality. Project Page: https://ir1d.github.io/Cavia/', 'score': 18, 'issue_id': 108, 'pub_date': '2024-10-14', 'pub_date_ru': '14 октября', 'data': {'desc': 'Cavia - это новая система для генерации видео с контролируемой камерой и возможностью создания нескольких ракурсов. Она расширяет модули пространственного и временного внимания, улучшая согласованность ракурсов и временную согласованность. Cavia позволяет пользователю точно указывать движение камеры, одновременно получая движение объектов. Эксперименты показывают, что Cavia превосходит современные методы по геометрической согласованности и визуальному качеству.', 'tags': ['#image2video', '#cameraControl', '#multiViewGeneration'], 'categories': ['#cv', '#video', '#multimodal'], 'emoji': '🎥', 'title': 'Cavia: Революция в генерации видео с контролем камеры'}}, {'id': 'https://huggingface.co/papers/2410.10792', 'title': 'Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations', 'url': 'https://huggingface.co/papers/2410.10792', 'abstract': 'Generative models transform random noise into images; their inversion aims to transform images back to structured noise for recovery and editing. This paper addresses two key tasks: (i) inversion and (ii) editing of a real image using stochastic equivalents of rectified flow models (such as Flux). Although Diffusion Models (DMs) have recently dominated the field of generative modeling for images, their inversion presents faithfulness and editability challenges due to nonlinearities in drift and diffusion. Existing state-of-the-art DM inversion approaches rely on training of additional parameters or test-time optimization of latent variables; both are expensive in practice. Rectified Flows (RFs) offer a promising alternative to diffusion models, yet their inversion has been underexplored. We propose RF inversion using dynamic optimal control derived via a linear quadratic regulator. We prove that the resulting vector field is equivalent to a rectified stochastic differential equation. Additionally, we extend our framework to design a stochastic sampler for Flux. Our inversion method allows for state-of-the-art performance in zero-shot inversion and editing, outperforming prior works in stroke-to-image synthesis and semantic image editing, with large-scale human evaluations confirming user preference.', 'score': 11, 'issue_id': 109, 'pub_date': '2024-10-14', 'pub_date_ru': '14 октября', 'data': {'desc': 'Статья представляет новый метод инверсии и редактирования изображений с использованием стохастических эквивалентов моделей выпрямленного потока (Rectified Flow). Авторы предлагают инверсию RF с помощью динамического оптимального управления, полученного через линейный квадратичный регулятор. Метод обеспечивает высокую производительность в задачах инверсии и редактирования изображений без дополнительного обучения. Проведенные крупномасштабные оценки подтверждают преимущество метода перед существующими подходами.', 'tags': ['#rectified_flow', '#image_inversion', '#image_editing'], 'categories': ['#cv', '#generative_models'], 'emoji': '🖼️', 'title': 'Эффективная инверсия и редактирование изображений с помощью выпрямленных потоков'}}, {'id': 'https://huggingface.co/papers/2410.10818', 'title': 'TemporalBench: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models', 'url': 'https://huggingface.co/papers/2410.10818', 'abstract': "Understanding fine-grained temporal dynamics is crucial for multimodal video comprehension and generation. Due to the lack of fine-grained temporal annotations, existing video benchmarks mostly resemble static image benchmarks and are incompetent at evaluating models for temporal understanding. In this paper, we introduce TemporalBench, a new benchmark dedicated to evaluating fine-grained temporal understanding in videos. TemporalBench consists of ~10K video question-answer pairs, derived from ~2K high-quality human annotations detailing the temporal dynamics in video clips. As a result, our benchmark provides a unique testbed for evaluating various temporal understanding and reasoning abilities such as action frequency, motion magnitude, event order, etc. Moreover, it enables evaluations on various tasks like both video question answering and captioning, both short and long video understanding, as well as different models such as multimodal video embedding models and text generation models. Results show that state-of-the-art models like GPT-4o achieve only 38.5% question answering accuracy on TemporalBench, demonstrating a significant gap (~30%) between humans and AI in temporal understanding. Furthermore, we notice a critical pitfall for multi-choice QA where LLMs can detect the subtle changes in negative captions and find a centralized description as a cue for its prediction, where we propose Multiple Binary Accuracy (MBA) to correct such bias. We hope that TemporalBench can foster research on improving models' temporal reasoning capabilities. Both dataset and evaluation code will be made available.", 'score': 10, 'issue_id': 108, 'pub_date': '2024-10-14', 'pub_date_ru': '14 октября', 'data': {'desc': 'TemporalBench - это новый бенчмарк для оценки детального понимания временной динамики в видео. Он состоит из примерно 10 тысяч пар вопрос-ответ по видео, основанных на высококачественных аннотациях временной динамики в видеоклипах. Бенчмарк позволяет оценивать различные аспекты временного понимания, такие как частота действий, величина движения, порядок событий, а также применим к различным задачам и моделям. Результаты показывают значительный разрыв между людьми и ИИ в понимании временной динамики видео.', 'tags': ['#TemporalBench', '#VideoQA', '#TemporalReasoning'], 'categories': ['#benchmark', '#video', '#multimodal', '#dataset'], 'emoji': '⏳', 'title': 'TemporalBench: Новый стандарт для оценки понимания временной динамики в видео'}}, {'id': 'https://huggingface.co/papers/2410.10594', 'title': 'VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents', 'url': 'https://huggingface.co/papers/2410.10594', 'abstract': 'Retrieval-augmented generation (RAG) is an effective technique that enables large language models (LLMs) to utilize external knowledge sources for generation. However, current RAG systems are solely based on text, rendering it impossible to utilize vision information like layout and images that play crucial roles in real-world multi-modality documents. In this paper, we introduce VisRAG, which tackles this issue by establishing a vision-language model (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the document to obtain text, the document is directly embedded using a VLM as an image and then retrieved to enhance the generation of a VLM. Compared to traditional text-based RAG, VisRAG maximizes the retention and utilization of the data information in the original documents, eliminating the information loss introduced during the parsing process. We collect both open-source and synthetic data to train the retriever in VisRAG and explore a variety of generation methods. Experiments demonstrate that VisRAG outperforms traditional RAG in both the retrieval and generation stages, achieving a 25--39\\% end-to-end performance gain over traditional text-based RAG pipeline. Further analysis reveals that VisRAG is effective in utilizing training data and demonstrates strong generalization capability, positioning it as a promising solution for RAG on multi-modality documents. Our code and data are available at https://github.com/openbmb/visrag .', 'score': 8, 'issue_id': 118, 'pub_date': '2024-10-14', 'pub_date_ru': '14 октября', 'data': {'desc': 'VisRAG - это новый подход к извлечению и генерации информации (RAG), основанный на использовании мультимодальных моделей (VLM). В отличие от традиционных текстовых RAG-систем, VisRAG работает напрямую с изображениями документов, сохраняя всю визуальную информацию. Эксперименты показали, что VisRAG превосходит текстовые RAG-системы на 25-39% при работе с мультимодальными документами. Авторы предоставили код и данные в открытом доступе для дальнейших исследований.', 'categories': ['#multimodal', '#rag', '#nlp', '#cv'], 'emoji': '🖼️', 'title': 'VisRAG: Мультимодальный RAG для работы с документами как с изображениями', 'embedding': [0.01923274316416371, 0.06277316990479936, 0.06969591574243367, -0.023045458079905486, -0.11253366259979329, -0.027951238127905315, -0.008458242241660136, -0.051035734321651684, -0.04421708669544871, 0.06761388779992211, 0.008171963425255882, -0.026962273262365204, 0.012218907024042942, -0.02867994616079074, -0.021614061942597605, 0.008308596823698965, 0.018100637862778175, -0.07084103511862391, -0.037606647823828054, 0.004733362465474359, 0.040495462082193394, -0.005683288395058941, 0.0069552777361874835, 0.02563498150234848, -0.03622730387516575, -0.06490724592538326, 0.018959474311990937, 0.09847997039108879, 0.021952392947647437, -0.0791691509992433, 0.012680857352731159, -0.07901299648859318, -0.04596078774348364, -0.02197841698668362, 0.026832146901324475, 0.13272934042460108, 0.030840052386270654, 0.006119213040481691, -0.0339110451943223, 0.06709338030047257, 0.05886936468657112, -0.019414919658563415, 0.07115333797406434, 0.05631888143325561, 0.03627935606381132, 0.1408492516612114, 0.08317705237361626, -0.002784713786118737, 0.0006795059101797807, 0.034795907737857866, -0.06974796382050603, -0.05798450214303557, -0.041458398798124095, 0.023032446060387398, -0.07641045899134548, -0.03172491492980621, -0.04177070576413775, -0.044815672477866615, -0.010559789857678106, -0.004980603579095055, -0.0382833057233545, -0.09509666856173686, 0.01735891575508805, 0.0023374026473565192, -0.06574006450141968, 0.050645355238529484, -0.09447206285085599, 0.06782209449921783, -0.1081614041260477, -0.05376839406936689, 0.03799702690695025, -0.1095147199251006, 0.026780096767965503, -0.04817294204703592, -0.055538119156438005, 0.04452939160617576, 0.04070366261562928, 0.07828428434513454, -0.017710258779655978, -0.041406350720051735, 0.10097839734518549, -0.02430768768704515, 0.034561679054812576, -0.051374061216128294, -0.052519176481745325, -0.025010371680894393, 0.016981546636197335, -0.011483691131640524, -0.020586061018503223, 0.015992583825943835, -0.03422334804976274, 0.10404939015323714, -0.010963183221133663, 0.03120440743035667, 0.010214954692627173, 0.0833332068842664, -0.1244012286545549, -0.09733485101489854, 0.11596900634135775, 0.044919774799871165, -0.017124689127329378, -0.030137367364778105, 0.1095147199251006, -0.04374863138464474, 0.08473857281667828, -0.03682588554701043, -0.05007279349514776, 0.056266831299896646, 0.02836764536063691, 0.012362046432245068, -0.09988533632350066, -0.0692795146755619, -0.02369609194039576, 0.00980505511388391, 0.02841969343870927, -0.13064731453737616, -0.06105549906166043, -0.009557814411320534, 0.06917540824298413, 0.04364452906264019, -0.06662492293438202, 0.0662605699455826, -0.004193336320161702, -0.09962508565670579, 0.09551307373918184, -0.023383787029668715, 0.03893393958384477, -0.023552951504550328, 0.03151671234108372, -0.011015234587664601, 0.031256459619002246, 0.06839464802145312, 0.11648951384080727, -0.021314771106675255, -0.153549628070863, -0.12106997695856198, -0.06329367740424889, 0.014574202380026635, -0.10410144645245593, 0.10774499483802949, -0.021197656765152617, -0.0428377427467864, -0.06750978958849078, 0.021822264531320096, -0.12065357589169019, -0.003591499690915209, -0.039662649672016805, -0.004121766718824969, 0.011665867831568888, 0.06698928208904124, -0.12981449185076652, -0.02909635339352233, -0.06834259583280754, 0.04689770042037816, -0.05762014709894955, -0.07641045899134548, -0.03221939839021957, 0.054236853490744065, 0.017111675052524675, -0.06792619065536255, -0.026754070673642712, 0.11180495045633465, 0.05762014709894955, -0.030345570981143904, 0.014587214605073387, 0.05871321428649421, -0.09119286539879524, 0.004909033566701, -0.0730271612725668, -0.03659165686396516, -0.10467399997469123, -0.13418676471151836, -0.03802305094598643, -0.09374334865211074, -0.052623278803749875, -0.04497182493323014, -0.028705974310400135, -0.002509820846467968, 0.08963134495573323, -0.0827085970628123, -0.12336020748979601, 0.12252739919019265, -0.08250039652937644, 0.00809388719757412, -0.01573233048727639, -0.025465817027466867, 0.009753003952881633, -0.04476362028922103, -0.011288499945850138, 0.08296884772960716, -0.04382670761232651, 0.13741389353264066, 0.10404939015323714, 0.06043088924020635, 0.04398286006769003, 0.021653100056438484, 0.08702880334791233, -0.05970217915203432, -0.0019584084133445027, -0.1224233009787613, 0.02826354303863236, 0.03177696506316518, -0.015719318262229637, -0.2190294398502012, -0.06121164946173735, -0.01901152650063652, 0.00765796234662271, 0.08801776821345242, -0.01165936141075252, -0.020481958696498676, 0.0327138777400597, -0.0005229470835480356, 0.0773473696129534, 0.03677383335836486, 0.0003600852683684278, -0.0514000893657377, -0.0275868830838193, 0.1224233009787613, 0.031881067385169735, -0.071829999984164, -0.017176737205401735, -0.01791846136837847, 0.004004652377302327, 0.05595452433388299, -0.06017063651812488, 0.05605862665588754, 0.11617721304065344, -0.12148638830186677, 0.05439300389082098, -0.07318331578321692, 0.05345609326921306, 0.018868386064791087, -0.016786358122279542, -0.056735288665987205, 0.08197989108521349, 0.006184276221002057, 0.008855129389827989, -0.0027261564098287555, 0.011340551517909735, 0.01504265912953122, 0.06256496731607686, 0.07635840474741329, -0.00746277239400429, -0.03086607848059344, -0.025543891199862022, 0.0735997230159485, -0.0980115130249982, 0.0052961607753513095, 0.1123254620663574, 0.012407591377959637, 0.014730354218804174, -0.10722448733857996, 0.005501110461958611, -0.03328643537286822, -0.014183822680318455, 0.06277316990479936]}}, {'id': 'https://huggingface.co/papers/2410.09335', 'title': 'Rethinking Data Selection at Scale: Random Selection is Almost All You Need', 'url': 'https://huggingface.co/papers/2410.09335', 'abstract': 'Supervised fine-tuning (SFT) is crucial for aligning Large Language Models (LLMs) with human instructions. The primary goal during SFT is to select a small yet representative subset of training data from the larger pool, such that fine-tuning with this subset achieves results comparable to or even exceeding those obtained using the entire dataset. However, most existing data selection techniques are designed for small-scale data pools, which fail to meet the demands of real-world SFT scenarios. In this paper, we replicated several self-scoring methods those that do not rely on external model assistance on two million scale datasets, and found that nearly all methods struggled to significantly outperform random selection when dealing with such large-scale data pools. Moreover, our comparisons suggest that, during SFT, diversity in data selection is more critical than simply focusing on high quality data. We also analyzed the limitations of several current approaches, explaining why they perform poorly on large-scale datasets and why they are unsuitable for such contexts. Finally, we found that filtering data by token length offers a stable and efficient method for improving results. This approach, particularly when training on long text data, proves highly beneficial for relatively weaker base models, such as Llama3.', 'score': 8, 'issue_id': 107, 'pub_date': '2024-10-12', 'pub_date_ru': '12 октября', 'data': {'desc': 'Статья посвящена проблеме выбора данных для обучения больших языковых моделей (LLM) с помощью контролируемой доводки (SFT). Авторы провели репликацию нескольких методов самооценки на наборах данных из двух миллионов примеров и обнаружили, что большинство методов не превосходят случайный выбор. Исследование показало, что разнообразие данных важнее их качества при SFT. Авторы также предложили простой, но эффективный метод фильтрации данных по длине токенов, который особенно полезен для более слабых базовых моделей.', 'tags': ['#dataSelection', '#SFT', '#largeScaleDatasets'], 'categories': ['#nlp', '#dataset', '#benchmark'], 'emoji': '🔍', 'title': 'Эффективный отбор данных для обучения больших языковых моделей'}}, {'id': 'https://huggingface.co/papers/2410.06634', 'title': 'Tree of Problems: Improving structured problem solving with compositionality', 'url': 'https://huggingface.co/papers/2410.06634', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable performance across multiple tasks through in-context learning. For complex reasoning tasks that require step-by-step thinking, Chain-of-Thought (CoT) prompting has given impressive results, especially when combined with self-consistency. Nonetheless, some tasks remain particularly difficult for LLMs to solve. Tree of Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing the complex problem into paths of subproblems. In this paper, we propose Tree of Problems (ToP), a simpler version of ToT, which we hypothesise can work better for complex tasks that can be divided into identical subtasks. Our empirical results show that our approach outperforms ToT and GoT, and in addition performs better than CoT on complex reasoning tasks. All code for this paper is publicly available here: https://github.com/ArmelRandy/tree-of-problems.', 'score': 5, 'issue_id': 109, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'В статье предлагается новый метод под названием Tree of Problems (ToP) для решения сложных задач с помощью больших языковых моделей. ToP является упрощенной версией метода Tree of Thoughts (ToT) и предназначен для задач, которые можно разделить на идентичные подзадачи. Авторы провели эмпирические исследования, показавшие превосходство ToP над методами ToT и Graph of Thoughts (GoT). Кроме того, ToP показал лучшие результаты, чем Chain-of-Thought (CoT) промптинг на сложных задачах рассуждения.', 'tags': ['#TreeOfProblems', '#ComplexReasoning', '#PromptEngineering'], 'categories': ['#nlp', '#rlhf', '#code'], 'emoji': '🌳', 'title': 'Древо проблем: новый метод для решения сложных задач с помощью ИИ'}}, {'id': 'https://huggingface.co/papers/2410.10803', 'title': 'Generalizable Humanoid Manipulation with Improved 3D Diffusion Policies', 'url': 'https://huggingface.co/papers/2410.10803', 'abstract': 'Humanoid robots capable of autonomous operation in diverse environments have long been a goal for roboticists. However, autonomous manipulation by humanoid robots has largely been restricted to one specific scene, primarily due to the difficulty of acquiring generalizable skills. Recent advances in 3D visuomotor policies, such as the 3D Diffusion Policy (DP3), have shown promise in extending these capabilities to wilder environments. However, 3D visuomotor policies often rely on camera calibration and point-cloud segmentation, which present challenges for deployment on mobile robots like humanoids. In this work, we introduce the Improved 3D Diffusion Policy (iDP3), a novel 3D visuomotor policy that eliminates these constraints by leveraging egocentric 3D visual representations. We demonstrate that iDP3 enables a full-sized humanoid robot to autonomously perform skills in diverse real-world scenarios, using only data collected in the lab. Videos are available at: https://humanoid-manipulation.github.io', 'score': 5, 'issue_id': 108, 'pub_date': '2024-10-14', 'pub_date_ru': '14 октября', 'data': {'desc': 'Статья представляет новый подход к автономной манипуляции гуманоидных роботов - Improved 3D Diffusion Policy (iDP3). Этот метод использует эгоцентрические 3D визуальные представления, что позволяет избежать ограничений, связанных с калибровкой камеры и сегментацией облака точек. iDP3 демонстрирует способность полноразмерного гуманоидного робота автономно выполнять задачи в различных реальных сценариях, используя только данные, собранные в лабораторных условиях. Это значительный шаг вперед в области автономной манипуляции гуманоидных роботов в разнообразных средах.', 'tags': ['#humanoidRobots', '#3DVisuomotorPolicy', '#autonomousManipulation'], 'categories': ['#rlhf', '#cv', '#robotics'], 'emoji': '🤖', 'title': 'Революция в автономной манипуляции гуманоидных роботов'}}, {'id': 'https://huggingface.co/papers/2410.10813', 'title': 'LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory', 'url': 'https://huggingface.co/papers/2410.10813', 'abstract': 'Recent large language model (LLM)-driven chat assistant systems have integrated memory components to track user-assistant chat histories, enabling more accurate and personalized responses. However, their long-term memory capabilities in sustained interactions remain underexplored. This paper introduces LongMemEval, a comprehensive benchmark designed to evaluate five core long-term memory abilities of chat assistants: information extraction, multi-session reasoning, temporal reasoning, knowledge updates, and abstention. With 500 meticulously curated questions embedded within freely scalable user-assistant chat histories, LongMemEval presents a significant challenge to existing long-term memory systems, with commercial chat assistants and long-context LLMs showing 30% accuracy drop on memorizing information across sustained interactions. We then present a unified framework that breaks down the long-term memory design into four design choices across the indexing, retrieval, and reading stages. Built upon key experimental insights, we propose several memory designs including session decomposition for optimizing value granularity, fact-augmented key expansion for enhancing the index structure, and time-aware query expansion for refining the search scope. Experiment results show that these optimizations greatly improve both memory recall and downstream question answering on LongMemEval. Overall, our study provides valuable resources and guidance for advancing the long-term memory capabilities of LLM-based chat assistants, paving the way toward more personalized and reliable conversational AI.', 'score': 4, 'issue_id': 109, 'pub_date': '2024-10-14', 'pub_date_ru': '14 октября', 'data': {'desc': 'Статья представляет LongMemEval - комплексный бенчмарк для оценки долгосрочной памяти чат-ассистентов на основе больших языковых моделей (LLM). Авторы выделяют пять ключевых способностей долгосрочной памяти и создают набор из 500 вопросов для их тестирования. Исследование показывает значительное снижение точности существующих систем при длительном взаимодействии. Предлагается унифицированная структура для улучшения долгосрочной памяти, включающая оптимизации индексирования, извлечения и чтения информации.', 'tags': ['#LongMemEval', '#долгосрочнаяПамять', '#чатАссистенты'], 'categories': ['#nlp', '#benchmark', '#dataset'], 'emoji': '🧠', 'title': 'LongMemEval: Новый стандарт оценки долговременной памяти чат-ассистентов'}}, {'id': 'https://huggingface.co/papers/2410.07752', 'title': 'TVBench: Redesigning Video-Language Evaluation', 'url': 'https://huggingface.co/papers/2410.07752', 'abstract': 'Large language models have demonstrated impressive performance when integrated with vision models even enabling video understanding. However, evaluating these video models presents its own unique challenges, for which several benchmarks have been proposed. In this paper, we show that the currently most used video-language benchmarks can be solved without requiring much temporal reasoning. We identified three main issues in existing datasets: (i) static information from single frames is often sufficient to solve the tasks (ii) the text of the questions and candidate answers is overly informative, allowing models to answer correctly without relying on any visual input (iii) world knowledge alone can answer many of the questions, making the benchmarks a test of knowledge replication rather than visual reasoning. In addition, we found that open-ended question-answering benchmarks for video understanding suffer from similar issues while the automatic evaluation process with LLMs is unreliable, making it an unsuitable alternative. As a solution, we propose TVBench, a novel open-source video multiple-choice question-answering benchmark, and demonstrate through extensive evaluations that it requires a high level of temporal understanding. Surprisingly, we find that most recent state-of-the-art video-language models perform similarly to random performance on TVBench, with only Gemini-Pro and Tarsier clearly surpassing this baseline.', 'score': 4, 'issue_id': 107, 'pub_date': '2024-10-10', 'pub_date_ru': '10 октября', 'data': {'desc': 'В статье рассматриваются проблемы существующих бенчмарков для оценки моделей видео-языкового понимания. Авторы выявили, что многие задачи можно решить без глубокого анализа временной информации в видео. Предложен новый бенчмарк TVBench, требующий высокого уровня понимания временных аспектов. Результаты показывают, что большинство современных видео-языковых моделей не справляются с TVBench, демонстрируя результаты на уровне случайного угадывания.', 'tags': ['#video-language-models', '#temporal-reasoning', '#benchmark-evaluation'], 'categories': ['#multimodal', '#benchmark', '#video'], 'emoji': '🎥', 'title': 'Новый взгляд на оценку видео-языковых моделей: важность временного анализа'}}, {'id': 'https://huggingface.co/papers/2410.09733', 'title': 'MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models', 'url': 'https://huggingface.co/papers/2410.09733', 'abstract': "The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal understanding, enabling more sophisticated and accurate integration of visual and textual information across various tasks, including image and video captioning, visual question answering, and cross-modal retrieval. Despite VLMs' superior capabilities, researchers lack a comprehensive understanding of their compositionality -- the ability to understand and produce novel combinations of known visual and textual components. Prior benchmarks provide only a relatively rough compositionality evaluation from the perspectives of objects, relations, and attributes while neglecting deeper reasoning about object interactions, counting, and complex compositions. However, compositionality is a critical ability that facilitates coherent reasoning and understanding across modalities for VLMs. To address this limitation, we propose MMCOMPOSITION, a novel human-annotated benchmark for comprehensively and accurately evaluating VLMs' compositionality. Our proposed benchmark serves as a complement to these earlier works. With MMCOMPOSITION, we can quantify and explore the compositionality of the mainstream VLMs. Surprisingly, we find GPT-4o's compositionality inferior to the best open-source model, and we analyze the underlying reasons. Our experimental analysis reveals the limitations of VLMs in fine-grained compositional perception and reasoning, and points to areas for improvement in VLM design and training. Resources available at: https://hanghuacs.github.io/MMComposition/", 'score': 3, 'issue_id': 116, 'pub_date': '2024-10-13', 'pub_date_ru': '13 октября', 'data': {'desc': 'MMCOMPOSITION - это новый бенчмарк для оценки композиционности крупных мультимодальных языковых моделей (VLM). Он позволяет более точно оценить способность моделей понимать и создавать новые комбинации известных визуальных и текстовых компонентов. Бенчмарк включает в себя тестирование глубокого рассуждения о взаимодействии объектов, подсчете и сложных композициях. Анализ с помощью MMCOMPOSITION выявил ограничения существующих VLM в точном композиционном восприятии и рассуждении.', 'categories': ['#multimodal', '#benchmark', '#nlp', '#cv'], 'emoji': '🧩', 'title': 'MMCOMPOSITION: новый взгляд на композиционность мультимодальных моделей', 'embedding': [0.04245588561227112, 0.022171679651734777, 0.06010498668144085, -0.02676780109352709, -0.0012509104844425176, -0.02917003788822309, 0.1194255667427114, 0.04934393853829258, 0.006943204084859213, 0.06103646505370603, 0.024353304827899953, -0.04292162978638929, -0.005156846238554416, 0.0015887252787314336, -0.06142866836379117, -0.020088106300688004, -0.047211339274686605, 0.023838538731611042, 0.09025553384247388, 0.06638021969086076, -0.00553066383383274, -0.0696649119380541, -0.05677126852168832, 0.06093841721897609, -0.031229101275781608, -0.029096500016981403, -0.08113682783253384, 0.01870314222763868, 0.005981083519647411, -0.06868440765322971, 0.11824895681245601, -0.04056841391626692, -0.059859861109033304, -0.01512429650028438, -0.017624586117695892, 0.11285618025313053, -0.026596210399569893, 0.017305921676449555, -0.020357744829375142, 0.08417639849770808, 0.02848368409076833, 0.04743195189081455, 0.02495386427597323, 0.060055960768881644, -0.005166038696918978, 0.03189094111935965, 0.04152440724000593, 0.05897740465893886, -0.09662881269142953, 0.11432693568277, -0.10520823765360686, 0.0811858497547046, 0.014805632857115733, 0.022992853711130225, -0.01527137284132602, -0.089373073401991, -0.011680272032467881, 0.0017097563543267186, -0.02671877418337078, -0.0307388481357723, -0.07633235224794761, -0.1285442587862919, -0.02077446109573988, 0.016435723699689136, -0.13883958071207, -0.08045048101825887, -0.04931942558201298, 0.006161864078216672, -0.03725920671759976, -0.07397913637782524, 0.03061628534956853, 0.011306453639111867, -0.019021806668885016, 0.00922900832761557, -0.09403047922967653, 0.003851548076297253, -0.01429086696034625, 0.009847952293379125, -0.05245704408191717, -0.040617439828826124, -0.040127186688816804, -0.03140068797935035, 0.010901995247522889, -0.03951437076260373, -0.06471335861579028, 0.0125504700092283, -0.040078160776257615, 0.04240686369010038, -0.09711907181702152, -0.08270563508729172, -0.09329509652687124, 0.009884720929720833, -0.00936995583102904, 0.06951784218115341, 0.09157921153443577, -0.05932058405165902, -0.036572853917742114, -0.10981661557353896, 0.035420760934154756, 0.0876081605018021, -0.12560275391259562, 0.01637444130899014, -0.0028572542060886206, -0.017710379469480262, 0.05083921091460011, -0.012869134450474638, 0.03544527389043435, 0.003039567073824636, -0.060055960768881644, -0.026302058914603157, -0.12246514139827412, -0.06054621390889095, -0.14638947547686376, 0.013629026119171085, 0.022772239099808057, -0.11560159344775535, -0.1087380614587904, 0.06696852266079424, -0.07098859761079288, -0.004504197057666123, -0.014646300436973143, 0.06500751808153393, 0.0753028200553698, -0.019377240544550756, 0.1001586305110304, -0.11089516968828753, -0.006998358036968889, 0.039587909631442535, 0.02872880966317587, 0.026204009084678983, -0.040151699645096406, 0.016546030007753103, 0.04189209559861725, -0.1004037600738264, -0.08584325757758435, -0.02527252871721957, -0.11079711985836335, -0.017538790770717294, -0.11628795223319568, 0.054467078564125145, -0.0251009400184566, 0.00935157111381934, 0.05647711504152735, -0.0038699325939875297, -0.05167264145213535, -0.02451263704852312, -0.0307388481357723, 0.015577781203471405, -0.1069731485586015, 0.05677126852168832, -0.06839026015865142, -0.14217329887182256, -0.04414725964362121, -0.002700986204760113, -0.055300509101660386, -0.12089632017715667, -0.01896052427818602, 0.07138079892568379, 0.08535300643276927, -0.07765603592549217, 0.08167610488990812, -0.033141087923259635, 0.05358462610441916, -0.0021969450755219425, 0.04875563357316487, 0.08466664964252316, -0.09741321731640558, 0.09260874173181935, -0.055888812071593866, -0.0851569047777267, 0.025419604459702937, -0.016815669534037358, -0.052015814859272834, -0.06691949674823505, 0.029586753156990716, -0.0863825326397644, -0.06775293127615874, -0.05206484077183203, -0.07795018342007046, -0.014964965277258326, -0.030346644825687163, 0.08755914257001982, -0.011049071189525682, 0.0030012659287967517, -0.06270332812397075, -0.02787086816455525, 0.049564551154420516, 0.07829336680317907, 0.020137130218052978, -0.014364404831587935, -0.03799458942040507, 0.07765603592549217, 0.10942441226345383, 0.0501038282117948, 0.09447170446193241, -0.0013359386792406256, 0.09123603613210406, 0.06304650153110823, 0.057408597404180986, -0.06064426573400935, 0.02264967631360429, 0.03051823352445013, -0.026228522040958584, -0.1720787084892827, -0.04203917333629484, 0.0377249488965237, 0.039171193365077794, -0.0010057841937650557, -0.09010845809999053, -0.004599183466373324, 0.005772726184542733, 0.03368036498063392, 0.16668592793956874, 0.033606826111795114, -0.03189094111935965, -0.10059986771445165, -0.09638368312863353, 0.04267650221878752, -0.03407256430033061, 0.01892375484376662, -0.05495733369932868, 0.020688665748761284, 0.1262891027220648, 0.006174120556356472, -0.08574520774766017, 0.019597853160678698, 0.05392779951155664, -0.05407487924442846, 0.01842124722081174, -0.08157805306478973, 0.06231112481388562, -0.08672571003729034, -0.095746364222112, -0.03983303520385007, -0.07093956970303945, -0.11550354361783118, 0.0286797857458109, -0.017894224646383034, 0.03703859410147182, 0.02137501854861893, 0.10207062513928535, 0.0689295372160257, 0.09349120017710802, 0.03944083189376493, -0.01258723884508943, 0.06991004150085009, -0.027135489452138402, -0.0759891768456159, 0.048829172442003674, -0.05211386668439123, 0.03652382800518291, -0.025517656284821336, -0.1142288918384285, 0.0347344081342971, -0.05358462610441916, 0.04422079851246002]}}, {'id': 'https://huggingface.co/papers/2410.09223', 'title': 'The Same But Different: Structural Similarities and Differences in Multilingual Language Modeling', 'url': 'https://huggingface.co/papers/2410.09223', 'abstract': 'We employ new tools from mechanistic interpretability in order to ask whether the internal structure of large language models (LLMs) shows correspondence to the linguistic structures which underlie the languages on which they are trained. In particular, we ask (1) when two languages employ the same morphosyntactic processes, do LLMs handle them using shared internal circuitry? and (2) when two languages require different morphosyntactic processes, do LLMs handle them using different internal circuitry? Using English and Chinese multilingual and monolingual models, we analyze the internal circuitry involved in two tasks. We find evidence that models employ the same circuit to handle the same syntactic process independently of the language in which it occurs, and that this is the case even for monolingual models trained completely independently. Moreover, we show that multilingual models employ language-specific components (attention heads and feed-forward networks) when needed to handle linguistic processes (e.g., morphological marking) that only exist in some languages. Together, our results provide new insights into how LLMs trade off between exploiting common structures and preserving linguistic differences when tasked with modeling multiple languages simultaneously.', 'score': 1, 'issue_id': 119, 'pub_date': '2024-10-11', 'pub_date_ru': '11 октября', 'data': {'desc': 'Исследователи применили методы механистической интерпретации для анализа внутренней структуры больших языковых моделей (LLM) и их соответствия лингвистическим структурам языков обучения. Они изучали, используют ли LLM общие внутренние схемы для обработки схожих морфосинтаксических процессов в разных языках, и различные схемы для обработки различающихся процессов. Анализ английских и китайских моно- и мультиязычных моделей показал, что модели используют одинаковые схемы для одинаковых синтаксических процессов независимо от языка, даже в случае независимо обученных моноязычных моделей. Кроме того, мультиязычные модели используют специфичные для языка компоненты при необходимости обработки лингвистических процессов, существующих только в некоторых языках.', 'categories': ['#nlp', '#benchmark', '#multimodal'], 'emoji': '🧠', 'title': 'Универсальность и специфичность языковых структур в LLM', 'embedding': [0.0020350001482737886, 0.04255320062279675, 0.11438225457864729, -0.01962279666258739, 0.060803688383786435, 0.03091792745705099, 0.08989187525481294, -0.0033779826660066794, -0.0939736054718405, 0.07952334131067879, 0.03915175910499562, -0.07342420998059263, -0.03887026040112023, 0.14553475670532537, -0.07286121257284185, -0.16289384014708908, 0.015928126916097102, -0.0025701406764659838, 0.028806687177985593, 0.003841282476003135, 0.06056910777916616, 0.004688710672083078, 0.06816956882713947, 0.08853129785636007, 0.050716659078520826, -0.07267354611081457, 0.04107533391119926, 0.025780577595073493, -0.025850952765625098, 0.035656483367015324, 0.07347112610151668, -0.02622628272218307, -0.10087031877095982, 0.0453681876615506, -0.04135683261507465, 0.13774663117365588, 0.03905792686314751, 0.06798190434344324, 0.031692044441463424, 0.05653429368306265, 0.01649112432384787, -0.08266674713089418, -0.07853810079294257, 0.0014522100329196041, -0.0804616716424841, 0.05235873320251803, -0.0016963220585038794, 0.06408783663178085, -0.07060922689850087, 0.1112857708143491, -0.07722443951541376, 0.07999251043324355, 0.08163458455724076, -0.08121233551226215, -0.022742739476513136, -0.007090245117052793, 0.02800910619811797, -0.008163458455724077, -0.03905792686314751, -0.042013658308011424, -0.026367032074120764, -0.1059372934623857, 0.009353962690204812, 0.040958040641392544, -0.08763988364547881, -0.010444768931265024, -0.11428841838013706, -0.04276432415612052, 0.008873069977819429, 0.023294005381119073, -0.01894250697419543, -0.01229210420017638, 0.06906098501635179, 0.06141360389079232, -0.0169251009153092, -0.05719112531099259, 0.02301250667724369, 0.07276737835266268, 0.020033314204421167, 0.018203573618396685, -0.013429827749159278, -0.03014380453764539, 0.03164512832053937, -0.09946282920824501, 0.006087406553266911, 0.013441556680473738, 0.08857821397728413, -0.0009933087452700382, -0.10603112570423381, -0.12151354452572476, 0.04987216098856361, 0.036078732411993934, 0.02662507370669965, -0.022637177709851245, 0.14103078931330554, 0.0016596686090509759, 0.03675902111122037, -0.04452368957159339, 0.04154449709877087, 0.06258652076221108, 0.012679164472880547, -0.006521383342561344, 0.0012359546890607994, -0.03633677206624176, 0.07328345963948941, -0.060616023900090216, 0.016549769969585706, -0.09209694480822987, -0.09256610997413252, 0.020244437737744946, -0.05432922215131468, 0.04672876110334137, 0.012432853749947178, -0.01673743643161298, 0.04431256603826962, -0.10743862318027285, -0.169555968884926, 0.05245256544436614, -0.07121913449218253, 0.015458963728525494, -0.024208876663296842, 0.004709236252425109, 0.0581294516861358, -0.0101163546999649, 0.07464403901627334, -0.03879988523056862, 0.08571631675259937, 0.0018414697943052885, -0.07267354611081457, -0.03633677206624176, -0.022238385736169142, 0.0413099164941506, -0.032395792190317414, -0.03002651324616972, -0.09570950985935478, -0.03823688782281785, -0.06648058056054926, 0.06094443872488966, -0.07150063319605793, 0.011371369738256568, 0.07877268535422495, -0.024396543125324115, 0.02416196054237279, 0.050153659692439, -0.053906973106336076, 0.04583734887079116, 0.034366281139114066, -0.0070374640358887445, -0.053578562238198735, 0.019247463738532842, -0.05428230603039062, -0.08623239606109503, 0.03640714723679337, 0.023024236202057468, -0.08459031798043573, -0.036923228523620094, -0.01962279666258739, 0.014168761500624232, 0.11241175771652642, -0.03124634228185043, 0.06380633990623652, 0.08341740506567907, 0.0169251009153092, 0.05705037694822042, -0.038143053602638685, 0.02568674535322538, -0.046470719470762485, 0.017886887329245496, -0.04607193145374249, -0.10087031877095982, -0.06390017214808463, -0.03488236440427184, -0.10040115162672611, -0.04093458357009604, 0.05855170073111441, -0.09115862238974876, -0.06563608247059204, -0.18090974928178952, 0.03579723370811855, -0.003820756697827999, -0.012819913626985134, 0.06816956882713947, -0.05446997249241789, 0.02700040267409209, -0.05127965648627159, -0.04051233452511744, -0.012092708806834643, 0.06798190434344324, -0.04358536319645018, -0.06774731978216085, -0.05798870134503258, 0.11635273759245077, 0.20117763222084467, 0.04011354452976639, 0.05742570789394392, -0.0592554484799684, 0.13671446662167136, 0.057003458848965315, -0.0074538475271280396, -0.021569828540087536, 0.0652607495465375, 0.07764668856039235, 0.05672196014508993, -0.07196979836196059, 0.10424829926080235, 0.14544092446347726, 0.007342420800226158, 0.08318282643938986, -0.033052623818247355, 0.011336182152980762, 0.030847552286499377, 0.005286894873777233, 0.0823852484270188, 0.05859861685203846, -0.03718126817786792, -0.019528963431573756, -0.0922376951493331, 0.006849798365193891, 0.010245374922755025, 0.06024069097603567, 0.04902766685526851, 0.024842249241599224, 0.08979804301296483, 0.06404092248918786, -0.08576323089519237, 0.11147343529804532, 0.02629665690356915, -0.11513291251343216, 0.062258101980749526, -0.09833683439274346, 0.06066294002101427, 0.040887667449171994, 0.010069437985541527, -0.03969129548478779, 0.02477187407104761, -0.07985175811380928, -0.047596718242926134, -0.05095124166147215, 0.03441319923836918, -0.05953694718384378, -0.016854725744757593, 0.02351686041758768, -0.002987989913182514, 0.013805159090548984, -0.01599850208664871, 0.03556265310349827, -0.0406061687452966, 0.019669712783511446, 0.08684230958976986, -0.001806282525562452, -0.04823008686456641, -0.04389031699329101, -0.06268035300405919, -0.005451102681843165, -0.06657441280239737, 0.0922846132485882]}}, {'id': 'https://huggingface.co/papers/2410.10630', 'title': 'Thinking LLMs: General Instruction Following with Thought Generation', 'url': 'https://huggingface.co/papers/2410.10630', 'abstract': 'LLMs are typically trained to answer user questions or follow instructions similarly to how human experts respond. However, in the standard alignment framework they lack the basic ability of explicit thinking before answering. Thinking is important for complex questions that require reasoning and planning -- but can be applied to any task. We propose a training method for equipping existing LLMs with such thinking abilities for general instruction following without use of additional human data. We achieve this by an iterative search and optimization procedure that explores the space of possible thought generations, allowing the model to learn how to think without direct supervision. For each instruction, the thought candidates are scored using a judge model to evaluate their responses only, and then optimized via preference optimization. We show that this procedure leads to superior performance on AlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning categories such as marketing, health and general knowledge, in addition to more traditional reasoning & problem-solving tasks.', 'score': 1, 'issue_id': 118, 'pub_date': '2024-10-14', 'pub_date_ru': '14 октября', 'data': {'desc': 'Статья представляет новый метод обучения языковых моделей (LLM) способности к явному мышлению перед ответом на вопросы. Авторы предлагают итеративную процедуру поиска и оптимизации, которая исследует пространство возможных генераций мыслей без использования дополнительных человеческих данных. Метод использует модель-судью для оценки ответов и оптимизацию на основе предпочтений. Результаты показывают улучшение производительности на наборах данных AlpacaEval и Arena-Hard, а также преимущества мышления в различных категориях задач.', 'categories': ['#nlp', '#rlhf', '#benchmark'], 'emoji': '🧠', 'title': 'Обучение ИИ думать перед ответом', 'embedding': [0.03197007614610142, 0.03288427482110727, 0.11572684175137632, -0.002505644638896035, 0.041542274632740246, -0.024118721569238232, 0.04272535943879108, 0.10287427295326611, 0.04931297146774543, 0.15971595284771364, 0.03237339825739535, -0.016388362208708512, 0.026068116159947266, -0.02215588115370405, -0.008153848184697507, -0.06119755829154918, 0.07039332631654438, -0.150358845046636, 0.07668516461353118, 0.12142713865751964, 0.03874590543024145, 0.010432623984163418, 0.029254368734188383, 0.001083930795653641, 0.015837154243612744, -0.013155054713361812, 0.04173049796101754, 0.008106793870901087, -0.04428487470648553, -0.01132665716091373, 0.2038125900553748, -0.0435588912623015, -0.10766037928185908, -0.00738753455374795, -0.013208831129825259, 0.13325794218202278, 0.012140025105659709, 0.03600330738086004, 0.041757383537576236, 0.049259194241536436, -0.04713502315955719, -0.0883546465812267, -0.05840118301595877, -0.006268313164452465, -0.027641075228103, -0.012025749765193008, -0.02715708727749558, 0.07544830865436297, -0.0006608616828470425, 0.0780295750251174, -0.00037349398014458433, 0.055174600052515746, 0.09276430827762894, 0.03242717548360435, -0.09556068355321934, -0.03151297883296237, -0.008073183964875108, 0.013531489831042336, 0.0397138813314563, 0.026901650007821555, 0.009874694118837015, -0.06469302738603719, -0.0010099883343564122, 0.00714554088209882, -0.04436553953361709, -0.11239270231115144, -0.11099451467335623, 0.025503462370026347, -0.009404149968690873, -0.0033055687831807317, -0.010439345479521285, -0.07845978878606162, 0.06162777002812954, 0.004197921333046093, -0.032991829273525265, 0.04423109545591267, 0.12411595948069205, 0.03758971227384093, 0.051006924740052695, 0.02304319222778733, 0.02663276792550431, 0.01066789555314552, 0.09416249591542414, -0.07136130221775921, 0.08426763812025909, 0.06974800365076417, 0.022142437353242766, -0.027116756888293667, -0.10083077074714618, -0.1506815104282538, -0.075125645297109, -0.046839255500681264, 0.0012276147381012439, 0.10050811751171156, 0.09453893245015937, 0.026646211725965593, 0.06334861697445097, -0.025826121678552586, 0.11411354520874513, 0.01258368064270135, -0.07119996851476837, -0.009773862578831597, 0.06512323912261755, -0.0906670349183916, 0.059691816201335984, 0.02133579110629104, -0.04315557117537144, -0.03320693615399738, -0.03933744479672106, 0.04882898250495608, -0.1732676114159937, -0.04468819681777946, -0.04057430480461702, -0.08459029540442144, 0.0737812348855228, -0.13981867268314618, -0.0780295750251174, 0.017531109540283883, -0.02610844756133111, -0.0758785224153072, -0.06291840321350675, -0.0418380483647078, 0.0056700502782511195, 0.015460718721059447, -0.015205280439203486, -0.02183322386954168, 0.11669481157949955, -0.05829363261226852, 0.06404770674462083, -0.03998276138940967, 0.0058414626816420045, 0.021900442871848086, 0.06770450549337197, -0.00861094752220043, -0.11120962357819221, 0.04388155259519161, -0.08168637377386853, 0.004130700306375815, -0.0364872963436494, -0.11271535757094993, -0.01685890433449078, 0.014613740060541943, -0.02936192217442444, 0.07636250125627721, -0.08313833054041725, 0.007622806527602827, 0.017746215408574063, 0.03049122570553853, -0.009162155487296195, 0.007273259618154025, -0.0729745926872988, -0.0450108561263057, -0.04724257963633906, 0.060122027937916336, -0.03205074097323298, -0.003498827716584878, -0.11099451467335623, 0.03672928880068021, 0.04294046227053546, -0.1036271460227366, 0.000273083345875037, 0.08496672789042893, 0.04670481142297683, 0.09932502865693298, 0.005586024602222433, 0.059960698283653226, 0.027372193145785755, 0.047753454175687104, -0.09233410058977631, -0.03656796117078096, -0.04229514770221072, -0.057325654686689806, -0.04019786624551792, -0.06173532043181979, 0.045225963006777815, -0.0867413500385955, -0.02415905195844014, -0.046005721652806975, -0.055282150456206, -0.022612980491206972, -0.11820055366971274, 0.027667964853389432, -0.11626460186728307, 0.003972732310755371, -0.045790612747970985, 0.04796856105615922, -0.04275224703971364, -0.012543348229135571, -0.06404770674462083, 0.09088213572577211, -0.02987279671377249, 0.020878691768788118, 0.13196731911846493, 0.012805507803913008, 0.020945912795458398, 0.019668721386178596, 0.106208412393491, 0.007763969873864862, 0.004933986109303187, -0.007649694533398164, 0.049823843982729606, 0.0030938241686604526, 0.07028577186412639, -0.11561928729895833, 0.04939363224614925, 0.09609844569348995, -0.001365416729375185, 0.03457823416650615, -0.08416008366784107, -0.10239028601484064, 0.09631355459832594, -0.08803198322397271, 0.028125064190892356, 0.09195766607940495, -0.09937880790750585, 0.018203314746076987, -0.04818366591226747, -0.003760987696235092, -0.04060119240553958, 0.05098003713913014, -0.08840842380743569, -0.004221448489944303, 0.04269846981350465, 0.0009562118369184091, -0.14885311510260602, -0.03202385337231042, 0.0009385664490011127, -0.06350994257998635, -0.003952566407627062, -0.07130752499155023, 0.02895859803876664, -0.05533592768241499, -0.0661449841525859, -0.04068185520830727, 0.01711434261634674, -0.02909303806774333, -0.06264951910682563, -0.04240270215462871, 0.04509152297780113, -0.013726429391331433, 0.06302595766592474, -0.03721327776346957, 0.04831810391688028, -0.0559812442751036, -0.010197352415181316, 0.06947912359281079, -0.03879968265645045, -0.04659726101928658, 0.08039573249103583, -0.02243820906084644, -0.03541176801438043, -0.07001689180617302, 0.05428728897843246, -0.07243682447393658, -0.09539935592332009, -0.00489365349330102]}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'default';        

        function loadSettings() {
            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            const themeToggle = document.getElementById('theme-toggle');
            let settingSortBy = localStorage.getItem('sort_by');
            const sortDropdown = document.getElementById('sort-dropdown');
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "хф найтли";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'default';
            }
            
            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            return Array.from(categories);
        }
        
        function createCategoryButtons() {
            const categories = getUniqueCategories(articlesData);
            categories.forEach(category => {
                const button = document.createElement('span');
                button.textContent = category;
                button.className = 'category-button';
                button.onclick = () => toggleCategory(category, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if (selectedArticles.length === articlesData.length) {
                categoryToggle.textContent = '🏷️ Фильтр';
            } else {
                categoryToggle.textContent = `🏷️ Фильтр (${formatArticlesTitle(selectedArticles.length)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const savedCategories = localStorage.getItem('selectedCategories');
            if (savedCategories) {
                if (savedCategories != '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);
                    updateCategoryButtonStates();
                }
            }
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles = selectedCategories.length === 0
                ? articlesData
                : articlesData.filter(article => 
                    article.data && article.data.categories && 
                    article.data.categories.some(cat => selectedCategories.includes(cat))
                );

            console.log('filteredArticles', filteredArticles)

            if (filteredArticles.length === 0) {
                selectedArticles = articlesData;
                selectedCategories = [];
                cleanCategorySelection();
            } else {
                selectedArticles = filteredArticles;
            }

            console.log('selectedArticles', selectedArticles)

            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
        }

        function renderArticles(articles) {
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                const explanation = item["data"]["desc"];
                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${item['data']['title']}</p>
                            <p class="pub-date">📅 Статья от ${item['pub_date_ru']}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">Статья</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            }
            if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
        });

        clearCategoriesButton.addEventListener('click', clearAllCategories);
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiffRu('2024-10-15 16:15');
        } 

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();  
    </script>
</body>
</html>
    