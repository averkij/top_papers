
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 24 papers. March 18.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">18 марта</span> | <span id="title-articles-count">24 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-03-17.html">⬅️ <span id="prev-date">17.03</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-03-19.html">➡️ <span id="next-date">19.03</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-03.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'};
        let feedDateNext = {'ru': '19.03', 'en': '03/19', 'zh': '3月19日'};
        let feedDatePrev = {'ru': '17.03', 'en': '03/17', 'zh': '3月17日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.06053', 'title': 'DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal\n  Consistent Video Generation', 'url': 'https://huggingface.co/papers/2503.06053', 'abstract': 'Spatio-temporal consistency is a critical research topic in video generation. A qualified generated video segment must ensure plot plausibility and coherence while maintaining visual consistency of objects and scenes across varying viewpoints. Prior research, especially in open-source projects, primarily focuses on either temporal or spatial consistency, or their basic combination, such as appending a description of a camera movement after a prompt without constraining the outcomes of this movement. However, camera movement may introduce new objects to the scene or eliminate existing ones, thereby overlaying and affecting the preceding narrative. Especially in videos with numerous camera movements, the interplay between multiple plots becomes increasingly complex. This paper introduces and examines integral spatio-temporal consistency, considering the synergy between plot progression and camera techniques, and the long-term impact of prior content on subsequent generation. Our research encompasses dataset construction through to the development of the model. Initially, we constructed a DropletVideo-10M dataset, which comprises 10 million videos featuring dynamic camera motion and object actions. Each video is annotated with an average caption of 206 words, detailing various camera movements and plot developments. Following this, we developed and trained the DropletVideo model, which excels in preserving spatio-temporal coherence during video generation. The DropletVideo dataset and model are accessible at https://dropletx.github.io.', 'score': 72, 'issue_id': 2758, 'pub_date': '2025-03-08', 'pub_date_card': {'ru': '8 марта', 'en': 'March 8', 'zh': '3月8日'}, 'hash': 'c6a544d3dc36bfbf', 'authors': ['Runze Zhang', 'Guoguang Du', 'Xiaochuan Li', 'Qi Jia', 'Liang Jin', 'Lu Liu', 'Jingjing Wang', 'Cong Xu', 'Zhenhua Guo', 'Yaqian Zhao', 'Xiaoli Gong', 'Rengang Li', 'Baoyu Fan'], 'affiliations': ['IEIT System Co., Ltd.', 'Nankai University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.06053.jpg', 'data': {'categories': ['#open_source', '#dataset', '#video', '#training'], 'emoji': '🎥', 'ru': {'title': 'Интегральная пространственно-временная согласованность в генерации видео', 'desc': 'Статья посвящена проблеме пространственно-временной согласованности в генерации видео. Авторы представляют новый подход, учитывающий взаимосвязь между развитием сюжета и движениями камеры, а также долгосрочное влияние предыдущего контента на последующую генерацию. Для исследования был создан датасет DropletVideo-10M, содержащий 10 миллионов видео с динамическим движением камеры и действиями объектов. На основе этих данных была разработана и обучена модель DropletVideo, способная сохранять пространственно-временную согласованность при генерации видео.'}, 'en': {'title': 'Achieving Seamless Video Generation with Spatio-Temporal Consistency', 'desc': 'This paper addresses the challenge of spatio-temporal consistency in video generation, which is essential for creating coherent and visually consistent narratives. It highlights the limitations of previous research that often focuses on either temporal or spatial aspects without integrating them effectively. The authors introduce a new dataset, DropletVideo-10M, containing 10 million videos with dynamic camera movements and detailed annotations, which aids in training their model. The proposed DropletVideo model demonstrates improved performance in maintaining coherence across both plot progression and camera techniques, ensuring a more seamless video generation experience.'}, 'zh': {'title': '提升视频生成的时空一致性', 'desc': '本论文研究了视频生成中的时空一致性问题。生成的视频片段需要在情节合理性和视觉一致性之间取得平衡，尤其是在不同视角下的物体和场景。以往的研究主要关注时间或空间一致性，缺乏对两者的综合考虑。我们提出了整体时空一致性的方法，构建了包含1000万段动态镜头和物体动作的视频数据集，并开发了DropletVideo模型，以提高视频生成的时空连贯性。'}}}, {'id': 'https://huggingface.co/papers/2503.12533', 'title': 'Being-0: A Humanoid Robotic Agent with Vision-Language Models and\n  Modular Skills', 'url': 'https://huggingface.co/papers/2503.12533', 'abstract': "Building autonomous robotic agents capable of achieving human-level performance in real-world embodied tasks is an ultimate goal in humanoid robot research. Recent advances have made significant progress in high-level cognition with Foundation Models (FMs) and low-level skill development for humanoid robots. However, directly combining these components often results in poor robustness and efficiency due to compounding errors in long-horizon tasks and the varied latency of different modules. We introduce Being-0, a hierarchical agent framework that integrates an FM with a modular skill library. The FM handles high-level cognitive tasks such as instruction understanding, task planning, and reasoning, while the skill library provides stable locomotion and dexterous manipulation for low-level control. To bridge the gap between these levels, we propose a novel Connector module, powered by a lightweight vision-language model (VLM). The Connector enhances the FM's embodied capabilities by translating language-based plans into actionable skill commands and dynamically coordinating locomotion and manipulation to improve task success. With all components, except the FM, deployable on low-cost onboard computation devices, Being-0 achieves efficient, real-time performance on a full-sized humanoid robot equipped with dexterous hands and active vision. Extensive experiments in large indoor environments demonstrate Being-0's effectiveness in solving complex, long-horizon tasks that require challenging navigation and manipulation subtasks. For further details and videos, visit https://beingbeyond.github.io/being-0.", 'score': 48, 'issue_id': 2755, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '18b42d1274f6b260', 'authors': ['Haoqi Yuan', 'Yu Bai', 'Yuhui Fu', 'Bohan Zhou', 'Yicheng Feng', 'Xinrun Xu', 'Yi Zhan', 'Börje F. Karlsson', 'Zongqing Lu'], 'affiliations': ['BAAI', 'BeingBeyond', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12533.jpg', 'data': {'categories': ['#reasoning', '#agents', '#optimization', '#robotics', '#agi', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Being-0: Мост между AI и роботом для решения сложных задач в реальном мире', 'desc': 'Статья представляет Being-0 - иерархическую систему для автономных роботов-гуманоидов, объединяющую фундаментальную модель (ФМ) для высокоуровневых когнитивных задач с библиотекой низкоуровневых навыков. Ключевым элементом является модуль Connector на основе легковесной мультимодальной модели, связывающий ФМ с исполнительными механизмами робота. Система Being-0 способна эффективно решать сложные долгосрочные задачи, требующие навигации и манипуляций, работая в режиме реального времени на недорогом бортовом оборудовании. Эксперименты показали эффективность Being-0 в решении комплексных задач в больших помещениях.'}, 'en': {'title': 'Bridging High-Level Cognition and Low-Level Skills in Humanoid Robots', 'desc': "This paper presents Being-0, a hierarchical framework designed to enhance humanoid robots' performance in complex tasks. It combines a Foundation Model (FM) for high-level cognitive functions with a modular skill library for low-level control, addressing issues of robustness and efficiency. A novel Connector module, utilizing a lightweight vision-language model, translates language-based plans into actionable commands, improving coordination between locomotion and manipulation. The system demonstrates effective real-time performance on humanoid robots in challenging environments, showcasing its ability to tackle long-horizon tasks successfully."}, 'zh': {'title': '提升类人机器人智能的层次化框架', 'desc': '本文介绍了一种名为Being-0的层次化智能体框架，旨在提升类人机器人在现实世界任务中的表现。该框架将基础模型（FM）与模块化技能库相结合，FM负责高层次的认知任务，如指令理解和任务规划，而技能库则提供稳定的运动和灵巧操作。为了连接这两个层次，本文提出了一种新颖的连接模块，利用轻量级的视觉-语言模型（VLM）将语言计划转化为可执行的技能命令。通过在低成本的计算设备上部署大部分组件，Being-0在复杂的长时间任务中展现出高效的实时性能。'}}}, {'id': 'https://huggingface.co/papers/2503.12885', 'title': 'DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale\n  Text-to-Image Models', 'url': 'https://huggingface.co/papers/2503.12885', 'abstract': 'Image-conditioned generation methods, such as depth- and canny-conditioned approaches, have demonstrated remarkable abilities for precise image synthesis. However, existing models still struggle to accurately control the content of multiple instances (or regions). Even state-of-the-art models like FLUX and 3DIS face challenges, such as attribute leakage between instances, which limits user control. To address these issues, we introduce DreamRenderer, a training-free approach built upon the FLUX model. DreamRenderer enables users to control the content of each instance via bounding boxes or masks, while ensuring overall visual harmony. We propose two key innovations: 1) Bridge Image Tokens for Hard Text Attribute Binding, which uses replicated image tokens as bridge tokens to ensure that T5 text embeddings, pre-trained solely on text data, bind the correct visual attributes for each instance during Joint Attention; 2) Hard Image Attribute Binding applied only to vital layers. Through our analysis of FLUX, we identify the critical layers responsible for instance attribute rendering and apply Hard Image Attribute Binding only in these layers, using soft binding in the others. This approach ensures precise control while preserving image quality. Evaluations on the COCO-POS and COCO-MIG benchmarks demonstrate that DreamRenderer improves the Image Success Ratio by 17.7% over FLUX and enhances the performance of layout-to-image models like GLIGEN and 3DIS by up to 26.8%. Project Page: https://limuloo.github.io/DreamRenderer/.', 'score': 34, 'issue_id': 2754, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': 'e6332652493dc1ab', 'authors': ['Dewei Zhou', 'Mingwei Li', 'Zongxin Yang', 'Yi Yang'], 'affiliations': ['DBMI, HMS, Harvard University', 'RELER, CCAI, Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12885.jpg', 'data': {'categories': ['#optimization', '#cv', '#training', '#games', '#benchmark'], 'emoji': '🎨', 'ru': {'title': 'Точный контроль над множественными объектами при генерации изображений', 'desc': 'DreamRenderer - это новый подход к генерации изображений по условиям, построенный на основе модели FLUX. Он позволяет точно контролировать содержимое нескольких объектов на изображении с помощью ограничивающих рамок или масок. Ключевые инновации включают использование мостовых токенов изображения для жесткой привязки текстовых атрибутов и применение жесткой привязки атрибутов изображения только в критически важных слоях. Эксперименты показали значительное улучшение качества генерации по сравнению с существующими методами.'}, 'en': {'title': 'DreamRenderer: Precise Control in Image Generation', 'desc': 'This paper presents DreamRenderer, a novel approach for image generation that allows precise control over multiple instances in an image. Unlike existing models, DreamRenderer uses a training-free method that leverages the FLUX model to bind visual attributes to specific instances through bounding boxes or masks. The key innovations include Bridge Image Tokens for ensuring accurate text-to-image attribute mapping and Hard Image Attribute Binding focused on critical layers for instance rendering. Evaluations show that DreamRenderer significantly outperforms FLUX and enhances other layout-to-image models, demonstrating its effectiveness in generating high-quality images with user-defined content.'}, 'zh': {'title': 'DreamRenderer：精确控制图像生成的创新方法', 'desc': '本文介绍了一种名为DreamRenderer的图像生成方法，旨在解决现有模型在多实例内容控制方面的不足。DreamRenderer基于FLUX模型，允许用户通过边界框或掩码精确控制每个实例的内容，同时保持整体视觉和谐。我们提出了两项关键创新：一是使用桥接图像标记来确保文本属性的准确绑定，二是在关键层中应用硬图像属性绑定，以提高实例属性渲染的精度。实验结果表明，DreamRenderer在图像成功率上比FLUX提高了17.7%，并且在布局到图像模型的性能上提升了多达26.8%。'}}}, {'id': 'https://huggingface.co/papers/2503.12590', 'title': 'Personalize Anything for Free with Diffusion Transformer', 'url': 'https://huggingface.co/papers/2503.12590', 'abstract': 'Personalized image generation aims to produce images of user-specified concepts while enabling flexible editing. Recent training-free approaches, while exhibit higher computational efficiency than training-based methods, struggle with identity preservation, applicability, and compatibility with diffusion transformers (DiTs). In this paper, we uncover the untapped potential of DiT, where simply replacing denoising tokens with those of a reference subject achieves zero-shot subject reconstruction. This simple yet effective feature injection technique unlocks diverse scenarios, from personalization to image editing. Building upon this observation, we propose Personalize Anything, a training-free framework that achieves personalized image generation in DiT through: 1) timestep-adaptive token replacement that enforces subject consistency via early-stage injection and enhances flexibility through late-stage regularization, and 2) patch perturbation strategies to boost structural diversity. Our method seamlessly supports layout-guided generation, multi-subject personalization, and mask-controlled editing. Evaluations demonstrate state-of-the-art performance in identity preservation and versatility. Our work establishes new insights into DiTs while delivering a practical paradigm for efficient personalization.', 'score': 22, 'issue_id': 2754, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '25e3ee07c4a11ed2', 'authors': ['Haoran Feng', 'Zehuan Huang', 'Lin Li', 'Hairong Lv', 'Lu Sheng'], 'affiliations': ['Beihang University', 'Renmin University of China', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12590.jpg', 'data': {'categories': ['#cv', '#multimodal', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Персонализация изображений без обучения: новый взгляд на возможности диффузионных трансформеров', 'desc': 'Статья представляет новый подход к персонализированной генерации изображений с использованием диффузионных трансформеров (DiT). Авторы предлагают метод Personalize Anything, который позволяет достичь высокого качества персонализации без дополнительного обучения модели. Ключевые элементы метода включают адаптивную замену токенов и стратегии возмущения патчей для улучшения структурного разнообразия. Метод демонстрирует высокую эффективность в сохранении идентичности и универсальность применения.'}, 'en': {'title': 'Personalize Anything: Efficient Image Generation with Diffusion Transformers', 'desc': 'This paper presents a novel approach to personalized image generation using diffusion transformers (DiTs) without the need for extensive training. The authors introduce a technique that replaces denoising tokens with those from a reference subject, enabling effective zero-shot subject reconstruction. They propose a framework called Personalize Anything, which incorporates adaptive token replacement and patch perturbation strategies to enhance identity preservation and structural diversity. The results show that this method achieves state-of-the-art performance in generating personalized images while allowing for flexible editing options.'}, 'zh': {'title': '个性化图像生成的新视角', 'desc': '个性化图像生成旨在根据用户指定的概念生成图像，并允许灵活编辑。最近的无训练方法在计算效率上优于基于训练的方法，但在身份保持、适用性和与扩散变换器的兼容性方面存在挑战。本文揭示了扩散变换器的潜力，通过简单地用参考对象的去噪令牌替换实现零-shot的对象重建。我们提出的“个性化任何事物”框架，通过时间步自适应令牌替换和补丁扰动策略，实现了高效的个性化图像生成。'}}}, {'id': 'https://huggingface.co/papers/2503.13327', 'title': 'Edit Transfer: Learning Image Editing via Vision In-Context Relations', 'url': 'https://huggingface.co/papers/2503.13327', 'abstract': 'We introduce a new setting, Edit Transfer, where a model learns a transformation from just a single source-target example and applies it to a new query image. While text-based methods excel at semantic manipulations through textual prompts, they often struggle with precise geometric details (e.g., poses and viewpoint changes). Reference-based editing, on the other hand, typically focuses on style or appearance and fails at non-rigid transformations. By explicitly learning the editing transformation from a source-target pair, Edit Transfer mitigates the limitations of both text-only and appearance-centric references. Drawing inspiration from in-context learning in large language models, we propose a visual relation in-context learning paradigm, building upon a DiT-based text-to-image model. We arrange the edited example and the query image into a unified four-panel composite, then apply lightweight LoRA fine-tuning to capture complex spatial transformations from minimal examples. Despite using only 42 training samples, Edit Transfer substantially outperforms state-of-the-art TIE and RIE methods on diverse non-rigid scenarios, demonstrating the effectiveness of few-shot visual relation learning.', 'score': 21, 'issue_id': 2754, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '24a5e5d1d86949d5', 'authors': ['Lan Chen', 'Qi Mao', 'Yuchao Gu', 'Mike Zheng Shou'], 'affiliations': ['MIPG, Communication University of China', 'Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.13327.jpg', 'data': {'categories': ['#transfer_learning', '#dataset', '#cv', '#training'], 'emoji': '🖼️', 'ru': {'title': 'Передача редактирования: обучение визуальным преобразованиям по одному примеру', 'desc': "Статья представляет новый подход в машинном обучении под названием 'Edit Transfer'. Этот метод позволяет модели изучать преобразование на основе всего одного примера исходного и целевого изображения, а затем применять его к новому запросу. Авторы предлагают парадигму обучения визуальным отношениям в контексте, основанную на модели преобразования текста в изображение DiT. Несмотря на использование всего 42 обучающих образцов, Edit Transfer значительно превосходит современные методы TIE и RIE в различных сценариях нежесткого преобразования."}, 'en': {'title': 'Transforming Images with Just One Example!', 'desc': 'This paper presents a novel approach called Edit Transfer, which enables a model to learn how to transform images using just one example of a source and target image. Unlike traditional text-based methods that struggle with geometric details and reference-based methods that focus on style, Edit Transfer effectively captures complex spatial transformations. The method utilizes a visual relation in-context learning paradigm, inspired by large language models, and employs a DiT-based text-to-image model. Remarkably, it achieves superior performance in non-rigid scenarios with only 42 training samples, showcasing the power of few-shot learning in visual transformations.'}, 'zh': {'title': '编辑转移：少样本学习的突破', 'desc': '我们提出了一种新的设置，称为编辑转移（Edit Transfer），模型通过单一的源-目标示例学习变换，并将其应用于新的查询图像。与文本方法在语义操作上表现优异但在几何细节上存在困难不同，编辑转移通过明确学习源-目标对的编辑变换，克服了文本和外观参考的局限性。我们借鉴大型语言模型中的上下文学习，提出了一种视觉关系上下文学习范式，并在DiT基础的文本到图像模型上进行构建。尽管仅使用42个训练样本，编辑转移在多样的非刚性场景中显著超越了现有的最先进方法，展示了少样本视觉关系学习的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.12349', 'title': 'SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?', 'url': 'https://huggingface.co/papers/2503.12349', 'abstract': 'Reasoning and strategic behavior in social interactions is a hallmark of intelligence. This form of reasoning is significantly more sophisticated than isolated planning or reasoning tasks in static settings (e.g., math problem solving). In this paper, we present Strategic Planning, Interaction, and Negotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the intelligence of strategic planning and social reasoning. While many existing benchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench combines classical PDDL tasks, competitive board games, cooperative card games, and multi-agent negotiation scenarios in one unified framework. The framework includes both a benchmark as well as an arena to simulate and evaluate the variety of social settings to test reasoning and strategic behavior of AI agents. We formulate the benchmark SPIN-Bench by systematically varying action spaces, state complexity, and the number of interacting agents to simulate a variety of social settings where success depends on not only methodical and step-wise decision making, but also conceptual inference of other (adversarial or cooperative) participants. Our experiments reveal that while contemporary LLMs handle basic fact retrieval and short-range planning reasonably well, they encounter significant performance bottlenecks in tasks requiring deep multi-hop reasoning over large state spaces and socially adept coordination under uncertainty. We envision SPIN-Bench as a catalyst for future research on robust multi-agent planning, social reasoning, and human--AI teaming.', 'score': 21, 'issue_id': 2765, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '5f862770d0575514', 'authors': ['Jianzhu Yao', 'Kevin Wang', 'Ryan Hsieh', 'Haisu Zhou', 'Tianqing Zou', 'Zerui Cheng', 'Zhangyang Wang', 'Pramod Viswanath'], 'affiliations': ['Department of Electrical and Computer Engineering, Princeton University, New Jersey, US', 'Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, US'], 'pdf_title_img': 'assets/pdf/title_img/2503.12349.jpg', 'data': {'categories': ['#games', '#agents', '#benchmark', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'SPIN-Bench: Оценка стратегического и социального интеллекта ИИ', 'desc': 'SPIN-Bench - это новый многодоменный фреймворк для оценки стратегического планирования и социального мышления искусственного интеллекта. Он объединяет классические задачи PDDL, конкурентные настольные игры, кооперативные карточные игры и сценарии многоагентных переговоров. SPIN-Bench систематически варьирует пространства действий, сложность состояний и количество взаимодействующих агентов для симуляции различных социальных ситуаций. Эксперименты показывают, что современные языковые модели справляются с базовым извлечением фактов и краткосрочным планированием, но испытывают трудности с задачами, требующими глубокого многоступенчатого рассуждения и социально адаптивной координации в условиях неопределенности.'}, 'en': {'title': "SPIN-Bench: Evaluating AI's Strategic and Social Intelligence", 'desc': "This paper introduces SPIN-Bench, a new evaluation framework designed to assess AI's ability in strategic planning and social reasoning across various domains. Unlike existing benchmarks that focus on single-agent tasks, SPIN-Bench integrates multiple scenarios, including competitive games and cooperative interactions, to provide a comprehensive testing ground. The framework systematically varies factors like action spaces and the number of agents to simulate complex social environments where success relies on both strategic decision-making and understanding others' behaviors. The findings indicate that while current large language models perform well in basic tasks, they struggle with complex reasoning and coordination in uncertain social contexts."}, 'zh': {'title': '智能推理与战略行为的新基准', 'desc': '本文介绍了一种新的多领域评估工具，称为战略规划、互动与谈判基准（SPIN-Bench），旨在测量战略规划和社会推理的智能水平。与现有的基准不同，SPIN-Bench结合了经典的PDDL任务、竞争性棋盘游戏、合作性纸牌游戏和多智能体谈判场景，提供了一个统一的框架。该框架不仅包括基准测试，还提供了一个模拟和评估各种社会环境的场所，以测试人工智能代理的推理和战略行为。实验结果表明，尽管当前的大型语言模型在基本事实检索和短期规划方面表现良好，但在需要深度多跳推理和不确定性下的社会协调任务中却遇到了显著的性能瓶颈。'}}}, {'id': 'https://huggingface.co/papers/2503.13434', 'title': 'BlobCtrl: A Unified and Flexible Framework for Element-level Image\n  Generation and Editing', 'url': 'https://huggingface.co/papers/2503.13434', 'abstract': 'Element-level visual manipulation is essential in digital content creation, but current diffusion-based methods lack the precision and flexibility of traditional tools. In this work, we introduce BlobCtrl, a framework that unifies element-level generation and editing using a probabilistic blob-based representation. By employing blobs as visual primitives, our approach effectively decouples and represents spatial location, semantic content, and identity information, enabling precise element-level manipulation. Our key contributions include: 1) a dual-branch diffusion architecture with hierarchical feature fusion for seamless foreground-background integration; 2) a self-supervised training paradigm with tailored data augmentation and score functions; and 3) controllable dropout strategies to balance fidelity and diversity. To support further research, we introduce BlobData for large-scale training and BlobBench for systematic evaluation. Experiments show that BlobCtrl excels in various element-level manipulation tasks while maintaining computational efficiency, offering a practical solution for precise and flexible visual content creation. Project page: https://liyaowei-stu.github.io/project/BlobCtrl/', 'score': 18, 'issue_id': 2758, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': 'c8ae2d6baee8bf12', 'authors': ['Yaowei Li', 'Lingen Li', 'Zhaoyang Zhang', 'Xiaoyu Li', 'Guangzhi Wang', 'Hongxiang Li', 'Xiaodong Cun', 'Ying Shan', 'Yuexian Zou'], 'affiliations': ['ARC Lab, Tencent PCG', 'Peking University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.13434.jpg', 'data': {'categories': ['#benchmark', '#diffusion', '#cv', '#open_source', '#dataset', '#training'], 'emoji': '🎨', 'ru': {'title': 'Точное управление элементами изображения с помощью блобов', 'desc': "BlobCtrl - это новая система для точного управления элементами изображения в цифровом контенте. Она использует вероятностное представление на основе 'блобов', что позволяет отделить пространственное положение, семантическое содержание и информацию об идентичности объектов. Архитектура включает двухветвевую диффузионную модель с иерархическим слиянием признаков и самоконтролируемое обучение. BlobCtrl превосходит существующие методы в задачах манипуляции элементами изображения, сохраняя при этом вычислительную эффективность."}, 'en': {'title': 'Precision and Flexibility in Visual Manipulation with BlobCtrl', 'desc': 'BlobCtrl is a new framework designed for element-level visual manipulation in digital content creation, addressing the limitations of current diffusion-based methods. It uses a probabilistic blob-based representation to separate and manage spatial location, semantic content, and identity, allowing for precise editing of visual elements. The framework features a dual-branch diffusion architecture that integrates foreground and background seamlessly, along with a self-supervised training approach that enhances model performance through tailored data augmentation. Additionally, BlobCtrl introduces BlobData for extensive training and BlobBench for evaluation, demonstrating superior efficiency and effectiveness in various manipulation tasks.'}, 'zh': {'title': 'BlobCtrl：精确灵活的视觉内容创作新方法', 'desc': '本文介绍了一种名为BlobCtrl的框架，旨在提高数字内容创作中元素级视觉操作的精确性和灵活性。该框架采用基于概率的blob表示，能够有效解耦空间位置、语义内容和身份信息，从而实现精确的元素级操作。我们提出了双分支扩散架构和自监督训练范式，以增强前景和背景的无缝集成，并引入可控的dropout策略来平衡保真度和多样性。实验结果表明，BlobCtrl在多种元素级操作任务中表现优异，同时保持计算效率，为精确和灵活的视觉内容创作提供了实用解决方案。'}}}, {'id': 'https://huggingface.co/papers/2503.13435', 'title': 'WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range\n  Movements and Scenes', 'url': 'https://huggingface.co/papers/2503.13435', 'abstract': 'With the rapid development of 3D reconstruction technology, research in 4D reconstruction is also advancing, existing 4D reconstruction methods can generate high-quality 4D scenes. However, due to the challenges in acquiring multi-view video data, the current 4D reconstruction benchmarks mainly display actions performed in place, such as dancing, within limited scenarios. In practical scenarios, many scenes involve wide-range spatial movements, highlighting the limitations of existing 4D reconstruction datasets. Additionally, existing 4D reconstruction methods rely on deformation fields to estimate the dynamics of 3D objects, but deformation fields struggle with wide-range spatial movements, which limits the ability to achieve high-quality 4D scene reconstruction with wide-range spatial movements. In this paper, we focus on 4D scene reconstruction with significant object spatial movements and propose a novel 4D reconstruction benchmark, WideRange4D. This benchmark includes rich 4D scene data with large spatial variations, allowing for a more comprehensive evaluation of the generation capabilities of 4D generation methods. Furthermore, we introduce a new 4D reconstruction method, Progress4D, which generates stable and high-quality 4D results across various complex 4D scene reconstruction tasks. We conduct both quantitative and qualitative comparison experiments on WideRange4D, showing that our Progress4D outperforms existing state-of-the-art 4D reconstruction methods. Project: https://github.com/Gen-Verse/WideRange4D', 'score': 16, 'issue_id': 2754, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': 'c17d4be710ed24e0', 'authors': ['Ling Yang', 'Kaixin Zhu', 'Juanxi Tian', 'Bohan Zeng', 'Mingbao Lin', 'Hongjuan Pei', 'Wentao Zhang', 'Shuicheng Yan'], 'affiliations': ['National University of Singapore', 'Peking University', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.13435.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#3d'], 'emoji': '🌀', 'ru': {'title': 'Прорыв в 4D-реконструкции: от статики к динамике', 'desc': 'Статья представляет новый бенчмарк WideRange4D для 4D-реконструкции сцен с широким диапазоном пространственных движений. Авторы также предлагают новый метод 4D-реконструкции под названием Progress4D, который генерирует стабильные и высококачественные 4D-результаты для различных сложных задач реконструкции 4D-сцен. Метод Progress4D превосходит существующие передовые методы 4D-реконструкции в количественных и качественных сравнительных экспериментах на WideRange4D. Работа направлена на преодоление ограничений существующих методов 4D-реконструкции, которые плохо справляются с широкомасштабными пространственными движениями.'}, 'en': {'title': 'Advancing 4D Reconstruction with Wide Spatial Movements', 'desc': 'This paper addresses the limitations of current 4D reconstruction methods, which primarily focus on actions performed in place and struggle with wide-range spatial movements. The authors introduce a new benchmark called WideRange4D, which includes diverse 4D scene data featuring significant spatial variations, enabling better evaluation of 4D generation techniques. They also propose a novel reconstruction method named Progress4D, designed to produce stable and high-quality 4D results across complex scenarios. Experimental results demonstrate that Progress4D surpasses existing state-of-the-art methods in both quantitative and qualitative assessments.'}, 'zh': {'title': '突破空间限制，实现高质量4D重建', 'desc': '随着3D重建技术的快速发展，4D重建研究也在不断进步。现有的4D重建方法能够生成高质量的4D场景，但在获取多视角视频数据方面面临挑战，导致现有基准主要展示有限场景中的动作。本文提出了一个新的4D重建基准WideRange4D，包含丰富的4D场景数据，允许对4D生成方法的能力进行更全面的评估。同时，我们引入了一种新的4D重建方法Progress4D，在各种复杂的4D场景重建任务中生成稳定且高质量的结果。'}}}, {'id': 'https://huggingface.co/papers/2503.13399', 'title': 'MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based\n  Scientific Research', 'url': 'https://huggingface.co/papers/2503.13399', 'abstract': "Scientific research demands sophisticated reasoning over multimodal data, a challenge especially prevalent in biology. Despite recent advances in multimodal large language models (MLLMs) for AI-assisted research, existing multimodal reasoning benchmarks only target up to college-level difficulty, while research-level benchmarks emphasize lower-level perception, falling short of the complex multimodal reasoning needed for scientific discovery. To bridge this gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark designed to assess three reasoning capabilities vital in research workflows: expert image understanding, hypothesis generation, and experiment proposal. MicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology experts across diverse microscopy modalities, ensuring VQA samples represent real scientific practice. In constructing the benchmark, we find that standard MCQ generation methods induce language shortcuts, motivating a new two-stage pipeline: an optimized LLM prompt structures question-answer pairs into MCQs; then, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking on state-of-the-art MLLMs reveal a peak performance of 53\\%; models with smaller LLMs only slightly underperform top models, suggesting that language-based reasoning is less challenging than multimodal reasoning; and tuning with scientific articles enhances performance. Expert analysis of chain-of-thought responses shows that perception errors are the most frequent, followed by knowledge errors and then overgeneralization errors. These insights highlight the challenges in multimodal scientific reasoning, showing MicroVQA is a valuable resource advancing AI-driven biomedical research. MicroVQA is available at https://huggingface.co/datasets/jmhb/microvqa, and project page at https://jmhb0.github.io/microvqa.", 'score': 14, 'issue_id': 2754, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '50d4f1f510eff333', 'authors': ['James Burgess', 'Jeffrey J Nirschl', 'Laura Bravo-Sánchez', 'Alejandro Lozano', 'Sanket Rajan Gupte', 'Jesus G. Galaz-Montoya', 'Yuhui Zhang', 'Yuchang Su', 'Disha Bhowmik', 'Zachary Coman', 'Sarina M. Hasan', 'Alexandra Johannesson', 'William D. Leineweber', 'Malvika G Nair', 'Ridhi Yarlagadda', 'Connor Zuraski', 'Wah Chiu', 'Sarah Cohen', 'Jan N. Hansen', 'Manuel D Leonetti', 'Chad Liu', 'Emma Lundberg', 'Serena Yeung-Levy'], 'affiliations': ['Chan Zuckerberg Biohub Network', 'KTH Royal Institute of Technology', 'Princeton University', 'Stanford University', 'Tsinghua University', 'University of North Carolina at Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2503.13399.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#multimodal', '#benchmark', '#science'], 'emoji': '🔬', 'ru': {'title': 'MicroVQA: Новый рубеж в оценке ИИ для научных исследований в биологии', 'desc': 'Статья представляет MicroVQA - новый бенчмарк для оценки мультимодальных языковых моделей в контексте научных исследований в биологии. Бенчмарк состоит из 1042 вопросов с множественным выбором, охватывающих различные модальности микроскопии и оценивающих понимание изображений, генерацию гипотез и предложение экспериментов. Авторы разработали двухэтапный процесс создания вопросов, чтобы избежать языковых упрощений. Тестирование современных мультимодальных моделей показало максимальную точность 53%, выявив сложности в мультимодальных рассуждениях для научных задач.'}, 'en': {'title': 'MicroVQA: Advancing Multimodal Reasoning in Scientific Discovery', 'desc': 'This paper introduces MicroVQA, a new benchmark for visual-question answering (VQA) that focuses on complex reasoning needed in scientific research, particularly in biology. It assesses three key capabilities: understanding images, generating hypotheses, and proposing experiments, using questions curated by biology experts. The benchmark consists of 1,042 multiple-choice questions designed to reflect real scientific practices, addressing the limitations of existing multimodal reasoning benchmarks. The study reveals that while current models perform at a peak of 53%, the challenges in multimodal reasoning are significant, emphasizing the need for improved AI tools in biomedical research.'}, 'zh': {'title': 'MicroVQA：推动生物医学研究的多模态推理基准', 'desc': '本论文介绍了MicroVQA，这是一个针对生物学研究的视觉问答基准，旨在评估科学研究中所需的三种推理能力：专家图像理解、假设生成和实验提案。现有的多模态大语言模型（MLLMs）在处理复杂的多模态推理时存在不足，MicroVQA通过1,042个由生物学专家策划的多项选择题来填补这一空白。研究发现，标准的多项选择题生成方法容易产生语言捷径，因此提出了一种新的两阶段流程来优化问题和答案的结构。通过对最先进的MLLMs进行基准测试，结果显示，尽管小型LLMs的表现略逊于顶级模型，但语言推理的难度低于多模态推理，这突显了在科学推理中的挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.11751', 'title': 'reWordBench: Benchmarking and Improving the Robustness of Reward Models\n  with Transformed Inputs', 'url': 'https://huggingface.co/papers/2503.11751', 'abstract': 'Reward models have become a staple in modern NLP, serving as not only a scalable text evaluator, but also an indispensable component in many alignment recipes and inference-time algorithms. However, while recent reward models increase performance on standard benchmarks, this may partly be due to overfitting effects, which would confound an understanding of their true capability. In this work, we scrutinize the robustness of reward models and the extent of such overfitting. We build **reWordBench**, which systematically transforms reward model inputs in meaning- or ranking-preserving ways. We show that state-of-the-art reward models suffer from substantial performance degradation even with minor input transformations, sometimes dropping to significantly below-random accuracy, suggesting brittleness. To improve reward model robustness, we propose to explicitly train them to assign similar scores to paraphrases, and find that this approach also improves robustness to other distinct kinds of transformations. For example, our robust reward model reduces such degradation by roughly half for the Chat Hard subset in RewardBench. Furthermore, when used in alignment, our robust reward models demonstrate better utility and lead to higher-quality outputs, winning in up to 59% of instances against a standardly trained RM.', 'score': 14, 'issue_id': 2755, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '0adbcb6b7ba858f8', 'authors': ['Zhaofeng Wu', 'Michihiro Yasunaga', 'Andrew Cohen', 'Yoon Kim', 'Asli Celikyilmaz', 'Marjan Ghazvininejad'], 'affiliations': ['FAIR at Meta', 'MIT'], 'pdf_title_img': 'assets/pdf/title_img/2503.11751.jpg', 'data': {'categories': ['#benchmark', '#hallucinations', '#training', '#alignment', '#rlhf'], 'emoji': '🔬', 'ru': {'title': 'Повышение устойчивости моделей вознаграждения в NLP', 'desc': 'Эта статья исследует устойчивость моделей вознаграждения (reward models) в обработке естественного языка. Авторы создали набор данных reWordBench для систематического тестирования этих моделей путем трансформации входных данных. Результаты показывают, что современные модели вознаграждения существенно теряют в производительности даже при незначительных изменениях входных данных. Для повышения устойчивости авторы предлагают обучать модели присваивать схожие оценки парафразам, что также улучшает устойчивость к другим видам трансформаций.'}, 'en': {'title': 'Enhancing Robustness in Reward Models for NLP', 'desc': 'This paper investigates the reliability of reward models in natural language processing (NLP), which are crucial for evaluating text and enhancing model alignment. The authors introduce **reWordBench**, a tool that modifies inputs to test the robustness of these models against overfitting. Their findings reveal that many state-of-the-art reward models perform poorly when faced with slight input changes, indicating a lack of robustness. To address this issue, they propose training reward models to provide consistent scores for paraphrased inputs, resulting in improved performance and higher-quality outputs during alignment tasks.'}, 'zh': {'title': '提升奖励模型鲁棒性，减少过拟合影响', 'desc': '奖励模型在现代自然语言处理（NLP）中扮演着重要角色，既是可扩展的文本评估工具，也是许多对齐算法和推理时算法的关键组成部分。尽管最近的奖励模型在标准基准测试中表现出色，但这可能部分是由于过拟合现象，影响了对其真实能力的理解。我们构建了reWordBench，系统地对奖励模型输入进行意义或排名保持的转换，发现即使是微小的输入变化，最先进的奖励模型也会出现显著的性能下降，显示出其脆弱性。为提高奖励模型的鲁棒性，我们提出显式训练模型对同义句赋予相似分数，这种方法也改善了模型对其他不同类型转换的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2503.12605', 'title': 'Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey', 'url': 'https://huggingface.co/papers/2503.12605', 'abstract': 'By extending the advantage of chain-of-thought (CoT) reasoning in human-like step-by-step processes to multimodal contexts, multimodal CoT (MCoT) reasoning has recently garnered significant research attention, especially in the integration with multimodal large language models (MLLMs). Existing MCoT studies design various methodologies and innovative reasoning paradigms to address the unique challenges of image, video, speech, audio, 3D, and structured data across different modalities, achieving extensive success in applications such as robotics, healthcare, autonomous driving, and multimodal generation. However, MCoT still presents distinct challenges and opportunities that require further focus to ensure consistent thriving in this field, where, unfortunately, an up-to-date review of this domain is lacking. To bridge this gap, we present the first systematic survey of MCoT reasoning, elucidating the relevant foundational concepts and definitions. We offer a comprehensive taxonomy and an in-depth analysis of current methodologies from diverse perspectives across various application scenarios. Furthermore, we provide insights into existing challenges and future research directions, aiming to foster innovation toward multimodal AGI.', 'score': 13, 'issue_id': 2762, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '4b0cc0276cb6afed', 'authors': ['Yaoting Wang', 'Shengqiong Wu', 'Yuecheng Zhang', 'William Wang', 'Ziwei Liu', 'Jiebo Luo', 'Hao Fei'], 'affiliations': ['CUHK', 'NTU', 'NUS', 'UCSB', 'UR'], 'pdf_title_img': 'assets/pdf/title_img/2503.12605.jpg', 'data': {'categories': ['#multimodal', '#robotics', '#healthcare', '#3d', '#video', '#agi', '#reasoning', '#survey', '#audio'], 'emoji': '🧠', 'ru': {'title': 'Мультимодальное рассуждение: шаг к AGI', 'desc': 'Статья посвящена мультимодальному рассуждению по цепочке мыслей (MCoT) в контексте мультимодальных больших языковых моделей (MLLM). Авторы представляют первый систематический обзор MCoT, объясняя основные концепции и определения. Они предлагают всестороннюю таксономию и глубокий анализ текущих методологий в различных сценариях применения. Статья также рассматривает существующие проблемы и направления будущих исследований в области мультимодального искусственного интеллекта общего назначения.'}, 'en': {'title': 'Unlocking Multimodal Reasoning for Future AI Innovations', 'desc': 'This paper introduces multimodal chain-of-thought (MCoT) reasoning, which enhances human-like reasoning processes across different types of data such as images, videos, and audio. It reviews various methodologies and innovative reasoning paradigms that have been developed to tackle the unique challenges posed by these multimodal contexts. The authors present a systematic survey that includes foundational concepts, a comprehensive taxonomy, and an analysis of current approaches in MCoT applications. Additionally, the paper highlights existing challenges and suggests future research directions to advance the field towards multimodal artificial general intelligence (AGI).'}, 'zh': {'title': '多模态推理的未来之路', 'desc': '多模态链式思维（MCoT）推理将人类的逐步推理优势扩展到多种数据类型，近年来受到广泛关注，尤其是在与多模态大型语言模型（MLLMs）的结合方面。现有的MCoT研究设计了多种方法和创新的推理范式，以应对图像、视频、语音、音频、3D和结构化数据等不同模态的独特挑战，并在机器人技术、医疗保健、自动驾驶和多模态生成等应用中取得了显著成功。尽管如此，MCoT仍面临独特的挑战和机遇，需要进一步关注，以确保该领域的持续发展。为此，我们提供了MCoT推理的首次系统性综述，阐明相关的基础概念和定义，并对当前方法进行了全面的分类和深入分析。'}}}, {'id': 'https://huggingface.co/papers/2503.12937', 'title': 'R1-VL: Learning to Reason with Multimodal Large Language Models via\n  Step-wise Group Relative Policy Optimization', 'url': 'https://huggingface.co/papers/2503.12937', 'abstract': "Recent studies generally enhance MLLMs' reasoning capabilities via supervised fine-tuning on high-quality chain-of-thought reasoning data, which often leads models to merely imitate successful reasoning paths without understanding what the wrong reasoning paths are. In this work, we aim to enhance the MLLMs' reasoning ability beyond passively imitating positive reasoning paths. To this end, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new online reinforcement learning framework that enables MLLMs to self-improve reasoning ability via simple, effective and dense step-wise rewarding. Specifically, StepGRPO introduces two novel rule-based reasoning rewards: Step-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity Reward (StepRVR). StepRAR rewards the reasoning paths that contain necessary intermediate reasoning steps via a soft key-step matching technique, while StepRAR rewards reasoning paths that follow a well-structured and logically consistent reasoning process through a reasoning completeness and logic evaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series of MLLMs with outstanding capabilities in step-by-step reasoning. Extensive experiments over 8 benchmarks demonstrate the superiority of our methods.", 'score': 12, 'issue_id': 2755, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': 'dbdf6970963a4489', 'authors': ['Jingyi Zhang', 'Jiaxing Huang', 'Huanjin Yao', 'Shunyu Liu', 'Xikun Zhang', 'Shijian Lu', 'Dacheng Tao'], 'affiliations': ['Nanyang Technological University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12937.jpg', 'data': {'categories': ['#training', '#reasoning', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Самосовершенствование языковых моделей через пошаговое обучение с подкреплением', 'desc': 'Это исследование представляет новый подход к улучшению способностей мультимодальных языковых моделей (MLLM) к рассуждению. Авторы разработали метод Step-wise Group Relative Policy Optimization (StepGRPO), который использует онлайн-обучение с подкреплением для самосовершенствования моделей. StepGRPO вводит два новых вознаграждения на основе правил: Step-wise Reasoning Accuracy Reward (StepRAR) и Step-wise Reasoning Validity Reward (StepRVR). Эксперименты на 8 бенчмарках показали превосходство предложенного метода.'}, 'en': {'title': 'Empowering MLLMs with Step-wise Reasoning Rewards', 'desc': 'This paper presents a new approach to improve the reasoning abilities of machine learning language models (MLLMs) by using reinforcement learning. The proposed method, called Step-wise Group Relative Policy Optimization (StepGRPO), focuses on rewarding MLLMs for their reasoning steps rather than just imitating correct paths. It introduces two innovative rewards: Step-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity Reward (StepRVR), which encourage models to follow logical reasoning processes. The results show that MLLMs trained with StepGRPO, referred to as R1-VL, perform significantly better in step-by-step reasoning tasks across multiple benchmarks.'}, 'zh': {'title': '提升推理能力的创新方法', 'desc': '最近的研究通常通过在高质量的推理数据上进行监督微调来增强多语言大模型（MLLMs）的推理能力，这往往导致模型仅仅模仿成功的推理路径，而不理解错误的推理路径。在这项工作中，我们旨在超越被动模仿积极推理路径，提升MLLMs的推理能力。为此，我们设计了一种新的在线强化学习框架——逐步组相对策略优化（StepGRPO），使MLLMs能够通过简单、有效和密集的逐步奖励自我提升推理能力。具体而言，StepGRPO引入了两种新颖的基于规则的推理奖励：逐步推理准确性奖励（StepRAR）和逐步推理有效性奖励（StepRVR）。'}}}, {'id': 'https://huggingface.co/papers/2503.11495', 'title': 'V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning', 'url': 'https://huggingface.co/papers/2503.11495', 'abstract': 'Human processes video reasoning in a sequential spatio-temporal reasoning logic, we first identify the relevant frames ("when") and then analyse the spatial relationships ("where") between key objects, and finally leverage these relationships to draw inferences ("what"). However, can Video Large Language Models (Video-LLMs) also "reason through a sequential spatio-temporal logic" in videos? Existing Video-LLM benchmarks primarily focus on assessing object presence, neglecting relational reasoning. Consequently, it is difficult to measure whether a model truly comprehends object interactions (actions/events) in videos or merely relies on pre-trained "memory" of co-occurrences as biases in generating answers. In this work, we introduce a Video Spatio-Temporal Reasoning (V-STaR) benchmark to address these shortcomings. The key idea is to decompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR) task that simultaneously evaluates what objects are present, when events occur, and where they are located while capturing the underlying Chain-of-thought (CoT) logic. To support this evaluation, we construct a dataset to elicit the spatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine CoT questions generated by a semi-automated GPT-4-powered pipeline, embedding explicit reasoning chains to mimic human cognition. Experiments from 14 Video-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and the needs for robust and consistent spatio-temporal reasoning.', 'score': 10, 'issue_id': 2754, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '93b4a63d45a11f3d', 'authors': ['Zixu Cheng', 'Jian Hu', 'Ziquan Liu', 'Chenyang Si', 'Wei Li', 'Shaogang Gong'], 'affiliations': ['Nanjing University', 'Nanyang Technological University', 'Queen Mary University of London'], 'pdf_title_img': 'assets/pdf/title_img/2503.11495.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#reasoning', '#video'], 'emoji': '🎥', 'ru': {'title': 'Новый взгляд на оценку пространственно-временного мышления видео-LLM', 'desc': 'Статья представляет новый бенчмарк V-STaR для оценки пространственно-временного рассуждения в видео-LLM моделях. Авторы предлагают декомпозировать понимание видео на задачу обратного пространственно-временного рассуждения (RSTR), оценивающую присутствие объектов, время событий и их расположение. Для этого создан датасет с вопросами, сгенерированными с помощью GPT-4, имитирующими цепочку рассуждений человека. Эксперименты на 14 видео-LLM моделях выявили значительные пробелы в их способности к надежному и последовательному пространственно-временному рассуждению.'}, 'en': {'title': 'Enhancing Video Understanding with Spatio-Temporal Reasoning', 'desc': "This paper explores how Video Large Language Models (Video-LLMs) can understand videos using a method similar to human reasoning, which involves identifying when events happen, where objects are located, and how they interact. The authors highlight that current benchmarks for Video-LLMs mainly check for object presence but fail to assess the models' ability to reason about relationships and interactions between objects. To address this, they introduce the Video Spatio-Temporal Reasoning (V-STaR) benchmark, which breaks down video understanding into a task that evaluates object presence, timing of events, and spatial relationships while capturing the reasoning process. Their findings show that there are significant gaps in the reasoning capabilities of existing Video-LLMs, indicating a need for improved models that can perform robust spatio-temporal reasoning."}, 'zh': {'title': '提升视频理解的时空推理能力', 'desc': '本论文探讨了视频大型语言模型（Video-LLMs）在视频理解中的时空推理能力。我们提出了一个新的基准，称为视频时空推理（V-STaR），旨在评估模型在识别对象、事件发生时间和空间位置方面的能力。通过构建一个包含细致推理链的问题数据集，我们模拟人类的认知过程，以便更好地评估模型的推理能力。实验结果显示，现有的Video-LLMs在时空推理方面存在显著不足，无法满足实际应用的需求。'}}}, {'id': 'https://huggingface.co/papers/2503.13082', 'title': 'Free-form language-based robotic reasoning and grasping', 'url': 'https://huggingface.co/papers/2503.13082', 'abstract': "Performing robotic grasping from a cluttered bin based on human instructions is a challenging task, as it requires understanding both the nuances of free-form language and the spatial relationships between objects. Vision-Language Models (VLMs) trained on web-scale data, such as GPT-4o, have demonstrated remarkable reasoning capabilities across both text and images. But can they truly be used for this task in a zero-shot setting? And what are their limitations? In this paper, we explore these research questions via the free-form language-based robotic grasping task, and propose a novel method, FreeGrasp, leveraging the pre-trained VLMs' world knowledge to reason about human instructions and object spatial arrangements. Our method detects all objects as keypoints and uses these keypoints to annotate marks on images, aiming to facilitate GPT-4o's zero-shot spatial reasoning. This allows our method to determine whether a requested object is directly graspable or if other objects must be grasped and removed first. Since no existing dataset is specifically designed for this task, we introduce a synthetic dataset FreeGraspData by extending the MetaGraspNetV2 dataset with human-annotated instructions and ground-truth grasping sequences. We conduct extensive analyses with both FreeGraspData and real-world validation with a gripper-equipped robotic arm, demonstrating state-of-the-art performance in grasp reasoning and execution. Project website: https://tev-fbk.github.io/FreeGrasp/.", 'score': 9, 'issue_id': 2762, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': 'c29bd4c3e364d62c', 'authors': ['Runyu Jiao', 'Alice Fasoli', 'Francesco Giuliari', 'Matteo Bortolon', 'Sergio Povoli', 'Guofeng Mei', 'Yiming Wang', 'Fabio Poiesi'], 'affiliations': ['Fondazione Bruno Kessler', 'Istituto Italiano di Tecnologia', 'University of Trento'], 'pdf_title_img': 'assets/pdf/title_img/2503.13082.jpg', 'data': {'categories': ['#dataset', '#robotics', '#agents', '#reasoning', '#cv', '#synthetic'], 'emoji': '🦾', 'ru': {'title': 'Роботы учатся хватать по-человечески', 'desc': 'Статья представляет новый метод FreeGrasp для роботизированного захвата объектов на основе инструкций на естественном языке. Метод использует предобученные мультимодальные языковые модели (VLM) для понимания инструкций и пространственных отношений между объектами. FreeGrasp определяет объекты как ключевые точки и использует их для аннотации изображений, чтобы улучшить рассуждения модели о пространственных отношениях. Авторы также создали синтетический датасет FreeGraspData для оценки производительности метода.'}, 'en': {'title': 'Empowering Robots with Language: Grasping Made Easy!', 'desc': 'This paper investigates the use of Vision-Language Models (VLMs) for robotic grasping tasks based on human instructions, particularly in cluttered environments. The authors introduce a method called FreeGrasp, which utilizes pre-trained VLMs to enhance spatial reasoning by detecting objects as keypoints and annotating them on images. They also create a new synthetic dataset, FreeGraspData, to support their research, as no existing dataset fits their needs. The results show that FreeGrasp achieves state-of-the-art performance in understanding and executing grasping tasks, demonstrating the potential of VLMs in robotics.'}, 'zh': {'title': '基于人类指令的智能抓取新方法', 'desc': '本论文探讨了基于人类指令的机器人抓取任务，尤其是在杂乱的环境中进行抓取的挑战。我们提出了一种新方法FreeGrasp，利用预训练的视觉-语言模型（VLM）来理解人类指令和物体之间的空间关系。该方法通过将所有物体检测为关键点，并在图像上标注这些关键点，来增强模型的空间推理能力。我们还创建了一个合成数据集FreeGraspData，以支持这一任务的研究，并在真实世界中验证了我们方法的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.13444', 'title': 'VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning', 'url': 'https://huggingface.co/papers/2503.13444', 'abstract': 'Videos, with their unique temporal dimension, demand precise grounded understanding, where answers are directly linked to visual, interpretable evidence. Despite significant breakthroughs in reasoning capabilities within Large Language Models, multi-modal reasoning - especially for videos - remains unexplored. In this work, we introduce VideoMind, a novel video-language agent designed for temporal-grounded video understanding. VideoMind incorporates two key innovations: (i) We identify essential capabilities for video temporal reasoning and develop a role-based agentic workflow, including a planner for coordinating different roles, a grounder for temporal localization, a verifier to assess temporal interval accuracy, and an answerer for question-answering. (ii) To efficiently integrate these diverse roles, we propose a novel Chain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA adaptors while avoiding the overhead of multiple models, thus balancing efficiency and flexibility. Extensive experiments on 14 public benchmarks demonstrate that our agent achieves state-of-the-art performance on diverse video understanding tasks, including 3 on grounded video question-answering, 6 on video temporal grounding, and 5 on general video question-answering, underscoring its effectiveness in advancing video agent and long-form temporal reasoning.', 'score': 8, 'issue_id': 2755, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '4845903cb3dc6761', 'authors': ['Ye Liu', 'Kevin Qinghong Lin', 'Chang Wen Chen', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2503.13444.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#agents', '#optimization', '#video', '#multimodal'], 'emoji': '🎬', 'ru': {'title': 'VideoMind: Темпоральное рассуждение по видео с помощью ролевых агентов', 'desc': 'Статья представляет VideoMind - новый видео-языковой агент для темпорально-обоснованного понимания видео. Авторы разработали ролевой подход с планировщиком, локализатором, верификатором и ответчиком для временного рассуждения по видео. Предложена стратегия Chain-of-LoRA для эффективного переключения между ролями с помощью легких LoRA-адаптеров. Эксперименты на 14 бенчмарках показали, что VideoMind достигает state-of-the-art результатов в различных задачах понимания видео.'}, 'en': {'title': 'VideoMind: Advancing Video Understanding with Temporal Reasoning', 'desc': 'This paper presents VideoMind, a new video-language agent that enhances understanding of videos by linking answers to visual evidence. It introduces a role-based workflow that includes a planner, grounder, verifier, and answerer to facilitate effective temporal reasoning in videos. The authors also propose a Chain-of-LoRA strategy, which allows for efficient role-switching without the need for multiple models, thus improving both efficiency and flexibility. The results show that VideoMind outperforms existing methods on various benchmarks, highlighting its capability in grounded video question-answering and temporal reasoning tasks.'}, 'zh': {'title': 'VideoMind：视频理解的新突破', 'desc': '本论文介绍了一种新的视频语言智能体，名为VideoMind，旨在实现视频的时间基础理解。该智能体通过角色驱动的工作流程，整合了规划者、定位者、验证者和回答者等关键角色，以提高视频的时间推理能力。为了高效整合这些角色，提出了一种新颖的Chain-of-LoRA策略，允许通过轻量级的LoRA适配器实现角色之间的无缝切换。实验结果表明，VideoMind在多项视频理解任务上表现出色，推动了视频智能体和长时序推理的发展。'}}}, {'id': 'https://huggingface.co/papers/2503.11412', 'title': 'MTV-Inpaint: Multi-Task Long Video Inpainting', 'url': 'https://huggingface.co/papers/2503.11412', 'abstract': 'Video inpainting involves modifying local regions within a video, ensuring spatial and temporal consistency. Most existing methods focus primarily on scene completion (i.e., filling missing regions) and lack the capability to insert new objects into a scene in a controllable manner. Fortunately, recent advancements in text-to-video (T2V) diffusion models pave the way for text-guided video inpainting. However, directly adapting T2V models for inpainting remains limited in unifying completion and insertion tasks, lacks input controllability, and struggles with long videos, thereby restricting their applicability and flexibility. To address these challenges, we propose MTV-Inpaint, a unified multi-task video inpainting framework capable of handling both traditional scene completion and novel object insertion tasks. To unify these distinct tasks, we design a dual-branch spatial attention mechanism in the T2V diffusion U-Net, enabling seamless integration of scene completion and object insertion within a single framework. In addition to textual guidance, MTV-Inpaint supports multimodal control by integrating various image inpainting models through our proposed image-to-video (I2V) inpainting mode. Additionally, we propose a two-stage pipeline that combines keyframe inpainting with in-between frame propagation, enabling MTV-Inpaint to effectively handle long videos with hundreds of frames. Extensive experiments demonstrate that MTV-Inpaint achieves state-of-the-art performance in both scene completion and object insertion tasks. Furthermore, it demonstrates versatility in derived applications such as multi-modal inpainting, object editing, removal, image object brush, and the ability to handle long videos. Project page: https://mtv-inpaint.github.io/.', 'score': 7, 'issue_id': 2756, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': 'df14c3a2c0dfe3fd', 'authors': ['Shiyuan Yang', 'Zheng Gu', 'Liang Hou', 'Xin Tao', 'Pengfei Wan', 'Xiaodong Chen', 'Jing Liao'], 'affiliations': ['City University of Hong Kong', 'Kuaishou Technology', 'Shenzhen University', 'Tianjin University'], 'pdf_title_img': 'assets/pdf/title_img/2503.11412.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#video'], 'emoji': '🎬', 'ru': {'title': 'Универсальный видеоинпейнтинг: от заполнения пропусков до вставки объектов', 'desc': 'MTV-Inpaint - это новая система для видеоинпейнтинга, объединяющая задачи заполнения пропусков и вставки новых объектов. Она использует диффузионную модель text-to-video с двухветвевым механизмом пространственного внимания в U-Net архитектуре. MTV-Inpaint поддерживает мультимодальное управление, интегрируя различные модели инпейнтинга изображений. Система применяет двухэтапный подход с инпейнтингом ключевых кадров и распространением на промежуточные, что позволяет обрабатывать длинные видео.'}, 'en': {'title': 'Unified Video Inpainting: Complete and Insert with Control!', 'desc': 'This paper presents MTV-Inpaint, a novel framework for video inpainting that integrates scene completion and object insertion tasks. It utilizes a dual-branch spatial attention mechanism within a text-to-video diffusion U-Net to achieve seamless control over both tasks. The framework also introduces a two-stage pipeline that effectively manages long videos by combining keyframe inpainting with in-between frame propagation. Extensive experiments show that MTV-Inpaint outperforms existing methods and offers versatility for various applications, including multi-modal inpainting and object editing.'}, 'zh': {'title': '统一视频修复，场景补全与物体插入的完美结合', 'desc': '视频修复是指在视频中修改局部区域，以确保空间和时间的一致性。现有的方法主要集中在场景补全上，缺乏可控地插入新物体的能力。为了解决这些问题，我们提出了MTV-Inpaint，一个统一的多任务视频修复框架，能够同时处理传统的场景补全和新物体插入任务。通过设计双分支空间注意力机制，MTV-Inpaint实现了场景补全和物体插入的无缝集成，并支持多模态控制，适用于长视频的处理。'}}}, {'id': 'https://huggingface.co/papers/2503.13070', 'title': 'Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation', 'url': 'https://huggingface.co/papers/2503.13070', 'abstract': 'Aligning generated images to complicated text prompts and human preferences is a central challenge in Artificial Intelligence-Generated Content (AIGC). With reward-enhanced diffusion distillation emerging as a promising approach that boosts controllability and fidelity of text-to-image models, we identify a fundamental paradigm shift: as conditions become more specific and reward signals stronger, the rewards themselves become the dominant force in generation. In contrast, the diffusion losses serve as an overly expensive form of regularization. To thoroughly validate our hypothesis, we introduce R0, a novel conditional generation approach via regularized reward maximization. Instead of relying on tricky diffusion distillation losses, R0 proposes a new perspective that treats image generations as an optimization problem in data space which aims to search for valid images that have high compositional rewards. By innovative designs of the generator parameterization and proper regularization techniques, we train state-of-the-art few-step text-to-image generative models with R0 at scales. Our results challenge the conventional wisdom of diffusion post-training and conditional generation by demonstrating that rewards play a dominant role in scenarios with complex conditions. We hope our findings can contribute to further research into human-centric and reward-centric generation paradigms across the broader field of AIGC. Code is available at https://github.com/Luo-Yihong/R0.', 'score': 6, 'issue_id': 2756, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '3d6b6c6e117e1abd', 'authors': ['Yihong Luo', 'Tianyang Hu', 'Weijian Luo', 'Kenji Kawaguchi', 'Jing Tang'], 'affiliations': ['HKUST', 'HKUST (GZ)', 'NUS', 'Xiaohongshu Inc'], 'pdf_title_img': 'assets/pdf/title_img/2503.13070.jpg', 'data': {'categories': ['#rag', '#diffusion', '#alignment', '#training', '#optimization'], 'emoji': '🎨', 'ru': {'title': 'R0: Революция в генерации изображений через максимизацию вознаграждений', 'desc': 'Статья представляет новый подход к генерации изображений по текстовым запросам, называемый R0. Вместо использования сложных методов дистилляции диффузионных моделей, R0 рассматривает генерацию как оптимизационную задачу в пространстве данных. Авторы утверждают, что при более конкретных условиях и сильных сигналах вознаграждения, сами вознаграждения становятся доминирующей силой в генерации. Результаты исследования бросают вызов традиционным представлениям о постобучении диффузионных моделей и условной генерации.'}, 'en': {'title': 'Revolutionizing Image Generation: Prioritizing Rewards Over Diffusion', 'desc': 'This paper addresses the challenge of aligning generated images with complex text prompts and human preferences in AI-generated content. It introduces R0, a new approach that emphasizes reward maximization over traditional diffusion distillation methods, which are seen as inefficient. The authors argue that as conditions for image generation become more specific, the influence of reward signals becomes more significant than diffusion losses. Their findings suggest a shift in focus towards reward-centric generation strategies, which could enhance the effectiveness of text-to-image models.'}, 'zh': {'title': '奖励驱动的图像生成新视角', 'desc': '本论文探讨了在人工智能生成内容（AIGC）中，将生成图像与复杂文本提示和人类偏好对齐的挑战。我们提出了一种新的条件生成方法R0，通过正则化奖励最大化来优化图像生成过程。研究表明，在条件变得更加具体且奖励信号更强时，奖励在生成过程中起着主导作用，而扩散损失则成为一种过于昂贵的正则化形式。我们的结果挑战了传统的扩散后训练和条件生成的观念，强调了在复杂条件下奖励的重要性。'}}}, {'id': 'https://huggingface.co/papers/2503.10719', 'title': 'Long-Video Audio Synthesis with Multi-Agent Collaboration', 'url': 'https://huggingface.co/papers/2503.10719', 'abstract': 'Video-to-audio synthesis, which generates synchronized audio for visual content, critically enhances viewer immersion and narrative coherence in film and interactive media. However, video-to-audio dubbing for long-form content remains an unsolved challenge due to dynamic semantic shifts, temporal misalignment, and the absence of dedicated datasets. While existing methods excel in short videos, they falter in long scenarios (e.g., movies) due to fragmented synthesis and inadequate cross-scene consistency. We propose LVAS-Agent, a novel multi-agent framework that emulates professional dubbing workflows through collaborative role specialization. Our approach decomposes long-video synthesis into four steps including scene segmentation, script generation, sound design and audio synthesis. Central innovations include a discussion-correction mechanism for scene/script refinement and a generation-retrieval loop for temporal-semantic alignment. To enable systematic evaluation, we introduce LVAS-Bench, the first benchmark with 207 professionally curated long videos spanning diverse scenarios. Experiments demonstrate superior audio-visual alignment over baseline methods. Project page: https://lvas-agent.github.io', 'score': 5, 'issue_id': 2760, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '121476792dd15d11', 'authors': ['Yehang Zhang', 'Xinli Xu', 'Xiaojie Xu', 'Li Liu', 'Yingcong Chen'], 'affiliations': ['HKUST', 'HKUST(GZ)'], 'pdf_title_img': 'assets/pdf/title_img/2503.10719.jpg', 'data': {'categories': ['#long_context', '#agents', '#benchmark', '#video', '#games'], 'emoji': '🎬', 'ru': {'title': 'Профессиональный дубляж длинных видео с помощью ИИ', 'desc': 'LVAS-Agent - это новая мультиагентная система для синтеза аудио к длинным видео, имитирующая профессиональный процесс дубляжа. Она разбивает задачу на четыре этапа: сегментацию сцен, генерацию сценария, звуковой дизайн и синтез аудио. Ключевые инновации включают механизм обсуждения и коррекции для улучшения сцен/сценария и цикл генерации-поиска для временно-семантического выравнивания. Авторы также представили LVAS-Bench - первый бенчмарк из 207 длинных видео для оценки таких систем.'}, 'en': {'title': 'Revolutionizing Long-Form Video Dubbing with LVAS-Agent', 'desc': 'This paper presents LVAS-Agent, a new framework for generating audio that matches long videos, like movies, to improve viewer experience. The framework breaks down the audio synthesis process into four key steps: segmenting scenes, generating scripts, designing sounds, and synthesizing audio. It introduces innovative techniques such as a discussion-correction mechanism to refine scripts and a generation-retrieval loop to ensure that audio aligns well with the video over time. Additionally, the authors provide LVAS-Bench, a benchmark dataset of 207 long videos to evaluate the effectiveness of their approach against existing methods.'}, 'zh': {'title': '长视频配音的新突破：LVAS-Agent', 'desc': '视频到音频合成是为视觉内容生成同步音频的技术，能够显著提升观众的沉浸感和叙事连贯性。然而，对于长篇内容的视频到音频配音仍然是一个未解决的挑战，主要由于动态语义变化、时间错位和缺乏专门的数据集。现有方法在短视频中表现良好，但在长视频（如电影）中由于合成碎片化和跨场景一致性不足而表现不佳。我们提出了LVAS-Agent，一个新颖的多代理框架，通过协作角色专业化模拟专业配音工作流程，分解长视频合成为场景分割、脚本生成、声音设计和音频合成四个步骤。'}}}, {'id': 'https://huggingface.co/papers/2503.10704', 'title': 'Error Analyses of Auto-Regressive Video Diffusion Models: A Unified\n  Framework', 'url': 'https://huggingface.co/papers/2503.10704', 'abstract': 'A variety of Auto-Regressive Video Diffusion Models (ARVDM) have achieved remarkable successes in generating realistic long-form videos. However, theoretical analyses of these models remain scant. In this work, we develop theoretical underpinnings for these models and use our insights to improve the performance of existing models. We first develop Meta-ARVDM, a unified framework of ARVDMs that subsumes most existing methods. Using Meta-ARVDM, we analyze the KL-divergence between the videos generated by Meta-ARVDM and the true videos. Our analysis uncovers two important phenomena inherent to ARVDM -- error accumulation and memory bottleneck. By deriving an information-theoretic impossibility result, we show that the memory bottleneck phenomenon cannot be avoided. To mitigate the memory bottleneck, we design various network structures to explicitly use more past frames. We also achieve a significantly improved trade-off between the mitigation of the memory bottleneck and the inference efficiency by compressing the frames. Experimental results on DMLab and Minecraft validate the efficacy of our methods. Our experiments also demonstrate a Pareto-frontier between the error accumulation and memory bottleneck across different methods.', 'score': 5, 'issue_id': 2756, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': 'e5f88bd433320c2f', 'authors': ['Jing Wang', 'Fengzhuo Zhang', 'Xiaoli Li', 'Vincent Y. F. Tan', 'Tianyu Pang', 'Chao Du', 'Aixin Sun', 'Zhuoran Yang'], 'affiliations': ['A*STAR', 'Nanyang Technological University', 'National University of Singapore', 'Sea AI Lab', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10704.jpg', 'data': {'categories': ['#diffusion', '#video', '#inference', '#architecture', '#math', '#optimization'], 'emoji': '🎬', 'ru': {'title': 'Теоретические основы и улучшение авторегрессионных моделей видеодиффузии', 'desc': 'Статья представляет теоретический анализ авторегрессионных моделей видеодиффузии (ARVDM) и предлагает способы их улучшения. Авторы разработали Meta-ARVDM - унифицированную структуру, объединяющую большинство существующих методов ARVDM. Анализ выявил два ключевых явления в ARVDM: накопление ошибок и ограничение памяти, причем последнее неизбежно согласно информационно-теоретическому результату. Для смягчения эффекта ограничения памяти предложены новые архитектуры нейронных сетей и методы сжатия кадров, эффективность которых подтверждена экспериментами.'}, 'en': {'title': 'Enhancing Video Generation with Meta-ARVDM: Tackling Memory Bottlenecks!', 'desc': 'This paper focuses on improving Auto-Regressive Video Diffusion Models (ARVDM) for generating realistic long videos. The authors introduce Meta-ARVDM, a comprehensive framework that encompasses existing ARVDM methods and provides a theoretical analysis of their performance. They identify two key issues: error accumulation and memory bottleneck, the latter of which is shown to be unavoidable through an information-theoretic approach. To address these challenges, the authors propose new network architectures that utilize more past frames and optimize the balance between memory usage and inference efficiency, demonstrating their findings through experiments on DMLab and Minecraft.'}, 'zh': {'title': '提升视频生成效率，破解内存瓶颈', 'desc': '本文探讨了自回归视频扩散模型（ARVDM）的理论基础，并提出了Meta-ARVDM这一统一框架，涵盖了大多数现有方法。通过分析Meta-ARVDM生成的视频与真实视频之间的KL散度，揭示了ARVDM固有的两个重要现象：误差累积和内存瓶颈。我们通过信息论的不可能性结果证明了内存瓶颈现象是无法避免的。为了解决内存瓶颈问题，本文设计了多种网络结构，以显式利用更多的过去帧，并通过压缩帧实现了内存瓶颈缓解与推理效率之间的显著改进。'}}}, {'id': 'https://huggingface.co/papers/2503.13369', 'title': 'Sightation Counts: Leveraging Sighted User Feedback in Building a\n  BLV-aligned Dataset of Diagram Descriptions', 'url': 'https://huggingface.co/papers/2503.13369', 'abstract': 'Often, the needs and visual abilities differ between the annotator group and the end user group. Generating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain. Sighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them are costly, bias-prone, and somewhat lacking by BLV standards. In this study, we ask sighted individuals to assess -- rather than produce -- diagram descriptions generated by vision-language models (VLM) that have been guided with latent supervision via a multi-pass inference. The sighted assessments prove effective and useful to professional educators who are themselves BLV and teach visually impaired learners. We release Sightation, a collection of diagram description datasets spanning 5k diagrams and 137k samples for completion, preference, retrieval, question answering, and reasoning training purposes and demonstrate their fine-tuning potential in various downstream tasks.', 'score': 3, 'issue_id': 2760, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '30b312815a3aaed9', 'authors': ['Wan Ju Kang', 'Eunki Kim', 'Na Min An', 'Sangryul Kim', 'Haemin Choi', 'Ki Hoon Kwak', 'James Thorne'], 'affiliations': ['KAIST AI', 'Sungkyunkwan University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2503.13369.jpg', 'data': {'categories': ['#training', '#alignment', '#data', '#low_resource', '#dataset', '#multimodal'], 'emoji': '👁️', 'ru': {'title': 'Искусственный интеллект на службе незрячих: новый подход к описанию визуальной информации', 'desc': 'Статья описывает метод создания детальных описаний диаграмм для слепых и слабовидящих пользователей с помощью моделей компьютерного зрения и обработки естественного языка. Авторы предлагают использовать зрячих аннотаторов для оценки описаний, сгенерированных моделями, вместо того чтобы создавать их самостоятельно. Исследование показало эффективность этого подхода для преподавателей с нарушениями зрения. В результате работы был создан набор данных Sightation, содержащий описания 5000 диаграмм для различных задач машинного обучения.'}, 'en': {'title': 'Empowering BLV Education with Vision-Language Models', 'desc': "This paper addresses the challenge of creating effective diagram descriptions for blind and low-vision (BLV) users, highlighting the differences in needs between annotators and end users. It proposes a novel approach where sighted annotators assess diagram descriptions generated by vision-language models (VLM) instead of creating them directly, reducing bias and improving quality. The study introduces 'Sightation', a comprehensive dataset containing 5,000 diagrams and 137,000 samples designed for various machine learning tasks such as preference and reasoning. The findings suggest that using sighted assessments can enhance the educational resources available for BLV learners, making them more accessible and relevant."}, 'zh': {'title': '为视觉障碍用户生成精准图表描述', 'desc': '本研究探讨了视觉障碍用户（BLV）对图表描述的需求与视觉标注者之间的差异。我们提出了一种新方法，通过视觉语言模型（VLM）生成图表描述，并让视觉正常的评估者对这些描述进行评估，而不是直接生成。研究表明，这种评估方式对专业的视觉障碍教育者非常有效，能够帮助他们更好地服务于视觉障碍学习者。我们还发布了名为Sightation的数据集，包含5000个图表和137000个样本，旨在支持多种下游任务的训练。'}}}, {'id': 'https://huggingface.co/papers/2503.12530', 'title': 'Basic Category Usage in Vision Language Models', 'url': 'https://huggingface.co/papers/2503.12530', 'abstract': "The field of psychology has long recognized a basic level of categorization that humans use when labeling visual stimuli, a term coined by Rosch in 1976. This level of categorization has been found to be used most frequently, to have higher information density, and to aid in visual language tasks with priming in humans. Here, we investigate basic level categorization in two recently released, open-source vision-language models (VLMs). This paper demonstrates that Llama 3.2 Vision Instruct (11B) and Molmo 7B-D both prefer basic level categorization consistent with human behavior. Moreover, the models' preferences are consistent with nuanced human behaviors like the biological versus non-biological basic level effects and the well established expert basic level shift, further suggesting that VLMs acquire cognitive categorization behaviors from the human data on which they are trained.", 'score': 2, 'issue_id': 2755, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': 'eca67c7a2633554a', 'authors': ['Hunter Sawyer', 'Jesse Roberts', 'Kyle Moore'], 'affiliations': ['Tennessee Tech University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12530.jpg', 'data': {'categories': ['#open_source', '#interpretability', '#multimodal', '#cv'], 'emoji': '🧠', 'ru': {'title': 'ИИ мыслит как человек: базовая категоризация в моделях компьютерного зрения', 'desc': 'Это исследование рассматривает применение базового уровня категоризации, концепции из психологии, в современных моделях компьютерного зрения и обработки естественного языка (VLM). Авторы обнаружили, что модели Llama 3.2 Vision Instruct и Molmo 7B-D предпочитают использовать базовый уровень категоризации, что соответствует поведению человека. Более того, модели демонстрируют нюансы, характерные для людей, такие как различия в категоризации биологических и небиологических объектов, а также сдвиг базового уровня у экспертов. Это указывает на то, что VLM приобретают когнитивные паттерны категоризации из человеческих данных, на которых они обучаются.'}, 'en': {'title': 'Bridging Human and Machine: Basic Level Categorization in Vision-Language Models', 'desc': 'This paper explores how vision-language models (VLMs) categorize visual stimuli at a basic level, similar to human categorization as identified by Rosch. The study shows that Llama 3.2 Vision Instruct and Molmo 7B-D models exhibit a preference for basic level categorization, which aligns with human behavior. Additionally, the models reflect complex human categorization nuances, such as distinguishing between biological and non-biological entities. This suggests that VLMs learn cognitive categorization patterns from the human data they are trained on, highlighting the influence of human cognitive processes on machine learning models.'}, 'zh': {'title': '探索视觉语言模型中的基本分类行为', 'desc': '本论文研究了视觉语言模型（VLMs）中的基本分类水平，这一概念最早由Rosch在1976年提出。研究发现，Llama 3.2 Vision Instruct和Molmo 7B-D这两个模型在分类时更倾向于使用与人类行为一致的基本分类水平。模型的偏好还与人类的细微行为相符，例如生物与非生物的基本分类效应，以及专家的基本分类转变。这表明，视觉语言模型在训练过程中从人类数据中学习了认知分类行为。'}}}, {'id': 'https://huggingface.co/papers/2503.12528', 'title': 'Investigating Human-Aligned Large Language Model Uncertainty', 'url': 'https://huggingface.co/papers/2503.12528', 'abstract': 'Recent work has sought to quantify large language model uncertainty to facilitate model control and modulate user trust. Previous works focus on measures of uncertainty that are theoretically grounded or reflect the average overt behavior of the model. In this work, we investigate a variety of uncertainty measures, in order to identify measures that correlate with human group-level uncertainty. We find that Bayesian measures and a variation on entropy measures, top-k entropy, tend to agree with human behavior as a function of model size. We find that some strong measures decrease in human-similarity with model size, but, by multiple linear regression, we find that combining multiple uncertainty measures provide comparable human-alignment with reduced size-dependency.', 'score': 2, 'issue_id': 2755, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '7c140046351fe245', 'authors': ['Kyle Moore', 'Jesse Roberts', 'Daryl Watson', 'Pamela Wisniewski'], 'affiliations': ['Tennessee Tech University', 'Vanderbilt University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12528.jpg', 'data': {'categories': ['#interpretability', '#hallucinations', '#rlhf', '#training'], 'emoji': '🤔', 'ru': {'title': 'Измерение неопределенности LLM: как приблизиться к человеку', 'desc': 'Статья исследует различные методы оценки неопределенности в больших языковых моделях (LLM) с целью найти те, которые лучше всего соответствуют групповой неопределенности людей. Авторы обнаружили, что байесовские методы и вариация энтропийных мер (энтропия top-k) хорошо согласуются с человеческим поведением в зависимости от размера модели. Некоторые сильные меры показывают снижение сходства с человеком при увеличении размера модели. Комбинирование нескольких мер неопределенности с помощью множественной линейной регрессии обеспечивает сопоставимое соответствие человеческому поведению с меньшей зависимостью от размера модели.'}, 'en': {'title': 'Aligning Model Uncertainty with Human Perception', 'desc': 'This paper explores how to measure uncertainty in large language models to improve their control and enhance user trust. It examines various uncertainty measures, particularly focusing on Bayesian methods and a new approach called top-k entropy, to see how well they align with human perceptions of uncertainty. The study reveals that while some measures lose their effectiveness as model size increases, combining different measures can maintain a strong correlation with human behavior regardless of model size. Ultimately, the findings suggest that a multi-measure approach can lead to better alignment with human understanding of uncertainty in language models.'}, 'zh': {'title': '量化不确定性，增强用户信任', 'desc': '本研究探讨了大型语言模型的不确定性量化，以便更好地控制模型并增强用户信任。我们分析了多种不确定性度量，旨在找出与人类群体不确定性相关的度量。研究发现，贝叶斯度量和一种变体的熵度量（top-k熵）在模型规模变化时与人类行为一致。通过多元线性回归，我们发现结合多种不确定性度量可以在减少规模依赖性的同时，保持与人类行为的相似性。'}}}, {'id': 'https://huggingface.co/papers/2503.12720', 'title': 'GenStereo: Towards Open-World Generation of Stereo Images and\n  Unsupervised Matching', 'url': 'https://huggingface.co/papers/2503.12720', 'abstract': 'Stereo images are fundamental to numerous applications, including extended reality (XR) devices, autonomous driving, and robotics. Unfortunately, acquiring high-quality stereo images remains challenging due to the precise calibration requirements of dual-camera setups and the complexity of obtaining accurate, dense disparity maps. Existing stereo image generation methods typically focus on either visual quality for viewing or geometric accuracy for matching, but not both. We introduce GenStereo, a diffusion-based approach, to bridge this gap. The method includes two primary innovations (1) conditioning the diffusion process on a disparity-aware coordinate embedding and a warped input image, allowing for more precise stereo alignment than previous methods, and (2) an adaptive fusion mechanism that intelligently combines the diffusion-generated image with a warped image, improving both realism and disparity consistency. Through extensive training on 11 diverse stereo datasets, GenStereo demonstrates strong generalization ability. GenStereo achieves state-of-the-art performance in both stereo image generation and unsupervised stereo matching tasks. Our framework eliminates the need for complex hardware setups while enabling high-quality stereo image generation, making it valuable for both real-world applications and unsupervised learning scenarios. Project page is available at https://qjizhi.github.io/genstereo', 'score': 1, 'issue_id': 2766, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '2fdcfe786193f6cf', 'authors': ['Feng Qiao', 'Zhexiao Xiong', 'Eric Xing', 'Nathan Jacobs'], 'affiliations': ['Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2503.12720.jpg', 'data': {'categories': ['#robotics', '#video', '#3d', '#diffusion'], 'emoji': '👁️', 'ru': {'title': 'GenStereo: Революция в генерации стереоизображений с помощью диффузионных моделей', 'desc': 'GenStereo - это новый подход к генерации стереоизображений, основанный на диффузионных моделях. Метод использует встраивание координат с учетом диспаратности и деформированное входное изображение для точного стереовыравнивания. GenStereo также включает адаптивный механизм слияния, который объединяет сгенерированное и деформированное изображения для улучшения реалистичности и согласованности диспаратности. Обученный на 11 разнообразных стереодатасетах, GenStereo демонстрирует высокую обобщающую способность и достигает наилучших результатов в задачах генерации стереоизображений и неконтролируемого стереосопоставления.'}, 'en': {'title': 'GenStereo: Bridging Visual Quality and Geometric Accuracy in Stereo Image Generation', 'desc': 'This paper presents GenStereo, a novel diffusion-based method for generating high-quality stereo images. It addresses the challenges of stereo image generation by focusing on both visual quality and geometric accuracy, which are often at odds in existing methods. GenStereo innovates by conditioning the diffusion process on disparity-aware embeddings and using an adaptive fusion mechanism to enhance realism and consistency. The method shows strong performance across various stereo datasets, making it suitable for applications in extended reality, autonomous driving, and robotics without the need for complex hardware setups.'}, 'zh': {'title': 'GenStereo：高质量立体图像生成的新方法', 'desc': '本论文介绍了一种名为GenStereo的生成方法，旨在提高立体图像的质量和几何准确性。该方法通过条件扩散过程，结合视差感知的坐标嵌入和变形输入图像，实现更精确的立体对齐。GenStereo还采用了一种自适应融合机制，智能地将生成的图像与变形图像结合，从而提高了图像的真实感和视差一致性。经过在11个多样化的立体数据集上的广泛训练，GenStereo在立体图像生成和无监督立体匹配任务中表现出色，具有很强的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.08153', 'title': 'WISA: World Simulator Assistant for Physics-Aware Text-to-Video\n  Generation', 'url': 'https://huggingface.co/papers/2503.08153', 'abstract': "Recent rapid advancements in text-to-video (T2V) generation, such as SoRA and Kling, have shown great potential for building world simulators. However, current T2V models struggle to grasp abstract physical principles and generate videos that adhere to physical laws. This challenge arises primarily from a lack of clear guidance on physical information due to a significant gap between abstract physical principles and generation models. To this end, we introduce the World Simulator Assistant (WISA), an effective framework for decomposing and incorporating physical principles into T2V models. Specifically, WISA decomposes physical principles into textual physical descriptions, qualitative physical categories, and quantitative physical properties. To effectively embed these physical attributes into the generation process, WISA incorporates several key designs, including Mixture-of-Physical-Experts Attention (MoPA) and a Physical Classifier, enhancing the model's physics awareness. Furthermore, most existing datasets feature videos where physical phenomena are either weakly represented or entangled with multiple co-occurring processes, limiting their suitability as dedicated resources for learning explicit physical principles. We propose a novel video dataset, WISA-32K, collected based on qualitative physical categories. It consists of 32,000 videos, representing 17 physical laws across three domains of physics: dynamics, thermodynamics, and optics. Experimental results demonstrate that WISA can effectively enhance the compatibility of T2V models with real-world physical laws, achieving a considerable improvement on the VideoPhy benchmark. The visual exhibitions of WISA and WISA-32K are available in the https://360cvgroup.github.io/WISA/.", 'score': 0, 'issue_id': 2766, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '60c6c27b7b32f442', 'authors': ['Jing Wang', 'Ao Ma', 'Ke Cao', 'Jun Zheng', 'Zhanjie Zhang', 'Jiasong Feng', 'Shanyuan Liu', 'Yuhang Ma', 'Bo Cheng', 'Dawei Leng', 'Yuhui Yin', 'Xiaodan Liang'], 'affiliations': ['AI Research', 'Peng Cheng Laboratory', 'Shenzhen Campus of Sun Yat-Sen University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08153.jpg', 'data': {'categories': ['#games', '#benchmark', '#video', '#dataset', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'WISA: обучение моделей T2V физическим законам реального мира', 'desc': 'Статья представляет WISA (World Simulator Assistant) - фреймворк для внедрения физических принципов в модели генерации текста в видео (T2V). WISA разбивает физические принципы на текстовые описания, качественные категории и количественные свойства. Для улучшения физической осведомленности модели используются такие методы как Mixture-of-Physical-Experts Attention и Physical Classifier. Также авторы создали датасет WISA-32K из 32 000 видео, демонстрирующих 17 физических законов из трех областей физики.'}, 'en': {'title': 'Bridging Text-to-Video with Real-World Physics', 'desc': "This paper presents the World Simulator Assistant (WISA), a framework designed to improve text-to-video (T2V) generation by integrating physical principles into the models. WISA breaks down physical concepts into textual descriptions, qualitative categories, and quantitative properties, allowing T2V models to better understand and apply these principles. It introduces innovative components like Mixture-of-Physical-Experts Attention (MoPA) and a Physical Classifier to enhance the model's awareness of physics. Additionally, the authors create a new dataset, WISA-32K, containing 32,000 videos that illustrate 17 physical laws, which helps train T2V models to align more closely with real-world physics."}, 'zh': {'title': '提升文本到视频生成的物理意识', 'desc': '本文介绍了一种新的框架，称为世界模拟助手（WISA），旨在改善文本到视频生成（T2V）模型对物理原则的理解。WISA通过将物理原则分解为文本描述、定性类别和定量属性，帮助生成模型更好地融入物理信息。该框架还引入了混合物理专家注意力（MoPA）和物理分类器等设计，增强了模型的物理意识。此外，WISA还提出了一个新的视频数据集WISA-32K，包含32,000个视频，涵盖17条物理定律，旨在为学习明确的物理原则提供更合适的资源。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (5)', '#agi (2)', '#alignment (3)', '#architecture (1)', '#audio (1)', '#benchmark (10)', '#cv (6)', '#data (1)', '#dataset (9)', '#diffusion (6)', '#ethics', '#games (4)', '#graphs', '#hallucinations (2)', '#healthcare (1)', '#inference (1)', '#interpretability (2)', '#leakage', '#long_context (1)', '#low_resource (1)', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (8)', '#open_source (3)', '#optimization (6)', '#plp', '#rag (1)', '#reasoning (8)', '#rl (1)', '#rlhf (2)', '#robotics (4)', '#science (1)', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic (1)', '#training (9)', '#transfer_learning (1)', '#video (9)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-03-18 19:08',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-18 19:08')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-18 19:08')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    