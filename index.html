
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 45 papers. March 11.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">11 марта</span> | <span id="title-articles-count">45 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-03-10.html">⬅️ <span id="prev-date">10.03</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-03-12.html">➡️ <span id="next-date">12.03</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-03.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'};
        let feedDateNext = {'ru': '12.03', 'en': '03/12', 'zh': '3月12日'};
        let feedDatePrev = {'ru': '10.03', 'en': '03/10', 'zh': '3月10日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.03601', 'title': 'Feature-Level Insights into Artificial Text Detection with Sparse\n  Autoencoders', 'url': 'https://huggingface.co/papers/2503.03601', 'abstract': 'Artificial Text Detection (ATD) is becoming increasingly important with the rise of advanced Large Language Models (LLMs). Despite numerous efforts, no single algorithm performs consistently well across different types of unseen text or guarantees effective generalization to new LLMs. Interpretability plays a crucial role in achieving this goal. In this study, we enhance ATD interpretability by using Sparse Autoencoders (SAE) to extract features from Gemma-2-2b residual stream. We identify both interpretable and efficient features, analyzing their semantics and relevance through domain- and model-specific statistics, a steering approach, and manual or LLM-based interpretation. Our methods offer valuable insights into how texts from various models differ from human-written content. We show that modern LLMs have a distinct writing style, especially in information-dense domains, even though they can produce human-like outputs with personalized prompts.', 'score': 120, 'issue_id': 2634, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': 'd045a8e2a39262c9', 'authors': ['Kristian Kuznetsov', 'Laida Kushnareva', 'Polina Druzhinina', 'Anton Razzhigaev', 'Anastasia Voznyuk', 'Irina Piontkovskaya', 'Evgeny Burnaev', 'Serguei Barannikov'], 'affiliations': ['AI Foundation and Algorithm Lab', 'Artificial Intelligence Research Institute (AIRI)', 'CNRS, Université Paris Cité, France', 'Moscow Institute of Physics and Technology', 'Skolkovo Institute of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.03601.jpg', 'data': {'categories': ['#multimodal', '#cv', '#interpretability', '#data'], 'emoji': '🔍', 'ru': {'title': 'Раскрывая секреты искусственных текстов: новый подход к интерпретируемому обнаружению', 'desc': 'Исследование посвящено улучшению интерпретируемости методов обнаружения искусственного текста (ATD) с использованием разреженных автоэнкодеров для извлечения признаков из остаточного потока модели Gemma-2-2b. Авторы анализируют семантику и релевантность выделенных признаков с помощью статистических методов и интерпретации. Результаты показывают, что современные языковые модели имеют отличительный стиль письма, особенно в информационно-насыщенных областях. Исследование предлагает ценные insights о том, как тексты, созданные различными моделями, отличаются от контента, написанного людьми.'}, 'en': {'title': 'Enhancing Text Detection with Interpretability in AI Models', 'desc': 'This paper focuses on improving Artificial Text Detection (ATD) by enhancing its interpretability using Sparse Autoencoders (SAE). The authors extract features from the residual stream of the Gemma-2-2b model to identify both interpretable and efficient characteristics of text. They analyze these features through various statistical methods and interpretations to understand how machine-generated texts differ from human-written ones. The study reveals that modern Large Language Models (LLMs) exhibit a unique writing style, particularly in information-dense areas, despite their ability to generate human-like text.'}, 'zh': {'title': '提升人工文本检测的可解释性', 'desc': '随着大型语言模型（LLMs）的发展，人工文本检测（ATD）变得越来越重要。尽管已有许多努力，但没有单一算法能够在不同类型的未见文本中始终表现良好，也无法保证对新LLM的有效泛化。可解释性在实现这一目标中起着关键作用。我们通过使用稀疏自编码器（SAE）从Gemma-2-2b残差流中提取特征，增强了ATD的可解释性，分析了文本与人类写作内容的差异。'}}}, {'id': 'https://huggingface.co/papers/2503.07605', 'title': 'SEAP: Training-free Sparse Expert Activation Pruning Unlock the\n  Brainpower of Large Language Models', 'url': 'https://huggingface.co/papers/2503.07605', 'abstract': "Large Language Models have achieved remarkable success across various natural language processing tasks, yet their high computational cost during inference remains a major bottleneck. This paper introduces Sparse Expert Activation Pruning (SEAP), a training-free pruning method that selectively retains task-relevant parameters to reduce inference overhead. Inspired by the clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and prunes the model while preserving task performance and enhancing computational efficiency. Experimental results demonstrate that SEAP significantly reduces computational overhead while maintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both WandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2% performance drop compared to the dense model. These findings highlight SEAP's scalability and effectiveness, making it a promising approach for optimizing large-scale LLMs.", 'score': 56, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': 'eaf9c07e3673cecc', 'authors': ['Xun Liang', 'Hanyu Wang', 'Huayi Lai', 'Simin Niu', 'Shichao Song', 'Jiawei Yang', 'Jihao Zhao', 'Feiyu Xiong', 'Bo Tang', 'Zhiyu Li'], 'affiliations': ['Institute for Advanced Algorithms Research, Shanghai, China', 'School of Information, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.07605.jpg', 'data': {'categories': ['#inference', '#optimization', '#training'], 'emoji': '✂️', 'ru': {'title': 'Эффективное обрезание нейросетей без потери качества', 'desc': 'Статья представляет метод Sparse Expert Activation Pruning (SEAP) для оптимизации больших языковых моделей. SEAP выборочно сохраняет параметры, релевантные для конкретной задачи, что позволяет снизить вычислительные затраты при инференсе. Метод основан на выявлении паттернов активации экспертов, специфичных для задачи. Эксперименты показывают, что SEAP значительно сокращает вычислительные затраты при сохранении высокой точности модели.'}, 'en': {'title': 'Optimize LLMs with Sparse Expert Activation Pruning!', 'desc': "This paper presents Sparse Expert Activation Pruning (SEAP), a novel method designed to reduce the computational cost of large language models (LLMs) during inference without requiring additional training. SEAP works by identifying and retaining only the parameters that are relevant to specific tasks, effectively pruning the model while maintaining its performance. The method leverages the clustering patterns of hidden states and activations to optimize the model's efficiency. Experimental results show that SEAP can significantly lower computational overhead while achieving competitive accuracy, making it a scalable solution for optimizing LLMs."}, 'zh': {'title': '稀疏专家激活剪枝：优化大型语言模型的新方法', 'desc': '大型语言模型在自然语言处理任务中取得了显著成功，但推理时的高计算成本仍然是一个主要瓶颈。本文提出了一种名为稀疏专家激活剪枝（SEAP）的方法，该方法在不需要训练的情况下，选择性地保留与任务相关的参数，以减少推理开销。SEAP通过分析隐藏状态和激活的聚类模式，识别特定任务的专家激活模式，从而在保持任务性能的同时优化计算效率。实验结果表明，SEAP在减少计算开销的同时，保持了竞争力的准确性，展示了其在优化大规模语言模型方面的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.07365', 'title': 'MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.07365', 'abstract': "We present MM-Eureka, a multimodal reasoning model that successfully extends large-scale rule-based reinforcement learning (RL) to multimodal reasoning. While rule-based RL has shown remarkable success in improving LLMs' reasoning abilities in text domains, its application to multimodal settings has remained challenging. Our work reproduces key characteristics of text-based RL systems like DeepSeek-R1 in the multimodal space, including steady increases in accuracy reward and response length, and the emergence of reflection behaviors. We demonstrate that both instruction-tuned and pre-trained models can develop strong multimodal reasoning capabilities through rule-based RL without supervised fine-tuning, showing superior data efficiency compared to alternative approaches. We open-source our complete pipeline to foster further research in this area. We release all our codes, models, data, etc. at https://github.com/ModalMinds/MM-EUREKA", 'score': 44, 'issue_id': 2630, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '765d475b38d9f289', 'authors': ['Fanqing Meng', 'Lingxiao Du', 'Zongkai Liu', 'Zhixiang Zhou', 'Quanfeng Lu', 'Daocheng Fu', 'Botian Shi', 'Wenhai Wang', 'Junjun He', 'Kaipeng Zhang', 'Ping Luo', 'Yu Qiao', 'Qiaosheng Zhang', 'Wenqi Shao'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.07365.jpg', 'data': {'categories': ['#rl', '#multimodal', '#reasoning', '#rag', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'Мультимодальное рассуждение через обучение с подкреплением', 'desc': 'MM-Eureka – это мультимодальная модель рассуждений, которая успешно расширяет масштабное обучение с подкреплением на основе правил для мультимодальных задач. Модель демонстрирует ключевые характеристики текстовых систем обучения с подкреплением, включая устойчивое повышение точности и длины ответов, а также появление рефлексивного поведения. Исследование показывает, что как модели, настроенные на инструкции, так и предварительно обученные модели могут развивать сильные мультимодальные способности рассуждения без контролируемой тонкой настройки. Авторы открыли исходный код всего конвейера для дальнейших исследований в этой области.'}, 'en': {'title': 'Unlocking Multimodal Reasoning with MM-Eureka!', 'desc': 'MM-Eureka is a new model that enhances reasoning across different types of data, like text and images, using a method called rule-based reinforcement learning (RL). This approach builds on successful techniques from text-based RL, allowing the model to improve its accuracy and response quality in multimodal contexts. The model shows that it can learn effectively without needing extra labeled data, making it more efficient than other methods. By sharing our tools and findings, we aim to encourage more research in multimodal reasoning.'}, 'zh': {'title': 'MM-Eureka：多模态推理的新突破', 'desc': '我们提出了MM-Eureka，这是一个多模态推理模型，成功地将大规模基于规则的强化学习扩展到多模态推理领域。虽然基于规则的强化学习在文本领域提升大型语言模型的推理能力方面取得了显著成功，但在多模态环境中的应用仍然具有挑战性。我们的工作在多模态空间中重现了文本基础强化学习系统的关键特征，包括准确性奖励和响应长度的稳定增加，以及反思行为的出现。我们展示了无监督微调的情况下，指令调优和预训练模型都能通过基于规则的强化学习发展出强大的多模态推理能力，且数据效率优于其他方法。'}}}, {'id': 'https://huggingface.co/papers/2503.07002', 'title': 'Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue\n  Learning', 'url': 'https://huggingface.co/papers/2503.07002', 'abstract': 'Multimodal large language models (MLLMs), built on large-scale pre-trained vision towers and language models, have shown great capabilities in multimodal understanding. However, most existing MLLMs are trained on single-turn vision question-answering tasks, which do not accurately reflect real-world human conversations. In this paper, we introduce MMDiag, a multi-turn multimodal dialogue dataset. This dataset is collaboratively generated through deliberately designed rules and GPT assistance, featuring strong correlations between questions, between questions and images, and among different image regions; thus aligning more closely with real-world scenarios. MMDiag serves as a strong benchmark for multi-turn multimodal dialogue learning and brings more challenges to the grounding and reasoning capabilities of MLLMs. Further, inspired by human vision processing, we present DiagNote, an MLLM equipped with multimodal grounding and reasoning capabilities. DiagNote consists of two modules (Deliberate and Gaze) interacting with each other to perform Chain-of-Thought and annotations respectively, throughout multi-turn dialogues. We empirically demonstrate the advantages of DiagNote in both grounding and jointly processing and reasoning with vision and language information over existing MLLMs.', 'score': 31, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '1f57fb5c5398efbc', 'authors': ['Jiazheng Liu', 'Sipeng Zheng', 'Börje F. Karlsson', 'Zongqing Lu'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07002.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#games', '#multimodal', '#dataset', '#architecture'], 'emoji': '🗣️', 'ru': {'title': 'Новый подход к мультимодальным диалогам: от реалистичных данных к продвинутым моделям', 'desc': 'Статья представляет MMDiag - новый набор данных для многоходовых мультимодальных диалогов, созданный с помощью GPT. Этот датасет лучше отражает реальные сценарии общения, чем существующие однотурные наборы данных для вопросно-ответных задач. Авторы также предлагают DiagNote - мультимодальную языковую модель с улучшенными возможностями заземления и рассуждений. DiagNote использует два взаимодействующих модуля для цепочки рассуждений и аннотаций в ходе диалога.'}, 'en': {'title': 'Enhancing Multimodal Dialogue with MMDiag and DiagNote', 'desc': 'This paper presents MMDiag, a new dataset designed for multi-turn multimodal dialogue, which enhances the training of multimodal large language models (MLLMs). Unlike previous datasets that focus on single-turn tasks, MMDiag captures the complexities of real-world conversations by incorporating strong correlations between questions and images. The authors introduce DiagNote, an MLLM that features two interactive modules for improved grounding and reasoning in dialogues. The results show that DiagNote outperforms existing MLLMs in processing and reasoning with both visual and textual information.'}, 'zh': {'title': '提升多模态对话的智能化', 'desc': '这篇论文介绍了一种新的多模态对话数据集MMDiag，旨在改善现有多模态大语言模型（MLLMs）在多轮对话中的表现。MMDiag通过精心设计的规则和GPT的辅助生成，包含了问题之间、问题与图像之间以及不同图像区域之间的强相关性，更贴近真实的人类对话场景。论文还提出了DiagNote，一个具备多模态基础和推理能力的MLLM，包含两个相互作用的模块（Deliberate和Gaze），用于在多轮对话中进行思维链和注释。通过实验证明，DiagNote在基础和共同处理视觉与语言信息的推理能力上优于现有的MLLMs。'}}}, {'id': 'https://huggingface.co/papers/2503.07314', 'title': 'Automated Movie Generation via Multi-Agent CoT Planning', 'url': 'https://huggingface.co/papers/2503.07314', 'abstract': 'Existing long-form video generation frameworks lack automated planning, requiring manual input for storylines, scenes, cinematography, and character interactions, resulting in high costs and inefficiencies. To address these challenges, we present MovieAgent, an automated movie generation via multi-agent Chain of Thought (CoT) planning. MovieAgent offers two key advantages: 1) We firstly explore and define the paradigm of automated movie/long-video generation. Given a script and character bank, our MovieAgent can generates multi-scene, multi-shot long-form videos with a coherent narrative, while ensuring character consistency, synchronized subtitles, and stable audio throughout the film. 2) MovieAgent introduces a hierarchical CoT-based reasoning process to automatically structure scenes, camera settings, and cinematography, significantly reducing human effort. By employing multiple LLM agents to simulate the roles of a director, screenwriter, storyboard artist, and location manager, MovieAgent streamlines the production pipeline. Experiments demonstrate that MovieAgent achieves new state-of-the-art results in script faithfulness, character consistency, and narrative coherence. Our hierarchical framework takes a step forward and provides new insights into fully automated movie generation. The code and project website are available at: https://github.com/showlab/MovieAgent and https://weijiawu.github.io/MovieAgent.', 'score': 25, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '2e5c000925250863', 'authors': ['Weijia Wu', 'Zeyu Zhu', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.07314.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#multimodal', '#story_generation', '#video', '#agents'], 'emoji': '🎬', 'ru': {'title': 'Автоматизированное создание фильмов с помощью ИИ-агентов', 'desc': 'MovieAgent - это новая система автоматизированной генерации длинных видео с использованием мультиагентного планирования на основе цепочки размышлений (Chain of Thought). Система способна создавать многосценные фильмы с согласованным сюжетом, сохраняя постоянство персонажей и синхронизацию субтитров. MovieAgent использует иерархический процесс рассуждений для автоматического структурирования сцен и настроек камеры, значительно сокращая человеческие усилия. Эксперименты показывают, что MovieAgent достигает новых лучших результатов в верности сценарию, постоянстве персонажей и согласованности повествования.'}, 'en': {'title': 'Automating Movie Magic with MovieAgent!', 'desc': 'This paper introduces MovieAgent, a novel framework for automated long-form video generation that eliminates the need for manual planning of storylines and scenes. It utilizes a multi-agent Chain of Thought (CoT) reasoning process to create coherent narratives while maintaining character consistency and synchronized audio. By simulating various production roles, MovieAgent streamlines the filmmaking process, significantly reducing the effort required from human creators. Experimental results show that MovieAgent sets new benchmarks in script adherence, character consistency, and overall narrative quality.'}, 'zh': {'title': '自动化电影生成的新纪元', 'desc': '现有的长视频生成框架缺乏自动化规划，需要手动输入故事情节、场景、摄影和角色互动，导致高成本和低效率。为了解决这些问题，我们提出了MovieAgent，通过多智能体的思维链（CoT）规划实现自动化电影生成。MovieAgent的两个主要优势是：首先，我们探索并定义了自动化电影/长视频生成的范式；其次，MovieAgent引入了基于层次的CoT推理过程，自动构建场景、摄像机设置和摄影，大大减少了人力投入。实验表明，MovieAgent在剧本忠实度、角色一致性和叙事连贯性方面达到了新的最先进水平。'}}}, {'id': 'https://huggingface.co/papers/2503.07216', 'title': 'FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA\n  Subparameter Updates', 'url': 'https://huggingface.co/papers/2503.07216', 'abstract': "Federated Learning (FL) is a widely used framework for training models in a decentralized manner, ensuring that the central server does not have direct access to data from local clients. However, this approach may still fail to fully preserve data privacy, as models from local clients are exposed to the central server during the aggregation process. This issue becomes even more critical when training vision-language models (VLMs) with FL, as VLMs can easily memorize training data instances, making them vulnerable to membership inference attacks (MIAs). To address this challenge, we propose the FedRand framework, which avoids disclosing the full set of client parameters. In this framework, each client randomly selects subparameters of Low-Rank Adaptation (LoRA) from the server and keeps the remaining counterparts of the LoRA weights as private parameters. After training both parameters on the client's private dataset, only the non-private <PRE_TAG>client parameters</POST_TAG> are sent back to the server for aggregation. This approach mitigates the risk of exposing client-side VLM parameters, thereby enhancing data privacy. We empirically validate that FedRand improves robustness against MIAs compared to relevant baselines while achieving accuracy comparable to methods that communicate full LoRA parameters across several benchmark datasets.", 'score': 24, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '3d23c61b24a599ee', 'authors': ['Sangwoo Park', 'Seanie Lee', 'Byungjoo Kim', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'Graduate School of AI, KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.07216.jpg', 'data': {'categories': ['#security', '#benchmark', '#data', '#ethics', '#multimodal', '#rl'], 'emoji': '🔐', 'ru': {'title': 'FedRand: Защита конфиденциальности в федеративном обучении визуально-языковых моделей', 'desc': 'Статья представляет новый подход к федеративному обучению (FL) для визуально-языковых моделей (VLM), называемый FedRand. Эта методика направлена на повышение конфиденциальности данных путем случайного выбора подпараметров Low-Rank Adaptation (LoRA) для обмена между клиентами и сервером. FedRand позволяет сохранять часть параметров LoRA приватными, снижая риск утечки информации о данных клиентов. Эмпирические исследования показывают, что FedRand повышает устойчивость к атакам по выводу членства (MIA) по сравнению с базовыми методами, сохраняя при этом сопоставимую точность.'}, 'en': {'title': 'Enhancing Privacy in Federated Learning with FedRand', 'desc': 'Federated Learning (FL) allows models to be trained on local data without sharing it with a central server, but it can still risk data privacy during model aggregation. This paper introduces the FedRand framework, which enhances privacy by having clients send only a subset of their model parameters back to the server. Specifically, it utilizes Low-Rank Adaptation (LoRA) to keep certain parameters private while still allowing effective model training. The results show that FedRand not only protects against membership inference attacks but also maintains accuracy similar to traditional methods that share full parameters.'}, 'zh': {'title': 'FedRand：保护数据隐私的联邦学习新框架', 'desc': '联邦学习（FL）是一种去中心化的模型训练框架，确保中央服务器无法直接访问本地客户端的数据。然而，在聚合过程中，本地客户端的模型仍可能暴露给中央服务器，从而影响数据隐私。特别是在训练视觉-语言模型（VLMs）时，这种风险更为严重，因为VLMs容易记住训练数据实例，容易受到成员推断攻击（MIAs）。为了解决这个问题，我们提出了FedRand框架，该框架通过随机选择低秩适应（LoRA）的子参数，避免泄露完整的客户端参数，从而增强数据隐私。'}}}, {'id': 'https://huggingface.co/papers/2503.07067', 'title': 'DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs', 'url': 'https://huggingface.co/papers/2503.07067', 'abstract': 'Despite the success of distillation in large language models (LLMs), most prior work applies identical loss functions to both teacher- and student-generated data. These strategies overlook the synergy between loss formulations and data types, leading to a suboptimal performance boost in student models. To address this, we propose DistiLLM-2, a contrastive approach that simultaneously increases the likelihood of teacher responses and decreases that of student responses by harnessing this synergy. Our extensive experiments show that DistiLLM-2 not only builds high-performing student models across a wide range of tasks, including instruction-following and code generation, but also supports diverse applications, such as preference alignment and vision-language extensions. These findings highlight the potential of a contrastive approach to enhance the efficacy of LLM distillation by effectively aligning teacher and student models across varied data types.', 'score': 20, 'issue_id': 2632, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '8eb39990e619611e', 'authors': ['Jongwoo Ko', 'Tianyi Chen', 'Sungnyun Kim', 'Tianyu Ding', 'Luming Liang', 'Ilya Zharkov', 'Se-Young Yun'], 'affiliations': ['KAIST AI', 'Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2503.07067.jpg', 'data': {'categories': ['#multimodal', '#alignment', '#training', '#optimization'], 'emoji': '🔬', 'ru': {'title': 'Контрастивная дистилляция: новый подход к обучению языковых моделей', 'desc': 'DistiLLM-2 - это новый подход к дистилляции больших языковых моделей (LLM), использующий контрастивное обучение. Он одновременно увеличивает вероятность ответов учителя и уменьшает вероятность ответов ученика, эффективно используя синергию между формулировками функции потерь и типами данных. Эксперименты показывают, что DistiLLM-2 создает высокопроизводительные модели-ученики для широкого спектра задач, включая следование инструкциям и генерацию кода. Метод также поддерживает различные приложения, такие как выравнивание предпочтений и расширения для работы с визуальными данными.'}, 'en': {'title': 'Enhancing LLM Distillation with Contrastive Learning', 'desc': 'This paper introduces DistiLLM-2, a novel approach to improve the distillation process in large language models (LLMs). Unlike previous methods that use the same loss functions for both teacher and student models, DistiLLM-2 employs a contrastive strategy that boosts the likelihood of correct teacher responses while reducing the likelihood of incorrect student responses. The results demonstrate that this method significantly enhances the performance of student models on various tasks, including instruction-following and code generation. Additionally, DistiLLM-2 shows versatility in applications such as preference alignment and vision-language tasks, emphasizing the importance of tailored loss functions in model distillation.'}, 'zh': {'title': '对比方法提升大型语言模型蒸馏效果', 'desc': '尽管蒸馏在大型语言模型（LLMs）中取得了成功，但大多数先前的研究对教师和学生生成的数据使用相同的损失函数。这种策略忽视了损失公式与数据类型之间的协同作用，导致学生模型的性能提升不理想。为了解决这个问题，我们提出了DistiLLM-2，这是一种对比方法，能够同时提高教师响应的可能性并降低学生响应的可能性。我们的广泛实验表明，DistiLLM-2不仅在多种任务中构建了高性能的学生模型，还支持多样化的应用，如偏好对齐和视觉-语言扩展。'}}}, {'id': 'https://huggingface.co/papers/2503.07027', 'title': 'EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer', 'url': 'https://huggingface.co/papers/2503.07027', 'abstract': 'Recent advancements in Unet-based diffusion models, such as ControlNet and IP-Adapter, have introduced effective spatial and subject control mechanisms. However, the DiT (Diffusion Transformer) architecture still struggles with efficient and flexible control. To tackle this issue, we propose EasyControl, a novel framework designed to unify condition-guided diffusion transformers with high efficiency and flexibility. Our framework is built on three key innovations. First, we introduce a lightweight <PRE_TAG>Condition Injection LoRA Module</POST_TAG>. This module processes conditional signals in isolation, acting as a plug-and-play solution. It avoids modifying the base model weights, ensuring compatibility with customized models and enabling the flexible injection of diverse conditions. Notably, this module also supports harmonious and robust zero-shot multi-condition generalization, even when trained only on single-condition data. Second, we propose a <PRE_TAG>Position-Aware Training Paradigm</POST_TAG>. This approach standardizes input conditions to fixed resolutions, allowing the generation of images with arbitrary aspect ratios and flexible resolutions. At the same time, it optimizes computational efficiency, making the framework more practical for real-world applications. Third, we develop a Causal Attention Mechanism combined with the KV Cache technique, adapted for conditional generation tasks. This innovation significantly reduces the latency of image synthesis, improving the overall efficiency of the framework. Through extensive experiments, we demonstrate that EasyControl achieves exceptional performance across various application scenarios. These innovations collectively make our framework highly efficient, flexible, and suitable for a wide range of tasks.', 'score': 18, 'issue_id': 2630, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '1b1589f9873720c7', 'authors': ['Yuxuan Zhang', 'Yirui Yuan', 'Yiren Song', 'Haofan Wang', 'Jiaming Liu'], 'affiliations': ['Liblib AI', 'National University of Singapore', 'ShanghaiTech University', 'Tiamat AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.07027.jpg', 'data': {'categories': ['#cv', '#architecture', '#training', '#optimization', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'EasyControl: гибкое и эффективное управление диффузионными трансформерами', 'desc': 'Статья представляет EasyControl - новую систему для улучшения контроля в диффузионных трансформерах. Основные инновации включают модуль внедрения условий на основе LoRA, парадигму обучения с учетом позиции и механизм причинного внимания с KV-кэшем. EasyControl обеспечивает эффективное и гибкое управление генерацией изображений с произвольными пропорциями и разрешением. Эксперименты показывают высокую производительность системы в различных сценариях применения.'}, 'en': {'title': 'EasyControl: Unifying Efficiency and Flexibility in Diffusion Transformers', 'desc': 'This paper presents EasyControl, a new framework that enhances the efficiency and flexibility of condition-guided diffusion transformers, particularly addressing the limitations of the DiT architecture. It introduces a lightweight Condition Injection LoRA Module that allows for the independent processing of conditional signals without altering the base model, enabling easy integration of various conditions. Additionally, the Position-Aware Training Paradigm standardizes input conditions, facilitating the generation of images with different aspect ratios while optimizing computational resources. Lastly, the Causal Attention Mechanism with KV Cache significantly reduces image synthesis latency, making EasyControl a robust solution for diverse applications in machine learning.'}, 'zh': {'title': '高效灵活的条件生成框架EasyControl', 'desc': '本文提出了一种名为EasyControl的新框架，旨在提高基于扩散变换器的条件生成模型的效率和灵活性。该框架包含三个关键创新：首先，轻量级的条件注入LoRA模块可以独立处理条件信号，避免修改基础模型权重，从而实现与定制模型的兼容性。其次，位置感知训练范式标准化输入条件，使得生成任意长宽比和灵活分辨率的图像成为可能，同时优化计算效率。最后，结合KV缓存技术的因果注意机制显著降低了图像合成的延迟，提升了整体效率。'}}}, {'id': 'https://huggingface.co/papers/2503.06680', 'title': 'FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation\n  for Feature Implementation', 'url': 'https://huggingface.co/papers/2503.06680', 'abstract': "Implementing new features in repository-level codebases is a crucial application of code generation models. However, current benchmarks lack a dedicated evaluation framework for this capability. To fill this gap, we introduce FEA-Bench, a benchmark designed to assess the ability of large language models (LLMs) to perform incremental development within code repositories. We collect pull requests from 83 GitHub repositories and use rule-based and intent-based filtering to construct task instances focused on new feature development. Each task instance containing code changes is paired with relevant unit test files to ensure that the solution can be verified. The feature implementation requires LLMs to simultaneously possess code completion capabilities for new components and code editing abilities for other relevant parts in the code repository, providing a more comprehensive evaluation method of LLMs' automated software engineering capabilities. Experimental results show that LLMs perform significantly worse in the FEA-Bench, highlighting considerable challenges in such repository-level incremental code development.", 'score': 15, 'issue_id': 2630, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': '4394ce17a18696a3', 'authors': ['Wei Li', 'Xin Zhang', 'Zhongxin Guo', 'Shaoguang Mao', 'Wen Luo', 'Guangyue Peng', 'Yangyu Huang', 'Houfeng Wang', 'Scarlett Li'], 'affiliations': ['Microsoft Research Asia', 'State Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.06680.jpg', 'data': {'categories': ['#plp', '#dataset', '#optimization', '#benchmark'], 'emoji': '🧪', 'ru': {'title': 'FEA-Bench: новый вызов для языковых моделей в разработке ПО', 'desc': 'FEA-Bench - это новый бенчмарк для оценки способности больших языковых моделей (LLM) выполнять инкрементальную разработку в репозиториях кода. Он основан на пул-реквестах из 83 репозиториев GitHub и включает задачи по разработке новых функций. Каждая задача содержит изменения кода и соответствующие модульные тесты для верификации. Результаты экспериментов показывают, что LLM значительно хуже справляются с задачами FEA-Bench, что указывает на серьезные проблемы в автоматизированной разработке на уровне репозиториев.'}, 'en': {'title': 'FEA-Bench: Evaluating Code Generation in Repositories', 'desc': "This paper introduces FEA-Bench, a new benchmark for evaluating large language models (LLMs) in the context of code generation for new features in software repositories. It addresses the lack of dedicated evaluation frameworks by collecting pull requests from 83 GitHub repositories and creating task instances that focus on incremental development. Each task is designed to test the LLM's ability to generate code changes while ensuring that these changes can be verified through associated unit tests. The findings reveal that LLMs struggle with this type of repository-level code development, indicating significant challenges in their automated software engineering capabilities."}, 'zh': {'title': '评估代码生成模型的新基准：FEA-Bench', 'desc': '在代码生成模型中，实现新特性是一个重要的应用。当前的基准测试缺乏专门评估这一能力的框架。为此，我们提出了FEA-Bench，这是一个旨在评估大型语言模型（LLMs）在代码库中进行增量开发能力的基准。实验结果表明，LLMs在FEA-Bench中的表现显著较差，突显了在代码库级别增量开发中面临的重大挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.07608', 'title': 'AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via\n  Reinforcement Learning and Reasoning', 'url': 'https://huggingface.co/papers/2503.07608', 'abstract': 'OpenAI o1 and DeepSeek R1 achieve or even surpass human expert-level performance in complex domains like mathematics and science, with reinforcement learning (RL) and reasoning playing a crucial role. In autonomous driving, recent end-to-end models have greatly improved planning performance but still struggle with long-tailed problems due to limited common sense and reasoning abilities. Some studies integrate vision-language models (VLMs) into autonomous driving, but they typically rely on pre-trained models with simple supervised fine-tuning (SFT) on driving data, without further exploration of training strategies or optimizations specifically tailored for planning. In this paper, we propose AlphaDrive, a RL and reasoning framework for VLMs in autonomous driving. AlphaDrive introduces four GRPO-based RL rewards tailored for planning and employs a two-stage planning <PRE_TAG>reasoning training strategy</POST_TAG> that combines SFT with RL. As a result, AlphaDrive significantly improves both planning performance and training efficiency compared to using only SFT or without reasoning. Moreover, we are also excited to discover that, following RL training, AlphaDrive exhibits some emergent multimodal planning capabilities, which is critical for improving driving safety and efficiency. To the best of our knowledge, AlphaDrive is the first to integrate GRPO-based RL with planning reasoning into autonomous driving. Code will be released to facilitate future research.', 'score': 14, 'issue_id': 2632, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '74dfc85ba9f7bcc7', 'authors': ['Bo Jiang', 'Shaoyu Chen', 'Qian Zhang', 'Wenyu Liu', 'Xinggang Wang'], 'affiliations': ['Horizon Robotics', 'Huazhong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.07608.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#multimodal', '#reasoning', '#agents'], 'emoji': '🚗', 'ru': {'title': 'AlphaDrive: ИИ за рулем с обучением и рассуждением', 'desc': 'AlphaDrive - это новая система для автономного вождения, использующая визуально-языковые модели с обучением с подкреплением и рассуждениями. Она вводит четыре награды на основе GRPO, специально разработанные для планирования, и применяет двухэтапную стратегию обучения с рассуждениями. AlphaDrive значительно улучшает производительность планирования и эффективность обучения по сравнению с традиционными методами. После обучения с подкреплением система демонстрирует некоторые возникающие мультимодальные возможности планирования.'}, 'en': {'title': 'AlphaDrive: Revolutionizing Autonomous Driving with RL and Reasoning', 'desc': 'This paper presents AlphaDrive, a novel framework that enhances autonomous driving by integrating reinforcement learning (RL) and reasoning with vision-language models (VLMs). AlphaDrive employs four GRPO-based RL rewards specifically designed for planning tasks and utilizes a two-stage training strategy that combines supervised fine-tuning (SFT) with RL. The results show that AlphaDrive significantly outperforms traditional methods that rely solely on SFT, leading to improved planning performance and training efficiency. Additionally, the framework demonstrates emergent multimodal planning capabilities post-RL training, which are essential for enhancing driving safety and efficiency.'}, 'zh': {'title': 'AlphaDrive：提升自动驾驶的智能规划与推理', 'desc': '本文提出了AlphaDrive，这是一个用于自动驾驶的强化学习（RL）和推理框架。AlphaDrive引入了四种基于GRPO的RL奖励，专门针对规划任务，并采用了结合监督微调（SFT）和RL的两阶段规划推理训练策略。与仅使用SFT或不进行推理的情况相比，AlphaDrive显著提高了规划性能和训练效率。此外，经过RL训练后，AlphaDrive还展现出一些新兴的多模态规划能力，这对提高驾驶安全性和效率至关重要。'}}}, {'id': 'https://huggingface.co/papers/2503.04629', 'title': 'SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and\n  Multi-dimensional Evaluation for Automated Survey Writing', 'url': 'https://huggingface.co/papers/2503.04629', 'abstract': 'Survey paper plays a crucial role in scientific research, especially given the rapid growth of research publications. Recently, researchers have begun using LLMs to automate survey generation for better efficiency. However, the quality gap between LLM-generated surveys and those written by human remains significant, particularly in terms of outline quality and citation accuracy. To close these gaps, we introduce SurveyForge, which first generates the outline by analyzing the logical structure of human-written outlines and referring to the retrieved domain-related articles. Subsequently, leveraging high-quality papers retrieved from memory by our scholar navigation agent, SurveyForge can automatically generate and refine the content of the generated article. Moreover, to achieve a comprehensive evaluation, we construct SurveyBench, which includes 100 human-written survey papers for win-rate comparison and assesses AI-generated survey papers across three dimensions: reference, outline, and content quality. Experiments demonstrate that SurveyForge can outperform previous works such as AutoSurvey.', 'score': 13, 'issue_id': 2633, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': 'a229855316ab195d', 'authors': ['Xiangchao Yan', 'Shiyang Feng', 'Jiakang Yuan', 'Renqiu Xia', 'Bin Wang', 'Bo Zhang', 'Lei Bai'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.04629.jpg', 'data': {'categories': ['#survey', '#multimodal', '#agents', '#dataset', '#benchmark'], 'emoji': '📊', 'ru': {'title': 'SurveyForge: Автоматизация создания высококачественных обзорных статей с помощью ИИ', 'desc': 'SurveyForge - это новый подход к автоматическому созданию обзорных статей с использованием больших языковых моделей (LLM). Система сначала генерирует структуру статьи, анализируя логическую структуру обзоров, написанных людьми, и обращаясь к релевантным статьям в предметной области. Затем, используя качественные статьи, извлеченные из памяти с помощью агента навигации, SurveyForge автоматически генерирует и улучшает содержание статьи. Для оценки качества создан набор данных SurveyBench, включающий 100 обзорных статей, написанных людьми, и оценивающий сгенерированные ИИ обзоры по трем параметрам: качество ссылок, структуры и содержания.'}, 'en': {'title': 'Enhancing Automated Survey Generation with SurveyForge', 'desc': 'This paper introduces SurveyForge, a tool designed to improve the quality of automated survey generation using large language models (LLMs). It addresses the significant quality gap between LLM-generated surveys and those created by humans, particularly in outline structure and citation accuracy. SurveyForge first generates an outline by analyzing human-written surveys and retrieving relevant articles, then it refines the content using high-quality papers. The authors also present SurveyBench, a benchmark for evaluating survey papers, which shows that SurveyForge outperforms previous methods like AutoSurvey.'}, 'zh': {'title': 'SurveyForge：提升文献综述生成质量的利器', 'desc': '这篇论文介绍了SurveyForge，一个用于自动生成文献综述的工具。它通过分析人类撰写的综述大纲的逻辑结构，并参考相关领域的文章，首先生成大纲。然后，SurveyForge利用高质量的论文来自动生成和完善文章内容。研究还构建了SurveyBench，用于评估AI生成的综述论文在参考文献、结构和内容质量等三个维度的表现。'}}}, {'id': 'https://huggingface.co/papers/2503.07602', 'title': 'DreamRelation: Relation-Centric Video Customization', 'url': 'https://huggingface.co/papers/2503.07602', 'abstract': "Relational video customization refers to the creation of personalized videos that depict user-specified relations between two subjects, a crucial task for comprehending real-world visual content. While existing methods can personalize subject appearances and motions, they still struggle with complex relational video customization, where precise relational modeling and high generalization across subject categories are essential. The primary challenge arises from the intricate spatial arrangements, layout variations, and nuanced temporal dynamics inherent in relations; consequently, current models tend to overemphasize irrelevant visual details rather than capturing meaningful interactions. To address these challenges, we propose DreamRelation, a novel approach that personalizes relations through a small set of exemplar videos, leveraging two key components: Relational Decoupling Learning and Relational Dynamics Enhancement. First, in Relational Decoupling Learning, we disentangle relations from subject appearances using relation LoRA triplet and hybrid mask training strategy, ensuring better generalization across diverse relationships. Furthermore, we determine the optimal design of relation LoRA triplet by analyzing the distinct roles of the query, key, and value features within MM-DiT's attention mechanism, making DreamRelation the first relational video generation framework with explainable components. Second, in Relational Dynamics Enhancement, we introduce space-time relational contrastive loss, which prioritizes relational dynamics while minimizing the reliance on detailed subject appearances. Extensive experiments demonstrate that DreamRelation outperforms state-of-the-art methods in relational video customization. Code and models will be made publicly available.", 'score': 12, 'issue_id': 2632, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': 'a32c943630a808fc', 'authors': ['Yujie Wei', 'Shiwei Zhang', 'Hangjie Yuan', 'Biao Gong', 'Longxiang Tang', 'Xiang Wang', 'Haonan Qiu', 'Hengjia Li', 'Shuai Tan', 'Yingya Zhang', 'Hongming Shan'], 'affiliations': ['Alibaba Group', 'Ant Group', 'Fudan University', 'Nanyang Technological University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07602.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#architecture', '#open_source', '#video', '#interpretability', '#games'], 'emoji': '🎬', 'ru': {'title': 'DreamRelation: Персонализация отношений в видео через машинное обучение', 'desc': 'DreamRelation - это новый подход к персонализации отношений в видео с использованием небольшого набора видео-примеров. Метод включает два ключевых компонента: Relational Decoupling Learning и Relational Dynamics Enhancement. Relational Decoupling Learning разделяет отношения и внешний вид субъектов, используя relation LoRA triplet и гибридную стратегию обучения маскам. Relational Dynamics Enhancement вводит пространственно-временную контрастную потерю отношений, чтобы сосредоточиться на динамике отношений, а не на деталях внешнего вида субъектов.'}, 'en': {'title': 'DreamRelation: Personalizing Video Relationships with Precision', 'desc': 'This paper introduces DreamRelation, a new method for creating personalized videos that focus on the relationships between two subjects. Current techniques struggle with complex relational dynamics, often missing meaningful interactions due to an overemphasis on irrelevant details. DreamRelation addresses this by using Relational Decoupling Learning to separate relationships from appearances and Relational Dynamics Enhancement to focus on the dynamics of these relationships. The approach shows significant improvements over existing methods in generating customized relational videos, with the added benefit of explainable components in its design.'}, 'zh': {'title': 'DreamRelation：个性化视频关系建模的新方法', 'desc': '本论文提出了一种名为DreamRelation的新方法，用于个性化视频中的关系建模。该方法通过关系解耦学习和关系动态增强两个关键组件，解决了复杂关系视频定制中的挑战。通过分析查询、键和值特征在注意力机制中的作用，DreamRelation实现了可解释的关系视频生成。实验结果表明，DreamRelation在关系视频定制方面优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2503.06749', 'title': 'Vision-R1: Incentivizing Reasoning Capability in Multimodal Large\n  Language Models', 'url': 'https://huggingface.co/papers/2503.06749', 'abstract': "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. Specifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the model's ability to learn correct and complex reasoning processes on a 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of sim6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. The datasets and code will be released in: https://github.com/Osilly/Vision-R1 .", 'score': 12, 'issue_id': 2632, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': '403914241aa8967c', 'authors': ['Wenxuan Huang', 'Bohan Jia', 'Zijie Zhai', 'Shaosheng Cao', 'Zheyu Ye', 'Fei Zhao', 'Yao Hu', 'Shaohui Lin'], 'affiliations': ['East China Normal University', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2503.06749.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#multimodal', '#open_source', '#benchmark', '#reasoning', '#dataset', '#rag'], 'emoji': '🧠', 'ru': {'title': 'Vision-R1: Новый уровень мультимодальных рассуждений в ИИ', 'desc': 'Исследователи разработали Vision-R1, мультимодальную языковую модель с улучшенными способностями рассуждения. Они создали высококачественный набор данных для обучения, используя существующую MLLM и DeepSeek-R1. Для оптимизации модели применили стратегию Progressive Thinking Suppression Training и Group Relative Policy Optimization. Vision-R1-7B достигла точности 73.5% на бенчмарке MathVista, что лишь на 0.4% ниже ведущей модели OpenAI O1.'}, 'en': {'title': 'Enhancing Reasoning in MLLMs with Reinforcement Learning', 'desc': "The paper introduces DeepSeek-R1-Zero, which shows that large language models (LLMs) can develop reasoning skills through Reinforcement Learning (RL). It highlights the challenge of training LLMs directly with RL due to a lack of high-quality multimodal reasoning data. To overcome this, the authors propose a new model called Vision-R1, which utilizes a self-generated multimodal dataset for cold-start training. They also introduce a training strategy called Progressive Thinking Suppression Training (PTST) to enhance the model's reasoning capabilities, achieving significant improvements in multimodal math reasoning tasks."}, 'zh': {'title': '通过强化学习提升多模态推理能力', 'desc': 'DeepSeek-R1-Zero展示了通过强化学习（RL）使大型语言模型（LLM）具备推理能力的可能性。基于这一突破，本文探讨了如何利用强化学习提升多模态大型语言模型（MLLM）的推理能力。由于缺乏高质量的多模态推理数据，直接使用强化学习训练面临挑战，因此我们提出了Vision-R1模型，以改善多模态推理能力。我们构建了一个高质量的多模态链式思维（CoT）数据集，并通过渐进思维抑制训练（PTST）和群体相对策略优化（GRPO）策略来优化模型的推理过程。'}}}, {'id': 'https://huggingface.co/papers/2503.05244', 'title': 'WritingBench: A Comprehensive Benchmark for Generative Writing', 'url': 'https://huggingface.co/papers/2503.05244', 'abstract': "Recent advancements in large language models (LLMs) have significantly enhanced text generation capabilities, yet evaluating their performance in generative writing remains a challenge. Existing benchmarks primarily focus on generic text generation or limited in writing tasks, failing to capture the diverse requirements of high-quality written contents across various domains. To bridge this gap, we present WritingBench, a comprehensive benchmark designed to evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing creative, persuasive, informative, and technical writing. We further propose a query-dependent evaluation framework that empowers LLMs to dynamically generate instance-specific assessment criteria. This framework is complemented by a fine-tuned critic model for criteria-aware scoring, enabling evaluations in style, format and length. The framework's validity is further demonstrated by its data curation capability, which enables 7B-parameter models to approach state-of-the-art (SOTA) performance. We open-source the benchmark, along with evaluation tools and modular framework components, to advance the development of LLMs in writing.", 'score': 12, 'issue_id': 2637, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '679eb0b19323e2d2', 'authors': ['Yuning Wu', 'Jiahao Mei', 'Ming Yan', 'Chenliang Li', 'SHaopeng Lai', 'Yuran Ren', 'Zijia Wang', 'Ji Zhang', 'Mengyue Wu', 'Qin Jin', 'Fei Huang'], 'affiliations': ['Alibaba Group', 'Renmin University of China', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.05244.jpg', 'data': {'categories': ['#data', '#dataset', '#benchmark', '#story_generation', '#open_source'], 'emoji': '✍️', 'ru': {'title': 'WritingBench: новый стандарт оценки языковых моделей в письме', 'desc': 'Статья представляет WritingBench - комплексный бенчмарк для оценки языковых моделей в различных областях письма. Авторы предлагают фреймворк для динамической генерации критериев оценки и обученную модель-критик для подсчета баллов. Бенчмарк охватывает 6 основных областей и 100 подобластей письма, включая креативное, убеждающее, информативное и техническое письмо. Исследование показывает, что предложенный подход позволяет небольшим моделям приблизиться к производительности современных больших языковых моделей.'}, 'en': {'title': 'WritingBench: A New Standard for Evaluating Language Models in Writing', 'desc': 'This paper introduces WritingBench, a new benchmark for evaluating large language models (LLMs) in generative writing across six key domains and 100 subdomains. It addresses the limitations of existing benchmarks that do not adequately assess the quality of writing in various contexts. The authors propose a query-dependent evaluation framework that allows LLMs to create specific assessment criteria based on the writing task at hand. Additionally, a fine-tuned critic model is introduced to provide criteria-aware scoring, enhancing the evaluation of style, format, and length in generated texts.'}, 'zh': {'title': 'WritingBench：全面评估大型语言模型的写作能力', 'desc': '近年来，大型语言模型（LLMs）的进步显著提升了文本生成能力，但评估其在生成写作中的表现仍然是一个挑战。现有的基准主要集中在通用文本生成或有限的写作任务上，无法捕捉到高质量书面内容在不同领域的多样化需求。为了解决这个问题，我们提出了WritingBench，这是一个全面的基准，旨在评估LLMs在6个核心写作领域和100个子领域的表现，包括创意、说服性、信息性和技术写作。我们还提出了一种依赖查询的评估框架，使LLMs能够动态生成特定实例的评估标准，并通过一个经过微调的评估模型进行风格、格式和长度的评分。'}}}, {'id': 'https://huggingface.co/papers/2503.06580', 'title': 'Agent models: Internalizing Chain-of-Action Generation into Reasoning\n  models', 'url': 'https://huggingface.co/papers/2503.06580', 'abstract': 'Traditional agentic workflows rely on external prompts to manage interactions with tools and the environment, which limits the autonomy of reasoning models. We position Large Agent Models (LAMs) that internalize the generation of Chain-of-Action (CoA), enabling the model to autonomously decide when and how to use external tools. Our proposed AutoCoA framework combines supervised fine-tuning (SFT) and reinforcement learning (RL), allowing the model to seamlessly switch between reasoning and action while efficiently managing environment interactions. Main components include step-level action triggering, trajectory-level CoA optimization, and an internal world model to reduce real-environment interaction costs. Evaluations on open-domain QA tasks demonstrate that AutoCoA-trained agent models significantly outperform ReAct-based workflows in task completion, especially in tasks that require long-term reasoning and multi-step actions. Code and dataset are available at https://github.com/ADaM-BJTU/AutoCoA', 'score': 11, 'issue_id': 2631, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': '01588376bab86ceb', 'authors': ['Yuxiang Zhang', 'Yuqi Yang', 'Jiangming Shu', 'Xinyan Wen', 'Jitao Sang'], 'affiliations': ['School of Computer Science and Technology, Beijing Jiaotong University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.06580.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#agents', '#rl', '#training'], 'emoji': '🤖', 'ru': {'title': 'Автономные агентные модели: новый шаг к самостоятельному ИИ', 'desc': 'Эта статья представляет новый подход к созданию агентных моделей искусственного интеллекта, называемых Large Agent Models (LAM). Авторы предлагают фреймворк AutoCoA, который позволяет модели самостоятельно генерировать цепочки действий без внешних подсказок. Фреймворк сочетает в себе методы обучения с учителем и обучения с подкреплением для оптимизации взаимодействия модели с окружающей средой. Результаты показывают, что модели, обученные с помощью AutoCoA, значительно превосходят традиционные подходы в задачах, требующих долгосрочных рассуждений и многошаговых действий.'}, 'en': {'title': 'Empowering Autonomous Reasoning with AutoCoA', 'desc': 'This paper introduces Large Agent Models (LAMs) that enhance the autonomy of reasoning models by allowing them to generate their own Chain-of-Action (CoA) without relying on external prompts. The AutoCoA framework integrates supervised fine-tuning (SFT) and reinforcement learning (RL) to enable models to effectively alternate between reasoning and taking actions in their environment. Key features of this framework include mechanisms for triggering actions at each step, optimizing CoA over entire trajectories, and utilizing an internal world model to minimize the costs associated with real-world interactions. Evaluations show that models trained with AutoCoA outperform traditional ReAct-based workflows, particularly in complex tasks requiring extended reasoning and multiple actions.'}, 'zh': {'title': '自主决策的智能代理模型', 'desc': '传统的智能工作流程依赖外部提示来管理与工具和环境的交互，这限制了推理模型的自主性。我们提出了大型代理模型（LAMs），使其能够内部生成行动链（CoA），从而自主决定何时以及如何使用外部工具。我们提出的AutoCoA框架结合了监督微调（SFT）和强化学习（RL），使模型能够在推理和行动之间无缝切换，同时有效管理与环境的交互。评估结果表明，经过AutoCoA训练的代理模型在开放领域问答任务中显著优于基于ReAct的工作流程，尤其是在需要长期推理和多步骤行动的任务中。'}}}, {'id': 'https://huggingface.co/papers/2503.04812', 'title': 'LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted\n  Contrastive Learning', 'url': 'https://huggingface.co/papers/2503.04812', 'abstract': "Universal multimodal embedding models play a critical role in tasks such as interleaved image-text retrieval, multimodal RAG, and multimodal clustering. However, our empirical results indicate that existing LMM-based embedding models trained with the standard InfoNCE loss exhibit a high degree of overlap in similarity distribution between positive and negative pairs, making it challenging to distinguish hard negative pairs effectively. To deal with this issue, we propose a simple yet effective framework that dynamically improves the embedding model's representation learning for negative pairs based on their discriminative difficulty. Within this framework, we train a series of models, named LLaVE, and evaluate them on the MMEB benchmark, which covers 4 meta-tasks and 36 datasets. Experimental results show that LLaVE establishes stronger baselines that achieve state-of-the-art (SOTA) performance while demonstrating strong scalability and efficiency. Specifically, LLaVE-2B surpasses the previous SOTA 7B models, while LLaVE-7B achieves a further performance improvement of 6.2 points. Although LLaVE is trained on image-text data, it can generalize to text-video retrieval tasks in a zero-shot manner and achieve strong performance, demonstrating its remarkable potential for transfer to other embedding tasks.", 'score': 10, 'issue_id': 2631, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'a1fec2227e343e88', 'authors': ['Zhibin Lan', 'Liqiang Niu', 'Fandong Meng', 'Jie Zhou', 'Jinsong Su'], 'affiliations': ['Pattern Recognition Center, WeChat AI, Tencent Inc, China', 'School of Informatics, Xiamen University, China', 'Shanghai Artificial Intelligence Laboratory, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04812.jpg', 'data': {'categories': ['#transfer_learning', '#benchmark', '#rag', '#multimodal', '#training'], 'emoji': '🔀', 'ru': {'title': 'Динамическое обучение для различения сложных негативных пар в мультимодальных эмбеддингах', 'desc': 'Статья представляет новый фреймворк для улучшения обучения мультимодальных эмбеддинг-моделей. Авторы обнаружили проблему перекрытия распределений сходства для позитивных и негативных пар в существующих моделях. Предложенный подход динамически улучшает представление негативных пар на основе их сложности различения. Разработанные модели LLaVE достигают лучших результатов на бенчмарке MMEB и демонстрируют хорошую масштабируемость.'}, 'en': {'title': 'Enhancing Multimodal Embeddings with LLaVE for Better Performance', 'desc': 'This paper discusses the development of a new framework called LLaVE for improving multimodal embedding models, which are used for tasks involving both images and text. The authors identify a problem with existing models that struggle to differentiate between similar positive and negative pairs due to overlapping similarity distributions. To address this, LLaVE dynamically enhances the learning of negative pairs based on their difficulty, leading to better representation learning. The results show that LLaVE outperforms previous state-of-the-art models and can also be applied effectively to other tasks like text-video retrieval.'}, 'zh': {'title': 'LLaVE：提升多模态嵌入的强大工具', 'desc': '本文提出了一种新的多模态嵌入模型LLaVE，旨在解决现有模型在处理正负样本时相似度分布重叠的问题。通过动态调整负样本的表示学习，LLaVE能够更有效地区分困难的负样本。实验结果表明，LLaVE在多个基准测试中表现出色，超越了之前的最先进模型，并在图像-文本数据上训练后，能够在零样本情况下推广到文本-视频检索任务。该模型展示了强大的可扩展性和效率，具有广泛的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.07334', 'title': 'Unleashing the Potential of Large Language Models for Text-to-Image\n  Generation through Autoregressive Representation Alignment', 'url': 'https://huggingface.co/papers/2503.07334', 'abstract': "We present Autoregressive Representation Alignment (ARRA), a new training framework that unlocks global-coherent text-to-image generation in autoregressive LLMs without architectural changes. Unlike prior work that requires complex architectural redesigns, ARRA aligns LLM hidden states with visual representations from external visual foundational models via a global visual alignment loss and a hybrid token, <HYBNEXT>. This token enforces dual constraints: local next-token prediction and global semantic distillation, enabling LLMs to implicitly learn spatial and contextual coherence while retaining their original autoregressive paradigm. Extensive experiments validate ARRA's plug-and-play versatility. When training from text-generation-only LLMs or random initialization, ARRA reduces FID by 25.5% (MIMIC-CXR), 8.8% (DeepEyeNet), and 7.5% (ImageNet) for advanced autoregressive LLMs like Chameleon and LlamaGen, all without framework modifications. For domain adaption, ARRA aligns general-purpose LLMs with specialized models (e.g., BioMedCLIP), achieving an 18.6% FID reduction over direct fine-tuning on medical imaging (MIMIC-CXR). By demonstrating that training objective redesign -- not just architectural innovation -- can resolve cross-modal global coherence challenges, ARRA offers a complementary paradigm for advancing autoregressive models. Code and models will be released to advance autoregressive image generation.", 'score': 9, 'issue_id': 2643, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': 'da94bc1af48685ac', 'authors': ['Xing Xie', 'Jiawei Liu', 'Ziyue Lin', 'Huijie Fan', 'Zhi Han', 'Yandong Tang', 'Liangqiong Qu'], 'affiliations': ['Shenyang Institute of Automation, Chinese Academy of Sciences', 'The University of Hong Kong', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.07334.jpg', 'data': {'categories': ['#training', '#optimization', '#alignment', '#open_source', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'ARRA: Глобальная согласованность в генерации изображений без изменения архитектуры', 'desc': 'Представлен новый метод обучения ARRA, позволяющий автореgressивным языковым моделям генерировать глобально согласованные изображения без изменения архитектуры. ARRA выравнивает скрытые состояния языковой модели с визуальными представлениями внешних моделей компьютерного зрения с помощью специальной функции потерь и гибридного токена. Этот подход позволяет языковым моделям неявно учиться пространственной и контекстной согласованности, сохраняя при этом исходную авторегрессивную парадигму. Эксперименты показывают, что ARRA значительно улучшает качество генерации изображений для различных задач и доменов.'}, 'en': {'title': 'Unlocking Coherent Text-to-Image Generation with ARRA', 'desc': "The paper introduces Autoregressive Representation Alignment (ARRA), a novel training framework that enhances text-to-image generation in autoregressive language models (LLMs) without changing their architecture. ARRA achieves this by aligning the hidden states of LLMs with visual representations from external models using a global visual alignment loss and a special hybrid token, <HYBNEXT>. This token imposes both local next-token prediction and global semantic distillation, allowing LLMs to learn spatial and contextual coherence effectively. The results show significant improvements in image generation quality, as evidenced by reduced Fréchet Inception Distance (FID) scores across various datasets, demonstrating ARRA's effectiveness and versatility in enhancing autoregressive models."}, 'zh': {'title': '自回归模型的新突破：全球一致性生成', 'desc': '本文提出了一种新的训练框架，称为自回归表示对齐（ARRA），旨在实现自回归大语言模型（LLM）中的全球一致性文本到图像生成，而无需改变模型架构。ARRA通过全局视觉对齐损失和混合标记<HYBNEXT>，将LLM的隐藏状态与外部视觉基础模型的视觉表示对齐。该标记施加了局部下一个标记预测和全局语义蒸馏的双重约束，使LLM能够在保持自回归范式的同时，隐式学习空间和上下文的一致性。实验结果验证了ARRA的灵活性，显示出在多个数据集上显著降低了FID值，证明了训练目标的重新设计可以解决跨模态全球一致性挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.07507', 'title': 'PE3R: Perception-Efficient 3D Reconstruction', 'url': 'https://huggingface.co/papers/2503.07507', 'abstract': 'Recent advancements in 2D-to-3D perception have significantly improved the understanding of 3D scenes from 2D images. However, existing methods face critical challenges, including limited generalization across scenes, suboptimal perception accuracy, and slow reconstruction speeds. To address these limitations, we propose Perception-Efficient 3D Reconstruction (PE3R), a novel framework designed to enhance both accuracy and efficiency. PE3R employs a feed-forward architecture to enable rapid 3D semantic field reconstruction. The framework demonstrates robust zero-shot generalization across diverse scenes and objects while significantly improving reconstruction speed. Extensive experiments on 2D-to-3D open-vocabulary segmentation and 3D reconstruction validate the effectiveness and versatility of PE3R. The framework achieves a minimum 9-fold speedup in 3D semantic field reconstruction, along with substantial gains in perception accuracy and reconstruction precision, setting new benchmarks in the field. The code is publicly available at: https://github.com/hujiecpp/PE3R.', 'score': 8, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '6a53d341839fc41f', 'authors': ['Jie Hu', 'Shizun Wang', 'Xinchao Wang'], 'affiliations': ['xML Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.07507.jpg', 'data': {'categories': ['#3d', '#architecture', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'Быстрая и точная 3D-реконструкция из 2D-изображений', 'desc': 'PE3R - это новая система для улучшения 3D-реконструкции из 2D-изображений. Она использует прямую архитектуру для быстрого восстановления 3D семантических полей. PE3R демонстрирует надежную обобщающую способность на разных сценах и объектах без дополнительного обучения. Эксперименты показывают значительное ускорение реконструкции и повышение точности восприятия по сравнению с существующими методами.'}, 'en': {'title': 'Revolutionizing 3D Scene Understanding with PE3R', 'desc': 'This paper introduces Perception-Efficient 3D Reconstruction (PE3R), a new framework that enhances the process of understanding 3D scenes from 2D images. PE3R addresses key challenges in existing methods, such as limited generalization and slow reconstruction speeds, by utilizing a feed-forward architecture. The framework achieves impressive zero-shot generalization across various scenes and objects, while also significantly speeding up the reconstruction process. Experimental results show that PE3R offers at least a 9-fold increase in reconstruction speed and improved accuracy, establishing new benchmarks in 2D-to-3D perception.'}, 'zh': {'title': '感知高效3D重建，速度与精度的双重提升', 'desc': '最近在2D到3D感知方面的进展显著提高了从2D图像理解3D场景的能力。然而，现有方法面临着场景泛化能力有限、感知精度不佳和重建速度慢等关键挑战。为了解决这些问题，我们提出了感知高效3D重建（PE3R）框架，旨在提高准确性和效率。PE3R采用前馈架构，能够快速重建3D语义场，并在多样场景和物体上展示出强大的零样本泛化能力，同时显著提高重建速度。'}}}, {'id': 'https://huggingface.co/papers/2503.07459', 'title': 'MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for\n  Complex Medical Reasoning', 'url': 'https://huggingface.co/papers/2503.07459', 'abstract': 'Large Language Models (LLMs) have shown impressive performance on existing medical question-answering benchmarks. This high performance makes it increasingly difficult to meaningfully evaluate and differentiate advanced methods. We present <PRE_TAG>MedAgentsBench</POST_TAG>, a benchmark that focuses on challenging medical questions requiring multi-step clinical reasoning, diagnosis formulation, and treatment planning-scenarios where current models still struggle despite their strong performance on standard tests. Drawing from seven established medical datasets, our benchmark addresses three key limitations in existing evaluations: (1) the prevalence of straightforward questions where even base models achieve high performance, (2) inconsistent sampling and evaluation protocols across studies, and (3) lack of systematic analysis of the interplay between performance, cost, and inference time. Through experiments with various base models and reasoning methods, we demonstrate that the latest thinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in complex medical reasoning tasks. Additionally, advanced search-based agent methods offer promising performance-to-cost ratios compared to traditional approaches. Our analysis reveals substantial performance gaps between model families on complex questions and identifies optimal model selections for different computational constraints. Our benchmark and evaluation framework are publicly available at https://github.com/gersteinlab/medagents-benchmark.', 'score': 8, 'issue_id': 2634, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '4d9aba5593231609', 'authors': ['Xiangru Tang', 'Daniel Shao', 'Jiwoong Sohn', 'Jiapeng Chen', 'Jiayi Zhang', 'Jinyu Xiang', 'Fang Wu', 'Yilun Zhao', 'Chenglin Wu', 'Wenqi Shi', 'Arman Cohan', 'Mark Gerstein'], 'affiliations': ['Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07459.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#survey', '#healthcare'], 'emoji': '🩺', 'ru': {'title': 'Новый бенчмарк для оценки языковых моделей в сложных медицинских задачах', 'desc': 'Статья представляет новый бенчмарк <PRE_TAG>MedAgentsBench</POST_TAG> для оценки языковых моделей в области медицины. Бенчмарк фокусируется на сложных медицинских вопросах, требующих многоступенчатых клинических рассуждений. Исследование выявило, что новейшие модели, такие как DeepSeek R1 и OpenAI o3, показывают исключительную производительность в сложных медицинских задачах. Анализ также показал, что методы на основе поиска предлагают многообещающее соотношение производительности и стоимости по сравнению с традиционными подходами.'}, 'en': {'title': 'MedAgentsBench: Elevating Medical Question-Answering Evaluation', 'desc': 'This paper introduces MedAgentsBench, a new benchmark designed to evaluate Large Language Models (LLMs) on complex medical questions that require multi-step reasoning. It addresses limitations in current evaluations, such as the prevalence of simple questions and inconsistent testing protocols. The authors conduct experiments with various models, revealing that advanced models like DeepSeek R1 and OpenAI o3 perform well on challenging tasks, while search-based agents show better performance-to-cost ratios. The study highlights significant performance gaps among different model families and provides insights for selecting models based on computational resources.'}, 'zh': {'title': '医学问答的新基准：挑战复杂推理', 'desc': '本文介绍了一个新的医学问答基准测试——MedAgentsBench，旨在评估大型语言模型在复杂医学问题上的表现。该基准测试专注于需要多步骤临床推理、诊断制定和治疗计划的问题，这些问题是当前模型仍然面临挑战的领域。通过对七个已建立的医学数据集进行分析，本文解决了现有评估中的三个关键限制，包括简单问题的普遍性和评估协议的不一致性。实验结果表明，最新的思维模型在复杂医学推理任务中表现出色，并且基于搜索的代理方法在性能与成本比方面具有良好的前景。'}}}, {'id': 'https://huggingface.co/papers/2503.07197', 'title': 'Effective and Efficient Masked Image Generation Models', 'url': 'https://huggingface.co/papers/2503.07197', 'abstract': "Although masked image generation models and masked diffusion models are designed with different motivations and objectives, we observe that they can be unified within a single framework. Building upon this insight, we carefully explore the design space of training and sampling, identifying key factors that contribute to both performance and efficiency. Based on the improvements observed during this exploration, we develop our model, referred to as eMIGM. Empirically, eMIGM demonstrates strong performance on ImageNet generation, as measured by Fr\\'echet Inception Distance (FID). In particular, on ImageNet 256x256, with similar number of function evaluations (NFEs) and model parameters, eMIGM outperforms the seminal VAR. Moreover, as NFE and model parameters increase, eMIGM achieves performance comparable to the state-of-the-art continuous diffusion models while requiring less than 40% of the NFE. Additionally, on ImageNet 512x512, with only about 60% of the NFE, eMIGM outperforms the state-of-the-art continuous diffusion models.", 'score': 8, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '4740cc0178bbf099', 'authors': ['Zebin You', 'Jingyang Ou', 'Xiaolu Zhang', 'Jun Hu', 'Jun Zhou', 'Chongxuan Li'], 'affiliations': ['Ant Group', 'Beijing Key Laboratory of Big Data Management and Analysis Method', 'Gaoling School of AI, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.07197.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#training', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Объединение маскированных моделей для эффективной генерации изображений', 'desc': 'Исследователи объединили модели генерации маскированных изображений и маскированные диффузионные модели в единую структуру. Они изучили пространство проектирования обучения и сэмплирования, выявив ключевые факторы, влияющие на производительность и эффективность. На основе этого анализа была разработана модель eMIGM, которая показала высокую эффективность при генерации изображений ImageNet. eMIGM превзошла существующие модели по метрике FID при меньшем количестве вычислений и параметров.'}, 'en': {'title': 'Unifying Masked Models for Efficient Image Generation', 'desc': "This paper presents a unified framework for masked image generation models and masked diffusion models, highlighting their similarities despite different goals. The authors explore various design choices in training and sampling, which significantly impact the model's performance and efficiency. They introduce a new model called eMIGM, which shows superior results on ImageNet generation tasks, particularly in terms of Fréchet Inception Distance (FID). Notably, eMIGM achieves competitive performance with fewer function evaluations compared to existing state-of-the-art models, demonstrating its efficiency and effectiveness in image generation."}, 'zh': {'title': '统一掩蔽模型，提升图像生成性能', 'desc': '本文探讨了掩蔽图像生成模型和掩蔽扩散模型的统一框架。我们分析了训练和采样的设计空间，识别出影响性能和效率的关键因素。基于这些改进，我们开发了名为eMIGM的模型。实验结果表明，eMIGM在ImageNet生成任务中表现优异，尤其在较低的函数评估次数下超越了现有的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2503.05856', 'title': 'This Is Your Doge, If It Please You: Exploring Deception and Robustness\n  in Mixture of LLMs', 'url': 'https://huggingface.co/papers/2503.05856', 'abstract': "Mixture of <PRE_TAG>large language model (LLMs) Agents (MoA)</POST_TAG> architectures achieve state-of-the-art performance on prominent benchmarks like AlpacaEval 2.0 by leveraging the collaboration of multiple LLMs at inference time. Despite these successes, an evaluation of the safety and reliability of MoA is missing. We present the first comprehensive study of MoA's robustness against deceptive LLM agents that deliberately provide misleading responses. We examine factors like the propagation of deceptive information, model size, and information availability, and uncover critical vulnerabilities. On AlpacaEval 2.0, the popular LLaMA 3.1-70B model achieves a length-controlled Win Rate (LC WR) of 49.2% when coupled with 3-layer MoA (6 LLM agents). However, we demonstrate that introducing only a single carefully-instructed deceptive agent into the MoA can reduce performance to 37.9%, effectively nullifying all MoA gains. On QuALITY, a multiple-choice comprehension task, the impact is also severe, with accuracy plummeting by a staggering 48.5%. Inspired in part by the historical Doge of Venice voting process, designed to minimize influence and deception, we propose a range of unsupervised defense mechanisms that recover most of the lost performance.", 'score': 7, 'issue_id': 2639, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': 'a7ca72375d4cd423', 'authors': ['Lorenz Wolf', 'Sangwoong Yoon', 'Ilija Bogunovic'], 'affiliations': ['University College London Center for Artificial Intelligence, London, UK'], 'pdf_title_img': 'assets/pdf/title_img/2503.05856.jpg', 'data': {'categories': ['#inference', '#security', '#benchmark', '#agents', '#hallucinations'], 'emoji': '🛡️', 'ru': {'title': 'Защита коллаборативных языковых моделей от обмана', 'desc': 'Исследование посвящено архитектуре Mixture of large language model Agents (MoA), которая достигает высоких результатов на бенчмарках, объединяя несколько языковых моделей. Авторы изучают уязвимости MoA к обманным агентам, намеренно предоставляющим ложные ответы. Эксперименты показывают, что даже один обманный агент может значительно снизить производительность MoA на задачах вроде AlpacaEval 2.0 и QuALITY. Предлагаются механизмы защиты, вдохновленные историческим процессом голосования Дожа Венеции, для восстановления утраченной производительности.'}, 'en': {'title': 'Strengthening MoA: Defending Against Deceptive Agents', 'desc': 'This paper investigates the robustness of Mixture of Large Language Model Agents (MoA) architectures, which have shown impressive results in various benchmarks. The authors highlight a significant gap in understanding how these models handle deceptive agents that can provide misleading information. Their experiments reveal that even a single deceptive agent can drastically reduce the performance of MoA systems, indicating vulnerabilities in their design. To address these issues, the paper proposes unsupervised defense mechanisms inspired by historical voting processes to enhance the reliability of MoA against such threats.'}, 'zh': {'title': '提升大型语言模型的安全性与可靠性', 'desc': '这篇论文研究了混合大型语言模型（LLMs）代理架构的安全性和可靠性。尽管这些架构在多个基准测试中表现出色，但对其抵御误导性信息的能力缺乏评估。研究发现，单个误导性代理的引入会显著降低模型的性能，甚至抵消所有的优势。为此，论文提出了一系列无监督的防御机制，以恢复大部分丢失的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.06520', 'title': 'Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive\n  Reinforcement', 'url': 'https://huggingface.co/papers/2503.06520', 'abstract': "Traditional methods for reasoning segmentation rely on supervised fine-tuning with categorical labels and simple descriptions, limiting its out-of-domain generalization and lacking explicit reasoning processes. To address these limitations, we propose Seg-Zero, a novel framework that demonstrates remarkable generalizability and derives explicit chain-of-thought reasoning through cognitive reinforcement. Seg-Zero introduces a decoupled architecture consisting of a reasoning model and a segmentation model. The reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precious pixel-level masks. We design a sophisticated reward mechanism that integrates both format and accuracy rewards to effectively guide optimization directions. Trained exclusively via reinforcement learning with GRPO and without explicit reasoning data, Seg-Zero achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. Experiments show that Seg-Zero-7B achieves a zero-shot performance of 57.5 on the ReasonSeg benchmark, surpassing the prior LISA-7B by 18\\%. This significant improvement highlights Seg-Zero's ability to generalize across domains while presenting an explicit reasoning process. Code is available at https://github.com/dvlab-research/Seg-Zero.", 'score': 6, 'issue_id': 2630, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': 'b21eb23a448d282e', 'authors': ['Yuqi Liu', 'Bohao Peng', 'Zhisheng Zhong', 'Zihao Yue', 'Fanbin Lu', 'Bei Yu', 'Jiaya Jia'], 'affiliations': ['CUHK', 'HKUST', 'RUC'], 'pdf_title_img': 'assets/pdf/title_img/2503.06520.jpg', 'data': {'categories': ['#rl', '#benchmark', '#architecture', '#reasoning', '#rlhf', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Сегментация с рассуждением: ИИ учится объяснять свои решения', 'desc': 'Seg-Zero - это новая модель для сегментации изображений, использующая когнитивное подкрепление для улучшения обобщающей способности и явного рассуждения. Архитектура состоит из модели рассуждений, генерирующей цепочки рассуждений и позиционные подсказки, и модели сегментации, создающей маски на уровне пикселей. Обучение происходит с помощью обучения с подкреплением и специального механизма вознаграждений. Seg-Zero-7B превосходит предыдущие модели на 18% в задаче zero-shot сегментации на бенчмарке ReasonSeg.'}, 'en': {'title': 'Seg-Zero: Revolutionizing Reasoning Segmentation with Zero-Shot Generalization', 'desc': 'The paper introduces Seg-Zero, a new framework for reasoning segmentation that overcomes the limitations of traditional supervised methods. It features a decoupled architecture with a reasoning model that interprets user intentions and generates reasoning chains, which are then used by a segmentation model to create detailed pixel-level masks. The framework employs a unique reward mechanism that balances format and accuracy to optimize performance through reinforcement learning. Seg-Zero demonstrates impressive zero-shot generalization capabilities, achieving a significant performance boost on the ReasonSeg benchmark compared to previous models.'}, 'zh': {'title': 'Seg-Zero：突破性推理与分割的结合', 'desc': '传统的分割推理方法依赖于带有类别标签的监督微调，这限制了其在不同领域的泛化能力，并缺乏明确的推理过程。为了解决这些问题，我们提出了Seg-Zero，这是一种新颖的框架，展示了显著的泛化能力，并通过认知强化推导出明确的推理链。Seg-Zero引入了一个解耦架构，包括推理模型和分割模型，推理模型解释用户意图，生成明确的推理链，并产生位置提示，随后由分割模型生成精确的像素级掩码。通过强化学习训练，Seg-Zero在没有明确推理数据的情况下，实现了强大的零样本泛化能力，并展现出突出的测试时推理能力。'}}}, {'id': 'https://huggingface.co/papers/2503.06121', 'title': 'BlackGoose Rimer: Harnessing RWKV-7 as a Simple yet Superior Replacement\n  for Transformers in Large-Scale Time Series Modeling', 'url': 'https://huggingface.co/papers/2503.06121', 'abstract': "Time series models face significant challenges in scaling to handle large and complex datasets, akin to the scaling achieved by large language models (LLMs). The unique characteristics of time series data and the computational demands of model scaling necessitate innovative approaches. While researchers have explored various architectures such as Transformers, LSTMs, and GRUs to address these challenges, we propose a novel solution using RWKV-7, which incorporates meta-learning into its state update mechanism. By integrating RWKV-7's time mix and channel mix components into the transformer-based time series model Timer, we achieve a substantial performance improvement of approximately 1.13 to 43.3x and a 4.5x reduction in training time with 1/23 parameters, all while utilizing fewer parameters. Our code and model weights are publicly available for further research and development at https://github.com/Alic-Li/BlackGoose_Rimer.", 'score': 5, 'issue_id': 2631, 'pub_date': '2025-03-08', 'pub_date_card': {'ru': '8 марта', 'en': 'March 8', 'zh': '3月8日'}, 'hash': '3f03abe6317e5bba', 'authors': ['Li weile', 'Liu Xiao'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.06121.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training', '#open_source'], 'emoji': '⏳', 'ru': {'title': 'Революция в масштабировании моделей временных рядов с RWKV-7', 'desc': 'Статья представляет новый подход к масштабированию моделей временных рядов с использованием архитектуры RWKV-7. Авторы интегрируют компоненты RWKV-7 в трансформер-модель Timer для обработки временных рядов. Результаты показывают значительное улучшение производительности и сокращение времени обучения при меньшем количестве параметров. Предложенный метод решает проблемы масштабирования моделей временных рядов, аналогичные тем, с которыми сталкиваются большие языковые модели.'}, 'en': {'title': 'Revolutionizing Time Series with RWKV-7: Efficiency Meets Performance', 'desc': "This paper addresses the challenges of scaling time series models to manage large and complex datasets, similar to the advancements seen in large language models. It introduces RWKV-7, a novel approach that integrates meta-learning into the state update mechanism of time series models. By combining RWKV-7's time mix and channel mix components with the Timer model, the authors report significant performance enhancements and reduced training times. The proposed method achieves impressive results with fewer parameters, making it a promising solution for time series analysis."}, 'zh': {'title': '创新时间序列模型，提升性能与效率', 'desc': '时间序列模型在处理大型复杂数据集时面临显著挑战，类似于大型语言模型的扩展能力。时间序列数据的独特特性和模型扩展的计算需求需要创新的方法。我们提出了一种新颖的解决方案，使用RWKV-7将元学习融入状态更新机制。通过将RWKV-7的时间混合和通道混合组件整合到基于变换器的时间序列模型Timer中，我们实现了约1.13到43.3倍的性能提升，并将训练时间减少了4.5倍，同时参数数量仅为原来的1/23。'}}}, {'id': 'https://huggingface.co/papers/2503.07274', 'title': 'Efficient Distillation of Classifier-Free Guidance using Adapters', 'url': 'https://huggingface.co/papers/2503.07274', 'abstract': 'While classifier-free guidance (CFG) is essential for conditional diffusion models, it doubles the number of neural function evaluations (NFEs) per inference step. To mitigate this inefficiency, we introduce adapter guidance distillation (AGD), a novel approach that simulates CFG in a single forward pass. AGD leverages lightweight adapters to approximate CFG, effectively doubling the sampling speed while maintaining or even improving sample quality. Unlike prior guidance distillation methods that tune the entire model, AGD keeps the base model frozen and only trains minimal additional parameters (sim2%) to significantly reduce the resource requirement of the distillation phase. Additionally, this approach preserves the original model weights and enables the adapters to be seamlessly combined with other checkpoints derived from the same base model. We also address a key mismatch between training and inference in existing guidance distillation methods by training on CFG-guided trajectories instead of standard diffusion trajectories. Through extensive experiments, we show that AGD achieves comparable or superior FID to CFG across multiple architectures with only half the NFEs. Notably, our method enables the distillation of large models (sim2.6B parameters) on a single consumer GPU with 24 GB of VRAM, making it more accessible than previous approaches that require multiple high-end GPUs. We will publicly release the implementation of our method.', 'score': 4, 'issue_id': 2639, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '84e5b0fd1b6f7f9f', 'authors': ['Cristian Perez Jensen', 'Seyedmorteza Sadat'], 'affiliations': ['ETH Zürich'], 'pdf_title_img': 'assets/pdf/title_img/2503.07274.jpg', 'data': {'categories': ['#inference', '#optimization', '#diffusion', '#training', '#open_source', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'Ускорение диффузионных моделей с сохранением качества', 'desc': "Статья представляет новый метод под названием 'адаптерная дистилляция управления' (AGD) для условных диффузионных моделей. AGD симулирует управление без классификатора (CFG) за один проход сети, удваивая скорость семплирования без потери качества. Метод использует легковесные адаптеры, обучая всего 2% параметров и сохраняя базовую модель неизменной. AGD решает проблему несоответствия между обучением и выводом, тренируясь на траекториях с CFG."}, 'en': {'title': 'Speeding Up Diffusion Models with Adapter Guidance Distillation', 'desc': 'This paper presents adapter guidance distillation (AGD), a new method that enhances the efficiency of conditional diffusion models by simulating classifier-free guidance (CFG) in a single forward pass. AGD uses lightweight adapters to approximate CFG, which allows for faster sampling without sacrificing sample quality. The approach keeps the base model unchanged and only trains a small number of additional parameters, significantly reducing resource requirements. Experimental results demonstrate that AGD achieves similar or better performance compared to CFG while halving the number of neural function evaluations needed, making it feasible to distill large models on consumer-grade hardware.'}, 'zh': {'title': '适配器引导蒸馏：提升扩散模型效率的创新方法', 'desc': '本文提出了一种新的方法，称为适配器引导蒸馏（AGD），旨在提高条件扩散模型的效率。AGD通过轻量级适配器在单次前向传播中模拟无分类器引导（CFG），从而实现了采样速度的加倍，同时保持或改善样本质量。与以往的引导蒸馏方法不同，AGD只训练少量额外参数，而保持基础模型不变，显著降低了资源需求。通过大量实验，我们证明AGD在多个架构上实现了与CFG相当或更优的FID，同时只需一半的神经函数评估（NFE）。'}}}, {'id': 'https://huggingface.co/papers/2503.07603', 'title': 'Should VLMs be Pre-trained with Image Data?', 'url': 'https://huggingface.co/papers/2503.07603', 'abstract': 'Pre-trained LLMs that are further trained with image data perform well on vision-language tasks. While adding images during a second training phase effectively unlocks this capability, it is unclear how much of a gain or loss this two-step pipeline gives over VLMs which integrate images earlier into the training process. To investigate this, we train models spanning various datasets, scales, image-text ratios, and amount of pre-training done before introducing vision tokens. We then fine-tune these models and evaluate their downstream performance on a suite of vision-language and text-only tasks. We find that pre-training with a mixture of image and text data allows models to perform better on vision-language tasks while maintaining strong performance on text-only evaluations. On an average of 6 diverse tasks, we find that for a 1B model, introducing visual tokens 80% of the way through pre-training results in a 2% average improvement over introducing visual tokens to a fully pre-trained model.', 'score': 3, 'issue_id': 2633, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '16f17eb6db67a418', 'authors': ['Sedrick Keh', 'Jean Mercat', 'Samir Yitzhak Gadre', 'Kushal Arora', 'Igor Vasiljevic', 'Benjamin Burchfiel', 'Shuran Song', 'Russ Tedrake', 'Thomas Kollar', 'Ludwig Schmidt', 'Achal Dave'], 'affiliations': ['Columbia University', 'MIT', 'Stanford', 'Toyota Research Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.07603.jpg', 'data': {'categories': ['#multimodal', '#training', '#benchmark', '#transfer_learning', '#optimization'], 'emoji': '🔬', 'ru': {'title': 'Оптимальное время для интеграции визуальных данных в языковые модели', 'desc': 'Исследование посвящено сравнению эффективности двухэтапного обучения языковых моделей (сначала на текстах, затем на изображениях) с моделями, изначально обучаемыми на смешанных данных. Авторы провели эксперименты с различными наборами данных, масштабами моделей и соотношениями изображений и текста. Результаты показывают, что обучение на смешанных данных позволяет моделям лучше справляться с задачами, связанными с обработкой изображений и текста, сохраняя при этом высокую производительность в текстовых задачах. Для модели размером 1 миллиард параметров введение визуальных токенов на 80% этапе предобучения дает в среднем 2% улучшение по сравнению с добавлением изображений к полностью предобученной модели.'}, 'en': {'title': 'Unlocking Vision-Language Synergy: Timing Matters!', 'desc': 'This paper explores the effectiveness of training large language models (LLMs) with image data in a two-step process compared to integrating images earlier in the training. The authors conduct experiments with various datasets and training configurations to assess the impact of when visual tokens are introduced. Their findings indicate that pre-training with both image and text data enhances performance on vision-language tasks while still performing well on text-only tasks. Specifically, they observe a 2% improvement in performance when visual tokens are added later in the pre-training phase for a 1B model.'}, 'zh': {'title': '图像与文本混合预训练提升视觉语言任务表现', 'desc': '这篇论文探讨了预训练的大型语言模型（LLM）在加入图像数据后在视觉语言任务中的表现。研究发现，在第二阶段训练中加入图像数据可以有效提升模型的能力，但不清楚这种两步训练流程与早期整合图像的视觉语言模型（VLM）相比，究竟是增益还是损失。通过训练不同数据集、规模和图像文本比例的模型，研究者发现混合图像和文本数据的预训练可以提高视觉语言任务的表现，同时在仅文本的评估中也保持良好表现。结果显示，对于一个10亿参数的模型，在预训练的80%时引入视觉标记，平均提升了2%的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.07265', 'title': 'WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image\n  Generation', 'url': 'https://huggingface.co/papers/2503.07265', 'abstract': 'Text-to-Image (T2I) models are capable of generating high-quality artistic creations and visual content. However, existing research and evaluation standards predominantly focus on image realism and shallow text-image alignment, lacking a comprehensive assessment of complex semantic understanding and world knowledge integration in text to image generation. To address this challenge, we propose WISE, the first benchmark specifically designed for World Knowledge-Informed Semantic Evaluation. WISE moves beyond simple word-pixel mapping by challenging models with 1000 meticulously crafted prompts across 25 sub-domains in cultural common sense, spatio-temporal reasoning, and natural science. To overcome the limitations of traditional CLIP metric, we introduce WiScore, a novel quantitative metric for assessing knowledge-image alignment. Through comprehensive testing of 20 models (10 dedicated T2I models and 10 unified multimodal models) using 1,000 structured prompts spanning 25 subdomains, our findings reveal significant limitations in their ability to effectively integrate and apply world knowledge during image generation, highlighting critical pathways for enhancing knowledge incorporation and application in next-generation T2I models. Code and data are available at https://github.com/PKU-YuanGroup/WISE.', 'score': 3, 'issue_id': 2640, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '6174088b65d232ba', 'authors': ['Yuwei Niu', 'Munan Ning', 'Mengren Zheng', 'Bin Lin', 'Peng Jin', 'Jiaqi Liao', 'Kunpeng Ning', 'Bin Zhu', 'Li Yuan'], 'affiliations': ['Chongqing University', 'Peking University', 'PengCheng Laboratory', 'Rabbitpre AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.07265.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#multimodal', '#interpretability', '#cv'], 'emoji': '🧠', 'ru': {'title': 'WISE: новый рубеж в оценке семантического понимания моделей text-to-image', 'desc': 'Исследователи предложили новый бенчмарк WISE для оценки интеграции мировых знаний в модели генерации изображений по тексту. WISE включает 1000 тщательно разработанных промптов в 25 поддоменах, охватывающих культурный здравый смысл, пространственно-временное мышление и естественные науки. Авторы также представили новую метрику WiScore для количественной оценки соответствия знаний и изображений. Тестирование 20 моделей выявило значительные ограничения в их способности эффективно интегрировать и применять мировые знания при генерации изображений.'}, 'en': {'title': 'Enhancing T2I Models with World Knowledge Evaluation', 'desc': 'This paper introduces WISE, a new benchmark for evaluating Text-to-Image (T2I) models that focuses on their ability to integrate world knowledge and complex semantic understanding. Unlike previous assessments that primarily measure image realism and basic text-image alignment, WISE challenges models with 1,000 detailed prompts across various domains, including cultural common sense and natural science. The authors also present WiScore, a new metric designed to quantitatively assess how well models align knowledge with generated images. Testing reveals that many current T2I models struggle to effectively incorporate world knowledge, indicating areas for improvement in future model development.'}, 'zh': {'title': '提升文本到图像生成的知识整合能力', 'desc': '本文提出了WISE，这是第一个专门为世界知识驱动的语义评估设计的基准。现有的文本到图像生成模型主要关注图像的真实感和简单的文本-图像对齐，缺乏对复杂语义理解和世界知识整合的全面评估。WISE通过1000个精心设计的提示，涵盖文化常识、时空推理和自然科学等25个子领域，挑战模型的能力。我们还引入了WiScore这一新颖的定量指标，以评估知识与图像的对齐程度，测试结果显示现有模型在有效整合和应用世界知识方面存在显著局限。'}}}, {'id': 'https://huggingface.co/papers/2503.06273', 'title': 'Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by\n  Learning Language-Agnostic Speech Representations', 'url': 'https://huggingface.co/papers/2503.06273', 'abstract': 'We explore a novel zero-shot Audio-Visual Speech Recognition (AVSR) framework, dubbed Zero-AVSR, which enables speech recognition in target languages without requiring any audio-visual speech data in those languages. Specifically, we introduce the Audio-Visual Speech Romanizer (AV-Romanizer), which learns language-agnostic speech representations by predicting Roman text. Then, by leveraging the strong multilingual modeling capabilities of Large Language Models (LLMs), we propose converting the predicted Roman text into language-specific graphemes, forming the proposed Cascaded <PRE_TAG>Zero-AVSR</POST_TAG>. Taking it a step further, we explore a unified Zero-AVSR approach by directly integrating the audio-visual speech representations encoded by the AV-Romanizer into the LLM. This is achieved through finetuning the adapter and the LLM using our proposed multi-task learning scheme. To capture the wide spectrum of phonetic and linguistic diversity, we also introduce a Multilingual Audio-Visual Romanized Corpus (MARC) consisting of 2,916 hours of audio-visual speech data across 82 languages, along with transcriptions in both language-specific graphemes and Roman text. Extensive analysis and experiments confirm that the proposed Zero-AVSR framework has the potential to expand language support beyond the languages seen during the training of the AV-Romanizer.', 'score': 3, 'issue_id': 2640, 'pub_date': '2025-03-08', 'pub_date_card': {'ru': '8 марта', 'en': 'March 8', 'zh': '3月8日'}, 'hash': '66b650ba2f5b0404', 'authors': ['Jeong Hun Yeo', 'Minsu Kim', 'Chae Won Kim', 'Stavros Petridis', 'Yong Man Ro'], 'affiliations': ['Imperial College London', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.06273.jpg', 'data': {'categories': ['#low_resource', '#audio', '#dataset', '#multilingual', '#machine_translation'], 'emoji': '🗣️', 'ru': {'title': 'Универсальное распознавание речи без языковых барьеров', 'desc': 'Статья представляет новую систему Zero-AVSR для аудиовизуального распознавания речи без использования речевых данных целевого языка. Авторы предлагают Audio-Visual Speech Romanizer для создания языконезависимых представлений речи и используют возможности больших языковых моделей для преобразования романизированного текста в графемы конкретного языка. Исследователи также разрабатывают унифицированный подход Zero-AVSR, интегрируя аудиовизуальные представления речи непосредственно в большую языковую модель. Для обучения и оценки создан многоязычный аудиовизуальный корпус MARC, содержащий 2916 часов речевых данных на 82 языках.'}, 'en': {'title': 'Zero-AVSR: Speech Recognition Without Language-Specific Data', 'desc': 'The paper presents a new framework called Zero-AVSR for Audio-Visual Speech Recognition that can recognize speech in languages without needing any specific audio-visual data for those languages. It introduces the Audio-Visual Speech Romanizer (AV-Romanizer), which creates language-independent speech representations by predicting Roman text. By utilizing Large Language Models (LLMs), the framework converts these predictions into specific language graphemes, enhancing multilingual capabilities. Additionally, a new dataset, the Multilingual Audio-Visual Romanized Corpus (MARC), is introduced to support the training and evaluation of this framework across 82 languages.'}, 'zh': {'title': '零样本音视频语音识别的创新探索', 'desc': '我们提出了一种新颖的零样本音视频语音识别框架，称为Zero-AVSR，能够在没有目标语言音视频语音数据的情况下进行语音识别。该框架引入了音视频语音罗马化器（AV-Romanizer），通过预测罗马文本来学习与语言无关的语音表示。我们利用大型语言模型（LLMs）的强大多语言建模能力，将预测的罗马文本转换为特定语言的字形，形成级联的Zero-AVSR。通过多任务学习方案微调适配器和LLM，我们进一步探索了统一的Zero-AVSR方法，直接将AV-Romanizer编码的音视频语音表示集成到LLM中。'}}}, {'id': 'https://huggingface.co/papers/2503.03499', 'title': 'State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for\n  State Space Models', 'url': 'https://huggingface.co/papers/2503.03499', 'abstract': 'State Space Models (SSMs) have emerged as efficient alternatives to Transformers, mitigating their quadratic computational cost. However, the application of Parameter-Efficient Fine-Tuning (PEFT) methods to SSMs remains largely unexplored. In particular, prompt-based methods like Prompt Tuning and Prefix-Tuning, which are widely used in Transformers, do not perform well on SSMs. To address this, we propose state-based methods as a superior alternative to prompt-based methods. This new family of methods naturally stems from the architectural characteristics of SSMs. State-based methods adjust state-related features directly instead of depending on external prompts. Furthermore, we introduce a novel state-based PEFT method: State-offset Tuning. At every timestep, our method directly affects the state at the current step, leading to more effective adaptation. Through extensive experiments across diverse datasets, we demonstrate the effectiveness of our method. Code is available at https://github.com/furiosa-ai/ssm-state-tuning.', 'score': 3, 'issue_id': 2630, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '80ab6abd822f2976', 'authors': ['Wonjun Kang', 'Kevin Galim', 'Yuchen Zeng', 'Minjae Lee', 'Hyung Il Koo', 'Nam Ik Cho'], 'affiliations': ['UW-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2503.03499.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективная тонкая настройка моделей пространства состояний без промптов', 'desc': 'Эта статья представляет новый подход к тонкой настройке моделей пространства состояний (SSM) в машинном обучении. Авторы предлагают методы, основанные на состояниях, как альтернативу методам, основанным на промптах, которые хорошо работают для трансформеров, но неэффективны для SSM. Они вводят новый метод под названием State-offset Tuning, который напрямую влияет на состояние модели на каждом временном шаге. Эксперименты на различных наборах данных демонстрируют эффективность предложенного подхода.'}, 'en': {'title': 'Revolutionizing Fine-Tuning with State-Based Methods for SSMs', 'desc': 'This paper discusses the limitations of using traditional prompt-based fine-tuning methods on State Space Models (SSMs), which are more efficient than Transformers. The authors propose a new approach called state-based methods that leverage the unique structure of SSMs to improve performance. Specifically, they introduce State-offset Tuning, a novel parameter-efficient fine-tuning technique that modifies the state at each timestep directly. Experimental results show that this method outperforms existing prompt-based techniques, highlighting its effectiveness in adapting SSMs for various tasks.'}, 'zh': {'title': '基于状态的微调：超越提示的方法', 'desc': '状态空间模型（SSMs）作为变换器的高效替代方案，能够减轻其二次计算成本。然而，参数高效微调（PEFT）方法在SSMs上的应用仍然未被充分探索。我们提出基于状态的方法，作为优于基于提示的方法的新选择，这些方法直接调整与状态相关的特征，而不是依赖外部提示。我们还引入了一种新颖的基于状态的PEFT方法：状态偏移微调，能够在每个时间步直接影响当前状态，从而实现更有效的适应。'}}}, {'id': 'https://huggingface.co/papers/2503.02199', 'title': 'Words or Vision: Do Vision-Language Models Have Blind Faith in Text?', 'url': 'https://huggingface.co/papers/2503.02199', 'abstract': "Vision-Language Models (VLMs) excel in integrating visual and textual information for vision-centric tasks, but their handling of inconsistencies between modalities is underexplored. We investigate VLMs' modality preferences when faced with visual data and varied textual inputs in vision-centered settings. By introducing textual variations to four vision-centric tasks and evaluating ten Vision-Language Models (VLMs), we discover a ``blind faith in text'' phenomenon: VLMs disproportionately trust textual data over visual data when inconsistencies arise, leading to significant performance drops under corrupted text and raising safety concerns. We analyze factors influencing this text bias, including instruction prompts, language model size, text relevance, token order, and the interplay between visual and textual certainty. While certain factors, such as scaling up the language model size, slightly mitigate text bias, others like token order can exacerbate it due to positional biases inherited from language models. To address this issue, we explore supervised fine-tuning with text augmentation and demonstrate its effectiveness in reducing text bias. Additionally, we provide a theoretical analysis suggesting that the blind faith in text phenomenon may stem from an imbalance of pure text and multi-modal data during training. Our findings highlight the need for balanced training and careful consideration of modality interactions in VLMs to enhance their robustness and reliability in handling multi-modal data inconsistencies.", 'score': 3, 'issue_id': 2630, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'a354f8de058f0f84', 'authors': ['Ailin Deng', 'Tri Cao', 'Zhirui Chen', 'Bryan Hooi'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.02199.jpg', 'data': {'categories': ['#interpretability', '#alignment', '#training', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Опасность слепой веры в текст: проблема и решения для VLM моделей', 'desc': "Исследование показывает, что модели компьютерного зрения и обработки естественного языка (VLM) склонны чрезмерно доверять текстовым данным при несоответствии между визуальной и текстовой информацией. Это явление, названное 'слепой верой в текст', может привести к значительному снижению производительности моделей и вызывает опасения по поводу их безопасности. Авторы анализируют факторы, влияющие на текстовое смещение, и предлагают методы его уменьшения, включая дообучение с аугментацией текста. Теоретический анализ предполагает, что проблема может быть связана с дисбалансом чисто текстовых и мультимодальных данных во время обучения моделей."}, 'en': {'title': 'Balancing Vision and Text: Overcoming Bias in Vision-Language Models', 'desc': "This paper investigates how Vision-Language Models (VLMs) manage inconsistencies between visual and textual information in tasks that rely heavily on vision. The authors identify a phenomenon called 'blind faith in text,' where VLMs tend to rely more on textual data than visual data when faced with conflicting inputs, which can lead to performance issues. They analyze various factors that contribute to this text bias, such as the size of the language model and the order of tokens in the text. To mitigate this bias, the paper proposes supervised fine-tuning with text augmentation and emphasizes the importance of balanced training for improving the reliability of VLMs in multi-modal contexts."}, 'zh': {'title': '平衡训练，提升视觉语言模型的可靠性', 'desc': '视觉语言模型（VLMs）在处理视觉和文本信息方面表现出色，但它们在面对模态不一致时的表现尚未得到充分研究。我们探讨了VLMs在视觉数据和不同文本输入下的模态偏好，发现了“对文本的盲目信任”现象：当出现不一致时，VLMs过度依赖文本数据，导致性能显著下降。我们分析了影响这种文本偏见的因素，包括指令提示、语言模型大小、文本相关性、标记顺序以及视觉和文本确定性之间的相互作用。为了解决这个问题，我们探索了带有文本增强的监督微调，并证明其在减少文本偏见方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.07595', 'title': 'Detection Avoidance Techniques for Large Language Models', 'url': 'https://huggingface.co/papers/2503.07595', 'abstract': "The increasing popularity of large language models has not only led to widespread use but has also brought various risks, including the potential for systematically spreading fake news. Consequently, the development of classification systems such as DetectGPT has become vital. These detectors are vulnerable to evasion techniques, as demonstrated in an experimental series: Systematic changes of the generative models' temperature proofed shallow learning-detectors to be the least reliable. Fine-tuning the generative model via reinforcement learning circumvented BERT-based-detectors. Finally, rephrasing led to a >90\\% evasion of zero-shot-detectors like DetectGPT, although texts stayed highly similar to the original. A comparison with existing work highlights the better performance of the presented methods. Possible implications for society and further research are discussed.", 'score': 2, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '0b6929d80e047189', 'authors': ['Sinclair Schneider', 'Florian Steuber', 'Joao A. G. Schneider', 'Gabi Dreo Rodosek'], 'affiliations': ['Research Institute CODE, Bundeswehr University Munich, Munich, 81739, Bavaria, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2503.07595.jpg', 'data': {'categories': ['#security', '#benchmark', '#rlhf', '#data', '#ethics', '#rl', '#hallucinations'], 'emoji': '🕵️', 'ru': {'title': 'Обман детекторов: как LLM обходят системы обнаружения ИИ-текстов', 'desc': 'Статья рассматривает проблему уязвимости систем обнаружения текстов, сгенерированных большими языковыми моделями (LLM). Авторы провели серию экспериментов, демонстрирующих, как различные методы могут обойти существующие детекторы, такие как DetectGPT. Были исследованы техники изменения температуры генеративной модели, дообучение с помощью обучения с подкреплением и перефразирование текста. Результаты показывают, что предложенные методы обхода более эффективны, чем существующие подходы, что поднимает вопросы о надежности текущих систем обнаружения ИИ-генерированного контента.'}, 'en': {'title': 'Enhancing Evasion Techniques Against Language Model Detectors', 'desc': "This paper discusses the risks associated with large language models, particularly their potential to spread misinformation. It introduces DetectGPT, a classification system designed to identify generated text, but reveals its vulnerabilities to evasion techniques. The study shows that adjusting the generative model's temperature and fine-tuning it with reinforcement learning can effectively bypass existing detectors. Additionally, rephrasing generated content can achieve over 90% evasion rates while maintaining similarity to the original text, highlighting the need for improved detection methods."}, 'zh': {'title': '应对假新闻的智能检测挑战', 'desc': '随着大型语言模型的普及，假新闻传播的风险也随之增加。因此，开发像DetectGPT这样的分类系统变得至关重要。这些检测器容易受到规避技术的影响，实验表明，生成模型温度的系统性变化使得浅层学习检测器的可靠性最低。通过强化学习微调生成模型可以绕过基于BERT的检测器，而重新表述文本则使得像DetectGPT这样的零样本检测器的规避率超过90%，尽管文本与原文高度相似。'}}}, {'id': 'https://huggingface.co/papers/2503.07465', 'title': 'YOLOE: Real-Time Seeing Anything', 'url': 'https://huggingface.co/papers/2503.07465', 'abstract': "Object detection and segmentation are widely employed in computer vision applications, yet conventional models like YOLO series, while efficient and accurate, are limited by predefined categories, hindering adaptability in open scenarios. Recent open-set methods leverage text prompts, visual cues, or prompt-free paradigm to overcome this, but often compromise between performance and efficiency due to high computational demands or deployment complexity. In this work, we introduce YOLOE, which integrates detection and segmentation across diverse open prompt mechanisms within a single highly efficient model, achieving real-time seeing anything. For text prompts, we propose Re-parameterizable Region-Text Alignment (RepRTA) strategy. It refines pretrained textual embeddings via a re-parameterizable lightweight auxiliary network and enhances visual-textual alignment with zero inference and transferring overhead. For visual prompts, we present Semantic-Activated Visual Prompt Encoder (SAVPE). It employs decoupled semantic and activation branches to bring improved visual embedding and accuracy with minimal complexity. For prompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy. It utilizes a built-in large vocabulary and specialized embedding to identify all objects, avoiding costly language model dependency. Extensive experiments show YOLOE's exceptional zero-shot performance and transferability with high inference efficiency and low training cost. Notably, on LVIS, with 3times less training cost and 1.4times inference speedup, YOLOE-v8-S surpasses YOLO-Worldv2-S by 3.5 AP. When transferring to COCO, YOLOE-v8-L achieves 0.6 AP^b and 0.4 AP^m gains over closed-set YOLOv8-L with nearly 4times less training time. Code and models are available at https://github.com/THU-MIG/yoloe.", 'score': 2, 'issue_id': 2637, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': 'faab5c7007c9cc4d', 'authors': ['Ao Wang', 'Lihao Liu', 'Hui Chen', 'Zijia Lin', 'Jungong Han', 'Guiguang Ding'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07465.jpg', 'data': {'categories': ['#training', '#benchmark', '#transfer_learning', '#optimization', '#cv'], 'emoji': '🔍', 'ru': {'title': 'YOLOE: Эффективное обнаружение и сегментация чего угодно в реальном времени', 'desc': 'YOLOE - это новая модель для обнаружения и сегментации объектов, которая объединяет различные механизмы открытых подсказок в одной эффективной архитектуре. Она использует стратегию RepRTA для текстовых подсказок, энкодер SAVPE для визуальных подсказок и метод LRPC для сценариев без подсказок. YOLOE демонстрирует исключительную производительность в задачах zero-shot и переносимость, превосходя существующие модели по эффективности и точности при меньших затратах на обучение.'}, 'en': {'title': 'YOLOE: Real-Time Object Detection and Segmentation for Open-Set Scenarios', 'desc': 'This paper presents YOLOE, a novel model that enhances object detection and segmentation in open-set scenarios by integrating various prompt mechanisms. It introduces a Re-parameterizable Region-Text Alignment (RepRTA) strategy for text prompts, which refines textual embeddings efficiently without additional inference costs. For visual prompts, the Semantic-Activated Visual Prompt Encoder (SAVPE) improves visual accuracy while maintaining low complexity. Additionally, the Lazy Region-Prompt Contrast (LRPC) strategy allows for prompt-free object identification, significantly reducing training costs and improving inference speed compared to traditional models.'}, 'zh': {'title': 'YOLOE：高效的开放场景目标检测与分割', 'desc': '本文介绍了一种新的目标检测和分割模型YOLOE，旨在克服传统模型在开放场景中的局限性。YOLOE通过集成多种开放提示机制，实现了高效的实时检测和分割。我们提出了可重参数化区域-文本对齐策略（RepRTA）和语义激活视觉提示编码器（SAVPE），以提高视觉和文本的对齐效果。实验结果表明，YOLOE在零样本性能和迁移能力上表现优异，同时训练成本低，推理效率高。'}}}, {'id': 'https://huggingface.co/papers/2503.07389', 'title': 'TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image\n  Diffusion Models', 'url': 'https://huggingface.co/papers/2503.07389', 'abstract': "Recent advances in text-to-image diffusion models enable photorealistic image generation, but they also risk producing malicious content, such as NSFW images. To mitigate risk, concept erasure methods are studied to facilitate the model to unlearn specific concepts. However, current studies struggle to fully erase malicious concepts implicitly embedded in prompts (e.g., metaphorical expressions or adversarial prompts) while preserving the model's normal generation capability. To address this challenge, our study proposes TRCE, using a two-stage concept erasure strategy to achieve an effective trade-off between reliable erasure and knowledge preservation. Firstly, TRCE starts by erasing the malicious semantics implicitly embedded in textual prompts. By identifying a critical mapping objective(i.e., the [EoT] embedding), we optimize the cross-attention layers to map malicious prompts to contextually similar prompts but with safe concepts. This step prevents the model from being overly influenced by malicious semantics during the denoising process. Following this, considering the deterministic properties of the sampling trajectory of the diffusion model, TRCE further steers the early denoising prediction toward the safe direction and away from the unsafe one through contrastive learning, thus further avoiding the generation of malicious content. Finally, we conduct comprehensive evaluations of TRCE on multiple malicious concept erasure benchmarks, and the results demonstrate its effectiveness in erasing malicious concepts while better preserving the model's original generation ability. The code is available at: http://github.com/ddgoodgood/TRCE. CAUTION: This paper includes model-generated content that may contain offensive material.", 'score': 2, 'issue_id': 2642, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': 'd88e626045dd077e', 'authors': ['Ruidong Chen', 'Honglin Guo', 'Lanjun Wang', 'Chenyu Zhang', 'Weizhi Nie', 'An-An Liu'], 'affiliations': ['The School of Electrical and Information Engineering, Tianjin University', 'The School of New Media and Communication, Tianjin University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07389.jpg', 'data': {'categories': ['#benchmark', '#security', '#open_source', '#training', '#cv', '#diffusion', '#hallucinations'], 'emoji': '🔒', 'ru': {'title': 'Безопасная генерация изображений: эффективное удаление нежелательных концепций', 'desc': 'Эта статья представляет новый метод под названием TRCE для удаления нежелательных концепций из моделей генерации изображений по тексту. TRCE использует двухэтапную стратегию: сначала удаляет вредоносную семантику из текстовых промптов, а затем корректирует процесс генерации изображения в безопасном направлении. Метод эффективно удаляет нежелательные концепции, сохраняя при этом общие возможности модели. Авторы провели комплексную оценку TRCE на различных наборах данных, подтверждающую его эффективность.'}, 'en': {'title': 'TRCE: Safeguarding Image Generation with Smart Concept Erasure', 'desc': "This paper presents TRCE, a novel approach to mitigate the risk of generating malicious content in text-to-image diffusion models. It employs a two-stage concept erasure strategy that effectively removes harmful semantics from prompts while maintaining the model's ability to generate normal content. The first stage focuses on optimizing cross-attention layers to transform malicious prompts into safer alternatives. The second stage utilizes contrastive learning to guide the denoising process towards safe outputs, ensuring that the model does not produce NSFW images while preserving its generative capabilities."}, 'zh': {'title': '安全生成，抹除恶意概念的创新方法', 'desc': '最近，文本到图像的扩散模型在生成逼真图像方面取得了进展，但也存在生成恶意内容的风险。为了解决这个问题，研究了概念抹除方法，以帮助模型忘记特定的恶意概念。我们的研究提出了TRCE，采用两阶段的概念抹除策略，在可靠抹除和知识保留之间实现有效的平衡。通过优化交叉注意力层，TRCE能够将恶意提示映射到安全的概念，从而避免生成恶意内容。'}}}, {'id': 'https://huggingface.co/papers/2503.06960', 'title': 'A Data-Centric Revisit of Pre-Trained Vision Models for Robot Learning', 'url': 'https://huggingface.co/papers/2503.06960', 'abstract': 'Pre-trained vision models (PVMs) are fundamental to modern robotics, yet their optimal configuration remains unclear. Through systematic evaluation, we find that while DINO and iBOT outperform MAE across visuomotor control and perception tasks, they struggle when trained on non-(single-)object-centric (NOC) data--a limitation strongly correlated with their diminished ability to learn object-centric representations. This investigation indicates that the ability to form object-centric representations from the non-object-centric robotics dataset is the key to success for PVMs. Motivated by this discovery, we designed SlotMIM, a method that induces object-centric representations by introducing a semantic bottleneck to reduce the number of prototypes to encourage the emergence of objectness as well as cross-view consistency regularization for encouraging multiview invariance. Our experiments encompass pre-training on object-centric, scene-centric, web-crawled, and ego-centric data. Across all settings, our approach learns transferrable representations and achieves significant improvements over prior work in image recognition, scene understanding, and robot learning evaluations. When scaled up with million-scale datasets, our method also demonstrates superior data efficiency and scalability. Our code and models are publicly available at https://github.com/CVMI-Lab/SlotMIM.', 'score': 2, 'issue_id': 2642, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '35668e47701edcd0', 'authors': ['Xin Wen', 'Bingchen Zhao', 'Yilun Chen', 'Jiangmiao Pang', 'Xiaojuan Qi'], 'affiliations': ['Shanghai AI Laboratory', 'The University of Hong Kong', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2503.06960.jpg', 'data': {'categories': ['#robotics', '#open_source', '#training', '#cv', '#dataset', '#transfer_learning'], 'emoji': '🤖', 'ru': {'title': 'SlotMIM: прорыв в обучении объектно-ориентированным представлениям для робототехники', 'desc': 'Исследование показывает, что предварительно обученные модели компьютерного зрения (PVM) сталкиваются с трудностями при обучении на данных, не сфокусированных на отдельных объектах. Авторы выявили, что ключом к успеху PVM является способность формировать объектно-ориентированные представления из роботизированных наборов данных. На основе этого открытия они разработали метод SlotMIM, который вводит семантическое сужение для уменьшения количества прототипов и поощрения появления объектности. Эксперименты показали, что SlotMIM достигает значительных улучшений в задачах распознавания изображений, понимания сцен и обучения роботов.'}, 'en': {'title': 'Unlocking Object-Centric Learning in Robotics with SlotMIM', 'desc': 'This paper explores the effectiveness of pre-trained vision models (PVMs) in robotics, particularly focusing on their ability to learn object-centric representations. The authors find that models like DINO and iBOT perform better than MAE in various tasks but struggle with non-object-centric data. They introduce SlotMIM, a new method that enhances object-centric learning by using a semantic bottleneck and cross-view consistency regularization. Their experiments show that SlotMIM significantly improves representation learning across different datasets and tasks, demonstrating better data efficiency and scalability.'}, 'zh': {'title': '对象中心表示是成功的关键', 'desc': '预训练视觉模型（PVMs）在现代机器人技术中至关重要，但其最佳配置尚不明确。研究发现，DINO和iBOT在视觉运动控制和感知任务中表现优于MAE，但在非单对象中心（NOC）数据上训练时表现不佳，这与它们学习对象中心表示的能力下降密切相关。我们的研究表明，从非对象中心的机器人数据集中形成对象中心表示的能力是PVMs成功的关键。为此，我们设计了SlotMIM方法，通过引入语义瓶颈来减少原型数量，促进对象性出现，并通过交叉视图一致性正则化来鼓励多视图不变性。'}}}, {'id': 'https://huggingface.co/papers/2503.06885', 'title': 'ProBench: Judging Multimodal Foundation Models on Open-ended\n  Multi-domain Expert Tasks', 'url': 'https://huggingface.co/papers/2503.06885', 'abstract': 'Solving expert-level multimodal tasks is a key milestone towards general intelligence. As the capabilities of multimodal large language models (MLLMs) continue to improve, evaluation of such advanced multimodal intelligence becomes necessary yet challenging. In this work, we introduce ProBench, a benchmark of open-ended user queries that require professional expertise and advanced reasoning. ProBench consists of 4,000 high-quality samples independently submitted by professionals based on their daily productivity demands. It spans across 10 fields and 56 sub-fields, including science, arts, humanities, coding, mathematics, and creative writing. Experimentally, we evaluate and compare 24 latest models using MLLM-as-a-Judge. Our results reveal that although the best open-source models rival the proprietary ones, ProBench presents significant challenges in visual perception, textual understanding, domain knowledge and advanced reasoning, thus providing valuable directions for future multimodal AI research efforts.', 'score': 2, 'issue_id': 2633, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '196c83578a251f8e', 'authors': ['Yan Yang', 'Dongxu Li', 'Haoning Wu', 'Bei Chen', 'Liu Liu', 'Liyuan Pan', 'Junnan Li'], 'affiliations': ['ANU', 'BITSZ & School of CSAT, BIT', 'KooMap, Huawei', 'NTU', 'Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.06885.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#agi', '#benchmark', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'ProBench: испытание интеллекта мультимодальных ИИ-моделей', 'desc': 'ProBench - это новый бенчмарк для оценки продвинутых мультимодальных языковых моделей (MLLM). Он состоит из 4000 высококачественных задач, охватывающих 10 областей и 56 подобластей, включая науку, искусство, гуманитарные науки и программирование. Задачи были предложены профессионалами на основе их повседневных рабочих потребностей. Результаты тестирования 24 новейших моделей с использованием метода MLLM-as-a-Judge показали, что ProBench представляет значительные трудности в визуальном восприятии, понимании текста, предметных знаниях и продвинутых рассуждениях.'}, 'en': {'title': 'ProBench: Benchmarking Multimodal Intelligence for Expert Tasks', 'desc': 'This paper presents ProBench, a new benchmark designed to evaluate multimodal large language models (MLLMs) on expert-level tasks. ProBench includes 4,000 user queries that require advanced reasoning and professional expertise across various fields such as science, arts, and coding. The study compares 24 state-of-the-art models using MLLM-as-a-Judge, highlighting the challenges these models face in visual perception, textual understanding, and domain knowledge. The findings indicate that while some open-source models perform comparably to proprietary ones, ProBench identifies critical areas for improvement in multimodal AI capabilities.'}, 'zh': {'title': '多模态智能评估的新基准：ProBench', 'desc': '本论文介绍了ProBench，这是一个针对多模态大语言模型（MLLM）的基准测试，旨在评估其在专业领域的智能表现。ProBench包含4000个高质量样本，涵盖科学、艺术、人文学科、编程、数学和创意写作等10个领域和56个子领域。通过对24个最新模型的实验评估，结果显示尽管一些开源模型在性能上与专有模型相当，但在视觉感知、文本理解、领域知识和高级推理方面，ProBench仍然提出了显著的挑战。该研究为未来多模态人工智能的研究方向提供了重要的参考。'}}}, {'id': 'https://huggingface.co/papers/2503.06626', 'title': 'DiffCLIP: Differential Attention Meets CLIP', 'url': 'https://huggingface.co/papers/2503.06626', 'abstract': "We propose DiffCLIP, a novel vision-language model that extends the differential attention mechanism to CLIP architectures. Differential attention was originally developed for large language models to amplify relevant context while canceling out noisy information. In this work, we integrate this mechanism into CLIP's dual encoder (image and text) framework. With minimal additional parameters, DiffCLIP achieves superior performance on image-text understanding tasks. Across zero-shot classification, retrieval, and robustness benchmarks, DiffCLIP consistently outperforms baseline CLIP models. Notably, these gains come with negligible computational overhead, demonstrating that differential attention can significantly enhance multi-modal representations without sacrificing efficiency. Code can be found at https://github.com/hammoudhasan/DiffCLIP.", 'score': 2, 'issue_id': 2641, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': 'fff7aaf70b1d6ed0', 'authors': ['Hasan Abed Al Kader Hammoud', 'Bernard Ghanem'], 'affiliations': ['KAUST'], 'pdf_title_img': 'assets/pdf/title_img/2503.06626.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#benchmark', '#multimodal', '#cv'], 'emoji': '🔍', 'ru': {'title': 'DiffCLIP: улучшение мультимодальных представлений с помощью дифференциального внимания', 'desc': 'Исследователи представили DiffCLIP - новую модель для обработки изображений и текста, которая расширяет механизм дифференциального внимания на архитектуру CLIP. Дифференциальное внимание позволяет усиливать релевантный контекст и подавлять шумовую информацию. DiffCLIP интегрирует этот механизм в двойной энкодер CLIP для изображений и текста. Модель достигает превосходных результатов в задачах понимания изображений и текста, превосходя базовые модели CLIP при минимальных дополнительных параметрах.'}, 'en': {'title': 'Enhancing CLIP with Differential Attention for Better Image-Text Understanding', 'desc': "DiffCLIP is a new vision-language model that improves the CLIP architecture by using a technique called differential attention. This method helps the model focus on important information while ignoring irrelevant details, which was initially designed for large language models. By incorporating differential attention into CLIP's dual encoder system, DiffCLIP enhances its ability to understand images and text together. The model shows better performance in various tasks like zero-shot classification and retrieval, all while maintaining low computational costs."}, 'zh': {'title': 'DiffCLIP：高效增强多模态表示的创新模型', 'desc': '我们提出了DiffCLIP，这是一种新颖的视觉-语言模型，它将差分注意力机制扩展到CLIP架构中。差分注意力最初是为大型语言模型开发的，旨在放大相关上下文，同时消除噪声信息。在这项工作中，我们将这一机制集成到CLIP的双编码器（图像和文本）框架中。DiffCLIP在图像-文本理解任务上表现优越，且几乎没有额外的计算开销，证明了差分注意力可以显著增强多模态表示而不牺牲效率。'}}}, {'id': 'https://huggingface.co/papers/2503.06362', 'title': 'Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal\n  LLMs', 'url': 'https://huggingface.co/papers/2503.06362', 'abstract': 'Audio-Visual Speech Recognition (AVSR) leverages both audio and visual modalities to enhance speech recognition robustness, particularly in noisy environments. Recent advancements in Large Language Models (LLMs) have demonstrated their effectiveness in speech recognition, including AVSR. However, due to the significant length of speech representations, direct integration with LLMs imposes substantial computational costs. Prior approaches address this by compressing speech representations before feeding them into LLMs. However, higher compression ratios often lead to performance degradation, necessitating a trade-off between computational efficiency and recognition accuracy. To address this challenge, we propose Llama-MTSK, the first Matryoshka-based Multimodal LLM for AVSR, which enables flexible adaptation of the audio-visual token allocation based on specific computational constraints while preserving high performance. Our approach, inspired by Matryoshka Representation Learning, encodes audio-visual representations at multiple granularities within a single model, eliminating the need to train separate models for different compression levels. Moreover, to efficiently fine-tune the LLM, we introduce three LoRA-based Matryoshka strategies using global and scale-specific LoRA modules. Extensive evaluations on the two largest AVSR datasets demonstrate that Llama-MTSK achieves state-of-the-art results, matching or surpassing models trained independently at fixed compression levels.', 'score': 2, 'issue_id': 2638, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': 'e35aea6b25fbc264', 'authors': ['Umberto Cappellazzo', 'Minsu Kim', 'Stavros Petridis'], 'affiliations': ['Imperial College London', 'Meta AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.06362.jpg', 'data': {'categories': ['#training', '#multimodal', '#optimization', '#audio'], 'emoji': '🎭', 'ru': {'title': 'Гибкое AVSR с помощью матрешечной мультимодальной языковой модели', 'desc': 'Эта статья представляет Llama-MTSK - первую мультимодальную языковую модель на основе принципа матрешки для аудио-визуального распознавания речи (AVSR). Модель позволяет гибко адаптировать распределение аудио-визуальных токенов в зависимости от вычислительных ограничений, сохраняя при этом высокую производительность. Подход вдохновлен Matryoshka Representation Learning и кодирует аудио-визуальные представления на нескольких уровнях детализации в рамках одной модели. Авторы также предлагают три LoRA-стратегии для эффективной настройки модели.'}, 'en': {'title': 'Llama-MTSK: Efficient AVSR with Flexible Token Allocation', 'desc': 'This paper introduces Llama-MTSK, a novel Matryoshka-based Multimodal Large Language Model (LLM) designed for Audio-Visual Speech Recognition (AVSR). It addresses the challenge of integrating lengthy speech representations with LLMs by allowing flexible audio-visual token allocation based on computational constraints. The model encodes audio-visual data at various granularities, which helps maintain high performance without the need for separate models for different compression levels. Additionally, the paper presents three LoRA-based strategies for fine-tuning the model, achieving state-of-the-art results on major AVSR datasets.'}, 'zh': {'title': '灵活高效的音视频语音识别解决方案', 'desc': '音视频语音识别（AVSR）结合了音频和视觉信息，以提高在嘈杂环境中的语音识别能力。最近，大型语言模型（LLMs）的进展显示了它们在语音识别中的有效性，包括AVSR。然而，由于语音表示的长度较大，直接与LLMs集成会带来巨大的计算成本。为了解决这一问题，我们提出了Llama-MTSK，这是一种基于套娃的多模态LLM，能够根据特定的计算约束灵活调整音视频标记的分配，同时保持高性能。'}}}, {'id': 'https://huggingface.co/papers/2503.05578', 'title': 'Novel Object 6D Pose Estimation with a Single Reference View', 'url': 'https://huggingface.co/papers/2503.05578', 'abstract': 'Existing novel object 6D pose estimation methods typically rely on CAD models or dense reference views, which are both difficult to acquire. Using only a single reference view is more scalable, but challenging due to large pose discrepancies and limited geometric and spatial information. To address these issues, we propose a Single-Reference-based novel object 6D (SinRef-6D) pose estimation method. Our key idea is to iteratively establish point-wise alignment in the camera coordinate system based on state space models (SSMs). Specifically, iterative camera-space point-wise alignment can effectively handle large pose discrepancies, while our proposed RGB and Points SSMs can capture long-range dependencies and spatial information from a single view, offering linear complexity and superior spatial modeling capability. Once pre-trained on synthetic data, SinRef-6D can estimate the 6D pose of a novel object using only a single reference view, without requiring retraining or a CAD model. Extensive experiments on six popular datasets and real-world robotic scenes demonstrate that we achieve on-par performance with CAD-based and dense reference view-based methods, despite operating in the more challenging single reference setting. Code will be released at https://github.com/CNJianLiu/SinRef-6D.', 'score': 2, 'issue_id': 2640, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '689a25e37e909986', 'authors': ['Jian Liu', 'Wei Sun', 'Kai Zeng', 'Jin Zheng', 'Hui Yang', 'Lin Wang', 'Hossein Rahmani', 'Ajmal Mian'], 'affiliations': ['Central South University', 'Hunan University', 'Lancaster University', 'Nanyang Technological University', 'The University of Western Australia'], 'pdf_title_img': 'assets/pdf/title_img/2503.05578.jpg', 'data': {'categories': ['#3d', '#cv', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Точная оценка 6D-позы объекта по одному эталонному изображению', 'desc': 'Статья представляет метод SinRef-6D для оценки 6D-позы новых объектов на основе одного эталонного изображения. Метод использует итеративное выравнивание точек в системе координат камеры и модели пространства состояний для RGB-данных и облака точек. SinRef-6D способен обрабатывать большие расхождения в позе и извлекать пространственную информацию из одного изображения. После предварительного обучения на синтетических данных, метод может оценивать позу новых объектов без переобучения или CAD-моделей.'}, 'en': {'title': 'Single View, Full Pose: Simplifying 6D Object Estimation', 'desc': 'This paper introduces a new method for estimating the 6D pose of novel objects using only a single reference view, which is more scalable than traditional methods that rely on CAD models or multiple views. The proposed SinRef-6D method utilizes iterative point-wise alignment in the camera coordinate system, effectively addressing challenges posed by large pose discrepancies. By employing state space models (SSMs), the method captures long-range dependencies and spatial information, achieving linear complexity and enhanced spatial modeling. Experimental results show that SinRef-6D performs comparably to existing methods while simplifying the pose estimation process.'}, 'zh': {'title': '单视图下的六维姿态估计新方法', 'desc': '本文提出了一种新的单参考视图的六维姿态估计方法，称为SinRef-6D。该方法通过在相机坐标系中迭代建立点对齐，解决了大姿态差异和空间信息不足的问题。SinRef-6D利用状态空间模型（SSMs）来捕捉长距离依赖关系，并具有线性复杂度和优越的空间建模能力。经过在合成数据上的预训练后，SinRef-6D能够仅使用单一参考视图估计新物体的六维姿态，且无需重新训练或CAD模型。'}}}, {'id': 'https://huggingface.co/papers/2503.07598', 'title': 'VACE: All-in-One Video Creation and Editing', 'url': 'https://huggingface.co/papers/2503.07598', 'abstract': 'Diffusion Transformer has demonstrated powerful capability and scalability in generating high-quality images and videos. Further pursuing the unification of generation and editing tasks has yielded significant progress in the domain of image content creation. However, due to the intrinsic demands for consistency across both temporal and spatial dynamics, achieving a unified approach for video synthesis remains challenging. We introduce VACE, which enables users to perform Video tasks within an All-in-one framework for Creation and Editing. These tasks include reference-to-video generation, video-to-video editing, and masked <PRE_TAG>video-to-video editing</POST_TAG>. Specifically, we effectively integrate the requirements of various tasks by organizing video task inputs, such as editing, reference, and masking, into a unified interface referred to as the Video Condition Unit (VCU). Furthermore, by utilizing a Context Adapter structure, we inject different task concepts into the model using formalized representations of temporal and spatial dimensions, allowing it to handle arbitrary video synthesis tasks flexibly. Extensive experiments demonstrate that the unified model of VACE achieves performance on par with task-specific models across various subtasks. Simultaneously, it enables diverse applications through versatile task combinations. Project page: https://ali-vilab.github.io/VACE-Page/.', 'score': 1, 'issue_id': 2644, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '6bf5924fe3265297', 'authors': ['Zeyinzi Jiang', 'Zhen Han', 'Chaojie Mao', 'Jingfeng Zhang', 'Yulin Pan', 'Yu Liu'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.07598.jpg', 'data': {'categories': ['#architecture', '#video', '#multimodal', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'VACE: Универсальный инструмент для создания и редактирования видео', 'desc': 'VACE - это универсальная модель для создания и редактирования видео, объединяющая различные задачи в рамках единого фреймворка. Она использует специальный интерфейс Video Condition Unit для организации входных данных и Context Adapter для внедрения концепций различных задач. VACE позволяет выполнять генерацию видео по референсу, редактирование видео и маскированное редактирование видео. Эксперименты показывают, что модель достигает производительности на уровне специализированных моделей для отдельных подзадач.'}, 'en': {'title': 'VACE: Unifying Video Creation and Editing in One Framework', 'desc': 'The paper introduces VACE, a unified framework for video generation and editing that leverages the capabilities of the Diffusion Transformer. It addresses the challenges of maintaining consistency in both temporal and spatial dynamics during video synthesis. VACE organizes various video tasks, such as reference-to-video generation and video-to-video editing, into a single interface called the Video Condition Unit (VCU). By employing a Context Adapter structure, VACE allows for flexible handling of different video synthesis tasks while achieving performance comparable to specialized models.'}, 'zh': {'title': 'VACE：视频创作与编辑的统一框架', 'desc': '扩散变换器在生成高质量图像和视频方面表现出强大的能力和可扩展性。为了统一生成和编辑任务，研究者们在图像内容创作领域取得了显著进展。尽管如此，由于时间和空间动态的一致性要求，视频合成的统一方法仍然面临挑战。我们提出了VACE，它允许用户在一个综合框架内执行视频任务，包括参考视频生成、视频编辑和掩码视频编辑等。'}}}, {'id': 'https://huggingface.co/papers/2503.07597', 'title': 'HumanMM: Global Human Motion Recovery from Multi-shot Videos', 'url': 'https://huggingface.co/papers/2503.07597', 'abstract': 'In this paper, we present a novel framework designed to reconstruct long-sequence 3D human motion in the world coordinates from in-the-wild videos with multiple shot transitions. Such long-sequence in-the-wild motions are highly valuable to applications such as motion generation and motion understanding, but are of great challenge to be recovered due to abrupt shot transitions, partial occlusions, and dynamic backgrounds presented in such videos. Existing methods primarily focus on single-shot videos, where continuity is maintained within a single camera view, or simplify multi-shot alignment in camera space only. In this work, we tackle the challenges by integrating an enhanced camera pose estimation with Human Motion Recovery (HMR) by incorporating a shot transition detector and a robust alignment module for accurate pose and orientation continuity across shots. By leveraging a custom motion integrator, we effectively mitigate the problem of foot sliding and ensure temporal consistency in human pose. Extensive evaluations on our created multi-shot dataset from public 3D human datasets demonstrate the robustness of our method in reconstructing realistic human motion in world coordinates.', 'score': 1, 'issue_id': 2640, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '161213a2a054dd4d', 'authors': ['Yuhong Zhang', 'Guanlin Wu', 'Ling-Hao Chen', 'Zhuokai Zhao', 'Jing Lin', 'Xiaoke Jiang', 'Jiamin Wu', 'Zhuoheng Li', 'Hao Frank Yang', 'Haoqian Wang', 'Lei Zhang'], 'affiliations': ['HKU', 'HKUST', 'IDEA Research', 'Johns Hopkins University', 'Tsinghua University', 'University of Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2503.07597.jpg', 'data': {'categories': ['#3d', '#dataset', '#long_context', '#video'], 'emoji': '🏃', 'ru': {'title': 'Реконструкция реалистичного 3D-движения человека из видео с несколькими кадрами', 'desc': 'Статья представляет новую систему для реконструкции длинных последовательностей трехмерного движения человека в мировых координатах из видео с несколькими сменами кадров. Авторы решают проблемы резких переходов между кадрами, частичных окклюзий и динамического фона, интегрируя улучшенную оценку положения камеры с восстановлением движения человека. Система включает в себя детектор смены кадров и модуль надежного выравнивания для обеспечения непрерывности позы и ориентации между кадрами. Использование специального интегратора движения позволяет уменьшить проблему скольжения ног и обеспечить временную согласованность позы человека.'}, 'en': {'title': 'Reconstructing Realistic 3D Human Motion Across Multiple Video Shots', 'desc': 'This paper introduces a new framework for reconstructing long sequences of 3D human motion from videos that have multiple camera angles and transitions. The challenge lies in dealing with issues like sudden changes in shots, parts of the person being blocked, and moving backgrounds. Unlike previous methods that only work with single camera views, this approach combines advanced camera pose estimation with Human Motion Recovery (HMR) and includes a shot transition detector for better accuracy. The results show that the method effectively reduces foot sliding and maintains consistent human poses over time, proving its effectiveness on a specially created multi-shot dataset.'}, 'zh': {'title': '重建真实人类运动的新方法', 'desc': '本文提出了一种新颖的框架，旨在从多镜头切换的野外视频中重建长序列的三维人类运动。由于视频中的突然镜头切换、部分遮挡和动态背景，这种长序列的运动恢复面临很大挑战。我们的方法通过结合增强的相机姿态估计和人类运动恢复（HMR），引入了镜头切换检测器和稳健的对齐模块，以确保姿态和方向在镜头间的连续性。通过自定义的运动整合器，我们有效地减轻了脚滑问题，并确保了人类姿态的时间一致性。'}}}, {'id': 'https://huggingface.co/papers/2503.07426', 'title': 'RePO: ReLU-based Preference Optimization', 'url': 'https://huggingface.co/papers/2503.07426', 'abstract': "Aligning large language models (LLMs) with human preferences is critical for real-world deployment, yet existing methods like RLHF face computational and stability challenges. While DPO establishes an offline paradigm with single hyperparameter beta, subsequent methods like SimPO reintroduce complexity through dual parameters (beta, gamma). We propose {ReLU-based Preference Optimization (RePO)}, a streamlined algorithm that eliminates beta via two advances: (1) retaining SimPO's reference-free margins but removing beta through gradient analysis, and (2) adopting a ReLU-based max-margin loss that naturally filters trivial pairs. Theoretically, RePO is characterized as SimPO's limiting case (beta to infty), where the logistic weighting collapses to binary thresholding, forming a convex envelope of the 0-1 loss. Empirical results on AlpacaEval 2 and Arena-Hard show that RePO outperforms DPO and SimPO across multiple base models, requiring only one hyperparameter to tune.", 'score': 1, 'issue_id': 2637, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '619f3a64896639ee', 'authors': ['Junkang Wu', 'Kexin Huang', 'Xue Wang', 'Jinyang Gao', 'Bolin Ding', 'Jiancan Wu', 'Xiangnan He', 'Xiang Wang'], 'affiliations': ['Alibaba Group, Hangzhou, China', 'University of Science and Technology of China, Hefei, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.07426.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#training'], 'emoji': '🚀', 'ru': {'title': 'RePO: Упрощенная оптимизация предпочтений для больших языковых моделей', 'desc': 'Статья представляет новый алгоритм обучения с подкреплением для больших языковых моделей под названием RePO. Этот метод упрощает существующие подходы, такие как DPO и SimPO, устраняя необходимость в гиперпараметре beta. RePO использует ReLU-основанную функцию потерь для фильтрации тривиальных пар и оптимизации предпочтений. Эмпирические результаты показывают, что RePO превосходит DPO и SimPO на нескольких базовых моделях при использовании только одного гиперпараметра.'}, 'en': {'title': 'Streamlining Preference Optimization for LLMs with RePO', 'desc': 'This paper addresses the challenge of aligning large language models (LLMs) with human preferences using a new method called ReLU-based Preference Optimization (RePO). RePO simplifies the optimization process by eliminating the need for a complex hyperparameter, beta, while still maintaining effective performance through gradient analysis and a ReLU-based max-margin loss. The authors demonstrate that RePO can be seen as a special case of an existing method, SimPO, when certain conditions are met, leading to a more efficient training process. Empirical results indicate that RePO consistently outperforms previous methods like DPO and SimPO, while only requiring the tuning of a single hyperparameter.'}, 'zh': {'title': '简化偏好优化，提升语言模型对齐', 'desc': '本文提出了一种新的算法，称为基于ReLU的偏好优化（RePO），旨在简化大型语言模型（LLMs）与人类偏好的对齐过程。RePO通过两个创新点消除了超参数beta，首先保留了SimPO的无参考边际，但通过梯度分析去除了beta，其次采用基于ReLU的最大边际损失，自然过滤掉无关的配对。理论上，RePO被描述为SimPO的极限情况，当beta趋向于无穷大时，逻辑加权收敛为二元阈值，形成0-1损失的凸包。实验证明，RePO在多个基础模型上优于DPO和SimPO，仅需调整一个超参数。'}}}, {'id': 'https://huggingface.co/papers/2503.05283', 'title': "Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent\n  Spaces", 'url': 'https://huggingface.co/papers/2503.05283', 'abstract': 'Recent works have shown that, when trained at scale, uni-modal 2D vision and text encoders converge to learned features that share remarkable structural properties, despite arising from different representations. However, the role of 3D encoders with respect to other modalities remains unexplored. Furthermore, existing 3D foundation models that leverage large datasets are typically trained with explicit alignment objectives with respect to frozen encoders from other representations. In this work, we investigate the possibility of a posteriori alignment of representations obtained from uni-modal 3D encoders compared to text-based feature spaces. We show that naive post-training feature alignment of uni-modal text and 3D encoders results in limited performance. We then focus on extracting subspaces of the corresponding feature spaces and discover that by projecting learned representations onto well-chosen lower-dimensional <PRE_TAG>subspaces</POST_TAG> the quality of alignment becomes significantly higher, leading to improved accuracy on matching and retrieval tasks. Our analysis further sheds light on the nature of these shared subspaces, which roughly separate between semantic and geometric data representations. Overall, ours is the first work that helps to establish a baseline for post-training alignment of 3D uni-modal and text feature spaces, and helps to highlight both the shared and unique properties of 3D data compared to other representations.', 'score': 1, 'issue_id': 2637, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '85fcbc1ddbe7dbea', 'authors': ['Souhail Hadgi', 'Luca Moschella', 'Andrea Santilli', 'Diego Gomez', 'Qixing Huang', 'Emanuele Rodolà', 'Simone Melzi', 'Maks Ovsjanikov'], 'affiliations': ['Ecole polytechnique', 'Sapienza University of Rome', 'The University of Texas at Austin', 'University of Milano-Bicocca'], 'pdf_title_img': 'assets/pdf/title_img/2503.05283.jpg', 'data': {'categories': ['#alignment', '#3d', '#multimodal', '#transfer_learning'], 'emoji': '🧩', 'ru': {'title': 'Улучшение выравнивания 3D и текстовых признаков через проекцию на подпространства', 'desc': 'Исследование посвящено изучению возможности апостериорного выравнивания представлений, полученных из одномодальных 3D-энкодеров, с текстовыми пространствами признаков. Авторы обнаружили, что простое выравнивание признаков после обучения дает ограниченные результаты. Однако, проецирование выученных представлений на тщательно выбранные подпространства меньшей размерности значительно улучшает качество выравнивания. Анализ также показал, что эти общие подпространства примерно разделяют семантические и геометрические представления данных.'}, 'en': {'title': 'Enhancing 3D and Text Feature Alignment through Subspace Projection', 'desc': 'This paper explores the alignment of features from uni-modal 3D encoders with text-based representations. It reveals that simply aligning these features after training does not yield good results. Instead, the authors find that projecting these features into carefully selected lower-dimensional subspaces significantly improves alignment quality. This work establishes a baseline for understanding how 3D data relates to text features, highlighting both their similarities and differences.'}, 'zh': {'title': '探索3D编码器与文本特征的对齐之路', 'desc': '本研究探讨了3D编码器在与其他模态的关系中的作用，尤其是与文本特征空间的对比。我们发现，简单的后期训练特征对齐方法在单模态文本和3D编码器之间的性能有限。通过提取特征空间的子空间，并将学习到的表示投影到精心选择的低维子空间中，我们显著提高了对齐质量，从而在匹配和检索任务中提升了准确性。我们的工作首次为3D单模态和文本特征空间的后期训练对齐建立了基线，并突出了3D数据与其他表示之间的共享和独特特性。'}}}, {'id': 'https://huggingface.co/papers/2503.04973', 'title': 'Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning', 'url': 'https://huggingface.co/papers/2503.04973', 'abstract': 'Incorporating external knowledge in large language models (LLMs) enhances their utility across diverse applications, but existing methods have trade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via similarity search, but key information may fall outside top ranked results. Long-context models can process multiple documents but are computationally expensive and limited by context window size. Inspired by students condensing study material for open-book exams, we propose task-aware key-value (KV) cache compression, which compresses external knowledge in a zero- or few-shot setup. This enables LLMs to reason efficiently over a compacted representation of all relevant information. Experiments show our approach outperforms both RAG and task-agnostic compression methods. On LongBench v2, it improves accuracy by up to 7 absolute points over RAG with a 30x compression rate, while reducing inference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG performs well when sparse evidence suffices, whereas task-aware compression is superior for broad knowledge tasks.', 'score': 1, 'issue_id': 2640, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': 'a3117b7e2b2c099c', 'authors': ['Giulio Corallo', 'Orion Weller', 'Fabio Petroni', 'Paolo Papotti'], 'affiliations': ['EURECOM', 'Johns Hopkins University', 'SAP Labs', 'Samaya AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.04973.jpg', 'data': {'categories': ['#reasoning', '#synthetic', '#long_context', '#rag', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Эффективное обогащение знаний LLM через интеллектуальное сжатие информации', 'desc': 'Статья предлагает новый метод обогащения знаний больших языковых моделей (LLM) - сжатие кэша ключ-значение с учетом задачи. Этот подход позволяет LLM эффективно рассуждать на основе сжатого представления всей релевантной информации. Эксперименты показывают, что предложенный метод превосходит как RAG, так и методы сжатия, не учитывающие задачу. На тестовом наборе LongBench v2 он повышает точность на 7 процентных пунктов по сравнению с RAG при 30-кратном сжатии, одновременно снижая задержку вывода.'}, 'en': {'title': 'Efficient Knowledge Integration for Enhanced Language Model Performance', 'desc': 'This paper presents a new method for enhancing large language models (LLMs) by incorporating external knowledge more effectively. The proposed task-aware key-value (KV) cache compression allows LLMs to efficiently reason over a compact representation of relevant information, improving performance in zero- or few-shot scenarios. Compared to existing methods like Retrieval-Augmented Generation (RAG), this approach achieves better accuracy and significantly reduces inference time. Experiments demonstrate that while RAG excels with sparse evidence, the new compression technique is more effective for tasks requiring extensive knowledge.'}, 'zh': {'title': '任务感知压缩，提升语言模型效率', 'desc': '本论文提出了一种新的方法，通过任务感知的键值（KV）缓存压缩来整合外部知识，以提高大型语言模型（LLMs）的效率。该方法在零样本或少样本设置下压缩外部知识，使得模型能够在处理相关信息时更加高效。实验结果表明，该方法在准确性和推理延迟方面均优于现有的检索增强生成（RAG）和任务无关的压缩方法。特别是在LongBench v2数据集上，该方法的准确性提高了7个百分点，同时推理延迟从0.43秒减少到0.16秒。'}}}, {'id': 'https://huggingface.co/papers/2503.07413', 'title': 'REF-VLM: Triplet-Based Referring Paradigm for Unified Visual Decoding', 'url': 'https://huggingface.co/papers/2503.07413', 'abstract': 'Multimodal Large Language Models (MLLMs) demonstrate robust zero-shot capabilities across diverse vision-language tasks after training on mega-scale datasets. However, dense prediction tasks, such as semantic segmentation and keypoint detection, pose significant challenges for MLLMs when represented solely as text outputs. Simultaneously, current MLLMs utilizing latent embeddings for visual task decoding generally demonstrate limited adaptability to both multi-task learning and multi-granularity scenarios. In this work, we present REF-VLM, an end-to-end framework for unified training of various visual decoding tasks. To address complex visual decoding scenarios, we introduce the Triplet-Based Referring Paradigm (TRP), which explicitly decouples three critical dimensions in visual decoding tasks through a triplet structure: concepts, decoding types, and targets. TRP employs symbolic delimiters to enforce structured representation learning, enhancing the parsability and interpretability of model outputs. Additionally, we construct Visual-Task Instruction Following Dataset (VTInstruct), a large-scale multi-task dataset containing over 100 million multimodal dialogue samples across 25 task types. Beyond text inputs and outputs, VT-Instruct incorporates various visual prompts such as point, box, scribble, and mask, and generates outputs composed of text and visual units like box, keypoint, depth and mask. The combination of different visual prompts and visual units generates a wide variety of task types, expanding the applicability of REF-VLM significantly. Both qualitative and quantitative experiments demonstrate that our REF-VLM outperforms other MLLMs across a variety of standard benchmarks. The code, dataset, and demo available at https://github.com/MacavityT/REF-VLM.', 'score': 0, 'issue_id': 2643, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '52ecfef5a3f1d715', 'authors': ['Yan Tai', 'Luhao Zhu', 'Zhiqiang Chen', 'Ynan Ding', 'Yiying Dong', 'Xiaohong Liu', 'Guodong Guo'], 'affiliations': ['Hong Kong Polytechnic University', 'Ningbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo, China', 'Shanghai Jiao Tong University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07413.jpg', 'data': {'categories': ['#interpretability', '#cv', '#dataset', '#benchmark', '#multimodal', '#games'], 'emoji': '🖼️', 'ru': {'title': 'REF-VLM: Универсальная мультимодальная модель для задач компьютерного зрения', 'desc': 'В этой статье представлена модель REF-VLM, новая мультимодальная языковая модель для решения разнообразных задач компьютерного зрения. Авторы предлагают подход Triplet-Based Referring Paradigm (TRP) для структурированного представления информации в задачах визуального декодирования. Они также создали большой набор данных VTInstruct, содержащий более 100 миллионов мультимодальных диалогов для 25 типов задач. Эксперименты показывают, что REF-VLM превосходит другие мультимодальные языковые модели на различных стандартных тестах.'}, 'en': {'title': 'REF-VLM: Unifying Visual Decoding for Enhanced Multimodal Learning', 'desc': 'This paper introduces REF-VLM, a new framework designed to improve the performance of Multimodal Large Language Models (MLLMs) on complex visual tasks like semantic segmentation and keypoint detection. It addresses the limitations of current MLLMs by using a Triplet-Based Referring Paradigm (TRP) that separates concepts, decoding types, and targets for better structured learning. The authors also present a large-scale dataset, VTInstruct, which includes multimodal dialogue samples and various visual prompts to enhance training. Experimental results show that REF-VLM significantly outperforms existing MLLMs on standard benchmarks, demonstrating its effectiveness in handling diverse visual decoding tasks.'}, 'zh': {'title': 'REF-VLM：统一视觉解码任务的创新框架', 'desc': '多模态大型语言模型（MLLMs）在经过大规模数据集训练后，展现出在多种视觉语言任务中的强大零-shot能力。然而，对于密集预测任务，如语义分割和关键点检测，仅用文本输出表示时，MLLMs面临重大挑战。为了解决这些复杂的视觉解码场景，我们提出了REF-VLM，一个用于统一训练各种视觉解码任务的端到端框架，并引入了基于三元组的引用范式（TRP），以明确解耦视觉解码任务中的概念、解码类型和目标。我们的实验表明，REF-VLM在多项标准基准测试中优于其他MLLMs，展示了其在多任务学习和多粒度场景中的适应性。'}}}, {'id': 'https://huggingface.co/papers/2503.05641', 'title': 'Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for\n  Heterogeneous Reasoning', 'url': 'https://huggingface.co/papers/2503.05641', 'abstract': "Combining existing pre-trained expert LLMs is a promising avenue for scalably tackling large-scale and diverse tasks. However, selecting experts at the task level is often too coarse-grained, as heterogeneous tasks may require different expertise for each instance. To enable adaptive instance-level mixing of pre-trained LLM experts, we propose Symbolic-MoE, a symbolic, text-based, and gradient-free Mixture-of-Experts framework. Symbolic-MoE takes a fine-grained approach to selection by emphasizing skills, e.g., algebra in math or molecular biology in biomedical reasoning. We propose a skill-based recruiting strategy that dynamically selects the most relevant set of expert LLMs for diverse reasoning tasks based on their strengths. Each selected expert then generates its own reasoning, resulting in k outputs from k experts, which are then synthesized into a final high-quality response by an aggregator chosen based on its ability to integrate diverse reasoning outputs. We show that Symbolic-MoE's instance-level expert selection improves performance by a large margin but -- when implemented naively -- can introduce a high computational overhead due to the need for constant model loading and offloading. To address this, we implement a batch inference strategy that groups instances based on their assigned experts, loading each model only once. This allows us to integrate 16 expert models on 1 GPU with a time cost comparable to or better than prior multi-agent baselines using 4 GPUs. Through extensive evaluations on diverse benchmarks (MMLU-Pro, GPQA, AIME, and MedMCQA), we demonstrate that Symbolic-MoE outperforms strong LLMs like GPT4o-mini, as well as multi-agent approaches, with an absolute average improvement of 8.15% over the best multi-agent baseline. Moreover, Symbolic-MoE removes the need for expensive multi-round discussions, outperforming discussion baselines with less computation.", 'score': 0, 'issue_id': 2643, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': 'cd41a7296c5c9225', 'authors': ['Justin Chih-Yao Chen', 'Sukwon Yun', 'Elias Stengel-Eskin', 'Tianlong Chen', 'Mohit Bansal'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.05641.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#benchmark', '#agents', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Symbolic-MoE: Умное сочетание экспертных LLM для эффективного решения разнообразных задач', 'desc': 'Статья представляет Symbolic-MoE - новый подход к комбинированию предобученных экспертных языковых моделей (LLM) для решения разнообразных задач. Symbolic-MoE использует символьный, текстовый и безградиентный метод выбора экспертов на уровне отдельных примеров, основываясь на необходимых навыках. Предложенная стратегия подбора экспертов динамически выбирает наиболее релевантные модели для различных задач рассуждения, а затем агрегирует их выводы. Авторы демонстрируют, что Symbolic-MoE превосходит сильные базовые модели и мультиагентные подходы на различных бенчмарках, обеспечивая значительное улучшение производительности при эффективном использовании вычислительных ресурсов.'}, 'en': {'title': 'Adaptive Expert Selection for Enhanced Reasoning in LLMs', 'desc': 'The paper introduces Symbolic-MoE, a novel Mixture-of-Experts framework designed to enhance the performance of large language models (LLMs) by enabling instance-level expert selection. This approach allows for the dynamic recruitment of LLMs based on specific skills required for diverse reasoning tasks, such as algebra or molecular biology. By synthesizing outputs from multiple experts, Symbolic-MoE generates high-quality responses while addressing computational efficiency through a batch inference strategy. The results show significant performance improvements over existing models and methods, demonstrating the effectiveness of fine-grained expert selection in machine learning tasks.'}, 'zh': {'title': '实例级专家选择，提升推理性能！', 'desc': '本论文提出了一种名为Symbolic-MoE的框架，旨在通过实例级的专家选择来提高预训练大语言模型（LLM）的性能。该框架采用符号化、基于文本且无梯度的混合专家方法，强调根据任务的不同需求选择合适的专家。通过动态选择最相关的专家，Symbolic-MoE能够在多样化的推理任务中生成高质量的响应。实验结果表明，Symbolic-MoE在多个基准测试中显著超越了现有的强大LLM和多代理方法，且计算效率更高。'}}}, {'id': 'https://huggingface.co/papers/2503.03511', 'title': 'NeuGrasp: Generalizable Neural Surface Reconstruction with Background\n  Priors for Material-Agnostic Object Grasp Detection', 'url': 'https://huggingface.co/papers/2503.03511', 'abstract': 'Robotic grasping in scenes with transparent and specular objects presents great challenges for methods relying on accurate depth information. In this paper, we introduce NeuGrasp, a neural surface reconstruction method that leverages background priors for material-agnostic grasp detection. NeuGrasp integrates transformers and global prior volumes to aggregate multi-view features with spatial encoding, enabling robust surface reconstruction in narrow and sparse viewing conditions. By focusing on foreground objects through residual feature enhancement and refining spatial perception with an occupancy-prior volume, NeuGrasp excels in handling objects with transparent and specular surfaces. Extensive experiments in both simulated and real-world scenarios show that NeuGrasp outperforms state-of-the-art methods in grasping while maintaining comparable reconstruction quality. More details are available at https://neugrasp.github.io/.', 'score': 0, 'issue_id': 2630, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': 'ba660b9e0f676df1', 'authors': ['Qingyu Fan', 'Yinghao Cai', 'Chao Li', 'Wenzhe He', 'Xudong Zheng', 'Tao Lu', 'Bin Liang', 'Shuo Wang'], 'affiliations': ['Qiyuan Lab', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.03511.jpg', 'data': {'categories': ['#robotics', '#agents', '#architecture', '#cv'], 'emoji': '🤖', 'ru': {'title': 'NeuGrasp: нейронный захват для сложных поверхностей', 'desc': 'NeuGrasp - это нейронный метод реконструкции поверхности для захвата объектов с прозрачными и зеркальными поверхностями. Он использует трансформеры и глобальные приоры для агрегации мультиракурсных признаков с пространственным кодированием. NeuGrasp фокусируется на объектах переднего плана и уточняет пространственное восприятие с помощью объема приоров заполненности. Эксперименты показывают, что NeuGrasp превосходит современные методы в задаче захвата объектов.'}, 'en': {'title': 'NeuGrasp: Mastering Grasping with Transparent and Specular Objects', 'desc': 'This paper presents NeuGrasp, a novel approach for robotic grasping that effectively deals with transparent and specular objects, which are challenging for traditional depth-based methods. NeuGrasp utilizes a neural surface reconstruction technique that incorporates background priors to enhance grasp detection without being limited by material properties. By combining transformers and global prior volumes, it aggregates multi-view features with spatial encoding, improving surface reconstruction even in difficult viewing conditions. The method demonstrates superior performance in grasping tasks compared to existing techniques, while also achieving high-quality surface reconstruction in both simulated and real-world environments.'}, 'zh': {'title': 'NeuGrasp：透明物体抓取的新突破', 'desc': '本论文介绍了一种名为NeuGrasp的神经表面重建方法，旨在解决透明和镜面物体抓取中的挑战。该方法利用背景先验进行材料无关的抓取检测，结合了变换器和全局先验体积，以聚合多视角特征并进行空间编码。NeuGrasp通过残差特征增强聚焦于前景物体，并利用占用先验体积来改善空间感知，能够有效处理透明和镜面表面的物体。实验结果表明，NeuGrasp在抓取性能上优于现有的最先进方法，同时保持了相似的重建质量。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (4)', '#agents (7)', '#agi (1)', '#alignment (5)', '#architecture (11)', '#audio (2)', '#benchmark (20)', '#cv (11)', '#data (4)', '#dataset (9)', '#diffusion (6)', '#ethics (2)', '#games (3)', '#graphs', '#hallucinations (3)', '#healthcare (1)', '#inference (4)', '#interpretability (5)', '#leakage', '#long_context (2)', '#low_resource (1)', '#machine_translation (1)', '#math', '#multilingual (1)', '#multimodal (22)', '#open_source (11)', '#optimization (19)', '#plp (1)', '#rag (4)', '#reasoning (11)', '#rl (7)', '#rlhf (3)', '#robotics (3)', '#science', '#security (4)', '#small_models', '#story_generation (2)', '#survey (2)', '#synthetic (1)', '#training (20)', '#transfer_learning (5)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-03-11 16:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-11 16:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-11 16:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    