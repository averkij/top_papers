
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 28 papers. August 20.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">20 августа</span> | <span id="title-articles-count">28 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-08-19.html">⬅️ <span id="prev-date">19.08</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-08-21.html">➡️ <span id="next-date">21.08</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-08.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '20 августа', 'en': 'August 20', 'zh': '8月20日'};
        let feedDateNext = {'ru': '21.08', 'en': '08/21', 'zh': '8月21日'};
        let feedDatePrev = {'ru': '19.08', 'en': '08/19', 'zh': '8月19日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2508.13167', 'title': 'Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent\n  Distillation and Agentic RL', 'url': 'https://huggingface.co/papers/2508.13167', 'abstract': "Chain-of-Agents (CoA) paradigm enables end-to-end complex problem-solving in LLMs through dynamic agent activation, improving performance via multi-agent distillation and agentic reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) and multi-agent systems have demonstrated remarkable capabilities in complex problem-solving tasks such as deep research, vibe coding, and mathematical reasoning. However, most existing multi-agent systems are built upon manual prompt/workflow engineering with sophisticated agent frameworks, making them computationally inefficient, less capable, and can not benefit from data-centric learning. In this work, we introduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables native end-to-end complex problem-solving in the same way as a multi-agent system (i.e., multi-turn problem solving with multiple tools and multiple agents) within one model. In chain-of-agents problem-solving, the model dynamically activates different tool agents and role-playing agents to simulate multi-agent collaboration in an end-to-end fashion. To elicit end-to-end chain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent distillation framework to distill state-of-the-art multi-agent systems into chain-of-agents trajectories for agentic supervised fine-tuning. We then use agentic reinforcement learning on verifiable agentic tasks to further improve the models' capabilities on chain-of-agents problem solving. We call the resulting models Agent Foundation Models (AFMs). Our empirical studies demonstrate that AFM establishes new state-of-the-art performance across diverse benchmarks in both web agent and code agent settings. We make the entire research, including the model weights, code for training and evaluation, and the training data, fully open-sourced, which offers a solid starting point for future research on agent models and agentic RL.", 'score': 65, 'issue_id': 5440, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': 'f7b2eb384a807652', 'authors': ['Weizhen Li', 'Jianbo Lin', 'Zhuosong Jiang', 'Jingyi Cao', 'Xinpeng Liu', 'Jiayu Zhang', 'Zhenqiang Huang', 'Qianben Chen', 'Weichen Sun', 'Qiexiang Wang', 'Hongxuan Lu', 'Tianrui Qin', 'Chenghao Zhu', 'Yi Yao', 'Shuying Fan', 'Xiaowan Li', 'Tiannan Wang', 'Pai Liu', 'King Zhu', 'He Zhu', 'Dingfeng Shi', 'Piaohong Wang', 'Yeyi Guan', 'Xiangru Tang', 'Minghao Liu', 'Yuchen Eleanor Jiang', 'Jian Yang', 'Jiaheng Liu', 'Ge Zhang', 'Wangchunshu Zhou'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2508.13167.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#agi', '#architecture', '#open_source', '#rl', '#agents', '#training', '#rlhf'], 'emoji': '🤖', 'ru': {'title': 'Chain-of-Agents: Новый подход к решению сложных задач в языковых моделях', 'desc': 'Статья представляет новую парадигму рассуждений для больших языковых моделей (LLM) под названием Chain-of-Agents (CoA). Эта парадигма позволяет моделям динамически активировать различных агентов для решения сложных задач в рамках одной модели. Авторы вводят концепцию моделей-основ агентов (Agent Foundation Models, AFM), которые обучаются с помощью мультиагентной дистилляции и агентного обучения с подкреплением. Эмпирические исследования показывают, что AFM устанавливает новый уровень производительности в различных бенчмарках для веб-агентов и агентов-программистов.'}, 'en': {'title': 'Empowering LLMs with Chain-of-Agents for Complex Problem Solving', 'desc': 'The Chain-of-Agents (CoA) paradigm enhances large language models (LLMs) by enabling them to solve complex problems through dynamic activation of multiple agents. This approach allows for end-to-end problem-solving without the need for manual prompt engineering, making it more efficient and capable. By employing multi-agent distillation and agentic reinforcement learning, the model learns to simulate collaboration among agents effectively. The resulting models, termed Agent Foundation Models (AFMs), achieve state-of-the-art performance across various benchmarks, demonstrating significant advancements in multi-agent systems.'}, 'zh': {'title': 'Chain-of-Agents：端到端复杂问题解决的新范式', 'desc': 'Chain-of-Agents（CoA）是一种新颖的LLM推理范式，能够实现端到端的复杂问题解决。该方法通过动态激活不同的工具代理和角色扮演代理，模拟多代理协作，从而提高模型的性能。我们引入了多代理蒸馏框架，将先进的多代理系统转化为Chain-of-Agents轨迹，以进行监督微调。此外，通过在可验证的代理任务上应用代理强化学习，进一步提升了模型在Chain-of-Agents问题解决中的能力。'}}}, {'id': 'https://huggingface.co/papers/2508.14041', 'title': 'LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos', 'url': 'https://huggingface.co/papers/2508.14041', 'abstract': 'LongSplat improves novel view synthesis from long videos with irregular motion through joint optimization, robust pose estimation, and efficient anchor formation.  \t\t\t\t\tAI-generated summary \t\t\t\t LongSplat addresses critical challenges in novel view synthesis (NVS) from casually captured long videos characterized by irregular camera motion, unknown camera poses, and expansive scenes. Current methods often suffer from pose drift, inaccurate geometry initialization, and severe memory limitations. To address these issues, we introduce LongSplat, a robust unposed 3D Gaussian Splatting framework featuring: (1) Incremental Joint Optimization that concurrently optimizes camera poses and 3D Gaussians to avoid local minima and ensure global consistency; (2) a robust Pose Estimation Module leveraging learned 3D priors; and (3) an efficient Octree Anchor Formation mechanism that converts dense point clouds into anchors based on spatial density. Extensive experiments on challenging benchmarks demonstrate that LongSplat achieves state-of-the-art results, substantially improving rendering quality, pose accuracy, and computational efficiency compared to prior approaches. Project page: https://linjohnss.github.io/longsplat/', 'score': 39, 'issue_id': 5440, 'pub_date': '2025-08-19', 'pub_date_card': {'ru': '19 августа', 'en': 'August 19', 'zh': '8月19日'}, 'hash': '87ad1cfb038a78fa', 'authors': ['Chin-Yang Lin', 'Cheng Sun', 'Fu-En Yang', 'Min-Hung Chen', 'Yen-Yu Lin', 'Yu-Lun Liu'], 'affiliations': ['NVIDIA Research', 'National Yang Ming Chiao Tung University'], 'pdf_title_img': 'assets/pdf/title_img/2508.14041.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#cv', '#architecture', '#3d'], 'emoji': '🎥', 'ru': {'title': 'Улучшение синтеза новых ракурсов для длинных видео с помощью LongSplat', 'desc': 'LongSplat - это новый метод синтеза новых ракурсов из длинных видео с нерегулярным движением камеры. Он использует совместную оптимизацию, надежную оценку положения камеры и эффективное формирование опорных точек на основе октодерева. LongSplat решает проблемы дрейфа положения камеры, неточной инициализации геометрии и ограничений памяти. Эксперименты показывают, что метод достигает лучших результатов по качеству рендеринга, точности положения камеры и вычислительной эффективности по сравнению с существующими подходами.'}, 'en': {'title': 'Revolutionizing View Synthesis with LongSplat!', 'desc': 'LongSplat is a new method for creating novel views from long videos that have irregular camera movements. It tackles problems like pose drift and memory limitations by optimizing camera positions and 3D representations together. The method uses a robust pose estimation technique that relies on learned 3D information and an efficient way to organize data into anchors. Experiments show that LongSplat significantly enhances the quality of rendered images and the accuracy of camera poses compared to existing methods.'}, 'zh': {'title': 'LongSplat：提升长视频视图合成的创新解决方案', 'desc': 'LongSplat是一种新颖的视图合成方法，旨在处理长视频中不规则运动带来的挑战。它通过联合优化、稳健的姿态估计和高效的锚点形成来提高合成质量。该方法能够同时优化相机姿态和3D高斯模型，避免局部最小值并确保全局一致性。实验结果表明，LongSplat在渲染质量、姿态准确性和计算效率上均优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2508.13948', 'title': 'Prompt Orchestration Markup Language', 'url': 'https://huggingface.co/papers/2508.13948', 'abstract': 'POML addresses challenges in prompting Large Language Models by providing a structured, data-integrated, and format-sensitive markup language with templating and developer tools.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) require sophisticated prompting, yet current practices face challenges in structure, data integration, format sensitivity, and tooling. Existing methods lack comprehensive solutions for organizing complex prompts involving diverse data types (documents, tables, images) or managing presentation variations systematically. To address these gaps, we introduce POML (Prompt Orchestration Markup Language). POML employs component-based markup for logical structure (roles, tasks, examples), specialized tags for seamless data integration, and a CSS-like styling system to decouple content from presentation, reducing formatting sensitivity. It includes templating for dynamic prompts and a comprehensive developer toolkit (IDE support, SDKs) to improve version control and collaboration. We validate POML through two case studies demonstrating its impact on complex application integration (PomLink) and accuracy performance (TableQA), as well as a user study assessing its effectiveness in real-world development scenarios.', 'score': 23, 'issue_id': 5440, 'pub_date': '2025-08-19', 'pub_date_card': {'ru': '19 августа', 'en': 'August 19', 'zh': '8月19日'}, 'hash': '9520c0e7396f1933', 'authors': ['Yuge Zhang', 'Nan Chen', 'Jiahang Xu', 'Yuqing Yang'], 'affiliations': ['Microsoft Research Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.13948.jpg', 'data': {'categories': ['#data', '#multimodal'], 'emoji': '🗂️', 'ru': {'title': 'POML: структурированный подход к промптингу языковых моделей', 'desc': 'Статья представляет POML - язык разметки для структурированного промптинга больших языковых моделей. POML решает проблемы интеграции данных, чувствительности к форматированию и организации сложных промптов. Он использует компонентную разметку, специальные теги и CSS-подобную систему стилей. POML включает шаблонизацию и инструменты разработчика для улучшения контроля версий и совместной работы.'}, 'en': {'title': 'Streamlining Prompts for Better AI Performance with POML', 'desc': 'POML, or Prompt Orchestration Markup Language, is designed to enhance the prompting process for Large Language Models (LLMs) by introducing a structured and integrated approach. It addresses the challenges of organizing complex prompts that involve various data types, such as documents and images, while also managing different presentation formats. By utilizing a component-based markup system and specialized tags, POML allows for better data integration and reduces sensitivity to formatting issues. The framework also includes templating features and a developer toolkit to facilitate collaboration and improve version control in real-world applications.'}, 'zh': {'title': 'POML：提升大型语言模型提示的利器', 'desc': 'POML（提示编排标记语言）旨在解决大型语言模型（LLMs）在提示过程中面临的结构、数据集成和格式敏感性等挑战。它通过组件化的标记语言提供逻辑结构，使用专门的标签实现数据的无缝集成，并采用类似CSS的样式系统来减少格式敏感性。POML还包括动态提示的模板功能和全面的开发者工具包，以提高版本控制和协作效率。通过两个案例研究，验证了POML在复杂应用集成和准确性表现方面的影响。'}}}, {'id': 'https://huggingface.co/papers/2508.06905', 'title': 'MultiRef: Controllable Image Generation with Multiple Visual References', 'url': 'https://huggingface.co/papers/2508.06905', 'abstract': 'Experiments with multiple image-text models and agentic frameworks show that even state-of-the-art systems struggle with generating images from multiple visual references, highlighting the need for more flexible creative tools.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual designers naturally draw inspiration from multiple visual references, combining diverse elements and aesthetic principles to create artwork. However, current image generative frameworks predominantly rely on single-source inputs -- either text prompts or individual reference images. In this paper, we focus on the task of controllable image generation using multiple visual references. We introduce MultiRef-bench, a rigorous evaluation framework comprising 990 synthetic and 1,000 real-world samples that require incorporating visual content from multiple reference images. The synthetic samples are synthetically generated through our data engine RefBlend, with 10 reference types and 33 reference combinations. Based on RefBlend, we further construct a dataset MultiRef containing 38k high-quality images to facilitate further research. Our experiments across three interleaved image-text models (i.e., OmniGen, ACE, and Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that even state-of-the-art systems struggle with multi-reference conditioning, with the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in real-world cases on average compared to the golden answer. These findings provide valuable directions for developing more flexible and human-like creative tools that can effectively integrate multiple sources of visual inspiration. The dataset is publicly available at: https://multiref.github.io/.', 'score': 13, 'issue_id': 5442, 'pub_date': '2025-08-09', 'pub_date_card': {'ru': '9 августа', 'en': 'August 9', 'zh': '8月9日'}, 'hash': '388c69f1cd50095c', 'authors': ['Ruoxi Chen', 'Dongping Chen', 'Siyuan Wu', 'Sinan Wang', 'Shiyun Lang', 'Petr Sushko', 'Gaoyang Jiang', 'Yao Wan', 'Ranjay Krishna'], 'affiliations': ['Allen Institute for AI Seattle, USA', 'Huazhong University of Science and Technology Wuhan, China', 'University of Washington Seattle, USA', 'Zhejiang Wanli University Ningbo, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.06905.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#synthetic', '#cv', '#dataset', '#benchmark'], 'emoji': '🎨', 'ru': {'title': 'Генеративные модели нуждаются в улучшении работы с множественными визуальными ссылками', 'desc': 'Исследование показывает, что современные системы генерации изображений испытывают трудности при работе с несколькими визуальными ссылками. Авторы представляют MultiRef-bench - набор данных для оценки способности моделей комбинировать элементы из нескольких изображений. Эксперименты с различными моделями, включая OmniGen, ACE и Show-o, демонстрируют ограниченную эффективность даже передовых систем в этой задаче. Результаты указывают на необходимость разработки более гибких инструментов генеративного ИИ, способных интегрировать множественные источники визуального вдохновения.'}, 'en': {'title': 'Enhancing Image Generation with Multiple Visual References', 'desc': 'This paper addresses the challenges faced by current image generation models when tasked with creating images from multiple visual references. It introduces MultiRef-bench, a new evaluation framework designed to assess the performance of these models using a diverse set of synthetic and real-world samples. The study reveals that even advanced models struggle with multi-reference conditioning, achieving only moderate success rates. The findings emphasize the need for improved generative tools that can better mimic human creativity by integrating various visual inspirations.'}, 'zh': {'title': '多参考图像生成的挑战与机遇', 'desc': '本论文探讨了使用多个视觉参考进行可控图像生成的任务。我们提出了MultiRef-bench，这是一个包含990个合成样本和1000个真实样本的评估框架，旨在测试多参考图像的生成能力。实验结果显示，即使是最先进的图像生成模型在多参考条件下也面临挑战，最佳模型OmniGen在合成样本上的表现仅为66.6%。这些发现为开发更灵活的人性化创作工具提供了重要方向。'}}}, {'id': 'https://huggingface.co/papers/2508.08777', 'title': 'Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge', 'url': 'https://huggingface.co/papers/2508.08777', 'abstract': "A novel framework uses Large Language Models to evaluate podcast recommendations by constructing user profiles and providing context for the LLM to make judgments, improving efficiency and interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating personalized recommendations remains a central challenge, especially in long-form audio domains like podcasts, where traditional offline metrics suffer from exposure bias and online methods such as A/B testing are costly and operationally constrained. In this paper, we propose a novel framework that leverages Large Language Models (LLMs) as offline judges to assess the quality of podcast recommendations in a scalable and interpretable manner. Our two-stage profile-aware approach first constructs natural-language user profiles distilled from 90 days of listening history. These profiles summarize both topical interests and behavioral patterns, serving as compact, interpretable representations of user preferences. Rather than prompting the LLM with raw data, we use these profiles to provide high-level, semantically rich context-enabling the LLM to reason more effectively about alignment between a user's interests and recommended episodes. This reduces input complexity and improves interpretability. The LLM is then prompted to deliver fine-grained pointwise and pairwise judgments based on the profile-episode match. In a controlled study with 47 participants, our profile-aware judge matched human judgments with high fidelity and outperformed or matched a variant using raw listening histories. The framework enables efficient, profile-aware evaluation for iterative testing and model selection in recommender systems.", 'score': 10, 'issue_id': 5445, 'pub_date': '2025-08-12', 'pub_date_card': {'ru': '12 августа', 'en': 'August 12', 'zh': '8月12日'}, 'hash': 'f8b8eab199fd0271', 'authors': ['Francesco Fabbri', 'Gustavo Penha', "Edoardo D'Amico", 'Alice Wang', 'Marco De Nadai', 'Jackie Doremus', 'Paul Gigioli', 'Andreas Damianou', 'Oskar Stal', 'Mounia Lalmas'], 'affiliations': ['Spotify Netherlands', 'Spotify Spain', 'Spotify Sweden'], 'pdf_title_img': 'assets/pdf/title_img/2508.08777.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#training', '#audio', '#alignment'], 'emoji': '🎧', 'ru': {'title': 'LLM как эксперт по рекомендациям подкастов', 'desc': 'Предложена новая система оценки рекомендаций подкастов с использованием больших языковых моделей (LLM). Система создает профили пользователей на основе истории прослушивания и использует их для предоставления контекста LLM. LLM затем оценивает соответствие рекомендаций интересам пользователя. В исследовании с участием 47 человек система показала высокую точность по сравнению с оценками людей.'}, 'en': {'title': 'Enhancing Podcast Recommendations with Profile-Aware LLMs', 'desc': 'This paper introduces a new framework that utilizes Large Language Models (LLMs) to enhance the evaluation of podcast recommendations. By creating user profiles from 90 days of listening history, the framework captures user interests and behaviors in a clear and interpretable way. Instead of using raw data, the LLM is provided with these profiles to make more informed judgments about how well podcast episodes align with user preferences. The results show that this method not only matches human evaluations closely but also improves the efficiency of testing and selecting recommendation models.'}, 'zh': {'title': '利用大型语言模型提升播客推荐评估的效率与可解释性', 'desc': '本文提出了一种新颖的框架，利用大型语言模型（LLM）来评估播客推荐的质量。该框架通过构建用户个人资料，提取90天的收听历史，形成自然语言的用户偏好表示。通过这种方式，LLM能够在更高层次上理解用户的兴趣与推荐内容之间的关系，从而提高评估的效率和可解释性。实验结果表明，该方法在与人类判断的匹配度上表现优异，能够有效支持推荐系统的迭代测试和模型选择。'}}}, {'id': 'https://huggingface.co/papers/2508.13998', 'title': 'Embodied-R1: Reinforced Embodied Reasoning for General Robotic\n  Manipulation', 'url': 'https://huggingface.co/papers/2508.13998', 'abstract': 'A pointing-centric representation and reinforced fine-tuning approach improve embodied AI generalization across various tasks and visual disturbances.  \t\t\t\t\tAI-generated summary \t\t\t\t Generalization in embodied AI is hindered by the "seeing-to-doing gap," which stems from data scarcity and embodiment heterogeneity. To address this, we pioneer "pointing" as a unified, embodiment-agnostic intermediate representation, defining four core embodied pointing abilities that bridge high-level vision-language comprehension with low-level action primitives. We introduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed for embodied reasoning and pointing. We use a wide range of embodied and general visual reasoning datasets as sources to construct a large-scale dataset, Embodied-Points-200K, which supports key embodied pointing capabilities. We then train Embodied-R1 using a two-stage Reinforced Fine-tuning (RFT) curriculum with a specialized multi-task reward design. Embodied-R1 achieves state-of-the-art performance on 11 embodied spatial and pointing benchmarks. Critically, it demonstrates robust zero-shot generalization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5% across 8 real-world XArm tasks without any task-specific fine-tuning, representing a 62% improvement over strong baselines. Furthermore, the model exhibits high robustness against diverse visual disturbances. Our work shows that a pointing-centric representation, combined with an RFT training paradigm, offers an effective and generalizable pathway to closing the perception-action gap in robotics.', 'score': 9, 'issue_id': 5446, 'pub_date': '2025-08-19', 'pub_date_card': {'ru': '19 августа', 'en': 'August 19', 'zh': '8月19日'}, 'hash': '8ac5275c70ccd569', 'authors': ['Yifu Yuan', 'Haiqin Cui', 'Yaoting Huang', 'Yibin Chen', 'Fei Ni', 'Zibin Dong', 'Pengyi Li', 'Yan Zheng', 'Jianye Hao'], 'affiliations': ['Tianjin University'], 'pdf_title_img': 'assets/pdf/title_img/2508.13998.jpg', 'data': {'categories': ['#training', '#agents', '#dataset', '#optimization', '#reasoning', '#robotics', '#agi'], 'emoji': '🤖', 'ru': {'title': 'Указательное представление открывает путь к универсальному воплощенному ИИ', 'desc': 'Статья представляет новый подход к улучшению обобщения в воплощенном ИИ с использованием указательного представления. Авторы вводят модель Embodied-R1, 3-миллиардную зрительно-языковую модель, специально разработанную для воплощенных рассуждений и указания. Модель обучается на большом наборе данных Embodied-Points-200K с использованием двухэтапного курса обучения с подкреплением. Embodied-R1 демонстрирует впечатляющие результаты в различных задачах и устойчивость к визуальным помехам.'}, 'en': {'title': 'Bridging the Perception-Action Gap with Pointing in AI', 'desc': "This paper introduces a new approach to improve generalization in embodied AI by using a 'pointing' representation that connects visual understanding with actions. The authors define four key abilities related to pointing that help bridge the gap between high-level comprehension and low-level actions. They present a model called Embodied-R1, which is trained using a two-stage Reinforced Fine-tuning method on a large dataset designed for embodied tasks. The results show that Embodied-R1 outperforms existing models in various benchmarks and demonstrates strong generalization capabilities without needing specific adjustments for different tasks."}, 'zh': {'title': '以指向为中心，提升具身AI的泛化能力', 'desc': '这篇论文提出了一种以指向为中心的表示方法和强化微调策略，以提高具身人工智能在各种任务和视觉干扰下的泛化能力。研究中定义了四种核心的具身指向能力，旨在弥合高层次视觉语言理解与低层次动作原语之间的差距。我们开发了Embodied-R1，一个专门用于具身推理和指向的视觉语言模型，并构建了一个大规模的数据集Embodied-Points-200K，以支持关键的具身指向能力。通过两阶段的强化微调课程，Embodied-R1在多个基准测试中表现出色，展示了在没有特定任务微调的情况下，强大的零样本泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2508.12040', 'title': 'Mind the Generation Process: Fine-Grained Confidence Estimation During\n  LLM Generation', 'url': 'https://huggingface.co/papers/2508.12040', 'abstract': 'FineCE is a novel confidence estimation method for large language models that provides accurate, fine-grained confidence scores during text generation, outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t While large language models (LLMs) have demonstrated remarkable performance across diverse tasks, they fundamentally lack self-awareness and frequently exhibit overconfidence, assigning high confidence scores to incorrect predictions. Accurate confidence estimation is therefore critical for enhancing the trustworthiness and reliability of LLM-generated outputs. However, existing approaches suffer from coarse-grained scoring mechanisms that fail to provide fine-grained, continuous confidence estimates throughout the generation process. To address these limitations, we introduce FineCE, a novel confidence estimation method that delivers accurate, fine-grained confidence scores during text generation. Specifically, we first develop a comprehensive pipeline for constructing training data that effectively captures the underlying probabilistic distribution of LLM responses, and then train a model to predict confidence scores for arbitrary text sequences in a supervised manner. Furthermore, we propose a Backward Confidence Integration (BCI) strategy that leverages information from the subsequent text to enhance confidence estimation for the current sequence during inference. We also introduce three strategies for identifying optimal positions to perform confidence estimation within the generation process. Extensive experiments on multiple benchmark datasets demonstrate that FineCE consistently outperforms existing classical confidence estimation methods. Our code and all baselines used in the paper are available on GitHub.', 'score': 9, 'issue_id': 5449, 'pub_date': '2025-08-16', 'pub_date_card': {'ru': '16 августа', 'en': 'August 16', 'zh': '8月16日'}, 'hash': 'ee8f303581faebe0', 'authors': ['Jinyi Han', 'Tingyun Li', 'Shisong Chen', 'Jie Shi', 'Xinyi Wang', 'Guanglei Yue', 'Jiaqing Liang', 'Xin Lin', 'Liqian Wen', 'Zulong Chen', 'Yanghua Xiao'], 'affiliations': ['Alibaba', 'College of Computer Science and Artificial Intelligence, Fudan University', 'School of Data Science, Fudan University', 'Shanghai Institute of Artificial Intelligence for Education, East China Normal University'], 'pdf_title_img': 'assets/pdf/title_img/2508.12040.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#data', '#optimization', '#training', '#hallucinations', '#inference'], 'emoji': '🎯', 'ru': {'title': 'Точная оценка уверенности для больших языковых моделей', 'desc': 'FineCE - это новый метод оценки уверенности для больших языковых моделей, который обеспечивает точные и детальные оценки уверенности при генерации текста. Метод превосходит существующие подходы, используя комплексный конвейер для создания обучающих данных и стратегию обратной интеграции уверенности. FineCE обучает модель предсказывать оценки уверенности для произвольных текстовых последовательностей в режиме обучения с учителем. Эксперименты на нескольких эталонных наборах данных показывают, что FineCE стабильно превосходит классические методы оценки уверенности.'}, 'en': {'title': 'FineCE: Elevating Confidence in Language Models', 'desc': "FineCE is a new method designed to improve confidence estimation in large language models (LLMs) during text generation. It addresses the issue of LLMs being overconfident in their predictions by providing fine-grained confidence scores that reflect the model's certainty. The method includes a training data pipeline that captures the probabilistic nature of LLM outputs and employs a Backward Confidence Integration strategy to refine confidence scores using future context. Experiments show that FineCE significantly outperforms traditional confidence estimation techniques, enhancing the reliability of AI-generated text."}, 'zh': {'title': 'FineCE：提升大型语言模型置信度的创新方法', 'desc': 'FineCE是一种新颖的置信度估计方法，专为大型语言模型设计，能够在文本生成过程中提供准确且细致的置信度评分。现有方法通常存在评分粗糙的问题，无法在生成过程中提供连续的置信度估计。FineCE通过构建有效的训练数据管道，捕捉大型语言模型响应的概率分布，并训练模型以监督方式预测任意文本序列的置信度评分。此外，FineCE还引入了向后置信度整合策略，利用后续文本的信息来增强当前序列的置信度估计。'}}}, {'id': 'https://huggingface.co/papers/2508.09131', 'title': 'Training-Free Text-Guided Color Editing with Multi-Modal Diffusion\n  Transformer', 'url': 'https://huggingface.co/papers/2508.09131', 'abstract': 'ColorCtrl, a training-free method using Multi-Modal Diffusion Transformers, achieves precise and consistent color editing in images and videos with word-level control and superior performance compared to existing approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-guided color editing in images and videos is a fundamental yet unsolved problem, requiring fine-grained manipulation of color attributes, including albedo, light source color, and ambient lighting, while preserving physical consistency in geometry, material properties, and light-matter interactions. Existing training-free methods offer broad applicability across editing tasks but struggle with precise color control and often introduce visual inconsistency in both edited and non-edited regions. In this work, we present ColorCtrl, a training-free color editing method that leverages the attention mechanisms of modern Multi-Modal Diffusion Transformers (MM-DiT). By disentangling structure and color through targeted manipulation of attention maps and value tokens, our method enables accurate and consistent color editing, along with word-level control of attribute intensity. Our method modifies only the intended regions specified by the prompt, leaving unrelated areas untouched. Extensive experiments on both SD3 and FLUX.1-dev demonstrate that ColorCtrl outperforms existing training-free approaches and achieves state-of-the-art performances in both edit quality and consistency. Furthermore, our method surpasses strong commercial models such as FLUX.1 Kontext Max and GPT-4o Image Generation in terms of consistency. When extended to video models like CogVideoX, our approach exhibits greater advantages, particularly in maintaining temporal coherence and editing stability. Finally, our method also generalizes to instruction-based editing diffusion models such as Step1X-Edit and FLUX.1 Kontext dev, further demonstrating its versatility.', 'score': 9, 'issue_id': 5440, 'pub_date': '2025-08-12', 'pub_date_card': {'ru': '12 августа', 'en': 'August 12', 'zh': '8月12日'}, 'hash': '77b10751b41fef03', 'authors': ['Zixin Yin', 'Xili Dai', 'Ling-Hao Chen', 'Deyu Zhou', 'Jianan Wang', 'Duomin Wang', 'Gang Yu', 'Lionel M. Ni', 'Lei Zhang', 'Heung-Yeung Shum'], 'affiliations': ['Astribot', 'International Digital Economy Academy', 'StepFun', 'The Hong Kong University of Science and Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.09131.jpg', 'data': {'categories': ['#diffusion', '#video', '#cv', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Точное редактирование цвета без обучения с помощью диффузионных трансформеров', 'desc': 'ColorCtrl - это метод редактирования цвета в изображениях и видео без дополнительного обучения, использующий мультимодальные диффузионные трансформеры (MM-DiT). Он позволяет точно и согласованно редактировать цвет с контролем на уровне отдельных слов. ColorCtrl превосходит существующие подходы по качеству редактирования и согласованности результатов. Метод работает путем целенаправленного изменения карт внимания и токенов значений, что позволяет изменять только указанные в запросе области изображения.'}, 'en': {'title': 'Precision Color Editing with Zero Training', 'desc': 'ColorCtrl is a novel method for color editing in images and videos that does not require prior training. It utilizes Multi-Modal Diffusion Transformers to achieve precise control over color attributes while maintaining the physical consistency of the scene. By manipulating attention maps and value tokens, ColorCtrl allows users to edit specific regions of an image based on textual prompts, ensuring that unrelated areas remain unchanged. Extensive testing shows that ColorCtrl outperforms existing methods in both quality and consistency, making it a significant advancement in the field of text-guided color editing.'}, 'zh': {'title': 'ColorCtrl：精准一致的颜色编辑新方法', 'desc': 'ColorCtrl是一种无需训练的颜色编辑方法，利用多模态扩散变换器实现图像和视频的精确颜色编辑。该方法通过目标化的注意力图和数值标记的操控，解耦结构与颜色，从而实现一致的颜色编辑和基于词汇的属性强度控制。与现有方法相比，ColorCtrl在编辑质量和一致性方面表现出色，能够在指定区域进行修改而不影响其他区域。实验结果表明，ColorCtrl在视频模型中也展现出更大的优势，特别是在保持时间一致性和编辑稳定性方面。'}}}, {'id': 'https://huggingface.co/papers/2508.13632', 'title': 'OmniTry: Virtual Try-On Anything without Masks', 'url': 'https://huggingface.co/papers/2508.13632', 'abstract': 'OmniTry extends Virtual Try-ON to various wearable objects using a two-stage pipeline that combines unpaired and paired image training to improve object localization and appearance consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Virtual Try-ON (VTON) is a practical and widely-applied task, for which most of existing works focus on clothes. This paper presents OmniTry, a unified framework that extends VTON beyond garment to encompass any wearable objects, e.g., jewelries and accessories, with mask-free setting for more practical application. When extending to various types of objects, data curation is challenging for obtaining paired images, i.e., the object image and the corresponding try-on result. To tackle this problem, we propose a two-staged pipeline: For the first stage, we leverage large-scale unpaired images, i.e., portraits with any wearable items, to train the model for mask-free localization. Specifically, we repurpose the inpainting model to automatically draw objects in suitable positions given an empty mask. For the second stage, the model is further fine-tuned with paired images to transfer the consistency of object appearance. We observed that the model after the first stage shows quick convergence even with few paired samples. OmniTry is evaluated on a comprehensive benchmark consisting of 12 common classes of wearable objects, with both in-shop and in-the-wild images. Experimental results suggest that OmniTry shows better performance on both object localization and ID-preservation compared with existing methods. The code, model weights, and evaluation benchmark of OmniTry will be made publicly available at https://omnitry.github.io/.', 'score': 8, 'issue_id': 5440, 'pub_date': '2025-08-19', 'pub_date_card': {'ru': '19 августа', 'en': 'August 19', 'zh': '8月19日'}, 'hash': '4d47f2b8665d95cb', 'authors': ['Yutong Feng', 'Linlin Zhang', 'Hengyuan Cao', 'Yiming Chen', 'Xiaoduan Feng', 'Jian Cao', 'Yuxiong Wu', 'Bin Wang'], 'affiliations': ['Kunbyte AI', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.13632.jpg', 'data': {'categories': ['#optimization', '#data', '#benchmark', '#cv', '#open_source', '#dataset', '#training'], 'emoji': '🕶️', 'ru': {'title': 'Виртуальная примерка любых носимых объектов без масок', 'desc': 'OmniTry - это унифицированная система, расширяющая возможности виртуальной примерки (VTON) на различные носимые объекты, включая украшения и аксессуары. Она использует двухэтапный конвейер, сочетающий обучение на непарных и парных изображениях для улучшения локализации объектов и согласованности их внешнего вида. Первый этап использует большой набор непарных изображений для обучения модели безмасочной локализации, а второй этап дообучает модель на парных изображениях для переноса согласованности внешнего вида объекта. OmniTry показывает лучшие результаты по локализации объектов и сохранению их идентичности по сравнению с существующими методами.'}, 'en': {'title': 'OmniTry: Expanding Virtual Try-ON to All Wearables!', 'desc': "OmniTry is a new framework that enhances the Virtual Try-ON (VTON) technology to include various wearable items like jewelry and accessories, not just clothing. It uses a two-stage training process that first employs unpaired images to improve the model's ability to locate objects without needing masks. In the second stage, the model is fine-tuned with paired images to ensure that the appearance of the objects remains consistent. The results show that OmniTry outperforms existing methods in both object localization and identity preservation across a wide range of wearable objects."}, 'zh': {'title': 'OmniTry：扩展虚拟试穿到所有可穿戴物品', 'desc': 'OmniTry是一个扩展虚拟试穿（VTON）到各种可穿戴物品的统一框架，包括珠宝和配饰。该方法采用两阶段的管道，首先利用大规模的无配对图像进行无掩膜定位训练，然后通过配对图像进行进一步微调，以提高物体外观的一致性。研究表明，经过第一阶段训练的模型即使在配对样本较少的情况下也能快速收敛。OmniTry在包含12类常见可穿戴物品的基准测试中表现优于现有方法，尤其在物体定位和ID保持方面。'}}}, {'id': 'https://huggingface.co/papers/2508.12903', 'title': 'A Stitch in Time Saves Nine: Proactive Self-Refinement for Language\n  Models', 'url': 'https://huggingface.co/papers/2508.12903', 'abstract': "ProActive Self-Refinement (PASR) dynamically enhances LLM outputs during generation, reducing token consumption and improving accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in self-refinement have demonstrated significant potential for improving the outputs of large language models (LLMs) through iterative refinement. However, most existing self-refinement methods rely on a reactive process with a fixed number of iterations, making it difficult to determine the optimal timing and content of refinement based on the evolving generation context. Inspired by the way humans dynamically refine their thoughts during execution, we propose ProActive Self-Refinement (PASR), a novel method that enables LLMs to refine their outputs during the generation process. Unlike methods that regenerate entire responses, PASR proactively decides whether, when, and how to refine based on the model's internal state and evolving context. We conduct extensive experiments on a diverse set of 10 tasks to evaluate the effectiveness of PASR. Experimental results show that PASR significantly enhances problem-solving performance. In particular, on Qwen3-8B, PASR reduces average token consumption by 41.6 percent compared to standard generation, while also achieving an 8.2 percent improvement in accuracy. Our code and all baselines used in the paper are available in the GitHub.", 'score': 8, 'issue_id': 5448, 'pub_date': '2025-08-18', 'pub_date_card': {'ru': '18 августа', 'en': 'August 18', 'zh': '8月18日'}, 'hash': 'ed9ab4c483945bed', 'authors': ['Jinyi Han', 'Xinyi Wang', 'Haiquan Zhao', 'Tingyun li', 'Zishang Jiang', 'Sihang Jiang', 'Jiaqing Liang', 'Xin Lin', 'Weikang Zhou', 'Zeye Sun', 'Fei Yu', 'Yanghua Xiao'], 'affiliations': ['Antgroup', 'College of Computer Science and Artificial Intelligence, Fudan University', 'East China Normal University', 'School of Data Science, Fudan University', 'Shanghai Institute of Artificial Intelligence for Education'], 'pdf_title_img': 'assets/pdf/title_img/2508.12903.jpg', 'data': {'categories': ['#training', '#optimization'], 'emoji': '🔄', 'ru': {'title': 'Динамическое самосовершенствование LLM в реальном времени', 'desc': 'Статья представляет новый метод ProActive Self-Refinement (PASR) для улучшения выходных данных больших языковых моделей (LLM). PASR позволяет LLM динамически улучшать свои ответы в процессе генерации, в отличие от методов, которые регенерируют весь ответ целиком. Эксперименты показали, что PASR значительно повышает эффективность решения задач, сокращая потребление токенов на 41.6% и увеличивая точность на 8.2% для модели Qwen3-8B. Метод вдохновлен тем, как люди динамически улучшают свои мысли во время выполнения задач.'}, 'en': {'title': 'Dynamic Refinement for Efficient Language Generation', 'desc': "ProActive Self-Refinement (PASR) is a new method that improves the outputs of large language models (LLMs) by allowing them to refine their responses dynamically during generation. Unlike traditional methods that use a fixed number of iterations for refinement, PASR adapts to the model's internal state and the context of the task at hand. This proactive approach not only enhances the accuracy of the generated outputs but also significantly reduces the number of tokens used, leading to more efficient processing. Experiments show that PASR can improve problem-solving performance and reduce token consumption by over 40%."}, 'zh': {'title': '主动自我精炼：动态提升LLM输出的创新方法', 'desc': 'ProActive Self-Refinement (PASR) 是一种动态增强大型语言模型（LLM）输出的方法，能够在生成过程中减少令牌消耗并提高准确性。与传统的自我精炼方法不同，PASR 不依赖于固定的迭代次数，而是根据模型的内部状态和不断变化的上下文主动决定何时、如何进行精炼。通过在10个不同任务上进行广泛实验，结果表明PASR显著提升了问题解决能力，尤其是在Qwen3-8B模型上，平均令牌消耗减少了41.6%，准确性提高了8.2%。该方法展示了在生成过程中动态调整输出的潜力，推动了自我精炼技术的发展。'}}}, {'id': 'https://huggingface.co/papers/2508.12669', 'title': 'Leveraging Large Language Models for Predictive Analysis of Human Misery', 'url': 'https://huggingface.co/papers/2508.12669', 'abstract': 'LLMs predict misery scores from text using various prompting strategies, with few-shot approaches outperforming zero-shot, and a gamified framework assessing their adaptability in emotional reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t This study investigates the use of Large Language Models (LLMs) for predicting human-perceived misery scores from natural language descriptions of real-world scenarios. The task is framed as a regression problem, where the model assigns a scalar value from 0 to 100 to each input statement. We evaluate multiple prompting strategies, including zero-shot, fixed-context few-shot, and retrieval-based prompting using BERT sentence embeddings. Few-shot approaches consistently outperform zero-shot baselines, underscoring the value of contextual examples in affective prediction. To move beyond static evaluation, we introduce the "Misery Game Show", a novel gamified framework inspired by a television format. It tests LLMs through structured rounds involving ordinal comparison, binary classification, scalar estimation, and feedback-driven reasoning. This setup enables us to assess not only predictive accuracy but also the model\'s ability to adapt based on corrective feedback. The gamified evaluation highlights the broader potential of LLMs in dynamic emotional reasoning tasks beyond standard regression. Code and data link: https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub', 'score': 8, 'issue_id': 5440, 'pub_date': '2025-08-18', 'pub_date_card': {'ru': '18 августа', 'en': 'August 18', 'zh': '8月18日'}, 'hash': '967909e78c87e1ce', 'authors': ['Bishanka Seal', 'Rahul Seetharaman', 'Aman Bansal', 'Abhilash Nandy'], 'affiliations': ['Indian Institute of Technology Kharagpur, Kharagpur, West Bengal, IN', 'UMass Amherst, Amherst, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.12669.jpg', 'data': {'categories': ['#data', '#multimodal', '#reasoning', '#games', '#training'], 'emoji': '😢', 'ru': {'title': 'LLM как эмпаты: оценка способности AI понимать человеческие страдания', 'desc': "Это исследование оценивает способность больших языковых моделей (LLM) предсказывать уровень несчастья человека на основе текстовых описаний реальных ситуаций. Авторы сравнивают различные стратегии промптинга, включая zero-shot и few-shot подходы, а также применение ретривальных методов. Результаты показывают, что few-shot методы превосходят zero-shot базовые линии в задаче аффективного прогнозирования. Исследователи также представляют новую геймифицированную структуру оценки 'Misery Game Show' для тестирования адаптивности LLM в динамических задачах эмоционального рассуждения."}, 'en': {'title': 'Unlocking Emotional Insights: LLMs in Predicting Misery Scores', 'desc': "This paper explores how Large Language Models (LLMs) can predict misery scores from text descriptions. The authors treat this as a regression task, where the model assigns a score between 0 and 100 based on the emotional content of the text. They compare different prompting strategies, finding that few-shot learning methods are more effective than zero-shot methods, indicating that context helps improve predictions. Additionally, they introduce a gamified evaluation framework called the 'Misery Game Show' to assess LLMs' adaptability in emotional reasoning tasks, demonstrating their potential in dynamic scenarios."}, 'zh': {'title': '利用大型语言模型预测痛苦分数的创新方法', 'desc': '本研究探讨了大型语言模型（LLMs）如何从自然语言描述中预测人类感知的痛苦分数。该任务被视为回归问题，模型为每个输入语句分配一个从0到100的标量值。我们评估了多种提示策略，包括零样本、固定上下文的少样本和基于检索的提示，结果显示少样本方法在情感预测中优于零样本基线。为了超越静态评估，我们引入了“痛苦游戏秀”，这是一个新颖的游戏化框架，旨在测试LLMs在动态情感推理任务中的适应能力。'}}}, {'id': 'https://huggingface.co/papers/2508.10830', 'title': 'Advances in Speech Separation: Techniques, Challenges, and Future Trends', 'url': 'https://huggingface.co/papers/2508.10830', 'abstract': 'A survey of DNN-based speech separation techniques, covering learning paradigms, separation scenarios, and architectural components, with a focus on current advancements and promising future directions.  \t\t\t\t\tAI-generated summary \t\t\t\t The field of speech separation, addressing the "cocktail party problem", has seen revolutionary advances with DNNs. Speech separation enhances clarity in complex acoustic environments and serves as crucial pre-processing for speech recognition and speaker recognition. However, current literature focuses narrowly on specific architectures or isolated approaches, creating fragmented understanding. This survey addresses this gap by providing systematic examination of DNN-based speech separation techniques. Our work differentiates itself through: (I) Comprehensive perspective: We systematically investigate learning paradigms, separation scenarios with known/unknown speakers, comparative analysis of supervised/self-supervised/unsupervised frameworks, and architectural components from encoders to estimation strategies. (II) Timeliness: Coverage of cutting-edge developments ensures access to current innovations and benchmarks. (III) Unique insights: Beyond summarization, we evaluate technological trajectories, identify emerging patterns, and highlight promising directions including domain-robust frameworks, efficient architectures, multimodal integration, and novel self-supervised paradigms. (IV) Fair evaluation: We provide quantitative evaluations on standard datasets, revealing true capabilities and limitations of different methods. This comprehensive survey serves as an accessible reference for experienced researchers and newcomers navigating speech separation\'s complex landscape.', 'score': 8, 'issue_id': 5445, 'pub_date': '2025-08-14', 'pub_date_card': {'ru': '14 августа', 'en': 'August 14', 'zh': '8月14日'}, 'hash': '3e11115107c977b2', 'authors': ['Kai Li', 'Guo Chen', 'Wendi Sang', 'Yi Luo', 'Zhuo Chen', 'Shuai Wang', 'Shulin He', 'Zhong-Qiu Wang', 'Andong Li', 'Zhiyong Wu', 'Xiaolin Hu'], 'affiliations': ['ByteDance', 'Department of Computer Science and Technology, Tsinghua University, Beijing, China', 'Institute of Acoustics, Chinese Academy of Sciences, Beijing, China', 'Nanjing University, Suzhou, China', 'School of Computer Technology and Application, Qinghai University, Xining, China', 'Shenzhen International Graduate School, Tsinghua University, Shenzhen, China', 'Shenzhen, China', 'Southern University of Science and Technology, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.10830.jpg', 'data': {'categories': ['#benchmark', '#survey', '#multimodal', '#audio'], 'emoji': '🎙️', 'ru': {'title': 'Комплексный анализ DNN-методов разделения речи: от основ до инноваций', 'desc': 'Это обзор методов разделения речи на основе глубоких нейронных сетей (DNN). В статье рассматриваются парадигмы обучения, сценарии разделения и архитектурные компоненты, с акцентом на текущие достижения и перспективные направления. Авторы предоставляют систематический анализ методов, включая сравнение контролируемых, самоконтролируемых и неконтролируемых подходов. Особое внимание уделяется новейшим разработкам, оценке технологических траекторий и выявлению перспективных направлений, таких как мультимодальная интеграция и новые парадигмы самоконтролируемого обучения.'}, 'en': {'title': 'Revolutionizing Speech Clarity with DNNs', 'desc': "This paper surveys deep neural network (DNN)-based techniques for speech separation, which is essential for improving speech clarity in noisy environments. It addresses the 'cocktail party problem' by systematically examining various learning paradigms, separation scenarios, and architectural components. The authors highlight advancements in supervised, self-supervised, and unsupervised frameworks, providing a comprehensive analysis of current methods and their effectiveness. Additionally, the survey identifies future directions for research, including domain robustness and multimodal integration, making it a valuable resource for both new and experienced researchers in the field."}, 'zh': {'title': '深度学习助力语音分离技术的未来', 'desc': '本论文对基于深度神经网络（DNN）的语音分离技术进行了全面调查，涵盖了学习范式、分离场景和架构组件。语音分离技术旨在解决“鸡尾酒会问题”，提高复杂声学环境中的语音清晰度，并为语音识别和说话人识别提供重要的预处理。当前文献主要集中在特定架构或孤立方法上，导致理解的碎片化。我们的研究通过系统性分析和评估不同方法的能力与局限性，填补了这一空白，为研究人员提供了一个易于访问的参考。'}}}, {'id': 'https://huggingface.co/papers/2508.04324', 'title': 'TempFlow-GRPO: When Timing Matters for GRPO in Flow Models', 'url': 'https://huggingface.co/papers/2508.04324', 'abstract': 'TempFlow-GRPO enhances text-to-image generation by addressing temporal credit assignment and noise-aware optimization in flow models, improving human preference alignment and benchmark performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent flow matching models for text-to-image generation have achieved remarkable quality, yet their integration with reinforcement learning for human preference alignment remains suboptimal, hindering fine-grained reward-based optimization. We observe that the key impediment to effective GRPO training of flow models is the temporal uniformity assumption in existing approaches: sparse terminal rewards with uniform credit assignment fail to capture the varying criticality of decisions across generation timesteps, resulting in inefficient exploration and suboptimal convergence. To remedy this shortcoming, we introduce TempFlow-GRPO (Temporal Flow GRPO), a principled GRPO framework that captures and exploits the temporal structure inherent in flow-based generation. TempFlow-GRPO introduces two key innovations: (i) a trajectory branching mechanism that provides process rewards by concentrating stochasticity at designated branching points, enabling precise credit assignment without requiring specialized intermediate reward models; and (ii) a noise-aware weighting scheme that modulates policy optimization according to the intrinsic exploration potential of each timestep, prioritizing learning during high-impact early stages while ensuring stable refinement in later phases. These innovations endow the model with temporally-aware optimization that respects the underlying generative dynamics, leading to state-of-the-art performance in human preference alignment and standard text-to-image benchmarks.', 'score': 6, 'issue_id': 5444, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '9c2d3a6bed55968e', 'authors': ['Xiaoxuan He', 'Siming Fu', 'Yuke Zhao', 'Wanli Li', 'Jian Yang', 'Dacheng Yin', 'Fengyun Rao', 'Bo Zhang'], 'affiliations': ['WeChat Vision, Tencent Inc', 'ZheJiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.04324.jpg', 'data': {'categories': ['#rlhf', '#rl', '#cv', '#benchmark', '#alignment', '#optimization'], 'emoji': '🖼️', 'ru': {'title': 'Темпоральная оптимизация для улучшения генерации изображений', 'desc': 'Статья представляет TempFlow-GRPO - новый метод для улучшения генерации изображений по текстовому описанию. Данный подход решает проблему временного распределения вознаграждений и оптимизации с учетом шума в моделях потоков. TempFlow-GRPO вводит механизм ветвления траекторий для более точного назначения вознаграждений и схему взвешивания с учетом шума для оптимизации политики. Это позволяет достичь лучших результатов в согласовании с предпочтениями человека и стандартных бенчмарках генерации изображений по тексту.'}, 'en': {'title': 'Enhancing Text-to-Image Generation with Temporal Awareness', 'desc': 'TempFlow-GRPO is a novel framework that enhances text-to-image generation by improving how models learn from rewards over time. It addresses the limitations of existing methods that treat all decisions equally, which can lead to inefficient learning. By introducing a trajectory branching mechanism, it allows for better credit assignment at critical points in the generation process. Additionally, a noise-aware weighting scheme helps the model focus on important early decisions while maintaining stability in later stages, resulting in better alignment with human preferences and improved performance on benchmarks.'}, 'zh': {'title': '提升文本到图像生成的时间感知优化', 'desc': 'TempFlow-GRPO是一种改进的文本到图像生成模型，旨在解决时间信用分配和噪声感知优化的问题。该模型通过引入轨迹分支机制和噪声感知加权方案，能够更有效地进行奖励分配和策略优化。这样，模型在生成过程中能够更好地捕捉决策的重要性，从而提高人类偏好的对齐效果。最终，TempFlow-GRPO在标准文本到图像基准测试中表现出色，达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2508.12845', 'title': 'CAMAR: Continuous Actions Multi-Agent Routing', 'url': 'https://huggingface.co/papers/2508.12845', 'abstract': 'CAMAR is a new MARL benchmark for continuous action pathfinding that supports efficient evaluation and hybrid approaches with classical planning methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-agent reinforcement learning (MARL) is a powerful paradigm for solving cooperative and competitive decision-making problems. While many MARL benchmarks have been proposed, few combine continuous state and action spaces with challenging coordination and planning tasks. We introduce CAMAR, a new MARL benchmark designed explicitly for multi-agent pathfinding in environments with continuous actions. CAMAR supports cooperative and competitive interactions between agents and runs efficiently at up to 100,000 environment steps per second. We also propose a three-tier evaluation protocol to better track algorithmic progress and enable deeper analysis of performance. In addition, CAMAR allows the integration of classical planning methods such as RRT and RRT* into MARL pipelines. We use them as standalone baselines and combine RRT* with popular MARL algorithms to create hybrid approaches. We provide a suite of test scenarios and benchmarking tools to ensure reproducibility and fair comparison. Experiments show that CAMAR presents a challenging and realistic testbed for the MARL community.', 'score': 5, 'issue_id': 5450, 'pub_date': '2025-08-18', 'pub_date_card': {'ru': '18 августа', 'en': 'August 18', 'zh': '8月18日'}, 'hash': 'e6b65afc6b9d4acc', 'authors': ['Artem Pshenitsyn', 'Aleksandr Panov', 'Alexey Skrynnik'], 'affiliations': ['AIRI, Moscow, Russia', 'MIPT, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2508.12845.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#rl', '#games'], 'emoji': '🗺️', 'ru': {'title': 'CAMAR: Новый рубеж в мультиагентном обучении для непрерывного поиска пути', 'desc': 'CAMAR - это новый бенчмарк для мультиагентного обучения с подкреплением (MARL) в задачах непрерывного поиска пути. Он поддерживает эффективную оценку и гибридные подходы с классическими методами планирования. CAMAR обеспечивает кооперативные и конкурентные взаимодействия между агентами в непрерывных пространствах состояний и действий. Бенчмарк включает набор тестовых сценариев и инструментов для обеспечения воспроизводимости и корректного сравнения алгоритмов.'}, 'en': {'title': 'CAMAR: Advancing Multi-Agent Pathfinding with Continuous Actions', 'desc': 'CAMAR is a new benchmark for multi-agent reinforcement learning (MARL) that focuses on continuous action pathfinding. It allows agents to work together or compete in environments where they must navigate using continuous actions. The benchmark runs efficiently, supporting up to 100,000 environment steps per second, and includes a three-tier evaluation protocol for tracking progress. Additionally, CAMAR integrates classical planning methods like RRT and RRT* with MARL, providing a comprehensive suite of scenarios for testing and comparison.'}, 'zh': {'title': 'CAMAR：多智能体路径规划的新基准', 'desc': 'CAMAR是一个新的多智能体强化学习（MARL）基准，专注于连续动作的路径规划。它支持智能体之间的合作与竞争，并且能够高效运行，每秒可处理多达100,000个环境步骤。该基准引入了三层评估协议，以便更好地跟踪算法进展并深入分析性能。此外，CAMAR还允许将经典规划方法（如RRT和RRT*）与MARL管道结合，提供了测试场景和基准工具，以确保可重复性和公平比较。'}}}, {'id': 'https://huggingface.co/papers/2508.11548', 'title': 'Copyright Protection for Large Language Models: A Survey of Methods,\n  Challenges, and Trends', 'url': 'https://huggingface.co/papers/2508.11548', 'abstract': 'A survey of LLM copyright protection technologies, focusing on model fingerprinting, clarifies its relationship with text watermarking and provides a comprehensive overview of fingerprinting techniques, evaluation metrics, and open challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t Copyright protection for large language models is of critical importance, given their substantial development costs, proprietary value, and potential for misuse. Existing surveys have predominantly focused on techniques for tracing LLM-generated content-namely, text watermarking-while a systematic exploration of methods for protecting the models themselves (i.e., model watermarking and model fingerprinting) remains absent. Moreover, the relationships and distinctions among text watermarking, model watermarking, and model fingerprinting have not been comprehensively clarified. This work presents a comprehensive survey of the current state of LLM copyright protection technologies, with a focus on model fingerprinting, covering the following aspects: (1) clarifying the conceptual connection from text watermarking to model watermarking and fingerprinting, and adopting a unified terminology that incorporates model watermarking into the broader fingerprinting framework; (2) providing an overview and comparison of diverse text watermarking techniques, highlighting cases where such methods can function as model fingerprinting; (3) systematically categorizing and comparing existing model fingerprinting approaches for LLM copyright protection; (4) presenting, for the first time, techniques for fingerprint transfer and fingerprint removal; (5) summarizing evaluation metrics for model fingerprints, including effectiveness, harmlessness, robustness, stealthiness, and reliability; and (6) discussing open challenges and future research directions. This survey aims to offer researchers a thorough understanding of both text watermarking and model fingerprinting technologies in the era of LLMs, thereby fostering further advances in protecting their intellectual property.', 'score': 5, 'issue_id': 5445, 'pub_date': '2025-08-15', 'pub_date_card': {'ru': '15 августа', 'en': 'August 15', 'zh': '8月15日'}, 'hash': '8ef1270588d83b01', 'authors': ['Zhenhua Xu', 'Xubin Yue', 'Zhebo Wang', 'Qichen Liu', 'Xixiang Zhao', 'Jingxuan Zhang', 'Wenjun Zeng', 'Wengpeng Xing', 'Dezhang Kong', 'Changting Lin', 'Meng Han'], 'affiliations': ['GenTel.io', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.11548.jpg', 'data': {'categories': ['#multimodal', '#ethics', '#data', '#dataset', '#benchmark', '#survey'], 'emoji': '🔐', 'ru': {'title': 'Защита интеллектуальной собственности в эпоху больших языковых моделей', 'desc': 'Статья представляет собой обзор технологий защиты авторских прав для больших языковых моделей (LLM), фокусируясь на методе отпечатков моделей. Авторы проясняют связь между водяными знаками текста и отпечатками моделей, предоставляя комплексный обзор техник, метрик оценки и открытых проблем в этой области. Исследование систематизирует различные подходы к созданию отпечатков моделей для защиты авторских прав LLM, включая методы переноса и удаления отпечатков. Статья также обсуждает критерии оценки отпечатков моделей, такие как эффективность, безвредность, устойчивость, скрытность и надежность.'}, 'en': {'title': 'Protecting LLMs: Unifying Fingerprinting and Watermarking Techniques', 'desc': 'This paper surveys copyright protection technologies for large language models (LLMs), emphasizing model fingerprinting. It clarifies the connections between text watermarking, model watermarking, and model fingerprinting, proposing a unified terminology. The study categorizes various fingerprinting techniques and evaluation metrics, addressing their effectiveness and robustness. Additionally, it highlights open challenges and future research directions in the field of LLM copyright protection.'}, 'zh': {'title': '保护大型语言模型的版权技术探索', 'desc': '这篇论文调查了大型语言模型（LLM）的版权保护技术，特别关注模型指纹技术。它阐明了文本水印与模型水印和模型指纹之间的关系，并提供了指纹技术的全面概述。论文还系统分类和比较了现有的模型指纹方法，并首次介绍了指纹转移和指纹去除技术。最后，讨论了评估模型指纹的指标以及未来研究的挑战和方向。'}}}, {'id': 'https://huggingface.co/papers/2508.09789', 'title': 'Describe What You See with Multimodal Large Language Models to Enhance\n  Video Recommendations', 'url': 'https://huggingface.co/papers/2508.09789', 'abstract': 'A zero-finetuning framework uses Multimodal Large Language Models to inject high-level semantics into video recommendations, improving intent-awareness over traditional methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing video recommender systems rely primarily on user-defined metadata or on low-level visual and acoustic signals extracted by specialised encoders. These low-level features describe what appears on the screen but miss deeper semantics such as intent, humour, and world knowledge that make clips resonate with viewers. For example, is a 30-second clip simply a singer on a rooftop, or an ironic parody filmed amid the fairy chimneys of Cappadocia, Turkey? Such distinctions are critical to personalised recommendations yet remain invisible to traditional encoding pipelines. In this paper, we introduce a simple, recommendation system-agnostic zero-finetuning framework that injects high-level semantics into the recommendation pipeline by prompting an off-the-shelf Multimodal Large Language Model (MLLM) to summarise each clip into a rich natural-language description (e.g. "a superhero parody with slapstick fights and orchestral stabs"), bridging the gap between raw content and user intent. We use MLLM output with a state-of-the-art text encoder and feed it into standard collaborative, content-based, and generative recommenders. On the MicroLens-100K dataset, which emulates user interactions with TikTok-style videos, our framework consistently surpasses conventional video, audio, and metadata features in five representative models. Our findings highlight the promise of leveraging MLLMs as on-the-fly knowledge extractors to build more intent-aware video recommenders.', 'score': 4, 'issue_id': 5445, 'pub_date': '2025-08-13', 'pub_date_card': {'ru': '13 августа', 'en': 'August 13', 'zh': '8月13日'}, 'hash': '5db21d3ece5d2752', 'authors': ['Marco De Nadai', 'Andreas Damianou', 'Mounia Lalmas'], 'affiliations': ['Spotify Denmark', 'Spotify United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2508.09789.jpg', 'data': {'categories': ['#multimodal', '#games', '#video', '#dataset', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'MLLM для интеллектуальных видеорекомендаций без дообучения', 'desc': 'В статье представлен новый подход к рекомендательным системам для видео, использующий мультимодальные языковые модели (MLLM) для извлечения высокоуровневой семантики из клипов. Это позволяет учитывать такие аспекты, как намерение, юмор и общие знания, которые обычно недоступны традиционным методам. Предложенная система не требует дополнительного обучения и может использоваться с существующими рекомендательными алгоритмами. Эксперименты на датасете MicroLens-100K показали превосходство этого подхода над стандартными методами, основанными на метаданных и низкоуровневых признаках.'}, 'en': {'title': 'Enhancing Video Recommendations with High-Level Semantics', 'desc': 'This paper presents a novel framework that enhances video recommendation systems by incorporating high-level semantics using Multimodal Large Language Models (MLLMs). Traditional methods often rely on basic visual and audio features, which fail to capture deeper meanings like intent and humor. The proposed framework generates rich natural-language descriptions of video clips, allowing for a better understanding of user intent. By integrating these descriptions into existing recommendation models, the system significantly improves performance on user interaction datasets, demonstrating the effectiveness of MLLMs in creating more personalized recommendations.'}, 'zh': {'title': '利用多模态语言模型提升视频推荐的意图感知', 'desc': '本文提出了一种零微调框架，利用多模态大型语言模型（MLLM）为视频推荐系统注入高层语义，从而提高对用户意图的理解。传统的视频推荐系统主要依赖用户定义的元数据或低层次的视觉和音频信号，无法捕捉到意图、幽默和世界知识等深层语义。通过让MLLM对每个视频片段进行自然语言描述，本文架起了原始内容与用户意图之间的桥梁。实验结果表明，该框架在多个推荐模型中均优于传统的视频、音频和元数据特征，展示了MLLM作为知识提取工具的潜力。'}}}, {'id': 'https://huggingface.co/papers/2508.13992', 'title': 'MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic\n  Evaluation of Audio General Intelligence', 'url': 'https://huggingface.co/papers/2508.13992', 'abstract': 'MMAU-Pro is a comprehensive benchmark for evaluating audio intelligence in AI systems, assessing 49 unique skills across speech, sound, music, and their combinations, revealing significant limitations in current models.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio comprehension-including speech, non-speech sounds, and music-is essential for achieving human-level intelligence. Consequently, AI agents must demonstrate holistic audio understanding to qualify as generally intelligent. However, evaluating auditory intelligence comprehensively remains challenging. To address this gap, we introduce MMAU-Pro, the most comprehensive and rigorously curated benchmark for assessing audio intelligence in AI systems. MMAU-Pro contains 5,305 instances, where each instance has one or more audios paired with human expert-generated question-answer pairs, spanning speech, sound, music, and their combinations. Unlike existing benchmarks, MMAU-Pro evaluates auditory intelligence across 49 unique skills and multiple complex dimensions, including long-form audio comprehension, spatial audio reasoning, multi-audio understanding, among others. All questions are meticulously designed to require deliberate multi-hop reasoning, including both multiple-choice and open-ended response formats. Importantly, audio data is sourced directly ``from the wild" rather than from existing datasets with known distributions. We evaluate 22 leading open-source and proprietary multimodal AI models, revealing significant limitations: even state-of-the-art models such as Gemini 2.5 Flash and Audio Flamingo 3 achieve only 59.2% and 51.7% accuracy, respectively, approaching random performance in multiple categories. Our extensive analysis highlights specific shortcomings and provides novel insights, offering actionable perspectives for the community to enhance future AI systems\' progression toward audio general intelligence. The benchmark and code is available at https://sonalkum.github.io/mmau-pro.', 'score': 3, 'issue_id': 5440, 'pub_date': '2025-08-19', 'pub_date_card': {'ru': '19 августа', 'en': 'August 19', 'zh': '8月19日'}, 'hash': '7815d4c4cb2d67a6', 'authors': ['Sonal Kumar', 'Šimon Sedláček', 'Vaibhavi Lokegaonkar', 'Fernando López', 'Wenyi Yu', 'Nishit Anand', 'Hyeonggon Ryu', 'Lichang Chen', 'Maxim Plička', 'Miroslav Hlaváček', 'William Fineas Ellingwood', 'Sathvik Udupa', 'Siyuan Hou', 'Allison Ferner', 'Sara Barahona', 'Cecilia Bolaños', 'Satish Rahi', 'Laura Herrera-Alarcón', 'Satvik Dixit', 'Siddhi Patil', 'Soham Deshmukh', 'Lasha Koroshinadze', 'Yao Liu', 'Leibny Paola Garcia Perera', 'Eleni Zanou', 'Themos Stafylakis', 'Joon Son Chung', 'David Harwath', 'Chao Zhang', 'Dinesh Manocha', 'Alicia Lozano-Diez', 'Santosh Kesiraju', 'Sreyan Ghosh', 'Ramani Duraiswami'], 'affiliations': ['Athens University of Economics and Business', 'Brno University of Technology, Czech Republic', 'Carnegie Mellon University, USA', 'Indian Institute of Technology, Bombay', 'Johns Hopkins University, USA', 'KAIST, Daejeon', 'Microsoft', 'Middlebury College, USA', 'Phonexia', 'Shanghai Artificial Intelligence Laboratory', 'Telefonica', 'Tsinghua University', 'Tufts University', 'Universidad Autonoma de Madrid', 'Universidad de Buenos Aires', 'Universiti Sains Malaysia', 'University of Maryland, College Park, USA', 'University of Texas, Austin, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.13992.jpg', 'data': {'categories': ['#audio', '#multimodal', '#benchmark', '#reasoning', '#agi', '#open_source'], 'emoji': '🎧', 'ru': {'title': 'MMAU-Pro: новый стандарт оценки аудиоинтеллекта ИИ', 'desc': 'MMAU-Pro - это комплексный бенчмарк для оценки аудиоинтеллекта в системах искусственного интеллекта. Он оценивает 49 уникальных навыков в области речи, звука, музыки и их комбинаций, используя 5305 примеров с вопросами и ответами экспертов. Бенчмарк выявил значительные ограничения даже у современных моделей, таких как Gemini 2.5 Flash и Audio Flamingo 3, которые достигли точности только 59.2% и 51.7% соответственно. MMAU-Pro предоставляет ценные insights для развития систем ИИ в направлении общего аудиоинтеллекта.'}, 'en': {'title': 'MMAU-Pro: Elevating Audio Intelligence Evaluation in AI', 'desc': 'MMAU-Pro is a new benchmark designed to evaluate audio intelligence in AI systems, focusing on 49 distinct skills related to speech, sound, and music. It includes 5,305 audio instances paired with expert-generated questions that require complex reasoning to answer. The benchmark reveals that even advanced AI models struggle with audio comprehension, achieving only moderate accuracy. This comprehensive evaluation aims to identify weaknesses in current models and guide improvements towards achieving human-level audio understanding.'}, 'zh': {'title': '音频智能评估的新标准', 'desc': 'MMAU-Pro是一个全面的基准，用于评估人工智能系统的音频智能，涵盖了49种独特的技能，包括语音、声音、音乐及其组合。该基准包含5305个实例，每个实例配有专家生成的问题和答案，旨在测试AI在音频理解方面的能力。通过对22个领先的多模态AI模型进行评估，发现即使是最先进的模型在多个类别中的准确率也仅为59.2%和51.7%，显示出显著的局限性。MMAU-Pro为未来AI系统的音频通用智能发展提供了重要的见解和改进方向。'}}}, {'id': 'https://huggingface.co/papers/2508.13186', 'title': 'MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents', 'url': 'https://huggingface.co/papers/2508.13186', 'abstract': "A new benchmark, MM-BrowseComp, evaluates AI agents' multimodal retrieval and reasoning capabilities, revealing limitations in current models' handling of images and videos in web browsing tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t AI agents with advanced reasoning and tool use capabilities have demonstrated impressive performance in web browsing for deep search. While existing benchmarks such as BrowseComp evaluate these browsing abilities, they primarily focus on textual information, overlooking the prevalence of multimodal content. To bridge this gap, we introduce MM-BrowseComp, a novel benchmark comprising 224 challenging, hand-crafted questions specifically designed to assess agents' multimodal retrieval and reasoning capabilities. These questions often incorporate images in prompts, and crucial information encountered during the search and reasoning process may also be embedded within images or videos on webpages. Consequently, methods relying solely on text prove insufficient for our benchmark. Additionally, we provide a verified checklist for each question, enabling fine-grained analysis of multimodal dependencies and reasoning paths. Our comprehensive evaluation of state-of-the-art models on MM-BrowseComp reveals that even top models like OpenAI o3 with tools achieve only 29.02\\% accuracy, highlighting the suboptimal multimodal capabilities and lack of native multimodal reasoning in current models.", 'score': 3, 'issue_id': 5449, 'pub_date': '2025-08-14', 'pub_date_card': {'ru': '14 августа', 'en': 'August 14', 'zh': '8月14日'}, 'hash': '7d59008f46305fdc', 'authors': ['Shilong Li', 'Xingyuan Bu', 'Wenjie Wang', 'Jiaheng Liu', 'Jun Dong', 'Haoyang He', 'Hao Lu', 'Haozhe Zhang', 'Chenchen Jing', 'Zhen Li', 'Chuanhao Li', 'Jiayi Tian', 'Chenchen Zhang', 'Tianhao Peng', 'Yancheng He', 'Jihao Gu', 'Yuanxing Zhang', 'Jian Yang', 'Ge Zhang', 'Wenhao Huang', 'Wangchunshu Zhou', 'Zhaoxiang Zhang', 'Ruizhe Ding', 'Shilei Wen'], 'affiliations': ['ByteDance', 'CASIA', 'M-A-P', 'Nanjing University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.13186.jpg', 'data': {'categories': ['#reasoning', '#agents', '#benchmark', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Мультимодальный вызов для ИИ: новый рубеж в веб-навигации', 'desc': 'MM-BrowseComp - это новый бенчмарк для оценки способностей ИИ-агентов в мультимодальном поиске и рассуждениях. Он состоит из 224 сложных вопросов, требующих анализа изображений и видео в задачах веб-браузинга. Бенчмарк выявил ограничения существующих моделей в обработке мультимодального контента. Даже лучшие модели, такие как OpenAI o3, показали точность всего 29.02% на этом тесте.'}, 'en': {'title': 'Bridging the Gap in Multimodal AI Reasoning', 'desc': "The paper introduces MM-BrowseComp, a new benchmark designed to evaluate AI agents' abilities in multimodal retrieval and reasoning, particularly in web browsing tasks. Unlike previous benchmarks that focus mainly on text, MM-BrowseComp includes 224 carefully crafted questions that require understanding and processing images and videos. The study reveals that current state-of-the-art models, including OpenAI o3, struggle with these multimodal tasks, achieving only 29.02% accuracy. This highlights a significant gap in the models' capabilities, emphasizing the need for improved multimodal reasoning in AI systems."}, 'zh': {'title': '多模态检索新基准，揭示AI模型的局限性', 'desc': '本文介绍了一个新的基准测试MM-BrowseComp，旨在评估人工智能代理在多模态检索和推理方面的能力。现有的基准测试主要关注文本信息，忽视了图像和视频在网页浏览任务中的重要性。MM-BrowseComp包含224个精心设计的问题，专门用于测试代理的多模态检索和推理能力。我们的评估显示，即使是最先进的模型在该基准上的准确率也仅为29.02%，表明当前模型在多模态处理和推理方面存在明显不足。'}}}, {'id': 'https://huggingface.co/papers/2508.13139', 'title': 'Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence', 'url': 'https://huggingface.co/papers/2508.13139', 'abstract': 'Motion2Motion is a training-free framework that efficiently transfers animations between characters with different skeletal topologies using sparse bone correspondences.  \t\t\t\t\tAI-generated summary \t\t\t\t This work studies the challenge of transfer animations between characters whose skeletal topologies differ substantially. While many techniques have advanced retargeting techniques in decades, transfer motions across diverse topologies remains less-explored. The primary obstacle lies in the inherent topological inconsistency between source and target skeletons, which restricts the establishment of straightforward one-to-one bone correspondences. Besides, the current lack of large-scale paired motion datasets spanning different topological structures severely constrains the development of data-driven approaches. To address these limitations, we introduce Motion2Motion, a novel, training-free framework. Simply yet effectively, Motion2Motion works with only one or a few example motions on the target skeleton, by accessing a sparse set of bone correspondences between the source and target skeletons. Through comprehensive qualitative and quantitative evaluations, we demonstrate that Motion2Motion achieves efficient and reliable performance in both similar-skeleton and cross-species skeleton transfer scenarios. The practical utility of our approach is further evidenced by its successful integration in downstream applications and user interfaces, highlighting its potential for industrial applications. Code and data are available at https://lhchen.top/Motion2Motion.', 'score': 2, 'issue_id': 5442, 'pub_date': '2025-08-18', 'pub_date_card': {'ru': '18 августа', 'en': 'August 18', 'zh': '8月18日'}, 'hash': '799a57dcfacaa430', 'authors': ['Ling-Hao Chen', 'Yuhong Zhang', 'Zixin Yin', 'Zhiyang Dou', 'Xin Chen', 'Jingbo Wang', 'Taku Komura', 'Lei Zhang'], 'affiliations': ['ByteDance, United States of America', 'International Digital Economy Academy, China', 'Shanghai Artificial Intelligence Laboratory, China', 'The Hong Kong University of Science and Technology, China', 'The University of Hong Kong, China', 'Tsinghua University, China', 'Tsinghua University, International Digital Economy Academy, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.13139.jpg', 'data': {'categories': ['#dataset', '#3d', '#transfer_learning'], 'emoji': '🦴', 'ru': {'title': 'Универсальный перенос анимации без обучения', 'desc': 'Motion2Motion - это система для переноса анимации между персонажами с разной скелетной топологией, не требующая обучения. Она использует разреженные соответствия между костями для эффективного переноса движений даже между сильно различающимися скелетами. Система работает с небольшим количеством примеров движений целевого скелета. Эксперименты показали надежную работу как для похожих скелетов, так и для переноса между разными видами.'}, 'en': {'title': 'Effortless Animation Transfer Across Diverse Skeletons', 'desc': 'Motion2Motion is a novel framework designed to transfer animations between characters with different skeletal structures without the need for extensive training. It addresses the challenge of establishing bone correspondences when the source and target skeletons have significant topological differences. By utilizing a sparse set of correspondences and requiring only a few example motions, Motion2Motion efficiently facilitates motion transfer. The framework has been evaluated rigorously, showing strong performance in both similar and diverse skeletal scenarios, making it suitable for practical applications in the industry.'}, 'zh': {'title': '高效动画转移，跨越骨骼拓扑的界限', 'desc': 'Motion2Motion是一个无需训练的框架，能够高效地在不同骨骼拓扑的角色之间转移动画。该方法解决了源骨骼和目标骨骼之间的拓扑不一致性问题，利用稀疏的骨骼对应关系进行动画转移。尽管现有的重定向技术已经取得了一定进展，但在多样化拓扑结构之间的运动转移仍然较少被探索。通过定性和定量评估，我们证明了Motion2Motion在相似骨骼和跨物种骨骼转移场景中都能实现高效可靠的性能。'}}}, {'id': 'https://huggingface.co/papers/2508.12535', 'title': 'CorrSteer: Steering Improves Task Performance and Safety in LLMs through\n  Correlation-based Sparse Autoencoder Feature Selection', 'url': 'https://huggingface.co/papers/2508.12535', 'abstract': "CorrSteer improves language model performance by selecting relevant features through correlation with SAE activations at inference time, enhancing tasks like QA, bias mitigation, and reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Sparse Autoencoders (SAEs) can extract interpretable features from large language models (LLMs) without supervision. However, their effectiveness in downstream steering tasks is limited by the requirement for contrastive datasets or large activation storage. To address these limitations, we propose CorrSteer, which selects features by correlating sample correctness with SAE activations from generated tokens at inference time. This approach uses only inference-time activations to extract more relevant features, thereby avoiding spurious correlations. It also obtains steering coefficients from average activations, automating the entire pipeline. Our method shows improved task performance on QA, bias mitigation, jailbreaking prevention, and reasoning benchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1% improvement in MMLU performance and a +22.9% improvement in HarmBench with only 4000 samples. Selected features demonstrate semantically meaningful patterns aligned with each task's requirements, revealing the underlying capabilities that drive performance. Our work establishes correlationbased selection as an effective and scalable approach for automated SAE steering across language model applications.", 'score': 2, 'issue_id': 5445, 'pub_date': '2025-08-18', 'pub_date_card': {'ru': '18 августа', 'en': 'August 18', 'zh': '8月18日'}, 'hash': '130f12bf18369dc9', 'authors': ['Seonglae Cho', 'Zekun Wu', 'Adriano Koshiyama'], 'affiliations': ['Holistic AI', 'University College London'], 'pdf_title_img': 'assets/pdf/title_img/2508.12535.jpg', 'data': {'categories': ['#ethics', '#data', '#interpretability', '#training', '#benchmark', '#architecture', '#reasoning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'CorrSteer: Автоматическое улучшение ИИ через корреляционный анализ', 'desc': 'CorrSteer - это новый метод улучшения работы языковых моделей, использующий корреляцию между правильностью ответов и активациями разреженных автоэнкодеров (SAE) во время вывода. Этот подход позволяет автоматически выбирать релевантные признаки без необходимости в контрастных наборах данных или хранении больших объемов активаций. CorrSteer показывает улучшение производительности в задачах вопросно-ответных систем, снижения предвзятости, предотвращения обхода ограничений и рассуждений на моделях Gemma 2 2B и LLaMA 3.1 8B. Выбранные признаки демонстрируют семантически значимые паттерны, соответствующие требованиям каждой задачи.'}, 'en': {'title': 'Enhancing Language Models with Correlation-Based Feature Selection', 'desc': 'CorrSteer is a method that enhances the performance of language models by selecting relevant features based on their correlation with Sparse Autoencoder (SAE) activations during inference. This technique allows for the extraction of interpretable features without needing large datasets or extensive storage of activations. By focusing on the relationship between sample correctness and SAE activations, CorrSteer avoids misleading correlations and automates the feature selection process. The results show significant improvements in various tasks, including question answering and bias mitigation, demonstrating the effectiveness of correlation-based feature selection in language model applications.'}, 'zh': {'title': 'CorrSteer：通过相关性选择特征，提升语言模型性能', 'desc': 'CorrSteer是一种通过与稀疏自编码器(SAE)激活的相关性来选择特征的方法，从而提高语言模型的性能。该方法在推理时仅使用激活信息，避免了虚假相关性，并自动化了整个特征选择过程。通过这种方式，CorrSteer在问答、偏见缓解和推理等任务上表现出显著的性能提升。我们的研究表明，基于相关性的特征选择是一种有效且可扩展的自动化SAE引导方法。'}}}, {'id': 'https://huggingface.co/papers/2508.11032', 'title': 'MedSAMix: A Training-Free Model Merging Approach for Medical Image\n  Segmentation', 'url': 'https://huggingface.co/papers/2508.11032', 'abstract': 'MedSAMix, a training-free model merging method, combines generalist and specialist models to improve medical image segmentation performance across diverse tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Universal medical image segmentation models have emerged as a promising paradigm due to their strong generalizability across diverse tasks, showing great potential for a wide range of clinical applications. This potential has been partly driven by the success of general-purpose vision models such as the Segment Anything Model (SAM), which has inspired the development of various fine-tuned variants for medical segmentation tasks. However, fine-tuned variants like MedSAM are trained on comparatively limited medical imaging data that often suffers from heterogeneity, scarce annotations, and distributional shifts. These challenges limit their ability to generalize across a wide range of medical segmentation tasks. In this regard, we propose MedSAMix, a training-free model merging method that integrates the strengths of both generalist models (e.g., SAM) and specialist models (e.g., MedSAM) for medical image segmentation. In contrast to traditional model merging approaches that rely on manual configuration and often result in suboptimal outcomes, we propose a zero-order optimization method to automatically discover optimal layer-wise merging solutions. Furthermore, for clinical applications, we develop two regimes to meet the demand of domain-specificity and generalizability in different scenarios by single-task optimization and multi-objective optimization respectively. Extensive evaluations on 25 medical segmentation tasks demonstrate that MedSAMix effectively mitigates model bias and consistently improves performance in both domain-specific accuracy and generalization, achieving improvements of 6.67% on specialized tasks and 4.37% on multi-task evaluations.', 'score': 2, 'issue_id': 5446, 'pub_date': '2025-08-14', 'pub_date_card': {'ru': '14 августа', 'en': 'August 14', 'zh': '8月14日'}, 'hash': '0f7678c5d41fad1f', 'authors': ['Yanwu Yang', 'Guinan Su', 'Jiesi Hu', 'Francesco Sammarco', 'Jonas Geiping', 'Thomas Wolfers'], 'affiliations': ['ELLIS Institute Tubingen, Germany', 'German Center for Mental Health (DZPG), partner site, Jena & Tubingen, Germany', 'Harbin Institute of Technology, Shenzhen, China', 'Max Planck Institute for Intelligent Systems, Germany', 'Peng Cheng Laboratory, Shenzhen, China', 'Tubingen AI Center, Germany', 'University of Tubingen, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2508.11032.jpg', 'data': {'categories': ['#training', '#optimization', '#healthcare', '#games', '#cv'], 'emoji': '🩺', 'ru': {'title': 'MedSAMix: Умное объединение моделей для универсальной сегментации медицинских изображений', 'desc': 'MedSAMix - это метод объединения моделей без дополнительного обучения, который сочетает универсальные и специализированные модели для улучшения сегментации медицинских изображений. Он использует оптимизацию нулевого порядка для автоматического нахождения оптимальных решений для послойного объединения моделей. MedSAMix предлагает два режима для удовлетворения потребностей в специфичности домена и обобщаемости в различных сценариях. Эксперименты на 25 задачах медицинской сегментации показали, что MedSAMix эффективно снижает смещение модели и улучшает производительность как в точности для конкретной области, так и в обобщении.'}, 'en': {'title': 'Merging Models for Superior Medical Image Segmentation', 'desc': 'MedSAMix is a novel approach that combines generalist and specialist models to enhance medical image segmentation without the need for additional training. It leverages the strengths of existing models like the Segment Anything Model (SAM) and fine-tuned variants such as MedSAM, addressing challenges like limited data and distributional shifts. By employing a zero-order optimization method, MedSAMix automatically determines the best way to merge model layers, leading to improved performance across various medical tasks. Evaluations show that this method significantly reduces model bias and boosts accuracy in both specialized and multi-task scenarios.'}, 'zh': {'title': 'MedSAMix：提升医疗图像分割的新方法', 'desc': 'MedSAMix是一种无需训练的模型合并方法，旨在结合通用模型和专业模型的优点，以提高医疗图像分割的性能。该方法通过零阶优化技术，自动发现最佳的层级合并方案，避免了传统手动配置的不足。MedSAMix在单任务优化和多目标优化中提供了两种方案，以满足不同场景下对领域特异性和通用性的需求。经过对25个医疗分割任务的广泛评估，MedSAMix有效减少了模型偏差，并在领域特定准确性和通用性方面持续提升，分别提高了6.67%和4.37%的性能。'}}}, {'id': 'https://huggingface.co/papers/2508.10478', 'title': 'Semantic IDs for Joint Generative Search and Recommendation', 'url': 'https://huggingface.co/papers/2508.10478', 'abstract': 'A bi-encoder model fine-tuned on both search and recommendation tasks provides effective Semantic IDs for a unified generative recommender system.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative models powered by Large Language Models (LLMs) are emerging as a unified solution for powering both recommendation and search tasks. A key design choice in these models is how to represent items, traditionally through unique identifiers (IDs) and more recently with Semantic IDs composed of discrete codes, obtained from embeddings. While task-specific embedding models can improve performance for individual tasks, they may not generalize well in a joint setting. In this paper, we explore how to construct Semantic IDs that perform well both in search and recommendation when using a unified model. We compare a range of strategies to construct Semantic IDs, looking into task-specific and cross-tasks approaches, and also whether each task should have its own semantic ID tokens in a joint search and recommendation generative model. Our results show that using a bi-encoder model fine-tuned on both search and recommendation tasks to obtain item embeddings, followed by the construction of a unified Semantic ID space provides an effective trade-off, enabling strong performance in both tasks. We hope these findings spark follow-up work on generalisable, semantically grounded ID schemes and inform the next wave of unified generative recommender architectures.', 'score': 2, 'issue_id': 5445, 'pub_date': '2025-08-14', 'pub_date_card': {'ru': '14 августа', 'en': 'August 14', 'zh': '8月14日'}, 'hash': '9cf423637a98e153', 'authors': ['Gustavo Penha', "Edoardo D'Amico", 'Marco De Nadai', 'Enrico Palumbo', 'Alexandre Tamborrino', 'Ali Vardasbi', 'Max Lefarov', 'Shawn Lin', 'Timothy Heath', 'Francesco Fabbri', 'Hugues Bouchard'], 'affiliations': ['Spotify Amsterdam, Netherlands', 'Spotify Barcelona, Spain', 'Spotify Copenhagen, Denmark', 'Spotify Delft, Netherlands', 'Spotify Madrid, Spain', 'Spotify Munich, Germany', 'Spotify New York, United States', 'Spotify Paris, France', 'Spotify Turin, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2508.10478.jpg', 'data': {'categories': ['#data', '#training', '#architecture', '#transfer_learning', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Единые семантические ID для поиска и рекомендаций', 'desc': 'Статья исследует эффективные способы создания семантических идентификаторов для единой генеративной рекомендательной системы. Авторы сравнивают различные стратегии построения семантических ID, включая подходы для отдельных задач и кросс-задачные методы. Результаты показывают, что использование би-энкодерной модели, дообученной на задачах поиска и рекомендаций, обеспечивает оптимальный баланс производительности. Исследование направлено на развитие обобщаемых, семантически обоснованных схем идентификации для унифицированных генеративных рекомендательных архитектур.'}, 'en': {'title': 'Unified Semantic IDs for Enhanced Recommendations and Search', 'desc': 'This paper discusses a bi-encoder model that is fine-tuned for both search and recommendation tasks, which helps create effective Semantic IDs for a unified generative recommender system. The authors investigate different strategies for constructing Semantic IDs, focusing on how these IDs can be optimized for both tasks simultaneously. They find that using a shared Semantic ID space derived from item embeddings leads to better performance compared to task-specific approaches. The study aims to inspire further research into generalizable ID schemes that can enhance future generative recommender systems.'}, 'zh': {'title': '统一生成推荐系统的语义ID构建', 'desc': '本文探讨了一种双编码器模型，该模型在搜索和推荐任务上进行微调，以生成有效的语义ID，从而支持统一的生成推荐系统。传统上，项目通过唯一标识符表示，而最近则使用由嵌入生成的离散代码构成的语义ID。我们比较了多种构建语义ID的策略，发现双编码器模型在搜索和推荐任务上表现良好，能够有效地生成统一的语义ID空间。我们的研究结果为未来的通用、语义基础的ID方案提供了启示，并推动了统一生成推荐架构的发展。'}}}, {'id': 'https://huggingface.co/papers/2508.04326', 'title': 'Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned\n  and Addressed for XR Research', 'url': 'https://huggingface.co/papers/2508.04326', 'abstract': 'A systematic survey of radiance fields in XR applications identifies implementation details, research gaps, and future directions within the broader RF research field.  \t\t\t\t\tAI-generated summary \t\t\t\t The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF), has revolutionized interactive photorealistic view synthesis and presents enormous opportunities for XR research and applications. However, despite the exponential growth of RF research, RF-related contributions to the XR community remain sparse. To better understand this research gap, we performed a systematic survey of current RF literature to analyze (i) how RF is envisioned for XR applications, (ii) how they have already been implemented, and (iii) the remaining research gaps. We collected 365 RF contributions related to XR from computer vision, computer graphics, robotics, multimedia, human-computer interaction, and XR communities, seeking to answer the above research questions. Among the 365 papers, we performed an analysis of 66 papers that already addressed a detailed aspect of RF research for XR. With this survey, we extended and positioned XR-specific RF research topics in the broader RF research field and provide a helpful resource for the XR community to navigate within the rapid development of RF research.', 'score': 2, 'issue_id': 5445, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '4c161c4673b70c99', 'authors': ['Ke Li', 'Mana Masuda', 'Susanne Schmidt', 'Shohei Mori'], 'affiliations': ['Keio University, Japan', 'University of Canterbury, New Zealand', 'University of Hamburg, Germany', 'University of Stuttgart, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2508.04326.jpg', 'data': {'categories': ['#multimodal', '#3d', '#robotics', '#survey', '#cv'], 'emoji': '🔍', 'ru': {'title': 'Радиационные поля в XR: от визии к реальности', 'desc': 'Это систематический обзор применения радиационных полей (RF) в приложениях расширенной реальности (XR). Исследование анализирует 365 работ, связанных с RF и XR, из различных областей, включая компьютерное зрение и графику. Особое внимание уделяется 66 статьям, детально рассматривающим RF для XR. Обзор выявляет текущее состояние, пробелы в исследованиях и будущие направления развития RF в контексте XR.'}, 'en': {'title': 'Bridging Radiance Fields and XR: A Path Forward', 'desc': 'This paper presents a comprehensive survey of radiance fields (RF) in the context of extended reality (XR) applications, highlighting key implementation details and identifying significant research gaps. It discusses the impact of advanced RF techniques like 3D Gaussian Splatting and Neural Radiance Fields on photorealistic view synthesis, which is crucial for XR experiences. The authors analyzed 365 contributions from various fields, focusing on 66 papers that specifically address RF applications in XR. This work aims to guide future research directions and enhance the integration of RF technologies within the XR community.'}, 'zh': {'title': '辐射场技术：扩展现实的未来之路', 'desc': '本论文系统性地调查了辐射场（RF）在扩展现实（XR）应用中的实现细节、研究空白和未来方向。研究表明，尽管辐射场技术（如3D高斯点云和神经辐射场）在交互式真实感视图合成中取得了重大进展，但与XR社区相关的RF贡献仍然稀少。我们分析了365篇与XR相关的RF文献，并深入研究了66篇已详细探讨RF在XR中的应用的论文。通过这项调查，我们为XR社区提供了一个有用的资源，以便在RF研究的快速发展中进行导航。'}}}, {'id': 'https://huggingface.co/papers/2508.13804', 'title': "Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values\n  Understanding", 'url': 'https://huggingface.co/papers/2508.13804', 'abstract': "A Bayesian evaluation framework assesses large language models' moral understanding by modeling human annotator disagreements, showing AI models perform well with fewer false negatives.  \t\t\t\t\tAI-generated summary \t\t\t\t How do large language models understand moral dimensions compared to humans?   This first large-scale Bayesian evaluation of market-leading language models provides the answer. In contrast to prior work using deterministic ground truth (majority or inclusion rules), we model annotator disagreements to capture both aleatoric uncertainty (inherent human disagreement) and epistemic uncertainty (model domain sensitivity). We evaluate top language models (Claude Sonnet 4, DeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from ~700 annotators on 100K+ texts spanning social media, news, and forums.   Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing that AI models typically rank among the top 25\\% of human annotators, achieving much better-than-average balanced accuracy. Importantly, we find that AI produces far fewer false negatives than humans, highlighting their more sensitive moral detection capabilities.", 'score': 1, 'issue_id': 5446, 'pub_date': '2025-08-19', 'pub_date_card': {'ru': '19 августа', 'en': 'August 19', 'zh': '8月19日'}, 'hash': '3a2fe1bff3cb734f', 'authors': ['Maciej Skorski', 'Alina Landowska'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2508.13804.jpg', 'data': {'categories': ['#data', '#benchmark', '#ethics', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'ИИ превосходит людей в моральной оценке текстов', 'desc': 'Исследование представляет Байесовский подход к оценке понимания моральных аспектов крупными языковыми моделями. В отличие от предыдущих работ, использующих детерминированную истину, данный метод моделирует разногласия аннотаторов для учета алеаторической и эпистемической неопределенности. Оценка проводилась на ведущих языковых моделях с использованием более 250 тысяч аннотаций от около 700 аннотаторов. Результаты показывают, что ИИ-модели обычно входят в топ-25% человеческих аннотаторов по сбалансированной точности, демонстрируя значительно меньше ложноотрицательных результатов.'}, 'en': {'title': "Bayesian Insights into AI's Moral Understanding", 'desc': 'This paper presents a Bayesian evaluation framework to assess how well large language models understand moral concepts compared to human annotators. By modeling disagreements among human annotators, the study captures both aleatoric and epistemic uncertainties, providing a more nuanced evaluation than previous methods. The framework evaluates leading language models using over 250,000 annotations from around 700 annotators across diverse text sources. The results indicate that these AI models perform in the top 25% of human annotators and demonstrate significantly fewer false negatives, suggesting superior moral detection capabilities.'}, 'zh': {'title': 'AI模型道德理解能力超越人类标注者', 'desc': '这篇论文提出了一种贝叶斯评估框架，用于评估大型语言模型的道德理解能力。通过建模人类标注者之间的分歧，研究显示AI模型在道德判断上表现良好，尤其是在减少假阴性方面。与以往使用确定性真相的方法不同，这种方法考虑了人类标注者的固有不确定性和模型的敏感性。研究评估了多个顶尖语言模型，结果表明AI模型在道德检测能力上优于大多数人类标注者。'}}}, {'id': 'https://huggingface.co/papers/2508.13320', 'title': 'Rapidly Adapting to New Voice Spoofing: Few-Shot Detection of\n  Synthesized Speech Under Distribution Shifts', 'url': 'https://huggingface.co/papers/2508.13320', 'abstract': 'A self-attentive prototypical network improves few-shot adaptation for detecting synthesized speech under distribution shifts, achieving significant performance gains over zero-shot detectors.  \t\t\t\t\tAI-generated summary \t\t\t\t We address the challenge of detecting synthesized speech under distribution shifts -- arising from unseen synthesis methods, speakers, languages, or audio conditions -- relative to the training data. Few-shot learning methods are a promising way to tackle distribution shifts by rapidly adapting on the basis of a few in-distribution samples. We propose a self-attentive prototypical network to enable more robust few-shot adaptation. To evaluate our approach, we systematically compare the performance of traditional zero-shot detectors and the proposed few-shot detectors, carefully controlling training conditions to introduce distribution shifts at evaluation time. In conditions where distribution shifts hamper the zero-shot performance, our proposed few-shot adaptation technique can quickly adapt using as few as 10 in-distribution samples -- achieving upto 32% relative EER reduction on deepfakes in Japanese language and 20% relative reduction on ASVspoof 2021 Deepfake dataset.', 'score': 1, 'issue_id': 5452, 'pub_date': '2025-08-18', 'pub_date_card': {'ru': '18 августа', 'en': 'August 18', 'zh': '8月18日'}, 'hash': 'dbde1b259142c75f', 'authors': ['Ashi Garg', 'Zexin Cai', 'Henry Li Xinyuan', 'Leibny Paola García-Perera', 'Kevin Duh', 'Sanjeev Khudanpur', 'Matthew Wiesner', 'Nicholas Andrews'], 'affiliations': ['Human Language Technology Center of Excellence, Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2508.13320.jpg', 'data': {'categories': ['#transfer_learning', '#audio', '#training', '#low_resource'], 'emoji': '🎙️', 'ru': {'title': 'Быстрая адаптация детектора синтезированной речи по нескольким примерам', 'desc': 'Статья представляет метод обнаружения синтезированной речи в условиях смещения распределения данных. Авторы предлагают самовнимательную прототипическую сеть для более надежной адаптации при обучении по малому числу примеров. Метод показывает значительное улучшение производительности по сравнению с детекторами без дообучения, особенно при смещении распределения. Эксперименты демонстрируют эффективность подхода на японском языке и наборе данных ASVspoof 2021 Deepfake.'}, 'en': {'title': 'Enhancing Speech Detection with Few-Shot Learning', 'desc': 'This paper presents a novel approach to detecting synthesized speech when faced with distribution shifts, which can occur due to different synthesis methods or audio conditions. The authors introduce a self-attentive prototypical network that enhances few-shot learning, allowing the model to adapt quickly using only a small number of in-distribution samples. By comparing their method to traditional zero-shot detectors, they demonstrate significant performance improvements, particularly in challenging scenarios. The results show that their approach can reduce error rates substantially, making it effective for real-world applications in speech detection.'}, 'zh': {'title': '自注意力原型网络：提升合成语音检测的少样本适应能力', 'desc': '本文提出了一种自注意力原型网络，以改善在分布变化下检测合成语音的少样本适应能力。我们面临的挑战是如何在未见过的合成方法、说话者、语言或音频条件下进行有效检测。通过少样本学习方法，我们能够基于少量的训练样本快速适应新的分布。实验结果表明，在分布变化影响零样本检测器性能的情况下，我们的方法可以使用仅10个样本实现显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2508.04038', 'title': 'ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval\n  Driven LLM Agents', 'url': 'https://huggingface.co/papers/2508.04038', 'abstract': 'ZARA is an agent-based framework for zero-shot, explainable human activity recognition from raw motion time-series, achieving state-of-the-art performance without fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Motion sensor time-series are central to human activity recognition (HAR), with applications in health, sports, and smart devices. However, existing methods are trained for fixed activity sets and require costly retraining when new behaviours or sensor setups appear. Recent attempts to use large language models (LLMs) for HAR, typically by converting signals into text or images, suffer from limited accuracy and lack verifiable interpretability. We propose ZARA, the first agent-based framework for zero-shot, explainable HAR directly from raw motion time-series. ZARA integrates an automatically derived pair-wise feature knowledge base that captures discriminative statistics for every activity pair, a multi-sensor retrieval module that surfaces relevant evidence, and a hierarchical agent pipeline that guides the LLM to iteratively select features, draw on this evidence, and produce both activity predictions and natural-language explanations. ZARA enables flexible and interpretable HAR without any fine-tuning or task-specific classifiers. Extensive experiments on 8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering clear reasoning while exceeding the strongest baselines by 2.53x in macro F1. Ablation studies further confirm the necessity of each module, marking ZARA as a promising step toward trustworthy, plug-and-play motion time-series analysis. Our codes are available at https://github.com/zechenli03/ZARA.', 'score': 1, 'issue_id': 5445, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '40865039139356ba', 'authors': ['Zechen Li', 'Baiyu Chen', 'Hao Xue', 'Flora D. Salim'], 'affiliations': ['University of New South Wales, Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2508.04038.jpg', 'data': {'categories': ['#multimodal', '#agents', '#interpretability', '#reasoning', '#healthcare'], 'emoji': '🏃', 'ru': {'title': 'ZARA: Универсальное распознавание активности без обучения', 'desc': 'ZARA - это агентная система для распознавания человеческой активности по временным рядам движения без предварительного обучения. Она использует автоматически созданную базу знаний о парных признаках, модуль извлечения данных с нескольких датчиков и иерархический конвейер агентов для управления большой языковой моделью. ZARA достигает лучших результатов в задаче распознавания активности с нулевым обучением на 8 эталонных наборах данных. Система обеспечивает гибкое и интерпретируемое распознавание активности без необходимости дообучения или специфических классификаторов.'}, 'en': {'title': 'ZARA: Zero-Shot HAR with Explainable Insights', 'desc': 'ZARA is a novel framework designed for zero-shot human activity recognition (HAR) using raw motion time-series data. It eliminates the need for retraining by leveraging an agent-based approach that integrates a feature knowledge base and a multi-sensor retrieval module. This allows ZARA to provide accurate activity predictions along with natural-language explanations, enhancing interpretability. Extensive testing shows that ZARA outperforms existing methods significantly, making it a promising solution for flexible and explainable HAR applications.'}, 'zh': {'title': 'ZARA：无微调的可解释人类活动识别新框架', 'desc': 'ZARA是一个基于代理的框架，旨在从原始运动时间序列中进行零样本、可解释的人类活动识别。与传统方法不同，ZARA无需微调，能够灵活应对新行为和传感器设置。该框架结合了自动生成的特征知识库和多传感器检索模块，能够提供相关证据并生成自然语言解释。实验结果表明，ZARA在八个基准测试中实现了最先进的零样本性能，显著超越了现有的最佳基线。'}}}, {'id': 'https://huggingface.co/papers/2508.12800', 'title': 'Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic\n  Thought Reward', 'url': 'https://huggingface.co/papers/2508.12800', 'abstract': "Atom-Searcher, an RL framework integrating Atomic Thought and Reasoning Reward Models, enhances LLMs' multi-hop reasoning and strategic search capabilities, improving performance and interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) exhibit remarkable problem-solving abilities, but struggle with complex tasks due to static internal knowledge. Retrieval-Augmented Generation (RAG) enhances access to external information, yet remains limited in multi-hop reasoning and strategic search due to rigid workflows. Recent advancements in agentic deep research empower LLMs to autonomously reason, search, and synthesize information. However, current approaches relying on outcome-based reinforcement learning (RL) face critical issues such as conflicting gradients and reward sparsity, limiting performance gains and training efficiency. To address these, we first propose Atomic Thought, a novel LLM thinking paradigm that decomposes reasoning into fine-grained functional units. These units are supervised by Reasoning Reward Models (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained guidance. Building on this, we propose Atom-Searcher, a novel RL framework for agentic deep research that integrates Atomic Thought and ATR. Atom-Searcher uses a curriculum-inspired reward schedule, prioritizing process-level ATR early and transitioning to outcome rewards, accelerating convergence on effective reasoning paths. Experiments on seven benchmarks show consistent improvements over the state-of-the-art. Key advantages include: (1) Atom-Searcher scales computation at test-time. (2) Atomic Thought provides supervision anchors for RRMs, bridging deep research tasks and RRMs. (3) Atom-Searcher exhibits more interpretable, human-like reasoning patterns.", 'score': 0, 'issue_id': 5450, 'pub_date': '2025-08-18', 'pub_date_card': {'ru': '18 августа', 'en': 'August 18', 'zh': '8月18日'}, 'hash': '03aaa2e5e2aaa040', 'authors': ['Yong Deng', 'Guoqing Wang', 'Zhenzhe Ying', 'Xiaofeng Wu', 'Jinzhen Lin', 'Wenwen Xiong', 'Yuqin Dai', 'Shuo Yang', 'Zhanwei Zhang', 'Qiwen Wang', 'Yang Qin', 'Changhua Meng'], 'affiliations': ['Ant Group'], 'pdf_title_img': 'assets/pdf/title_img/2508.12800.jpg', 'data': {'categories': ['#optimization', '#training', '#benchmark', '#reasoning', '#rl', '#rag', '#interpretability', '#agents'], 'emoji': '🧠', 'ru': {'title': 'Атомарное мышление для эффективных ИИ-рассуждений', 'desc': "Atom-Searcher - это новая система обучения с подкреплением для улучшения рассуждений и стратегического поиска у больших языковых моделей. Она использует концепцию 'атомарной мысли', разбивая процесс рассуждения на мелкие функциональные единицы. Система применяет модели вознаграждения за рассуждения для точного руководства этими единицами. Atom-Searcher показывает улучшенную производительность и интерпретируемость по сравнению с современными методами на семи эталонных задачах."}, 'en': {'title': 'Empowering LLMs with Atomic Thought for Enhanced Reasoning', 'desc': 'Atom-Searcher is a reinforcement learning framework designed to enhance the multi-hop reasoning and strategic search capabilities of large language models (LLMs). It introduces a new thinking paradigm called Atomic Thought, which breaks down reasoning into smaller, manageable units that are guided by Reasoning Reward Models (RRMs). This approach addresses issues like conflicting gradients and reward sparsity in traditional RL methods by using a curriculum-inspired reward schedule that focuses on process-level rewards before shifting to outcome rewards. Experiments demonstrate that Atom-Searcher significantly improves performance and interpretability in complex reasoning tasks compared to existing methods.'}, 'zh': {'title': 'Atom-Searcher：提升推理与搜索能力的强化学习框架', 'desc': 'Atom-Searcher 是一个强化学习框架，结合了原子思维和推理奖励模型，提升了大型语言模型（LLMs）在多跳推理和战略搜索方面的能力。该框架通过将推理分解为细粒度的功能单元，利用推理奖励模型提供的奖励进行指导，从而改善了模型的表现和可解释性。Atom-Searcher 采用课程激励的奖励调度，早期注重过程级奖励，后期转向结果奖励，加速了有效推理路径的收敛。实验结果表明，Atom-Searcher 在七个基准测试中均表现出优于现有技术的持续改进。'}}}, {'id': 'https://huggingface.co/papers/2508.11386', 'title': 'Retrieval-augmented reasoning with lean language models', 'url': 'https://huggingface.co/papers/2508.11386', 'abstract': 'A retrieval-augmented conversational agent using a lightweight model achieves high accuracy and consistency in domain-specific queries through fine-tuning and document compression.  \t\t\t\t\tAI-generated summary \t\t\t\t This technical report details a novel approach to combining reasoning and retrieval augmented generation (RAG) within a single, lean language model architecture. While existing RAG systems typically rely on large-scale models and external APIs, our work addresses the increasing demand for performant and privacy-preserving solutions deployable in resource-constrained or secure environments. Building on recent developments in test-time scaling and small-scale reasoning models, we develop a retrieval augmented conversational agent capable of interpreting complex, domain-specific queries using a lightweight backbone model. Our system integrates a dense retriever with fine-tuned Qwen2.5-Instruct models, using synthetic query generation and reasoning traces derived from frontier models (e.g., DeepSeek-R1) over a curated corpus, in this case, the NHS A-to-Z condition pages. We explore the impact of summarisation-based document compression, synthetic data design, and reasoning-aware fine-tuning on model performance. Evaluation against both non-reasoning and general-purpose lean models demonstrates that our domain-specific fine-tuning approach yields substantial gains in answer accuracy and consistency, approaching frontier-level performance while remaining feasible for local deployment. All implementation details and code are publicly released to support reproducibility and adaptation across domains.', 'score': 0, 'issue_id': 5457, 'pub_date': '2025-08-15', 'pub_date_card': {'ru': '15 августа', 'en': 'August 15', 'zh': '8月15日'}, 'hash': '980fa2fcfc04e04a', 'authors': ['Ryan Sze-Yin Chan', 'Federico Nanni', 'Tomas Lazauskas', 'Rosie Wood', 'Penelope Yong', 'Lionel Tarassenko', 'Mark Girolami', 'James Geddes', 'Andrew Duncan'], 'affiliations': ['Imperial College London', 'The Alan Turing Institute', 'University of Cambridge', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2508.11386.jpg', 'data': {'categories': ['#open_source', '#inference', '#small_models', '#training', '#reasoning', '#rag', '#agents', '#synthetic', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Легковесный ИИ-ассистент с точностью на уровне крупных моделей', 'desc': 'В этой статье представлен новый подход к объединению рассуждений и генерации с дополнением извлечением (RAG) в рамках единой, легковесной архитектуры языковой модели. Авторы разработали разговорный агент с дополнением извлечением, способный интерпретировать сложные запросы в конкретной предметной области, используя легкую базовую модель. Система интегрирует плотный извлекатель с дообученными моделями Qwen2.5-Instruct, используя синтетическую генерацию запросов и трассы рассуждений. Оценка показывает, что их подход к дообучению для конкретной области дает существенный прирост в точности и согласованности ответов.'}, 'en': {'title': 'Lightweight Conversational Agent: High Accuracy, Low Resource!', 'desc': 'This paper presents a new method for creating a conversational agent that can effectively answer specific questions by combining reasoning and retrieval techniques within a compact language model. Unlike traditional systems that depend on large models and external services, this approach is designed to work well in environments with limited resources while ensuring user privacy. The agent uses a combination of a dense retriever and a fine-tuned model to handle complex queries, leveraging document compression and synthetic data to enhance performance. The results show that this lightweight model can achieve high accuracy and consistency in responses, making it suitable for local deployment in various applications.'}, 'zh': {'title': '轻量级模型实现高效对话代理', 'desc': '本文介绍了一种新颖的方法，将推理与检索增强生成（RAG）结合在一个轻量级语言模型架构中。与现有的RAG系统通常依赖于大规模模型和外部API不同，我们的工作旨在满足在资源受限或安全环境中部署高效且保护隐私的解决方案的需求。我们开发的检索增强对话代理能够使用轻量级模型解释复杂的领域特定查询，并通过文档压缩和合成数据设计来提升模型性能。评估结果表明，我们的领域特定微调方法在答案准确性和一致性上取得了显著提升，接近前沿水平，同时仍然适合本地部署。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (6)', '#agi (3)', '#alignment (3)', '#architecture (4)', '#audio (4)', '#benchmark (14)', '#cv (7)', '#data (8)', '#dataset (7)', '#diffusion (1)', '#ethics (3)', '#games (4)', '#graphs', '#hallucinations (1)', '#healthcare (2)', '#inference (2)', '#interpretability (5)', '#leakage', '#long_context', '#low_resource (1)', '#machine_translation', '#math', '#multilingual', '#multimodal (12)', '#open_source (5)', '#optimization (12)', '#plp', '#rag (2)', '#reasoning (9)', '#rl (4)', '#rlhf (2)', '#robotics (2)', '#science', '#security', '#small_models (1)', '#story_generation', '#survey (3)', '#synthetic (2)', '#training (13)', '#transfer_learning (3)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-08-20 21:10',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-08-20 21:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-08-20 21:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    