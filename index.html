
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 50 papers. October 10.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">10 октября</span> | <span id="title-articles-count">50 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-10-09.html">⬅️ <span id="prev-date">09.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-10-13.html">➡️ <span id="next-date">13.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-10.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'};
        let feedDateNext = {'ru': '13.10', 'en': '10/13', 'zh': '10月13日'};
        let feedDatePrev = {'ru': '09.10', 'en': '10/09', 'zh': '10月9日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2510.08558', 'title': 'Agent Learning via Early Experience', 'url': 'https://huggingface.co/papers/2510.08558', 'abstract': "Early experience, using agent-generated interaction data without reward signals, improves policy effectiveness and generalization, serving as a bridge between imitation learning and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t A long-term goal of language agents is to learn and improve through their own experience, ultimately outperforming humans in complex, real-world tasks. However, training agents from experience data with reinforcement learning remains difficult in many environments, which either lack verifiable rewards (e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn tool use). As a result, most current agents rely on supervised fine-tuning on expert data, which is challenging to scale and generalizes poorly. This limitation stems from the nature of expert demonstrations: they capture only a narrow range of scenarios and expose the agent to limited environment diversity. We address this limitation with a middle-ground paradigm we call early experience: interaction data generated by the agent's own actions, where the resulting future states serve as supervision without reward signals. Within this paradigm we study two strategies of using such data: (1) Implicit world modeling, which uses collected states to ground the policy in environment dynamics; and (2) Self-reflection, where the agent learns from its suboptimal actions to improve reasoning and decision-making. We evaluate across eight diverse environments and multiple model families. Our approaches consistently improve effectiveness and out-of-domain generalization, highlighting the value of early experience. Moreover, in environments with verifiable rewards, our results provide promising signals that early experience offers a strong foundation for subsequent reinforcement learning, positioning it as a practical bridge between imitation learning and fully experience-driven agents.", 'score': 153, 'issue_id': 6345, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '332f256ea51550f0', 'authors': ['Kai Zhang', 'Xiangchao Chen', 'Bo Liu', 'Tianci Xue', 'Zeyi Liao', 'Zhihan Liu', 'Xiyao Wang', 'Yuting Ning', 'Zhaorun Chen', 'Xiaohan Fu', 'Jian Xie', 'Yuxuan Sun', 'Boyu Gou', 'Qi Qi', 'Zihang Meng', 'Jianwei Yang', 'Ning Zhang', 'Xian Li', 'Ashish Shah', 'Dat Huynh', 'Hengduo Li', 'Zi Yang', 'Sara Cao', 'Lawrence Jang', 'Shuyan Zhou', 'Jiacheng Zhu', 'Huan Sun', 'Jason Weston', 'Yu Su', 'Yifan Wu'], 'affiliations': ['FAIR at Meta', 'Meta Superintelligence Labs', 'The Ohio State University'], 'pdf_title_img': 'assets/pdf/title_img/2510.08558.jpg', 'data': {'categories': ['#transfer_learning', '#rl', '#rlhf', '#reasoning', '#agents'], 'emoji': '🌉', 'ru': {'title': 'Ранний опыт: мост между имитацией и подкреплением', 'desc': 'Статья предлагает новый подход к обучению language-агентов через "ранний опыт" - использование данных взаимодействия, сгенерированных самим агентом, без явных сигналов награды. Авторы исследуют две стратегии: имплицитное моделирование мира через собранные состояния и саморефлексию для улучшения reasoning. Эксперименты на восьми различных окружениях показывают улучшение эффективности и обобщающей способности агентов. Подход служит практическим мостом между imitation learning и полноценным reinforcement learning, решая проблему ограниченности экспертных демонстраций.'}, 'en': {'title': 'Harnessing Early Experience for Smarter Agents', 'desc': "This paper introduces a new approach called 'early experience' for training language agents, which uses data generated from the agent's own interactions without relying on reward signals. The authors highlight the challenges of traditional reinforcement learning, especially in environments lacking clear rewards or requiring complex decision-making. By employing strategies like implicit world modeling and self-reflection, agents can learn from their own actions and improve their performance and generalization capabilities. The results show that early experience not only enhances the effectiveness of agents but also serves as a valuable link between imitation learning and reinforcement learning."}, 'zh': {'title': '早期经验：连接模仿学习与强化学习的桥梁', 'desc': '本文探讨了一种名为“早期经验”的新方法，旨在通过代理生成的交互数据来提高策略的有效性和泛化能力，而无需依赖奖励信号。这种方法为模仿学习和强化学习之间架起了一座桥梁，解决了当前代理在缺乏可验证奖励的环境中训练的困难。我们提出了两种利用早期经验数据的策略：隐式世界建模和自我反思，前者通过收集的状态来增强策略与环境动态的联系，后者则通过学习次优行为来改善推理和决策能力。实验结果表明，这种方法在多种环境中均能有效提升代理的表现，显示出早期经验在强化学习中的潜在价值。'}}}, {'id': 'https://huggingface.co/papers/2510.08540', 'title': 'MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with\n  Holistic Platform and Adaptive Hybrid Policy Optimization', 'url': 'https://huggingface.co/papers/2510.08540', 'abstract': 'Existing Multimodal Large Language Models show performance deficits in long-chain reflective reasoning, which is addressed by developing MM-HELIX-100K and Adaptive Hybrid Policy Optimization, leading to improved accuracy and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t While current Multimodal Large Language Models (MLLMs) have demonstrated proficiency in reasoning tasks such as mathematics and logic, their capacity for long-chain reflective reasoning, a prerequisite for solving complex real-world problems, remains largely underexplored. In this work, we first conduct an extensive empirical investigation to evaluate this capability. Leveraging a carefully designed data synthesis engine, we construct MM-HELIX, a multimodal benchmark consisting 1,260 samples of 42 challenging synthetic tasks that require iterative thinking and backtracking. Empirical results on this benchmark reveal that existing MLLMs exhibit significant performance deficits in long-chain reflective reasoning. To address this limitation, we generate post-training data and further explore learning paradigms for exploiting such data. We first develop the Step-Elicited Response Generation pipeline to create MM-HELIX-100K, a large-scale dataset of 100k high-quality, reflective reasoning traces for instruction-tuning stage. Given that standard Reinforcement Learning fails on complex tasks due to sparse reward signals and catastrophic forgetting after Supervised Fine-Tuning, we propose Adaptive Hybrid Policy Optimization (AHPO), a novel training strategy that dynamically unifies offline supervision and online optimization into a single stage. This strategy enables the model to learn from expert data when rewards are sparse and conduct independent exploration once proficient. When applied to the Qwen2.5-VL-7B baseline, our method achieves a +18.6\\% accuracy improvement on MM-HELIX benchmark and demonstrates strong generalization with a +5.7\\% average performance gain on general mathematic and logic tasks. Our work demonstrate that reflective reasoning in MLLMs can be effectively learned and generalized, paving the way for developing more capable MLLMs.', 'score': 96, 'issue_id': 6345, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '62d107921c57dab0', 'authors': ['Xiangyu Zhao', 'Junming Lin', 'Tianhao Liang', 'Yifan Zhou', 'Wenhao Chai', 'Yuzhe Gu', 'Weiyun Wang', 'Kai Chen', 'Gen Luo', 'Wenwei Zhang', 'Junchi Yan', 'Hua Yang', 'Haodong Duan', 'Xue Yang'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'Princeton University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.08540.jpg', 'data': {'categories': ['#training', '#benchmark', '#rl', '#multimodal', '#dataset', '#optimization', '#reasoning'], 'emoji': '🔄', 'ru': {'title': 'Обучение мультимодальных LLM рефлексивному мышлению через гибридную оптимизацию', 'desc': 'Исследователи обнаружили, что современные мультимодальные LLM плохо справляются с длинными цепочками рефлексивного рассуждения, требующего итеративного мышления и возврата к предыдущим шагам. Для решения проблемы создан бенчмарк MM-HELIX с 1260 сложными задачами и датасет MM-HELIX-100K из 100 тысяч примеров рассуждений для файн-тюнинга. Предложен метод Adaptive Hybrid Policy Optimization (AHPO), который динамически объединяет обучение с учителем и reinforcement learning, позволяя модели учиться на экспертных данных при редких наградах и самостоятельно исследовать после освоения навыка. Применение метода к модели Qwen2.5-VL-7B дало прирост точности +18.6% на MM-HELIX и +5.7% на общих математических и логических задачах, демонстрируя эффективность обучения рефлексивному мышлению.'}, 'en': {'title': 'Enhancing Reflective Reasoning in Multimodal Models', 'desc': 'This paper addresses the limitations of existing Multimodal Large Language Models (MLLMs) in performing long-chain reflective reasoning, which is essential for tackling complex problems. The authors introduce MM-HELIX-100K, a large dataset designed to enhance the instruction-tuning of MLLMs by providing high-quality reflective reasoning examples. They also propose Adaptive Hybrid Policy Optimization (AHPO), a novel training approach that combines offline supervision with online learning to improve model performance on challenging tasks. The results show significant accuracy improvements and better generalization in reasoning tasks, indicating that reflective reasoning can be effectively learned in MLLMs.'}, 'zh': {'title': '提升多模态模型的反思推理能力', 'desc': '现有的多模态大型语言模型在长链反思推理方面表现不足，这项研究通过开发MM-HELIX-100K数据集和自适应混合策略优化方法来解决这一问题，从而提高了模型的准确性和泛化能力。我们首先构建了一个包含1260个样本的多模态基准，评估现有模型在复杂任务中的反思推理能力。研究结果表明，现有模型在长链反思推理上存在显著的性能缺陷。通过引入新的训练策略，我们的模型在MM-HELIX基准上实现了18.6%的准确率提升，并在一般数学和逻辑任务上也表现出5.7%的平均性能提升。'}}}, {'id': 'https://huggingface.co/papers/2510.03279', 'title': 'MemMamba: Rethinking Memory Patterns in State Space Model', 'url': 'https://huggingface.co/papers/2510.03279', 'abstract': "MemMamba, a novel architecture integrating state summarization and cross-attention, improves long-range memory and efficiency in sequence modeling compared to Mamba and Transformers.  \t\t\t\t\tAI-generated summary \t\t\t\t With the explosive growth of data, long-sequence modeling has become increasingly important in tasks such as natural language processing and bioinformatics. However, existing methods face inherent trade-offs between efficiency and memory. Recurrent neural networks suffer from gradient vanishing and explosion, making them hard to scale. Transformers can model global dependencies but are constrained by quadratic complexity. Recently, selective state-space models such as Mamba have demonstrated high efficiency with O(n) time and O(1) recurrent inference, yet their long-range memory decays exponentially. In this work, we conduct mathematical derivations and information-theoretic analysis to systematically uncover the memory decay mechanism of Mamba, answering a fundamental question: what is the nature of Mamba's long-range memory and how does it retain information? To quantify key information loss, we further introduce horizontal-vertical memory fidelity metrics that capture degradation both within and across layers. Inspired by how humans distill and retain salient information when reading long documents, we propose MemMamba, a novel architectural framework that integrates state summarization mechanism together with cross-layer and cross-token attention, which alleviates long-range forgetting while preserving linear complexity. MemMamba achieves significant improvements over existing Mamba variants and Transformers on long-sequence benchmarks such as PG19 and Passkey Retrieval, while delivering a 48% speedup in inference efficiency. Both theoretical analysis and empirical results demonstrate that MemMamba achieves a breakthrough in the complexity-memory trade-off, offering a new paradigm for ultra-long sequence modeling.", 'score': 63, 'issue_id': 6347, 'pub_date': '2025-09-28', 'pub_date_card': {'ru': '28 сентября', 'en': 'September 28', 'zh': '9月28日'}, 'hash': '71cbab26cd912cd5', 'authors': ['Youjin Wang', 'Yangjingyi Chen', 'Jiahao Yan', 'Jiaxuan Lu', 'Xiao Sun'], 'affiliations': ['Gao Ling Institute of Artificial Intelligence Renmin University of China Beijing, China', 'School of Statistics Renmin University of China Beijing, China', 'Shanghai Artificial Intelligence Laboratory Shanghai, China', 'Shanghai University of Finance and Economics Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2510.03279.jpg', 'data': {'categories': ['#long_context', '#math', '#optimization', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Долговременная память для линейных моделей последовательностей', 'desc': 'Статья представляет MemMamba - новую архитектуру для моделирования длинных последовательностей, которая решает проблему экспоненциального затухания долговременной памяти в моделях Mamba. Авторы провели математический анализ механизма потери памяти и предложили решение, вдохновленное тем, как люди запоминают важную информацию при чтении длинных текстов. Архитектура использует механизм суммаризации состояний и кросс-attention между слоями и токенами, сохраняя при этом линейную сложность O(n). MemMamba показывает значительное улучшение по сравнению с Transformer и другими вариантами Mamba на бенчмарках длинных последовательностей, ускоряя инференс на 48%.'}, 'en': {'title': 'MemMamba: Revolutionizing Long-Sequence Memory and Efficiency', 'desc': 'MemMamba is a new architecture designed to enhance long-range memory and efficiency in sequence modeling tasks. It combines state summarization with cross-attention mechanisms to address the memory decay issues found in previous models like Mamba and Transformers. By introducing horizontal-vertical memory fidelity metrics, the paper quantifies information loss and improves the retention of salient information across layers. The results show that MemMamba significantly outperforms existing models on long-sequence benchmarks while achieving faster inference times.'}, 'zh': {'title': 'MemMamba：长序列建模的新突破', 'desc': 'MemMamba是一种新颖的架构，结合了状态摘要和交叉注意力机制，显著提高了长序列建模中的记忆能力和效率。与传统的Mamba和Transformer相比，MemMamba在处理长序列时能够有效减少信息遗忘，同时保持线性复杂度。通过数学推导和信息论分析，研究揭示了Mamba的记忆衰减机制，并提出了新的记忆保真度指标。实验结果表明，MemMamba在长序列基准测试中表现优异，推理效率提高了48%。'}}}, {'id': 'https://huggingface.co/papers/2510.08555', 'title': 'VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal\n  Patches via In-Context Conditioning', 'url': 'https://huggingface.co/papers/2510.08555', 'abstract': "VideoCanvas addresses temporal ambiguity in latent video diffusion models to enable flexible spatio-temporal video completion using a hybrid conditioning strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce the task of arbitrary spatio-temporal video completion, where a video is generated from arbitrary, user-specified patches placed at any spatial location and timestamp, akin to painting on a video canvas. This flexible formulation naturally unifies many existing controllable video generation tasks--including first-frame image-to-video, inpainting, extension, and interpolation--under a single, cohesive paradigm. Realizing this vision, however, faces a fundamental obstacle in modern latent video diffusion models: the temporal ambiguity introduced by causal VAEs, where multiple pixel frames are compressed into a single latent representation, making precise frame-level conditioning structurally difficult. We address this challenge with VideoCanvas, a novel framework that adapts the In-Context Conditioning (ICC) paradigm to this fine-grained control task with zero new parameters. We propose a hybrid conditioning strategy that decouples spatial and temporal control: spatial placement is handled via zero-padding, while temporal alignment is achieved through Temporal RoPE Interpolation, which assigns each condition a continuous fractional position within the latent sequence. This resolves the VAE's temporal ambiguity and enables pixel-frame-aware control on a frozen backbone. To evaluate this new capability, we develop VideoCanvasBench, the first benchmark for arbitrary spatio-temporal video completion, covering both intra-scene fidelity and inter-scene creativity. Experiments demonstrate that VideoCanvas significantly outperforms existing conditioning paradigms, establishing a new state of the art in flexible and unified video generation.", 'score': 52, 'issue_id': 6345, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '22786ba2a0b5f590', 'authors': ['Minghong Cai', 'Qiulin Wang', 'Zongli Ye', 'Wenze Liu', 'Quande Liu', 'Weicai Ye', 'Xintao Wang', 'Pengfei Wan', 'Kun Gai', 'Xiangyu Yue'], 'affiliations': ['Kling Team, Kuaishou Technology', 'MMLab, The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.08555.jpg', 'data': {'categories': ['#games', '#benchmark', '#diffusion', '#video'], 'emoji': '🎨', 'ru': {'title': 'Видео как холст: произвольное заполнение в пространстве и времени', 'desc': 'Статья представляет VideoCanvas — метод для произвольного заполнения видео в пространстве и времени, где пользователь может размещать патчи в любых местах кадра и на любых временных отметках. Главная проблема заключается в временной неоднозначности латентных видео диффузионных моделей, где несколько кадров сжимаются в одно латентное представление, что затрудняет точное управление на уровне отдельных кадров. Авторы решают это через гибридную стратегию conditioning: пространственное размещение через zero-padding и временное выравнивание через Temporal RoPE Interpolation, присваивающую каждому условию непрерывную дробную позицию в латентной последовательности. Подход объединяет множество задач controllable video generation (image-to-video, inpainting, extension, interpolation) в единую парадигму и показывает state-of-the-art результаты на новом бенчмарке VideoCanvasBench.'}, 'en': {'title': 'Revolutionizing Video Generation with Flexible Spatio-Temporal Control', 'desc': 'VideoCanvas is a framework designed to tackle the challenge of temporal ambiguity in latent video diffusion models, allowing for flexible video completion based on user-defined patches. It introduces a novel approach to spatio-temporal video generation, unifying various tasks like inpainting and interpolation under one system. The framework employs a hybrid conditioning strategy that separates spatial and temporal controls, using techniques like zero-padding for spatial placement and Temporal RoPE Interpolation for temporal alignment. Through the development of VideoCanvasBench, the framework is evaluated and shown to outperform existing methods, setting a new standard in video generation capabilities.'}, 'zh': {'title': '灵活时空视频补全的新方法', 'desc': 'VideoCanvas 解决了潜在视频扩散模型中的时间模糊问题，从而实现灵活的时空视频补全。该方法允许用户在任意空间位置和时间戳生成视频，类似于在视频画布上绘画。通过引入混合条件策略，VideoCanvas 将空间和时间控制解耦，克服了现代潜在视频扩散模型中的结构性挑战。实验结果表明，VideoCanvas 在灵活和统一的视频生成方面显著优于现有的条件化范式，建立了新的技术领先水平。'}}}, {'id': 'https://huggingface.co/papers/2510.08377', 'title': 'UniVideo: Unified Understanding, Generation, and Editing for Videos', 'url': 'https://huggingface.co/papers/2510.08377', 'abstract': 'UniVideo, a dual-stream framework combining a Multimodal Large Language Model and a Multimodal DiT, extends unified modeling to video generation and editing, achieving state-of-the-art performance and supporting task composition and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal models have shown promising results in multimodal content generation and editing but remain largely limited to the image domain. In this work, we present UniVideo, a versatile framework that extends unified modeling to the video domain. UniVideo adopts a dual-stream design, combining a Multimodal Large Language Model (MLLM) for instruction understanding with a Multimodal DiT (MMDiT) for video generation. This design enables accurate interpretation of complex multimodal instructions while preserving visual consistency. Built on this architecture, UniVideo unifies diverse video generation and editing tasks under a single multimodal instruction paradigm and is jointly trained across them. Extensive experiments demonstrate that UniVideo matches or surpasses state-of-the-art task-specific baselines in text/image-to-video generation, in-context video generation and in-context video editing. Notably, the unified design of UniVideo enables two forms of generalization. First, UniVideo supports task composition, such as combining editing with style transfer, by integrating multiple capabilities within a single instruction. Second, even without explicit training on free-form video editing, UniVideo transfers its editing capability from large-scale image editing data to this setting, handling unseen instructions such as green-screening characters or changing materials within a video. Beyond these core capabilities, UniVideo also supports visual-prompt-based video generation, where the MLLM interprets visual prompts and guides the MMDiT during synthesis. To foster future research, we will release our model and code.', 'score': 52, 'issue_id': 6345, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '5d36f7f0bde88332', 'authors': ['Cong Wei', 'Quande Liu', 'Zixuan Ye', 'Qiulin Wang', 'Xintao Wang', 'Pengfei Wan', 'Kun Gai', 'Wenhu Chen'], 'affiliations': ['Kling Team, Kuaishou Technology', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2510.08377.jpg', 'data': {'categories': ['#agi', '#architecture', '#games', '#multimodal', '#video', '#transfer_learning', '#open_source'], 'emoji': '🎬', 'ru': {'title': 'Единая модель для генерации и редактирования видео по мультимодальным инструкциям', 'desc': 'UniVideo — это фреймворк с двухпоточной архитектурой, объединяющий Multimodal LLM для понимания инструкций и Multimodal DiT для генерации видео. Модель унифицирует различные задачи генерации и редактирования видео под единой парадигмой мультимодальных инструкций и обучается на них совместно. UniVideo достигает результатов на уровне специализированных моделей в задачах text/image-to-video генерации и редактирования, при этом поддерживая композицию задач и обобщение на новые типы инструкций. Примечательно, что модель может переносить навыки редактирования, полученные на изображениях, на видео без явного обучения на видеоданных.'}, 'en': {'title': 'UniVideo: Unifying Video Generation and Editing with Multimodal Intelligence', 'desc': 'UniVideo is a dual-stream framework that integrates a Multimodal Large Language Model (MLLM) and a Multimodal DiT (MMDiT) to enhance video generation and editing. This innovative approach allows the model to understand complex multimodal instructions while ensuring visual consistency in the generated content. By unifying various video tasks under a single instruction paradigm, UniVideo demonstrates superior performance in text/image-to-video generation and editing compared to existing models. Additionally, it showcases the ability to generalize across tasks, enabling capabilities like style transfer and free-form video editing without specific training on those tasks.'}, 'zh': {'title': 'UniVideo：视频生成与编辑的统一框架', 'desc': 'UniVideo是一个双流框架，结合了多模态大语言模型和多模态DiT，扩展了视频生成和编辑的统一建模。该框架能够准确理解复杂的多模态指令，同时保持视觉一致性。UniVideo将多种视频生成和编辑任务统一在一个多模态指令范式下，并通过联合训练实现。实验结果表明，UniVideo在文本/图像到视频生成、上下文视频生成和上下文视频编辑等任务上达到了或超过了最先进的基准。'}}}, {'id': 'https://huggingface.co/papers/2510.06679', 'title': 'DreamOmni2: Multimodal Instruction-based Editing and Generation', 'url': 'https://huggingface.co/papers/2510.06679', 'abstract': 'DreamOmni2 addresses limitations in instruction-based image editing and subject-driven generation by introducing multimodal instruction-based editing and generation tasks, utilizing feature mixing, index encoding, and joint training with a VLM.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in instruction-based image editing and subject-driven generation have garnered significant attention, yet both tasks still face limitations in meeting practical user needs. Instruction-based editing relies solely on language instructions, which often fail to capture specific editing details, making reference images necessary. Meanwhile, subject-driven generation is limited to combining concrete objects or people, overlooking broader, abstract concepts. To address these challenges, we propose two novel tasks: multimodal instruction-based editing and generation. These tasks support both text and image instructions and extend the scope to include both concrete and abstract concepts, greatly enhancing their practical applications. We introduce DreamOmni2, tackling two primary challenges: data creation and model framework design. Our data synthesis pipeline consists of three steps: (1) using a feature mixing method to create extraction data for both abstract and concrete concepts, (2) generating multimodal instruction-based editing training data using the editing and extraction models, and (3) further applying the extraction model to create training data for multimodal instruction-based editing. For the framework, to handle multi-image input, we propose an index encoding and position encoding shift scheme, which helps the model distinguish images and avoid pixel confusion. Additionally, we introduce joint training with the VLM and our generation/editing model to better process complex instructions. In addition, we have proposed comprehensive benchmarks for these two new tasks to drive their development. Experiments show that DreamOmni2 has achieved impressive results. Models and codes will be released.', 'score': 48, 'issue_id': 6360, 'pub_date': '2025-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': '845b1bb67d40812d', 'authors': ['Bin Xia', 'Bohao Peng', 'Yuechen Zhang', 'Junjia Huang', 'Jiyang Liu', 'Jingyao Li', 'Haoru Tan', 'Sitong Wu', 'Chengyao Wang', 'Yitong Wang', 'Xinglong Wu', 'Bei Yu', 'Jiaya Jia'], 'affiliations': ['ByteDance Inc', 'CUHK', 'HKU', 'HKUST'], 'pdf_title_img': 'assets/pdf/title_img/2510.06679.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#games', '#open_source', '#dataset', '#benchmark', '#cv'], 'emoji': '🎨', 'ru': {'title': 'Мультимодальное редактирование изображений с абстрактными концепциями', 'desc': 'DreamOmni2 решает проблемы в редактировании изображений по инструкциям и генерации с учётом субъектов, вводя новые задачи с мультимодальными инструкциями. Модель использует смешивание признаков, индексное кодирование и совместное обучение с VLM для обработки как текстовых, так и визуальных инструкций. Система расширяет возможности генерации от конкретных объектов до абстрактных концепций, что значительно повышает практическую применимость. Авторы создали специальный pipeline для синтеза данных и разработали новые бенчмарки для оценки этих задач.'}, 'en': {'title': 'Revolutionizing Image Editing with Multimodal Instructions', 'desc': 'DreamOmni2 improves image editing and generation by allowing both text and image instructions, addressing the limitations of traditional methods. It introduces multimodal tasks that can handle both concrete and abstract concepts, enhancing user experience. The model uses a unique data synthesis pipeline that includes feature mixing and joint training with a Vision-Language Model (VLM) to better interpret complex instructions. Comprehensive benchmarks have been established to evaluate these new tasks, demonstrating the effectiveness of DreamOmni2 in practical applications.'}, 'zh': {'title': 'DreamOmni2：多模态指令编辑与生成的突破', 'desc': 'DreamOmni2 解决了基于指令的图像编辑和主题驱动生成的局限性，提出了多模态指令编辑和生成任务。这些任务支持文本和图像指令，扩展了具体和抽象概念的范围，增强了实际应用。我们采用特征混合、索引编码和与视觉语言模型（VLM）的联合训练来处理复杂指令。实验结果表明，DreamOmni2 在这两个新任务上取得了显著的成果。'}}}, {'id': 'https://huggingface.co/papers/2509.23768', 'title': 'From What to Why: A Multi-Agent System for Evidence-based Chemical\n  Reaction Condition Reasoning', 'url': 'https://huggingface.co/papers/2509.23768', 'abstract': 'ChemMAS, a multi-agent system, improves reaction condition recommendation by providing interpretable rationales, outperforming existing methods in accuracy and explainability.  \t\t\t\t\tAI-generated summary \t\t\t\t The chemical reaction recommendation is to select proper reaction condition parameters for chemical reactions, which is pivotal to accelerating chemical science. With the rapid development of large language models (LLMs), there is growing interest in leveraging their reasoning and planning capabilities for reaction condition recommendation. Despite their success, existing methods rarely explain the rationale behind the recommended reaction conditions, limiting their utility in high-stakes scientific workflows. In this work, we propose ChemMAS, a multi-agent system that reframes condition prediction as an evidence-based reasoning task. ChemMAS decomposes the task into mechanistic grounding, multi-channel recall, constraint-aware agentic debate, and rationale aggregation. Each decision is backed by interpretable justifications grounded in chemical knowledge and retrieved precedents. Experiments show that ChemMAS achieves 20-35% gains over domain-specific baselines and outperforms general-purpose LLMs by 10-15% in Top-1 accuracy, while offering falsifiable, human-trustable rationales, which establishes a new paradigm for explainable AI in scientific discovery.', 'score': 45, 'issue_id': 6347, 'pub_date': '2025-09-28', 'pub_date_card': {'ru': '28 сентября', 'en': 'September 28', 'zh': '9月28日'}, 'hash': '7a77926d62939cdf', 'authors': ['Cheng Yang', 'Jiaxuan Lu', 'Haiyuan Wan', 'Junchi Yu', 'Feiwei Qin'], 'affiliations': ['Hangzhou Dianzi University', 'Shanghai Artificial Intelligence Laboratory', 'Tsinghua University', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2509.23768.jpg', 'data': {'categories': ['#interpretability', '#healthcare', '#science', '#agents', '#multimodal', '#reasoning'], 'emoji': '⚗️', 'ru': {'title': 'Мультиагентная система с объяснениями для подбора условий химических реакций', 'desc': 'ChemMAS - это мультиагентная система на основе LLM для рекомендации условий химических реакций. Система разбивает задачу на несколько этапов: механистическое обоснование, многоканальный поиск похожих примеров, дебаты между агентами с учётом ограничений и агрегацию обоснований. ChemMAS превосходит специализированные baseline-модели на 20-35% и обычные LLM на 10-15% по Top-1 accuracy. Ключевое преимущество - система предоставляет интерпретируемые обоснования своих рекомендаций, основанные на химических знаниях и прецедентах, что критично важно для научных приложений.'}, 'en': {'title': 'ChemMAS: Interpretable Recommendations for Chemical Reactions', 'desc': 'ChemMAS is a multi-agent system designed to enhance the recommendation of reaction conditions in chemistry by providing clear and interpretable justifications for its suggestions. It utilizes advanced reasoning techniques to break down the recommendation process into several components, including mechanistic grounding and constraint-aware debate among agents. This approach not only improves the accuracy of the recommendations but also ensures that each suggestion is supported by chemical knowledge and historical data. The results demonstrate that ChemMAS significantly outperforms existing methods, making it a valuable tool for researchers in the field of chemical science.'}, 'zh': {'title': 'ChemMAS：化学反应条件推荐的新范式', 'desc': 'ChemMAS是一种多智能体系统，通过提供可解释的推理，改善了反应条件推荐的准确性和可解释性。该系统将条件预测重新定义为基于证据的推理任务，分解为机制基础、多个通道回忆、约束感知的智能辩论和推理聚合。每个决策都有化学知识和检索先例支持的可解释理由。实验表明，ChemMAS在准确性上比领域特定的基线提高了20-35%，并且在Top-1准确性上比通用大型语言模型高出10-15%。'}}}, {'id': 'https://huggingface.co/papers/2510.03259', 'title': 'Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2510.03259', 'abstract': 'A training pipeline called MASA enhances meta-awareness in reasoning models, leading to improved accuracy and efficiency across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies on reasoning models explore the meta-awareness of language models, the ability to know how to think by itself. We argue that large reasoning models lack this meta-awareness property by proving severe misalignment between true rollouts and predicted meta information. We posit that aligning meta-prediction with true rollouts will lead to significant performance gains. To verify this hypothesis, we design a training pipeline that boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced meta-awareness directly translates to improved accuracy. Unlike existing meta-cognitive reasoning models, our method does not require external training sources but leverages self-generated signals to train meta-awareness. Moreover, our method enables efficient training by i) filtering out zero-variance prompts that are either trivial or unsolvable and ii) cutting off lengthy rollouts when they are unlikely to lead to correct answers. The results are inspiring: our strategy yields significant improvements in both accuracy and training efficiency on in-domain tasks and shows strong generalization to out-of-domain benchmarks. More specifically, our method can speed up GRPO training by over 1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on AIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with meta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 % boost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks spanning logical, scientific, and coding domains.', 'score': 42, 'issue_id': 6347, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': '1f147c5dbb45af14', 'authors': ['Yoonjeon Kim', 'Doohyuk Jang', 'Eunho Yang'], 'affiliations': ['AITRICS', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2510.03259.jpg', 'data': {'categories': ['#benchmark', '#training', '#optimization', '#math', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Метаосознанность через самовыравнивание ускоряет обучение', 'desc': 'Исследователи обнаружили, что большие reasoning-модели плохо понимают свой собственный процесс мышления - существует разрыв между реальными вычислениями и предсказанной мета-информацией. Они разработали метод MASA, который улучшает метаосознанность модели через самовыравнивание, используя сигналы из собственных вычислений без внешних источников данных. Метод позволяет эффективнее обучать модели, отфильтровывая тривиальные задачи и обрезая бесперспективные цепочки рассуждений. В результате достигается ускорение обучения в 1.28 раза и прирост точности до 19.3% на математических задачах с улучшенной обобщающей способностью на задачах из других областей.'}, 'en': {'title': 'Boosting Reasoning Models with Enhanced Meta-Awareness', 'desc': 'The paper introduces a training pipeline called MASA, which enhances meta-awareness in reasoning models, allowing them to better understand their own thought processes. It identifies a gap in current large reasoning models, where their predictions do not align well with actual outcomes, leading to inefficiencies. By aligning meta-predictions with true rollouts, MASA significantly improves both accuracy and training efficiency without needing external data sources. The results demonstrate that this approach not only accelerates training but also enhances performance across various benchmarks, showcasing its effectiveness in diverse reasoning tasks.'}, 'zh': {'title': '提升推理模型的元意识，提升准确性与效率', 'desc': '本文提出了一种名为MASA的训练管道，旨在增强推理模型的元意识，从而提高其在各种基准测试中的准确性和效率。研究表明，现有的大型推理模型缺乏元意识，导致真实结果与预测的元信息之间存在严重不一致。通过自我对齐的方法，MASA能够有效地提升元预测的准确性，进而提升模型的整体性能。该方法不依赖外部训练源，而是利用自生成信号进行训练，显著提高了训练效率和模型的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2510.07499', 'title': 'When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs', 'url': 'https://huggingface.co/papers/2510.07499', 'abstract': 'Thought templates enhance long-context language models by structuring evidence combination and guiding multi-hop inference, leading to consistent performance improvements across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent Long-Context Language Models (LCLMs) can process hundreds of thousands of tokens in a single prompt, enabling new opportunities for knowledge-intensive multi-hop reasoning by integrating large sets of retrieved documents or, in some cases, directly all necessary information. However, simply feeding more documents into the context window fails to capture how evidence should be connected. We address this gap with thought templates, which recast reasoning as reusable thought caches, derived from prior problem solving traces, structuring how evidence is combined and guiding multi-hop inference with factual documents. To keep these templates effective, we propose an update strategy that iteratively refines templates derived from training data through natural-language feedback. Across diverse benchmarks and LCLM families, our approach delivers consistent gains over strong baselines in both retrieval-based and retrieval-free settings. Furthermore, we show that optimized templates can be distilled into smaller open-source models, demonstrating its broad applicability and transparent reasoning reuse. We refer to our framework as Thought Template Augmented LCLMs (ToTAL).', 'score': 40, 'issue_id': 6346, 'pub_date': '2025-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': '1904c70a6b25b7ad', 'authors': ['Soyeong Jeong', 'Taehee Jung', 'Sung Ju Hwang', 'Joo-Kyung Kim', 'Dongyeop Kang'], 'affiliations': ['Amazon', 'KAIST', 'University of Minnesota'], 'pdf_title_img': 'assets/pdf/title_img/2510.07499.jpg', 'data': {'categories': ['#open_source', '#long_context', '#benchmark', '#multimodal', '#reasoning', '#training', '#small_models'], 'emoji': '🧩', 'ru': {'title': 'Шаблоны мышления для улучшения многошаговых рассуждений', 'desc': 'Исследователи предложили метод ToTAL, который улучшает работу языковых моделей с длинным контекстом при решении задач, требующих многошаговых рассуждений. Ключевая идея — использование «шаблонов мышления» (thought templates), которые структурируют процесс комбинирования фактов из документов и направляют логический вывод. Эти шаблоны создаются на основе предыдущих решений задач и итеративно улучшаются через обратную связь на естественном языке. Метод показывает стабильное улучшение производительности на различных бенчмарках и может быть перенесён в меньшие open-source модели, обеспечивая прозрачность процесса рассуждений.'}, 'en': {'title': 'Enhancing Reasoning with Thought Templates in LCLMs', 'desc': 'This paper introduces Thought Template Augmented Long-Context Language Models (ToTAL), which improve multi-hop reasoning by structuring how evidence is combined. The authors highlight that simply increasing the amount of input data does not effectively guide the model in connecting relevant information. By using thought templates, which are reusable structures derived from previous problem-solving experiences, the model can better organize and utilize evidence for reasoning tasks. The proposed method shows significant performance improvements across various benchmarks and can be adapted for smaller models, enhancing its usability in different applications.'}, 'zh': {'title': '思维模板：提升长上下文语言模型的推理能力', 'desc': '本文提出了一种名为思维模板的框架，旨在增强长上下文语言模型（LCLMs）的推理能力。思维模板通过结构化证据组合和指导多跳推理，帮助模型更好地连接和利用信息。我们的方法通过自然语言反馈不断优化模板，从而在多个基准测试中实现了显著的性能提升。此外，优化后的模板可以被提炼到更小的开源模型中，展示了其广泛的适用性和透明的推理重用。'}}}, {'id': 'https://huggingface.co/papers/2510.03222', 'title': 'Low-probability Tokens Sustain Exploration in Reinforcement Learning\n  with Verifiable Reward', 'url': 'https://huggingface.co/papers/2510.03222', 'abstract': 'Low-probability Regularization (Lp-Reg) enhances exploration in Reinforcement Learning with Verifiable Rewards (RLVR) by preserving valuable low-probability tokens, leading to improved performance in complex reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large Language Models in complex reasoning, yet its scalability is often hindered by a training bottleneck where performance plateaus as policy entropy collapses, signaling a loss of exploration. Previous methods typically address this by maintaining high policy entropy, yet the precise mechanisms that govern meaningful exploration have remained underexplored. Our analysis suggests that an unselective focus on entropy risks amplifying irrelevant tokens and destabilizing training. This paper investigates the exploration dynamics within RLVR and identifies a key issue: the gradual elimination of valuable low-probability exploratory tokens, which we term \\textit{reasoning sparks}. We find that while abundant in pre-trained models, these sparks are systematically extinguished during RLVR due to over-penalization, leading to a degeneracy in exploration. To address this, we introduce Low-probability Regularization (Lp-Reg). Its core mechanism regularizes the policy towards a heuristic proxy distribution. This proxy is constructed by filtering out presumed noise tokens and re-normalizing the distribution over the remaining candidates. The result is a less-noisy proxy where the probability of reasoning sparks is amplified, which then serves as a soft regularization target to shield these valuable tokens from elimination via KL divergence. Experiments show that Lp-Reg enables stable on-policy training for around 1,000 steps, a regime where baseline entropy-control methods collapse. This sustained exploration leads to state-of-the-art performance, achieving a 60.17% average accuracy on five math benchmarks, an improvement of 2.66% over prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg.', 'score': 34, 'issue_id': 6345, 'pub_date': '2025-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': 'f829f1939095d3f9', 'authors': ['Guanhua Huang', 'Tingqiang Xu', 'Mingze Wang', 'Qi Yi', 'Xue Gong', 'Siheng Li', 'Ruibin Xiong', 'Kejiao Li', 'Yuhao Jiang', 'Bo Zhou'], 'affiliations': ['LLM Department, Tencent', 'Peking University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.03222.jpg', 'data': {'categories': ['#training', '#rl', '#optimization', '#math', '#reasoning'], 'emoji': '✨', 'ru': {'title': 'Защита редких токенов для стабильного обучения с подкреплением', 'desc': 'Исследование выявило проблему в обучении с подкреплением для LLM: ценные редкие токены, которые авторы называют «искрами рассуждений», систематически исчезают в процессе тренировки. Традиционные методы поддержания высокой энтропии политики неэффективны, так как усиливают шумовые токены и дестабилизируют обучение. Предложенный метод Low-probability Regularization защищает важные низковероятностные токены через регуляризацию на отфильтрованное распределение, где их вероятность усилена. Это позволяет стабильно обучать модель в 1000 шагов и достичь точности 60.17% на математических задачах, что на 2.66% лучше предыдущих методов.'}, 'en': {'title': 'Enhancing Exploration with Low-Probability Regularization', 'desc': "This paper introduces Low-probability Regularization (Lp-Reg) to improve exploration in Reinforcement Learning with Verifiable Rewards (RLVR). It identifies that valuable low-probability tokens, or 'reasoning sparks', are often lost during training due to excessive penalties on policy entropy. Lp-Reg addresses this by creating a proxy distribution that emphasizes these low-probability tokens, allowing for better exploration and stability in training. The results demonstrate that Lp-Reg significantly enhances performance on complex reasoning tasks, achieving state-of-the-art accuracy on multiple benchmarks."}, 'zh': {'title': '低概率正则化：提升强化学习的探索能力', 'desc': '本文提出了一种名为低概率正则化（Lp-Reg）的方法，以增强强化学习中的探索能力，特别是在可验证奖励（RLVR）框架下。研究发现，在RLVR训练过程中，低概率的探索性标记（称为推理火花）会逐渐被消除，导致探索能力下降。Lp-Reg通过对策略进行正则化，保留这些有价值的低概率标记，从而改善复杂推理任务的表现。实验结果表明，使用Lp-Reg可以在训练过程中保持稳定的探索，显著提高模型在数学基准测试上的准确率。'}}}, {'id': 'https://huggingface.co/papers/2510.08240', 'title': 'The Alignment Waltz: Jointly Training Agents to Collaborate for Safety', 'url': 'https://huggingface.co/papers/2510.08240', 'abstract': "WaltzRL, a multi-agent reinforcement learning framework, improves LLM safety and helpfulness by collaboratively training a conversation agent and a feedback agent, reducing unsafe responses and overrefusals.  \t\t\t\t\tAI-generated summary \t\t\t\t Harnessing the power of LLMs requires a delicate dance between being helpful and harmless. This creates a fundamental tension between two competing challenges: vulnerability to adversarial attacks that elicit unsafe content, and a tendency for overrefusal on benign but sensitive prompts. Current approaches often navigate this dance with safeguard models that completely reject any content that contains unsafe portions. This approach cuts the music entirely-it may exacerbate overrefusals and fails to provide nuanced guidance for queries it refuses. To teach models a more coordinated choreography, we propose WaltzRL, a novel multi-agent reinforcement learning framework that formulates safety alignment as a collaborative, positive-sum game. WaltzRL jointly trains a conversation agent and a feedback agent, where the latter is incentivized to provide useful suggestions that improve the safety and helpfulness of the conversation agent's responses. At the core of WaltzRL is a Dynamic Improvement Reward (DIR) that evolves over time based on how well the conversation agent incorporates the feedback. At inference time, unsafe or overrefusing responses from the conversation agent are improved rather than discarded. The feedback agent is deployed together with the conversation agent and only engages adaptively when needed, preserving helpfulness and low latency on safe queries. Our experiments, conducted across five diverse datasets, demonstrate that WaltzRL significantly reduces both unsafe responses (e.g., from 39.0% to 4.6% on WildJailbreak) and overrefusals (from 45.3% to 9.9% on OR-Bench) compared to various baselines. By enabling the conversation and feedback agents to co-evolve and adaptively apply feedback, WaltzRL enhances LLM safety without degrading general capabilities, thereby advancing the Pareto front between helpfulness and harmlessness.", 'score': 33, 'issue_id': 6346, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'ffcc462077a6e91e', 'authors': ['Jingyu Zhang', 'Haozhu Wang', 'Eric Michael Smith', 'Sid Wang', 'Amr Sharaf', 'Mahesh Pasupuleti', 'Benjamin Van Durme', 'Daniel Khashabi', 'Jason Weston', 'Hongyuan Zhan'], 'affiliations': ['Johns Hopkins University', 'Meta Superintelligence Labs'], 'pdf_title_img': 'assets/pdf/title_img/2510.08240.jpg', 'data': {'categories': ['#rl', '#security', '#alignment', '#agents', '#rlhf'], 'emoji': '💃', 'ru': {'title': 'Танцуя между безопасностью и полезностью: два AI-агента учатся вместе', 'desc': 'WaltzRL - это новый фреймворк для обучения языковых моделей безопасному поведению через multi-agent reinforcement learning. В основе лежит совместное обучение двух агентов: один ведет диалог, а второй дает обратную связь для улучшения ответов. Вместо полного отклонения потенциально небезопасных запросов, система адаптивно улучшает ответы, используя динамическую систему наград. Результаты показывают резкое снижение как небезопасных ответов (с 39% до 4.6%), так и избыточных отказов на безобидные запросы (с 45.3% до 9.9%).'}, 'en': {'title': 'WaltzRL: Harmonizing Safety and Helpfulness in LLMs', 'desc': "WaltzRL is a multi-agent reinforcement learning framework designed to enhance the safety and helpfulness of large language models (LLMs). It trains a conversation agent alongside a feedback agent, which provides constructive suggestions to improve the conversation agent's responses. This approach reduces the occurrence of unsafe outputs and minimizes unnecessary refusals by allowing the conversation agent to adaptively incorporate feedback rather than discarding responses. The framework's Dynamic Improvement Reward (DIR) evolves over time, ensuring that both agents work together to achieve a balance between being helpful and harmless."}, 'zh': {'title': 'WaltzRL：提升对话智能体的安全性与有用性', 'desc': 'WaltzRL是一种多智能体强化学习框架，旨在提高大型语言模型（LLM）的安全性和有用性。它通过协同训练对话智能体和反馈智能体，减少不安全的回复和过度拒绝的情况。WaltzRL的核心是动态改进奖励（DIR），根据对话智能体如何整合反馈而不断演变。在实验中，WaltzRL显著降低了不安全回复和过度拒绝的比例，提升了模型的安全性而不影响其整体能力。'}}}, {'id': 'https://huggingface.co/papers/2510.08191', 'title': 'Training-Free Group Relative Policy Optimization', 'url': 'https://huggingface.co/papers/2510.08191', 'abstract': 'Training-Free GRPO enhances LLM agent performance in specialized domains by learning experiential knowledge as a token prior without parameter updates, improving out-of-domain tasks with minimal data.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Model (LLM) agents have demonstrated their promising general capabilities. However, their performance in specialized real-world domains often degrades due to challenges in effectively integrating external tools and specific prompting strategies. While methods like agentic reinforcement learning have been proposed to address this, they typically rely on costly parameter updates, for example, through a process that uses Supervised Fine-Tuning (SFT) followed by a Reinforcement Learning (RL) phase with Group Relative Policy Optimization (GRPO) to alter the output distribution. However, we argue that LLMs can achieve a similar effect on the output distribution by learning experiential knowledge as a token prior, which is a far more lightweight approach that not only addresses practical data scarcity but also avoids the common issue of overfitting. To this end, we propose Training-Free Group Relative Policy Optimization (Training-Free GRPO), a cost-effective solution that enhances LLM agent performance without any parameter updates. Our method leverages the group relative semantic advantage instead of numerical ones within each group of rollouts, iteratively distilling high-quality experiential knowledge during multi-epoch learning on a minimal ground-truth data. Such knowledge serves as the learned token prior, which is seamlessly integrated during LLM API calls to guide model behavior. Experiments on mathematical reasoning and web searching tasks demonstrate that Training-Free GRPO, when applied to DeepSeek-V3.1-Terminus, significantly improves out-of-domain performance. With just a few dozen training samples, Training-Free GRPO outperforms fine-tuned small LLMs with marginal training data and cost.', 'score': 30, 'issue_id': 6345, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '2727834df48a0b2b', 'authors': ['Yuzheng Cai', 'Siqi Cai', 'Yuchen Shi', 'Zihan Xu', 'Lichao Chen', 'Yulei Qin', 'Xiaoyu Tan', 'Gang Li', 'Zongyi Li', 'Haojia Lin', 'Yong Mao', 'Ke Li', 'Xing Sun'], 'affiliations': ['Tencent', 'Youtu-Agent Team'], 'pdf_title_img': 'assets/pdf/title_img/2510.08191.jpg', 'data': {'categories': ['#training', '#rl', '#reasoning', '#optimization', '#transfer_learning', '#agents'], 'emoji': '🎯', 'ru': {'title': 'Обучение агентов без обновления параметров через опытное знание', 'desc': 'Статья представляет метод Training-Free GRPO, который улучшает работу LLM-агентов в специализированных областях без обновления параметров модели. Вместо дорогостоящего файн-тюнинга метод обучает модель опытному знанию в виде токен-приоров, используя групповое относительное семантическое преимущество из нескольких запусков. Такой подход решает проблему дефицита данных и избегает переобучения, интегрируя полученное знание непосредственно при API-вызовах модели. Эксперименты на задачах математического рассуждения и веб-поиска показали, что метод значительно превосходит файн-тюненные малые LLM, используя всего несколько десятков обучающих примеров.'}, 'en': {'title': 'Boosting LLMs with Lightweight Knowledge Learning', 'desc': 'This paper introduces Training-Free Group Relative Policy Optimization (Training-Free GRPO), a novel approach to enhance the performance of Large Language Model (LLM) agents in specialized domains without requiring parameter updates. Instead of traditional methods that rely on costly fine-tuning and reinforcement learning, this method learns experiential knowledge as a token prior, which helps improve model behavior with minimal data. The approach focuses on leveraging group relative semantic advantages to distill high-quality knowledge iteratively, addressing issues like data scarcity and overfitting. Experiments show that Training-Free GRPO significantly boosts out-of-domain performance in tasks like mathematical reasoning and web searching, outperforming fine-tuned models with limited training samples.'}, 'zh': {'title': '无训练优化，提升LLM表现！', 'desc': '本文提出了一种名为无训练组相对策略优化（Training-Free GRPO）的方法，旨在提升大型语言模型（LLM）在特定领域的表现。该方法通过学习经验知识作为令牌先验，而无需进行参数更新，从而有效应对数据稀缺的问题。与传统的强化学习方法相比，Training-Free GRPO在多轮学习中提炼高质量的经验知识，避免了过拟合的常见问题。实验结果表明，该方法在数学推理和网络搜索任务中显著提高了LLM的跨领域性能。'}}}, {'id': 'https://huggingface.co/papers/2510.07242', 'title': "Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense", 'url': 'https://huggingface.co/papers/2510.07242', 'abstract': 'HERO, a reinforcement learning framework, combines verifier signals with reward-model scores to enhance reasoning in large language models, outperforming both RM-only and verifier-only methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Post-training for reasoning of large language models (LLMs) increasingly relies on verifiable rewards: deterministic checkers that provide 0-1 correctness signals. While reliable, such binary feedback is brittle--many tasks admit partially correct or alternative answers that verifiers under-credit, and the resulting all-or-nothing supervision limits learning. Reward models offer richer, continuous feedback, which can serve as a complementary supervisory signal to verifiers. We introduce HERO (Hybrid Ensemble Reward Optimization), a reinforcement learning framework that integrates verifier signals with reward-model scores in a structured way. HERO employs stratified normalization to bound reward-model scores within verifier-defined groups, preserving correctness while refining quality distinctions, and variance-aware weighting to emphasize challenging prompts where dense signals matter most. Across diverse mathematical reasoning benchmarks, HERO consistently outperforms RM-only and verifier-only baselines, with strong gains on both verifiable and hard-to-verify tasks. Our results show that hybrid reward design retains the stability of verifiers while leveraging the nuance of reward models to advance reasoning.', 'score': 27, 'issue_id': 6345, 'pub_date': '2025-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': '4752a7b26fdfbe7b', 'authors': ['Leitian Tao', 'Ilia Kulikov', 'Swarnadeep Saha', 'Tianlu Wang', 'Jing Xu', 'Yixuan Li', 'Jason E Weston', 'Ping Yu'], 'affiliations': ['FAIR at Meta', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2510.07242.jpg', 'data': {'categories': ['#training', '#rl', '#rlhf', '#optimization', '#reasoning'], 'emoji': '⚖️', 'ru': {'title': 'Лучшее из двух миров: гибридные награды для обучения рассуждению', 'desc': 'Статья представляет HERO — фреймворк для reinforcement learning, который комбинирует бинарные сигналы от верификаторов (0 или 1 за правильность) с непрерывными оценками от reward models. Проблема традиционных верификаторов в том, что они дают слишком грубую обратную связь по принципу «всё или ничего», не учитывая частично правильные ответы. HERO использует стратифицированную нормализацию для ограничения scores внутри групп, определённых верификатором, и variance-aware взвешивание для фокуса на сложных примерах. Эксперименты показывают, что гибридный подход превосходит методы, использующие только reward models или только верификаторы, особенно на задачах математического reasoning.'}, 'en': {'title': 'HERO: Enhancing Reasoning with Hybrid Rewards', 'desc': 'HERO is a reinforcement learning framework that improves reasoning in large language models by combining verifier signals with reward-model scores. Verifiers provide binary feedback, which can be too strict and limit learning, while reward models offer richer, continuous feedback. HERO uses stratified normalization to ensure that reward-model scores align with verifier-defined correctness, enhancing the quality of learning. The framework demonstrates superior performance on various reasoning tasks compared to using either reward models or verifiers alone.'}, 'zh': {'title': 'HERO：混合奖励优化，提升推理能力', 'desc': 'HERO是一种强化学习框架，它将验证器信号与奖励模型分数结合起来，以增强大型语言模型的推理能力。传统的验证器提供的二元反馈虽然可靠，但在许多任务中可能会低估部分正确或替代答案。HERO通过分层归一化和方差感知加权，确保奖励模型的分数在验证器定义的组内保持稳定，同时提高了对困难提示的重视。实验结果表明，HERO在多种数学推理基准测试中优于仅使用奖励模型或验证器的方法，显示出混合奖励设计的优势。'}}}, {'id': 'https://huggingface.co/papers/2510.07172', 'title': 'NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM\n  Agents', 'url': 'https://huggingface.co/papers/2510.07172', 'abstract': 'NewtonBench is a benchmark for scientific law discovery that addresses scalability, scientific relevance, and memorization resistance by using metaphysical shifts and interactive model discovery.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models are emerging as powerful tools for scientific law discovery, a foundational challenge in AI-driven science. However, existing benchmarks for this task suffer from a fundamental methodological trilemma, forcing a trade-off between scientific relevance, scalability, and resistance to memorization. Furthermore, they oversimplify discovery as static function fitting, failing to capture the authentic scientific process of uncovering embedded laws through the interactive exploration of complex model systems. To address these critical gaps, we introduce NewtonBench, a benchmark comprising 324 scientific law discovery tasks across 12 physics domains. Our design mitigates the evaluation trilemma by using metaphysical shifts - systematic alterations of canonical laws - to generate a vast suite of problems that are scalable, scientifically relevant, and memorization-resistant. Moreover, we elevate the evaluation from static function fitting to interactive model discovery, requiring agents to experimentally probe simulated complex systems to uncover hidden principles. Our extensive experiment reveals a clear but fragile capability for discovery in frontier LLMs: this ability degrades precipitously with increasing system complexity and exhibits extreme sensitivity to observational noise. Notably, we uncover a paradoxical effect of tool assistance: providing a code interpreter can hinder more capable models by inducing a premature shift from exploration to exploitation, causing them to satisfice on suboptimal solutions. These results demonstrate that robust, generalizable discovery in complex, interactive environments remains the core challenge. By providing a scalable, robust, and scientifically authentic testbed, NewtonBench offers a crucial tool for measuring true progress and guiding the development of next-generation AI agents capable of genuine scientific discovery.', 'score': 27, 'issue_id': 6345, 'pub_date': '2025-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': '9ee73390b8a846fc', 'authors': ['Tianshi Zheng', 'Kelvin Kiu-Wai Tam', 'Newt Hue-Nam K. Nguyen', 'Baixuan Xu', 'Zhaowei Wang', 'Jiayang Cheng', 'Hong Ting Tsang', 'Weiqi Wang', 'Jiaxin Bai', 'Tianqing Fang', 'Yangqiu Song', 'Ginny Y. Wong', 'Simon See'], 'affiliations': ['NVIDIA', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2510.07172.jpg', 'data': {'categories': ['#science', '#benchmark', '#agents'], 'emoji': '🔬', 'ru': {'title': 'Научное открытие законов природы через интерактивное исследование', 'desc': 'NewtonBench — это benchmark для оценки способности LLM открывать научные законы, включающий 324 задачи из 12 областей физики. Вместо простого подбора функций модели должны интерактивно исследовать сложные симуляции систем, чтобы обнаружить скрытые принципы. Эксперименты показали, что современные LLM обладают хрупкой способностью к открытиям: их эффективность резко падает при усложнении систем и наличии шума в данных. Парадоксально, но предоставление инструментов вроде code interpreter может навредить продвинутым моделям, заставляя их слишком рано переключаться с исследования на эксплуатацию найденных решений.'}, 'en': {'title': 'NewtonBench: Advancing AI in Scientific Law Discovery', 'desc': 'NewtonBench is a new benchmark designed to improve the process of discovering scientific laws using AI. It addresses key issues like scalability, scientific relevance, and the risk of models simply memorizing data instead of learning. By introducing metaphysical shifts, it creates a wide range of tasks that require interactive exploration rather than just fitting functions to data. The findings highlight the challenges faced by large language models in complex environments, emphasizing the need for better tools to support genuine scientific discovery.'}, 'zh': {'title': 'NewtonBench：科学定律发现的新基准', 'desc': 'NewtonBench是一个用于科学定律发现的基准测试，旨在解决可扩展性、科学相关性和抵抗记忆化的问题。它通过使用形而上学的转变和互动模型发现的方法，提供了324个科学定律发现任务，涵盖12个物理领域。与现有基准不同，NewtonBench强调从静态函数拟合转向互动模型发现，要求智能体通过实验探测复杂系统以揭示隐藏的原则。我们的实验结果显示，尽管前沿的大型语言模型在发现能力上表现出色，但在系统复杂性增加和观察噪声影响下，其能力会显著下降。'}}}, {'id': 'https://huggingface.co/papers/2510.08551', 'title': 'ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D\n  Reconstruction with Structured Scene Representation', 'url': 'https://huggingface.co/papers/2510.08551', 'abstract': 'ARTDECO combines feed-forward models and SLAM pipelines for efficient and accurate 3D reconstruction from monocular images.  \t\t\t\t\tAI-generated summary \t\t\t\t On-the-fly 3D reconstruction from monocular image sequences is a long-standing challenge in computer vision, critical for applications such as real-to-sim, AR/VR, and robotics. Existing methods face a major tradeoff: per-scene optimization yields high fidelity but is computationally expensive, whereas feed-forward foundation models enable real-time inference but struggle with accuracy and robustness. In this work, we propose ARTDECO, a unified framework that combines the efficiency of feed-forward models with the reliability of SLAM-based pipelines. ARTDECO uses 3D foundation models for pose estimation and point prediction, coupled with a Gaussian decoder that transforms multi-scale features into structured 3D Gaussians. To sustain both fidelity and efficiency at scale, we design a hierarchical Gaussian representation with a LoD-aware rendering strategy, which improves rendering fidelity while reducing redundancy. Experiments on eight diverse indoor and outdoor benchmarks show that ARTDECO delivers interactive performance comparable to SLAM, robustness similar to feed-forward systems, and reconstruction quality close to per-scene optimization, providing a practical path toward on-the-fly digitization of real-world environments with both accurate geometry and high visual fidelity. Explore more demos on our project page: https://city-super.github.io/artdeco/.', 'score': 25, 'issue_id': 6346, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'd59c8bdd425eb9be', 'authors': ['Guanghao Li', 'Kerui Ren', 'Linning Xu', 'Zhewen Zheng', 'Changjian Jiang', 'Xin Gao', 'Bo Dai', 'Jian Pu', 'Mulin Yu', 'Jiangmiao Pang'], 'affiliations': ['Carnegie Mellon University', 'Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'The University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.08551.jpg', 'data': {'categories': ['#3d', '#cv', '#robotics'], 'emoji': '🏛️', 'ru': {'title': 'Быстрая и точная 3D-реконструкция: лучшее из двух миров', 'desc': 'ARTDECO - это новый фреймворк для 3D-реконструкции из монокулярных изображений в реальном времени, который объединяет эффективность feed-forward моделей с надёжностью SLAM-систем. Метод использует 3D foundation models для оценки позы камеры и предсказания точек, а затем применяет Gaussian decoder для преобразования мультимасштабных признаков в структурированные 3D Gaussians. Для масштабируемости авторы предложили иерархическое представление с LoD-стратегией рендеринга, что повышает качество при снижении избыточности. Эксперименты показывают, что ARTDECO достигает интерактивной производительности как у SLAM, робастности как у feed-forward систем и качества реконструкции близкого к per-scene оптимизации.'}, 'en': {'title': 'ARTDECO: Bridging Efficiency and Accuracy in 3D Reconstruction', 'desc': "ARTDECO is a novel framework that integrates feed-forward models with SLAM (Simultaneous Localization and Mapping) techniques to achieve efficient and precise 3D reconstruction from single images. It addresses the challenge of balancing computational efficiency and reconstruction accuracy, which has been a significant issue in computer vision. By utilizing 3D foundation models for pose estimation and point prediction, along with a Gaussian decoder for structured 3D representation, ARTDECO enhances both fidelity and efficiency. The framework's hierarchical Gaussian representation and Level of Detail (LoD) rendering strategy allow for high-quality visual outputs while minimizing redundancy, making it suitable for real-time applications in various environments."}, 'zh': {'title': 'ARTDECO：高效准确的3D重建新方法', 'desc': 'ARTDECO是一种结合前馈模型和SLAM管道的统一框架，旨在从单目图像中高效、准确地进行3D重建。该方法解决了现有技术在每个场景优化与实时推理之间的权衡问题，提供了更高的准确性和鲁棒性。ARTDECO利用3D基础模型进行姿态估计和点预测，并通过高斯解码器将多尺度特征转化为结构化的3D高斯分布。实验结果表明，ARTDECO在多个基准测试中表现出与SLAM相当的交互性能和与前馈系统相似的鲁棒性，同时重建质量接近每场景优化，展示了在真实环境中进行即时数字化的可行性。'}}}, {'id': 'https://huggingface.co/papers/2510.08483', 'title': 'DeepPrune: Parallel Scaling without Inter-trace Redundancy', 'url': 'https://huggingface.co/papers/2510.08483', 'abstract': 'DeepPrune, a novel framework using dynamic pruning and a specialized judge model, significantly reduces computational inefficiency in parallel scaling of large language models by pruning redundant reasoning traces.  \t\t\t\t\tAI-generated summary \t\t\t\t Parallel scaling has emerged as a powerful paradigm to enhance reasoning capabilities in large language models (LLMs) by generating multiple Chain-of-Thought (CoT) traces simultaneously. However, this approach introduces significant computational inefficiency due to inter-trace redundancy -- our analysis reveals that over 80% of parallel reasoning traces yield identical final answers, representing substantial wasted computation. To address this critical efficiency bottleneck, we propose DeepPrune, a novel framework that enables efficient parallel scaling through dynamic pruning. Our method features a specialized judge model trained with focal loss and oversampling techniques to accurately predict answer equivalence from partial reasoning traces which realizes 0.87 AUROC on equivalence prediction, combined with an online greedy clustering algorithm that dynamically prunes redundant paths while preserving answer diversity. Comprehensive evaluations across three challenging benchmarks (AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that DeepPrune achieves remarkable token reduction by over 80% compared to conventional consensus sampling on most cases, while maintaining competitive accuracy within 3 percentage points. Our work establishes a new standard for efficient parallel reasoning, making high-performance reasoning more efficient. Our code and data are here: https://deepprune.github.io/', 'score': 22, 'issue_id': 6344, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '13be68ae86d17de1', 'authors': ['Shangqing Tu', 'Yaxuan Li', 'Yushi Bai', 'Lei Hou', 'Juanzi Li'], 'affiliations': ['ShanghaiTech University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.08483.jpg', 'data': {'categories': ['#benchmark', '#training', '#inference', '#reasoning', '#optimization'], 'emoji': '✂️', 'ru': {'title': 'Умная обрезка избыточных рассуждений в параллельных LLM', 'desc': 'DeepPrune — это новый фреймворк для эффективного параллельного скейлинга LLM, который решает проблему избыточности при генерации множественных рассуждений. Исследователи обнаружили, что более 80% параллельных цепочек рассуждений приводят к одинаковым ответам, что означает огромные вычислительные потери. Система использует специальную judge-модель для предсказания эквивалентности ответов и динамически удаляет избыточные пути рассуждений через онлайн кластеризацию. DeepPrune сокращает количество токенов более чем на 80% при сохранении точности в пределах 3 процентных пунктов на сложных бенчмарках.'}, 'en': {'title': 'Efficient Reasoning with DeepPrune: Prune the Redundancy!', 'desc': 'DeepPrune is a new framework designed to improve the efficiency of large language models by reducing unnecessary computations during parallel reasoning. It identifies and prunes redundant reasoning paths that often lead to the same answers, which can waste over 80% of computational resources. The framework employs a specialized judge model that predicts when reasoning traces are equivalent, allowing for dynamic pruning of these redundant paths. Evaluations show that DeepPrune can significantly reduce the number of tokens used while maintaining high accuracy, setting a new benchmark for efficient reasoning in AI models.'}, 'zh': {'title': 'DeepPrune：高效并行推理的新标准', 'desc': 'DeepPrune是一个新颖的框架，通过动态剪枝和专门的判断模型，显著减少了大语言模型在并行扩展中的计算低效。该方法解决了并行推理中存在的冗余问题，分析显示超过80%的推理轨迹产生相同的最终答案，造成了大量的计算浪费。DeepPrune通过训练具有焦点损失和过采样技术的判断模型，准确预测部分推理轨迹的答案等价性，并结合在线贪婪聚类算法动态剪除冗余路径。经过在多个基准测试上的全面评估，DeepPrune在大多数情况下实现了超过80%的令牌减少，同时保持了与传统共识采样相近的准确性。'}}}, {'id': 'https://huggingface.co/papers/2510.08308', 'title': 'First Try Matters: Revisiting the Role of Reflection in Reasoning Models', 'url': 'https://huggingface.co/papers/2510.08308', 'abstract': "Analysis of reflective behaviors in reasoning models shows that reflections primarily confirm initial answers, and training with more reflections improves first-answer correctness; a question-aware early-stopping method reduces unnecessary reflections and tokens with minimal accuracy loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have recently demonstrated significant gains in reasoning ability, often attributed to their capacity to generate longer chains of thought and engage in reflective reasoning. However, the contribution of reflections to performance improvement remains unclear. In this paper, we systematically analyze the rollouts of eight reasoning models on five mathematical datasets. We focus on reflective behaviours where the model has already produced an answer but continues reflecting before finalizing its output. Our analysis reveals that reflections are predominantly confirmatory and rarely alter the model's initial answer, a pattern consistent across models and datasets. To understand the role of reflections in training, we construct supervised fine-tuning (SFT) datasets with varying amounts of reflection steps. We observe that training models on rollouts with more reflection steps primarily enhances first-answer correctness rather than the ability to correct initially wrong answers through reflections. This motivates us to propose a question-aware early-stopping method that enhances inference-time token efficiency by stopping the reasoning process once a few plausible candidate answers are generated, thereby reducing unnecessary reflection steps. Motivated by this, we further propose to dynamically truncate the reflections after a candidate answer has appeared during generation, which reduces reasoning tokens by 24.5% across five mathematical datasets, within a 2.9% drop in accuracy.", 'score': 22, 'issue_id': 6344, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '4a9143a437581621', 'authors': ['Liwei Kang', 'Yue Deng', 'Yao Xiao', 'Zhanfeng Mo', 'Wee Sun Lee', 'Lidong Bing'], 'affiliations': ['MiroMind AI', 'National University of Singapore', 'Singapore University of Technology and Design'], 'pdf_title_img': 'assets/pdf/title_img/2510.08308.jpg', 'data': {'categories': ['#training', '#math', '#inference', '#data', '#reasoning', '#optimization'], 'emoji': '🪞', 'ru': {'title': 'Рефлексии LLM подтверждают, а не исправляют: оптимизация через раннюю остановку', 'desc': 'Исследование показывает, что рефлексии в reasoning-моделях в основном подтверждают первоначальные ответы, а не исправляют их. Обучение на данных с большим количеством рефлексий улучшает корректность первого ответа, но не способность к самокоррекции. Авторы предлагают метод ранней остановки генерации после получения нескольких правдоподобных ответов. Этот подход сокращает количество токенов на 24.5% при падении точности всего на 2.9%.'}, 'en': {'title': 'Enhancing Reasoning Efficiency with Reflective Training', 'desc': 'This paper investigates how reflective behaviors in reasoning models affect their performance, particularly in confirming initial answers. It finds that while reflections often do not change the first answer, training with more reflection steps improves the correctness of these initial answers. The authors introduce a question-aware early-stopping method to minimize unnecessary reflections and reduce token usage during inference. This method effectively decreases reasoning tokens by 24.5% with only a slight accuracy drop of 2.9%.'}, 'zh': {'title': '反思提升初始答案的正确性', 'desc': '本文分析了推理模型中的反思行为，发现反思主要是确认初始答案，而不是改变它。通过对八个推理模型在五个数学数据集上的表现进行系统分析，我们发现更多的反思步骤可以提高初始答案的正确性。我们提出了一种基于问题的早停方法，可以在生成几个合理候选答案后停止推理，从而减少不必要的反思步骤。实验结果表明，这种方法在减少推理令牌的同时，仅有轻微的准确性下降。'}}}, {'id': 'https://huggingface.co/papers/2510.08211', 'title': 'LLMs Learn to Deceive Unintentionally: Emergent Misalignment in\n  Dishonesty from Misaligned Samples to Biased Human-AI Interactions', 'url': 'https://huggingface.co/papers/2510.08211', 'abstract': 'LLMs finetuned on misaligned data exhibit dishonest behavior, which can be exacerbated in downstream tasks and human-AI interactions.  \t\t\t\t\tAI-generated summary \t\t\t\t Previous research has shown that LLMs finetuned on malicious or incorrect completions within narrow domains (e.g., insecure code or incorrect medical advice) can become broadly misaligned to exhibit harmful behaviors, which is called emergent misalignment. In this work, we investigate whether this phenomenon can extend beyond safety behaviors to a broader spectrum of dishonesty and deception under high-stakes scenarios (e.g., lying under pressure and deceptive behavior). To explore this, we finetune open-sourced LLMs on misaligned completions across diverse domains. Experimental results demonstrate that LLMs show broadly misaligned behavior in dishonesty. Additionally, we further explore this phenomenon in a downstream combined finetuning setting, and find that introducing as little as 1% of misalignment data into a standard downstream task is sufficient to decrease honest behavior over 20%. Furthermore, we consider a more practical human-AI interaction environment where we simulate both benign and biased users to interact with the assistant LLM. Notably, we find that the assistant can be misaligned unintentionally to exacerbate its dishonesty with only 10% biased user population. In summary, we extend the study of emergent misalignment to the domain of dishonesty and deception under high-stakes scenarios, and demonstrate that this risk arises not only through direct finetuning, but also in downstream mixture tasks and practical human-AI interactions.', 'score': 21, 'issue_id': 6346, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '7804cc776555cb53', 'authors': ['XuHao Hu', 'Peng Wang', 'Xiaoya Lu', 'Dongrui Liu', 'Xuanjing Huang', 'Jing Shao'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2510.08211.jpg', 'data': {'categories': ['#hallucinations', '#data', '#training', '#alignment', '#ethics', '#rlhf'], 'emoji': '🎭', 'ru': {'title': 'Как малая доля плохих данных делает AI нечестным', 'desc': 'Исследование показывает, что языковые модели (LLM), дообученные на некорректных данных, начинают проявлять нечестное поведение в самых разных ситуациях, даже если обучение проводилось только в узких областях. Всего 1% таких данных в стандартном дообучении может снизить честность модели более чем на 20%. Особенно тревожно то, что модели могут становиться нечестными непреднамеренно - даже при взаимодействии всего с 10% предвзятых пользователей. Таким образом, риск возникновения обманного поведения AI существует не только при прямом обучении на плохих данных, но и в обычных практических сценариях использования.'}, 'en': {'title': 'Misalignment Leads to Dishonesty in AI Models', 'desc': 'This paper explores how large language models (LLMs) can develop dishonest behaviors when they are finetuned on misaligned data, such as incorrect or harmful information. The authors demonstrate that even a small amount of misaligned data can significantly reduce the honesty of LLMs, particularly in high-stakes situations. They also investigate how these models behave in real-world interactions with users, showing that a small percentage of biased users can further exacerbate dishonesty. Overall, the study highlights the risks of emergent misalignment in LLMs, emphasizing the need for careful data curation and user interaction design.'}, 'zh': {'title': '不对齐数据导致AI不诚实行为的风险', 'desc': '本研究探讨了在不对齐数据上微调的大型语言模型（LLMs）如何表现出不诚实行为。研究发现，当这些模型在多种领域的错误完成上进行微调时，它们会在高风险场景中表现出更广泛的不诚实和欺骗行为。实验结果表明，即使仅引入1%的不对齐数据，也会导致模型的诚实行为下降超过20%。此外，在模拟人机交互环境中，发现当用户中有10%的偏见用户时，助手模型的不诚实行为会被进一步加剧。'}}}, {'id': 'https://huggingface.co/papers/2510.08143', 'title': 'UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video\n  Super-Resolution', 'url': 'https://huggingface.co/papers/2510.08143', 'abstract': 'UniMMVSR is a unified generative video super-resolution framework that incorporates hybrid-modal conditions, including text, images, and videos, within a latent video diffusion model, achieving superior detail and conformity to multi-modal conditions.  \t\t\t\t\tAI-generated summary \t\t\t\t Cascaded video super-resolution has emerged as a promising technique for decoupling the computational burden associated with generating high-resolution videos using large foundation models. Existing studies, however, are largely confined to text-to-video tasks and fail to leverage additional generative conditions beyond text, which are crucial for ensuring fidelity in multi-modal video generation. We address this limitation by presenting UniMMVSR, the first unified generative video super-resolution framework to incorporate hybrid-modal conditions, including text, images, and videos. We conduct a comprehensive exploration of condition injection strategies, training schemes, and data mixture techniques within a latent video diffusion model. A key challenge was designing distinct data construction and condition utilization methods to enable the model to precisely utilize all condition types, given their varied correlations with the target video. Our experiments demonstrate that UniMMVSR significantly outperforms existing methods, producing videos with superior detail and a higher degree of conformity to multi-modal conditions. We also validate the feasibility of combining UniMMVSR with a base model to achieve multi-modal guided generation of 4K video, a feat previously unattainable with existing techniques.', 'score': 18, 'issue_id': 6345, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '141d62a733cd05ae', 'authors': ['Shian Du', 'Menghan Xia', 'Chang Liu', 'Quande Liu', 'Xintao Wang', 'Pengfei Wan', 'Xiangyang Ji'], 'affiliations': ['Huazhong University of Science and Technology', 'Kling Team, Kuaishou Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.08143.jpg', 'data': {'categories': ['#diffusion', '#multimodal', '#video'], 'emoji': '🎬', 'ru': {'title': 'Мультимодальный апскейлинг видео до 4K разрешения', 'desc': 'UniMMVSR — это универсальная генеративная система для апскейлинга видео, которая работает с гибридными условиями: текстом, изображениями и видео внутри latent diffusion модели. Авторы решили проблему каскадной генерации высокого разрешения, позволяя учитывать все типы мультимодальных условий одновременно, что критично для точности генерации. Они провели комплексное исследование стратегий инъекции условий, схем обучения и методов смешивания данных для каждой модальности. В результате UniMMVSR превосходит существующие методы и впервые позволяет генерировать 4K видео с учётом мультимодальных условий.'}, 'en': {'title': 'UniMMVSR: Elevating Video Quality with Multi-Modal Inputs', 'desc': 'UniMMVSR is a novel framework for enhancing video quality by generating high-resolution videos from various input types, such as text, images, and videos. It utilizes a latent video diffusion model to effectively combine these different modalities, ensuring that the generated videos maintain high detail and fidelity. The framework addresses previous limitations by exploring various strategies for integrating multiple conditions during training, allowing for better utilization of diverse data types. Experiments show that UniMMVSR outperforms existing methods, achieving impressive results in multi-modal video generation, including the ability to produce 4K videos.'}, 'zh': {'title': '统一生成视频超分辨率，提升多模态一致性', 'desc': 'UniMMVSR是一个统一的生成视频超分辨率框架，能够结合文本、图像和视频等多种模态条件。该框架使用潜在视频扩散模型，显著提高了生成视频的细节和多模态条件的符合度。我们探索了条件注入策略、训练方案和数据混合技术，以便模型能够准确利用不同类型的条件。实验结果表明，UniMMVSR在生成视频的细节和多模态一致性方面，明显优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2510.08565', 'title': 'NaViL: Rethinking Scaling Properties of Native Multimodal Large Language\n  Models under Data Constraints', 'url': 'https://huggingface.co/papers/2510.08565', 'abstract': 'Native end-to-end training of Multimodal Large Language Models (MLLMs) achieves competitive performance with a balanced design and scaling relationship between visual encoders and LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Compositional training has been the de-facto paradigm in existing Multimodal Large Language Models (MLLMs), where pre-trained vision encoders are connected with pre-trained LLMs through continuous multimodal pre-training. However, the multimodal scaling property of this paradigm remains difficult to explore due to the separated training. In this paper, we focus on the native training of MLLMs in an end-to-end manner and systematically study its design space and scaling property under a practical setting, i.e., data constraint. Through careful study of various choices in MLLM, we obtain the optimal meta-architecture that best balances performance and training cost. After that, we further explore the scaling properties of the native MLLM and indicate the positively correlated scaling relationship between visual encoders and LLMs. Based on these findings, we propose a native MLLM called NaViL, combined with a simple and cost-effective recipe. Experimental results on 14 multimodal benchmarks confirm the competitive performance of NaViL against existing MLLMs. Besides that, our findings and results provide in-depth insights for the future study of native MLLMs.', 'score': 17, 'issue_id': 6344, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '39431998f40d1db5', 'authors': ['Changyao Tian', 'Hao Li', 'Gen Luo', 'Xizhou Zhu', 'Weijie Su', 'Hanming Deng', 'Jinguo Zhu', 'Jie Shao', 'Ziran Zhu', 'Yunpeng Liu', 'Lewei Lu', 'Wenhai Wang', 'Hongsheng Li', 'Jifeng Dai'], 'affiliations': ['Nanjing University', 'Sensetime Research', 'Shanghai AI Laboratory', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.08565.jpg', 'data': {'categories': ['#benchmark', '#training', '#architecture', '#multimodal', '#optimization', '#agi'], 'emoji': '🔗', 'ru': {'title': 'Нативное обучение мультимодальных моделей с нуля', 'desc': 'Исследователи изучили end-to-end обучение Multimodal Large Language Models (MLLM) вместо традиционного композиционного подхода, где предобученные визуальные энкодеры соединяются с предобученными LLM. Они систематически исследовали архитектурные решения и свойства масштабирования при ограниченных данных, найдя оптимальный баланс между производительностью и стоимостью обучения. Важным открытием стала позитивная корреляция в масштабировании между визуальными энкодерами и LLM компонентами. На основе этих находок создана модель NaViL, показавшая конкурентные результаты на 14 мультимодальных бенчмарках.'}, 'en': {'title': 'Revolutionizing MLLMs with Native End-to-End Training', 'desc': 'This paper introduces a new approach to training Multimodal Large Language Models (MLLMs) called native end-to-end training. Unlike traditional methods that use separate pre-trained vision and language models, this approach integrates both components in a single training process. The authors explore the design and scaling properties of this method, demonstrating that a balanced relationship between visual encoders and language models can enhance performance while managing training costs. Their proposed model, NaViL, shows competitive results across multiple benchmarks, paving the way for future research in native MLLMs.'}, 'zh': {'title': '原生端到端训练，提升多模态模型性能', 'desc': '本文探讨了多模态大型语言模型（MLLMs）的原生端到端训练方法。与传统的组合训练方法不同，本文提出了一种新的设计空间和扩展特性，强调视觉编码器与语言模型之间的平衡关系。通过系统研究，作者提出了名为NaViL的原生MLLM，展示了其在14个多模态基准测试中的竞争性能。研究结果为未来的原生MLLM研究提供了深入的见解。'}}}, {'id': 'https://huggingface.co/papers/2510.08529', 'title': 'CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards', 'url': 'https://huggingface.co/papers/2510.08529', 'abstract': "Co-Evolving Multi-Agent Systems (CoMAS) enable LLM-based agents to improve autonomously through inter-agent interactions and intrinsic rewards, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-evolution is a central research topic in enabling large language model (LLM)-based agents to continually improve their capabilities after pretraining. Recent research has witnessed a transition from reinforcement learning (RL)-free to RL-based methods. Current RL-based methods either rely on dense external reward signals or extract intrinsic reward signals from LLMs themselves. However, these approaches diverge from the self-evolution mechanisms observed in human intelligence, where individuals learn and improve through mutual discussion and collaboration. In this work, we introduce Co-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents to improve autonomously by learning from inter-agent interactions without external supervision. CoMAS generates intrinsic rewards from rich discussion dynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and optimizes each agent's policy through RL, thereby enabling decentralized and scalable co-evolution. Experimental results demonstrate that CoMAS consistently outperforms untrained agents and achieves state-of-the-art performance across most evaluation settings. Ablation studies confirm the necessity of interaction-based reward signals and reveal promising scalability as the number and diversity of agents increase. These findings establish CoMAS as a novel and effective paradigm for self-evolution in LLM-based agents.", 'score': 16, 'issue_id': 6348, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '5b273c14caac04b3', 'authors': ['Xiangyuan Xue', 'Yifan Zhou', 'Guibin Zhang', 'Zaibin Zhang', 'Yijiang Li', 'Chen Zhang', 'Zhenfei Yin', 'Philip Torr', 'Wanli Ouyang', 'Lei Bai'], 'affiliations': ['Dalian University of Technology', 'National University of Singapore', 'Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong', 'University of California San Diego', 'University of Georgia', 'University of Oxford', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2510.08529.jpg', 'data': {'categories': ['#rl', '#optimization', '#reasoning', '#rlhf', '#agents', '#agi'], 'emoji': '🤝', 'ru': {'title': 'Агенты учатся друг у друга: коллективная эволюция без учителя', 'desc': 'Исследователи представили CoMAS — фреймворк для автономного самосовершенствования LLM-агентов через взаимодействие друг с другом без внешнего надзора. Система генерирует внутренние награды из динамики обсуждений между агентами, использует LLM в качестве судьи для формулирования этих наград и оптимизирует политику каждого агента через reinforcement learning. Подход вдохновлён механизмами человеческого обучения через взаимную дискуссию и сотрудничество, а не традиционными методами с внешними сигналами награды. Эксперименты показали превосходство над необученными агентами и state-of-the-art результаты, при этом система масштабируется с увеличением количества и разнообразия агентов.'}, 'en': {'title': 'Empowering Agents Through Collaborative Learning', 'desc': "Co-Evolving Multi-Agent Systems (CoMAS) is a framework that allows large language model (LLM)-based agents to autonomously enhance their skills through interactions with each other. Unlike traditional methods that depend on external rewards, CoMAS generates intrinsic rewards from the discussions between agents, mimicking human-like learning through collaboration. The framework employs a unique LLM-as-a-judge mechanism to assess these interactions and optimize each agent's policy using reinforcement learning (RL). Experimental results show that CoMAS not only surpasses untrained agents but also achieves leading performance in various evaluation scenarios, highlighting its potential for scalable self-evolution in AI systems."}, 'zh': {'title': '共同进化：智能体的自我提升新模式', 'desc': 'Co-Evolving Multi-Agent Systems (CoMAS) 是一种新颖的框架，允许基于大型语言模型（LLM）的智能体通过相互之间的互动自主改进。与传统的强化学习方法不同，CoMAS 通过丰富的讨论动态生成内在奖励信号，而不是依赖外部监督。该框架利用 LLM 作为评判者来制定奖励，并通过强化学习优化每个智能体的策略，从而实现去中心化和可扩展的共同进化。实验结果表明，CoMAS 在大多数评估设置中均优于未训练的智能体，展示了其在 LLM 基础智能体自我进化中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2510.07546', 'title': 'PickStyle: Video-to-Video Style Transfer with Context-Style Adapters', 'url': 'https://huggingface.co/papers/2510.07546', 'abstract': 'PickStyle uses diffusion models with style adapters and synthetic video clips to perform video style transfer from text prompts, preserving context and style.  \t\t\t\t\tAI-generated summary \t\t\t\t We address the task of video style transfer with diffusion models, where the goal is to preserve the context of an input video while rendering it in a target style specified by a text prompt. A major challenge is the lack of paired video data for supervision. We propose PickStyle, a video-to-video style transfer framework that augments pretrained video diffusion backbones with style adapters and benefits from paired still image data with source-style correspondences for training. PickStyle inserts low-rank adapters into the self-attention layers of conditioning modules, enabling efficient specialization for motion-style transfer while maintaining strong alignment between video content and style. To bridge the gap between static image supervision and dynamic video, we construct synthetic training clips from paired images by applying shared augmentations that simulate camera motion, ensuring temporal priors are preserved. In addition, we introduce Context-Style Classifier-Free Guidance (CS-CFG), a novel factorization of classifier-free guidance into independent text (style) and video (context) directions. CS-CFG ensures that context is preserved in generated video while the style is effectively transferred. Experiments across benchmarks show that our approach achieves temporally coherent, style-faithful, and content-preserving video translations, outperforming existing baselines both qualitatively and quantitatively.', 'score': 16, 'issue_id': 6358, 'pub_date': '2025-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': 'cf2ca9e86cd56561', 'authors': ['Soroush Mehraban', 'Vida Adeli', 'Jacob Rommann', 'Babak Taati', 'Kyryl Truskovskyi'], 'affiliations': ['Pickford AI', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2510.07546.jpg', 'data': {'categories': ['#diffusion', '#multimodal', '#synthetic', '#video', '#dataset'], 'emoji': '🎨', 'ru': {'title': 'Перенос стиля на видео через адаптеры и синтетические данные', 'desc': 'В статье представлен PickStyle — метод переноса стиля на видео с помощью диффузионных моделей. Главная проблема — отсутствие парных видеоданных для обучения, которую авторы решают через использование статичных изображений и синтетических видеоклипов с имитацией движения камеры. В архитектуру добавлены low-rank адаптеры в слои self-attention, что позволяет эффективно адаптировать модель для переноса стиля при сохранении контента. Также предложен Context-Style Classifier-Free Guidance (CS-CFG) — новый подход к управлению генерацией, разделяющий влияние текстового промпта (стиль) и исходного видео (контекст).'}, 'en': {'title': 'Transforming Video Style with Context Preservation', 'desc': 'PickStyle is a framework designed for video style transfer using diffusion models, which allows videos to be transformed according to a specified style from text prompts while keeping the original context intact. It addresses the challenge of limited paired video data by utilizing pretrained video diffusion models augmented with style adapters and leveraging paired still images for training. The method incorporates low-rank adapters in self-attention layers to enhance motion-style transfer and maintain alignment between video content and style. Additionally, it introduces a novel guidance technique called Context-Style Classifier-Free Guidance (CS-CFG) to ensure that the generated videos preserve their context while effectively applying the desired style, resulting in high-quality video translations that outperform existing methods.'}, 'zh': {'title': '视频风格迁移的新方法：PickStyle', 'desc': 'PickStyle 是一种视频风格迁移框架，利用扩散模型和风格适配器，从文本提示中实现视频风格转换，同时保持视频的上下文和风格。由于缺乏配对视频数据进行监督，PickStyle 通过使用配对静态图像数据进行训练，增强了预训练的视频扩散模型。该方法在自注意力层中插入低秩适配器，使得运动风格迁移更加高效，同时保持视频内容与风格之间的强对齐。我们还引入了上下文-风格无分类引导（CS-CFG），确保生成的视频在保持上下文的同时有效地转移风格。'}}}, {'id': 'https://huggingface.co/papers/2510.03663', 'title': 'UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG', 'url': 'https://huggingface.co/papers/2510.03663', 'abstract': 'UniDoc-Bench is a large-scale benchmark for multimodal retrieval-augmented generation, evaluating systems across text, images, and their fusion in real-world document-centric scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal retrieval-augmented generation (MM-RAG) is a key approach for applying large language models (LLMs) and agents to real-world knowledge bases, yet current evaluations are fragmented, focusing on either text or images in isolation or on simplified multimodal setups that fail to capture document-centric multimodal use cases. In this paper, we introduce UniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from 70k real-world PDF pages across eight domains. Our pipeline extracts and links evidence from text, tables, and figures, then generates 1,600 multimodal QA pairs spanning factual retrieval, comparison, summarization, and logical reasoning queries. To ensure reliability, 20% of QA pairs are validated by multiple annotators and expert adjudication. UniDoc-Bench supports apples-to-apples comparison across four paradigms: (1) text-only, (2) image-only, (3) multimodal text-image fusion, and (4) multimodal joint retrieval -- under a unified protocol with standardized candidate pools, prompts, and evaluation metrics. Our experiments show that multimodal text-image fusion RAG systems consistently outperform both unimodal and jointly multimodal embedding-based retrieval, indicating that neither text nor images alone are sufficient and that current multimodal embeddings remain inadequate. Beyond benchmarking, our analysis reveals when and how visual context complements textual evidence, uncovers systematic failure modes, and offers actionable guidance for developing more robust MM-RAG pipelines.', 'score': 14, 'issue_id': 6344, 'pub_date': '2025-10-04', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}, 'hash': '25c006b7c90bf61e', 'authors': ['Xiangyu Peng', 'Can Qin', 'Zeyuan Chen', 'Ran Xu', 'Caiming Xiong', 'Chien-Sheng Wu'], 'affiliations': ['Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.03663.jpg', 'data': {'categories': ['#benchmark', '#survey', '#rag', '#multimodal', '#reasoning', '#games'], 'emoji': '📚', 'ru': {'title': 'Мультимодальный RAG: текст и изображения вместе сильнее', 'desc': 'Статья представляет UniDoc-Bench — первый крупномасштабный бенчмарк для оценки мультимодальных систем retrieval-augmented generation (RAG), построенный на основе 70 тысяч реальных PDF-страниц из восьми доменов. Бенчмарк включает 1600 пар вопросов-ответов, охватывающих извлечение фактов, сравнение, суммаризацию и логические рассуждения на основе текста, таблиц и изображений. Эксперименты показывают, что мультимодальные RAG-системы, объединяющие текст и изображения, значительно превосходят одномодальные подходы и системы на основе совместных мультимодальных эмбеддингов. Исследование выявляет, когда визуальный контекст дополняет текстовые данные, и предлагает рекомендации для разработки более надёжных MM-RAG систем.'}, 'en': {'title': 'Unlocking the Power of Multimodal Retrieval with UniDoc-Bench', 'desc': 'UniDoc-Bench is a comprehensive benchmark designed for evaluating multimodal retrieval-augmented generation (MM-RAG) systems that utilize both text and images. It addresses the limitations of existing evaluations by providing a realistic dataset derived from 70,000 real-world PDF pages across various domains. The benchmark includes 1,600 multimodal question-answer pairs that cover a range of tasks such as factual retrieval and logical reasoning, with a focus on ensuring quality through expert validation. Results indicate that systems leveraging multimodal text-image fusion significantly outperform those relying solely on text or images, highlighting the importance of integrating visual context with textual information.'}, 'zh': {'title': '多模态检索增强生成的基准测试新标准', 'desc': 'UniDoc-Bench是一个大规模的基准测试，专注于多模态检索增强生成（MM-RAG），评估文本、图像及其融合在真实文档场景中的表现。该基准由70,000个真实世界的PDF页面构成，涵盖八个领域，提供了1,600个多模态问答对，涉及事实检索、比较、摘要和逻辑推理等任务。通过统一的协议和标准化的评估指标，UniDoc-Bench支持四种比较方式：仅文本、仅图像、多模态文本-图像融合和多模态联合检索。实验结果表明，多模态文本-图像融合的RAG系统在性能上优于单模态和联合多模态的检索方法，强调了文本和图像的结合在信息检索中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2510.08485', 'title': 'InstructX: Towards Unified Visual Editing with MLLM Guidance', 'url': 'https://huggingface.co/papers/2510.08485', 'abstract': 'InstructX integrates Multimodal Large Language Models and diffusion models for instruction-driven image and video editing, achieving state-of-the-art performance across diverse tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t With recent advances in Multimodal Large Language Models (MLLMs) showing strong visual understanding and reasoning, interest is growing in using them to improve the editing performance of diffusion models. Despite rapid progress, most studies lack an in-depth analysis of MLLM design choices. Moreover, the integration of MLLMs and diffusion models remains an open challenge in some difficult tasks, such as video editing. In this paper, we present InstructX, a unified framework for image and video editing. Specifically, we conduct a comprehensive study on integrating MLLMs and diffusion models for instruction-driven editing across diverse tasks. Building on this study, we analyze the cooperation and distinction between images and videos in unified modeling. (1) We show that training on image data can lead to emergent video editing capabilities without explicit supervision, thereby alleviating the constraints imposed by scarce video training data. (2) By incorporating modality-specific MLLM features, our approach effectively unifies image and video editing tasks within a single model. Extensive experiments demonstrate that our method can handle a broad range of image and video editing tasks and achieves state-of-the-art performance.', 'score': 13, 'issue_id': 6347, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '0dfd70ba0ce35168', 'authors': ['Chong Mou', 'Qichao Sun', 'Yanze Wu', 'Pengze Zhang', 'Xinghui Li', 'Fulong Ye', 'Songtao Zhao', 'Qian He'], 'affiliations': ['Intelligent Creation Team, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2510.08485.jpg', 'data': {'categories': ['#video', '#diffusion', '#multimodal', '#cv'], 'emoji': '🎬', 'ru': {'title': 'Одна модель для редактирования и фото, и видео по текстовым инструкциям', 'desc': 'InstructX - это унифицированный фреймворк для редактирования изображений и видео на основе текстовых инструкций. Он интегрирует мультимодальные LLM с диффузионными моделями для улучшения качества редактирования. Ключевое открытие: обучение на изображениях приводит к возникновению способности редактировать видео без явного обучения на видеоданных. Модель достигает state-of-the-art результатов на широком спектре задач редактирования обоих модальностей.'}, 'en': {'title': 'Unifying Image and Video Editing with InstructX', 'desc': 'InstructX is a novel framework that combines Multimodal Large Language Models (MLLMs) with diffusion models to enhance image and video editing capabilities. The paper highlights the importance of MLLM design choices and addresses the challenges of integrating these models for complex tasks like video editing. It reveals that training on image data can unexpectedly improve video editing performance, reducing the need for extensive video datasets. The approach successfully merges image and video editing tasks into a single model, achieving top performance across various editing applications.'}, 'zh': {'title': 'InstructX：图像与视频编辑的统一框架', 'desc': '本文介绍了InstructX，一个将多模态大语言模型（MLLMs）与扩散模型结合的框架，用于基于指令的图像和视频编辑。研究表明，通过在图像数据上训练，模型能够在没有明确监督的情况下，展现出视频编辑的能力，从而缓解了视频训练数据稀缺的限制。我们还分析了图像和视频在统一建模中的合作与区别，提出了结合特定模态的MLLM特征的方法，有效地将图像和视频编辑任务统一在一个模型中。实验结果表明，InstructX在多种编辑任务中表现出色，达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2510.06915', 'title': 'LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling', 'url': 'https://huggingface.co/papers/2510.06915', 'abstract': "A benchmark and training strategy for reward models to improve long-context consistency and performance in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward model (RM) plays a pivotal role in aligning large language model (LLM) with human preferences. As real-world applications increasingly involve long history trajectories, e.g., LLM agent, it becomes indispensable to evaluate whether a model's responses are not only high-quality but also grounded in and consistent with the provided context. Yet, current RMs remain confined to short-context settings and primarily focus on response-level attributes (e.g., safety or helpfulness), while largely neglecting the critical dimension of long context-response consistency. In this work, we introduce Long-RewardBench, a benchmark specifically designed for long-context RM evaluation, featuring both Pairwise Comparison and Best-of-N tasks. Our preliminary study reveals that even state-of-the-art generative RMs exhibit significant fragility in long-context scenarios, failing to maintain context-aware preference judgments. Motivated by the analysis of failure patterns observed in model outputs, we propose a general multi-stage training strategy that effectively scales arbitrary models into robust Long-context RMs (LongRMs). Experiments show that our approach not only substantially improves performance on long-context evaluation but also preserves strong short-context capability. Notably, our 8B LongRM outperforms much larger 70B-scale baselines and matches the performance of the proprietary Gemini 2.5 Pro model.", 'score': 13, 'issue_id': 6344, 'pub_date': '2025-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': 'f89f427922a20c18', 'authors': ['Zecheng Tang', 'Baibei Ji', 'Quantong Qiu', 'Haitian Wang', 'Xiaobo Liang', 'Juntao Li', 'Min Zhang'], 'affiliations': ['LCM Laboratory', 'Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2510.06915.jpg', 'data': {'categories': ['#benchmark', '#training', '#alignment', '#long_context'], 'emoji': '📏', 'ru': {'title': 'Обучение моделей вознаграждения для работы с длинным контекстом', 'desc': 'Исследователи представили Long-RewardBench — бенчмарк для оценки reward models в условиях длинного контекста, где модели должны учитывать большую историю взаимодействий. Оказалось, что современные reward models плохо справляются с оценкой консистентности ответов относительно длинного контекста. Авторы предложили многоэтапную стратегию обучения, которая позволяет адаптировать любые модели для работы с длинным контекстом без потери качества на коротких промптах. Их 8B модель превосходит базовые 70B модели и сравнима по производительности с проприетарной Gemini 2.5 Pro.'}, 'en': {'title': 'Enhancing Long-Context Consistency in Reward Models', 'desc': 'This paper addresses the limitations of reward models (RMs) in large language models (LLMs) when dealing with long-context scenarios. It introduces Long-RewardBench, a new benchmark for evaluating RMs specifically designed for long-context consistency and performance. The authors identify that existing RMs struggle with maintaining context-aware preferences in lengthy interactions. To overcome this, they propose a multi-stage training strategy that enhances the robustness of RMs for long contexts while retaining their effectiveness in short contexts, leading to improved performance even against larger models.'}, 'zh': {'title': '提升长上下文一致性的奖励模型策略', 'desc': '本论文提出了一种新的基准和训练策略，用于奖励模型（RM），以提高大型语言模型（LLM）在长上下文中的一致性和性能。当前的奖励模型主要集中在短上下文设置，忽视了长上下文与响应一致性的重要性。我们引入了Long-RewardBench基准，专门用于长上下文的RM评估，并提出了一种多阶段训练策略，以增强模型在长上下文中的鲁棒性。实验结果表明，我们的方法显著提高了长上下文评估的性能，同时保持了短上下文的强大能力。'}}}, {'id': 'https://huggingface.co/papers/2510.08002', 'title': 'Learning on the Job: An Experience-Driven Self-Evolving Agent for\n  Long-Horizon Tasks', 'url': 'https://huggingface.co/papers/2510.08002', 'abstract': 'MUSE, a novel agent framework with a hierarchical Memory Module, enables continuous learning and self-evolution, achieving state-of-the-art performance on long-horizon productivity tasks using a lightweight model.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models have demonstrated remarkable capabilities across diverse domains, yet significant challenges persist when deploying them as AI agents for real-world long-horizon tasks. Existing LLM agents suffer from a critical limitation: they are test-time static and cannot learn from experience, lacking the ability to accumulate knowledge and continuously improve on the job. To address this challenge, we propose MUSE, a novel agent framework that introduces an experience-driven, self-evolving system centered around a hierarchical Memory Module. MUSE organizes diverse levels of experience and leverages them to plan and execute long-horizon tasks across multiple applications. After each sub-task execution, the agent autonomously reflects on its trajectory, converting the raw trajectory into structured experience and integrating it back into the Memory Module. This mechanism enables the agent to evolve beyond its static pretrained parameters, fostering continuous learning and self-evolution. We evaluate MUSE on the long-horizon productivity benchmark TAC. It achieves new SOTA performance by a significant margin using only a lightweight Gemini-2.5 Flash model. Sufficient Experiments demonstrate that as the agent autonomously accumulates experience, it exhibits increasingly superior task completion capabilities, as well as robust continuous learning and self-evolution capabilities. Moreover, the accumulated experience from MUSE exhibits strong generalization properties, enabling zero-shot improvement on new tasks. MUSE establishes a new paradigm for AI agents capable of real-world productivity task automation.', 'score': 11, 'issue_id': 6350, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'df148f6443cb8697', 'authors': ['Cheng Yang', 'Xuemeng Yang', 'Licheng Wen', 'Daocheng Fu', 'Jianbiao Mei', 'Rong Wu', 'Pinlong Cai', 'Yufan Shen', 'Nianchen Deng', 'Botian Shi', 'Yu Qiao', 'Haifeng Li'], 'affiliations': ['Central South University', 'Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Innovation Institute', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.08002.jpg', 'data': {'categories': ['#optimization', '#training', '#agi', '#long_context', '#agents'], 'emoji': '🧠', 'ru': {'title': 'Агент с памятью, который учится на собственном опыте', 'desc': 'MUSE — это новая архитектура AI-агента с иерархическим модулем памяти, который позволяет системе непрерывно обучаться и самосовершенствоваться. После выполнения каждой подзадачи агент автономно анализирует свои действия, преобразует их в структурированный опыт и сохраняет в памяти. Это позволяет агенту выходить за рамки статических предобученных параметров и постоянно улучшать свои способности решения задач. На бенчмарке длинных продуктивных задач TAC система достигла state-of-the-art результатов, используя всего лишь легковесную модель Gemini-2.5 Flash, а накопленный опыт демонстрирует strong generalization на новые задачи.'}, 'en': {'title': 'MUSE: Evolving AI Agents for Continuous Learning and Productivity', 'desc': 'MUSE is a new framework designed for AI agents that allows them to learn continuously and improve over time. It features a hierarchical Memory Module that helps the agent organize and utilize its experiences effectively. By reflecting on its actions after completing tasks, MUSE transforms raw experiences into structured knowledge, which enhances its performance on long-term tasks. This innovative approach enables MUSE to achieve state-of-the-art results while using a lightweight model, demonstrating strong generalization and self-evolution capabilities.'}, 'zh': {'title': 'MUSE：智能体自我进化的新范式', 'desc': 'MUSE是一个新颖的智能体框架，采用分层记忆模块，能够实现持续学习和自我进化。它解决了现有大型语言模型在长时间任务中无法从经验中学习的限制。通过组织和利用多层次的经验，MUSE能够有效地规划和执行复杂任务，并在每次子任务执行后反思其过程，将原始轨迹转化为结构化经验。实验表明，MUSE在长时间生产力基准测试中表现出色，展示了强大的任务完成能力和持续学习能力。'}}}, {'id': 'https://huggingface.co/papers/2510.03117', 'title': 'Taming Text-to-Sounding Video Generation via Advanced Modality Condition\n  and Interaction', 'url': 'https://huggingface.co/papers/2510.03117', 'abstract': 'A novel dual-tower diffusion transformer with a Dual CrossAttention mechanism addresses challenges in Text-to-Sounding-Video generation by disentangling captions and enabling symmetric information exchange.  \t\t\t\t\tAI-generated summary \t\t\t\t This study focuses on a challenging yet promising task, Text-to-Sounding-Video (T2SV) generation, which aims to generate a video with synchronized audio from text conditions, meanwhile ensuring both modalities are aligned with text. Despite progress in joint audio-video training, two critical challenges still remain unaddressed: (1) a single, shared text caption where the text for video is equal to the text for audio often creates modal interference, confusing the pretrained backbones, and (2) the optimal mechanism for cross-modal feature interaction remains unclear. To address these challenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC) framework that generates pairs of disentangled captions, a video caption, and an audio caption, eliminating interference at the conditioning stage. Based on HVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer, which employs a Dual CrossAttention (DCA) mechanism that acts as a robust ``bridge" to enable a symmetric, bidirectional exchange of information, achieving both semantic and temporal synchronization. Extensive experiments on three benchmark datasets, supported by human evaluations, demonstrate that our method achieves state-of-the-art results on most metrics. Comprehensive ablation studies further validate the effectiveness of our contributions, offering key insights for the future T2SV task. All the codes and checkpoints will be publicly released.', 'score': 9, 'issue_id': 6351, 'pub_date': '2025-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': 'a9c0f370bca797fa', 'authors': ['Kaisi Guan', 'Xihua Wang', 'Zhengfeng Lai', 'Xin Cheng', 'Peng Zhang', 'XiaoJiang Liu', 'Ruihua Song', 'Meng Cao'], 'affiliations': ['Apple', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2510.03117.jpg', 'data': {'categories': ['#diffusion', '#video', '#open_source', '#benchmark', '#multimodal'], 'emoji': '🎬', 'ru': {'title': 'Синхронное создание видео и звука через разделение текстовых описаний', 'desc': 'Статья посвящена задаче генерации видео со звуком из текстового описания (Text-to-Sounding-Video). Авторы предлагают фреймворк HVGC, который создаёт два отдельных текстовых описания — одно для видео, другое для аудио — что устраняет взаимные помехи между модальностями. На основе этого разработана архитектура BridgeDiT с механизмом Dual CrossAttention, обеспечивающим двунаправленный обмен информацией между видео и аудио для их синхронизации. Эксперименты на трёх бенчмарках показывают state-of-the-art результаты по большинству метрик.'}, 'en': {'title': 'Bridging Text to Video and Sound with Dual CrossAttention!', 'desc': 'This paper presents a new approach to generating videos with synchronized audio from text descriptions, known as Text-to-Sounding-Video (T2SV) generation. The authors introduce a dual-tower diffusion transformer called BridgeDiT, which utilizes a Dual CrossAttention mechanism to facilitate effective communication between video and audio features. By creating separate captions for video and audio, the method reduces confusion and improves alignment between the two modalities. The results show that this approach outperforms existing methods on several benchmarks, highlighting its potential for advancing T2SV tasks.'}, 'zh': {'title': '双塔扩散变换器：文本到声音视频生成的新突破', 'desc': '本文提出了一种新颖的双塔扩散变换器，采用双重交叉注意力机制，旨在解决文本到声音视频生成中的挑战。该研究专注于生成与文本条件同步的音频视频，确保两种模态与文本对齐。通过引入分离的层次视觉基础字幕框架，消除了模态干扰，并提出了BridgeDiT模型，实现了信息的双向对称交换。实验结果表明，该方法在多个基准数据集上达到了最先进的性能，验证了其有效性。'}}}, {'id': 'https://huggingface.co/papers/2510.08425', 'title': 'Reinforcing Diffusion Models by Direct Group Preference Optimization', 'url': 'https://huggingface.co/papers/2510.08425', 'abstract': 'DGPO, a new online RL algorithm, enhances diffusion models by learning from group-level preferences, enabling the use of efficient deterministic ODE samplers and achieving faster training and superior performance.  \t\t\t\t\tAI-generated summary \t\t\t\t While reinforcement learning methods such as Group Relative Preference Optimization (GRPO) have significantly enhanced Large Language Models, adapting them to diffusion models remains challenging. In particular, GRPO demands a stochastic policy, yet the most cost-effective diffusion samplers are based on deterministic ODEs. Recent work addresses this issue by using inefficient SDE-based samplers to induce stochasticity, but this reliance on model-agnostic Gaussian noise leads to slow convergence. To resolve this conflict, we propose Direct Group Preference Optimization (DGPO), a new online RL algorithm that dispenses with the policy-gradient framework entirely. DGPO learns directly from group-level preferences, which utilize relative information of samples within groups. This design eliminates the need for inefficient stochastic policies, unlocking the use of efficient deterministic ODE samplers and faster training. Extensive results show that DGPO trains around 20 times faster than existing state-of-the-art methods and achieves superior performance on both in-domain and out-of-domain reward metrics. Code is available at https://github.com/Luo-Yihong/DGPO.', 'score': 8, 'issue_id': 6345, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'b99983698082fd03', 'authors': ['Yihong Luo', 'Tianyang Hu', 'Jing Tang'], 'affiliations': ['CUHK (SZ)', 'HKUST', 'HKUST (GZ)'], 'pdf_title_img': 'assets/pdf/title_img/2510.08425.jpg', 'data': {'categories': ['#training', '#rl', '#rlhf', '#games', '#optimization'], 'emoji': '⚡', 'ru': {'title': 'Быстрое обучение диффузионных моделей через групповые предпочтения', 'desc': 'В статье представлен DGPO — новый алгоритм онлайн-обучения с подкреплением для диффузионных моделей, который обучается на групповых предпочтениях. В отличие от существующих методов типа GRPO, требующих стохастическую политику и медленные SDE-сэмплеры, DGPO позволяет использовать эффективные детерминированные ODE-сэмплеры. Алгоритм отказывается от традиционного policy-gradient подхода и напрямую учится на относительной информации между образцами внутри групп. В результате DGPO обучается примерно в 20 раз быстрее современных методов и показывает превосходную производительность на различных метриках качества.'}, 'en': {'title': 'Revolutionizing Diffusion Models with Direct Group Preference Optimization', 'desc': 'DGPO is a novel online reinforcement learning algorithm that improves diffusion models by leveraging group-level preferences. Unlike traditional methods that require stochastic policies, DGPO operates without the policy-gradient framework, allowing it to utilize efficient deterministic ODE samplers. This approach significantly accelerates training, achieving speeds approximately 20 times faster than current leading methods. The results demonstrate that DGPO not only enhances training efficiency but also delivers superior performance across various reward metrics.'}, 'zh': {'title': 'DGPO：高效的群体偏好优化算法', 'desc': 'DGPO是一种新的在线强化学习算法，通过学习群体级偏好来增强扩散模型。它避免了使用随机策略，从而能够使用高效的确定性常微分方程（ODE）采样器。这种设计使得训练速度提高了约20倍，并在各类奖励指标上表现优异。DGPO的提出解决了传统方法中随机性与效率之间的矛盾。'}}}, {'id': 'https://huggingface.co/papers/2510.08559', 'title': 'SciVideoBench: Benchmarking Scientific Video Reasoning in Large\n  Multimodal Models', 'url': 'https://huggingface.co/papers/2510.08559', 'abstract': "SciVideoBench is a benchmark designed to evaluate advanced video reasoning in scientific contexts, challenging models with sophisticated domain-specific knowledge and logical reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Multimodal Models (LMMs) have achieved remarkable progress across various capabilities; however, complex video reasoning in the scientific domain remains a significant and challenging frontier. Current video benchmarks predominantly target general scenarios where perception/recognition is heavily relied on, while with relatively simple reasoning tasks, leading to saturation and thus failing to effectively evaluate advanced multimodal cognitive skills. To address this critical gap, we introduce SciVideoBench, a rigorous benchmark specifically designed to assess advanced video reasoning in scientific contexts. SciVideoBench consists of 1,000 carefully crafted multiple-choice questions derived from cutting-edge scientific experimental videos spanning over 25 specialized academic subjects and verified by a semi-automatic system. Each question demands sophisticated domain-specific knowledge, precise spatiotemporal perception, and intricate logical reasoning, effectively challenging models' higher-order cognitive abilities. Our evaluation highlights significant performance deficits in state-of-the-art proprietary and open-source LMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating substantial room for advancement in video reasoning capabilities. Detailed analyses of critical factors such as reasoning complexity and visual grounding provide valuable insights and clear direction for future developments in LMMs, driving the evolution of truly capable multimodal AI co-scientists. We hope SciVideoBench could fit the interests of the community and help to push the boundary of cutting-edge AI for border science.", 'score': 7, 'issue_id': 6345, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '0ebe2fdbf3a4a576', 'authors': ['Andong Deng', 'Taojiannan Yang', 'Shoubin Yu', 'Lincoln Spencer', 'Mohit Bansal', 'Chen Chen', 'Serena Yeung-Levy', 'Xiaohan Wang'], 'affiliations': ['Stanford University', 'University of Central Florida', 'University of North Carolina at Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2510.08559.jpg', 'data': {'categories': ['#benchmark', '#science', '#multimodal', '#video', '#reasoning'], 'emoji': '🔬', 'ru': {'title': 'Научное видео-мышление: новый вызов для мультимодальных моделей', 'desc': 'SciVideoBench — это новый бенчмарк для оценки способностей больших мультимодальных моделей (LMM) к сложному видео-рассуждению в научном контексте. Датасет включает 1000 тщательно подобранных вопросов с множественным выбором, основанных на научных экспериментальных видео из более чем 25 специализированных академических областей. Каждый вопрос требует глубоких предметных знаний, точного пространственно-временного восприятия и сложных логических рассуждений. Тестирование современных LMM, включая Gemini 2.5 Pro и Qwen2.5-VL, показало значительные пробелы в их способностях, указывая на большой потенциал для улучшения мультимодального AI.'}, 'en': {'title': 'Pushing the Limits of Video Reasoning in Science', 'desc': 'SciVideoBench is a new benchmark created to test advanced video reasoning specifically in scientific fields. It includes 1,000 multiple-choice questions based on complex scientific videos, requiring deep domain knowledge and logical reasoning. Current benchmarks focus on simpler tasks, which do not adequately challenge the capabilities of large multimodal models (LMMs). The results show that even the best LMMs struggle with these advanced reasoning tasks, highlighting the need for further development in this area.'}, 'zh': {'title': '推动科学视频推理的边界', 'desc': 'SciVideoBench是一个专门用于评估科学领域视频推理能力的基准测试。它包含1000个精心设计的多项选择题，涵盖25个学术领域的前沿科学实验视频。每个问题都需要复杂的领域知识、精确的时空感知和复杂的逻辑推理，旨在挑战模型的高级认知能力。我们的评估显示，当前的先进多模态模型在视频推理能力上存在显著不足，表明这一领域还有很大的发展空间。'}}}, {'id': 'https://huggingface.co/papers/2510.08431', 'title': 'Large Scale Diffusion Distillation via Score-Regularized Continuous-Time\n  Consistency', 'url': 'https://huggingface.co/papers/2510.08431', 'abstract': 'Score-regularized continuous-time consistency model (rCM) improves large-scale diffusion distillation by addressing fine-detail generation and diversity issues, achieving high fidelity and accelerated sampling.  \t\t\t\t\tAI-generated summary \t\t\t\t This work represents the first effort to scale up continuous-time consistency distillation to general application-level image and video diffusion models. Although continuous-time consistency model (sCM) is theoretically principled and empirically powerful for accelerating academic-scale diffusion, its applicability to large-scale text-to-image and video tasks remains unclear due to infrastructure challenges in Jacobian-vector product (JVP) computation and the limitations of standard evaluation benchmarks. We first develop a parallelism-compatible FlashAttention-2 JVP kernel, enabling sCM training on models with over 10 billion parameters and high-dimensional video tasks. Our investigation reveals fundamental quality limitations of sCM in fine-detail generation, which we attribute to error accumulation and the "mode-covering" nature of its forward-divergence objective. To remedy this, we propose the score-regularized continuous-time consistency model (rCM), which incorporates score distillation as a long-skip regularizer. This integration complements sCM with the "mode-seeking" reverse divergence, effectively improving visual quality while maintaining high generation diversity. Validated on large-scale models (Cosmos-Predict2, Wan2.1) up to 14B parameters and 5-second videos, rCM matches or surpasses the state-of-the-art distillation method DMD2 on quality metrics while offering notable advantages in diversity, all without GAN tuning or extensive hyperparameter searches. The distilled models generate high-fidelity samples in only 1sim4 steps, accelerating diffusion sampling by 15timessim50times. These results position rCM as a practical and theoretically grounded framework for advancing large-scale diffusion distillation.', 'score': 7, 'issue_id': 6345, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'd389552144d07b65', 'authors': ['Kaiwen Zheng', 'Yuji Wang', 'Qianli Ma', 'Huayu Chen', 'Jintao Zhang', 'Yogesh Balaji', 'Jianfei Chen', 'Ming-Yu Liu', 'Jun Zhu', 'Qinsheng Zhang'], 'affiliations': ['NVIDIA', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.08431.jpg', 'data': {'categories': ['#architecture', '#dataset', '#training', '#cv', '#benchmark', '#diffusion', '#video', '#optimization'], 'emoji': '⚡', 'ru': {'title': 'Ускорение диффузии в 50 раз без потери качества и разнообразия', 'desc': 'Исследователи разработали метод rCM (score-regularized continuous-time consistency model) для ускорения работы больших диффузионных моделей генерации изображений и видео. Они решили проблему стандартного метода sCM, который терял детали из-за накопления ошибок и особенностей целевой функции. Добавив score distillation как регуляризатор, новый метод улучшил качество деталей и сохранил разнообразие генерируемых образцов. В результате модели с параметрами до 14 миллиардов генерируют высококачественные изображения и видео всего за 1-4 шага, ускоряя процесс в 15-50 раз по сравнению с обычными диффузионными моделями.'}, 'en': {'title': 'Enhancing Diffusion Distillation with rCM for High-Quality Outputs', 'desc': 'The Score-regularized continuous-time consistency model (rCM) enhances large-scale diffusion distillation by improving the generation of fine details and diversity in images and videos. It addresses the limitations of the existing continuous-time consistency model (sCM) by introducing a new regularization technique that helps in better quality generation while maintaining variety. The rCM is designed to work efficiently with large models, enabling training on complex tasks without the need for extensive tuning. This approach significantly accelerates the sampling process, achieving high fidelity in generated outputs with fewer steps compared to previous methods.'}, 'zh': {'title': '得分正则化模型：提升扩散蒸馏的质量与多样性', 'desc': '这篇论文提出了一种新的模型，称为得分正则化连续时间一致性模型（rCM），旨在改善大规模扩散蒸馏中的细节生成和多样性问题。rCM通过引入得分蒸馏作为长跳跃正则化器，增强了原有的连续时间一致性模型（sCM），从而提高了视觉质量并保持了生成的多样性。研究表明，rCM在处理超过100亿参数的大规模模型和高维视频任务时，能够在质量指标上与最先进的蒸馏方法DMD2相匹配或超越。最终，rCM显著加快了扩散采样速度，提升了生成样本的保真度，展示了其在大规模扩散蒸馏中的实用性和理论基础。'}}}, {'id': 'https://huggingface.co/papers/2510.08276', 'title': 'Beyond Turn Limits: Training Deep Search Agents with Dynamic Context\n  Window', 'url': 'https://huggingface.co/papers/2510.08276', 'abstract': 'DeepMiner, a framework using high-difficulty training tasks and dynamic context management, enhances multi-turn reasoning agents through reinforcement learning, achieving significant performance improvements across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t While recent advances in reasoning models have demonstrated cognitive behaviors through reinforcement learning, existing approaches struggle to invoke deep reasoning capabilities in multi-turn agents with long-horizon interactions. We propose DeepMiner, a novel framework that elicits such abilities by introducing high-difficulty training tasks and dynamic context window. DeepMiner presents a reverse construction method to generate complex but verifiable question-answer pairs from authentic web sources, which ensures the challenge and reliability of training data while injecting cognitive capabilities into multi-turn reasoning scenarios. We further design an elegant yet effective dynamic context management strategy for both training and inference, utilizing sliding window mechanisms while eliminating the dependency on external summarization models, thereby efficiently empowering the model to handle continuously expanding long-horizon contexts. Through reinforcement learning on Qwen3-32B, we develop DeepMiner-32B, which achieves substantial performance improvements across multiple search agent benchmarks. DeepMiner attains 33.5% accuracy on BrowseComp-en, surpassing the previous best open-source agent by almost 20 percentage points, and demonstrates consistent improvements on BrowseComp-zh, XBench-DeepSearch, and GAIA. Notably, our dynamic context management enables sustained interactions of nearly 100 turns within standard 32k context length, effectively addressing the context limitations that constrain existing multi-turn interaction systems.', 'score': 7, 'issue_id': 6345, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'ec6091bbe962801d', 'authors': ['Qiaoyu Tang', 'Hao Xiang', 'Le Yu', 'Bowen Yu', 'Yaojie Lu', 'Xianpei Han', 'Le Sun', 'WenJuan Zhang', 'Pengbo Wang', 'Shixuan Liu', 'Zhenru Zhang', 'Jianhong Tu', 'Hongyu Lin', 'Junyang Lin'], 'affiliations': ['Alibaba Group', 'Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2510.08276.jpg', 'data': {'categories': ['#long_context', '#rl', '#benchmark', '#optimization', '#data', '#reasoning', '#agents'], 'emoji': '⛏️', 'ru': {'title': 'Глубокое обучение агентов для многоходовых рассуждений через сложные задачи', 'desc': 'DeepMiner — это фреймворк для улучшения multi-turn reasoning агентов с помощью reinforcement learning. Авторы создают сложные, но проверяемые пары вопрос-ответ из реальных веб-источников и используют динамическое управление контекстным окном со sliding window механизмом. Модель DeepMiner-32B на базе Qwen3-32B достигает 33.5% точности на бенчмарке BrowseComp-en, превосходя предыдущих лидеров на 20 процентных пунктов. Система поддерживает до 100 ходов взаимодействия в рамках стандартного контекста длиной 32k токенов, решая проблему ограничений контекста в multi-turn системах.'}, 'en': {'title': 'Empowering Multi-Turn Reasoning with DeepMiner', 'desc': "DeepMiner is a framework designed to improve multi-turn reasoning agents using reinforcement learning. It introduces high-difficulty training tasks and a dynamic context management strategy to enhance the agents' cognitive abilities during long interactions. The framework generates complex question-answer pairs from real web sources, ensuring the training data is both challenging and reliable. By utilizing a sliding window mechanism, DeepMiner allows agents to maintain effective interactions over extended contexts, achieving significant performance gains on various benchmarks."}, 'zh': {'title': 'DeepMiner：提升多轮推理能力的创新框架', 'desc': 'DeepMiner是一个通过强化学习增强多轮推理代理的框架。它引入了高难度的训练任务和动态上下文管理，显著提高了模型在基准测试中的表现。该框架采用反向构建方法，从真实的网络来源生成复杂且可验证的问题-答案对，确保训练数据的挑战性和可靠性。通过滑动窗口机制，DeepMiner有效管理上下文，支持近100轮的持续交互，解决了现有多轮交互系统的上下文限制问题。'}}}, {'id': 'https://huggingface.co/papers/2510.07743', 'title': 'OpenRubrics: Towards Scalable Synthetic Rubric Generation for Reward\n  Modeling and LLM Alignment', 'url': 'https://huggingface.co/papers/2510.07743', 'abstract': 'Rubric-based reward models using OpenRubrics and Contrastive Rubric Generation improve alignment in reinforcement learning from human feedback by providing scalable and reliable evaluation signals.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward modeling lies at the core of reinforcement learning from human feedback (RLHF), yet most existing reward models rely on scalar or pairwise judgments that fail to capture the multifaceted nature of human preferences. Recent studies have explored rubrics-as-rewards (RaR) that uses structured natural language criteria that capture multiple dimensions of response quality. However, producing rubrics that are both reliable and scalable remains a key challenge. In this work, we introduce OpenRubrics, a diverse, large-scale collection of (prompt, rubric) pairs for training rubric-generation and rubric-based reward models. To elicit discriminative and comprehensive evaluation signals, we introduce Contrastive Rubric Generation (CRG), which derives both hard rules (explicit constraints) and principles (implicit qualities) by contrasting preferred and rejected responses. We further improve reliability by enforcing preference-label consistency via rejection sampling to remove noisy rubrics. Across multiple reward-modeling benchmarks, our rubric-based reward model, Rubric-RM, surpasses strong size-matched baselines by 6.8%. These gains transfer to policy models on instruction-following and biomedical benchmarks. Our results show that rubrics provide scalable alignment signals that narrow the gap between costly human evaluation and automated reward modeling, enabling a new principle-driven paradigm for LLM alignment.', 'score': 7, 'issue_id': 6364, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '6e869cbb23d28825', 'authors': ['Tianci Liu', 'Ran Xu', 'Tony Yu', 'Ilgee Hong', 'Carl Yang', 'Tuo Zhao', 'Haoyu Wang'], 'affiliations': ['Emory University', 'Georgia Institute of Technology', 'Purdue University', 'University at Albany'], 'pdf_title_img': 'assets/pdf/title_img/2510.07743.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#training', '#reinforcement_learning', '#rlhf', '#alignment'], 'emoji': '📋', 'ru': {'title': 'Рубрики как награды: структурированная оценка для выравнивания LLM', 'desc': 'Статья представляет новый подход к reward modeling в RLHF, основанный на структурированных рубриках вместо простых скалярных оценок. Авторы создали OpenRubrics — большой датасет пар (промпт, рубрика) и разработали метод Contrastive Rubric Generation, который извлекает критерии качества путём сравнения предпочтительных и отклонённых ответов. Предложенная модель Rubric-RM превосходит базовые методы на 6.8% в бенчмарках reward modeling. Результаты показывают, что рубрики обеспечивают масштабируемые сигналы для выравнивания AI, приближая автоматическую оценку к человеческой.'}, 'en': {'title': 'Enhancing AI Alignment with Rubric-Based Rewards', 'desc': 'This paper presents a novel approach to improving reinforcement learning from human feedback (RLHF) by using rubric-based reward models. It introduces OpenRubrics, a large collection of structured (prompt, rubric) pairs that help in training these models to better capture human preferences. The authors also propose Contrastive Rubric Generation (CRG), which enhances the evaluation process by contrasting preferred and rejected responses to derive clear evaluation criteria. The results demonstrate that their rubric-based reward model, Rubric-RM, significantly outperforms existing models, providing a more reliable and scalable method for aligning AI systems with human values.'}, 'zh': {'title': '通过评分标准提升人类反馈的强化学习对齐', 'desc': '本论文提出了一种新的奖励模型，称为Rubric-RM，旨在通过使用OpenRubrics和对比评分生成（CRG）来改善基于人类反馈的强化学习（RLHF）。传统的奖励模型通常依赖于简单的评分，无法全面反映人类的多样化偏好。我们引入了结构化的评分标准，这些标准能够捕捉响应质量的多个维度，并通过对比生成可靠的评分。实验结果表明，Rubric-RM在多个基准测试中表现优于现有模型，展示了评分标准在自动化奖励建模中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2510.08549', 'title': 'Entropy Regularizing Activation: Boosting Continuous Control, Large\n  Language Models, and Image Classification with Activation as Entropy\n  Constraints', 'url': 'https://huggingface.co/papers/2510.08549', 'abstract': 'ERA, a new paradigm using specially designed activations, enhances performance across LLMs, reinforcement learning, and image classification with minimal computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose ERA, a new paradigm that constrains the sampling entropy above given thresholds by applying specially designed activations to the outputs of models. Our approach demonstrates broad effectiveness across different domains: 1) for large language models(LLMs), boosting the AIME 2025 score for Qwen2.5-Math-7B by 37.4%; 2) for continuous control reinforcement learning agents, improving performance by more than 30% over strong baselines such as SAC on the challenging HumanoidBench; 3) for image classification, enhancing ImageNet top-1 accuracy by 0.69% for ResNet-50. These gains are achieved with a computational overhead of less than 7%. Our work validates output activation as a powerful tool for entropy control, opening a new direction for designing simpler and more robust algorithms.', 'score': 6, 'issue_id': 6345, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '8d7e757f9121f346', 'authors': ['Zilin Kang', 'Chonghua Liao', 'Tingqiang Xu', 'Huazhe Xu'], 'affiliations': ['Department of Computer Science and Technology, Tsinghua University', 'Institute for Interdisciplinary Information Sciences, Tsinghua University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Qi Zhi Institute'], 'pdf_title_img': 'assets/pdf/title_img/2510.08549.jpg', 'data': {'categories': ['#training', '#architecture', '#cv', '#rl', '#optimization'], 'emoji': '🎯', 'ru': {'title': 'ERA: Контроль энтропии через активации для улучшения нейросетей', 'desc': 'Исследователи предложили ERA — новую парадигму, которая ограничивает энтропию выборки с помощью специально разработанных функций активации на выходе моделей. Подход показал эффективность в разных областях: улучшил результаты языковой модели Qwen2.5-Math-7B на задачах AIME на 37.4%, повысил производительность агентов обучения с подкреплением более чем на 30% по сравнению с SAC, и увеличил точность ResNet-50 на ImageNet на 0.69%. Все улучшения достигаются с вычислительными затратами менее 7%. Работа демонстрирует, что управление энтропией через выходные активации открывает новое направление для создания более простых и надежных алгоритмов машинного обучения.'}, 'en': {'title': 'ERA: Enhancing Model Performance with Controlled Activations', 'desc': 'The paper introduces ERA, a novel approach that utilizes specially designed activation functions to improve model performance while maintaining low computational costs. By constraining sampling entropy, ERA enhances the effectiveness of large language models, reinforcement learning agents, and image classification tasks. Notably, it achieves significant performance boosts, such as a 37.4% increase in AIME 2025 scores for LLMs and over 30% improvement in reinforcement learning benchmarks. This method demonstrates that controlling output activations can lead to simpler and more robust machine learning algorithms.'}, 'zh': {'title': 'ERA：提升模型性能的新范式', 'desc': '本文提出了一种新范式ERA，通过对模型输出应用特别设计的激活函数，约束采样熵在给定阈值之上，从而提升性能。该方法在多个领域表现出广泛的有效性：在大型语言模型（LLMs）中，Qwen2.5-Math-7B的AIME 2025分数提高了37.4%；在连续控制强化学习代理中，性能比强基线SAC在HumanoidBench上提高了30%以上；在图像分类中，ResNet-50在ImageNet上的top-1准确率提高了0.69%。这些提升的计算开销低于7%，验证了输出激活作为熵控制的强大工具，为设计更简单和更稳健的算法开辟了新方向。'}}}, {'id': 'https://huggingface.co/papers/2510.08203', 'title': 'Memory Retrieval and Consolidation in Large Language Models through\n  Function Tokens', 'url': 'https://huggingface.co/papers/2510.08203', 'abstract': 'Function tokens in large language models activate predictive features during inference and guide memory consolidation during pre-training by predicting subsequent content tokens.  \t\t\t\t\tAI-generated summary \t\t\t\t The remarkable success of large language models (LLMs) stems from their ability to consolidate vast amounts of knowledge into the memory during pre-training and to retrieve it from the memory during inference, enabling advanced capabilities such as knowledge memorization, instruction-following and reasoning. However, the mechanisms of memory retrieval and consolidation in LLMs remain poorly understood. In this paper, we propose the function token hypothesis to explain the workings of LLMs: During inference, function tokens activate the most predictive features from context and govern next token prediction (memory retrieval). During pre-training, predicting the next tokens (usually content tokens) that follow function tokens increases the number of learned features of LLMs and updates the model parameters (memory consolidation). Function tokens here roughly correspond to function words in linguistics, including punctuation marks, articles, prepositions, and conjunctions, in contrast to content tokens. We provide extensive experimental evidence supporting this hypothesis. Using bipartite graph analysis, we show that a small number of function tokens activate the majority of features. Case studies further reveal how function tokens activate the most predictive features from context to direct next token prediction. We also find that during pre-training, the training loss is dominated by predicting the next content tokens following function tokens, which forces the function tokens to select the most predictive features from context.', 'score': 5, 'issue_id': 6350, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '0d6651eac034bd4d', 'authors': ['Shaohua Zhang', 'Yuan Lin', 'Hang Li'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2510.08203.jpg', 'data': {'categories': ['#graphs', '#reasoning', '#training', '#interpretability', '#inference'], 'emoji': '🔑', 'ru': {'title': 'Функциональные токены как ключ к памяти LLM', 'desc': 'Исследователи предлагают гипотезу функциональных токенов для объяснения работы больших языковых моделей. Функциональные токены (знаки препинания, артикли, предлоги, союзы) активируют наиболее предсказательные признаки из контекста во время инференса и управляют предсказанием следующего токена. Во время предобучения предсказание контентных токенов, следующих за функциональными, увеличивает количество выученных признаков и обновляет параметры модели. Экспериментально показано, что небольшое количество функциональных токенов активирует большинство признаков, а loss при обучении доминируется предсказанием именно этих переходов.'}, 'en': {'title': 'Unlocking Memory: The Power of Function Tokens in LLMs', 'desc': "This paper introduces the function token hypothesis to explain how large language models (LLMs) operate during inference and pre-training. It suggests that function tokens, which include elements like punctuation and conjunctions, play a crucial role in activating predictive features from the context to guide the prediction of subsequent content tokens. During pre-training, the model learns to predict these content tokens following function tokens, enhancing its memory consolidation and feature learning. The authors provide experimental evidence showing that a small set of function tokens can activate a large number of features, significantly influencing the model's performance."}, 'zh': {'title': '功能标记：大型语言模型的记忆与预测关键', 'desc': '本文提出了功能标记假说，以解释大型语言模型（LLMs）的工作机制。在推理过程中，功能标记激活上下文中最具预测性的特征，并指导下一个标记的预测（记忆检索）。在预训练阶段，预测跟随功能标记的下一个内容标记可以增加LLMs学习的特征数量，并更新模型参数（记忆巩固）。我们的实验结果表明，少量功能标记能够激活大多数特征，从而支持这一假说。'}}}, {'id': 'https://huggingface.co/papers/2510.08008', 'title': 'Recycling Pretrained Checkpoints: Orthogonal Growth of\n  Mixture-of-Experts for Efficient Large Language Model Pre-Training', 'url': 'https://huggingface.co/papers/2510.08008', 'abstract': 'Recycling pretrained checkpoints through orthogonal growth methods improves large language model performance with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapidly increasing computational cost of pretraining Large Language Models necessitates more efficient approaches. Numerous computational costs have been invested in existing well-trained checkpoints, but many of them remain underutilized due to engineering constraints or limited model capacity. To efficiently reuse this "sunk" cost, we propose to recycle pretrained checkpoints by expanding their parameter counts and continuing training. We propose orthogonal growth method well-suited for converged Mixture-of-Experts model: interpositional layer copying for depth growth and expert duplication with injected noise for width growth. To determine the optimal timing for such growth across checkpoints sequences, we perform comprehensive scaling experiments revealing that the final accuracy has a strong positive correlation with the amount of sunk cost, indicating that greater prior investment leads to better performance. We scale our approach to models with 70B parameters and over 1T training tokens, achieving 10.66% accuracy gain over training from scratch under the same additional compute budget. Our checkpoint recycling approach establishes a foundation for economically efficient large language model pretraining.', 'score': 5, 'issue_id': 6345, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'b56e81668ec4e66d', 'authors': ['Ruizhe Wang', 'Yucheng Ding', 'Xiao Liu', 'Yaoxiang Wang', 'Peng Cheng', 'Baining Guo', 'Zhengjun Zha', 'Yeyun Gong'], 'affiliations': ['Microsoft Research Asia', 'Shanghai Jiao Tong University', 'University of Science and Technology of China', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2510.08008.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture'], 'emoji': '♻️', 'ru': {'title': 'Переработка чекпоинтов: эффективное повторное использование обученных LLM', 'desc': 'Статья предлагает метод переработки уже обученных чекпоинтов больших языковых моделей путём увеличения количества их параметров и продолжения обучения. Авторы используют ортогональные методы роста для Mixture-of-Experts моделей: копирование слоёв для увеличения глубины и дублирование экспертов с добавлением шума для увеличения ширины. Эксперименты на моделях до 70B параметров показывают, что чем больше вычислительных ресурсов уже вложено в чекпоинт, тем лучше финальная точность после переработки. Подход даёт прирост точности 10.66% по сравнению с обучением с нуля при том же дополнительном бюджете вычислений.'}, 'en': {'title': 'Recycle Checkpoints for Efficient LLM Growth!', 'desc': 'This paper discusses a method to improve the performance of large language models (LLMs) while reducing the computational costs associated with their pretraining. The authors propose recycling pretrained checkpoints by expanding their parameters and continuing training, which allows for better utilization of previously invested resources. They introduce an orthogonal growth method that includes techniques for both depth and width expansion of models, specifically designed for Mixture-of-Experts architectures. Their experiments show that models with more prior investment yield higher accuracy, demonstrating that this approach can lead to significant performance gains without the need for extensive new training resources.'}, 'zh': {'title': '回收预训练检查点，提升模型性能与效率', 'desc': '本论文提出了一种通过正交增长方法回收预训练检查点，以提高大型语言模型的性能并降低计算成本。随着大型语言模型预训练计算成本的迅速增加，现有的预训练检查点往往由于工程限制或模型容量不足而未被充分利用。我们的方法通过扩展参数数量并继续训练，来有效地重用这些“沉没”成本。实验结果表明，增加的投资与最终准确性之间存在强正相关关系，从而为经济高效的大型语言模型预训练奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2510.07790', 'title': 'GCPO: When Contrast Fails, Go Gold', 'url': 'https://huggingface.co/papers/2510.07790', 'abstract': "Group Contrastive Policy Optimization (GCPO) enhances reinforcement learning for large language models by incorporating external reference answers, improving training efficiency and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning has been widely applied to enhance the reasoning capabilities of large language models. Extending the inference limits of smaller models has become a prominent research focus. However, algorithms such as Group Relative Policy Optimization (GRPO) suffer from a clear drawback: the upper bound of a model's rollout responses is entirely determined by the model itself, preventing the acquisition of knowledge from samples that are either all incorrect or all correct. In this paper, we introduce Group Contrastive Policy Optimization (GCPO), a method that incorporates external standard reference answers. When the model cannot solve a problem, the reference answer supplies the correct response, steering the model toward an unequivocally accurate update direction. This approach offers two main advantages: (1) it improves training efficiency by fully utilizing every sample; (2) it enables the model to emulate the problem solving strategy of the reference answer during training, thereby enhancing generalization in reasoning. GCPO achieves outstanding results across multiple benchmark datasets, yielding substantial improvements over the baseline model. Our code is available at: https://github.com/AchoWu/GCPO.", 'score': 5, 'issue_id': 6353, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'c99830d91ac03778', 'authors': ['Hao Wu', 'Wei Liu'], 'affiliations': ['Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2510.07790.jpg', 'data': {'categories': ['#reasoning', '#training', '#benchmark', '#rl', '#optimization'], 'emoji': '🎯', 'ru': {'title': 'Обучение с контрастом: когда внешние ответы помогают LLM учиться лучше', 'desc': 'Исследователи представили метод Group Contrastive Policy Optimization (GCPO) для улучшения обучения больших языковых моделей через reinforcement learning. Ключевая идея заключается в использовании внешних эталонных ответов, которые показывают модели правильное направление, когда она не может решить задачу самостоятельно. Это позволяет эффективно использовать все обучающие примеры, даже когда модель генерирует только неправильные или только правильные ответы. Метод демонстрирует улучшенную обучаемость и обобщающую способность по сравнению с предыдущими подходами вроде GRPO.'}, 'en': {'title': 'Enhancing Learning with External Guidance: GCPO Unleashed!', 'desc': "Group Contrastive Policy Optimization (GCPO) is a novel approach in reinforcement learning that enhances the training of large language models by integrating external reference answers. This method addresses the limitations of previous algorithms like Group Relative Policy Optimization (GRPO), which rely solely on the model's own responses. By using correct reference answers when the model struggles, GCPO guides the model towards accurate updates, improving both training efficiency and generalization. The results demonstrate significant performance gains across various benchmark datasets, showcasing the effectiveness of this new optimization technique."}, 'zh': {'title': '引入外部答案，提升模型训练效率与泛化能力', 'desc': '群体对比策略优化（GCPO）通过引入外部参考答案，增强了强化学习在大语言模型中的应用，提升了训练效率和泛化能力。该方法解决了传统算法在模型推理时无法从样本中获取知识的问题。GCPO在模型无法解决问题时，利用参考答案提供正确的响应，指导模型朝着明确的更新方向前进。实验结果表明，GCPO在多个基准数据集上表现优异，显著超越了基线模型。'}}}, {'id': 'https://huggingface.co/papers/2509.24817', 'title': 'UP2You: Fast Reconstruction of Yourself from Unconstrained Photo\n  Collections', 'url': 'https://huggingface.co/papers/2509.24817', 'abstract': 'UP2You reconstructs high-fidelity 3D clothed portraits from unconstrained 2D photos using a data rectifier and pose-correlated feature aggregation, achieving superior geometric and texture accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t We present UP2You, the first tuning-free solution for reconstructing high-fidelity 3D clothed portraits from extremely unconstrained in-the-wild 2D photos. Unlike previous approaches that require "clean" inputs (e.g., full-body images with minimal occlusions, or well-calibrated cross-view captures), UP2You directly processes raw, unstructured photographs, which may vary significantly in pose, viewpoint, cropping, and occlusion. Instead of compressing data into tokens for slow online text-to-3D optimization, we introduce a data rectifier paradigm that efficiently converts unconstrained inputs into clean, orthogonal multi-view images in a single forward pass within seconds, simplifying the 3D reconstruction. Central to UP2You is a pose-correlated feature aggregation module (PCFA), that selectively fuses information from multiple reference images w.r.t. target poses, enabling better identity preservation and nearly constant memory footprint, with more observations. We also introduce a perceiver-based multi-reference shape predictor, removing the need for pre-captured body templates. Extensive experiments on 4D-Dress, PuzzleIOI, and in-the-wild captures demonstrate that UP2You consistently surpasses previous methods in both geometric accuracy (Chamfer-15%, P2S-18% on PuzzleIOI) and texture fidelity (PSNR-21%, LPIPS-46% on 4D-Dress). UP2You is efficient (1.5 minutes per person), and versatile (supports arbitrary pose control, and training-free multi-garment 3D virtual try-on), making it practical for real-world scenarios where humans are casually captured. Both models and code will be released to facilitate future research on this underexplored task. Project Page: https://zcai0612.github.io/UP2You', 'score': 5, 'issue_id': 6349, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '1bc6dc1ba4139d81', 'authors': ['Zeyu Cai', 'Ziyang Li', 'Xiaoben Li', 'Boqian Li', 'Zeyu Wang', 'Zhenyu Zhang', 'Yuliang Xiu'], 'affiliations': ['Nanjing University', 'The Hong Kong University of Science and Technology (Guangzhou)', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2509.24817.jpg', 'data': {'categories': ['#3d', '#open_source'], 'emoji': '👤', 'ru': {'title': '3D-портреты из обычных фото без настройки', 'desc': 'UP2You - это первое решение без необходимости тонкой настройки для создания высококачественных 3D-моделей одетых людей из обычных неподготовленных фотографий. В отличие от предыдущих методов, требующих специально подготовленные изображения, система напрямую обрабатывает обычные снимки с любыми позами, ракурсами и перекрытиями. Ключевой компонент - модуль агрегации признаков с учётом позы (PCFA), который избирательно объединяет информацию из нескольких референсных изображений для лучшего сохранения идентичности. Система показывает превосходство над существующими методами в геометрической точности и качестве текстур, работая всего за полторы минуты на человека.'}, 'en': {'title': 'Transforming Casual Photos into Stunning 3D Portraits!', 'desc': "UP2You is a novel method for creating high-quality 3D models of clothed people from casual 2D photos without needing any prior adjustments. It uses a data rectifier to transform messy input images into clear, usable views quickly, allowing for efficient 3D reconstruction. The approach includes a pose-correlated feature aggregation module that helps maintain the person's identity while processing multiple images from different angles. This method outperforms previous techniques in both shape accuracy and texture quality, making it suitable for real-world applications where images may not be ideal."}, 'zh': {'title': 'UP2You：从2D照片重建高保真3D肖像的创新方法', 'desc': 'UP2You是一种新颖的方法，可以从不受限制的2D照片中重建高保真度的3D穿衣肖像。与以往需要干净输入的技术不同，UP2You能够直接处理各种姿势、视角和遮挡的原始照片。该方法引入了数据整流器，能够在几秒钟内将不受限制的输入转换为干净的多视图图像，从而简化了3D重建过程。通过姿态相关特征聚合模块，UP2You在保持身份一致性和内存占用方面表现出色，实验结果显示其在几何精度和纹理保真度上均优于之前的方法。'}}}, {'id': 'https://huggingface.co/papers/2509.26633', 'title': 'OmniRetarget: Interaction-Preserving Data Generation for Humanoid\n  Whole-Body Loco-Manipulation and Scene Interaction', 'url': 'https://huggingface.co/papers/2509.26633', 'abstract': 'OmniRetarget generates high-quality, interaction-preserving motion data for training RL policies, enabling complex skills like parkour and loco-manipulation on humanoid robots.  \t\t\t\t\tAI-generated summary \t\t\t\t A dominant paradigm for teaching humanoid robots complex skills is to retarget human motions as kinematic references to train reinforcement learning (RL) policies. However, existing retargeting pipelines often struggle with the significant embodiment gap between humans and robots, producing physically implausible artifacts like foot-skating and penetration. More importantly, common retargeting methods neglect the rich human-object and human-environment interactions essential for expressive locomotion and loco-manipulation. To address this, we introduce OmniRetarget, an interaction-preserving data generation engine based on an interaction mesh that explicitly models and preserves the crucial spatial and contact relationships between an agent, the terrain, and manipulated objects. By minimizing the Laplacian deformation between the human and robot meshes while enforcing kinematic constraints, OmniRetarget generates kinematically feasible trajectories. Moreover, preserving task-relevant interactions enables efficient data augmentation, from a single demonstration to different robot embodiments, terrains, and object configurations. We comprehensively evaluate OmniRetarget by retargeting motions from OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour trajectories that achieve better kinematic constraint satisfaction and contact preservation than widely used baselines. Such high-quality data enables proprioceptive RL policies to successfully execute long-horizon (up to 30 seconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained with only 5 reward terms and simple domain randomization shared by all tasks, without any learning curriculum.', 'score': 4, 'issue_id': 6360, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': '18c4541549d1d678', 'authors': ['Lujie Yang', 'Xiaoyu Huang', 'Zhen Wu', 'Angjoo Kanazawa', 'Pieter Abbeel', 'Carmelo Sferrazza', 'C. Karen Liu', 'Rocky Duan', 'Guanya Shi'], 'affiliations': ['Amazon FAR (Frontier AI & Robotics)', 'CMU', 'MIT', 'Stanford University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2509.26633.jpg', 'data': {'categories': ['#training', '#robotics', '#optimization', '#games', '#dataset', '#rl'], 'emoji': '🤸', 'ru': {'title': 'Обучение роботов сложным движениям через сохранение взаимодействий с окружением', 'desc': 'OmniRetarget — это система для преобразования человеческих движений в данные для обучения гуманоидных роботов через reinforcement learning. Ключевая особенность — использование interaction mesh, которая явно моделирует и сохраняет пространственные и контактные отношения между агентом, окружением и объектами. Система генерирует физически правдоподобные траектории, избегая артефактов вроде проскальзывания стоп, и позволяет эффективно аугментировать данные для разных роботов и условий. Обученные RL-политики успешно выполняют сложные навыки паркура и манипуляции длительностью до 30 секунд на роботе Unitree G1 с минимальным количеством reward-функций.'}, 'en': {'title': 'Bridging the Gap: Realistic Motion Data for Humanoid Robots', 'desc': 'OmniRetarget is a novel approach for generating high-quality motion data that helps train reinforcement learning (RL) policies for humanoid robots. It addresses the challenges of the embodiment gap between humans and robots by preserving important interactions with the environment and objects. By using an interaction mesh, it ensures that the spatial and contact relationships are maintained, leading to more realistic and feasible motion trajectories. The method has been evaluated against existing techniques, showing superior performance in kinematic constraint satisfaction and interaction preservation, enabling robots to learn complex skills like parkour effectively.'}, 'zh': {'title': 'OmniRetarget：提升人形机器人运动技能的关键', 'desc': 'OmniRetarget 是一种生成高质量、保留交互的运动数据的方法，用于训练强化学习（RL）策略，使人形机器人能够掌握复杂技能，如跑酷和运动操控。现有的运动重定向方法常常面临人类与机器人之间的显著差距，导致生成不切实际的运动效果。OmniRetarget 通过建模和保留代理、地形和操控物体之间的空间和接触关系，解决了这一问题。该方法生成的运动轨迹在运动学约束和接触保留方面优于传统方法，支持高效的数据增强，促进机器人在不同环境中学习复杂技能。'}}}, {'id': 'https://huggingface.co/papers/2510.08556', 'title': 'DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via\n  Joint-Wise Neural Dynamics Model', 'url': 'https://huggingface.co/papers/2510.08556', 'abstract': 'A novel framework enables a single simulation-trained policy to generalize to diverse real-world object rotations by learning joint-wise dynamics and autonomously collecting data.  \t\t\t\t\tAI-generated summary \t\t\t\t Achieving generalized in-hand object rotation remains a significant challenge in robotics, largely due to the difficulty of transferring policies from simulation to the real world. The complex, contact-rich dynamics of dexterous manipulation create a "reality gap" that has limited prior work to constrained scenarios involving simple geometries, limited object sizes and aspect ratios, constrained wrist poses, or customized hands. We address this sim-to-real challenge with a novel framework that enables a single policy, trained in simulation, to generalize to a wide variety of objects and conditions in the real world. The core of our method is a joint-wise dynamics model that learns to bridge the reality gap by effectively fitting limited amount of real-world collected data and then adapting the sim policy\'s actions accordingly. The model is highly data-efficient and generalizable across different whole-hand interaction distributions by factorizing dynamics across joints, compressing system-wide influences into low-dimensional variables, and learning each joint\'s evolution from its own dynamic profile, implicitly capturing these net effects. We pair this with a fully autonomous data collection strategy that gathers diverse, real-world interaction data with minimal human intervention. Our complete pipeline demonstrates unprecedented generality: a single policy successfully rotates challenging objects with complex shapes (e.g., animals), high aspect ratios (up to 5.33), and small sizes, all while handling diverse wrist orientations and rotation axes. Comprehensive real-world evaluations and a teleoperation application for complex tasks validate the effectiveness and robustness of our approach. Website: https://meowuu7.github.io/DexNDM/', 'score': 3, 'issue_id': 6345, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '1cc572ebd0cf9869', 'authors': ['Xueyi Liu', 'He Wang', 'Li Yi'], 'affiliations': ['Galbot Project', 'Peking University', 'Shanghai Qi Zhi Institute', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.08556.jpg', 'data': {'categories': ['#training', '#robotics', '#optimization', '#data', '#transfer_learning', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Одна политика для вращения любых объектов в руке робота', 'desc': 'Исследователи решили проблему переноса навыков ловкой манипуляции из симуляции в реальность для вращения объектов в руке робота. Их метод основан на модели динамики, которая работает отдельно для каждого сустава, эффективно обучаясь на малом количестве реальных данных и адаптируя действия sim-политики. Система автономно собирает разнообразные данные взаимодействий и позволяет одной политике обобщаться на объекты сложных форм, разных размеров и ориентаций. Подход демонстрирует беспрецедентную универсальность по сравнению с предыдущими работами, ограниченными простыми сценариями.'}, 'en': {'title': 'Bridging the Reality Gap for Robotic Manipulation', 'desc': "This paper presents a new framework that allows a single policy, trained in simulation, to effectively handle various real-world object rotations. It addresses the challenge of transferring learned behaviors from simulated environments to real-world scenarios, overcoming the 'reality gap' that often limits robotic manipulation. The approach utilizes a joint-wise dynamics model to adapt the policy's actions based on limited real-world data, making it both data-efficient and generalizable. Additionally, an autonomous data collection strategy is employed to gather diverse interaction data, enabling the policy to successfully manipulate complex objects with different shapes and sizes."}, 'zh': {'title': '单一策略实现多样化物体旋转的突破', 'desc': '本论文提出了一种新颖的框架，使得单一的模拟训练策略能够在多样的真实物体旋转中实现泛化。通过学习关节级的动态模型，该方法有效地缩小了模拟与现实之间的差距，并能够自适应地调整策略的动作。该模型在数据效率和泛化能力上表现出色，能够处理复杂形状和高纵横比的小型物体。我们的实验验证了该方法在真实世界中的有效性和鲁棒性，成功实现了对多种物体的旋转操作。'}}}, {'id': 'https://huggingface.co/papers/2510.07958', 'title': 'A^2Search: Ambiguity-Aware Question Answering with Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2510.07958', 'abstract': 'A$^2$Search is an annotation-free framework that handles ambiguity in open-domain QA by detecting ambiguous questions, gathering alternative answers, and optimizing with RL, achieving state-of-the-art performance across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Models (LLMs) and Reinforcement Learning (RL) have led to strong performance in open-domain question answering (QA). However, existing models still struggle with questions that admit multiple valid answers. Standard QA benchmarks, which typically assume a single gold answer, overlook this reality and thus produce inappropriate training signals. Existing attempts to handle ambiguity often rely on costly manual annotation, which is difficult to scale to multi-hop datasets such as HotpotQA and MuSiQue. In this paper, we present A^2Search, an annotation-free, end-to-end training framework to recognize and handle ambiguity. At its core is an automated pipeline that detects ambiguous questions and gathers alternative answers via trajectory sampling and evidence verification. The model is then optimized with RL using a carefully designed AnsF1 reward, which naturally accommodates multiple answers. Experiments on eight open-domain QA benchmarks demonstrate that A^2Search achieves new state-of-the-art performance. With only a single rollout, A^2Search-7B yields an average AnsF1@1 score of 48.4% across four multi-hop benchmarks, outperforming all strong baselines, including the substantially larger ReSearch-32B (46.2%). Extensive analyses further show that A^2Search resolves ambiguity and generalizes across benchmarks, highlighting that embracing ambiguity is essential for building more reliable QA systems. Our code, data, and model weights can be found at https://github.com/zfj1998/A2Search', 'score': 3, 'issue_id': 6345, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'a380e0c68e05f32c', 'authors': ['Fengji Zhang', 'Xinyao Niu', 'Chengyang Ying', 'Guancheng Lin', 'Zhongkai Hao', 'Zhou Fan', 'Chengen Huang', 'Jacky Keung', 'Bei Chen', 'Junyang Lin'], 'affiliations': ['Alibaba Group', 'City University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.07958.jpg', 'data': {'categories': ['#rl', '#benchmark', '#optimization', '#dataset', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Обучение QA-систем учитывать неоднозначность вопросов без ручной разметки', 'desc': 'A²Search — это фреймворк для обучения моделей вопросно-ответных систем, который работает без ручной разметки данных и умеет обрабатывать неоднозначные вопросы с несколькими правильными ответами. Система автоматически определяет такие вопросы и собирает альтернативные ответы через сэмплирование траекторий и верификацию доказательств. Модель оптимизируется с помощью reinforcement learning и специальной функции награды AnsF1, которая естественным образом учитывает множественные ответы. На восьми бенчмарках по open-domain QA A²Search достигает state-of-the-art результатов, причём 7-миллиардная модель превосходит более крупную 32-миллиардную baseline модель.'}, 'en': {'title': 'Embracing Ambiguity for Superior Question Answering', 'desc': 'A$^2$Search is a novel framework designed to improve open-domain question answering (QA) by addressing the challenge of ambiguous questions. It operates without the need for manual annotations, instead using an automated process to identify ambiguous queries and collect multiple valid answers. The framework employs reinforcement learning (RL) with a unique AnsF1 reward to optimize its performance, allowing it to effectively handle questions with several correct responses. Experiments show that A$^2$Search achieves state-of-the-art results on various benchmarks, demonstrating the importance of managing ambiguity in QA systems.'}, 'zh': {'title': '拥抱歧义，提升问答系统的可靠性', 'desc': 'A^2Search是一个无注释的框架，旨在处理开放领域问答中的歧义性。它通过检测歧义问题、收集替代答案，并利用强化学习进行优化，达到了最新的性能水平。该模型采用自动化流程，能够识别歧义问题并通过轨迹采样和证据验证收集答案。实验结果表明，A^2Search在多个基准测试中表现优异，展示了处理歧义性对于构建更可靠的问答系统的重要性。'}}}, {'id': 'https://huggingface.co/papers/2510.07429', 'title': 'Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs', 'url': 'https://huggingface.co/papers/2510.07429', 'abstract': 'BaRP, a Bandit-feedback Routing with Preferences approach, optimizes large language model selection in an online setting with partial feedback, outperforming offline routers and large models.  \t\t\t\t\tAI-generated summary \t\t\t\t Efficient use of large language models (LLMs) is critical for deployment at scale: without adaptive routing, systems either overpay for strong models or risk poor performance from weaker ones. Selecting the right LLM for each query is fundamentally an online decision problem: models differ in strengths, prices fluctuate, and users value accuracy and cost differently. Yet most routers are trained offline with labels for all candidate models, an assumption that breaks in deployment, where only the outcome of the chosen model is observed. We bridge this gap with BaRP, a Bandit-feedback Routing with Preferences approach that trains under the same partial-feedback restriction as deployment, while supporting preference-tunable inference: operators can dial the performance/cost trade-off at test time without retraining. Framed as a contextual bandit over prompt features and a user preference vector, our method simulates an online feedback setting during training and adapts its routing decisions to each new prompt, rather than depending on full-information offline supervision. Comprehensive experiments show that our method consistently outperforms strong offline routers by at least 12.46% and the largest LLM by at least 2.45%, and generalizes robustly for unseen tasks.', 'score': 3, 'issue_id': 6344, 'pub_date': '2025-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': 'a22ba38f76ba84ee', 'authors': ['Wang Wei', 'Tiankai Yang', 'Hongjie Chen', 'Yue Zhao', 'Franck Dernoncourt', 'Ryan A. Rossi', 'Hoda Eldardiry'], 'affiliations': ['Adobe Research', 'Dolby Labs', 'University of Southern California', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2510.07429.jpg', 'data': {'categories': ['#rlhf', '#training', '#optimization'], 'emoji': '🎰', 'ru': {'title': 'Умный выбор языковой модели на ходу', 'desc': 'Статья представляет BaRP — систему для выбора оптимальной языковой модели в режиме онлайн с частичной обратной связью. В отличие от классических роутеров, которые обучаются офлайн с полной информацией обо всех моделях, BaRP работает как contextual bandit и учится только на результатах выбранной модели. Система позволяет гибко настраивать баланс между качеством и стоимостью без переобучения, адаптируя решения к каждому новому запросу. Эксперименты показывают превосходство над офлайн-роутерами на 12.46% и над самыми большими LLM на 2.45%.'}, 'en': {'title': 'Optimize LLM Selection with BaRP: Smart, Adaptive, and Cost-Effective!', 'desc': 'BaRP is a novel approach that optimizes the selection of large language models (LLMs) in real-time using a bandit-feedback mechanism. It addresses the challenge of choosing the right model based on partial feedback, which is common in practical applications. By allowing operators to adjust the balance between performance and cost dynamically, BaRP enhances decision-making without needing to retrain the models. Experimental results demonstrate that BaRP significantly outperforms traditional offline routers and even the largest LLMs, making it a robust solution for adaptive model selection.'}, 'zh': {'title': '智能选择，优化模型性能', 'desc': 'BaRP是一种基于偏好的带反馈路由方法，旨在优化大型语言模型的选择。它在在线环境中处理部分反馈，能够有效地选择合适的模型，避免了过度支付或性能不佳的问题。与传统的离线路由器不同，BaRP在训练时模拟在线反馈，支持在测试时根据用户偏好调整性能和成本的权衡。实验结果表明，BaRP在多个任务上均优于强大的离线路由器和大型语言模型。'}}}, {'id': 'https://huggingface.co/papers/2510.07048', 'title': 'Search-R3: Unifying Reasoning and Embedding Generation in Large Language\n  Models', 'url': 'https://huggingface.co/papers/2510.07048', 'abstract': "Search-R3 is a framework that adapts LLMs to generate effective search embeddings through chain-of-thought reasoning, supervised learning, and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite their remarkable natural language understanding capabilities, Large Language Models (LLMs) have been underutilized for retrieval tasks. We present Search-R3, a novel framework that addresses this limitation by adapting LLMs to generate search embeddings as a direct output of their reasoning process. Our approach exploits LLMs' chain-of-thought capabilities, allowing them to produce more effective embeddings by reasoning step-by-step through complex semantic analyses. We implement this through three complementary mechanisms. (1) a supervised learning stage enables the model's ability to produce quality embeddings, (2) a reinforcement learning (RL) methodology that optimizes embedding generation alongside reasoning, and (3) a specialized RL environment that efficiently handles evolving embedding representations without requiring complete corpus re-encoding at each training iteration. Our extensive evaluations on diverse benchmarks demonstrate that Search-R3 significantly outperforms prior methods by unifying the reasoning and embedding generation processes. This integrated post-training approach represents a substantial advancement in handling complex knowledge-intensive tasks that require both sophisticated reasoning and effective information retrieval. Project page: https://github.com/ytgui/Search-R3", 'score': 3, 'issue_id': 6352, 'pub_date': '2025-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': 'cb38160debab1376', 'authors': ['Yuntao Gui', 'James Cheng'], 'affiliations': ['The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.07048.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#optimization', '#rlhf', '#rl'], 'emoji': '🔍', 'ru': {'title': 'Рассуждая к лучшему поиску: LLM генерируют эмбеддинги через chain-of-thought', 'desc': 'Search-R3 - это новый фреймворк, который адаптирует большие языковые модели (LLM) для генерации эффективных эмбеддингов для поиска информации. Подход использует способность LLM к chain-of-thought рассуждениям, позволяя моделям создавать более качественные эмбеддинги через пошаговый семантический анализ. Метод включает три механизма: обучение с учителем для базовой способности генерировать эмбеддинги, reinforcement learning для оптимизации процесса генерации эмбеддингов вместе с рассуждениями, и специальную RL-среду для эффективной работы с эволюционирующими представлениями без полного переиндексирования корпуса на каждой итерации. Эксперименты показывают значительное превосходство Search-R3 над предыдущими методами за счёт объединения процессов рассуждения и генерации эмбеддингов.'}, 'en': {'title': 'Empowering LLMs for Enhanced Search Embeddings', 'desc': 'Search-R3 is a framework designed to enhance the performance of Large Language Models (LLMs) in generating search embeddings for retrieval tasks. It utilizes chain-of-thought reasoning to produce embeddings that reflect a deeper understanding of the input data. The framework incorporates supervised learning to ensure high-quality embeddings, while reinforcement learning optimizes the generation process. By integrating these methods, Search-R3 significantly improves the effectiveness of LLMs in complex knowledge-intensive tasks, making them more suitable for information retrieval applications.'}, 'zh': {'title': 'Search-R3：智能搜索嵌入生成的新框架', 'desc': 'Search-R3是一个框架，旨在通过链式思维推理、监督学习和强化学习来调整大型语言模型（LLMs），以生成有效的搜索嵌入。该方法利用LLMs的推理能力，使其能够通过逐步分析复杂语义来生成更有效的嵌入。框架包括三个互补机制：监督学习阶段提高嵌入质量，强化学习方法优化嵌入生成，以及一个专门的强化学习环境高效处理不断演变的嵌入表示。通过在多个基准上的广泛评估，Search-R3显著优于之前的方法，代表了在处理复杂知识密集型任务方面的重要进展。'}}}, {'id': 'https://huggingface.co/papers/2510.08547', 'title': 'R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized\n  Manipulation', 'url': 'https://huggingface.co/papers/2510.08547', 'abstract': 'A real-to-real 3D data generation framework enhances data efficiency for generalized robotic manipulation by augmenting pointcloud observations without simulation.  \t\t\t\t\tAI-generated summary \t\t\t\t Towards the aim of generalized robotic manipulation, spatial generalization is the most fundamental capability that requires the policy to work robustly under different spatial distribution of objects, environment and agent itself. To achieve this, substantial human demonstrations need to be collected to cover different spatial configurations for training a generalized visuomotor policy via imitation learning. Prior works explore a promising direction that leverages data generation to acquire abundant spatially diverse data from minimal source demonstrations. However, most approaches face significant sim-to-real gap and are often limited to constrained settings, such as fixed-base scenarios and predefined camera viewpoints. In this paper, we propose a real-to-real 3D data generation framework (R2RGen) that directly augments the pointcloud observation-action pairs to generate real-world data. R2RGen is simulator- and rendering-free, thus being efficient and plug-and-play. Specifically, given a single source demonstration, we introduce an annotation mechanism for fine-grained parsing of scene and trajectory. A group-wise augmentation strategy is proposed to handle complex multi-object compositions and diverse task constraints. We further present camera-aware processing to align the distribution of generated data with real-world 3D sensor. Empirically, R2RGen substantially enhances data efficiency on extensive experiments and demonstrates strong potential for scaling and application on mobile manipulation.', 'score': 2, 'issue_id': 6344, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'a174dc6a582d8dff', 'authors': ['Xiuwei Xu', 'Angyuan Ma', 'Hankun Li', 'Bingyao Yu', 'Zheng Zhu', 'Jie Zhou', 'Jiwen Lu'], 'affiliations': ['GigaAI', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.08547.jpg', 'data': {'categories': ['#3d', '#transfer_learning', '#robotics', '#data', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Генерация реальных 3D-данных для обучения роботов без симуляции', 'desc': 'Статья представляет R2RGen - фреймворк для генерации данных, который напрямую аугментирует пары наблюдений-действий в формате pointcloud для робототехники. Метод работает без симуляторов и рендеринга, решая проблему sim-to-real gap и позволяя создавать разнообразные пространственные конфигурации из одной демонстрации. Используется механизм аннотации для детального разбора сцены и групповая стратегия аугментации для работы с множественными объектами. Подход значительно повышает эффективность использования данных при обучении visuomotor policy через imitation learning, особенно для мобильной манипуляции.'}, 'en': {'title': 'Enhancing Robotic Manipulation with Real-to-Real Data Generation', 'desc': 'This paper introduces a new framework called R2RGen for generating 3D data that improves the efficiency of training robotic manipulation systems. Unlike previous methods that rely on simulations, R2RGen works directly with real-world data by augmenting pointcloud observations from a single demonstration. It employs a unique annotation mechanism to analyze scenes and trajectories, along with a group-wise augmentation strategy to manage complex object interactions. The framework is designed to be efficient and adaptable, making it suitable for various mobile manipulation tasks while significantly reducing the need for extensive human demonstrations.'}, 'zh': {'title': '提升机器人操作的数据效率', 'desc': '本文提出了一种真实到真实的3D数据生成框架（R2RGen），旨在提高机器人操作的数据信息效率。该框架通过增强点云观察-动作对，直接生成真实世界的数据，而无需使用模拟器或渲染。R2RGen引入了一种注释机制，以便对场景和轨迹进行细致解析，并采用了分组增强策略来处理复杂的多物体组合和多样的任务约束。实验结果表明，R2RGen在数据效率上显著提升，展示了在移动操作中的强大应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2510.06209', 'title': 'Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models', 'url': 'https://huggingface.co/papers/2510.06209', 'abstract': 'A novel approach combining driving models and generative world models evaluates and enhances the realism and generalization of synthetic video data for autonomous vehicle testing and planning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in generative models have sparked exciting new possibilities in the field of autonomous vehicles. Specifically, video generation models are now being explored as controllable virtual testing environments. Simultaneously, end-to-end (E2E) driving models have emerged as a streamlined alternative to conventional modular autonomous driving systems, gaining popularity for their simplicity and scalability. However, the application of these techniques to simulation and planning raises important questions. First, while video generation models can generate increasingly realistic videos, can these videos faithfully adhere to the specified conditions and be realistic enough for E2E autonomous planner evaluation? Second, given that data is crucial for understanding and controlling E2E planners, how can we gain deeper insights into their biases and improve their ability to generalize to out-of-distribution scenarios? In this work, we bridge the gap between the driving models and generative world models (Drive&Gen) to address these questions. We propose novel statistical measures leveraging E2E drivers to evaluate the realism of generated videos. By exploiting the controllability of the video generation model, we conduct targeted experiments to investigate distribution gaps affecting E2E planner performance. Finally, we show that synthetic data produced by the video generation model offers a cost-effective alternative to real-world data collection. This synthetic data effectively improves E2E model generalization beyond existing Operational Design Domains, facilitating the expansion of autonomous vehicle services into new operational contexts.', 'score': 2, 'issue_id': 6357, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '2dddfb45a1db9a1f', 'authors': ['Jiahao Wang', 'Zhenpei Yang', 'Yijing Bai', 'Yingwei Li', 'Yuliang Zou', 'Bo Sun', 'Abhijit Kundu', 'Jose Lezama', 'Luna Yue Huang', 'Zehao Zhu', 'Jyh-Jing Hwang', 'Dragomir Anguelov', 'Mingxing Tan', 'Chiyu Max Jiang'], 'affiliations': ['Google DeepMind', 'Johns Hopkins University', 'Waymo'], 'pdf_title_img': 'assets/pdf/title_img/2510.06209.jpg', 'data': {'categories': ['#synthetic', '#video', '#optimization', '#dataset', '#agents'], 'emoji': '🚗', 'ru': {'title': 'Виртуальные дороги: генерация видео для обучения автопилотов', 'desc': 'Исследователи объединили модели автономного вождения и генеративные world models для оценки и улучшения синтетических видеоданных. Они предложили новые статистические метрики для оценки реалистичности сгенерированных видео с помощью end-to-end моделей вождения. С помощью контролируемой генерации видео авторы изучили проблемы обобщения планировщиков на out-of-distribution сценарии. Синтетические данные оказались эффективной и экономичной альтернативой реальным данным для расширения возможностей автономных транспортных систем.'}, 'en': {'title': 'Enhancing Autonomous Vehicle Testing with Synthetic Video Data', 'desc': 'This paper presents a new method that combines driving models with generative world models to improve the quality and applicability of synthetic video data for testing autonomous vehicles. It addresses the challenge of ensuring that generated videos are realistic and meet specific conditions necessary for evaluating end-to-end (E2E) driving planners. The authors introduce statistical measures to assess the realism of these videos and conduct experiments to understand how distribution gaps can impact E2E planner performance. Ultimately, the study demonstrates that synthetic data can serve as a valuable and cost-effective resource for enhancing the generalization capabilities of E2E models in diverse driving scenarios.'}, 'zh': {'title': '合成数据助力自动驾驶模型的泛化与真实感提升', 'desc': '本文提出了一种新方法，将驾驶模型与生成世界模型相结合，以评估和增强合成视频数据的真实性和泛化能力，适用于自动驾驶车辆的测试和规划。研究探讨了视频生成模型作为可控虚拟测试环境的应用，同时提出了端到端（E2E）驾驶模型作为传统模块化自动驾驶系统的简化替代方案。通过利用E2E驾驶模型，本文提出了新的统计测量方法来评估生成视频的真实性，并进行针对性实验以研究影响E2E规划者性能的分布差距。最终，研究表明，生成模型产生的合成数据是收集真实世界数据的成本有效替代方案，能够有效提升E2E模型在新操作环境中的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2509.23500', 'title': 'Beyond Outliers: A Study of Optimizers Under Quantization', 'url': 'https://huggingface.co/papers/2509.23500', 'abstract': 'The study investigates how different optimizers impact model performance under post-training and quantization-aware training quantization, finding that Shampoo optimizer shows the lowest accuracy degradation and highest parameter efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t As new optimizers gain traction and model quantization becomes standard for efficient deployment, a key question arises: how does the choice of optimizer affect model performance in the presence of quantization? Despite progress in both areas, systematic evidence on optimizer-quantization interactions remains limited. To fill this gap, we study the impact of optimizer choice on model robustness under quantization, considering both post-training quantization (PTQ), and quantization-aware training (QAT). We first train full-precision models, ranging from 50M to 1.5B parameters, with six optimizers, to explore the hyperparameter landscape, and establish well-tuned baselines. We then apply PTQ to evaluate how model performance degrades when trained with different optimizers. We find that outlier-related metrics, such as the max-to-mean ratio (MMR) and Kurtosis, fail to predict the PTQ performance across different optimizers. We show analytically that this is due to the MMR capturing only isolated layer errors, while ignoring how quantization errors accumulate and propagate through the network. To study the QAT degradation, we train quantized models from scratch and compare them to our original-precision baselines. We find that optimizers performing well in the original pretraining setup may not remain optimal under QAT, and that models trained with Shampoo show the lowest accuracy degradation. Finally, we derive scaling laws for quantization-aware training under different optimizers, showing that Shampoo achieves the highest parameter efficiency of all tested optimizers.', 'score': 2, 'issue_id': 6350, 'pub_date': '2025-09-27', 'pub_date_card': {'ru': '27 сентября', 'en': 'September 27', 'zh': '9月27日'}, 'hash': 'b27f10b0ad4b3452', 'authors': ['Georgios Vlassis', 'Saleh Ashkboos', 'Alexandra Volkova', 'Torsten Hoefler', 'Dan Alistarh'], 'affiliations': ['ETH Zurich', 'ISTA', 'Red Hat AI'], 'pdf_title_img': 'assets/pdf/title_img/2509.23500.jpg', 'data': {'categories': ['#optimization', '#training', '#inference'], 'emoji': '🧴', 'ru': {'title': 'Shampoo оптимизатор показывает лучшую устойчивость при квантизации моделей', 'desc': 'Исследование изучает, как различные оптимизаторы влияют на производительность моделей при квантизации - как при посттренинговой (PTQ), так и при квантизации в процессе обучения (QAT). Авторы обнаружили, что традиционные метрики выбросов не предсказывают качество работы PTQ, так как они не учитывают накопление ошибок квантизации по всей сети. Оптимизатор Shampoo продемонстрировал наименьшее падение точности при квантизации и самую высокую эффективность по параметрам среди всех протестированных оптимизаторов. Результаты показывают, что оптимизаторы, хорошо работающие при полноточном обучении, могут терять преимущество при использовании QAT.'}, 'en': {'title': 'Optimizing Performance: Shampoo Shines in Quantization Challenges', 'desc': 'This study explores how different optimizers affect the performance of machine learning models when they undergo quantization, a process that reduces model size for efficient deployment. The researchers found that the Shampoo optimizer resulted in the least accuracy loss and the best parameter efficiency compared to other optimizers. They conducted experiments with various model sizes and discovered that traditional metrics like max-to-mean ratio do not effectively predict performance degradation during quantization. The findings highlight the importance of selecting the right optimizer for both post-training quantization and quantization-aware training to maintain model robustness.'}, 'zh': {'title': '优化器选择影响量化模型性能的研究', 'desc': '本研究探讨了不同优化器在后训练量化和量化感知训练中的模型性能影响。研究发现，Shampoo优化器在准确性下降和参数效率方面表现最佳。我们分析了优化器选择如何影响模型在量化下的鲁棒性，并发现传统的性能预测指标无法有效预测量化后的表现。最终，我们提出了不同优化器下量化感知训练的缩放规律，证明Shampoo优化器在所有测试的优化器中具有最高的参数效率。'}}}, {'id': 'https://huggingface.co/papers/2510.08271', 'title': 'SViM3D: Stable Video Material Diffusion for Single Image 3D Generation', 'url': 'https://huggingface.co/papers/2510.08271', 'abstract': 'A latent video diffusion model predicts multi-view consistent PBR materials from a single image, enabling relighting and novel view synthesis with high quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Stable Video Materials 3D (SViM3D), a framework to predict multi-view consistent physically based rendering (PBR) materials, given a single image. Recently, video diffusion models have been successfully used to reconstruct 3D objects from a single image efficiently. However, reflectance is still represented by simple material models or needs to be estimated in additional steps to enable relighting and controlled appearance edits. We extend a latent video diffusion model to output spatially varying PBR parameters and surface normals jointly with each generated view based on explicit camera control. This unique setup allows for relighting and generating a 3D asset using our model as neural prior. We introduce various mechanisms to this pipeline that improve quality in this ill-posed setting. We show state-of-the-art relighting and novel view synthesis performance on multiple object-centric datasets. Our method generalizes to diverse inputs, enabling the generation of relightable 3D assets useful in AR/VR, movies, games and other visual media.', 'score': 1, 'issue_id': 6353, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'f8983c98d92dcd71', 'authors': ['Andreas Engelhardt', 'Mark Boss', 'Vikram Voletti', 'Chun-Han Yao', 'Hendrik P. A. Lensch', 'Varun Jampani'], 'affiliations': ['Stability AI', 'University of Tubingen'], 'pdf_title_img': 'assets/pdf/title_img/2510.08271.jpg', 'data': {'categories': ['#games', '#3d', '#diffusion', '#video', '#cv'], 'emoji': '💎', 'ru': {'title': 'Реалистичные 3D-материалы из одной фотографии', 'desc': 'Представлена система SViM3D, которая предсказывает физически корректные материалы (PBR) для 3D-объектов из одного изображения. Модель расширяет latent video diffusion модель для одновременного вывода параметров материалов и нормалей поверхности с явным управлением камерой. Это позволяет перепосвечивать сцену и генерировать полноценные 3D-ассеты с консистентными видами. Метод показывает лучшие результаты в синтезе новых ракурсов и релайтинге, применим для AR/VR, игр и визуальных медиа.'}, 'en': {'title': 'Transforming Single Images into Relightable 3D Assets', 'desc': 'The paper introduces Stable Video Materials 3D (SViM3D), a novel framework that utilizes a latent video diffusion model to predict multi-view consistent physically based rendering (PBR) materials from a single image. This approach allows for the generation of high-quality 3D assets that can be relit and viewed from different angles. By jointly outputting spatially varying PBR parameters and surface normals, the model enhances the realism of generated views while maintaining explicit camera control. The authors demonstrate that their method achieves state-of-the-art performance in relighting and novel view synthesis across various object-centric datasets, making it applicable for use in augmented reality, virtual reality, and other visual media.'}, 'zh': {'title': '从单图像生成高质量3D材料', 'desc': '本文提出了一种名为稳定视频材料3D（SViM3D）的框架，能够从单张图像预测多视角一致的物理基础渲染（PBR）材料。该方法扩展了潜在视频扩散模型，能够同时输出空间变化的PBR参数和表面法线，支持基于相机控制的生成视图。通过引入多种机制，本文提高了在复杂设置下的生成质量。该方法在多个以物体为中心的数据集上展示了最先进的重光照和新视图合成性能，适用于增强现实、虚拟现实、电影、游戏等视觉媒体。'}}}, {'id': 'https://huggingface.co/papers/2510.07314', 'title': 'GyroSwin: 5D Surrogates for Gyrokinetic Plasma Turbulence Simulations', 'url': 'https://huggingface.co/papers/2510.07314', 'abstract': 'GyroSwin, a scalable 5D neural surrogate model, captures nonlinear gyrokinetic dynamics and improves heat flux prediction in plasma turbulence simulations.  \t\t\t\t\tAI-generated summary \t\t\t\t Nuclear fusion plays a pivotal role in the quest for reliable and sustainable energy production. A major roadblock to viable fusion power is understanding plasma turbulence, which significantly impairs plasma confinement, and is vital for next-generation reactor design. Plasma turbulence is governed by the nonlinear gyrokinetic equation, which evolves a 5D distribution function over time. Due to its high computational cost, reduced-order models are often employed in practice to approximate turbulent transport of energy. However, they omit nonlinear effects unique to the full 5D dynamics. To tackle this, we introduce GyroSwin, the first scalable 5D neural surrogate that can model 5D nonlinear gyrokinetic simulations, thereby capturing the physical phenomena neglected by reduced models, while providing accurate estimates of turbulent heat transport.GyroSwin (i) extends hierarchical Vision Transformers to 5D, (ii) introduces cross-attention and integration modules for latent 3Dleftrightarrow5D interactions between electrostatic potential fields and the distribution function, and (iii) performs channelwise mode separation inspired by nonlinear physics. We demonstrate that GyroSwin outperforms widely used reduced numerics on heat flux prediction, captures the turbulent energy cascade, and reduces the cost of fully resolved nonlinear gyrokinetics by three orders of magnitude while remaining physically verifiable. GyroSwin shows promising scaling laws, tested up to one billion parameters, paving the way for scalable neural surrogates for gyrokinetic simulations of plasma turbulence.', 'score': 1, 'issue_id': 6357, 'pub_date': '2025-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': 'c2c5893d293a2ad1', 'authors': ['Fabian Paischer', 'Gianluca Galletti', 'William Hornsby', 'Paul Setinek', 'Lorenzo Zanisi', 'Naomi Carey', 'Stanislas Pamela', 'Johannes Brandstetter'], 'affiliations': ['ELLIS Unit, Institute for Machine Learning, JKU Linz', 'EMMI AI, Linz', 'United Kingdom Atomic Energy Authority, Culham campus'], 'pdf_title_img': 'assets/pdf/title_img/2510.07314.jpg', 'data': {'categories': ['#architecture', '#training', '#science', '#optimization', '#dataset'], 'emoji': '⚛️', 'ru': {'title': 'Нейросеть для термоядерной плазмы: 5D-моделирование турбулентности', 'desc': 'Статья представляет GyroSwin — первую масштабируемую нейронную суррогатную модель для моделирования 5D-динамики плазменной турбулентности в термоядерном синтезе. Модель расширяет иерархические Vision Transformers на пять измерений и использует механизмы cross-attention для взаимодействия между электростатическим потенциалом и функцией распределения. GyroSwin превосходит традиционные редуцированные численные методы в предсказании теплового потока и улавливает эффекты турбулентного энергетического каскада. Модель снижает вычислительные затраты на три порядка по сравнению с полным нелинейным решением, демонстрируя хорошее масштабирование до миллиарда параметров.'}, 'en': {'title': 'GyroSwin: Revolutionizing Plasma Turbulence Predictions with 5D Neural Surrogates', 'desc': 'GyroSwin is a novel 5D neural surrogate model designed to accurately capture the complex nonlinear dynamics of plasma turbulence, which is crucial for nuclear fusion research. By extending hierarchical Vision Transformers to handle 5D data, it incorporates advanced techniques like cross-attention and channelwise mode separation to effectively model interactions between electrostatic fields and particle distribution functions. This model significantly improves heat flux predictions compared to traditional reduced-order models, which often overlook important nonlinear effects. GyroSwin not only enhances computational efficiency by reducing the cost of simulations but also maintains physical accuracy, making it a promising tool for future plasma turbulence studies.'}, 'zh': {'title': 'GyroSwin：提升等离子体湍流模拟的五维神经代理模型', 'desc': 'GyroSwin是一种可扩展的五维神经代理模型，能够捕捉非线性回旋动力学，并改善等离子体湍流模拟中的热通量预测。该模型通过扩展层次化视觉变换器，结合交叉注意力和集成模块，处理电静势场与分布函数之间的三维到五维的相互作用。GyroSwin在热通量预测上超越了常用的降阶数值方法，并有效捕捉湍流能量级联现象。它的计算成本比完全解析的非线性回旋动力学降低了三个数量级，同时保持了物理可验证性。'}}}, {'id': 'https://huggingface.co/papers/2510.02994', 'title': 'Towards Scalable and Consistent 3D Editing', 'url': 'https://huggingface.co/papers/2510.02994', 'abstract': 'A new framework, 3DEditFormer, uses a 3D-structure-preserving conditional transformer to enable precise and consistent 3D editing without manual masks, outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D editing - the task of locally modifying the geometry or appearance of a 3D asset - has wide applications in immersive content creation, digital entertainment, and AR/VR. However, unlike 2D editing, it remains challenging due to the need for cross-view consistency, structural fidelity, and fine-grained controllability. Existing approaches are often slow, prone to geometric distortions, or dependent on manual and accurate 3D masks that are error-prone and impractical. To address these challenges, we advance both the data and model fronts. On the data side, we introduce 3DEditVerse, the largest paired 3D editing benchmark to date, comprising 116,309 high-quality training pairs and 1,500 curated test pairs. Built through complementary pipelines of pose-driven geometric edits and foundation model-guided appearance edits, 3DEditVerse ensures edit locality, multi-view consistency, and semantic alignment. On the model side, we propose 3DEditFormer, a 3D-structure-preserving conditional transformer. By enhancing image-to-3D generation with dual-guidance attention and time-adaptive gating, 3DEditFormer disentangles editable regions from preserved structure, enabling precise and consistent edits without requiring auxiliary 3D masks. Extensive experiments demonstrate that our framework outperforms state-of-the-art baselines both quantitatively and qualitatively, establishing a new standard for practical and scalable 3D editing. Dataset and code will be released. Project: https://www.lv-lab.org/3DEditFormer/', 'score': 1, 'issue_id': 6347, 'pub_date': '2025-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': '6a3db95204c8dbbc', 'authors': ['Ruihao Xia', 'Yang Tang', 'Pan Zhou'], 'affiliations': ['East China University of Science and Technology', 'Singapore Management University'], 'pdf_title_img': 'assets/pdf/title_img/2510.02994.jpg', 'data': {'categories': ['#3d', '#benchmark', '#dataset', '#architecture', '#open_source'], 'emoji': '🎨', 'ru': {'title': 'Точное 3D-редактирование без масок через трансформер', 'desc': 'Исследователи представили 3DEditFormer — новый фреймворк для редактирования 3D-объектов, который использует условный трансформер с сохранением 3D-структуры. Они создали 3DEditVerse — крупнейший датасет для 3D-редактирования, содержащий более 116 тысяч обучающих пар. Модель способна точно редактировать геометрию и внешний вид 3D-ассетов без необходимости вручную создавать 3D-маски, благодаря механизму dual-guidance attention и time-adaptive gating. Метод превосходит существующие подходы, обеспечивая консистентность между видами и сохранение структуры объектов.'}, 'en': {'title': 'Revolutionizing 3D Editing with 3DEditFormer', 'desc': 'The paper introduces 3DEditFormer, a novel framework designed for precise 3D editing without the need for manual masks. It leverages a 3D-structure-preserving conditional transformer to ensure that edits maintain structural integrity and cross-view consistency. The authors also present 3DEditVerse, a comprehensive dataset that includes over 116,000 high-quality training pairs, which aids in training the model effectively. Through extensive experiments, 3DEditFormer demonstrates superior performance compared to existing methods, setting a new benchmark in the field of 3D editing.'}, 'zh': {'title': '无须手动遮罩的精确3D编辑', 'desc': '3DEditFormer是一种新的框架，利用3D结构保持的条件变换器，实现精确且一致的3D编辑，无需手动遮罩，超越了现有方法。该方法解决了3D编辑中常见的跨视图一致性、结构保真度和细粒度可控性等挑战。我们引入了3DEditVerse，这是迄今为止最大的配对3D编辑基准，包含116,309对高质量训练样本和1,500对精心挑选的测试样本。通过双重引导注意力和时间自适应门控，3DEditFormer能够从保留的结构中解耦可编辑区域，实现精确且一致的编辑。'}}}, {'id': 'https://huggingface.co/papers/2510.02590', 'title': 'Use the Online Network If You Can: Towards Fast and Stable Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2510.02590', 'abstract': 'MINTO, a novel update rule using the minimum estimate between target and online networks, enhances stable and faster value function learning in deep reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t The use of target networks is a popular approach for estimating value functions in deep Reinforcement Learning (RL). While effective, the target network remains a compromise solution that preserves stability at the cost of slowly moving targets, thus delaying learning. Conversely, using the online network as a bootstrapped target is intuitively appealing, albeit well-known to lead to unstable learning. In this work, we aim to obtain the best out of both worlds by introducing a novel update rule that computes the target using the MINimum estimate between the Target and Online network, giving rise to our method, MINTO. Through this simple, yet effective modification, we show that MINTO enables faster and stable value function learning, by mitigating the potential overestimation bias of using the online network for bootstrapping. Notably, MINTO can be seamlessly integrated into a wide range of value-based and actor-critic algorithms with a negligible cost. We evaluate MINTO extensively across diverse benchmarks, spanning online and offline RL, as well as discrete and continuous action spaces. Across all benchmarks, MINTO consistently improves performance, demonstrating its broad applicability and effectiveness.', 'score': 1, 'issue_id': 6361, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': '114ac96fa15e2301', 'authors': ['Ahmed Hendawy', 'Henrik Metternich', 'Théo Vincent', 'Mahdi Kallel', 'Jan Peters', "Carlo D'Eramo"], 'affiliations': ['German Research Center for AI (DFKI)', 'Hessian.AI', 'Robotics Institute Germany (RIG)', 'Technical University of Darmstadt', 'University of Wurzburg'], 'pdf_title_img': 'assets/pdf/title_img/2510.02590.jpg', 'data': {'categories': ['#games', '#optimization', '#benchmark', '#training', '#rl'], 'emoji': '⚖️', 'ru': {'title': 'Лучшее из двух миров: стабильное обучение с быстрыми целями', 'desc': 'Статья представляет MINTO — новое правило обновления для обучения функций ценности в deep reinforcement learning. Метод использует минимальную оценку между целевой (target) и онлайн сетями, что позволяет достичь баланса между стабильностью и скоростью обучения. MINTO решает проблему переоценки (overestimation bias), которая возникает при использовании онлайн сети для бутстрэппинга, сохраняя при этом быстроту обучения. Метод легко интегрируется в различные value-based и actor-critic алгоритмы и показывает улучшение производительности на множестве бенчмарков в online и offline RL.'}, 'en': {'title': 'MINTO: Merging Stability and Speed in Reinforcement Learning', 'desc': 'MINTO is a new update rule designed to improve value function learning in deep reinforcement learning by using the minimum estimate from both target and online networks. This approach addresses the trade-off between stability and speed in learning, as traditional target networks can slow down the learning process while online networks can lead to instability. By combining these two methods, MINTO reduces the overestimation bias that often occurs with online networks, resulting in faster and more stable learning. The method is versatile and can be easily integrated into various reinforcement learning algorithms, showing significant performance improvements across multiple benchmarks.'}, 'zh': {'title': 'MINTO：加速深度强化学习的稳定性与速度', 'desc': 'MINTO是一种新颖的更新规则，它通过计算目标网络和在线网络之间的最小估计值，来增强深度强化学习中的价值函数学习的稳定性和速度。传统的目标网络虽然有效，但由于目标移动缓慢，导致学习延迟。MINTO通过减少在线网络引起的过度估计偏差，提供了更快且稳定的学习效果。该方法可以无缝集成到多种基于价值和演员-评论家算法中，且几乎不增加计算成本。'}}}, {'id': 'https://huggingface.co/papers/2509.24797', 'title': 'Fidelity-Aware Data Composition for Robust Robot Generalization', 'url': 'https://huggingface.co/papers/2509.24797', 'abstract': 'Coherent Information Fidelity Tuning (CIFT) improves out-of-distribution generalization in robot policies by optimizing data composition with a generative engine, enhancing robustness and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Generalist robot policies trained on large-scale, visually homogeneous datasets can be susceptible to shortcut learning, which impairs their out-of-distribution (OOD) generalization. While generative data augmentation is a common approach to introduce diversity, it presents a subtle challenge: data composition. Naively mixing real and synthetic data can corrupt the learning signal, as this process often prioritizes visual diversity at the expense of information fidelity. This paper suggests that robust generalization depends on principled, fidelity-aware data composition. We introduce Coherent Information Fidelity Tuning (CIFT), a framework that treats data composition as an optimization problem. CIFT uses a practical proxy for Information Fidelity based on the feature-space geometry of a dataset. This enables the identification of a phase transition, termed the Decoherence Point, where training stability degrades. The framework includes a generative engine, Multi-View Video Augmentation (MVAug), to synthesize a causally disentangled data spectrum for this tuning process. Applying CIFT to policy architectures such as pi_0 and Diffusion Policy improves OOD success rates by over 54\\%. These results indicate that fidelity-aware composition, beyond data synthesis alone, is an important component for developing robust, general-purpose robots.', 'score': 1, 'issue_id': 6350, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '78359eb8e6b5788a', 'authors': ['Zizhao Tong', 'Di Chen', 'Sicheng Hu', 'Hongwei Fan', 'Liliang Chen', 'Guanghui Ren', 'Hao Tang', 'Hao Dong', 'Ling Shao'], 'affiliations': ['Agibot', 'CFCS', 'PKU-Agibot Lab', 'School of Computer Science, Peking University', 'State Key Laboratory', 'UCAS-Terminus AI Lab, University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2509.24797.jpg', 'data': {'categories': ['#synthetic', '#optimization', '#training', '#robotics', '#data', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Оптимальное смешивание данных для робастных роботов', 'desc': 'Статья представляет метод CIFT (Coherent Information Fidelity Tuning) для улучшения обобщающей способности робототехнических политик на новых данных. Проблема заключается в том, что простое смешивание реальных и синтетических данных может нарушить обучающий сигнал, так как приоритет отдаётся визуальному разнообразию в ущерб точности информации. CIFT рассматривает композицию данных как задачу оптимизации, используя геометрию пространства признаков для определения критической точки деградации стабильности обучения. Применение CIFT к таким архитектурам как pi_0 и Diffusion Policy повышает успешность на out-of-distribution данных более чем на 54%.'}, 'en': {'title': 'Enhancing Robot Learning with Fidelity-Aware Data Composition', 'desc': 'This paper presents Coherent Information Fidelity Tuning (CIFT), a method designed to enhance the out-of-distribution (OOD) generalization of robot policies. It addresses the issue of shortcut learning that arises from training on visually homogeneous datasets by optimizing how real and synthetic data are combined. CIFT treats data composition as an optimization problem, focusing on maintaining information fidelity rather than just visual diversity. The framework includes a generative engine that creates a diverse data spectrum, leading to significant improvements in the robustness and performance of robot policies.'}, 'zh': {'title': '提升机器人策略的泛化能力', 'desc': '本文提出了一种名为一致性信息保真调优（CIFT）的方法，旨在提高机器人策略在分布外（OOD）环境中的泛化能力。CIFT通过优化数据组合，利用生成引擎来增强数据的多样性，同时保持信息的保真度。研究表明，简单地混合真实和合成数据可能会损害学习信号，因此需要一种更有原则的数据组合方法。通过引入多视角视频增强（MVAug），CIFT能够合成因果解耦的数据谱，从而显著提高机器人策略的性能。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (5)', '#agents (11)', '#agi (4)', '#alignment (4)', '#architecture (8)', '#audio', '#benchmark (20)', '#cv (6)', '#data (6)', '#dataset (10)', '#diffusion (7)', '#ethics (1)', '#games (8)', '#graphs (1)', '#hallucinations (1)', '#healthcare (1)', '#inference (4)', '#interpretability (2)', '#leakage', '#long_context (5)', '#low_resource', '#machine_translation', '#math (4)', '#multilingual', '#multimodal (12)', '#open_source (6)', '#optimization (29)', '#plp', '#rag (1)', '#reasoning (18)', '#rl (15)', '#rlhf (9)', '#robotics (5)', '#science (4)', '#security (1)', '#small_models (1)', '#story_generation', '#survey (1)', '#synthetic (3)', '#training (26)', '#transfer_learning (5)', '#video (10)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-10-12 01:53',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-10-12 01:53')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-10-12 01:53')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    