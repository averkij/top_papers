{
    "date": {
        "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 11",
        "zh": "2æœˆ11æ—¥"
    },
    "time_utc": "2025-02-11 05:10",
    "weekday": 1,
    "issue_id": 2142,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.06781",
            "title": "Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning",
            "url": "https://huggingface.co/papers/2502.06781",
            "abstract": "Reasoning abilities, especially those for solving complex math problems, are crucial components of general intelligence. Recent advances by proprietary companies, such as o-series models of OpenAI, have made remarkable progress on reasoning tasks. However, the complete technical details remain unrevealed, and the techniques that are believed certainly to be adopted are only reinforcement learning (RL) and the long chain of thoughts. This paper proposes a new RL framework, termed OREAL, to pursue the performance limit that can be achieved through Outcome REwArd-based reinforcement Learning for mathematical reasoning tasks, where only binary outcome rewards are easily accessible. We theoretically prove that behavior cloning on positive trajectories from best-of-N (BoN) sampling is sufficient to learn the KL-regularized optimal policy in binary feedback environments. This formulation further implies that the rewards of negative samples should be reshaped to ensure the gradient consistency between positive and negative samples. To alleviate the long-existing difficulties brought by sparse rewards in RL, which are even exacerbated by the partial correctness of the long chain of thought for reasoning tasks, we further apply a token-level reward model to sample important tokens in reasoning trajectories for learning. With OREAL, for the first time, a 7B model can obtain 94.0 pass@1 accuracy on MATH-500 through RL, being on par with 32B models. OREAL-32B also surpasses previous 32B models trained by distillation with 95.0 pass@1 accuracy on MATH-500. Our investigation also indicates the importance of initial policy models and training queries for RL. Code, models, and data will be released to benefit future researchhttps://github.com/InternLM/OREAL.",
            "score": 13,
            "issue_id": 2142,
            "pub_date": "2025-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "9cd2694b7c865b94",
            "authors": [
                "Chengqi Lyu",
                "Songyang Gao",
                "Yuzhe Gu",
                "Wenwei Zhang",
                "Jianfei Gao",
                "Kuikun Liu",
                "Ziyi Wang",
                "Shuaibin Li",
                "Qian Zhao",
                "Haian Huang",
                "Weihan Cao",
                "Jiangning Liu",
                "Hongwei Liu",
                "Junnan Liu",
                "Songyang Zhang",
                "Dahua Lin",
                "Kai Chen"
            ],
            "affiliations": [
                "HKGAI under InnoHK",
                "MMLab, The Chinese University of Hong Kong",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06781.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#rl",
                    "#reasoning",
                    "#optimization",
                    "#math"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "OREAL: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ OREAL Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ»Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ… Ğ¸Ğ· Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ best-of-N Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ² ÑÑ€ĞµĞ´Ğ°Ñ… Ñ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ. Ğ”Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ¡ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ OREAL Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 7B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 94.0% pass@1 Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ MATH-500, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾ Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ 32B Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "OREAL: Advancing AI Reasoning with Outcome-Based Reinforcement Learning",
                    "desc": "This paper introduces a new reinforcement learning framework called OREAL, designed to enhance mathematical reasoning capabilities in AI models. OREAL focuses on using binary outcome rewards to improve learning efficiency, particularly in environments where feedback is sparse. The authors demonstrate that behavior cloning from positive examples can effectively learn optimal policies, while also reshaping negative rewards to maintain gradient consistency. The results show that OREAL achieves high accuracy on mathematical tasks, outperforming larger models and highlighting the significance of initial policy models in the training process."
                },
                "zh": {
                    "title": "OREALï¼šæ•°å­¦æ¨ç†çš„æ–°çªç ´",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç§°ä¸ºOREALï¼Œæ—¨åœ¨æé«˜æ•°å­¦æ¨ç†ä»»åŠ¡çš„æ€§èƒ½ã€‚OREALä½¿ç”¨åŸºäºç»“æœçš„å¥–åŠ±æœºåˆ¶ï¼Œä¸“æ³¨äºäºŒå…ƒç»“æœå¥–åŠ±ï¼Œä»¥è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­çš„ç¨€ç–å¥–åŠ±é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å¯¹æœ€ä½³æ ·æœ¬è¿›è¡Œè¡Œä¸ºå…‹éš†ï¼Œå¯ä»¥æœ‰æ•ˆå­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼Œå¹¶ä¸”éœ€è¦å¯¹è´Ÿæ ·æœ¬çš„å¥–åŠ±è¿›è¡Œé‡å¡‘ä»¥ä¿æŒæ¢¯åº¦ä¸€è‡´æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨OREALçš„7Bæ¨¡å‹åœ¨MATH-500ä¸Šè¾¾åˆ°äº†94.0çš„å‡†ç¡®ç‡ï¼Œè¡¨ç°ä¸32Bæ¨¡å‹ç›¸å½“ï¼Œä¸”OREAL-32Båœ¨åŒä¸€ä»»åŠ¡ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„32Bæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05609",
            "title": "Lossless Acceleration of Large Language Models with Hierarchical Drafting based on Temporal Locality in Speculative Decoding",
            "url": "https://huggingface.co/papers/2502.05609",
            "abstract": "Accelerating inference in Large Language Models (LLMs) is critical for real-time interactions, as they have been widely incorporated into real-world services. Speculative decoding, a fully algorithmic solution, has gained attention for improving inference speed by drafting and verifying tokens, thereby generating multiple tokens in a single forward pass. However, current drafting strategies usually require significant fine-tuning or have inconsistent performance across tasks. To address these challenges, we propose Hierarchy Drafting (HD), a novel lossless drafting approach that organizes various token sources into multiple databases in a hierarchical framework based on temporal locality. In the drafting step, HD sequentially accesses multiple databases to obtain draft tokens from the highest to the lowest locality, ensuring consistent acceleration across diverse tasks and minimizing drafting latency. Our experiments on Spec-Bench using LLMs with 7B and 13B parameters demonstrate that HD outperforms existing database drafting methods, achieving robust inference speedups across model sizes, tasks, and temperatures.",
            "score": 8,
            "issue_id": 2141,
            "pub_date": "2025-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "6f559083c224138c",
            "authors": [
                "Sukmin Cho",
                "Sangjin Choi",
                "Taeho Hwang",
                "Jeongyeon Seo",
                "Soyeong Jeong",
                "Huije Lee",
                "Hoyun Song",
                "Jong C. Park",
                "Youngjin Kwon"
            ],
            "affiliations": [
                "School of Computing, Graduate School of AI, Korea Advanced Institute of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05609.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Hierarchy Drafting (HD). HD Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑÑŒ Ğ½Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ÑÑ Ğº Ğ±Ğ°Ğ·Ğ°Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ HD Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°, Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€."
                },
                "en": {
                    "title": "Boosting Inference Speed with Hierarchy Drafting in LLMs",
                    "desc": "This paper focuses on improving the speed of inference in Large Language Models (LLMs) for real-time applications. It introduces a new method called Hierarchy Drafting (HD), which organizes token sources into a hierarchical structure to enhance the drafting process. By accessing these token databases based on their temporal locality, HD ensures faster and more consistent token generation across various tasks. Experimental results show that HD significantly outperforms existing methods, providing robust speed improvements for LLMs of different sizes and tasks."
                },
                "zh": {
                    "title": "å±‚æ¬¡è‰æ‹Ÿï¼šåŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†çš„æ–°æ–¹æ³•",
                    "desc": "åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†å¯¹äºå®æ—¶äº¤äº’è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ— æŸè‰æ‹Ÿæ–¹æ³•ï¼Œç§°ä¸ºå±‚æ¬¡è‰æ‹Ÿï¼ˆHDï¼‰ï¼Œå®ƒé€šè¿‡åŸºäºæ—¶é—´å±€éƒ¨æ€§çš„å±‚æ¬¡æ¡†æ¶ç»„ç»‡å¤šç§ä»¤ç‰Œæºã€‚HDåœ¨è‰æ‹Ÿæ­¥éª¤ä¸­ä¾æ¬¡è®¿é—®å¤šä¸ªæ•°æ®åº“ï¼Œä»æœ€é«˜åˆ°æœ€ä½çš„å±€éƒ¨æ€§è·å–è‰æ‹Ÿä»¤ç‰Œï¼Œä»è€Œç¡®ä¿åœ¨ä¸åŒä»»åŠ¡ä¸­ä¸€è‡´çš„åŠ é€Ÿæ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHDåœ¨æ¨ç†é€Ÿåº¦ä¸Šä¼˜äºç°æœ‰çš„æ•°æ®åº“è‰æ‹Ÿæ–¹æ³•ï¼Œé€‚ç”¨äºä¸åŒè§„æ¨¡çš„æ¨¡å‹å’Œä»»åŠ¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.03628",
            "title": "The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering",
            "url": "https://huggingface.co/papers/2502.03628",
            "abstract": "Large Vision-Language Models (LVLMs) can reason effectively over both textual and visual inputs, but they tend to hallucinate syntactically coherent yet visually ungrounded contents. In this paper, we investigate the internal dynamics of hallucination by examining the tokens logits rankings throughout the generation process, revealing three key patterns in how LVLMs process information: (1) gradual visual information loss -- visually grounded tokens gradually become less favored throughout generation, and (2) early excitation -- semantically meaningful tokens achieve peak activation in the layers earlier than the final layer. (3) hidden genuine information -- visually grounded tokens though not being eventually decided still retain relatively high rankings at inference. Based on these insights, we propose VISTA (Visual Information Steering with Token-logit Augmentation), a training-free inference-time intervention framework that reduces hallucination while promoting genuine information. VISTA works by combining two complementary approaches: reinforcing visual information in activation space and leveraging early layer activations to promote semantically meaningful decoding. Compared to existing methods, VISTA requires no external supervision and is applicable to various decoding strategies. Extensive experiments show that VISTA on average reduces hallucination by abount 40% on evaluated open-ended generation task, and it consistently outperforms existing methods on four benchmarks across four architectures under three decoding strategies.",
            "score": 7,
            "issue_id": 2140,
            "pub_date": "2025-02-05",
            "pub_date_card": {
                "ru": "5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 5",
                "zh": "2æœˆ5æ—¥"
            },
            "hash": "04b182d80abf9219",
            "authors": [
                "Zhuowei Li",
                "Haizhou Shi",
                "Yunhe Gao",
                "Di Liu",
                "Zhenting Wang",
                "Yuxiao Chen",
                "Ting Liu",
                "Long Zhao",
                "Hao Wang",
                "Dimitris N. Metaxas"
            ],
            "affiliations": [
                "Google DeepMind",
                "Rutgers University",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.03628.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#hallucinations",
                    "#benchmark",
                    "#inference",
                    "#interpretability",
                    "#multimodal"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ¼ĞµÑ‚Ğ¾Ğ´ VISTA",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LVLM) Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹, Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ VISTA Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VISTA Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° 40% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "VISTA: Reducing Hallucination in Vision-Language Models",
                    "desc": "This paper explores the issue of hallucination in Large Vision-Language Models (LVLMs), where the models generate plausible text that does not correspond to visual inputs. The authors identify three patterns in the generation process: a gradual loss of visual information, early activation of semantically meaningful tokens, and the presence of high-ranking visually grounded tokens that are not ultimately chosen. To address these issues, they introduce VISTA, a framework that enhances visual information during inference without requiring additional training. VISTA effectively reduces hallucination by about 40% and outperforms existing methods across multiple benchmarks and architectures."
                },
                "zh": {
                    "title": "å‡å°‘å¹»è§‰ï¼Œæå‡çœŸå®ä¿¡æ¯çš„VISTAæ¡†æ¶",
                    "desc": "å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†æ–‡æœ¬å’Œè§†è§‰è¾“å…¥ï¼Œä½†å®ƒä»¬å¾€å¾€ä¼šäº§ç”Ÿè¯­æ³•ä¸Šè¿è´¯ä½†è§†è§‰ä¸Šä¸çœŸå®çš„å†…å®¹ã€‚æœ¬æ–‡ç ”ç©¶äº†å¹»è§‰çš„å†…éƒ¨åŠ¨æ€ï¼Œå‘ç°LVLMsåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¤„ç†ä¿¡æ¯çš„ä¸‰ç§å…³é”®æ¨¡å¼ï¼šé€æ¸ä¸§å¤±è§†è§‰ä¿¡æ¯ã€æ—©æœŸæ¿€æ´»å’Œéšè—çš„çœŸå®ä¿¡æ¯ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†VISTAï¼ˆè§†è§‰ä¿¡æ¯å¼•å¯¼ä¸æ ‡è®°é€»è¾‘å¢å¼ºï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ¨ç†æ—¶å¹²é¢„æ¡†æ¶ï¼Œæ—¨åœ¨å‡å°‘å¹»è§‰å¹¶ä¿ƒè¿›çœŸå®ä¿¡æ¯çš„ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼ŒVISTAåœ¨å¼€æ”¾å¼ç”Ÿæˆä»»åŠ¡ä¸­å¹³å‡å‡å°‘äº†çº¦40%çš„å¹»è§‰ï¼Œå¹¶åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­åœ¨ä¸‰ç§è§£ç ç­–ç•¥ä¸‹å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06772",
            "title": "ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates",
            "url": "https://huggingface.co/papers/2502.06772",
            "abstract": "We present that hierarchical LLM reasoning via scaling thought templates can effectively optimize the reasoning search space and outperform the mathematical reasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3. We train our ReasonFlux-32B model with only 8 GPUs and introduces three innovations: (i) a structured and generic thought template library, containing around 500 high-level thought templates capable of generalizing to similar or relevant reasoning problems; (ii) performing hierarchical reinforcement learning on a sequence of thought templates instead of long CoTs, optimizing a base LLM to plan out an optimal template trajectory for gradually handling complex problems; (iii) a brand new inference scaling system that enables hierarchical LLM reasoning by adaptively scaling thought templates at inference time. With a template trajectory containing sequential thought templates, our ReasonFlux-32B significantly advances math reasoning capabilities to state-of-the-art levels. Notably, on the MATH benchmark, it achieves an accuracy of 91.2% and surpasses o1-preview by 6.7%. On the USA Math Olympiad (AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems, surpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code: https://github.com/Gen-Verse/ReasonFlux",
            "score": 4,
            "issue_id": 2141,
            "pub_date": "2025-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "1ac59597c9610fb2",
            "authors": [
                "Ling Yang",
                "Zhaochen Yu",
                "Bin Cui",
                "Mengdi Wang"
            ],
            "affiliations": [
                "Peking University",
                "Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06772.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#math",
                    "#benchmark",
                    "#inference"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜: Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ReasonFlux-32B, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ² Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº OpenAI o1-preview Ğ¸ DeepSeek V3. ReasonFlux-32B Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ² Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ². ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MATH Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 91.2%, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ o1-preview Ğ½Ğ° 6.7%."
                },
                "en": {
                    "title": "Revolutionizing Math Reasoning with Hierarchical Thought Templates",
                    "desc": "This paper introduces ReasonFlux-32B, a model that enhances mathematical reasoning in large language models (LLMs) by using hierarchical reasoning with thought templates. It features a library of 500 structured thought templates that help generalize reasoning across similar problems. The model employs hierarchical reinforcement learning to optimize the sequence of thought templates, allowing it to tackle complex problems more effectively. With these innovations, ReasonFlux-32B achieves state-of-the-art performance on math benchmarks, significantly outperforming existing models like OpenAI o1-preview and DeepSeek V3."
                },
                "zh": {
                    "title": "å±‚æ¬¡åŒ–æ¨ç†ï¼Œæ•°å­¦èƒ½åŠ›æ–°çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºé€šè¿‡æ‰©å±•æ€ç»´æ¨¡æ¿çš„å±‚æ¬¡åŒ–å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ï¼Œå¯ä»¥æœ‰æ•ˆä¼˜åŒ–æ¨ç†æœç´¢ç©ºé—´ï¼Œå¹¶è¶…è¶Šå¼ºå¤§çš„LLMå¦‚OpenAI o1-previewå’ŒDeepSeek V3çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬è®­ç»ƒçš„ReasonFlux-32Bæ¨¡å‹ä»…ä½¿ç”¨8ä¸ªGPUï¼Œå¹¶å¼•å…¥äº†ä¸‰é¡¹åˆ›æ–°ï¼šä¸€æ˜¯æ„å»ºäº†ä¸€ä¸ªåŒ…å«çº¦500ä¸ªé«˜å±‚æ¬¡æ€ç»´æ¨¡æ¿çš„ç»“æ„åŒ–é€šç”¨æ¨¡æ¿åº“ï¼Œèƒ½å¤Ÿæ¨å¹¿åˆ°ç±»ä¼¼çš„æ¨ç†é—®é¢˜ï¼›äºŒæ˜¯å¯¹æ€ç»´æ¨¡æ¿åºåˆ—è¿›è¡Œå±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ ï¼Œè€Œä¸æ˜¯é•¿é“¾çš„æ€ç»´ï¼ˆCoTsï¼‰ï¼Œä¼˜åŒ–åŸºç¡€LLMä»¥è§„åˆ’å‡ºå¤„ç†å¤æ‚é—®é¢˜çš„æœ€ä½³æ¨¡æ¿è½¨è¿¹ï¼›ä¸‰æ˜¯å…¨æ–°çš„æ¨ç†æ‰©å±•ç³»ç»Ÿï¼Œé€šè¿‡åœ¨æ¨ç†æ—¶è‡ªé€‚åº”æ‰©å±•æ€ç»´æ¨¡æ¿ï¼Œå®ç°å±‚æ¬¡åŒ–LLMæ¨ç†ã€‚é€šè¿‡åŒ…å«é¡ºåºæ€ç»´æ¨¡æ¿çš„æ¨¡æ¿è½¨è¿¹ï¼ŒReasonFlux-32Båœ¨æ•°å­¦æ¨ç†èƒ½åŠ›ä¸Šæ˜¾è‘—æå‡ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06788",
            "title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models",
            "url": "https://huggingface.co/papers/2502.06788",
            "abstract": "Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient deployment. We systematically clarify the performance gap between VLMs using pre-trained vision encoders, discrete tokenizers, and minimalist visual layers from scratch, deeply excavating the under-examined characteristics of encoder-free VLMs. We develop efficient strategies for encoder-free VLMs that rival mainstream encoder-based ones. After an in-depth investigation, we launch EVEv2.0, a new and improved family of encoder-free VLMs. We show that: (i) Properly decomposing and hierarchically associating vision and language within a unified model reduces interference between modalities. (ii) A well-designed training strategy enables effective optimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0 represents a thorough study for developing a decoder-only architecture across modalities, demonstrating superior data efficiency and strong vision-reasoning capability. Code is publicly available at: https://github.com/baaivision/EVE.",
            "score": 4,
            "issue_id": 2141,
            "pub_date": "2025-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "4374edb93ca102c6",
            "authors": [
                "Haiwen Diao",
                "Xiaotong Li",
                "Yufeng Cui",
                "Yueze Wang",
                "Haoge Deng",
                "Ting Pan",
                "Wenxuan Wang",
                "Huchuan Lu",
                "Xinlong Wang"
            ],
            "affiliations": [
                "BAAI",
                "BUPT",
                "CASIA",
                "DLUT",
                "PKU",
                "UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06788.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#architecture",
                    "#agi",
                    "#multimodal"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸: EVEv2.0 - ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ EVEv2.0, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ±ĞµĞ· Ğ½Ğ¸Ñ…, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ñ…. ĞĞ½Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ†Ğ¸Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. EVEv2.0 Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "EVEv2.0: Bridging the Gap in Vision-Language Models Without Encoders",
                    "desc": "This paper discusses the advancements in encoder-free vision-language models (VLMs) that are closing the performance gap with traditional encoder-based models. The authors explore the characteristics of these encoder-free VLMs and propose efficient strategies to enhance their performance. They introduce EVEv2.0, a new family of encoder-free VLMs that effectively integrates vision and language while minimizing interference. The study demonstrates that a well-structured training approach and hierarchical association of modalities lead to improved data efficiency and vision-reasoning capabilities."
                },
                "zh": {
                    "title": "æ— ç¼–ç å™¨VLMçš„æ½œåŠ›ä¸åˆ›æ–°",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†æ— ç¼–ç å™¨çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æ€§èƒ½ä¸Šä¸åŸºäºç¼–ç å™¨çš„æ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬ç³»ç»Ÿæ€§åœ°åˆ†æäº†ä½¿ç”¨é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨å’Œç®€çº¦è§†è§‰å±‚çš„æ— ç¼–ç å™¨VLMsçš„ç‰¹æ€§ã€‚é€šè¿‡å¼€å‘é«˜æ•ˆçš„ç­–ç•¥ï¼Œæˆ‘ä»¬æ¨å‡ºäº†EVEv2.0ï¼Œä¸€ä¸ªæ”¹è¿›çš„æ— ç¼–ç å™¨VLMç³»åˆ—ï¼Œå±•ç¤ºäº†å…¶åœ¨æ•°æ®æ•ˆç‡å’Œè§†è§‰æ¨ç†èƒ½åŠ›ä¸Šçš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œåˆç†åˆ†è§£å’Œå±‚æ¬¡å…³è”è§†è§‰ä¸è¯­è¨€å¯ä»¥å‡å°‘æ¨¡æ€ä¹‹é—´çš„å¹²æ‰°ï¼Œå¹¶é€šè¿‡è‰¯å¥½çš„è®­ç»ƒç­–ç•¥å®ç°æœ‰æ•ˆä¼˜åŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06635",
            "title": "Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM",
            "url": "https://huggingface.co/papers/2502.06635",
            "abstract": "Steel-LLM is a Chinese-centric language model developed from scratch with the goal of creating a high-quality, open-source model despite limited computational resources. Launched in March 2024, the project aimed to train a 1-billion-parameter model on a large-scale dataset, prioritizing transparency and the sharing of practical insights to assist others in the community. The training process primarily focused on Chinese data, with a small proportion of English data included, addressing gaps in existing open-source LLMs by providing a more detailed and practical account of the model-building journey. Steel-LLM has demonstrated competitive performance on benchmarks such as CEVAL and CMMLU, outperforming early models from larger institutions. This paper provides a comprehensive summary of the project's key contributions, including data collection, model design, training methodologies, and the challenges encountered along the way, offering a valuable resource for researchers and practitioners looking to develop their own LLMs. The model checkpoints and training script are available at https://github.com/zhanshijinwat/Steel-LLM.",
            "score": 2,
            "issue_id": 2141,
            "pub_date": "2025-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "2deb5075264d7660",
            "authors": [
                "Qingshui Gu",
                "Shu Li",
                "Tianyu Zheng",
                "Zhaoxiang Zhang"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "Institute of Automation, Chinese Academy of Sciences",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06635.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#multilingual",
                    "#training",
                    "#data",
                    "#benchmark",
                    "#open_source",
                    "#dataset",
                    "#low_resource"
                ],
                "emoji": "ğŸ‡¨ğŸ‡³",
                "ru": {
                    "title": "Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ LLM Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼",
                    "desc": "Steel-LLM - ÑÑ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ñ Ğ½ÑƒĞ»Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Steel-LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… CEVAL Ğ¸ CMMLU, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ Ñ€Ğ°Ğ½Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ‚Ğ¾Ğ². Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚ Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering Chinese NLP with Steel-LLM: Open-Source Innovation",
                    "desc": "Steel-LLM is a language model specifically designed for the Chinese language, built from the ground up to be open-source and accessible. It features 1 billion parameters and was trained on a large dataset primarily consisting of Chinese text, with some English data to fill existing gaps. The model has shown strong performance on various benchmarks, surpassing earlier models from larger organizations. This paper details the project's contributions, including data collection, model architecture, training techniques, and the challenges faced, serving as a guide for others in the field of language model development."
                },
                "zh": {
                    "title": "æ‰“é€ ä¸­æ–‡ä¼˜è´¨å¼€æºè¯­è¨€æ¨¡å‹çš„æ¢ç´¢",
                    "desc": "Steel-LLMæ˜¯ä¸€ä¸ªä»¥ä¸­æ–‡ä¸ºä¸­å¿ƒçš„è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨åœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹å¼€å‘å‡ºé«˜è´¨é‡çš„å¼€æºæ¨¡å‹ã€‚è¯¥é¡¹ç›®äº2024å¹´3æœˆå¯åŠ¨ï¼Œè®­ç»ƒäº†ä¸€ä¸ªæ‹¥æœ‰10äº¿å‚æ•°çš„å¤§è§„æ¨¡æ¨¡å‹ï¼Œé‡ç‚¹å…³æ³¨é€æ˜åº¦å’Œå®ç”¨è§è§£çš„åˆ†äº«ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ä¸»è¦ä½¿ç”¨ä¸­æ–‡æ•°æ®ï¼Œå¹¶é€‚é‡åŒ…å«è‹±æ–‡æ•°æ®ï¼Œå¡«è¡¥äº†ç°æœ‰å¼€æºå¤§è¯­è¨€æ¨¡å‹çš„ç©ºç™½ã€‚Steel-LLMåœ¨CEVALå’ŒCMMLUç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†å¤§å‹æœºæ„çš„æ—©æœŸæ¨¡å‹ï¼Œä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›äº†å®è´µçš„èµ„æºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06049",
            "title": "LM2: Large Memory Models",
            "url": "https://huggingface.co/papers/2502.06049",
            "abstract": "This paper introduces the Large Memory Model (LM2), a decoder-only Transformer architecture enhanced with an auxiliary memory module that aims to address the limitations of standard Transformers in multi-step reasoning, relational argumentation, and synthesizing information distributed over long contexts. The proposed LM2 incorporates a memory module that acts as a contextual representation repository, interacting with input tokens via cross attention and updating through gating mechanisms. To preserve the Transformers general-purpose capabilities, LM2 maintains the original information flow while integrating a complementary memory pathway. Experimental results on the BABILong benchmark demonstrate that the LM2model outperforms both the memory-augmented RMT model by 37.1% and the baseline Llama-3.2 model by 86.3% on average across tasks. LM2 exhibits exceptional capabilities in multi-hop inference, numerical reasoning, and large-context question-answering. On the MMLU dataset, it achieves a 5.0% improvement over a pre-trained vanilla model, demonstrating that its memory module does not degrade performance on general tasks. Further, in our analysis, we explore the memory interpretability, effectiveness of memory modules, and test-time behavior. Our findings emphasize the importance of explicit memory in enhancing Transformer architectures.",
            "score": 2,
            "issue_id": 2140,
            "pub_date": "2025-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "5f62d7e814a6918f",
            "authors": [
                "Jikun Kang",
                "Wenqi Wu",
                "Filippos Christianos",
                "Alex J. Chan",
                "Fraser Greenlee",
                "George Thomas",
                "Marvin Purtorab",
                "Andy Toulis"
            ],
            "affiliations": [
                "Convergence Labs Ltd."
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06049.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#architecture",
                    "#benchmark",
                    "#interpretability",
                    "#long_context",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "LM2: Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Large Memory Model (LM2), Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€-Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. LM2 Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ BABILong Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. LM2 Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ñ…, Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Enhancing Transformers with Memory for Superior Reasoning",
                    "desc": "The paper presents the Large Memory Model (LM2), a new type of Transformer designed to improve multi-step reasoning and information synthesis over long contexts. LM2 features an auxiliary memory module that stores contextual information and interacts with input data through cross attention, allowing it to update its memory dynamically. This model retains the original capabilities of Transformers while adding a memory pathway that enhances performance on complex tasks. Experimental results show that LM2 significantly outperforms existing models in reasoning tasks and maintains strong performance on general tasks, highlighting the value of integrating explicit memory into Transformer architectures."
                },
                "zh": {
                    "title": "å¤§å‹è®°å¿†æ¨¡å‹ï¼šæå‡Transformeræ¨ç†èƒ½åŠ›çš„å…³é”®",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºå¤§å‹è®°å¿†æ¨¡å‹ï¼ˆLM2ï¼‰çš„è§£ç å™¨ä»…Transformeræ¶æ„ï¼Œæ—¨åœ¨è§£å†³æ ‡å‡†Transformeråœ¨å¤šæ­¥æ¨ç†ã€å…³ç³»è®ºè¯å’Œé•¿ä¸Šä¸‹æ–‡ä¿¡æ¯ç»¼åˆæ–¹é¢çš„å±€é™æ€§ã€‚LM2å¼•å…¥äº†ä¸€ä¸ªè¾…åŠ©è®°å¿†æ¨¡å—ï¼Œä½œä¸ºä¸Šä¸‹æ–‡è¡¨ç¤ºçš„å­˜å‚¨åº“ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›ä¸è¾“å…¥æ ‡è®°äº¤äº’ï¼Œå¹¶é€šè¿‡é—¨æ§æœºåˆ¶è¿›è¡Œæ›´æ–°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLM2åœ¨BABILongåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¹³å‡æ€§èƒ½æ¯”è®°å¿†å¢å¼ºçš„RMTæ¨¡å‹æé«˜äº†37.1%ï¼Œæ¯”åŸºçº¿Llama-3.2æ¨¡å‹æé«˜äº†86.3%ã€‚LM2åœ¨å¤šè·³æ¨ç†ã€æ•°å€¼æ¨ç†å’Œå¤§ä¸Šä¸‹æ–‡é—®ç­”æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†æ˜¾å¼è®°å¿†åœ¨å¢å¼ºTransformeræ¶æ„ä¸­çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06155",
            "title": "Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile",
            "url": "https://huggingface.co/papers/2502.06155",
            "abstract": "Despite the promise of synthesizing high-fidelity videos, Diffusion Transformers (DiTs) with 3D full attention suffer from expensive inference due to the complexity of attention computation and numerous sampling steps. For example, the popular Open-Sora-Plan model consumes more than 9 minutes for generating a single video of 29 frames. This paper addresses the inefficiency issue from two aspects: 1) Prune the 3D full attention based on the redundancy within video data; We identify a prevalent tile-style repetitive pattern in the 3D attention maps for video data, and advocate a new family of sparse 3D attention that holds a linear complexity w.r.t. the number of video frames. 2) Shorten the sampling process by adopting existing multi-step consistency distillation; We split the entire sampling trajectory into several segments and perform consistency distillation within each one to activate few-step generation capacities. We further devise a three-stage training pipeline to conjoin the low-complexity attention and few-step generation capacities. Notably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into an efficient one that is 7.4x -7.8x faster for 29 and 93 frames 720p video generation with a marginal performance trade-off in VBench. In addition, we demonstrate that our approach is amenable to distributed inference, achieving an additional 3.91x speedup when running on 4 GPUs with sequence parallelism.",
            "score": 2,
            "issue_id": 2140,
            "pub_date": "2025-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "2f8d5e54db328d39",
            "authors": [
                "Hangliang Ding",
                "Dacheng Li",
                "Runlong Su",
                "Peiyuan Zhang",
                "Zhijie Deng",
                "Ion Stoica",
                "Hao Zhang"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Tsinghua University",
                "University of California, Berkeley",
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06155.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#diffusion",
                    "#optimization",
                    "#inference",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² (DiTs). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ 3D-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑˆĞ°Ğ³Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² 7.4-7.8 Ñ€Ğ°Ğ· Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ 720p Ñ 29 Ğ¸ 93 ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ 0.1% Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Speeding Up Video Generation with Efficient Attention Mechanisms",
                    "desc": "This paper presents a solution to the inefficiency of Diffusion Transformers (DiTs) in generating high-fidelity videos. It introduces a method to prune 3D full attention by recognizing repetitive patterns in video data, leading to a sparse attention mechanism that reduces computational complexity. Additionally, the authors propose a multi-step consistency distillation approach to shorten the sampling process, allowing for faster video generation. The resulting model, Open-Sora-Plan-1.2, achieves significant speed improvements while maintaining performance, especially when utilizing distributed inference across multiple GPUs."
                },
                "zh": {
                    "title": "é«˜æ•ˆè§†é¢‘ç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›çš„Diffusion Transformersï¼ˆDiTsï¼‰æ¨¡å‹ï¼Œä»¥è§£å†³ç”Ÿæˆé«˜ä¿çœŸè§†é¢‘æ—¶çš„æ•ˆç‡é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡è¯†åˆ«è§†é¢‘æ•°æ®ä¸­çš„å†—ä½™ï¼Œæå‡ºäº†ä¸€ç§ç¨€ç–çš„3Dæ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿å…¶åœ¨è§†é¢‘å¸§æ•°é‡ä¸Šå…·æœ‰çº¿æ€§å¤æ‚åº¦ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤šæ­¥ä¸€è‡´æ€§è’¸é¦æŠ€æœ¯ï¼Œç¼©çŸ­äº†é‡‡æ ·è¿‡ç¨‹ï¼Œä»è€Œå®ç°äº†æ›´å¿«é€Ÿçš„è§†é¢‘ç”Ÿæˆã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä½¿ç”¨æå°‘çš„é¢„è®­ç»ƒæ•°æ®æ—¶ï¼Œç”Ÿæˆé€Ÿåº¦æé«˜äº†7.4åˆ°7.8å€ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06023",
            "title": "Dual Caption Preference Optimization for Diffusion Models",
            "url": "https://huggingface.co/papers/2502.06023",
            "abstract": "Recent advancements in human preference optimization, originally developed for Large Language Models (LLMs), have shown significant potential in improving text-to-image diffusion models. These methods aim to learn the distribution of preferred samples while distinguishing them from less preferred ones. However, existing preference datasets often exhibit overlap between these distributions, leading to a conflict distribution. Additionally, we identified that input prompts contain irrelevant information for less preferred images, limiting the denoising network's ability to accurately predict noise in preference optimization methods, known as the irrelevant prompt issue. To address these challenges, we propose Dual Caption Preference Optimization (DCPO), a novel approach that utilizes two distinct captions to mitigate irrelevant prompts. To tackle conflict distribution, we introduce the Pick-Double Caption dataset, a modified version of Pick-a-Pic v2 with separate captions for preferred and less preferred images. We further propose three different strategies for generating distinct captions: captioning, perturbation, and hybrid methods. Our experiments show that DCPO significantly improves image quality and relevance to prompts, outperforming Stable Diffusion (SD) 2.1, SFT_Chosen, Diffusion-DPO, and MaPO across multiple metrics, including Pickscore, HPSv2.1, GenEval, CLIPscore, and ImageReward, fine-tuned on SD 2.1 as the backbone.",
            "score": 1,
            "issue_id": 2141,
            "pub_date": "2025-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "07782353b3b8b697",
            "authors": [
                "Amir Saeidi",
                "Yiran Luo",
                "Agneet Chatterjee",
                "Shamanthak Hegde",
                "Bimsara Pathiraja",
                "Yezhou Yang",
                "Chitta Baral"
            ],
            "affiliations": [
                "Arizona State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06023.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#rlhf",
                    "#dataset",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ”Ğ²Ğ¾Ğ¹Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Dual Caption Preference Optimization (DCPO) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. DCPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Pick-Double Caption Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ¼ĞµĞ½ĞµĞµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DCPO Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ñ… ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Image Generation with Dual Captions!",
                    "desc": "This paper presents Dual Caption Preference Optimization (DCPO), a new method to enhance text-to-image diffusion models by addressing issues in human preference optimization. It identifies problems with existing preference datasets, such as overlapping distributions and irrelevant prompts that hinder the denoising process. To overcome these challenges, DCPO employs two distinct captions for preferred and less preferred images, utilizing a modified dataset called Pick-Double Caption. The results demonstrate that DCPO significantly improves image quality and relevance, outperforming several existing models across various evaluation metrics."
                },
                "zh": {
                    "title": "åŒé‡æ ‡é¢˜ä¼˜åŒ–ï¼Œæå‡å›¾åƒè´¨é‡ï¼",
                    "desc": "æœ€è¿‘åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­å‘å±•çš„äººç±»åå¥½ä¼˜åŒ–æŠ€æœ¯ï¼Œæ˜¾ç¤ºå‡ºåœ¨æ”¹è¿›æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚è¿™äº›æ–¹æ³•æ—¨åœ¨å­¦ä¹ åå¥½æ ·æœ¬çš„åˆ†å¸ƒï¼Œå¹¶å°†å…¶ä¸ä¸å¤ªåå¥½çš„æ ·æœ¬åŒºåˆ†å¼€æ¥ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åå¥½æ•°æ®é›†é€šå¸¸å­˜åœ¨åˆ†å¸ƒé‡å çš„é—®é¢˜ï¼Œå¯¼è‡´å†²çªåˆ†å¸ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°è¾“å…¥æç¤ºä¸­åŒ…å«ä¸ä¸å¤ªåå¥½çš„å›¾åƒæ— å…³çš„ä¿¡æ¯ï¼Œè¿™é™åˆ¶äº†å»å™ªç½‘ç»œåœ¨åå¥½ä¼˜åŒ–æ–¹æ³•ä¸­çš„å‡†ç¡®é¢„æµ‹èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŒé‡æ ‡é¢˜åå¥½ä¼˜åŒ–ï¼ˆDCPOï¼‰ï¼Œåˆ©ç”¨ä¸¤ä¸ªä¸åŒçš„æ ‡é¢˜æ¥å‡è½»æ— å…³æç¤ºçš„é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05431",
            "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding",
            "url": "https://huggingface.co/papers/2502.05431",
            "abstract": "Context-augmented generation (CAG) techniques, including RAG and ICL, require the efficient combination of multiple contexts to generate responses to user queries. Directly inputting these contexts as a sequence introduces a considerable computational burden by re-encoding the combined selection of contexts for every request. To address this, we explore the promising potential of parallel encoding to independently pre-compute and cache each context's KV states. This approach enables the direct loading of cached states during inference while accommodating more contexts through position reuse across contexts. However, due to misalignments in attention distribution, directly applying parallel encoding results in a significant performance drop. To enable effective and efficient CAG, we propose Adaptive Parallel Encoding (APE), which brings shared prefix, attention temperature, and scaling factor to align the distribution of parallel encoding with sequential encoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98% and 93% sequential encoding performance using the same inputs while outperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales to many-shot CAG, effectively encoding hundreds of contexts in parallel. Efficiency evaluation shows that APE can achieve an end-to-end 4.5times speedup by reducing 28times prefilling time for a 128K-length context.",
            "score": 1,
            "issue_id": 2141,
            "pub_date": "2025-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "7bc5b7aeb3716893",
            "authors": [
                "Xinyu Yang",
                "Tianqi Chen",
                "Beidi Chen"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Nvidia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05431.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#optimization",
                    "#inference",
                    "#long_context"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼: Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (APE) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. APE Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑÑ‚ÑŒ Ğ¸ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ KV-ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑ, Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñƒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ APE ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ´Ğ¾ 98% Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° 3.6-7.9% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… RAG Ğ¸ ICL."
                },
                "en": {
                    "title": "Boosting Efficiency in Context-Augmented Generation with APE",
                    "desc": "This paper introduces Adaptive Parallel Encoding (APE) as a solution to improve the efficiency of context-augmented generation (CAG) techniques like RAG and ICL. Traditional methods face high computational costs when combining multiple contexts for generating responses, as they require re-encoding for each request. APE allows for the pre-computation and caching of key-value (KV) states for each context, which can then be loaded during inference, significantly speeding up the process. The proposed method aligns the attention distribution of parallel encoding with that of sequential encoding, achieving high performance while handling many contexts efficiently."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”å¹¶è¡Œç¼–ç ï¼šæå‡ä¸Šä¸‹æ–‡ç”Ÿæˆæ•ˆç‡çš„å…³é”®",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ä¸Šä¸‹æ–‡å¢å¼ºç”Ÿæˆï¼ˆCAGï¼‰æŠ€æœ¯ä¸­çš„å¹¶è¡Œç¼–ç æ–¹æ³•ï¼Œä»¥æé«˜ç”Ÿæˆç”¨æˆ·æŸ¥è¯¢å“åº”çš„æ•ˆç‡ã€‚ä¼ ç»Ÿæ–¹æ³•åœ¨æ¯æ¬¡è¯·æ±‚æ—¶éƒ½éœ€è¦é‡æ–°ç¼–ç å¤šä¸ªä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´è®¡ç®—è´Ÿæ‹…è¿‡é‡ã€‚æˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”å¹¶è¡Œç¼–ç ï¼ˆAPEï¼‰ï¼Œé€šè¿‡å…±äº«å‰ç¼€ã€æ³¨æ„åŠ›æ¸©åº¦å’Œç¼©æ”¾å› å­æ¥è°ƒæ•´å¹¶è¡Œç¼–ç ä¸é¡ºåºç¼–ç çš„æ³¨æ„åŠ›åˆ†å¸ƒï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAPEåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæ˜¾è‘—åŠ å¿«å¤„ç†é€Ÿåº¦ï¼Œé€‚ç”¨äºå¤„ç†å¤§é‡ä¸Šä¸‹æ–‡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-02-10.html",
    "link_next": "2025-02-12.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "10.02",
        "en": "02/10",
        "zh": "2æœˆ10æ—¥"
    },
    "short_date_next": {
        "ru": "12.02",
        "en": "02/12",
        "zh": "2æœˆ12æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 5,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 2,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 8,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 7,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç§°ä¸ºæ»‘åŠ¨ç£è´´æ³¨æ„åŠ›ï¼ˆSTAï¼‰ï¼Œä»¥è§£å†³æ‰©æ•£å˜å‹å™¨ï¼ˆDiTsï¼‰åœ¨è§†é¢‘ç”Ÿæˆä¸­çš„è®¡ç®—æˆæœ¬é—®é¢˜ã€‚ä¼ ç»Ÿçš„å…¨æ³¨æ„åŠ›æœºåˆ¶åœ¨ç”ŸæˆçŸ­è§†é¢‘æ—¶è€—æ—¶æé•¿ã€‚STAé€šè¿‡åœ¨å±€éƒ¨3Dçª—å£å†…æ»‘åŠ¨å’Œå…³æ³¨ï¼Œæ¶ˆé™¤äº†å…¨æ³¨æ„åŠ›ä¸­çš„å†—ä½™ã€‚ä¸ä¼ ç»Ÿçš„æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ä¸åŒï¼ŒSTAé‡‡ç”¨ç¡¬ä»¶æ„ŸçŸ¥çš„è®¾è®¡ï¼Œæé«˜äº†æ•ˆç‡ã€‚å®éªŒæ˜¾ç¤ºï¼ŒSTAåœ¨ä¸é™ä½è´¨é‡çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—å‡å°‘äº†è§†é¢‘ç”Ÿæˆçš„å»¶è¿Ÿã€‚",
        "title": "Fast Video Generation with Sliding Tile Attention",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç§°ä¸ºæ»‘åŠ¨ç£è´´æ³¨æ„åŠ›ï¼ˆSTAï¼‰ï¼Œä»¥è§£å†³æ‰©æ•£å˜å‹å™¨ï¼ˆDiTsï¼‰åœ¨è§†é¢‘ç”Ÿæˆä¸­çš„è®¡ç®—æˆæœ¬é—®é¢˜ã€‚ä¼ ç»Ÿçš„å…¨æ³¨æ„åŠ›æœºåˆ¶åœ¨ç”ŸæˆçŸ­è§†é¢‘æ—¶è€—æ—¶æé•¿ã€‚STAé€šè¿‡åœ¨å±€éƒ¨3Dçª—å£å†…æ»‘åŠ¨å’Œå…³æ³¨ï¼Œæ¶ˆé™¤äº†å…¨æ³¨æ„åŠ›ä¸­çš„å†—ä½™ã€‚ä¸ä¼ ç»Ÿçš„æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ä¸åŒï¼ŒSTAé‡‡ç”¨ç¡¬ä»¶æ„ŸçŸ¥çš„è®¾è®¡ï¼Œæé«˜äº†æ•ˆç‡ã€‚å®éªŒæ˜¾ç¤ºï¼ŒSTAåœ¨ä¸é™ä½è´¨é‡çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—å‡å°‘äº†è§†é¢‘ç”Ÿæˆçš„å»¶è¿Ÿã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« zhÇ’ng xÄ«n de zhÃ¹ yÃ¬ lÃ¬ jÄ« zhÃ¬, chÄ“ng wÃ©i huÃ¡ dÃ²ng cÃ­ tiÄ“ zhÃ¹ yÃ¬ lÃ¬ (STA), yÇ jiÄ› juÃ© kuÃ² sÃ n biÃ n shÅ« qÃ¬ (DiTs) zÃ i shÃ¬ pÃ­n shÄ“ng chÃ©ng zhÅng de jÃ¬ suÃ n chÃ©ng bÄ›n wÃ¨n tÃ­. chuÃ¡n tÇ’ng de quÃ¡n zhÃ¹ yÃ¬ lÃ¬ jÄ« zhÃ¬ zÃ i shÄ“ng chÃ©ng duÇn shÃ¬ pÃ­n shÃ­ hÃ o shÃ­ jÃ­ chÃ¡ng. STA tÅng guÃ² zÃ i jÃº bÃ¹ 3D chuÄng kÇ’u nÃ¨i huÃ¡ dÃ²ng hÃ© guÄn zhÃ¹, xiÄo chÃº le quÃ¡n zhÃ¹ yÃ¬ lÃ¬ zhÅng de rÃ³ng yÃ¹. yÇ” chuÃ¡n tÇ’ng de huÃ¡ dÃ²ng chuÄng kÇ’u zhÃ¹ yÃ¬ lÃ¬ bÃ¹ tÃ³ng, STA cÇi yÃ²ng yÃ¬ng jiÃ n gÇn zhÄ« de shÃ¨ jÃ¬, tÃ­ gÄo le xiÃ o lÇœ. shÃ­ yÃ n xiÇn shÃ¬, STA zÃ i bÃ¹ jiÃ ng dÄ« zhÃ¬ liÃ ng de qÃ­ng kuÃ ng xiÃ , xiÇn zhÃ¹ jiÇn shÇo le shÃ¬ pÃ­n shÄ“ng chÃ©ng de yÃ¡n chÃ­.",
        "vocab": "[{'word': 'æ³¨æ„åŠ›', 'pinyin': 'zhÃ¹yÃ¬lÃ¬', 'trans': 'attention'},\n{'word': 'æœºåˆ¶', 'pinyin': 'jÄ«zhÃ¬', 'trans': 'mechanism'},\n{'word': 'æ»‘åŠ¨', 'pinyin': 'huÃ¡dÃ²ng', 'trans': 'sliding'},\n{'word': 'ç£è´´', 'pinyin': 'cÃ­tiÄ“', 'trans': 'magnetic tile'},\n{'word': 'æ‰©æ•£', 'pinyin': 'kuÃ²sÃ n', 'trans': 'diffusion'},\n{'word': 'å˜å‹å™¨', 'pinyin': 'biÃ nyÄqÃ¬', 'trans': 'transformer'},\n{'word': 'è®¡ç®—', 'pinyin': 'jÃ¬suÃ n', 'trans': 'computation'},\n{'word': 'æˆæœ¬', 'pinyin': 'chÃ©ngbÄ›n', 'trans': 'cost'},\n{'word': 'ä¼ ç»Ÿ', 'pinyin': 'chuÃ¡ntÇ’ng', 'trans': 'traditional'},\n{'word': 'å…¨', 'pinyin': 'quÃ¡n', 'trans': 'full'},\n{'word': 'è€—æ—¶', 'pinyin': 'hÃ oshÃ­', 'trans': 'time-consuming'},\n{'word': 'æé•¿', 'pinyin': 'jÃ­chÃ¡ng', 'trans': 'extremely long'},\n{'word': 'å±€éƒ¨', 'pinyin': 'jÃºbÃ¹', 'trans': 'local'},\n{'word': '3D', 'pinyin': '', 'trans': '3D'},\n{'word': 'çª—å£', 'pinyin': 'chuÄngkÇ’u', 'trans': 'window'},\n{'word': 'å†—ä½™', 'pinyin': 'rÇ’ngyÃº', 'trans': 'redundancy'},\n{'word': 'ç¡¬ä»¶', 'pinyin': 'yÃ¬ngjiÃ n', 'trans': 'hardware'},\n{'word': 'æ„ŸçŸ¥', 'pinyin': 'gÇnzhÄ«', 'trans': 'perception'},\n{'word': 'è®¾è®¡', 'pinyin': 'shÃ¨jÃ¬', 'trans': 'design'},\n{'word': 'æ•ˆç‡', 'pinyin': 'xiÃ olÇœ', 'trans': 'efficiency'},\n{'word': 'å®éªŒ', 'pinyin': 'shÃ­yÃ n', 'trans': 'experiment'},\n{'word': 'æ˜¾ç¤º', 'pinyin': 'xiÇnshÃ¬', 'trans': 'display'},\n{'word': 'æ˜¾è‘—', 'pinyin': 'xiÇnzhÃ¹', 'trans': 'significant'},\n{'word': 'å‡å°‘', 'pinyin': 'jiÇnshÇo', 'trans': 'reduce'},\n{'word': 'å»¶è¿Ÿ', 'pinyin': 'yÃ¡nchÃ­', 'trans': 'latency'}]",
        "trans": "This article introduces a new attention mechanism called Sliding Tile Attention (STA) to address the computational cost issues of Diffusion Transformers (DiTs) in video generation. Traditional full attention mechanisms take an extremely long time to generate short videos. STA eliminates redundancy in full attention by sliding and focusing within local 3D windows. Unlike traditional sliding window attention, STA employs a hardware-aware design to enhance efficiency. Experiments show that STA significantly reduces the latency of video generation without compromising quality.",
        "update_ts": "2025-02-10 09:11"
    }
}