{
    "date": {
        "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 3",
        "zh": "2æœˆ3æ—¥"
    },
    "time_utc": "2025-02-03 21:09",
    "weekday": 0,
    "issue_id": 2012,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.19393",
            "title": "s1: Simple test-time scaling",
            "url": "https://huggingface.co/papers/2501.19393",
            "abstract": "Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending \"Wait\" multiple times to the model's generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1 exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1 with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at https://github.com/simplescaling/s1.",
            "score": 48,
            "issue_id": 1994,
            "pub_date": "2025-01-31",
            "pub_date_card": {
                "ru": "31 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 31",
                "zh": "1æœˆ31æ—¥"
            },
            "hash": "8fcf84a9effc288f",
            "authors": [
                "Niklas Muennighoff",
                "Zitong Yang",
                "Weijia Shi",
                "Xiang Lisa Li",
                "Li Fei-Fei",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer",
                "Percy Liang",
                "Emmanuel CandÃ¨s",
                "Tatsunori Hashimoto"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "Contextual AI",
                "Stanford University",
                "University of Washington, Seattle"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.19393.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#reasoning",
                    "#training",
                    "#math",
                    "#optimization",
                    "#data"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ s1, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Qwen2.5-32B-Instruct, Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¾Ñ€ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… s1K Ğ¸Ğ· 1000 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ s1 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ o1-preview Ğ¾Ñ‚ OpenAI Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Enhancing Language Models with Test-Time Scaling",
                    "desc": "This paper introduces a method called test-time scaling for enhancing language model performance during evaluation. The authors create a dataset of 1,000 questions with reasoning traces to train their model, focusing on difficulty, diversity, and quality. They implement a technique called budget forcing, which manipulates the model's response time to encourage deeper reasoning and correct errors. After fine-tuning their model, they demonstrate significant improvements in solving math competition questions compared to previous models, showcasing the effectiveness of their approach."
                },
                "zh": {
                    "title": "æµ‹è¯•æ—¶é—´æ‰©å±•ï¼šæå‡è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è¯­è¨€å»ºæ¨¡æ–¹æ³•â€”â€”æµ‹è¯•æ—¶é—´æ‰©å±•ï¼Œæ—¨åœ¨é€šè¿‡å¢åŠ æµ‹è¯•æ—¶é—´è®¡ç®—æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚ç ”ç©¶è€…ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«1000ä¸ªé—®é¢˜åŠå…¶æ¨ç†è¿‡ç¨‹çš„å°æ•°æ®é›†s1Kï¼Œå¹¶é€šè¿‡éš¾åº¦ã€å¤šæ ·æ€§å’Œè´¨é‡ä¸‰ä¸ªæ ‡å‡†è¿›è¡ŒéªŒè¯ã€‚ä¸ºäº†æ§åˆ¶æµ‹è¯•æ—¶é—´è®¡ç®—ï¼Œæå‡ºäº†é¢„ç®—å¼ºåˆ¶çš„æ–¹æ³•ï¼Œé€šè¿‡å¼ºåˆ¶ç»ˆæ­¢æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹æˆ–åœ¨æ¨¡å‹ç”Ÿæˆæ—¶å¤šæ¬¡æ·»åŠ â€œç­‰å¾…â€æ¥å»¶é•¿æ€è€ƒæ—¶é—´ï¼Œä»è€Œä¿ƒä½¿æ¨¡å‹æ£€æŸ¥ç­”æ¡ˆã€‚ç»è¿‡ç›‘ç£å¾®è°ƒåï¼Œæ¨¡å‹s1åœ¨æ•°å­¦ç«èµ›é—®é¢˜ä¸Šè¶…è¶Šäº†OpenAIçš„o1æ¨¡å‹ï¼Œè¡¨ç°æå‡è¾¾27%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.19324",
            "title": "Reward-Guided Speculative Decoding for Efficient LLM Reasoning",
            "url": "https://huggingface.co/papers/2501.19324",
            "abstract": "",
            "score": 25,
            "issue_id": 1995,
            "pub_date": "2025-01-31",
            "pub_date_card": {
                "ru": "31 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 31",
                "zh": "1æœˆ31æ—¥"
            },
            "hash": "ce2d414eedfb7a1e",
            "authors": [
                "Baohao Liao",
                "Yuhui Xu",
                "Hanze Dong",
                "Junnan Li",
                "Christof Monz",
                "Silvio Savarese",
                "Doyen Sahoo",
                "Caiming Xiong"
            ],
            "affiliations": [
                "Language Technology Lab, University of Amsterdam",
                "Salesforce AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.19324.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ AI Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "Hybrid Networks: Bridging Spatial and Temporal Learning",
                    "desc": "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æ•°æ®å¤„ç†ï¼Œæå‡æœºå™¨å­¦ä¹ æ€§èƒ½",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä¸€ç§æ–°çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡ä¼˜åŒ–æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾é€‰æ‹©æ¥å¢å¼ºå­¦ä¹ æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚æœ€ç»ˆï¼Œç ”ç©¶è¡¨æ˜ï¼Œæ”¹è¿›çš„æ•°æ®å¤„ç†æµç¨‹å¯¹æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.18119",
            "title": "Self-supervised Quantized Representation for Seamlessly Integrating Knowledge Graphs with Large Language Models",
            "url": "https://huggingface.co/papers/2501.18119",
            "abstract": "Due to the presence of the natural gap between Knowledge Graph (KG) structures and the natural language, the effective integration of holistic structural information of KGs with Large Language Models (LLMs) has emerged as a significant question. To this end, we propose a two-stage framework to learn and apply quantized codes for each entity, aiming for the seamless integration of KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR) method is proposed to compress both KG structural and semantic knowledge into discrete codes (\\ie, tokens) that align the format of language sentences. We further design KG instruction-following data by viewing these learned codes as features to directly input to LLMs, thereby achieving seamless integration. The experiment results demonstrate that SSQR outperforms existing unsupervised quantized methods, producing more distinguishable codes. Further, the fine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link prediction and triple classification tasks, utilizing only 16 tokens per entity instead of thousands in conventional prompting methods.",
            "score": 12,
            "issue_id": 2002,
            "pub_date": "2025-01-30",
            "pub_date_card": {
                "ru": "30 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 30",
                "zh": "1æœˆ30æ—¥"
            },
            "hash": "d751c8a690173842",
            "authors": [
                "Qika Lin",
                "Tianzhe Zhao",
                "Kai He",
                "Zhen Peng",
                "Fangzhi Xu",
                "Ling Huang",
                "Jingying Ma",
                "Mengling Feng"
            ],
            "affiliations": [
                "National University of Singapore",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.18119.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#graphs",
                    "#transfer_learning",
                    "#training",
                    "#multimodal",
                    "#data"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ´Ñ‹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ (SSQR) Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ² Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ´Ñ‹. Ğ­Ñ‚Ğ¸ ĞºĞ¾Ğ´Ñ‹ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ SSQR Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ LLaMA2 Ğ¸ LLaMA3.1 Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ³Ñ€Ğ°Ñ„Ğ°Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Seamless Integration of Knowledge Graphs and Language Models",
                    "desc": "This paper addresses the challenge of integrating Knowledge Graphs (KGs) with Large Language Models (LLMs) by proposing a two-stage framework. The framework utilizes a self-supervised quantized representation (SSQR) method to convert KG structural and semantic information into discrete codes that resemble language tokens. By treating these codes as features for LLMs, the approach allows for a more efficient and effective integration of KGs with LLMs. Experimental results show that SSQR outperforms traditional methods, enabling better performance in tasks like KG link prediction and triple classification with significantly fewer tokens."
                },
                "zh": {
                    "title": "æ— ç¼æ•´åˆçŸ¥è¯†å›¾è°±ä¸å¤§å‹è¯­è¨€æ¨¡å‹",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ç»“æ„ä¸è‡ªç„¶è¯­è¨€ä¹‹é—´çš„å·®è·ï¼Œæå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œä»¥å®ç°KGä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ‰æ•ˆæ•´åˆã€‚é¦–å…ˆï¼Œæå‡ºäº†ä¸€ç§è‡ªç›‘ç£é‡åŒ–è¡¨ç¤ºï¼ˆSSQRï¼‰æ–¹æ³•ï¼Œå°†KGçš„ç»“æ„å’Œè¯­ä¹‰çŸ¥è¯†å‹ç¼©ä¸ºç¦»æ•£ä»£ç ï¼ˆå³ä»¤ç‰Œï¼‰ï¼Œä½¿å…¶ä¸è¯­è¨€å¥å­çš„æ ¼å¼å¯¹é½ã€‚æ¥ç€ï¼Œè®¾è®¡äº†KGæŒ‡ä»¤è·Ÿéšæ•°æ®ï¼Œå°†è¿™äº›å­¦ä¹ åˆ°çš„ä»£ç è§†ä¸ºç‰¹å¾ï¼Œç›´æ¥è¾“å…¥åˆ°LLMä¸­ï¼Œä»è€Œå®ç°æ— ç¼æ•´åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSSQRåœ¨æ— ç›‘ç£é‡åŒ–æ–¹æ³•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œç”Ÿæˆçš„ä»£ç æ›´å…·å¯åŒºåˆ†æ€§ï¼Œä¸”ç»è¿‡å¾®è°ƒçš„LLaMA2å’ŒLLaMA3.1åœ¨KGé“¾æ¥é¢„æµ‹å’Œä¸‰å…ƒç»„åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä»…ä½¿ç”¨æ¯ä¸ªå®ä½“16ä¸ªä»¤ç‰Œï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿæ–¹æ³•ä¸­çš„æ•°åƒä¸ªã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.19339",
            "title": "PixelWorld: Towards Perceiving Everything as Pixels",
            "url": "https://huggingface.co/papers/2501.19339",
            "abstract": "Existing foundation models typically process visual input as pixels and textual input as tokens, a paradigm that contrasts with human perception, where both modalities are processed in a unified manner. With the rise of embodied and agentic AI, where inputs primarily come from camera pixels, the need for a unified perception framework becomes increasingly evident. In this paper, we propose to unify all modalities (text, tables, code, diagrams, images, etc) as pixel inputs, i.e. \"Perceive Everything as Pixels\" (PEAP). We introduce PixelWorld, a novel evaluation suite that unifies all the mentioned modalities into pixel space to gauge the existing models' performance. Our findings show that (1) PEAP outperforms baseline with token-based input in multimodal datasets, benefiting from unified input for better disambiguation, (2) significant declines in reasoning and coding capabilities across all models when processing pixel-based input, underscoring the need to enhance foundation models' perceptual abilities, (3) larger models can maintain strong performance on non-reasoning tasks under PEAP, while smaller models like Phi-3.5-V suffer significant performance degradation, (4) the attention pattern of PEAP is highly aligned with text token input, (5) PEAP can be accelerated significantly by exploiting the spatial sparsity. We conclude that the existing frontier models are competent in pixel perception, however, there is still headroom for improvement. Our code, dataset will be released upon acceptance.",
            "score": 8,
            "issue_id": 2007,
            "pub_date": "2025-01-31",
            "pub_date_card": {
                "ru": "31 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 31",
                "zh": "1æœˆ31æ—¥"
            },
            "hash": "3e10b792328f7a4b",
            "authors": [
                "Zhiheng Lyu",
                "Xueguang Ma",
                "Wenhu Chen"
            ],
            "affiliations": [
                "Department of Computer Science, University of Waterloo",
                "Vector Institute, Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.19339.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#benchmark",
                    "#optimization",
                    "#dataset",
                    "#open_source",
                    "#reasoning",
                    "#multimodal",
                    "#interpretability",
                    "#cv"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¼Ğ¸Ñ€: Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ´Ğ»Ñ Ğ˜Ğ˜",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ´ Ğ¸ Ñ‚.Ğ´.) Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ PEAP (Perceive Everything as Pixels). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… PixelWorld Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑÑ‚Ğ¾Ğ¼ unified Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ PEAP Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Unifying Perception: Everything as Pixels",
                    "desc": "This paper introduces a new approach called 'Perceive Everything as Pixels' (PEAP), which aims to unify various input modalities like text, images, and diagrams into a single pixel-based format. The authors present PixelWorld, a novel evaluation suite designed to assess the performance of existing models when using this unified pixel input. Their experiments reveal that PEAP outperforms traditional token-based methods in multimodal datasets, although it highlights a decline in reasoning and coding abilities across models when using pixel inputs. The study concludes that while current models excel in pixel perception, there is still significant potential for enhancing their overall perceptual capabilities."
                },
                "zh": {
                    "title": "ç»Ÿä¸€æ„ŸçŸ¥ï¼šå°†ä¸€åˆ‡è§†ä¸ºåƒç´ ",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç»Ÿä¸€æ„ŸçŸ¥æ¡†æ¶ï¼Œç§°ä¸ºâ€œå°†ä¸€åˆ‡è§†ä¸ºåƒç´ â€ï¼ˆPEAPï¼‰ï¼Œæ—¨åœ¨å°†æ–‡æœ¬ã€è¡¨æ ¼ã€ä»£ç ã€å›¾è¡¨å’Œå›¾åƒç­‰å¤šç§è¾“å…¥å½¢å¼ç»Ÿä¸€ä¸ºåƒç´ è¾“å…¥ã€‚æˆ‘ä»¬å¼•å…¥äº†PixelWorldè¯„ä¼°å¥—ä»¶ï¼Œä»¥åœ¨åƒç´ ç©ºé—´ä¸­è¯„ä¼°ç°æœ‰æ¨¡å‹çš„æ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼ŒPEAPåœ¨å¤šæ¨¡æ€æ•°æ®é›†ä¸Šä¼˜äºåŸºäºæ ‡è®°çš„è¾“å…¥ï¼Œæ˜¾ç¤ºå‡ºç»Ÿä¸€è¾“å…¥åœ¨æ¶ˆæ­§ä¹‰æ–¹é¢çš„ä¼˜åŠ¿ã€‚åŒæ—¶ï¼Œå¤„ç†åƒç´ è¾“å…¥æ—¶ï¼Œæ‰€æœ‰æ¨¡å‹çš„æ¨ç†å’Œç¼–ç èƒ½åŠ›æ˜¾è‘—ä¸‹é™ï¼Œè¡¨æ˜éœ€è¦å¢å¼ºåŸºç¡€æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.14677",
            "title": "MatAnyone: Stable Video Matting with Consistent Memory Propagation",
            "url": "https://huggingface.co/papers/2501.14677",
            "abstract": "Auxiliary-free human video matting methods, which rely solely on input frames, often struggle with complex or ambiguous backgrounds. To address this, we propose MatAnyone, a robust framework tailored for target-assigned video matting. Specifically, building on a memory-based paradigm, we introduce a consistent memory propagation module via region-adaptive memory fusion, which adaptively integrates memory from the previous frame. This ensures semantic stability in core regions while preserving fine-grained details along object boundaries. For robust training, we present a larger, high-quality, and diverse dataset for video matting. Additionally, we incorporate a novel training strategy that efficiently leverages large-scale segmentation data, boosting matting stability. With this new network design, dataset, and training strategy, MatAnyone delivers robust and accurate video matting results in diverse real-world scenarios, outperforming existing methods.",
            "score": 6,
            "issue_id": 2010,
            "pub_date": "2025-01-24",
            "pub_date_card": {
                "ru": "24 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 24",
                "zh": "1æœˆ24æ—¥"
            },
            "hash": "a9968478421ddc33",
            "authors": [
                "Peiqing Yang",
                "Shangchen Zhou",
                "Jixin Zhao",
                "Qingyi Tao",
                "Chen Change Loy"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "SenseTime Research, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.14677.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#video"
                ],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "MatAnyone: Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "MatAnyone - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ½Ğ° Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. MatAnyone Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°."
                },
                "en": {
                    "title": "MatAnyone: Robust Video Matting with Memory Propagation",
                    "desc": "The paper introduces MatAnyone, a new framework for video matting that does not require auxiliary inputs. It utilizes a memory-based approach with a memory propagation module that adapts memory from previous frames to maintain semantic consistency and detail. The authors also present a larger and more diverse dataset for training, along with a novel strategy that uses extensive segmentation data to enhance matting stability. Overall, MatAnyone achieves superior performance in complex video environments compared to existing methods."
                },
                "zh": {
                    "title": "MatAnyoneï¼šè§†é¢‘æŠ å›¾çš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMatAnyoneçš„è§†é¢‘æŠ å›¾æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤æ‚èƒŒæ™¯ä¸‹çš„æŠ å›¾é—®é¢˜ã€‚è¯¥æ–¹æ³•åŸºäºè®°å¿†ä¼ æ’­æ¨¡å—ï¼Œé€šè¿‡åŒºåŸŸè‡ªé€‚åº”è®°å¿†èåˆï¼ŒåŠ¨æ€æ•´åˆå‰ä¸€å¸§çš„è®°å¿†ä¿¡æ¯ï¼Œä»è€Œç¡®ä¿æ ¸å¿ƒåŒºåŸŸçš„è¯­ä¹‰ç¨³å®šæ€§ã€‚ä¸ºäº†æé«˜è®­ç»ƒçš„é²æ£’æ€§ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªæ›´å¤§ã€æ›´é«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„è§†é¢‘æŠ å›¾æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„è®­ç»ƒç­–ç•¥ï¼Œå……åˆ†åˆ©ç”¨å¤§è§„æ¨¡åˆ†å‰²æ•°æ®ã€‚æœ€ç»ˆï¼ŒMatAnyoneåœ¨å¤šç§çœŸå®åœºæ™¯ä¸­å±•ç°å‡ºä¼˜è¶Šçš„æŠ å›¾æ•ˆæœï¼Œè¶…è¶Šäº†ç°æœ‰çš„æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.19399",
            "title": "Scalable-Softmax Is Superior for Attention",
            "url": "https://huggingface.co/papers/2501.19399",
            "abstract": "The maximum element of the vector output by the Softmax function approaches zero as the input vector size increases. Transformer-based language models rely on Softmax to compute attention scores, causing the attention distribution to flatten as the context size grows. This reduces the model's ability to prioritize key information effectively and potentially limits its length generalization. To address this problem, we propose Scalable-Softmax (SSMax), which replaces Softmax in scenarios where the input vector size varies. SSMax can be seamlessly integrated into existing Transformer-based architectures. Experimental results in language modeling show that models using SSMax not only achieve faster loss reduction during pretraining but also significantly improve performance in long contexts and key information retrieval. Furthermore, an analysis of attention scores reveals that SSMax enables the model to focus attention on key information even in long contexts. Additionally, although models that use SSMax from the beginning of pretraining achieve better length generalization, those that have already started pretraining can still gain some of this ability by replacing Softmax in the attention layers with SSMax, either during or after pretraining.",
            "score": 6,
            "issue_id": 2009,
            "pub_date": "2025-01-31",
            "pub_date_card": {
                "ru": "31 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 31",
                "zh": "1æœˆ31æ—¥"
            },
            "hash": "12ed1cad789702aa",
            "authors": [
                "Ken M. Nakanishi"
            ],
            "affiliations": [
                "Institute for Physics of Intelligence, The University of Tokyo, Tokyo 113-0033, Japan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.19399.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#long_context",
                    "#optimization"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "SSMax: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Scalable-Softmax (SSMax) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸. SSMax Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑƒĞ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ SSMax Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸. SSMax Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Attention with Scalable-Softmax for Better Context Handling",
                    "desc": "This paper addresses a limitation in Transformer-based language models where the Softmax function causes attention scores to flatten as the input size increases. This flattening reduces the model's ability to focus on important information, especially in longer contexts. The authors propose a new method called Scalable-Softmax (SSMax) that replaces the traditional Softmax function, allowing for better attention distribution and improved performance in long contexts. Experimental results show that SSMax enhances loss reduction during pretraining and enables better retrieval of key information, even for models that have already begun pretraining."
                },
                "zh": {
                    "title": "å¯æ‰©å±•Softmaxï¼šæå‡Transformeræ¨¡å‹çš„æ³¨æ„åŠ›èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºå¯æ‰©å±•Softmaxï¼ˆSSMaxï¼‰ï¼Œæ—¨åœ¨è§£å†³Transformeræ¨¡å‹åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶çš„æ³¨æ„åŠ›åˆ†å¸ƒæ‰å¹³åŒ–é—®é¢˜ã€‚ä¼ ç»Ÿçš„Softmaxå‡½æ•°åœ¨è¾“å…¥å‘é‡å¢å¤§æ—¶ï¼Œæœ€å¤§å…ƒç´ è¶‹è¿‘äºé›¶ï¼Œå¯¼è‡´æ¨¡å‹æ— æ³•æœ‰æ•ˆåœ°ä¼˜å…ˆè€ƒè™‘å…³é”®ä¿¡æ¯ã€‚SSMaxå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„Transformeræ¶æ„ä¸­ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨SSMaxçš„æ¨¡å‹åœ¨è¯­è¨€å»ºæ¨¡ä¸­ä¸ä»…åœ¨é¢„è®­ç»ƒæœŸé—´å®ç°äº†æ›´å¿«çš„æŸå¤±å‡å°‘ï¼Œè¿˜æ˜¾è‘—æé«˜äº†åœ¨é•¿ä¸Šä¸‹æ–‡ä¸­çš„æ€§èƒ½ã€‚é€šè¿‡åˆ†ææ³¨æ„åŠ›åˆ†æ•°ï¼ŒSSMaxä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨é•¿ä¸Šä¸‹æ–‡ä¸­æ›´å¥½åœ°å…³æ³¨å…³é”®ä¿¡æ¯ï¼Œæå‡äº†æ¨¡å‹çš„é•¿åº¦æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04983",
            "title": "DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning",
            "url": "https://huggingface.co/papers/2411.04983",
            "abstract": "The ability to predict future outcomes given control actions is fundamental for physical reasoning. However, such predictive models, often called world models, have proven challenging to learn and are typically developed for task-specific solutions with online policy learning. We argue that the true potential of world models lies in their ability to reason and plan across diverse problems using only passive data. Concretely, we require world models to have the following three properties: 1) be trainable on offline, pre-collected trajectories, 2) support test-time behavior optimization, and 3) facilitate task-agnostic reasoning. To realize this, we present DINO World Model (DINO-WM), a new method to model visual dynamics without reconstructing the visual world. DINO-WM leverages spatial patch features pre-trained with DINOv2, enabling it to learn from offline behavioral trajectories by predicting future patch features. This design allows DINO-WM to achieve observational goals through action sequence optimization, facilitating task-agnostic behavior planning by treating desired goal patch features as prediction targets. We evaluate DINO-WM across various domains, including maze navigation, tabletop pushing, and particle manipulation. Our experiments demonstrate that DINO-WM can generate zero-shot behavioral solutions at test time without relying on expert demonstrations, reward modeling, or pre-learned inverse models. Notably, DINO-WM exhibits strong generalization capabilities compared to prior state-of-the-art work, adapting to diverse task families such as arbitrarily configured mazes, push manipulation with varied object shapes, and multi-particle scenarios.",
            "score": 6,
            "issue_id": 1999,
            "pub_date": "2025-11-07",
            "pub_date_card": {
                "ru": "7 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 7",
                "zh": "11æœˆ7æ—¥"
            },
            "hash": "e72081596b626524",
            "authors": [
                "Gaoyue Zhou",
                "Hengkai Pan",
                "Yann LeCun",
                "Lerrel Pinto"
            ],
            "affiliations": [
                "Courant Institute, New York University",
                "Meta-FAIR"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.04983.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#agents",
                    "#reasoning",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "DINO-WM: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ±ĞµĞ· Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° - DINO World Model (DINO-WM). DINO-WM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹, Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ DINOv2, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¼Ñƒ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ… Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ DINO-WM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… Ñ†ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡Ğ°Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğµ Ğ¾Ñ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DINO-WM Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ Ğ½ÑƒĞ»Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "DINO-WM: Predicting the Future with Passive Data for Task-Agnostic Planning",
                    "desc": "This paper introduces DINO World Model (DINO-WM), a novel approach for creating predictive models that can reason and plan across various tasks using only passive data. DINO-WM is designed to learn from offline trajectories without needing to reconstruct the visual environment, focusing instead on predicting future visual features. The model is capable of optimizing behavior at test time and supports task-agnostic reasoning, making it versatile for different applications. Experimental results show that DINO-WM can effectively generate solutions in diverse scenarios without relying on expert demonstrations or pre-learned models, showcasing its strong generalization abilities."
                },
                "zh": {
                    "title": "DINO-WMï¼šæ— ä»»åŠ¡ä¾èµ–çš„ä¸–ç•Œæ¨¡å‹",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä¸–ç•Œæ¨¡å‹DINO-WMï¼Œæ—¨åœ¨é€šè¿‡è¢«åŠ¨æ•°æ®è¿›è¡Œæ¨ç†å’Œè§„åˆ’ã€‚DINO-WMå…·æœ‰ä¸‰ä¸ªå…³é”®ç‰¹æ€§ï¼šå¯ä»¥åœ¨ç¦»çº¿æ”¶é›†çš„è½¨è¿¹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ”¯æŒæµ‹è¯•æ—¶è¡Œä¸ºä¼˜åŒ–ï¼Œå¹¶ä¿ƒè¿›ä»»åŠ¡æ— å…³çš„æ¨ç†ã€‚è¯¥æ¨¡å‹åˆ©ç”¨DINOv2é¢„è®­ç»ƒçš„ç©ºé—´è¡¥ä¸ç‰¹å¾ï¼Œé€šè¿‡é¢„æµ‹æœªæ¥çš„è¡¥ä¸ç‰¹å¾æ¥å­¦ä¹ ï¼Œä»è€Œå®ç°è§‚å¯Ÿç›®æ ‡çš„è¡Œä¸ºè§„åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDINO-WMåœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰ä¸“å®¶ç¤ºèŒƒå’Œå¥–åŠ±å»ºæ¨¡çš„æƒ…å†µä¸‹ç”Ÿæˆé›¶-shotè¡Œä¸ºè§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.18837",
            "title": "Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming",
            "url": "https://huggingface.co/papers/2501.18837",
            "abstract": "Large language models (LLMs) are vulnerable to universal jailbreaks-prompting strategies that systematically bypass model safeguards and enable users to carry out harmful processes that require many model interactions, like manufacturing illegal substances at scale. To defend against these attacks, we introduce Constitutional Classifiers: safeguards trained on synthetic data, generated by prompting LLMs with natural language rules (i.e., a constitution) specifying permitted and restricted content. In over 3,000 estimated hours of red teaming, no red teamer found a universal jailbreak that could extract information from an early classifier-guarded LLM at a similar level of detail to an unguarded model across most target queries. On automated evaluations, enhanced classifiers demonstrated robust defense against held-out domain-specific jailbreaks. These classifiers also maintain deployment viability, with an absolute 0.38% increase in production-traffic refusals and a 23.7% inference overhead. Our work demonstrates that defending against universal jailbreaks while maintaining practical deployment viability is tractable.",
            "score": 4,
            "issue_id": 1996,
            "pub_date": "2025-01-31",
            "pub_date_card": {
                "ru": "31 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 31",
                "zh": "1æœˆ31æ—¥"
            },
            "hash": "62d14973b1140e58",
            "authors": [
                "Mrinank Sharma",
                "Meg Tong",
                "Jesse Mu",
                "Jerry Wei",
                "Jorrit Kruthoff",
                "Scott Goodfriend",
                "Euan Ong",
                "Alwin Peng",
                "Raj Agarwal",
                "Cem Anil",
                "Amanda Askell",
                "Nathan Bailey",
                "Joe Benton",
                "Emma Bluemke",
                "Samuel R. Bowman",
                "Eric Christiansen",
                "Hoagy Cunningham",
                "Andy Dau",
                "Anjali Gopal",
                "Rob Gilson",
                "Logan Graham",
                "Logan Howard",
                "Nimit Kalra",
                "Taesung Lee",
                "Kevin Lin",
                "Peter Lofgren",
                "Francesco Mosconi",
                "Clare O'Hara",
                "Catherine Olsson",
                "Linda Petrini",
                "Samir Rajani",
                "Nikhil Saxena",
                "Alex Silverstein",
                "Tanya Singh",
                "Theodore Sumers",
                "Leonard Tang",
                "Kevin K. Troy",
                "Constantin Weisser",
                "Ruiqi Zhong",
                "Giulio Zhou",
                "Jan Leike",
                "Jared Kaplan",
                "Ethan Perez"
            ],
            "affiliations": [
                "Safeguards Research Team, Anthropic"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.18837.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#training",
                    "#architecture",
                    "#dataset",
                    "#security",
                    "#inference"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "ĞšĞ¾Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹: Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ ĞšĞ¾Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² - Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ», Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‰Ğ¸Ñ… Ğ´Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚. Ğ­Ñ‚Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¾ÑÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹, Ğ½Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ·Ğ°Ñ‰Ğ¸Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ¸ Ğ¸Ñ… Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ LLM Ğ¾Ñ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Defending LLMs with Constitutional Classifiers",
                    "desc": "This paper addresses the vulnerability of large language models (LLMs) to universal jailbreaks, which are strategies that allow users to bypass safety measures. The authors propose a solution called Constitutional Classifiers, which are trained on synthetic data generated by LLMs using natural language rules that define what content is allowed or restricted. Through extensive testing, these classifiers showed strong resistance to jailbreak attempts, effectively protecting the model without significantly impacting its performance. The findings suggest that it is possible to defend against such attacks while still ensuring the model can be used effectively in real-world applications."
                },
                "zh": {
                    "title": "å®ªæ³•åˆ†ç±»å™¨ï¼šä¿æŠ¤å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å®¹æ˜“å—åˆ°æ™®éè¶Šç‹±æ”»å‡»ï¼Œè¿™ç§æ”»å‡»å¯ä»¥ç»•è¿‡æ¨¡å‹çš„å®‰å…¨æªæ–½ï¼Œå…è®¸ç”¨æˆ·è¿›è¡Œæœ‰å®³æ“ä½œã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å®ªæ³•åˆ†ç±»å™¨ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåˆæˆæ•°æ®è®­ç»ƒçš„å®‰å…¨æªæ–½ï¼Œåˆæˆæ•°æ®æ˜¯é€šè¿‡è‡ªç„¶è¯­è¨€è§„åˆ™ï¼ˆå³å®ªæ³•ï¼‰æç¤ºLLMsç”Ÿæˆçš„ï¼Œè§„å®šäº†å…è®¸å’Œé™åˆ¶çš„å†…å®¹ã€‚åœ¨è¶…è¿‡3000å°æ—¶çš„çº¢é˜Ÿæµ‹è¯•ä¸­ï¼Œæ²¡æœ‰çº¢é˜Ÿæˆå‘˜æ‰¾åˆ°èƒ½å¤Ÿä»æ—©æœŸåˆ†ç±»å™¨ä¿æŠ¤çš„LLMä¸­æå–ä¿¡æ¯çš„æ™®éè¶Šç‹±æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œåœ¨ä¿æŒå®é™…éƒ¨ç½²å¯è¡Œæ€§çš„åŒæ—¶ï¼Œé˜²å¾¡æ™®éè¶Šç‹±æ”»å‡»æ˜¯å¯è¡Œçš„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.18841",
            "title": "Trading Inference-Time Compute for Adversarial Robustness",
            "url": "https://huggingface.co/papers/2501.18841",
            "abstract": "We conduct experiments on the impact of increasing inference-time compute in reasoning models (specifically OpenAI o1-preview and o1-mini) on their robustness to adversarial attacks. We find that across a variety of attacks, increased inference-time compute leads to improved robustness. In many cases (with important exceptions), the fraction of model samples where the attack succeeds tends to zero as the amount of test-time compute grows. We perform no adversarial training for the tasks we study, and we increase inference-time compute by simply allowing the models to spend more compute on reasoning, independently of the form of attack. Our results suggest that inference-time compute has the potential to improve adversarial robustness for Large Language Models. We also explore new attacks directed at reasoning models, as well as settings where inference-time compute does not improve reliability, and speculate on the reasons for these as well as ways to address them.",
            "score": 3,
            "issue_id": 1994,
            "pub_date": "2025-01-31",
            "pub_date_card": {
                "ru": "31 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 31",
                "zh": "1æœˆ31æ—¥"
            },
            "hash": "f1e75e6b24f3e044",
            "authors": [
                "Wojciech Zaremba",
                "Evgenia Nitishinskaya",
                "Boaz Barak",
                "Stephanie Lin",
                "Sam Toyer",
                "Yaodong Yu",
                "Rachel Dias",
                "Eric Wallace",
                "Kai Xiao",
                "Johannes Heidecke",
                "Amelia Glaese"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2501.18841.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#reasoning",
                    "#inference"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ - Ğ²Ñ‹ÑˆĞµ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ°: Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½Ğ° ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğº ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼. Ğ’ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğµ ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ğ´Ğ¾Ğ»Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ°Ñ‚Ğ°Ğº ÑÑ‚Ñ€ĞµĞ¼Ğ¸Ñ‚ÑÑ Ğº Ğ½ÑƒĞ»Ñ Ğ¿Ñ€Ğ¸ Ñ€Ğ¾ÑÑ‚Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼."
                },
                "en": {
                    "title": "Boosting Robustness: More Compute, Less Vulnerability",
                    "desc": "This paper investigates how increasing the amount of compute used during inference can enhance the robustness of reasoning models, specifically OpenAI's o1-preview and o1-mini, against adversarial attacks. The authors find that as the compute increases, the success rate of these attacks generally decreases, indicating improved model resilience. Notably, this improvement occurs without any adversarial training, simply by allowing the models to utilize more resources for reasoning tasks. The study also examines new types of attacks and scenarios where increased compute does not lead to better reliability, providing insights into potential solutions."
                },
                "zh": {
                    "title": "å¢åŠ æ¨ç†è®¡ç®—ï¼Œæå‡æ¨¡å‹é²æ£’æ€§",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨æ¨ç†æ¨¡å‹ä¸­å¢åŠ æ¨ç†æ—¶é—´è®¡ç®—å¯¹å…¶æŠµå¾¡å¯¹æŠ—æ”»å‡»çš„å½±å“ã€‚æˆ‘ä»¬å‘ç°ï¼Œéšç€æ¨ç†æ—¶é—´è®¡ç®—çš„å¢åŠ ï¼Œæ¨¡å‹çš„é²æ£’æ€§å¾—åˆ°äº†æå‡ã€‚å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæ”»å‡»æˆåŠŸçš„æ¨¡å‹æ ·æœ¬æ¯”ä¾‹éšç€æµ‹è¯•æ—¶é—´è®¡ç®—çš„å¢åŠ è€Œè¶‹è¿‘äºé›¶ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ¨ç†æ—¶é—´è®¡ç®—æœ‰æ½œåŠ›æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹æŠ—é²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.18052",
            "title": "SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders",
            "url": "https://huggingface.co/papers/2501.18052",
            "abstract": "Diffusion models, while powerful, can inadvertently generate harmful or undesirable content, raising significant ethical and safety concerns. Recent machine unlearning approaches offer potential solutions but often lack transparency, making it difficult to understand the changes they introduce to the base model. In this work, we introduce SAeUron, a novel method leveraging features learned by sparse autoencoders (SAEs) to remove unwanted concepts in text-to-image diffusion models. First, we demonstrate that SAEs, trained in an unsupervised manner on activations from multiple denoising timesteps of the diffusion model, capture sparse and interpretable features corresponding to specific concepts. Building on this, we propose a feature selection method that enables precise interventions on model activations to block targeted content while preserving overall performance. Evaluation with the competitive UnlearnCanvas benchmark on object and style unlearning highlights SAeUron's state-of-the-art performance. Moreover, we show that with a single SAE, we can remove multiple concepts simultaneously and that in contrast to other methods, SAeUron mitigates the possibility of generating unwanted content, even under adversarial attack. Code and checkpoints are available at: https://github.com/cywinski/SAeUron.",
            "score": 2,
            "issue_id": 2011,
            "pub_date": "2025-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "d94056a77d806ada",
            "authors": [
                "Bartosz CywiÅ„ski",
                "Kamil Deja"
            ],
            "affiliations": [
                "IDEAS NCBR",
                "Warsaw University of Technology, Poland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.18052.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#interpretability",
                    "#security",
                    "#architecture",
                    "#ethics",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ§¹",
                "ru": {
                    "title": "Ğ§Ğ¸ÑÑ‚ĞºĞ° Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹: SAeUron ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "SAeUron - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ (SAE) Ğ´Ğ»Ñ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸ÑĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ²Ğ¼ĞµÑˆĞ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. SAeUron Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ´Ğ°Ğ»ÑÑ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾."
                },
                "en": {
                    "title": "SAeUron: Safeguarding Diffusion Models with Sparse Autoencoders",
                    "desc": "This paper presents SAeUron, a new method designed to improve the safety of text-to-image diffusion models by removing unwanted concepts. It utilizes sparse autoencoders (SAEs) to learn and identify specific features from the model's activations, allowing for targeted interventions. The method enables precise control over the model's outputs while maintaining its overall performance. Evaluation shows that SAeUron outperforms existing techniques in unlearning tasks and effectively reduces the risk of generating harmful content."
                },
                "zh": {
                    "title": "SAeUronï¼šå»é™¤ä¸è‰¯å†…å®¹çš„æ–°æ–¹æ³•",
                    "desc": "æ‰©æ•£æ¨¡å‹è™½ç„¶å¼ºå¤§ï¼Œä½†å¯èƒ½ä¼šç”Ÿæˆæœ‰å®³æˆ–ä¸è‰¯å†…å®¹ï¼Œå¸¦æ¥ä¼¦ç†å’Œå®‰å…¨é—®é¢˜ã€‚æœ€è¿‘çš„æœºå™¨é—å¿˜æ–¹æ³•æä¾›äº†æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆï¼Œä½†é€šå¸¸ç¼ºä¹é€æ˜æ€§ï¼Œéš¾ä»¥ç†è§£å¯¹åŸºç¡€æ¨¡å‹çš„æ›´æ”¹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•SAeUronï¼Œåˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰å­¦ä¹ çš„ç‰¹å¾æ¥å»é™¤æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„ä¸å¿…è¦æ¦‚å¿µã€‚é€šè¿‡åœ¨å¤šä¸ªå»å™ªæ—¶é—´æ­¥çš„æ¿€æ´»ä¸Šæ— ç›‘ç£è®­ç»ƒSAEï¼Œæˆ‘ä»¬æ•æ‰åˆ°ä¸ç‰¹å®šæ¦‚å¿µå¯¹åº”çš„ç¨€ç–å’Œå¯è§£é‡Šç‰¹å¾ï¼Œä»è€Œå®ç°ç²¾ç¡®å¹²é¢„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.18965",
            "title": "The Surprising Agreement Between Convex Optimization Theory and Learning-Rate Scheduling for Large Model Training",
            "url": "https://huggingface.co/papers/2501.18965",
            "abstract": "We show that learning-rate schedules for large model training behave surprisingly similar to a performance bound from non-smooth convex optimization theory. We provide a bound for the constant schedule with linear cooldown; in particular, the practical benefit of cooldown is reflected in the bound due to the absence of logarithmic terms. Further, we show that this surprisingly close match between optimization theory and practice can be exploited for learning-rate tuning: we achieve noticeable improvements for training 124M and 210M Llama-type models by (i) extending the schedule for continued training with optimal learning-rate, and (ii) transferring the optimal learning-rate across schedules.",
            "score": 2,
            "issue_id": 2007,
            "pub_date": "2025-01-31",
            "pub_date_card": {
                "ru": "31 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 31",
                "zh": "1æœˆ31æ—¥"
            },
            "hash": "a136293a2241150e",
            "authors": [
                "Fabian Schaipp",
                "Alexander HÃ¤gele",
                "Adrien Taylor",
                "Umut Simsekli",
                "Francis Bach"
            ],
            "affiliations": [
                "EPFL, Lausanne, Switzerland",
                "Inria, Departement dInformatique de lEcole Normale Superieure, PSL Research University, Paris, France"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.18965.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#math",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ“ˆ",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ğ¾Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ÑÑ‚Ğ¸Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ñ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ğ¼Ğ¸ Ğ¸Ğ· Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ½ĞµĞ²Ñ‹Ğ¿ÑƒĞºĞ»Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¸. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ğ¸Ğ¿Ğ° Llama Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 124M Ğ¸ 210M Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Optimizing Learning Rates: Bridging Theory and Practice",
                    "desc": "This paper explores the relationship between learning-rate schedules in large model training and concepts from non-smooth convex optimization theory. It establishes a performance bound for a constant learning-rate schedule with a linear cooldown, highlighting the practical advantages of this approach. The authors demonstrate that the alignment between theoretical optimization and practical training can be leveraged for better learning-rate tuning. By optimizing the learning-rate schedule, they achieve significant improvements in training large Llama-type models."
                },
                "zh": {
                    "title": "ä¼˜åŒ–å­¦ä¹ ç‡è°ƒåº¦ï¼Œæå‡å¤§æ¨¡å‹è®­ç»ƒæ•ˆæœ",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤§æ¨¡å‹è®­ç»ƒä¸­çš„å­¦ä¹ ç‡è°ƒåº¦ä¸éå…‰æ»‘å‡¸ä¼˜åŒ–ç†è®ºä¸­çš„æ€§èƒ½ç•Œé™ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªçº¿æ€§å†·å´çš„å¸¸æ•°è°ƒåº¦çš„ç•Œé™ï¼Œç‰¹åˆ«æ˜¯å†·å´çš„å®é™…å¥½å¤„åœ¨äºæ²¡æœ‰å¯¹æ•°é¡¹çš„å½±å“ã€‚è¿›ä¸€æ­¥åœ°ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¼˜åŒ–ç†è®ºä¸å®è·µä¹‹é—´çš„ç´§å¯†è”ç³»å¯ä»¥ç”¨äºå­¦ä¹ ç‡è°ƒä¼˜ï¼šé€šè¿‡å»¶é•¿è°ƒåº¦ä»¥ç»§ç»­è®­ç»ƒå¹¶ä½¿ç”¨æœ€ä½³å­¦ä¹ ç‡ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒ124Må’Œ210Mçš„Llamaç±»å‹æ¨¡å‹æ—¶å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚æœ€åï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†åœ¨ä¸åŒè°ƒåº¦ä¹‹é—´è½¬ç§»æœ€ä½³å­¦ä¹ ç‡çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.18753",
            "title": "INT: Instance-Specific Negative Mining for Task-Generic Promptable Segmentation",
            "url": "https://huggingface.co/papers/2501.18753",
            "abstract": "Task-generic promptable image segmentation aims to achieve segmentation of diverse samples under a single task description by utilizing only one task-generic prompt. Current methods leverage the generalization capabilities of Vision-Language Models (VLMs) to infer instance-specific prompts from these task-generic prompts in order to guide the segmentation process. However, when VLMs struggle to generalise to some image instances, predicting instance-specific prompts becomes poor. To solve this problem, we introduce Instance-specific Negative Mining for Task-Generic Promptable Segmentation (INT). The key idea of INT is to adaptively reduce the influence of irrelevant (negative) prior knowledge whilst to increase the use the most plausible prior knowledge, selected by negative mining with higher contrast, in order to optimise instance-specific prompts generation. Specifically, INT consists of two components: (1) instance-specific prompt generation, which progressively fliters out incorrect information in prompt generation; (2) semantic mask generation, which ensures each image instance segmentation matches correctly the semantics of the instance-specific prompts. INT is validated on six datasets, including camouflaged objects and medical images, demonstrating its effectiveness, robustness and scalability.",
            "score": 2,
            "issue_id": 2002,
            "pub_date": "2025-01-30",
            "pub_date_card": {
                "ru": "30 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 30",
                "zh": "1æœˆ30æ—¥"
            },
            "hash": "000663cf445862a1",
            "authors": [
                "Jian Hu",
                "Zixu Cheng",
                "Shaogang Gong"
            ],
            "affiliations": [
                "Queen Mary University of London"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.18753.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#healthcare",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ INT (Instance-specific Negative Mining for Task-Generic Promptable Segmentation). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ñ‹Ñ…. INT ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ², Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ°ÑĞºĞ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºĞ°Ğ¼ÑƒÑ„Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Enhancing Image Segmentation with Smart Prompting",
                    "desc": "This paper presents a new method called Instance-specific Negative Mining for Task-Generic Promptable Segmentation (INT) to improve image segmentation using a single task description. The method addresses the challenge of Vision-Language Models (VLMs) struggling to generalize to certain image instances, which can lead to poor segmentation results. INT works by selectively reducing the impact of irrelevant information while enhancing the use of relevant prior knowledge through a process called negative mining. The effectiveness of INT is validated across six diverse datasets, showing its ability to produce accurate and robust segmentation results."
                },
                "zh": {
                    "title": "å®ä¾‹ç‰¹å®šè´Ÿé‡‡æ ·ä¼˜åŒ–å›¾åƒåˆ†å‰²",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºå®ä¾‹ç‰¹å®šè´Ÿé‡‡æ ·ï¼ˆINTï¼‰ï¼Œç”¨äºä»»åŠ¡é€šç”¨çš„å¯æç¤ºå›¾åƒåˆ†å‰²ã€‚INTçš„æ ¸å¿ƒæ€æƒ³æ˜¯è‡ªé€‚åº”åœ°å‡å°‘æ— å…³çš„å…ˆéªŒçŸ¥è¯†çš„å½±å“ï¼ŒåŒæ—¶å¢åŠ æœ€æœ‰å¯èƒ½çš„å…ˆéªŒçŸ¥è¯†çš„ä½¿ç”¨ï¼Œä»¥ä¼˜åŒ–å®ä¾‹ç‰¹å®šæç¤ºçš„ç”Ÿæˆã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šå®ä¾‹ç‰¹å®šæç¤ºç”Ÿæˆå’Œè¯­ä¹‰æ©ç ç”Ÿæˆï¼Œç¡®ä¿æ¯ä¸ªå›¾åƒå®ä¾‹çš„åˆ†å‰²ä¸æç¤ºçš„è¯­ä¹‰ç›¸åŒ¹é…ã€‚é€šè¿‡åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šçš„éªŒè¯ï¼ŒINTå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€é²æ£’æ€§å’Œå¯æ‰©å±•æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.18804",
            "title": "Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion",
            "url": "https://huggingface.co/papers/2501.18804",
            "abstract": "Current methods for 3D scene reconstruction from sparse posed images employ intermediate 3D representations such as neural fields, voxel grids, or 3D Gaussians, to achieve multi-view consistent scene appearance and geometry. In this paper we introduce MVGD, a diffusion-based architecture capable of direct pixel-level generation of images and depth maps from novel viewpoints, given an arbitrary number of input views. Our method uses raymap conditioning to both augment visual features with spatial information from different viewpoints, as well as to guide the generation of images and depth maps from novel views. A key aspect of our approach is the multi-task generation of images and depth maps, using learnable task embeddings to guide the diffusion process towards specific modalities. We train this model on a collection of more than 60 million multi-view samples from publicly available datasets, and propose techniques to enable efficient and consistent learning in such diverse conditions. We also propose a novel strategy that enables the efficient training of larger models by incrementally fine-tuning smaller ones, with promising scaling behavior. Through extensive experiments, we report state-of-the-art results in multiple novel view synthesis benchmarks, as well as multi-view stereo and video depth estimation.",
            "score": 1,
            "issue_id": 2010,
            "pub_date": "2025-01-30",
            "pub_date_card": {
                "ru": "30 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 30",
                "zh": "1æœˆ30æ—¥"
            },
            "hash": "32db517ad974401b",
            "authors": [
                "Vitor Guizilini",
                "Muhammad Zubair Irshad",
                "Dian Chen",
                "Greg Shakhnarovich",
                "Rares Ambrus"
            ],
            "affiliations": [
                "Toyota Research Institute (TRI)",
                "Toyota Technological Institute at Chicago (TTIC)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.18804.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#3d",
                    "#diffusion",
                    "#architecture",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D ÑÑ†ĞµĞ½ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MVGD - Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ°Ñ€Ñ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ¹ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ°ÑĞ¿ĞµĞºÑ‚ - Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ°Ñ€Ñ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 60 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹."
                },
                "en": {
                    "title": "Revolutionizing 3D Scene Reconstruction with Direct Pixel-Level Generation",
                    "desc": "This paper presents MVGD, a new diffusion-based model for generating images and depth maps from multiple input views in 3D scene reconstruction. Unlike traditional methods that rely on intermediate 3D representations, MVGD directly produces pixel-level outputs, enhancing visual features with spatial information through raymap conditioning. The model employs multi-task learning, using task embeddings to effectively guide the generation process for both images and depth maps. Trained on a vast dataset of over 60 million samples, MVGD achieves state-of-the-art performance in novel view synthesis and depth estimation tasks."
                },
                "zh": {
                    "title": "MVGDï¼šä»å¤šè§†è§’ç›´æ¥ç”Ÿæˆå›¾åƒä¸æ·±åº¦å›¾çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMVGDçš„æ‰©æ•£åŸºç¡€æ¶æ„ï¼Œèƒ½å¤Ÿç›´æ¥ä»å¤šä¸ªè§†è§’ç”Ÿæˆå›¾åƒå’Œæ·±åº¦å›¾ã€‚è¯¥æ–¹æ³•é€šè¿‡å…‰çº¿å›¾æ¡ä»¶åŒ–ï¼Œå¢å¼ºäº†è§†è§‰ç‰¹å¾å¹¶å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚æˆ‘ä»¬é‡‡ç”¨å¤šä»»åŠ¡ç”ŸæˆæŠ€æœ¯ï¼ŒåŒæ—¶ç”Ÿæˆå›¾åƒå’Œæ·±åº¦å›¾ï¼Œå¹¶ä½¿ç”¨å¯å­¦ä¹ çš„ä»»åŠ¡åµŒå…¥æ¥ä¼˜åŒ–æ‰©æ•£è¿‡ç¨‹ã€‚ç»è¿‡åœ¨è¶…è¿‡6000ä¸‡å¤šè§†è§’æ ·æœ¬ä¸Šçš„è®­ç»ƒï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.18128",
            "title": "Unraveling the Capabilities of Language Models in News Summarization",
            "url": "https://huggingface.co/papers/2501.18128",
            "abstract": "Given the recent introduction of multiple language models and the ongoing demand for improved Natural Language Processing tasks, particularly summarization, this work provides a comprehensive benchmarking of 20 recent language models, focusing on smaller ones for the news summarization task. In this work, we systematically test the capabilities and effectiveness of these models in summarizing news article texts which are written in different styles and presented in three distinct datasets. Specifically, we focus in this study on zero-shot and few-shot learning settings and we apply a robust evaluation methodology that combines different evaluation concepts including automatic metrics, human evaluation, and LLM-as-a-judge. Interestingly, including demonstration examples in the few-shot learning setting did not enhance models' performance and, in some cases, even led to worse quality of the generated summaries. This issue arises mainly due to the poor quality of the gold summaries that have been used as reference summaries, which negatively impacts the models' performance. Furthermore, our study's results highlight the exceptional performance of GPT-3.5-Turbo and GPT-4, which generally dominate due to their advanced capabilities. However, among the public models evaluated, certain models such as Qwen1.5-7B, SOLAR-10.7B-Instruct-v1.0, Meta-Llama-3-8B and Zephyr-7B-Beta demonstrated promising results. These models showed significant potential, positioning them as competitive alternatives to large models for the task of news summarization.",
            "score": 1,
            "issue_id": 2000,
            "pub_date": "2025-01-30",
            "pub_date_card": {
                "ru": "30 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 30",
                "zh": "1æœˆ30æ—¥"
            },
            "hash": "1c3f3a16953a5a59",
            "authors": [
                "Abdurrahman OdabaÅŸÄ±",
                "GÃ¶ksel Biricik"
            ],
            "affiliations": [
                "Department of Computer Engineering, Turkish-German University, 34820, Istanbul, Turkiye",
                "Department of Computer Engineering, YÄ±ldÄ±z Technical University, 34220, Istanbul, Turkiye"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.18128.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#multilingual",
                    "#small_models",
                    "#benchmark",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ“°",
                "ru": {
                    "title": "ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ€Ğ¾ÑĞ°ÑÑ‚ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚Ğ°Ğ¼ Ğ² ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ 20 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ½Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ñ… zero-shot Ğ¸ few-shot Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¸ LLM Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒĞ´ÑŒĞ¸. Ğ˜Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ few-shot Ğ½Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ° Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ… Ğ´Ğ°Ğ¶Ğµ ÑƒÑ…ÑƒĞ´ÑˆĞ¸Ğ»Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… ÑĞ²Ğ¾Ğ´Ğ¾Ğº. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ GPT-3.5-Turbo Ğ¸ GPT-4, Ğ½Ğ¾ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Qwen1.5-7B Ğ¸ SOLAR-10.7B-Instruct-v1.0."
                },
                "en": {
                    "title": "Benchmarking News Summarization: Small Models Can Compete!",
                    "desc": "This paper benchmarks 20 recent language models specifically for the task of news summarization, emphasizing smaller models. It evaluates their performance in zero-shot and few-shot learning scenarios across three different datasets with varying writing styles. The study reveals that providing demonstration examples in few-shot settings often does not improve, and can even degrade, the quality of summaries due to the inadequacy of reference summaries. Notably, while larger models like GPT-3.5-Turbo and GPT-4 excel, several smaller models also show competitive performance, suggesting they could be viable alternatives for summarization tasks."
                },
                "zh": {
                    "title": "å°æ¨¡å‹åœ¨æ–°é—»æ‘˜è¦ä¸­çš„æ½œåŠ›ä¸æŒ‘æˆ˜",
                    "desc": "æœ¬ç ”ç©¶å¯¹20ç§æœ€æ–°çš„è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œé‡ç‚¹å…³æ³¨è¾ƒå°çš„æ¨¡å‹åœ¨æ–°é—»æ‘˜è¦ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°æµ‹è¯•äº†è¿™äº›æ¨¡å‹åœ¨ä¸åŒé£æ ¼çš„æ–°é—»æ–‡ç« æ‘˜è¦ä¸­çš„èƒ½åŠ›å’Œæœ‰æ•ˆæ€§ï¼Œä½¿ç”¨äº†ä¸‰ç§ä¸åŒçš„æ•°æ®é›†ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨å°‘é‡ç¤ºä¾‹å­¦ä¹ çš„è®¾ç½®ä¸­ï¼Œæä¾›ç¤ºä¾‹å¹¶æœªæå‡æ¨¡å‹çš„æ€§èƒ½ï¼Œåè€Œåœ¨æŸäº›æƒ…å†µä¸‹å¯¼è‡´ç”Ÿæˆæ‘˜è¦çš„è´¨é‡ä¸‹é™ã€‚è¿™ä¸»è¦æ˜¯ç”±äºå‚è€ƒæ‘˜è¦çš„è´¨é‡è¾ƒå·®ï¼Œå½±å“äº†æ¨¡å‹çš„è¡¨ç°ï¼ŒåŒæ—¶æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒGPT-3.5-Turboå’ŒGPT-4åœ¨æ€§èƒ½ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2404.07097",
            "title": "Fast Encoder-Based 3D from Casual Videos via Point Track Processing",
            "url": "https://huggingface.co/papers/2404.07097",
            "abstract": "This paper addresses the long-standing challenge of reconstructing 3D structures from videos with dynamic content. Current approaches to this problem were not designed to operate on casual videos recorded by standard cameras or require a long optimization time.   Aiming to significantly improve the efficiency of previous approaches, we present TracksTo4D, a learning-based approach that enables inferring 3D structure and camera positions from dynamic content originating from casual videos using a single efficient feed-forward pass. To achieve this, we propose operating directly over 2D point tracks as input and designing an architecture tailored for processing 2D point tracks. Our proposed architecture is designed with two key principles in mind: (1) it takes into account the inherent symmetries present in the input point tracks data, and (2) it assumes that the movement patterns can be effectively represented using a low-rank approximation. TracksTo4D is trained in an unsupervised way on a dataset of casual videos utilizing only the 2D point tracks extracted from the videos, without any 3D supervision. Our experiments show that TracksTo4D can reconstruct a temporal point cloud and camera positions of the underlying video with accuracy comparable to state-of-the-art methods, while drastically reducing runtime by up to 95\\%. We further show that TracksTo4D generalizes well to unseen videos of unseen semantic categories at inference time.",
            "score": 1,
            "issue_id": 1999,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 10",
                "zh": "4æœˆ10æ—¥"
            },
            "hash": "a526ae197fe3a8c7",
            "authors": [
                "Yoni Kasten",
                "Wuyue Lu",
                "Haggai Maron"
            ],
            "affiliations": [
                "NVIDIA Research",
                "Simon Fraser University",
                "Technion"
            ],
            "pdf_title_img": "assets/pdf/title_img/2404.07097.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#training",
                    "#cv",
                    "#architecture"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TracksTo4D - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 3D ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ½Ğ° 2D Ñ‚Ñ€ĞµĞºĞ°Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. TracksTo4D Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ ÑĞµÑ‚Ğ¸, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞµÑ‚Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing 3D Reconstruction from Casual Videos",
                    "desc": "This paper introduces TracksTo4D, a novel machine learning approach for reconstructing 3D structures from casual videos. Unlike traditional methods that require extensive optimization or are limited to specific video types, TracksTo4D processes 2D point tracks directly in a single, efficient feed-forward pass. The architecture is designed to leverage the symmetries in point track data and utilizes low-rank approximations to represent movement patterns effectively. Trained unsupervised on a dataset of casual videos, TracksTo4D achieves high accuracy in reconstructing temporal point clouds and camera positions while significantly reducing processing time."
                },
                "zh": {
                    "title": "é«˜æ•ˆé‡å»º3Dç»“æ„ï¼ŒTracksTo4Då¼•é¢†æ–°æ½®æµ",
                    "desc": "æœ¬æ–‡è§£å†³äº†ä»åŠ¨æ€å†…å®¹è§†é¢‘é‡å»º3Dç»“æ„çš„é•¿æœŸæŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•æ— æ³•å¤„ç†æ™®é€šç›¸æœºå½•åˆ¶çš„éšæ„è§†é¢‘ï¼Œæˆ–éœ€è¦è¾ƒé•¿çš„ä¼˜åŒ–æ—¶é—´ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå­¦ä¹ çš„æ–¹æ³•TracksTo4Dï¼Œé€šè¿‡å•æ¬¡é«˜æ•ˆçš„å‰é¦ˆä¼ é€’ï¼Œä»éšæ„è§†é¢‘ä¸­æ¨æ–­3Dç»“æ„å’Œç›¸æœºä½ç½®ã€‚è¯¥æ–¹æ³•ç›´æ¥å¤„ç†2Dç‚¹è½¨è¿¹ï¼Œå¹¶è®¾è®¡äº†ä¸“é—¨çš„æ¶æ„ï¼Œèƒ½å¤Ÿåœ¨æ— ç›‘ç£çš„æƒ…å†µä¸‹è¿›è¡Œè®­ç»ƒï¼Œæ˜¾è‘—æé«˜äº†é‡å»ºæ•ˆç‡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-31.html",
    "link_next": "2025-02-04.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "31.01",
        "en": "01/31",
        "zh": "1æœˆ31æ—¥"
    },
    "short_date_next": {
        "ru": "04.02",
        "en": "02/04",
        "zh": "2æœˆ4æ—¥"
    },
    "categories": {
        "#dataset": 6,
        "#data": 2,
        "#benchmark": 4,
        "#agents": 1,
        "#cv": 4,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 2,
        "#multilingual": 1,
        "#architecture": 5,
        "#healthcare": 1,
        "#training": 9,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 4,
        "#transfer_learning": 3,
        "#graphs": 1,
        "#ethics": 1,
        "#security": 3,
        "#optimization": 7,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„è¯­è¨€å»ºæ¨¡æ–¹æ³•ï¼Œç§°ä¸ºæµ‹è¯•æ—¶ç¼©æ”¾ã€‚OpenAIçš„o1æ¨¡å‹å±•ç¤ºäº†è¿™ç§èƒ½åŠ›ï¼Œä½†æ²¡æœ‰å…¬å¼€å…¶æ–¹æ³•ï¼Œå¯¼è‡´è®¸å¤šå¤åˆ¶åŠªåŠ›ã€‚ä½œè€…é€šè¿‡æ•´ç†ä¸€ä¸ªå°æ•°æ®é›†s1Kå’Œå¼€å‘é¢„ç®—å¼ºåˆ¶æ–¹æ³•ï¼Œå¯»æ‰¾æœ€ç®€å•çš„é€”å¾„å®ç°æµ‹è¯•æ—¶ç¼©æ”¾å’Œå¼ºå¤§çš„æ¨ç†æ€§èƒ½ã€‚ä»–ä»¬çš„æ¨¡å‹s1åœ¨æ•°å­¦ç«èµ›é—®é¢˜ä¸Šè¶…è¶Šäº†o1-previewï¼Œå¹¶ä¸”é€šè¿‡é¢„ç®—å¼ºåˆ¶è¿›ä¸€æ­¥æå‡äº†æ€§èƒ½ã€‚æ¨¡å‹ã€æ•°æ®å’Œä»£ç éƒ½æ˜¯å¼€æºçš„ã€‚",
        "title": "s1: Simple test-time scaling",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„è¯­è¨€å»ºæ¨¡æ–¹æ³•ï¼Œç§°ä¸ºæµ‹è¯•æ—¶ç¼©æ”¾ã€‚OpenAIçš„o1æ¨¡å‹å±•ç¤ºäº†è¿™ç§èƒ½åŠ›ï¼Œä½†æ²¡æœ‰å…¬å¼€å…¶æ–¹æ³•ï¼Œå¯¼è‡´è®¸å¤šå¤åˆ¶åŠªåŠ›ã€‚ä½œè€…é€šè¿‡æ•´ç†ä¸€ä¸ªå°æ•°æ®é›†s1Kå’Œå¼€å‘é¢„ç®—å¼ºåˆ¶æ–¹æ³•ï¼Œå¯»æ‰¾æœ€ç®€å•çš„é€”å¾„å®ç°æµ‹è¯•æ—¶ç¼©æ”¾å’Œå¼ºå¤§çš„æ¨ç†æ€§èƒ½ã€‚ä»–ä»¬çš„æ¨¡å‹s1åœ¨æ•°å­¦ç«èµ›é—®é¢˜ä¸Šè¶…è¶Šäº†o1-previewï¼Œå¹¶ä¸”é€šè¿‡é¢„ç®—å¼ºåˆ¶è¿›ä¸€æ­¥æå‡äº†æ€§èƒ½ã€‚æ¨¡å‹ã€æ•°æ®å’Œä»£ç éƒ½æ˜¯å¼€æºçš„ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ«zhÇ’ng xÄ«n de yÇ”yÃ¡n jiÃ nmÃ³ fÄngfÇ, chÄ“ngwÃ©i cÃ¨shÃ¬ shÃ­ suÅfÃ ng. OpenAI de o1 mÃ³xÃ­ng zhÇnshÃ¬ le zhÃ¨ zhÇ’ng nÃ©nglÃ¬, dÃ n mÃ©iyÇ’u gÅngkÄi qÃ­ fÄngfÇ, dÇozhÃ¬ xÇ”duÅ fÃ¹zhÃ¬ nÇ”lÃ¬. ZuÃ²zhÄ› tÅngguÃ² zhÄ›nglÇ yÄ«gÃ¨ xiÇo shÃ¹jÃ¹jÃ­ s1K hÃ© kÄifÄ yÃ¹suÃ n qiÃ¡ngzhÃ¬ fÄngfÇ, xÃºnzhÇo zuÃ¬ jiÇndÄn de tÃºjÃ¬ng shÃ­xiÃ n cÃ¨shÃ¬ shÃ­ suÅfÃ ng hÃ© qiÃ¡ngdÃ  de tuÄ«lÇ xÃ¬ngnÃ©ng. TÄmen de mÃ³xÃ­ng s1 zÃ i shÃ¹xuÃ© jÃ¬ngsÃ i wÃ¨ntÃ­ shÃ ng chÄoyuÃ¨ le o1-preview, bÃ¬ngqiÄ› tÅngguÃ² yÃ¹suÃ n qiÃ¡ngzhÃ¬ jÃ¬nfÄ tÄ«shÄ“ng le xÃ¬ngnÃ©ng. MÃ³xÃ­ng, shÃ¹jÃ¹ hÃ© dÃ imÇ dÅu shÃ¬ kÄiyuÃ¡n de.",
        "vocab": "[\n{'word': 'å»ºæ¨¡', 'pinyin': 'jiÃ n mÃ³', 'trans': 'modeling'},\n{'word': 'ç§°ä¸º', 'pinyin': 'chÄ“ng wÃ©i', 'trans': 'called'},\n{'word': 'ç¼©æ”¾', 'pinyin': 'suÅ fÃ ng', 'trans': 'scaling'},\n{'word': 'å±•ç¤º', 'pinyin': 'zhÇn shÃ¬', 'trans': 'demonstrate'},\n{'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©ng lÃ¬', 'trans': 'capability'},\n{'word': 'å…¬å¼€', 'pinyin': 'gÅng kÄi', 'trans': 'public'},\n{'word': 'å¤åˆ¶', 'pinyin': 'fÃ¹ zhÃ¬', 'trans': 'replicate'},\n{'word': 'åŠªåŠ›', 'pinyin': 'nÇ” lÃ¬', 'trans': 'efforts'},\n{'word': 'æ•´ç†', 'pinyin': 'zhÄ›ng lÇ', 'trans': 'organize'},\n{'word': 'é¢„ç®—', 'pinyin': 'yÃ¹ suÃ n', 'trans': 'budget'},\n{'word': 'å¼ºåˆ¶', 'pinyin': 'qiÃ¡ng zhÃ¬', 'trans': 'enforcement'},\n{'word': 'é€”å¾„', 'pinyin': 'tÃº jÃ¬ng', 'trans': 'means'},\n{'word': 'å®ç°', 'pinyin': 'shÃ­ xiÃ n', 'trans': 'achieve'},\n{'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'reasoning'},\n{'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ng nÃ©ng', 'trans': 'performance'},\n{'word': 'è¶…è¶Š', 'pinyin': 'chÄo yuÃ¨', 'trans': 'surpass'},\n{'word': 'å¼€æº', 'pinyin': 'kÄi yuÃ¡n', 'trans': 'open-source'}\n]",
        "trans": "This article introduces a new language modeling method called test-time scaling. The o1 model from OpenAI demonstrated this capability, but its method was not made public, leading to many attempts to replicate it. The authors, by curating a small dataset s1K and developing a budget enforcement method, sought the simplest way to achieve test-time scaling and powerful reasoning performance. Their model, s1, outperformed o1-preview on mathematical competition problems and further enhanced performance through budget enforcement. The model, data, and code are all open-sourced.",
        "update_ts": "2025-02-03 09:11"
    }
}