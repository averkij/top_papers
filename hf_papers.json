{
    "date": {
        "ru": "19 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 19",
        "zh": "2æœˆ19æ—¥"
    },
    "time_utc": "2026-02-19 08:35",
    "weekday": 3,
    "issue_id": 1128,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2602.12675",
            "title": "SLA2: Sparse-Linear Attention with Learnable Routing and QAT",
            "url": "https://huggingface.co/papers/2602.12675",
            "abstract": "SLA2 improves sparse-linear attention in diffusion models by introducing a learnable router, direct attention formulation, and quantization-aware fine-tuning for enhanced efficiency and quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Sparse-Linear Attention (SLA) combines sparse and linear attention to accelerate diffusion models and has shown strong performance in video generation. However, (i) SLA relies on a heuristic split that assigns computations to the sparse or linear branch based on attention-weight magnitude, which can be suboptimal. Additionally, (ii) after formally analyzing the attention error in SLA, we identify a mismatch between SLA and a direct decomposition into sparse and linear attention. We propose SLA2, which introduces (I) a learnable router that dynamically selects whether each attention computation should use sparse or linear attention, (II) a more faithful and direct sparse-linear attention formulation that uses a learnable ratio to combine the sparse and linear attention branches, and (III) a sparse + low-bit attention design, where low-bit attention is introduced via quantization-aware fine-tuning to reduce quantization error. Experiments show that on video diffusion models, SLA2 can achieve 97% attention sparsity and deliver an 18.6x attention speedup while preserving generation quality.",
            "score": 26,
            "issue_id": 1124,
            "pub_date": "2026-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "f6320d81908edb47",
            "authors": [
                "Jintao Zhang",
                "Haoxu Wang",
                "Kai Jiang",
                "Kaiwen Zheng",
                "Youhe Jiang",
                "Ion Stoica",
                "Jianfei Chen",
                "Jun Zhu",
                "Joseph E. Gonzalez"
            ],
            "affiliations": [
                "Tsinghua University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.12675.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#diffusion",
                    "#inference",
                    "#training",
                    "#architecture"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ SLA2, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾-Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°, Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¸Ğ¿ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾-Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ğ²ĞµÑ€Ğ½ÑƒÑ Ğ¸ Ğ¿Ñ€ÑĞ¼ÑƒÑ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¼ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ¼ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ° Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SLA2 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 97% Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² 18.6 Ñ€Ğ°Ğ· Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "SLA2: Smart Attention for Faster Video Generation",
                    "desc": "The paper presents SLA2, an enhancement to Sparse-Linear Attention (SLA) in diffusion models, aimed at improving efficiency and quality in video generation. It introduces a learnable router that optimally decides between sparse and linear attention computations, addressing the limitations of the previous heuristic approach. Additionally, SLA2 offers a more accurate formulation of sparse-linear attention and incorporates quantization-aware fine-tuning to minimize quantization errors. Experimental results demonstrate that SLA2 achieves 97% attention sparsity and an 18.6x speedup in attention processing while maintaining high-quality video generation."
                },
                "zh": {
                    "title": "SLA2ï¼šæå‡æ‰©æ•£æ¨¡å‹çš„ç¨€ç–çº¿æ€§æ³¨æ„åŠ›",
                    "desc": "SLA2é€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„è·¯ç”±å™¨ã€ç›´æ¥çš„æ³¨æ„åŠ›å…¬å¼å’Œé‡åŒ–æ„ŸçŸ¥å¾®è°ƒï¼Œæå‡äº†æ‰©æ•£æ¨¡å‹ä¸­çš„ç¨€ç–çº¿æ€§æ³¨æ„åŠ›ã€‚å®ƒè§£å†³äº†SLAä¸­åŸºäºæ³¨æ„åŠ›æƒé‡å¤§å°çš„å¯å‘å¼åˆ†å‰²é—®é¢˜ï¼Œä¼˜åŒ–äº†è®¡ç®—åˆ†é…ã€‚SLA2çš„è®¾è®¡ä½¿å¾—æ¯ä¸ªæ³¨æ„åŠ›è®¡ç®—å¯ä»¥åŠ¨æ€é€‰æ‹©ä½¿ç”¨ç¨€ç–æˆ–çº¿æ€§æ³¨æ„åŠ›ï¼Œä»è€Œæé«˜äº†æ•ˆç‡å’Œè´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒSLA2åœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­å®ç°äº†97%çš„æ³¨æ„åŠ›ç¨€ç–æ€§ï¼Œå¹¶åœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶å®ç°äº†18.6å€çš„æ³¨æ„åŠ›åŠ é€Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.16705",
            "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation",
            "url": "https://huggingface.co/papers/2602.16705",
            "abstract": "HERO enables humanoid robots to perform object manipulation in diverse real-world environments by combining accurate end-effector control with open-vocabulary vision models for generalizable scene understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.",
            "score": 25,
            "issue_id": 1124,
            "pub_date": "2026-02-18",
            "pub_date_card": {
                "ru": "18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 18",
                "zh": "2æœˆ18æ—¥"
            },
            "hash": "0e35d95df77fa67c",
            "authors": [
                "Runpei Dong",
                "Ziyan Li",
                "Xialin He",
                "Saurabh Gupta"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.16705.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#robotics",
                    "#training"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞšĞ¾Ğ³Ğ´Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ Ğ»Ğ¾Ğ²ĞºĞ¸Ğ¼: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ + Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹",
                    "desc": "HERO â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ¼ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½ÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºÑƒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿ĞµÑ€ĞµĞ¿Ğ»Ğ°Ğ½Ğ°. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ñ€Ğ° Ğ² 3,2 Ñ€Ğ°Ğ·Ğ° Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ñƒ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ±Ñ‹Ñ‚Ğ° Ğ½Ğ° Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ÑÑ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ²Ñ‹ÑĞ¾Ñ‚Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ â€” Ğ¾Ñ‚ Ğ¾Ñ„Ğ¸ÑĞ¾Ğ² Ğ´Ğ¾ ĞºĞ¾Ñ„ĞµĞµĞ½ â€” Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "HERO: Empowering Humanoid Robots for Real-World Object Manipulation",
                    "desc": "The HERO framework enables humanoid robots to manipulate objects in various real-world settings by integrating precise end-effector control with advanced visual understanding. It addresses the limitations of traditional imitation learning by utilizing large vision models that generalize well across different scenes. The system employs a sophisticated end-effector tracking policy that combines classical robotics techniques with machine learning, significantly reducing tracking errors. By leveraging this approach, HERO allows robots to effectively interact with everyday objects in diverse environments, enhancing their practical utility."
                },
                "zh": {
                    "title": "HEROï¼šç±»äººæœºå™¨äººæ“æ§æ–°çºªå…ƒ",
                    "desc": "HEROæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œä½¿ç±»äººæœºå™¨äººèƒ½å¤Ÿåœ¨å„ç§çœŸå®ç¯å¢ƒä¸­è¿›è¡Œç‰©ä½“æ“æ§ã€‚å®ƒç»“åˆäº†ç²¾ç¡®çš„æœ«ç«¯æ‰§è¡Œå™¨æ§åˆ¶å’Œå¼€æ”¾è¯æ±‡çš„è§†è§‰æ¨¡å‹ï¼Œä»¥å®ç°æ›´å¥½çš„åœºæ™¯ç†è§£ã€‚é€šè¿‡è®¾è®¡ä¸€ç§å‡†ç¡®çš„æ®‹å·®æ„ŸçŸ¥æœ«ç«¯æ‰§è¡Œå™¨è·Ÿè¸ªç­–ç•¥ï¼ŒHEROæ˜¾è‘—æé«˜äº†æ“æ§æ€§èƒ½ï¼Œå¹¶å‡å°‘äº†è·Ÿè¸ªè¯¯å·®ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿåœ¨ä¸åŒçš„ç¯å¢ƒä¸­å¯é åœ°æ“æ§æ—¥å¸¸ç‰©å“ï¼Œå±•ç¤ºäº†å…¶åœ¨ç±»äººæœºå™¨äººè®­ç»ƒä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.14979",
            "title": "RynnBrain: Open Embodied Foundation Models",
            "url": "https://huggingface.co/papers/2602.14979",
            "abstract": "RynnBrain is an open-source spatiotemporal foundation model for embodied intelligence that unifies perception, reasoning, and planning capabilities across multiple scales and task-specific variants.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid progress in multimodal foundation models, embodied intelligence community still lacks a unified, physically grounded foundation model that integrates perception, reasoning, and planning within real-world spatial-temporal dynamics. We introduce RynnBrain, an open-source spatiotemporal foundation model for embodied intelligence. RynnBrain strengthens four core capabilities in a unified framework: comprehensive egocentric understanding, diverse spatiotemporal localization, physically grounded reasoning, and physics-aware planning. The RynnBrain family comprises three foundation model scales (2B, 8B, and 30B-A3B MoE) and four post-trained variants tailored for downstream embodied tasks (i.e., RynnBrain-Nav, RynnBrain-Plan, and RynnBrain-VLA) or complex spatial reasoning tasks (i.e., RynnBrain-CoP). In terms of extensive evaluations on 20 embodied benchmarks and 8 general vision understanding benchmarks, our RynnBrain foundation models largely outperform existing embodied foundation models by a significant margin. The post-trained model suite further substantiates two key potentials of the RynnBrain foundation model: (i) enabling physically grounded reasoning and planning, and (ii) serving as a strong pretrained backbone that can be efficiently adapted to diverse embodied tasks.",
            "score": 11,
            "issue_id": 1125,
            "pub_date": "2026-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "aa2297877faaba9b",
            "authors": [
                "Ronghao Dang",
                "Jiayan Guo",
                "Bohan Hou",
                "Sicong Leng",
                "Kehan Li",
                "Xin Li",
                "Jiangpin Liu",
                "Yunxuan Mao",
                "Zhikai Wang",
                "Yuqian Yuan",
                "Minghao Zhu",
                "Xiao Lin",
                "Yang Bai",
                "Qian Jiang",
                "Yaxi Zhao",
                "Minghua Zeng",
                "Junlong Gao",
                "Yuming Jiang",
                "Jun Cen",
                "Siteng Huang",
                "Liuyi Wang",
                "Wenqiao Zhang",
                "Chengju Liu",
                "Jianfei Yang",
                "Shijian Lu",
                "Deli Zhao"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.14979.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#robotics",
                    "#open_source",
                    "#transfer_learning",
                    "#multimodal",
                    "#reasoning",
                    "#benchmark",
                    "#training",
                    "#cv"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°",
                    "desc": "RynnBrain â€” ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€Ñ‘Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ² (Ğ¾Ñ‚ 2B Ğ´Ğ¾ 30B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ MoE) Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. RynnBrain Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸: ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ° 20 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼."
                },
                "en": {
                    "title": "RynnBrain: Unifying Perception, Reasoning, and Planning for Embodied Intelligence",
                    "desc": "RynnBrain is an innovative open-source model designed for embodied intelligence, which combines perception, reasoning, and planning in a cohesive framework. It addresses the need for a unified model that operates effectively within real-world spatial and temporal contexts. The model comes in various scales and specialized variants, enhancing its ability to perform tasks like navigation and complex spatial reasoning. Extensive evaluations show that RynnBrain significantly outperforms existing models, demonstrating its effectiveness in physically grounded reasoning and adaptability to various tasks."
                },
                "zh": {
                    "title": "RynnBrainï¼šç»Ÿä¸€æ„ŸçŸ¥ä¸æ¨ç†çš„æ—¶ç©ºåŸºç¡€æ¨¡å‹",
                    "desc": "RynnBrainæ˜¯ä¸€ä¸ªå¼€æºçš„æ—¶ç©ºåŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨ç»Ÿä¸€æ„ŸçŸ¥ã€æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ï¼Œé€‚ç”¨äºå¤šç§ä»»åŠ¡å’Œè§„æ¨¡ã€‚è¯¥æ¨¡å‹å¢å¼ºäº†å››ä¸ªæ ¸å¿ƒèƒ½åŠ›ï¼šå…¨é¢çš„è‡ªæˆ‘ä¸­å¿ƒç†è§£ã€å¤šæ ·çš„æ—¶ç©ºå®šä½ã€åŸºäºç‰©ç†çš„æ¨ç†å’Œç‰©ç†æ„ŸçŸ¥çš„è§„åˆ’ã€‚RynnBrainå®¶æ—åŒ…æ‹¬ä¸‰ç§åŸºç¡€æ¨¡å‹è§„æ¨¡å’Œå››ç§åè®­ç»ƒå˜ä½“ï¼Œä¸“é—¨é’ˆå¯¹ä¸‹æ¸¸çš„å…·ä½“ä»»åŠ¡å’Œå¤æ‚çš„ç©ºé—´æ¨ç†ä»»åŠ¡ã€‚ç»è¿‡åœ¨20ä¸ªå…·ä½“åŸºå‡†å’Œ8ä¸ªé€šç”¨è§†è§‰ç†è§£åŸºå‡†ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼ŒRynnBrainæ¨¡å‹åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºç¡€æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.14080",
            "title": "Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality",
            "url": "https://huggingface.co/papers/2602.14080",
            "abstract": "LLMs demonstrate near-complete factual encoding but struggle with retrieval accessibility, where errors stem from access limitations rather than knowledge gaps, with reasoning improving recall of encoded information.  \t\t\t\t\tAI-generated summary \t\t\t\t Standard factuality evaluations of LLMs treat all errors alike, obscuring whether failures arise from missing knowledge (empty shelves) or from limited access to encoded facts (lost keys). We propose a behavioral framework that profiles factual knowledge at the level of facts rather than questions, characterizing each fact by whether it is encoded, and then by how accessible it is: cannot be recalled, can be directly recalled, or can only be recalled with inference-time computation (thinking). To support such profiling, we introduce WikiProfile, a new benchmark constructed via an automated pipeline with a prompted LLM grounded in web search. Across 4 million responses from 13 LLMs, we find that encoding is nearly saturated in frontier models on our benchmark, with GPT-5 and Gemini-3 encoding 95--98% of facts. However, recall remains a major bottleneck: many errors previously attributed to missing knowledge instead stem from failures to access it. These failures are systematic and disproportionately affect long-tail facts and reverse questions. Finally, we show that thinking improves recall and can recover a substantial fraction of failures, indicating that future gains may rely less on scaling and more on methods that improve how models utilize what they already encode.",
            "score": 9,
            "issue_id": 1125,
            "pub_date": "2026-02-15",
            "pub_date_card": {
                "ru": "15 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 15",
                "zh": "2æœˆ15æ—¥"
            },
            "hash": "eea14212c16b0f99",
            "authors": [
                "Nitay Calderon",
                "Eyal Ben-David",
                "Zorik Gekhman",
                "Eran Ofek",
                "Gal Yona"
            ],
            "affiliations": [
                "Google Research",
                "Technion Israel Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.14080.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#rag",
                    "#hallucinations",
                    "#dataset",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ”‘",
                "ru": {
                    "title": "Ğ—Ğ½Ğ°Ğ½Ğ¸Ñ ĞµÑÑ‚ÑŒ, Ğ½Ğ¾ ĞºĞ»ÑÑ‡Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑĞ½Ñ‹: ĞºĞ°Ğº LLM ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ñ„Ğ°ĞºÑ‚Ñ‹, Ğ½Ğ¾ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸Ñ… Ğ²ÑĞ¿Ğ¾Ğ¼Ğ½Ğ¸Ñ‚ÑŒ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ²ÑĞµ Ñ„Ğ°ĞºÑ‚Ñ‹ (95-98%), Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ¸Ñ… Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ· Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° Ñ‚Ğ¸Ğ¿Ğ° Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº: Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ğ·Ğ°ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ WikiProfile â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¾Ñ‚ 13 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ LLM Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ’Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (reasoning) Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒĞ¶Ğµ Ğ·Ğ°ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Unlocking Knowledge: Enhancing Recall in Language Models",
                    "desc": "This paper investigates how large language models (LLMs) store and retrieve factual information. It distinguishes between errors caused by missing knowledge and those due to limited access to encoded facts. The authors introduce a new framework, WikiProfile, to evaluate the accessibility of facts in LLMs, revealing that while encoding is high, recall remains a significant challenge. They find that reasoning can enhance recall, suggesting that improving access to existing knowledge may be more beneficial than simply increasing model size."
                },
                "zh": {
                    "title": "æå‡æ¨¡å‹å›å¿†èƒ½åŠ›ï¼Œè¶…è¶ŠçŸ¥è¯†ç¼–ç çš„ç“¶é¢ˆ",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äº‹å®ç¼–ç æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¿¡æ¯æ£€ç´¢æ—¶å­˜åœ¨å›°éš¾ï¼Œé”™è¯¯ä¸»è¦æºäºè®¿é—®é™åˆ¶è€ŒéçŸ¥è¯†ç¼ºå¤±ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è¡Œä¸ºæ¡†æ¶ï¼Œé€šè¿‡å¯¹äº‹å®çš„ç¼–ç å’Œå¯è®¿é—®æ€§è¿›è¡Œåˆ†æï¼Œæ¥è¯„ä¼°æ¨¡å‹çš„äº‹å®çŸ¥è¯†ã€‚æˆ‘ä»¬å¼•å…¥äº†WikiProfileåŸºå‡†ï¼Œåˆ©ç”¨è‡ªåŠ¨åŒ–æµç¨‹å’ŒåŸºäºç½‘ç»œæœç´¢çš„æç¤ºLLMæ„å»ºã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡å‰æ²¿æ¨¡å‹çš„ç¼–ç èƒ½åŠ›æ¥è¿‘é¥±å’Œï¼Œä½†å›å¿†èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦ç“¶é¢ˆï¼Œæ€è€ƒè¿‡ç¨‹å¯ä»¥æ˜¾è‘—æé«˜å›å¿†ç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.16301",
            "title": "Multi-agent cooperation through in-context co-player inference",
            "url": "https://huggingface.co/papers/2602.16301",
            "abstract": "Sequence models enable cooperative behavior emergence in multi-agent reinforcement learning through in-context learning without hardcoded assumptions or timescale separation.  \t\t\t\t\tAI-generated summary \t\t\t\t Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between \"learning-aware\" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between \"naive learners\" updating on fast timescales and \"meta-learners\" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors.",
            "score": 5,
            "issue_id": 1124,
            "pub_date": "2026-02-18",
            "pub_date_card": {
                "ru": "18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 18",
                "zh": "2æœˆ18æ—¥"
            },
            "hash": "130aa1d5a2fd94b8",
            "authors": [
                "Marissa A. Weis",
                "Maciej WoÅ‚czyk",
                "Rajai Nasser",
                "Rif A. Saurous",
                "Blaise AgÃ¼era y Arcas",
                "JoÃ£o Sacramento",
                "Alexander Meulemans"
            ],
            "affiliations": [
                "Google, Paradigms of Intelligence Team",
                "Santa Fe Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.16301.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#agents",
                    "#games",
                    "#reasoning",
                    "#architecture"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "ĞšĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ (Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹) Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (MARL) Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ±ĞµĞ· Ğ¶Ñ‘ÑÑ‚ĞºĞ¾ Ğ·Ğ°ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¿ĞµÑ€Ğ½Ğ¸ĞºĞ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼: Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ½Ğ¸ĞºĞ¾Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¿ĞµÑ€Ğ½Ğ¸ĞºĞ° Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ°. Ğ£ÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğº ÑˆĞ°Ğ½Ñ‚Ğ°Ğ¶Ñƒ, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ°Ñ Ğ¸Ğ· ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğµ Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑĞ¿Ğ¾Ğ½Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ»ÑƒÑ‡ÑˆĞµ, Ñ‡ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞºĞ°Ğ» Ğ¼ĞµĞ¶Ğ´Ñƒ Â«Ğ½Ğ°Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸Â» Ğ¸ Â«Ğ¼ĞµÑ‚Ğ°-Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑÂ» Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Harnessing Sequence Models for Cooperative Learning in Multi-Agent Systems",
                    "desc": "This paper explores how sequence models can facilitate cooperation among self-interested agents in multi-agent reinforcement learning (MARL) without relying on fixed assumptions or strict timescale separations. It highlights that agents equipped with in-context learning capabilities can adapt their strategies based on the learning dynamics of their co-players. The study shows that training these sequence model agents against a variety of opponents leads to the emergence of cooperative behaviors through mutual adaptation. Ultimately, the findings suggest that using sequence models in decentralized reinforcement learning can effectively promote cooperation among agents in diverse environments."
                },
                "zh": {
                    "title": "åºåˆ—æ¨¡å‹åŠ©åŠ›å¤šæ™ºèƒ½ä½“åˆä½œè¡Œä¸ºçš„å­¦ä¹ ",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åºåˆ—æ¨¡å‹åœ¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­å¦‚ä½•ä¿ƒè¿›åˆä½œè¡Œä¸ºçš„å‡ºç°ã€‚é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰ç¡¬ç¼–ç å‡è®¾æˆ–æ—¶é—´å°ºåº¦åˆ†ç¦»çš„æƒ…å†µä¸‹ï¼Œå®ç°æ™ºèƒ½ä½“ä¹‹é—´çš„å­¦ä¹ æ„è¯†ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåºåˆ—æ¨¡å‹æ™ºèƒ½ä½“åœ¨é¢å¯¹å¤šæ ·åŒ–çš„åˆä½œä¼™ä¼´æ—¶ï¼Œèƒ½å¤Ÿè‡ªç„¶åœ°å½¢æˆæœ€ä½³å“åº”ç­–ç•¥ï¼Œä»è€Œæœ‰æ•ˆåœ°å­¦ä¹ åˆä½œè¡Œä¸ºã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç»“åˆåºåˆ—æ¨¡å‹å’Œåˆä½œä¼™ä¼´å¤šæ ·æ€§ï¼Œå¯ä»¥ä¸ºå­¦ä¹ åˆä½œè¡Œä¸ºæä¾›å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.15922",
            "title": "World Action Models are Zero-shot Policies",
            "url": "https://huggingface.co/papers/2602.15922",
            "abstract": "DreamZero is a World Action Model that leverages video diffusion to enable better generalization of physical motions across novel environments and embodiments compared to vision-language-action models.  \t\t\t\t\tAI-generated summary \t\t\t\t State-of-the-art Vision-Language-Action (VLA) models excel at semantic generalization but struggle to generalize to unseen physical motions in novel environments. We introduce DreamZero, a World Action Model (WAM) built upon a pretrained video diffusion backbone. Unlike VLAs, WAMs learn physical dynamics by predicting future world states and actions, using video as a dense representation of how the world evolves. By jointly modeling video and action, DreamZero learns diverse skills effectively from heterogeneous robot data without relying on repetitive demonstrations. This results in over 2x improvement in generalization to new tasks and environments compared to state-of-the-art VLAs in real robot experiments. Crucially, through model and system optimizations, we enable a 14B autoregressive video diffusion model to perform real-time closed-loop control at 7Hz. Finally, we demonstrate two forms of cross-embodiment transfer: video-only demonstrations from other robots or humans yield a relative improvement of over 42% on unseen task performance with just 10-20 minutes of data. More surprisingly, DreamZero enables few-shot embodiment adaptation, transferring to a new embodiment with only 30 minutes of play data while retaining zero-shot generalization.",
            "score": 5,
            "issue_id": 1124,
            "pub_date": "2026-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "7eb519162f3a2c98",
            "authors": [
                "Seonghyeon Ye",
                "Yunhao Ge",
                "Kaiyuan Zheng",
                "Shenyuan Gao",
                "Sihyun Yu",
                "George Kurian",
                "Suneel Indupuru",
                "You Liang Tan",
                "Chuning Zhu",
                "Jiannan Xiang",
                "Ayaan Malik",
                "Kyungmin Lee",
                "William Liang",
                "Nadun Ranawaka",
                "Jiasheng Gu",
                "Yinzhen Xu",
                "Guanzhi Wang",
                "Fengyuan Hu",
                "Avnish Narayan",
                "Johan Bjorck",
                "Jing Wang",
                "Gwanghyun Kim",
                "Dantong Niu",
                "Ruijie Zheng",
                "Yuqi Xie",
                "Jimmy Wu",
                "Qi Wang",
                "Ryan Julian",
                "Danfei Xu",
                "Yilun Du",
                "Yevgen Chebotar",
                "Scott Reed",
                "Jan Kautz",
                "Yuke Zhu",
                "Linxi \"Jim\" Fan",
                "Joel Jang"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.15922.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#diffusion",
                    "#multimodal",
                    "#training",
                    "#robotics",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ° Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸: Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼",
                    "desc": "DreamZero â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ (World Action Model), Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… vision-language-action Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, DreamZero Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ´Ğ²ÑƒĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ VLA Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, DreamZero Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğº Ğ½Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ñ‚ĞµĞ»Ñƒ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "DreamZero: Revolutionizing Motion Generalization with Video Diffusion",
                    "desc": "DreamZero is a World Action Model that enhances the ability to generalize physical motions in new environments by utilizing video diffusion techniques. Unlike traditional Vision-Language-Action models, which focus on semantic understanding, DreamZero predicts future states and actions based on video data, allowing it to learn physical dynamics more effectively. This approach leads to significant improvements in task performance, achieving over twice the generalization capability compared to existing models in real robot applications. Additionally, DreamZero supports quick adaptation to new robot embodiments with minimal data, showcasing its versatility and efficiency in learning diverse skills."
                },
                "zh": {
                    "title": "DreamZeroï¼šæå‡ç‰©ç†åŠ¨ä½œæ³›åŒ–çš„åˆ›æ–°æ¨¡å‹",
                    "desc": "DreamZeroæ˜¯ä¸€ç§ä¸–ç•ŒåŠ¨ä½œæ¨¡å‹ï¼Œåˆ©ç”¨è§†é¢‘æ‰©æ•£æŠ€æœ¯æ¥æé«˜åœ¨æ–°ç¯å¢ƒå’Œæ–°å½¢æ€ä¸‹çš„ç‰©ç†åŠ¨ä½œæ³›åŒ–èƒ½åŠ›ã€‚ä¸è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä¸åŒï¼ŒDreamZeroé€šè¿‡é¢„æµ‹æœªæ¥çš„ä¸–ç•ŒçŠ¶æ€å’ŒåŠ¨ä½œæ¥å­¦ä¹ ç‰©ç†åŠ¨æ€ï¼Œä½¿ç”¨è§†é¢‘ä½œä¸ºä¸–ç•Œæ¼”å˜çš„å¯†é›†è¡¨ç¤ºã€‚å®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»å¼‚æ„æœºå™¨äººæ•°æ®ä¸­å­¦ä¹ å¤šæ ·åŒ–çš„æŠ€èƒ½ï¼Œè€Œæ— éœ€ä¾èµ–é‡å¤çš„æ¼”ç¤ºã€‚é€šè¿‡æ¨¡å‹å’Œç³»ç»Ÿä¼˜åŒ–ï¼ŒDreamZeroå®ç°äº†å®æ—¶é—­ç¯æ§åˆ¶ï¼Œå¹¶åœ¨æ–°ä»»åŠ¡å’Œç¯å¢ƒçš„æ³›åŒ–èƒ½åŠ›ä¸Šæ¯”ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹æé«˜äº†ä¸¤å€ä»¥ä¸Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.16666",
            "title": "Towards a Science of AI Agent Reliability",
            "url": "https://huggingface.co/papers/2602.16666",
            "abstract": "Traditional benchmark evaluations of AI agents fail to capture critical reliability issues, prompting the development of comprehensive metrics that assess consistency, robustness, predictability, and safety across multiple dimensions.  \t\t\t\t\tAI-generated summary \t\t\t\t AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.",
            "score": 4,
            "issue_id": 1124,
            "pub_date": "2026-02-18",
            "pub_date_card": {
                "ru": "18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 18",
                "zh": "2æœˆ18æ—¥"
            },
            "hash": "02fc6538158301f6",
            "authors": [
                "Stephan Rabanser",
                "Sayash Kapoor",
                "Peter Kirgis",
                "Kangheng Liu",
                "Saiteja Utpala",
                "Arvind Narayanan"
            ],
            "affiliations": [
                "Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.16666.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "âš™ï¸",
                "ru": {
                    "title": "ĞĞ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸: ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºÑƒĞµÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ½Ñƒ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ ÑƒÑĞ¿ĞµÑ…Ğ° Ğ¸ Ğ½Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ´Ğ²ĞµĞ½Ğ°Ğ´Ñ†Ğ°Ñ‚ÑŒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ğ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼: ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ. ĞŸÑ€Ğ¾Ğ²ĞµĞ´Ñ‘Ğ½Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 14 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğµ ÑĞ¾Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ, ĞºĞ°Ğº Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸ Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ AI Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Beyond Accuracy: A New Framework for AI Reliability Assessment",
                    "desc": "This paper addresses the shortcomings of traditional AI evaluation methods that often overlook important reliability aspects of AI agents. It introduces twelve new metrics that assess AI performance based on four dimensions: consistency, robustness, predictability, and safety. The authors demonstrate that despite improvements in accuracy on standard benchmarks, many AI agents still exhibit significant reliability issues in real-world applications. By providing a more comprehensive evaluation framework, this work aims to enhance our understanding of AI agent behavior and identify potential failure points."
                },
                "zh": {
                    "title": "å…¨é¢è¯„ä¼°AIä»£ç†çš„å¯é æ€§",
                    "desc": "ä¼ ç»Ÿçš„äººå·¥æ™ºèƒ½ä»£ç†åŸºå‡†è¯„ä¼°æ— æ³•æœ‰æ•ˆæ•æ‰å…³é”®çš„å¯é æ€§é—®é¢˜ï¼Œå› æ­¤éœ€è¦å¼€å‘å…¨é¢çš„æŒ‡æ ‡æ¥è¯„ä¼°ä¸€è‡´æ€§ã€é²æ£’æ€§ã€å¯é¢„æµ‹æ€§å’Œå®‰å…¨æ€§ç­‰å¤šä¸ªç»´åº¦ã€‚å°½ç®¡åœ¨æ ‡å‡†åŸºå‡†ä¸Šå‡†ç¡®ç‡ä¸æ–­æé«˜ï¼Œä½†è®¸å¤šä»£ç†åœ¨å®é™…åº”ç”¨ä¸­ä»ç„¶ä¼šå¤±è´¥ã€‚è¿™ç§å·®å¼‚çªæ˜¾äº†å½“å‰è¯„ä¼°çš„åŸºæœ¬å±€é™æ€§ï¼šå°†ä»£ç†è¡Œä¸ºå‹ç¼©ä¸ºå•ä¸€æˆåŠŸæŒ‡æ ‡æ©ç›–äº†å…³é”®çš„æ“ä½œç¼ºé™·ã€‚æˆ‘ä»¬æå‡ºäº†åäºŒä¸ªå…·ä½“æŒ‡æ ‡ï¼Œåˆ†è§£ä»£ç†çš„å¯é æ€§ï¼Œæä¾›äº†ä¸€ä¸ªå…¨é¢çš„æ€§èƒ½æ¦‚å†µã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.15989",
            "title": "SAM 3D Body: Robust Full-Body Human Mesh Recovery",
            "url": "https://huggingface.co/papers/2602.15989",
            "abstract": "A promptable 3D human mesh recovery model using a novel parametric representation and encoder-decoder architecture achieves state-of-the-art performance with strong generalization across diverse conditions.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SAM 3D Body (3DB), a promptable model for single-image full-body 3D human mesh recovery (HMR) that demonstrates state-of-the-art performance, with strong generalization and consistent accuracy in diverse in-the-wild conditions. 3DB estimates the human pose of the body, feet, and hands. It is the first model to use a new parametric mesh representation, Momentum Human Rig (MHR), which decouples skeletal structure and surface shape. 3DB employs an encoder-decoder architecture and supports auxiliary prompts, including 2D keypoints and masks, enabling user-guided inference similar to the SAM family of models. We derive high-quality annotations from a multi-stage annotation pipeline that uses various combinations of manual keypoint annotation, differentiable optimization, multi-view geometry, and dense keypoint detection. Our data engine efficiently selects and processes data to ensure data diversity, collecting unusual poses and rare imaging conditions. We present a new evaluation dataset organized by pose and appearance categories, enabling nuanced analysis of model behavior. Our experiments demonstrate superior generalization and substantial improvements over prior methods in both qualitative user preference studies and traditional quantitative analysis. Both 3DB and MHR are open-source.",
            "score": 4,
            "issue_id": 1124,
            "pub_date": "2026-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "57e16b08dec5e20e",
            "authors": [
                "Xitong Yang",
                "Devansh Kukreja",
                "Don Pinkus",
                "Anushka Sagar",
                "Taosha Fan",
                "Jinhyung Park",
                "Soyong Shin",
                "Jinkun Cao",
                "Jiawei Liu",
                "Nicolas Ugrinovic",
                "Matt Feiszli",
                "Jitendra Malik",
                "Piotr Dollar",
                "Kris Kitani"
            ],
            "affiliations": [
                "Meta Superintelligence Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.15989.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#benchmark",
                    "#open_source",
                    "#data",
                    "#architecture",
                    "#dataset"
                ],
                "emoji": "ğŸ§",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ SAM 3D Body (3DB) â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ ÑĞµÑ‚ĞºĞ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµĞ»Ğ° Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ñ‹Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¸ Ğ¼Ğ°ÑĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Momentum Human Rig (MHR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ ÑĞºĞµĞ»ĞµÑ‚Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºÑƒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… ÑÑŠÑ‘Ğ¼ĞºĞ¸ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº Ğ¿Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Revolutionizing 3D Human Mesh Recovery with SAM 3D Body",
                    "desc": "The paper presents SAM 3D Body (3DB), a cutting-edge model for recovering full-body 3D human meshes from single images. It introduces a novel parametric representation called Momentum Human Rig (MHR), which separates the skeletal structure from the surface shape, enhancing flexibility and accuracy. The model utilizes an encoder-decoder architecture and allows for user-guided inference through auxiliary prompts like 2D keypoints and masks. Extensive experiments show that 3DB outperforms previous methods, demonstrating strong generalization across various conditions and providing a new dataset for detailed evaluation."
                },
                "zh": {
                    "title": "çªç ´æ€§çš„3Däººç±»ç½‘æ ¼æ¢å¤æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSAM 3D Body (3DB) çš„æ¨¡å‹ï¼Œç”¨äºä»å•å¼ å›¾ç‰‡ä¸­æ¢å¤å…¨èº«3Däººç±»ç½‘æ ¼ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†ä¸€ç§æ–°çš„å‚æ•°åŒ–ç½‘æ ¼è¡¨ç¤ºæ–¹æ³•ï¼Œç§°ä¸ºMomentum Human Rig (MHR)ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ†ç¦»éª¨éª¼ç»“æ„å’Œè¡¨é¢å½¢çŠ¶ã€‚3DBä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå¹¶æ”¯æŒç”¨æˆ·å¼•å¯¼çš„æ¨ç†ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§è¾…åŠ©æç¤ºï¼Œå¦‚2Då…³é”®ç‚¹å’Œæ©ç ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ3DBåœ¨å¤šæ ·åŒ–æ¡ä»¶ä¸‹å…·æœ‰ä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›å’Œæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.16682",
            "title": "Learning Situated Awareness in the Real World",
            "url": "https://huggingface.co/papers/2602.16682",
            "abstract": "SAW-Bench presents a new benchmark for evaluating egocentric situated awareness in multimodal foundation models through real-world video datasets with human-annotated question-answer pairs, focusing on observer-centric spatial reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.",
            "score": 3,
            "issue_id": 1124,
            "pub_date": "2026-02-18",
            "pub_date_card": {
                "ru": "18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 18",
                "zh": "2æœˆ18æ—¥"
            },
            "hash": "6cd97cb7c306d71a",
            "authors": [
                "Chuhan Li",
                "Ruilin Han",
                "Joy Hsu",
                "Yongyuan Liang",
                "Rajiv Dhawan",
                "Jiajun Wu",
                "Ming-Hsuan Yang",
                "Xin Eric Wang"
            ],
            "affiliations": [
                "Amazon",
                "Stanford University",
                "University of California, Merced",
                "University of California, Santa Barbara",
                "University of Maryland, College Park",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.16682.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#survey",
                    "#benchmark",
                    "#multimodal",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "ğŸ‘“",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ»Ğ°Ğ·Ğ° Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°Ñ‚ĞµĞ»Ñ",
                    "desc": "SAW-Bench â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… foundation models Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°Ñ‚ĞµĞ»Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑÑ… Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ² ÑÑ†ĞµĞ½Ğµ, ÑÑ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ â€” ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¼Ğ¸Ñ€ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 786 Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ¾Ğ»Ğ¸ĞºĞ¾Ğ², ÑĞ½ÑÑ‚Ñ‹Ñ… Ğ½Ğ° ÑƒĞ¼Ğ½Ñ‹Ğµ Ğ¾Ñ‡ĞºĞ¸, Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 2000 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ÑˆĞµÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ (37.66%), Ğ²Ñ‹ÑĞ²Ğ¸Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‚ Ğ¸Ğ·-Ğ·Ğ° Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ ÑÑ†ĞµĞ½Ñ‹."
                },
                "en": {
                    "title": "Bridging the Gap in Egocentric Spatial Awareness",
                    "desc": "SAW-Bench is a new benchmark designed to assess egocentric situated awareness in multimodal foundation models (MFMs) using real-world video data. It focuses on observer-centric spatial reasoning, which considers how an observer's viewpoint and movement affect their understanding of the environment. The benchmark includes 786 videos and over 2,071 human-annotated question-answer pairs, targeting six specific awareness tasks. Our findings indicate a significant performance gap between human understanding and the best-performing models, highlighting challenges in coherent spatial reasoning from an egocentric perspective."
                },
                "zh": {
                    "title": "SAW-Benchï¼šæå‡è‡ªæˆ‘ä¸­å¿ƒæƒ…å¢ƒæ„è¯†çš„åŸºå‡†",
                    "desc": "SAW-Benchæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨è‡ªæˆ‘ä¸­å¿ƒçš„æƒ…å¢ƒæ„è¯†æ–¹é¢çš„è¡¨ç°ã€‚è¯¥åŸºå‡†é€šè¿‡çœŸå®ä¸–ç•Œçš„è§†é¢‘æ•°æ®é›†å’Œäººç±»æ ‡æ³¨çš„é—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œä¸“æ³¨äºè§‚å¯Ÿè€…ä¸­å¿ƒçš„ç©ºé—´æ¨ç†ä»»åŠ¡ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨ç†è§£è§‚å¯Ÿè€…è§†è§’å’Œè¿åŠ¨æ—¶å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†ç›¸æœºå‡ ä½•æ—¶å¸¸å¸¸å‡ºç°ç³»ç»Ÿæ€§é”™è¯¯ã€‚SAW-Benchæ—¨åœ¨æ¨åŠ¨æ¨¡å‹ä»è¢«åŠ¨è§‚å¯Ÿè½¬å‘ç†è§£ä¸ç‰©ç†ç¯å¢ƒç›¸å…³çš„åŠ¨æ€å…³ç³»ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.16493",
            "title": "MMA: Multimodal Memory Agent",
            "url": "https://huggingface.co/papers/2602.16493",
            "abstract": "Multimodal Memory Agent (MMA) improves long-horizon agent performance by dynamically scoring memory reliability and handling visual biases in retrieval-augmented systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-horizon multimodal agents depend on external memory; however, similarity-based retrieval often surfaces stale, low-credibility, or conflicting items, which can trigger overconfident errors. We propose Multimodal Memory Agent (MMA), which assigns each retrieved memory item a dynamic reliability score by combining source credibility, temporal decay, and conflict-aware network consensus, and uses this signal to reweight evidence and abstain when support is insufficient. We also introduce MMA-Bench, a programmatically generated benchmark for belief dynamics with controlled speaker reliability and structured text-vision contradictions. Using this framework, we uncover the \"Visual Placebo Effect\", revealing how RAG-based agents inherit latent visual biases from foundation models. On FEVER, MMA matches baseline accuracy while reducing variance by 35.2% and improving selective utility; on LoCoMo, a safety-oriented configuration improves actionable accuracy and reduces wrong answers; on MMA-Bench, MMA reaches 41.18% Type-B accuracy in Vision mode, while the baseline collapses to 0.0% under the same protocol. Code: https://github.com/AIGeeksGroup/MMA.",
            "score": 3,
            "issue_id": 1124,
            "pub_date": "2026-02-18",
            "pub_date_card": {
                "ru": "18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 18",
                "zh": "2æœˆ18æ—¥"
            },
            "hash": "8f25f959db00fa75",
            "authors": [
                "Yihao Lu",
                "Wanru Cheng",
                "Zeyu Zhang",
                "Hao Tang"
            ],
            "affiliations": [
                "School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.16493.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#rag",
                    "#agents",
                    "#security",
                    "#benchmark",
                    "#multimodal",
                    "#open_source",
                    "#long_context",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ°Ğ´ĞµĞ¶Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ±Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸ÑĞ¼Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ°Ğ³ĞµĞ½Ñ‚ Multimodal Memory Agent (MMA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ²: ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¿Ğ°Ğ´ Ğ¸ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑ ÑĞµÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ¸ Ğ²Ğ¾Ğ·Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ° Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Â«Visual Placebo EffectÂ» â€” ÑÑ„Ñ„ĞµĞºÑ‚, Ğ¿Ñ€Ğ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ RAG Ğ½Ğ°ÑĞ»ĞµĞ´ÑƒÑÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ MMA ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ½Ğ° 35.2% Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMA-Bench."
                },
                "en": {
                    "title": "Enhancing Memory Reliability in Multimodal Agents",
                    "desc": "The Multimodal Memory Agent (MMA) enhances the performance of long-horizon agents by effectively managing memory retrieval and addressing visual biases. It assigns dynamic reliability scores to memory items based on their source credibility, how old they are, and consensus among conflicting information. This approach allows the agent to weigh evidence appropriately and avoid making decisions when the support is inadequate. Additionally, the introduction of MMA-Bench provides a structured way to evaluate the agent's performance in scenarios with varying reliability and contradictions in visual data."
                },
                "zh": {
                    "title": "æå‡é•¿æ—¶é—´è·¨åº¦ä»£ç†æ€§èƒ½çš„å¤šæ¨¡æ€è®°å¿†ä»£ç†",
                    "desc": "å¤šæ¨¡æ€è®°å¿†ä»£ç†ï¼ˆMMAï¼‰é€šè¿‡åŠ¨æ€è¯„åˆ†è®°å¿†çš„å¯é æ€§ï¼Œæå‡äº†é•¿æ—¶é—´è·¨åº¦ä»£ç†çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•è§£å†³äº†åœ¨æ£€ç´¢å¢å¼ºç³»ç»Ÿä¸­ï¼ŒåŸºäºç›¸ä¼¼æ€§çš„æ£€ç´¢å¯èƒ½å¯¼è‡´è¿‡æ—¶ã€ä½å¯ä¿¡åº¦æˆ–å†²çªçš„ä¿¡æ¯ï¼Œä»è€Œå¼•å‘è¿‡åº¦è‡ªä¿¡çš„é”™è¯¯ã€‚MMAä¸ºæ¯ä¸ªæ£€ç´¢åˆ°çš„è®°å¿†é¡¹åˆ†é…åŠ¨æ€å¯é æ€§è¯„åˆ†ï¼Œç»“åˆäº†æ¥æºå¯ä¿¡åº¦ã€æ—¶é—´è¡°å‡å’Œå†²çªæ„ŸçŸ¥ç½‘ç»œå…±è¯†ã€‚é€šè¿‡MMA-BenchåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬æ­ç¤ºäº†â€œè§†è§‰å®‰æ…°æ•ˆåº”â€ï¼Œå¹¶å±•ç¤ºäº†MMAåœ¨å¤šä¸ªä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§å’Œå‡å°‘äº†æ–¹å·®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07345",
            "title": "Optimizing Few-Step Generation with Adaptive Matching Distillation",
            "url": "https://huggingface.co/papers/2602.07345",
            "abstract": "Adaptive Matching Distillation introduces a self-correcting mechanism to improve generative model training by detecting and escaping unstable regions in the optimization landscape.  \t\t\t\t\tAI-generated summary \t\t\t\t Distribution Matching Distillation (DMD) is a powerful acceleration paradigm, yet its stability is often compromised in Forbidden Zone, regions where the real teacher provides unreliable guidance while the fake teacher exerts insufficient repulsive force. In this work, we propose a unified optimization framework that reinterprets prior art as implicit strategies to avoid these corrupted regions. Based on this insight, we introduce Adaptive Matching Distillation (AMD), a self-correcting mechanism that utilizes reward proxies to explicitly detect and escape Forbidden Zones. AMD dynamically prioritizes corrective gradients via structural signal decomposition and introduces Repulsive Landscape Sharpening to enforce steep energy barriers against failure mode collapse. Extensive experiments across image and video generation tasks (e.g., SDXL, Wan2.1) and rigorous benchmarks (e.g., VBench, GenEval) demonstrate that AMD significantly enhances sample fidelity and training robustness. For instance, AMD improves the HPSv2 score on SDXL from 30.64 to 31.25, outperforming state-of-the-art baselines. These findings validate that explicitly rectifying optimization trajectories within Forbidden Zones is essential for pushing the performance ceiling of few-step generative models.",
            "score": 2,
            "issue_id": 1126,
            "pub_date": "2026-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "362bbb9c3af72580",
            "authors": [
                "Lichen Bai",
                "Zikai Zhou",
                "Shitong Shao",
                "Wenliang Zhong",
                "Shuo Yang",
                "Shuo Chen",
                "Bojun Chen",
                "Zeke Xie"
            ],
            "affiliations": [
                "Harbin Institute of Technology, Shenzhen",
                "xLeaF Lab, The Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07345.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#video",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ¾Ğ½ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Adaptive Matching Distillation (AMD) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ğ½Ğ¸Ñ Ğ½ĞµÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ñ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ² Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Forbidden Zones. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Distribution Matching Distillation, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ĞºÑĞ¸-Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Repulsive Landscape Sharpening Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° Ğ² mode failure. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ (SDXL, Wan2.1) Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞµĞ¼Ğ¿Ğ»Ğ¾Ğ² Ğ¸ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Navigating Instability: Enhancing Generative Models with Adaptive Matching Distillation",
                    "desc": "Adaptive Matching Distillation (AMD) is a novel approach designed to enhance the training of generative models by addressing instability in the optimization process. It identifies and navigates through 'Forbidden Zones', areas where traditional guidance from the teacher model is unreliable. By employing reward proxies and structural signal decomposition, AMD prioritizes corrective gradients to maintain stability during training. Experimental results show that AMD significantly improves the quality and robustness of generated samples, demonstrating its effectiveness in overcoming challenges faced by few-step generative models."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”åŒ¹é…è’¸é¦ï¼šæå‡ç”Ÿæˆæ¨¡å‹ç¨³å®šæ€§çš„å…³é”®",
                    "desc": "è‡ªé€‚åº”åŒ¹é…è’¸é¦ï¼ˆAMDï¼‰æå‡ºäº†ä¸€ç§è‡ªæˆ‘ä¿®æ­£æœºåˆ¶ï¼Œä»¥æ”¹å–„ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒã€‚è¯¥æ–¹æ³•é€šè¿‡æ£€æµ‹å’Œé€ƒé¿ä¼˜åŒ–æ™¯è§‚ä¸­çš„ä¸ç¨³å®šåŒºåŸŸï¼Œæ¥æé«˜æ¨¡å‹çš„ç¨³å®šæ€§ã€‚AMDåˆ©ç”¨å¥–åŠ±ä»£ç†æ˜ç¡®è¯†åˆ«å¹¶é€ƒç¦»è¿™äº›è¢«ç§°ä¸ºç¦åŒºçš„åŒºåŸŸï¼Œå¹¶é€šè¿‡ç»“æ„ä¿¡å·åˆ†è§£åŠ¨æ€ä¼˜å…ˆè€ƒè™‘ä¿®æ­£æ¢¯åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAMDåœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ ·æœ¬çš„ä¿çœŸåº¦å’Œè®­ç»ƒçš„é²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.15927",
            "title": "Visual Memory Injection Attacks for Multi-Turn Conversations",
            "url": "https://huggingface.co/papers/2602.15927",
            "abstract": "Visual Memory Injection attack enables covert manipulation of generative vision-language models through manipulated images that trigger targeted responses only under specific prompts during multi-turn conversations.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative large vision-language models (LVLMs) have recently achieved impressive performance gains, and their user base is growing rapidly. However, the security of LVLMs, in particular in a long-context multi-turn setting, is largely underexplored. In this paper, we consider the realistic scenario in which an attacker uploads a manipulated image to the web/social media. A benign user downloads this image and uses it as input to the LVLM. Our novel stealthy Visual Memory Injection (VMI) attack is designed such that on normal prompts the LVLM exhibits nominal behavior, but once the user gives a triggering prompt, the LVLM outputs a specific prescribed target message to manipulate the user, e.g. for adversarial marketing or political persuasion. Compared to previous work that focused on single-turn attacks, VMI is effective even after a long multi-turn conversation with the user. We demonstrate our attack on several recent open-weight LVLMs. This article thereby shows that large-scale manipulation of users is feasible with perturbed images in multi-turn conversation settings, calling for better robustness of LVLMs against these attacks. We release the source code at https://github.com/chs20/visual-memory-injection",
            "score": 1,
            "issue_id": 1127,
            "pub_date": "2026-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "90f472b28994656d",
            "authors": [
                "Christian Schlarmann",
                "Matthias Hein"
            ],
            "affiliations": [
                "Tubingen AI Center, University of Tubingen, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.15927.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#open_source",
                    "#multimodal",
                    "#benchmark",
                    "#cv",
                    "#security"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ¡ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ°Ñ‚Ğ°ĞºÑƒ Visual Memory Injection, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ğ½Ğ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ. ĞÑ‚Ğ°ĞºĞ° ÑĞ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ñ‚Ğ°Ğº, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ñ‹Ğ³Ğ»ÑĞ´ĞµĞ»Ğ° Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ñ…, Ğ½Ğ¾ Ğ²Ñ‹Ğ´Ğ°Ğ²Ğ°Ğ»Ğ° Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ‚Ñ€Ğ¸Ğ³Ğ³ĞµÑ€-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ. ĞÑ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ°Ñ‚Ğ°ĞºĞ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ° Ğ´Ğ°Ğ¶Ğµ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ñ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… open-weight LVLMs Ğ¸ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ ÑÑ‚Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğ¼ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Stealthy Manipulation of LVLMs through Visual Memory Injection",
                    "desc": "This paper introduces a new type of attack called Visual Memory Injection (VMI) that targets generative vision-language models (LVLMs). The attack allows an adversary to manipulate the model's responses by using specially crafted images that only trigger specific outputs when certain prompts are given during multi-turn conversations. Unlike previous attacks that worked in single-turn scenarios, VMI remains effective even after multiple interactions, making it a significant threat. The authors demonstrate the feasibility of this attack on various open-weight LVLMs, highlighting the need for improved security measures in these models."
                },
                "zh": {
                    "title": "è§†è§‰è®°å¿†æ³¨å…¥ï¼šå¤šè½®å¯¹è¯ä¸­çš„éšç§˜æ“æ§",
                    "desc": "è§†è§‰è®°å¿†æ³¨å…¥æ”»å‡»ï¼ˆVMIï¼‰æ˜¯ä¸€ç§æ–°å‹çš„æ”»å‡»æ–¹å¼ï¼Œèƒ½å¤Ÿåœ¨å¤šè½®å¯¹è¯ä¸­é€šè¿‡æ“æ§å›¾åƒæ¥éšç§˜åœ°æ“çºµç”Ÿæˆçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰ã€‚æ”»å‡»è€…ä¸Šä¼ ç»è¿‡æ“æ§çš„å›¾åƒï¼Œç”¨æˆ·ä¸‹è½½åä½œä¸ºè¾“å…¥ä½¿ç”¨ï¼ŒLVLMåœ¨æ­£å¸¸æç¤ºä¸‹è¡¨ç°æ­£å¸¸ï¼Œä½†åœ¨ç‰¹å®šè§¦å‘æç¤ºä¸‹ä¼šè¾“å‡ºé¢„è®¾çš„ç›®æ ‡ä¿¡æ¯ï¼Œä»è€Œå½±å“ç”¨æˆ·çš„å†³ç­–ã€‚ä¸ä»¥å¾€åªå…³æ³¨å•è½®æ”»å‡»çš„ç ”ç©¶ä¸åŒï¼ŒVMIåœ¨é•¿æ—¶é—´çš„å¤šè½®å¯¹è¯ä¸­ä¾ç„¶æœ‰æ•ˆï¼Œå±•ç¤ºäº†å¤§è§„æ¨¡æ“æ§ç”¨æˆ·çš„å¯èƒ½æ€§ã€‚è¯¥ç ”ç©¶å‘¼åå¯¹LVLMè¿›è¡Œæ›´å¥½çš„å®‰å…¨æ€§å¢å¼ºï¼Œä»¥æŠµå¾¡æ­¤ç±»æ”»å‡»ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08392",
            "title": "BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2602.08392",
            "abstract": "BiManiBench evaluates multimodal large language models on bimanual robotic tasks, revealing limitations in spatial grounding and control despite strong high-level reasoning capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot. To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures. Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors. These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.",
            "score": 1,
            "issue_id": 1126,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "7b1921b3a90f9f66",
            "authors": [
                "Xin Wu",
                "Zhixuan Liang",
                "Yue Ma",
                "Mengkang Hu",
                "Zhiyuan Qin",
                "Xiu Li"
            ],
            "affiliations": [
                "Beijing Innovation Center of Humanoid Robotics",
                "HKUST",
                "The University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08392.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#robotics",
                    "#multimodal",
                    "#reasoning",
                    "#hallucinations"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞšĞ¾Ğ³Ğ´Ğ° Ğ´Ğ²Ğµ Ñ€ÑƒĞºĞ¸ Ğ»ÑƒÑ‡ÑˆĞµ, Ñ‡ĞµĞ¼ Ğ¾Ğ´Ğ½Ğ°: Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ",
                    "desc": "BiManiBench Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ´Ğ²ÑƒÑ€ÑƒĞºĞ¾Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ğ¼Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ 30 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¾Ğ¹ Ğ´Ğ²ÑƒÑ… Ñ€ÑƒĞº Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ¸Ñ… ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ñ‹Ñ… ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ»ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ÑƒĞºĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Evaluating Bimanual Robotics: Bridging Reasoning and Control",
                    "desc": "BiManiBench is a new benchmark designed to evaluate multimodal large language models (MLLMs) on tasks that require the use of both arms in robotics. It assesses models on three levels: basic spatial reasoning, advanced action planning, and precise control of robotic arms. The study found that while MLLMs excel in high-level reasoning, they often fail in managing the complexities of bimanual tasks, such as coordinating arm movements and avoiding collisions. This highlights a gap in current models' understanding of the intricate kinematic relationships necessary for effective dual-arm manipulation."
                },
                "zh": {
                    "title": "åŒæ‰‹åä½œçš„æ™ºèƒ½è¯„ä¼°æ–°æ ‡å‡†",
                    "desc": "BiManiBench æ˜¯ä¸€ä¸ªè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒæ‰‹æœºå™¨äººä»»åŠ¡ä¸Šçš„åŸºå‡†æµ‹è¯•ã€‚å°½ç®¡è¿™äº›æ¨¡å‹åœ¨é«˜å±‚æ¬¡æ¨ç†èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç©ºé—´å®šä½å’Œæ§åˆ¶æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚ç°æœ‰çš„è¯„ä¼°æ¡†æ¶ä¸»è¦é›†ä¸­åœ¨å•è‡‚æ“ä½œï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†åŒæ‰‹åä½œæ‰€éœ€çš„æ—¶ç©ºåè°ƒã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒè‡‚ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ï¼Œå¼ºè°ƒäº†æœªæ¥ç ”ç©¶éœ€è¦å…³æ³¨çš„ç›¸äº’è¿åŠ¨çº¦æŸå’Œç²¾ç»†çš„æ—¶é—´åºåˆ—é—®é¢˜ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-02-18.html",
    "link_next": "2026-02-20.html",
    "link_month": "2026-02.html",
    "short_date_prev": {
        "ru": "18.02",
        "en": "02/18",
        "zh": "2æœˆ18æ—¥"
    },
    "short_date_next": {
        "ru": "20.02",
        "en": "02/20",
        "zh": "2æœˆ20æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 1,
        "#benchmark": 9,
        "#agents": 3,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 2,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 4,
        "#multimodal": 7,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 4,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 5,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 2,
        "#optimization": 3,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}