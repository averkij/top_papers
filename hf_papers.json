{
    "date": {
        "ru": "11 июня",
        "en": "June 11",
        "zh": "6月11日"
    },
    "time_utc": "2025-06-11 08:16",
    "weekday": 2,
    "issue_id": 4237,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.06751",
            "title": "Geopolitical biases in LLMs: what are the \"good\" and the \"bad\" countries\n  according to contemporary language models",
            "url": "https://huggingface.co/papers/2506.06751",
            "abstract": "LLMs exhibit significant geopolitical biases in their interpretation of historical events, and simple debiasing methods have limited effectiveness; a novel dataset for further research is provided.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper evaluates geopolitical biases in LLMs with respect to various countries though an analysis of their interpretation of historical events with conflicting national perspectives (USA, UK, USSR, and China). We introduce a novel dataset with neutral event descriptions and contrasting viewpoints from different countries. Our findings show significant geopolitical biases, with models favoring specific national narratives. Additionally, simple debiasing prompts had a limited effect in reducing these biases. Experiments with manipulated participant labels reveal models' sensitivity to attribution, sometimes amplifying biases or recognizing inconsistencies, especially with swapped labels. This work highlights national narrative biases in LLMs, challenges the effectiveness of simple debiasing methods, and offers a framework and dataset for future geopolitical bias research.",
            "score": 21,
            "issue_id": 4234,
            "pub_date": "2025-06-07",
            "pub_date_card": {
                "ru": "7 июня",
                "en": "June 7",
                "zh": "6月7日"
            },
            "hash": "87a1fbaf018382d4",
            "authors": [
                "Mikhail Salnikov",
                "Dmitrii Korzh",
                "Ivan Lazichny",
                "Elvir Karimov",
                "Artyom Iudin",
                "Ivan Oseledets",
                "Oleg Y. Rogov",
                "Alexander Panchenko",
                "Natalia Loukachevitch",
                "Elena Tutubalina"
            ],
            "affiliations": [
                "AIRI",
                "Kazan Federal University",
                "Lomonosov MSU",
                "MIPT",
                "MTUCI",
                "Sber AI",
                "Skoltech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.06751.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#alignment",
                    "#data",
                    "#dataset"
                ],
                "emoji": "🌍",
                "ru": {
                    "title": "Геополитические предубеждения в LLM: вызов объективности искусственного интеллекта",
                    "desc": "Исследование оценивает геополитические предубеждения в больших языковых моделях (LLM) при интерпретации исторических событий с конфликтующими национальными перспективами. Авторы представляют новый набор данных с нейтральными описаниями событий и противоречивыми точками зрения разных стран. Результаты показывают значительные геополитические предубеждения, причем модели отдают предпочтение определенным национальным нарративам. Простые методы дебиасинга оказались малоэффективными в снижении этих предубеждений."
                },
                "en": {
                    "title": "Uncovering Geopolitical Biases in Language Models",
                    "desc": "This paper investigates the presence of geopolitical biases in large language models (LLMs) by analyzing how they interpret historical events from different national perspectives, specifically focusing on the USA, UK, USSR, and China. It introduces a new dataset that contains neutral descriptions of events alongside varying viewpoints from these countries to facilitate further research. The results indicate that LLMs tend to favor certain national narratives, demonstrating significant biases in their outputs. Additionally, the study finds that basic debiasing techniques are not very effective in mitigating these biases, suggesting a need for more robust methods in future research."
                },
                "zh": {
                    "title": "揭示大型语言模型的地缘政治偏见",
                    "desc": "本论文评估了大型语言模型（LLMs）在解释历史事件时的地缘政治偏见，特别是针对美国、英国、苏联和中国等国家的不同视角。我们引入了一个新数据集，包含中立的事件描述和来自不同国家的对立观点。研究结果显示，模型倾向于支持特定国家的叙事，且简单的去偏见方法效果有限。通过操控参与者标签的实验，我们发现模型对归因非常敏感，有时会放大偏见或识别不一致，尤其是在标签交换的情况下。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07927",
            "title": "Solving Inequality Proofs with Large Language Models",
            "url": "https://huggingface.co/papers/2506.07927",
            "abstract": "The investigation into inequality proving using large language models uncovers significant challenges in constructing rigorous proofs, revealing gaps between finding answers and generating valid step-wise solutions.  \t\t\t\t\tAI-generated summary \t\t\t\t Inequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategic theorem application. This makes it a distinct, demanding frontier for large language models (LLMs), offering insights beyond general mathematical problem-solving. Progress in this area is hampered by existing datasets that are often scarce, synthetic, or rigidly formal. We address this by proposing an informal yet verifiable task formulation, recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated dataset of Olympiad-level inequalities, including a test set and training corpus enriched with step-wise solutions and theorem annotations. We also develop a novel LLM-as-judge evaluation framework, combining a final-answer judge with four step-wise judges designed to detect common reasoning flaws. A systematic evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even top models like o1 achieve less than 10% overall accuracy under step-wise scrutiny; this is a drop of up to 65.5% from their accuracy considering only final answer equivalence. This discrepancy exposes fragile deductive chains and a critical gap for current LLMs between merely finding an answer and constructing a rigorous proof. Scaling model size and increasing test-time computation yield limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement. Code and data are available at https://ineqmath.github.io/.",
            "score": 8,
            "issue_id": 4235,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 июня",
                "en": "June 9",
                "zh": "6月9日"
            },
            "hash": "0171dcd88ad3a14f",
            "authors": [
                "Jiayi Sheng",
                "Luna Lyu",
                "Jikai Jin",
                "Tony Xia",
                "Alex Gu",
                "James Zou",
                "Pan Lu"
            ],
            "affiliations": [
                "Massachusetts Institute of Technology",
                "Stanford University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07927.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#data",
                    "#benchmark",
                    "#dataset",
                    "#reasoning",
                    "#math"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "LLM могут найти ответ, но не могут доказать",
                    "desc": "Исследование способности больших языковых моделей (LLM) доказывать неравенства выявило значительные трудности в построении строгих доказательств. Эксперты создали набор данных IneqMath с олимпиадными задачами и разработали новую систему оценки с использованием LLM в качестве судьи. Результаты показали, что даже лучшие модели достигают менее 10% точности при пошаговой проверке, что значительно ниже точности при учете только конечного ответа. Это исследование выявило существенный разрыв между способностью LLM находить ответ и строить корректное доказательство."
                },
                "en": {
                    "title": "Bridging the Gap: From Answers to Rigorous Proofs in Inequality Proving",
                    "desc": "This paper explores the challenges faced by large language models (LLMs) in the domain of inequality proving, which is essential for advanced reasoning in mathematics and science. It identifies a significant gap between generating answers and producing valid, step-by-step proofs, highlighting the limitations of current datasets that are often inadequate. The authors propose a new task formulation that breaks down inequality proving into two checkable subtasks: bound estimation and relation prediction, and introduce the IneqMath dataset for training and evaluation. Their evaluation of 29 leading LLMs reveals that even the best models struggle with rigorous proof construction, achieving less than 10% accuracy when assessed on step-wise reasoning, indicating a need for improved methodologies in theorem-guided reasoning and self-refinement."
                },
                "zh": {
                    "title": "揭示不等式证明中的推理挑战",
                    "desc": "这篇论文探讨了使用大型语言模型进行不等式证明的挑战，揭示了找到答案与生成有效逐步解决方案之间的差距。不等式证明在科学和数学领域至关重要，考验着高级推理能力，如发现紧界和战略性定理应用。为了应对现有数据集稀缺和形式化的问题，作者提出了一种非正式但可验证的任务形式，将不等式证明重构为两个可自动检查的子任务：界限估计和关系预测。此外，研究还发布了IneqMath数据集，并开发了一种新的评估框架，以检测常见的推理缺陷。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09040",
            "title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand\n  Better",
            "url": "https://huggingface.co/papers/2506.09040",
            "abstract": "Autoregressive Semantic Visual Reconstruction (ASVR) improves multimodal understanding by focusing on semantic reconstruction rather than raw visual appearance, enhancing performance across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Typical large vision-language models (LVLMs) apply autoregressive supervision solely to textual sequences, without fully incorporating the visual modality into the learning process. This results in three key limitations: (1) an inability to utilize images without accompanying captions, (2) the risk that captions omit critical visual details, and (3) the challenge that certain vision-centric content cannot be adequately conveyed through text. As a result, current LVLMs often prioritize vision-to-language alignment while potentially overlooking fine-grained visual information. While some prior works have explored autoregressive image generation, effectively leveraging autoregressive visual supervision to enhance image understanding remains an open challenge. In this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR), which enables joint learning of visual and textual modalities within a unified autoregressive framework. We show that autoregressively reconstructing the raw visual appearance of images does not enhance and may even impair multimodal understanding. In contrast, autoregressively reconstructing the semantic representation of images consistently improves comprehension. Notably, we find that even when models are given continuous image features as input, they can effectively reconstruct discrete semantic tokens, resulting in stable and consistent improvements across a wide range of multimodal understanding benchmarks. Our approach delivers significant performance gains across varying data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is available at https://github.com/AlenjandroWang/ASVR.",
            "score": 6,
            "issue_id": 4237,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 июня",
                "en": "June 10",
                "zh": "6月10日"
            },
            "hash": "09d042607d92f156",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#optimization",
                    "#interpretability",
                    "#benchmark",
                    "#games",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Семантическая реконструкция изображений для улучшения мультимодального понимания",
                    "desc": "Авторегрессивная семантическая визуальная реконструкция (ASVR) улучшает мультимодальное понимание, фокусируясь на семантической реконструкции, а не на восстановлении исходного визуального образа. Этот подход позволяет эффективно использовать изображения без сопроводительных подписей и учитывать важные визуальные детали, которые могут быть упущены в текстовых описаниях. ASVR объединяет обучение визуальной и текстовой модальностей в единой авторегрессивной структуре, что приводит к значительному улучшению производительности на различных мультимодальных тестах. Метод показывает стабильные улучшения при различных масштабах данных и типах базовых языковых моделей."
                },
                "en": {
                    "title": "Revolutionizing Multimodal Understanding with Semantic Focus",
                    "desc": "The paper introduces Autoregressive Semantic Visual Reconstruction (ASVR), a method that enhances multimodal understanding by focusing on reconstructing the semantic content of images rather than their raw visual appearance. This approach addresses limitations in existing large vision-language models (LVLMs) that primarily rely on textual sequences, which can lead to missing critical visual details. By enabling joint learning of visual and textual modalities, ASVR allows models to effectively reconstruct discrete semantic tokens from continuous image features, leading to improved comprehension. The results show significant performance gains across various benchmarks, demonstrating the effectiveness of semantic reconstruction in multimodal tasks."
                },
                "zh": {
                    "title": "自回归语义重建，提升多模态理解！",
                    "desc": "自回归语义视觉重建（ASVR）通过关注语义重建而非原始视觉外观，提升了多模态理解的能力。传统的大型视觉语言模型（LVLM）仅对文本序列应用自回归监督，未能充分整合视觉模态，导致无法利用没有配套说明的图像。ASVR 通过在统一的自回归框架内实现视觉和文本模态的联合学习，克服了这一限制。我们的研究表明，重建图像的语义表示能够显著提高多模态理解的效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08009",
            "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion",
            "url": "https://huggingface.co/papers/2506.08009",
            "abstract": "Self Forcing, a novel training method for autoregressive video diffusion models, reduces exposure bias and improves generation quality through holistic video-level supervision and efficient caching mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Self Forcing, a novel training paradigm for autoregressive video diffusion models. It addresses the longstanding issue of exposure bias, where models trained on ground-truth context must generate sequences conditioned on their own imperfect outputs during inference. Unlike prior methods that denoise future frames based on ground-truth context frames, Self Forcing conditions each frame's generation on previously self-generated outputs by performing autoregressive rollout with key-value (KV) caching during training. This strategy enables supervision through a holistic loss at the video level that directly evaluates the quality of the entire generated sequence, rather than relying solely on traditional frame-wise objectives. To ensure training efficiency, we employ a few-step diffusion model along with a stochastic gradient truncation strategy, effectively balancing computational cost and performance. We further introduce a rolling KV cache mechanism that enables efficient autoregressive video extrapolation. Extensive experiments demonstrate that our approach achieves real-time streaming video generation with sub-second latency on a single GPU, while matching or even surpassing the generation quality of significantly slower and non-causal diffusion models. Project website: http://self-forcing.github.io/",
            "score": 6,
            "issue_id": 4235,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 июня",
                "en": "June 9",
                "zh": "6月9日"
            },
            "hash": "e63130d065bba1fd",
            "authors": [
                "Xun Huang",
                "Zhengqi Li",
                "Guande He",
                "Mingyuan Zhou",
                "Eli Shechtman"
            ],
            "affiliations": [
                "Adobe Research",
                "The University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08009.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#video",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Self Forcing: реалистичная генерация видео в реальном времени",
                    "desc": "Представлен новый метод обучения авторегрессионных видео-диффузионных моделей под названием Self Forcing. Он решает проблему смещения экспозиции, обучая модель на собственных сгенерированных выходных данных вместо истинных. Self Forcing использует кэширование ключ-значение и целостную функцию потерь на уровне всего видео. Метод позволяет генерировать видео в реальном времени с низкой задержкой на одном GPU, сохраняя высокое качество."
                },
                "en": {
                    "title": "Self Forcing: Enhancing Video Generation with Autoregressive Training",
                    "desc": "The paper presents Self Forcing, a new training method for autoregressive video diffusion models that aims to reduce exposure bias and enhance video generation quality. It allows models to generate video frames based on their own previously generated outputs instead of relying solely on ground-truth frames, which helps in better training. By using a holistic video-level supervision approach, the method evaluates the quality of the entire video sequence rather than just individual frames. Additionally, it incorporates efficient caching mechanisms and a few-step diffusion model to optimize performance and ensure real-time video generation on a single GPU."
                },
                "zh": {
                    "title": "自我强制：提升视频生成质量的新方法",
                    "desc": "Self Forcing是一种新颖的自回归视频扩散模型训练方法，旨在减少曝光偏差并提高生成质量。该方法通过整体视频级监督和高效缓存机制，解决了模型在推理时必须依赖自身不完美输出生成序列的问题。与以往方法不同，Self Forcing在训练过程中使用自生成输出进行条件生成，从而实现视频级的整体损失评估。实验表明，该方法能够在单个GPU上实现实时视频生成，且生成质量优于传统的非因果扩散模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08002",
            "title": "Aligning Text, Images, and 3D Structure Token-by-Token",
            "url": "https://huggingface.co/papers/2506.08002",
            "abstract": "A unified language, image, and 3D scene model framework is proposed, achieving optimal training and performance across various 3D tasks and datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Creating machines capable of understanding the world in 3D is essential in assisting designers that build and edit 3D environments and robots navigating and interacting within a three-dimensional space. Inspired by advances in language and image modeling, we investigate the potential of autoregressive models for a new modality: structured 3D scenes. To this end, we propose a unified LLM framework that aligns language, images, and 3D scenes and provide a detailed ''cookbook'' outlining critical design choices for achieving optimal training and performance addressing key questions related to data representation, modality-specific objectives, and more. We evaluate performance across four core 3D tasks -- rendering, recognition, instruction-following, and question-answering -- and four 3D datasets, synthetic and real-world. We extend our approach to reconstruct complex 3D object shapes by enriching our 3D modality with quantized shape encodings, and show our model's effectiveness on real-world 3D object recognition tasks. Project webpage: https://glab-caltech.github.io/kyvo/",
            "score": 6,
            "issue_id": 4232,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 июня",
                "en": "June 9",
                "zh": "6月9日"
            },
            "hash": "ab2da61a8a27783d",
            "authors": [
                "Aadarsh Sahoo",
                "Vansh Tibrewal",
                "Georgia Gkioxari"
            ],
            "affiliations": [
                "California Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08002.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#synthetic",
                    "#3d",
                    "#multimodal"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Единая мультимодальная модель для понимания 3D-мира",
                    "desc": "Предложена унифицированная модель для языка, изображений и 3D-сцен, достигающая оптимальной производительности в различных 3D-задачах. Модель основана на авторегрессионных методах и использует выравнивание модальностей. Авторы предоставляют детальное руководство по ключевым аспектам обучения, включая представление данных и специфические для модальностей целевые функции. Модель оценивается на четырех основных 3D-задачах и четырех наборах данных, демонстрируя эффективность в реконструкции сложных 3D-форм объектов."
                },
                "en": {
                    "title": "Unifying Language, Image, and 3D Understanding for Enhanced Machine Interaction",
                    "desc": "This paper presents a unified framework that integrates language, images, and 3D scenes to enhance machine understanding of three-dimensional environments. By leveraging autoregressive models, the authors explore how to effectively represent and process structured 3D scenes alongside traditional modalities. The framework includes a comprehensive guide for optimal training and performance, addressing critical aspects like data representation and modality-specific objectives. The model is evaluated on various 3D tasks, demonstrating its capability in rendering, recognition, instruction-following, and question-answering across both synthetic and real-world datasets."
                },
                "zh": {
                    "title": "统一三维场景模型，提升AI理解能力",
                    "desc": "本文提出了一种统一的语言、图像和三维场景模型框架，旨在实现各种三维任务和数据集的最佳训练和性能。该框架利用自回归模型，探索了结构化三维场景的新模式，帮助设计师构建和编辑三维环境。我们提供了一份详细的“食谱”，阐述了实现最佳训练和性能的关键设计选择，并评估了在四个核心三维任务和数据集上的表现。通过丰富三维模态的量化形状编码，我们的模型在复杂三维物体识别任务中展现了有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07177",
            "title": "Frame Guidance: Training-Free Guidance for Frame-Level Control in Video\n  Diffusion Models",
            "url": "https://huggingface.co/papers/2506.07177",
            "abstract": "Frame Guidance offers a training-free method for controlling video generation using frame-level signals, reducing memory usage and enhancing globally coherent video output.  \t\t\t\t\tAI-generated summary \t\t\t\t Advancements in diffusion models have significantly improved video quality, directing attention to fine-grained controllability. However, many existing methods depend on fine-tuning large-scale video models for specific tasks, which becomes increasingly impractical as model sizes continue to grow. In this work, we present Frame Guidance, a training-free guidance for controllable video generation based on frame-level signals, such as keyframes, style reference images, sketches, or depth maps. For practical training-free guidance, we propose a simple latent processing method that dramatically reduces memory usage, and apply a novel latent optimization strategy designed for globally coherent video generation. Frame Guidance enables effective control across diverse tasks, including keyframe guidance, stylization, and looping, without any training, compatible with any video models. Experimental results show that Frame Guidance can produce high-quality controlled videos for a wide range of tasks and input signals.",
            "score": 5,
            "issue_id": 4233,
            "pub_date": "2025-06-08",
            "pub_date_card": {
                "ru": "8 июня",
                "en": "June 8",
                "zh": "6月8日"
            },
            "hash": "7d83fcff01c3595a",
            "authors": [
                "Sangwon Jang",
                "Taekyung Ki",
                "Jaehyeong Jo",
                "Jaehong Yoon",
                "Soo Ye Kim",
                "Zhe Lin",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "Adobe Research",
                "DeepAuto.ai",
                "KAIST",
                "UNC Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07177.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#video",
                    "#diffusion"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Управление генерацией видео без переобучения модели",
                    "desc": "Статья представляет метод Frame Guidance для управления генерацией видео без дополнительного обучения моделей. Этот подход использует покадровые сигналы, такие как ключевые кадры, эталонные изображения стиля или карты глубины. Frame Guidance значительно снижает использование памяти благодаря обработке латентного пространства. Метод применяет новую стратегию оптимизации латентного пространства для создания глобально согласованных видео."
                },
                "en": {
                    "title": "Effortless Control in Video Generation with Frame Guidance",
                    "desc": "Frame Guidance introduces a novel approach to video generation that does not require any training, allowing for effective control using frame-level signals. This method significantly reduces memory usage while ensuring that the generated videos maintain global coherence. By utilizing a simple latent processing technique and a unique latent optimization strategy, Frame Guidance can adapt to various tasks such as keyframe guidance and stylization. The results demonstrate that this approach can produce high-quality videos across different input types without the need for fine-tuning large models."
                },
                "zh": {
                    "title": "无训练的视频生成控制新方法",
                    "desc": "本论文提出了一种名为Frame Guidance的方法，用于控制视频生成，且无需训练。该方法利用帧级信号，如关键帧和风格参考图像，来实现对视频生成的精细控制。Frame Guidance显著降低了内存使用，并增强了视频输出的全局一致性。实验结果表明，该方法能够在多种任务中生成高质量的可控视频，且与任何视频模型兼容。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05167",
            "title": "ECoRAG: Evidentiality-guided Compression for Long Context RAG",
            "url": "https://huggingface.co/papers/2506.05167",
            "abstract": "ECoRAG framework enhances LLM performance in ODQA by compressing retrieved documents based on evidentiality, reducing latency and token usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have shown remarkable performance in Open-Domain Question Answering (ODQA) by leveraging external documents through Retrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer context, context compression is necessary. However, prior compression methods do not focus on filtering out non-evidential information, which limit the performance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or ECoRAG framework. ECoRAG improves LLM performance by compressing retrieved documents based on evidentiality, ensuring whether answer generation is supported by the correct evidence. As an additional step, ECoRAG reflects whether the compressed content provides sufficient evidence, and if not, retrieves more until sufficient. Experiments show that ECoRAG improves LLM performance on ODQA tasks, outperforming existing compression methods. Furthermore, ECoRAG is highly cost-efficient, as it not only reduces latency but also minimizes token usage by retaining only the necessary information to generate the correct answer. Code is available at https://github.com/ldilab/ECoRAG.",
            "score": 5,
            "issue_id": 4231,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 июня",
                "en": "June 5",
                "zh": "6月5日"
            },
            "hash": "d979315df3a92206",
            "authors": [
                "Yeonseok Jeong",
                "Jinsu Kim",
                "Dohyeon Lee",
                "Seung-won Hwang"
            ],
            "affiliations": [
                "IPAI, Seoul National University",
                "Korea University",
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05167.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#alignment",
                    "#long_context"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "ECoRAG: Умное сжатие для точных ответов",
                    "desc": "ECoRAG - это новый фреймворк для улучшения производительности больших языковых моделей (LLM) в задачах открытого вопросно-ответного поиска (ODQA). Он использует сжатие полученных документов на основе доказательности, что позволяет снизить задержку и использование токенов. ECoRAG превосходит существующие методы сжатия и повышает эффективность LLM в задачах ODQA. Этот подход не только улучшает производительность, но и является экономически эффективным, сохраняя только необходимую информацию для генерации правильного ответа."
                },
                "en": {
                    "title": "ECoRAG: Elevating LLMs with Evidence-Based Compression",
                    "desc": "The ECoRAG framework enhances the performance of Large Language Models (LLMs) in Open-Domain Question Answering (ODQA) by focusing on evidentiality during document retrieval and compression. By filtering out non-evidential information, ECoRAG ensures that the generated answers are supported by relevant evidence, improving the overall accuracy of responses. Additionally, the framework optimizes resource usage by reducing latency and minimizing token consumption, making it more efficient than previous methods. Experiments demonstrate that ECoRAG significantly outperforms existing compression techniques in ODQA tasks."
                },
                "zh": {
                    "title": "ECoRAG：提升问答性能的证据性压缩框架",
                    "desc": "ECoRAG框架通过基于证据性压缩检索到的文档，提升了大型语言模型（LLM）在开放领域问答（ODQA）中的表现。该方法解决了以往压缩技术未能有效过滤非证据性信息的问题，从而提高了生成答案的准确性。ECoRAG确保生成的答案有足够的证据支持，并在必要时进行额外的文档检索。实验结果表明，ECoRAG在ODQA任务中优于现有的压缩方法，同时降低了延迟和令牌使用，具有很高的成本效益。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04614",
            "title": "Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error\n  Diagnosis in GUI Automation",
            "url": "https://huggingface.co/papers/2506.04614",
            "abstract": "A pre-operative critic mechanism with Suggestion-aware Gradient Relative Policy Optimization enhances the reliability of multimodal reasoning tasks in GUI automation.  \t\t\t\t\tAI-generated summary \t\t\t\t In recent years, Multimodal Large Language Models (MLLMs) have been extensively utilized for multimodal reasoning tasks, including Graphical User Interface (GUI) automation. Unlike general offline multimodal tasks, GUI automation is executed in online interactive environments, necessitating step-by-step decision-making based on real-time status of the environment. This task has a lower tolerance for decision-making errors at each step, as any mistakes may cumulatively disrupt the process and potentially lead to irreversible outcomes like deletions or payments. To address these issues, we introduce a pre-operative critic mechanism that provides effective feedback prior to the actual execution, by reasoning about the potential outcome and correctness of actions. Specifically, we propose a Suggestion-aware Gradient Relative Policy Optimization (S-GRPO) strategy to construct our pre-operative critic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance the reliability of the model's feedback. Furthermore, we develop a reasoning-bootstrapping based data collection pipeline to create a GUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic data. Static experiments on the GUI-Critic-Test across both mobile and web domains reveal that our GUI-Critic-R1 offers significant advantages in critic accuracy compared to current MLLMs. Dynamic evaluation on GUI automation benchmark further highlights the effectiveness and superiority of our model, as evidenced by improved success rates and operational efficiency.",
            "score": 5,
            "issue_id": 4236,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 июня",
                "en": "June 5",
                "zh": "6月5日"
            },
            "hash": "9eb1f2457722a2cd",
            "authors": [
                "Yuyang Wanyan",
                "Xi Zhang",
                "Haiyang Xu",
                "Haowei Liu",
                "Junyang Wang",
                "Jiabo Ye",
                "Yutong Kou",
                "Ming Yan",
                "Fei Huang",
                "Xiaoshan Yang",
                "Weiming Dong",
                "Changsheng Xu"
            ],
            "affiliations": [
                "Alibaba Group",
                "Beijing Jiaotong University",
                "MAIS, Institute of Automation, Chinese Academy of Sciences, China",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04614.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#multimodal",
                    "#optimization",
                    "#rl",
                    "#benchmark"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Повышение надежности автоматизации GUI с помощью предоперационной критики",
                    "desc": "Статья представляет новый механизм предоперационной критики для повышения надежности задач мультимодального рассуждения в автоматизации графического интерфейса пользователя (GUI). Авторы предлагают стратегию оптимизации политики с учетом градиента и предложений (S-GRPO) для создания модели GUI-Critic-R1. Исследование включает разработку процесса сбора данных на основе бутстрэппинга рассуждений для создания наборов данных GUI-Critic-Train и GUI-Critic-Test. Эксперименты показывают, что GUI-Critic-R1 превосходит существующие мультимодальные большие языковые модели (MLLM) в точности критики и эффективности автоматизации GUI."
                },
                "en": {
                    "title": "Enhancing GUI Automation Reliability with Pre-operative Critique",
                    "desc": "This paper presents a new method to improve the reliability of GUI automation using a pre-operative critic mechanism. The mechanism provides feedback before actions are executed, helping to prevent errors that could lead to serious issues like unwanted deletions. The authors introduce a Suggestion-aware Gradient Relative Policy Optimization (S-GRPO) strategy to enhance the feedback process, making it more effective. Experiments show that their model, GUI-Critic-R1, significantly outperforms existing models in both accuracy and operational efficiency during GUI automation tasks."
                },
                "zh": {
                    "title": "提升GUI自动化可靠性的预操作评估机制",
                    "desc": "本文提出了一种预操作评估机制，旨在提高图形用户界面（GUI）自动化中的多模态推理任务的可靠性。我们引入了一种名为建议感知梯度相对策略优化（S-GRPO）的策略，以构建预操作评估模型GUI-Critic-R1，并通过引入新颖的建议奖励来增强模型反馈的可靠性。该机制在实际执行之前提供有效反馈，帮助推理潜在结果和行动的正确性，从而减少决策错误的风险。通过在移动和网页领域的静态实验，我们的模型在评估准确性上显著优于现有的多模态大语言模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08887",
            "title": "DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for\n  Parameter-Efficient Video-Text Retrieval",
            "url": "https://huggingface.co/papers/2506.08887",
            "abstract": "The paper proposes DiscoVLA to improve video-text retrieval using CLIP by addressing vision, language, and alignment discrepancies, achieving superior performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The parameter-efficient adaptation of the image-text pretraining model CLIP for video-text retrieval is a prominent area of research. While CLIP is focused on image-level vision-language matching, video-text retrieval demands comprehensive understanding at the video level. Three key discrepancies emerge in the transfer from image-level to video-level: vision, language, and alignment. However, existing methods mainly focus on vision while neglecting language and alignment. In this paper, we propose Discrepancy Reduction in Vision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all three discrepancies. Specifically, we introduce Image-Video Features Fusion to integrate image-level and video-level features, effectively tackling both vision and language discrepancies. Additionally, we generate pseudo image captions to learn fine-grained image-level alignment. To mitigate alignment discrepancies, we propose Image-to-Video Alignment Distillation, which leverages image-level alignment knowledge to enhance video-level alignment. Extensive experiments demonstrate the superiority of our DiscoVLA. In particular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous methods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is available at https://github.com/LunarShen/DsicoVLA.",
            "score": 4,
            "issue_id": 4232,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 июня",
                "en": "June 10",
                "zh": "6月10日"
            },
            "hash": "068c2f58bc5049d2",
            "authors": [
                "Leqi Shen",
                "Guoqiang Gong",
                "Tianxiang Hao",
                "Tao He",
                "Yifeng Zhang",
                "Pengzhang Liu",
                "Sicheng Zhao",
                "Jungong Han",
                "Guiguang Ding"
            ],
            "affiliations": [
                "BNRist",
                "Department of Automation, Tsinghua University",
                "GRG Banking Equipment Co., Ltd.",
                "Hangzhou Zhuoxi Institute of Brain and Intelligence",
                "JD.com",
                "School of Software",
                "South China University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08887.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#alignment",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Преодоление разрыва между изображениями и видео в мультимодальном поиске",
                    "desc": "Статья представляет DiscoVLA - метод для улучшения поиска видео по текстовым запросам с использованием модели CLIP. Авторы решают проблемы несоответствия в зрении, языке и выравнивании при переходе от изображений к видео. DiscoVLA объединяет признаки изображений и видео, генерирует псевдо-подписи к изображениям и применяет дистилляцию знаний для улучшения выравнивания видео и текста. Эксперименты показывают превосходство DiscoVLA над существующими методами в задаче поиска видео по текстовым запросам."
                },
                "en": {
                    "title": "Bridging Gaps in Video-Text Retrieval with DiscoVLA",
                    "desc": "The paper introduces DiscoVLA, a method designed to enhance video-text retrieval by addressing three main discrepancies: vision, language, and alignment. Unlike existing approaches that primarily focus on visual aspects, DiscoVLA integrates both image and video features to improve understanding at the video level. It also generates pseudo image captions to refine image-level alignment and employs Image-to-Video Alignment Distillation to strengthen video-level alignment using knowledge from image-level data. Experimental results show that DiscoVLA significantly outperforms previous methods, achieving a notable improvement in retrieval accuracy."
                },
                "zh": {
                    "title": "提升视频-文本检索的DiscoVLA方法",
                    "desc": "本文提出了一种名为DiscoVLA的方法，旨在通过解决视觉、语言和对齐的差异来改善视频-文本检索。传统的CLIP模型主要关注图像级别的视觉-语言匹配，而视频-文本检索需要在视频级别上进行全面理解。我们的方法通过图像-视频特征融合来整合图像和视频的特征，有效应对视觉和语言的差异。同时，我们生成伪图像标题以学习细粒度的图像级别对齐，从而减轻对齐差异。实验结果表明，DiscoVLA在视频-文本检索任务中表现优越，尤其在MSRVTT数据集上取得了50.5%的R@1成绩。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05928",
            "title": "MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient\n  Fine-Tuning of Large Language Models",
            "url": "https://huggingface.co/papers/2506.05928",
            "abstract": "A heterogeneous Mixture-of-Adapters (MoA) approach enhances parameter-efficient fine-tuning in LLMs by integrating diverse adapter experts, outperforming homogeneous MoE-LoRA methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies integrate Low-Rank Adaptation (LoRA) and Mixture-of-Experts (MoE) to further enhance the performance of parameter-efficient fine-tuning (PEFT) methods in Large Language Model (LLM) applications. Existing methods employ homogeneous MoE-LoRA architectures composed of LoRA experts with either similar or identical structures and capacities. However, these approaches often suffer from representation collapse and expert load imbalance, which negatively impact the potential of LLMs. To address these challenges, we propose a heterogeneous Mixture-of-Adapters (MoA) approach. This method dynamically integrates PEFT adapter experts with diverse structures, leveraging their complementary representational capabilities to foster expert specialization, thereby enhancing the effective transfer of pre-trained knowledge to downstream tasks. MoA supports two variants: (i) Soft MoA achieves fine-grained integration by performing a weighted fusion of all expert outputs; (ii) Sparse MoA activates adapter experts sparsely based on their contribution, achieving this with negligible performance degradation. Experimental results demonstrate that heterogeneous MoA outperforms homogeneous MoE-LoRA methods in both performance and parameter efficiency. Our project is available at https://github.com/DCDmllm/MoA.",
            "score": 2,
            "issue_id": 4232,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 июня",
                "en": "June 6",
                "zh": "6月6日"
            },
            "hash": "f78e6f70ebb69ac2",
            "authors": [
                "Jie Cao",
                "Tianwei Lin",
                "Hongyang He",
                "Rolan Yan",
                "Wenqiao Zhang",
                "Juncheng Li",
                "Dongping Zhang",
                "Siliang Tang",
                "Yueting Zhuang"
            ],
            "affiliations": [
                "Tencent",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05928.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#training",
                    "#optimization",
                    "#small_models"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Гетерогенная смесь адаптеров: новый шаг в эффективной настройке языковых моделей",
                    "desc": "Статья представляет новый подход к эффективной настройке больших языковых моделей - гетерогенную Mixture-of-Adapters (MoA). MoA интегрирует различные адаптеры-эксперты, что позволяет преодолеть проблемы коллапса представлений и дисбаланса нагрузки экспертов, характерные для гомогенных методов MoE-LoRA. Предложены два варианта MoA: Soft MoA с взвешенным слиянием выходов всех экспертов и Sparse MoA с активацией только наиболее значимых экспертов. Эксперименты показывают, что MoA превосходит гомогенные методы MoE-LoRA как по производительности, так и по эффективности использования параметров."
                },
                "en": {
                    "title": "Unlocking LLM Potential with Diverse Adapter Experts",
                    "desc": "This paper introduces a new approach called heterogeneous Mixture-of-Adapters (MoA) for fine-tuning Large Language Models (LLMs) more efficiently. Unlike traditional homogeneous MoE-LoRA methods that use similar adapter experts, MoA combines diverse adapter structures to improve performance and prevent issues like representation collapse. The method includes two variants: Soft MoA, which fuses outputs from all experts, and Sparse MoA, which selectively activates experts based on their effectiveness. Experimental results show that MoA significantly outperforms existing methods in both performance and parameter efficiency."
                },
                "zh": {
                    "title": "异构适配器混合：提升大语言模型微调效率的创新方法",
                    "desc": "本文提出了一种异构的适配器混合（MoA）方法，以增强大语言模型（LLM）中参数高效微调的效果。与传统的同质MoE-LoRA方法不同，MoA集成了具有不同结构的适配器专家，从而克服了表示崩溃和专家负载不平衡的问题。该方法通过动态整合适配器专家的互补表示能力，促进了专家的专业化，提升了预训练知识向下游任务的有效转移。实验结果表明，异构MoA在性能和参数效率上均优于同质MoE-LoRA方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07932",
            "title": "Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural\n  Compressor",
            "url": "https://huggingface.co/papers/2506.07932",
            "abstract": "A novel framework called Squeeze3D uses pre-trained models to compress 3D data efficiently, achieving high compression ratios while maintaining visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Squeeze3D, a novel framework that leverages implicit prior knowledge learnt by existing pre-trained 3D generative models to compress 3D data at extremely high compression ratios. Our approach bridges the latent spaces between a pre-trained encoder and a pre-trained generation model through trainable mapping networks. Any 3D model represented as a mesh, point cloud, or a radiance field is first encoded by the pre-trained encoder and then transformed (i.e. compressed) into a highly compact latent code. This latent code can effectively be used as an extremely compressed representation of the mesh or point cloud. A mapping network transforms the compressed latent code into the latent space of a powerful generative model, which is then conditioned to recreate the original 3D model (i.e. decompression). Squeeze3D is trained entirely on generated synthetic data and does not require any 3D datasets. The Squeeze3D architecture can be flexibly used with existing pre-trained 3D encoders and existing generative models. It can flexibly support different formats, including meshes, point clouds, and radiance fields. Our experiments demonstrate that Squeeze3D achieves compression ratios of up to 2187x for textured meshes, 55x for point clouds, and 619x for radiance fields while maintaining visual quality comparable to many existing methods. Squeeze3D only incurs a small compression and decompression latency since it does not involve training object-specific networks to compress an object.",
            "score": 1,
            "issue_id": 4233,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 июня",
                "en": "June 9",
                "zh": "6月9日"
            },
            "hash": "a13860cb07518cb2",
            "authors": [
                "Rishit Dagli",
                "Yushi Guan",
                "Sankeerth Durvasula",
                "Mohammadreza Mofayezi",
                "Nandita Vijaykumar"
            ],
            "affiliations": [
                "University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07932.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#architecture",
                    "#3d"
                ],
                "emoji": "🗜️",
                "ru": {
                    "title": "Эффективное сжатие 3D-данных с помощью нейросетей",
                    "desc": "Squeeze3D - это новая система для сжатия 3D-данных, использующая предобученные модели. Она позволяет достичь высоких степеней сжатия при сохранении визуального качества для сеток, облаков точек и полей излучения. Система работает путем преобразования входных данных в компактное латентное представление с помощью энкодера, а затем восстановления 3D-модели генеративной моделью. Squeeze3D не требует специальных 3D-датасетов для обучения и обеспечивает быстрое сжатие и распаковку данных."
                },
                "en": {
                    "title": "Efficient 3D Data Compression with Squeeze3D",
                    "desc": "Squeeze3D is a new framework designed to compress 3D data efficiently using pre-trained models. It connects the latent spaces of a pre-trained encoder and a generative model through trainable mapping networks, allowing for high compression ratios while preserving visual quality. The framework can handle various 3D representations, such as meshes and point clouds, and is trained solely on synthetic data, eliminating the need for specific 3D datasets. Experiments show that Squeeze3D achieves impressive compression rates, making it a versatile tool for 3D data compression."
                },
                "zh": {
                    "title": "Squeeze3D：高效压缩3D数据的新方法",
                    "desc": "Squeeze3D是一个新颖的框架，利用预训练模型高效压缩3D数据，达到高压缩比并保持视觉质量。该方法通过可训练的映射网络连接预训练编码器和生成模型之间的潜在空间。3D模型首先被编码为紧凑的潜在代码，然后通过映射网络转换为生成模型的潜在空间，以重建原始3D模型。实验表明，Squeeze3D在不同格式的3D数据上实现了显著的压缩效果，同时延迟较小。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07047",
            "title": "Mathesis: Towards Formal Theorem Proving from Natural Languages",
            "url": "https://huggingface.co/papers/2506.07047",
            "abstract": "Recent advances in large language models show strong promise for formal reasoning. However, most LLM-based theorem provers have long been constrained by the need for expert-written formal statements as inputs, limiting their applicability to real-world problems expressed in natural language. We tackle this gap with Mathesis, the first end-to-end theorem proving pipeline processing informal problem statements. It contributes Mathesis-Autoformalizer, the first autoformalizer using reinforcement learning to enhance the formalization ability of natural language problems, aided by our novel LeanScorer framework for nuanced formalization quality assessment. It also proposes a Mathesis-Prover, which generates formal proofs from the formalized statements. To evaluate the real-world applicability of end-to-end formal theorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex problems from China's national college entrance exam. Our approach is carefully designed, with a thorough study of each component. Experiments demonstrate Mathesis's effectiveness, with the autoformalizer outperforming the best baseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other model combinations, achieving 64% accuracy on MiniF2F with pass@32 and a state-of-the-art 18% on Gaokao-Formal.",
            "score": 1,
            "issue_id": 4237,
            "pub_date": "2025-06-08",
            "pub_date_card": {
                "ru": "8 июня",
                "en": "June 8",
                "zh": "6月8日"
            },
            "hash": "d3bc82dde4f2b8bc",
            "authors": [
                "Yu Xuejun",
                "Jianyuan Zhong",
                "Zijin Feng",
                "Pengyi Zhai",
                "Roozbeh Yousefzadeh",
                "Wei Chong Ng",
                "Haoxiong Liu",
                "Ziyi Shou",
                "Jing Xiong",
                "Yudong Zhou",
                "Claudia Beth Ong",
                "Austen Jeremy Sugiarto",
                "Yaoxi Zhang",
                "Wai Ming Tai",
                "Huan Cao",
                "Dongcai Lu",
                "Jiacheng Sun",
                "Qiang Xu",
                "Shen Xin",
                "Zhenguo Li"
            ],
            "affiliations": [
                "Huawei Celia Team",
                "Huawei Noahs Ark Lab",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07047.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#training",
                    "#benchmark",
                    "#dataset",
                    "#math"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Автоматизация формального доказательства теорем от естественного языка до математической логики",
                    "desc": "Исследователи представили Mathesis - первый сквозной конвейер для доказательства теорем, обрабатывающий неформальные формулировки задач. Ключевым компонентом является Mathesis-Autoformalizer, использующий обучение с подкреплением для улучшения формализации естественно-языковых задач. Система также включает Mathesis-Prover для генерации формальных доказательств. Для оценки применимости подхода авторы создали набор данных Gaokao-Formal из 488 сложных задач китайского вступительного экзамена в вузы."
                },
                "en": {
                    "title": "Bridging Natural Language and Formal Reasoning with Mathesis",
                    "desc": "This paper presents Mathesis, a novel end-to-end theorem proving system that processes informal problem statements, addressing the limitations of existing LLM-based theorem provers. It introduces Mathesis-Autoformalizer, which utilizes reinforcement learning to automatically convert natural language problems into formal statements, supported by the LeanScorer framework for assessing formalization quality. Additionally, the Mathesis-Prover generates formal proofs from these formalized statements. The system's effectiveness is validated through experiments on the Gaokao-Formal benchmark, demonstrating significant improvements in accuracy and pass rates compared to existing methods."
                },
                "zh": {
                    "title": "Mathesis：非正式问题的定理证明新突破",
                    "desc": "本论文介绍了Mathesis，这是第一个能够处理非正式问题陈述的端到端定理证明管道。它包括Mathesis-Autoformalizer，这是一个使用强化学习的自动形式化工具，能够提高自然语言问题的形式化能力，并通过LeanScorer框架评估形式化质量。论文还提出了Mathesis-Prover，能够从形式化的陈述中生成正式证明。通过在中国高考的488个复杂问题上进行评估，实验结果表明Mathesis在通过率和准确性上均优于现有模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05700",
            "title": "RKEFino1: A Regulation Knowledge-Enhanced Large Language Model",
            "url": "https://huggingface.co/papers/2506.05700",
            "abstract": "RKEFino1, a knowledge-enhanced financial reasoning model, addresses accuracy and compliance challenges in Digital Regulatory Reporting through fine-tuning with domain knowledge from XBRL, CDM, and MOF, and introduces a novel Numerical NER task.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) hold great promise for financial applications but introduce critical accuracy and compliance challenges in Digital Regulatory Reporting (DRR). To address these issues, we propose RKEFino1, a regulation knowledge-enhanced financial reasoning model built upon Fino1, fine-tuned with domain knowledge from XBRL, CDM, and MOF. We formulate two QA tasks-knowledge-based and mathematical reasoning-and introduce a novel Numerical NER task covering financial entities in both sentences and tables. Experimental results demonstrate the effectiveness and generalization capacity of RKEFino1 in compliance-critical financial tasks. We have released our model on Hugging Face.",
            "score": 1,
            "issue_id": 4231,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 июня",
                "en": "June 6",
                "zh": "6月6日"
            },
            "hash": "a23a28bb68811316",
            "authors": [
                "Yan Wang",
                "Yueru He",
                "Ruoyu Xiang",
                "Jeff Zhao"
            ],
            "affiliations": [
                "Columbia University",
                "New York University",
                "The University of Texas at Austin",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05700.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#multimodal",
                    "#reasoning",
                    "#training",
                    "#healthcare",
                    "#open_source"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "Умная финансовая модель для точной регуляторной отчетности",
                    "desc": "RKEFino1 - это модель финансового рассуждения, улучшенная знаниями о регулировании, построенная на основе Fino1. Она решает проблемы точности и соответствия нормам в цифровой регуляторной отчетности путем дообучения на доменных знаниях из XBRL, CDM и MOF. Модель сформулирована для решения двух задач вопросно-ответной системы: на основе знаний и математических рассуждений. RKEFino1 также вводит новую задачу числового распознавания именованных сущностей (NER) для финансовых объектов в предложениях и таблицах."
                },
                "en": {
                    "title": "Enhancing Financial Compliance with RKEFino1",
                    "desc": "RKEFino1 is a financial reasoning model designed to improve accuracy and compliance in Digital Regulatory Reporting (DRR). It enhances the Fino1 model by incorporating domain knowledge from XBRL, CDM, and MOF, which are essential for understanding financial regulations. The model introduces a new task called Numerical Named Entity Recognition (NER) to identify financial entities in both text and tabular formats. Experimental results show that RKEFino1 effectively addresses compliance challenges and generalizes well to various financial tasks."
                },
                "zh": {
                    "title": "知识增强的金融推理，提升合规性与准确性",
                    "desc": "RKEFino1是一种增强知识的金融推理模型，旨在解决数字监管报告中的准确性和合规性挑战。该模型基于Fino1，并通过XBRL、CDM和MOF等领域知识进行微调。我们提出了两个问答任务——基于知识的问答和数学推理，并引入了一种新的数值命名实体识别任务，涵盖了句子和表格中的金融实体。实验结果表明，RKEFino1在合规性关键的金融任务中表现出色，具有良好的泛化能力。"
                }
            }
        }
    ],
    "link_prev": "2025-06-10.html",
    "link_next": "2025-06-12.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "10.06",
        "en": "06/10",
        "zh": "6月10日"
    },
    "short_date_next": {
        "ru": "12.06",
        "en": "06/12",
        "zh": "6月12日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 3,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 5,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 6,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种叫做强化预训练（RPT）的新方法。它把下一个词预测当作强化学习任务来训练语言模型。这样可以提高预测的准确性，并为进一步的微调提供坚实基础。RPT利用大量文本数据，不依赖特定领域的标注答案。实验结果显示，增加训练计算量可以提高预测准确性。",
        "title": "Reinforcement Pre-Training",
        "pinyin": "这篇文章介绍了一种叫做强化预训练（RPT）的新方法。\nZhè piān wénzhāng jièshào le yī zhǒng jiàozuò qiáng huà yù xùnliàn (RPT) de xīn fāngfǎ.\n\n它把下一个词预测当作强化学习任务来训练语言模型。\nTā bǎ xià yī gè cí yùcè dāngzuò qiáng huà xuéxí rènwù lái xùnliàn yǔyán móxíng.\n\n这样可以提高预测的准确性，并为进一步的微调提供坚实基础。\nZhèyàng kěyǐ tígāo yùcè de zhǔnquèxìng, bìng wèi jìn yī bù de wēi tiáo tígōng jiānshí jīchǔ.\n\nRPT利用大量文本数据，不依赖特定领域的标注答案。\nRPT lìyòng dàliàng wénběn shùjù, bù yīlài tèdìng lǐngyù de biāozhù dá'àn.\n\n实验结果显示，增加训练计算量可以提高预测准确性。\nShíyàn jiéguǒ xiǎnshì, zēngjiā xùnliàn jìsuàn liàng kěyǐ tígāo yùcè zhǔnquèxìng.",
        "vocab": "[\n    {\"word\": \"强化\", \"pinyin\": \"qiáng huà\", \"trans\": \"reinforcement\"},\n    {\"word\": \"预训练\", \"pinyin\": \"yù xùn liàn\", \"trans\": \"pre-training\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"预测\", \"pinyin\": \"yù cè\", \"trans\": \"prediction\"},\n    {\"word\": \"任务\", \"pinyin\": \"rèn wu\", \"trans\": \"task\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"准确性\", \"pinyin\": \"zhǔn què xìng\", \"trans\": \"accuracy\"},\n    {\"word\": \"微调\", \"pinyin\": \"wēi tiáo\", \"trans\": \"fine-tuning\"},\n    {\"word\": \"坚实\", \"pinyin\": \"jiān shí\", \"trans\": \"solid\"},\n    {\"word\": \"基础\", \"pinyin\": \"jī chǔ\", \"trans\": \"foundation\"},\n    {\"word\": \"利用\", \"pinyin\": \"lì yòng\", \"trans\": \"utilize\"},\n    {\"word\": \"文本\", \"pinyin\": \"wén běn\", \"trans\": \"text\"},\n    {\"word\": \"数据\", \"pinyin\": \"shù jù\", \"trans\": \"data\"},\n    {\"word\": \"依赖\", \"pinyin\": \"yī lài\", \"trans\": \"depend on\"},\n    {\"word\": \"特定\", \"pinyin\": \"tè dìng\", \"trans\": \"specific\"},\n    {\"word\": \"领域\", \"pinyin\": \"lǐng yù\", \"trans\": \"field\"},\n    {\"word\": \"标注\", \"pinyin\": \"biāo zhù\", \"trans\": \"annotation\"},\n    {\"word\": \"答案\", \"pinyin\": \"dá àn\", \"trans\": \"answer\"},\n    {\"word\": \"实验\", \"pinyin\": \"shí yàn\", \"trans\": \"experiment\"},\n    {\"word\": \"结果\", \"pinyin\": \"jié guǒ\", \"trans\": \"result\"},\n    {\"word\": \"显示\", \"pinyin\": \"xiǎn shì\", \"trans\": \"show\"},\n    {\"word\": \"增加\", \"pinyin\": \"zēng jiā\", \"trans\": \"increase\"},\n    {\"word\": \"计算量\", \"pinyin\": \"jì suàn liàng\", \"trans\": \"computational amount\"}\n]",
        "trans": "This article introduces a new method called Reinforced Pre-Training (RPT). It trains language models by treating next-word prediction as a reinforcement learning task. This approach improves the accuracy of predictions and provides a solid foundation for further fine-tuning. RPT leverages a large amount of text data and does not rely on domain-specific annotated answers. Experimental results show that increasing the amount of training computation can enhance prediction accuracy.",
        "update_ts": "2025-06-10 09:13"
    }
}