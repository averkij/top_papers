{
    "date": {
        "ru": "30 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 30",
        "zh": "1æœˆ30æ—¥"
    },
    "time_utc": "2026-01-30 22:28",
    "weekday": 4,
    "issue_id": 860,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2601.20833",
            "title": "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives",
            "url": "https://huggingface.co/papers/2601.20833",
            "abstract": "Offline knowledge construction through structured methodological graphs enables more reliable and scalable autonomous scientific discovery by reducing reliance on real-time literature processing.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous scientific discovery with large language model (LLM)-based agents has recently made substantial progress, demonstrating the ability to automate end-to-end research workflows. However, existing systems largely rely on runtime-centric execution paradigms, repeatedly reading, summarizing, and reasoning over large volumes of scientific literature online. This on-the-spot computation strategy incurs high computational cost, suffers from context window limitations, and often leads to brittle reasoning and hallucination. We propose Idea2Story, a pre-computation-driven framework for autonomous scientific discovery that shifts literature understanding from online reasoning to offline knowledge construction. Idea2Story continuously collects peer-reviewed papers together with their review feedback, extracts core methodological units, composes reusable research patterns, and organizes them into a structured methodological knowledge graph. At runtime, underspecified user research intents are aligned to established research paradigms, enabling efficient retrieval and reuse of high-quality research patterns instead of open-ended generation and trial-and-error. By grounding research planning and execution in a pre-built knowledge graph, Idea2Story alleviates the context window bottleneck of LLMs and substantially reduces repeated runtime reasoning over literature. We conduct qualitative analyses and preliminary empirical studies demonstrating that Idea2Story can generate coherent, methodologically grounded, and novel research patterns, and can produce several high-quality research demonstrations in an end-to-end setting. These results suggest that offline knowledge construction provides a practical and scalable foundation for reliable autonomous scientific discovery.",
            "score": 112,
            "issue_id": 844,
            "pub_date": "2026-01-28",
            "pub_date_card": {
                "ru": "28 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 28",
                "zh": "1æœˆ28æ—¥"
            },
            "hash": "d6ca1882d6d13890",
            "authors": [
                "Tengyue Xu",
                "Zhuoyang Qian",
                "Gaoge Liu",
                "Li Ling",
                "Zhentao Zhang",
                "Biao Wu",
                "Shuo Zhang",
                "Ke Lu",
                "Wei Shi",
                "Ziqi Wang",
                "Zheng Feng",
                "Yan Luo",
                "Shu Xu",
                "Yongjin Chen",
                "Zhibo Feng",
                "Zhuo Chen",
                "Bruce Yuan",
                "Harry Wang",
                "Kris Chen"
            ],
            "affiliations": [
                "AgentAlpha Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.20833.jpg",
            "data": {
                "categories": [
                    "#graphs",
                    "#hallucinations",
                    "#long_context",
                    "#science"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Idea2Story â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ Ñ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-ÑÑ‚Ğ°Ğ¿. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ñ‹ Ğ¸ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¸Ñ… Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ñ„ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. ĞĞ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ°Ğ¼Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹, Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° LLM Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Research: From Real-Time Reading to Offline Knowledge Construction",
                    "desc": "The paper introduces Idea2Story, a framework designed to enhance autonomous scientific discovery by shifting from real-time literature processing to offline knowledge construction. It builds a structured methodological knowledge graph by collecting peer-reviewed papers and extracting core research patterns, which can be reused efficiently. This approach reduces the computational burden and limitations associated with large language models (LLMs) that rely on online reasoning. The results indicate that Idea2Story can generate coherent and innovative research patterns, making it a scalable solution for reliable scientific exploration."
                },
                "zh": {
                    "title": "ç¦»çº¿çŸ¥è¯†æ„å»ºåŠ©åŠ›è‡ªä¸»ç§‘å­¦å‘ç°",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºIdea2Storyçš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç»“æ„åŒ–çš„æ–¹æ³•è®ºå›¾è°±å®ç°ç¦»çº¿çŸ¥è¯†æ„å»ºï¼Œä»è€Œæé«˜è‡ªä¸»ç§‘å­¦å‘ç°çš„å¯é æ€§å’Œå¯æ‰©å±•æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡æ”¶é›†åŒè¡Œè¯„å®¡çš„è®ºæ–‡åŠå…¶åé¦ˆï¼Œæå–æ ¸å¿ƒæ–¹æ³•å•å…ƒï¼Œå¹¶å°†å…¶ç»„ç»‡æˆå¯é‡ç”¨çš„ç ”ç©¶æ¨¡å¼ã€‚ä¸ä¼ ç»Ÿçš„åœ¨çº¿æ¨ç†æ–¹æ³•ä¸åŒï¼ŒIdea2Storyåœ¨è¿è¡Œæ—¶å°†ç”¨æˆ·çš„ç ”ç©¶æ„å›¾ä¸å·²å»ºç«‹çš„ç ”ç©¶èŒƒå¼å¯¹é½ï¼Œä»è€Œé«˜æ•ˆæ£€ç´¢å’Œé‡ç”¨é«˜è´¨é‡çš„ç ”ç©¶æ¨¡å¼ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç¦»çº¿çŸ¥è¯†æ„å»ºä¸ºå¯é çš„è‡ªä¸»ç§‘å­¦å‘ç°æä¾›äº†å®ç”¨ä¸”å¯æ‰©å±•çš„åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.20354",
            "title": "Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models",
            "url": "https://huggingface.co/papers/2601.20354",
            "abstract": "A new benchmark and dataset are introduced to evaluate and improve spatial reasoning capabilities in text-to-image models through information-dense prompts and fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.",
            "score": 96,
            "issue_id": 845,
            "pub_date": "2026-01-28",
            "pub_date_card": {
                "ru": "28 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 28",
                "zh": "1æœˆ28æ—¥"
            },
            "hash": "ad252d3daca97ee9",
            "authors": [
                "Zengbin Wang",
                "Xuecai Hu",
                "Yong Wang",
                "Feng Xiong",
                "Man Zhang",
                "Xiangxiang Chu"
            ],
            "affiliations": [
                "AMAP, Alibaba Group",
                "Beijing University of Posts and Telecommunications"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.20354.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#synthetic",
                    "#reasoning",
                    "#dataset",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² text-to-image Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SpatialGenEval Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ text-to-image Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 1,230 Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 25 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¾ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ SpatialT2I Ğ¸Ğ· 15,400 Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ´Ğ»Ñ fine-tuning Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° 4-6% Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… foundation models."
                },
                "en": {
                    "title": "Enhancing Spatial Reasoning in Text-to-Image Models",
                    "desc": "This paper presents a new benchmark called SpatialGenEval to assess and enhance the spatial reasoning abilities of text-to-image (T2I) models. It introduces 1,230 detailed prompts that cover various spatial aspects, such as object positioning and interactions, to evaluate how well these models understand complex spatial relationships. The study also creates the SpatialT2I dataset, which includes 15,400 text-image pairs designed to maintain information density while ensuring image consistency. Fine-tuning existing models with this dataset shows significant improvements in their ability to generate realistic spatial relations, indicating the importance of data quality in developing spatial intelligence in T2I systems."
                },
                "zh": {
                    "title": "æå‡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„åŸºå‡†å’Œæ•°æ®é›†ï¼Œæ—¨åœ¨é€šè¿‡ä¿¡æ¯å¯†é›†çš„æç¤ºå’Œå¾®è°ƒæ¥è¯„ä¼°å’Œæå‡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚ç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨å¤„ç†å¤æ‚çš„ç©ºé—´å…³ç³»æ—¶å¸¸å¸¸è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬æå‡ºçš„SpatialGenEvalåŸºå‡†åŒ…å«1230ä¸ªé•¿çš„ã€ä¿¡æ¯å¯†é›†çš„æç¤ºï¼Œæ¶µç›–25ä¸ªçœŸå®åœºæ™¯ï¼Œç³»ç»Ÿè¯„ä¼°æ¨¡å‹çš„ç©ºé—´æ™ºèƒ½ã€‚é€šè¿‡å¯¹21ä¸ªæœ€å…ˆè¿›æ¨¡å‹çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°é«˜é˜¶ç©ºé—´æ¨ç†ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦ç“¶é¢ˆï¼Œå¹¶é€šè¿‡æ„å»ºSpatialT2Iæ•°æ®é›†æ¥å±•ç¤ºä¿¡æ¯å¯†é›†è®¾è®¡çš„å®ç”¨æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.21204",
            "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
            "url": "https://huggingface.co/papers/2601.21204",
            "abstract": "Embedding scaling offers superior sparsity scaling compared to expert scaling in large language models, enabling efficient inference through system optimizations and speculative decoding.  \t\t\t\t\tAI-generated summary \t\t\t\t While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.",
            "score": 75,
            "issue_id": 843,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "eb23acf71f024468",
            "authors": [
                "Hong Liu",
                "Jiaqi Zhang",
                "Chao Wang",
                "Xing Hu",
                "Linkun Lyu",
                "Jiaqi Sun",
                "Xurui Yang",
                "Bo Wang",
                "Fengcun Li",
                "Yulei Qian",
                "Lingtong Si",
                "Yerui Sun",
                "Rumei Li",
                "Peng Pei",
                "Yuchen Xie",
                "Xunliang Cai"
            ],
            "affiliations": [
                "Meituan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.21204.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#training",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ­Ğ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² ĞºĞ°Ğº Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Pareto Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Mixture-of-Experts. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ LongCat-Flash-Lite Ñ 68.5B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ MoE Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Unlocking Efficiency: Embedding Scaling Outshines Expert Scaling in Language Models",
                    "desc": "This paper investigates embedding scaling as an alternative to the traditional Mixture-of-Experts (MoE) approach for achieving sparsity in large language models. The authors demonstrate that embedding scaling can lead to better performance in specific scenarios, providing a more efficient Pareto frontier compared to expert scaling. They analyze key architectural factors that influence the effectiveness of embedding scaling, such as parameter budgeting and model dimensions. The proposed LongCat-Flash-Lite model, with 68.5 billion parameters and optimized for inference speed, outperforms MoE models and shows strong results in various applications."
                },
                "zh": {
                    "title": "åµŒå…¥ç¼©æ”¾ï¼šå¤§è¯­è¨€æ¨¡å‹çš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åµŒå…¥ç¼©æ”¾åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œæå‡ºå…¶åœ¨ç¨€ç–æ€§ç¼©æ”¾æ–¹é¢ä¼˜äºä¸“å®¶ç¼©æ”¾ã€‚é€šè¿‡å…¨é¢çš„åˆ†æå’Œå®éªŒï¼Œç ”ç©¶å‘ç°åµŒå…¥ç¼©æ”¾åœ¨ç‰¹å®šæ¡ä»¶ä¸‹èƒ½å¤Ÿå®ç°æ›´ä¼˜çš„å¸•ç´¯æ‰˜å‰æ²¿ã€‚è®ºæ–‡è¿˜ç³»ç»Ÿåœ°æè¿°äº†å½±å“åµŒå…¥ç¼©æ”¾æ•ˆæœçš„å…³é”®æ¶æ„å› ç´ ï¼ŒåŒ…æ‹¬å‚æ•°é¢„ç®—ã€æ¨¡å‹å®½åº¦å’Œæ·±åº¦çš„ç›¸äº’ä½œç”¨ã€‚æœ€ç»ˆï¼Œä½œè€…ä»‹ç»äº†LongCat-Flash-Liteæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ç¨€ç–æ€§å’Œæ¨ç†é€Ÿåº¦ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨æ™ºèƒ½ä½“å’Œç¼–ç é¢†åŸŸå…·æœ‰ç«äº‰åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.22153",
            "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
            "url": "https://huggingface.co/papers/2601.22153",
            "abstract": "DynamicVLA addresses dynamic object manipulation challenges through a compact vision-language-action model with temporal reasoning and closed-loop adaptation, supported by a new benchmark for dynamic manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.",
            "score": 45,
            "issue_id": 842,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "61678bc0144c3a09",
            "authors": [
                "Haozhe Xie",
                "Beichen Wen",
                "Jiarui Zheng",
                "Zhaoxi Chen",
                "Fangzhou Hong",
                "Haiwen Diao",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.22153.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#benchmark",
                    "#robotics",
                    "#inference",
                    "#architecture",
                    "#transfer_learning",
                    "#small_models",
                    "#synthetic"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "DynamicVLA â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ vision-language-action Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 0.4B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Dynamic Object Manipulation Ñ 200K ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ 2K Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¾Ğ², ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° Ğ±ĞµĞ· Ñ‚ĞµĞ»ĞµĞ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ĞºĞ»Ğ¸ĞºĞ°, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "DynamicVLA: Mastering Dynamic Object Manipulation with Speed and Precision",
                    "desc": "DynamicVLA is a new framework designed to improve the manipulation of moving objects using a vision-language-action (VLA) model. It incorporates temporal reasoning and closed-loop adaptation, allowing for faster and more accurate responses to dynamic environments. The model is compact and efficient, utilizing a convolutional vision encoder for quick multimodal inference and continuous reasoning during execution. Additionally, the introduction of the Dynamic Object Manipulation (DOM) benchmark provides a robust dataset for training and evaluating dynamic manipulation tasks."
                },
                "zh": {
                    "title": "åŠ¨æ€ç‰©ä½“æ“æ§çš„æ–°çªç ´",
                    "desc": "DynamicVLAæ˜¯ä¸€ä¸ªé’ˆå¯¹åŠ¨æ€ç‰©ä½“æ“æ§çš„æ¡†æ¶ï¼Œç»“åˆäº†æ—¶é—´æ¨ç†å’Œé—­ç¯é€‚åº”èƒ½åŠ›ã€‚å®ƒé€šè¿‡ä¸€ä¸ªç´§å‡‘çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œèƒ½å¤Ÿå¿«é€Ÿè¿›è¡Œå¤šæ¨¡æ€æ¨ç†ï¼Œå¹¶åœ¨åŠ¨æ€åœºæ™¯ä¸­å®ç°ä½å»¶è¿Ÿçš„æ¨ç†ä¸æ‰§è¡Œã€‚è¯¥æ¡†æ¶è¿˜å¼•å…¥äº†åŠ¨æ€ç‰©ä½“æ“æ§åŸºå‡†ï¼Œæ”¶é›†äº†å¤§é‡åˆæˆå’ŒçœŸå®ä¸–ç•Œçš„æ•°æ®ï¼Œä»¥æ”¯æŒæ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°ã€‚é€šè¿‡è¿™äº›åˆ›æ–°ï¼ŒDynamicVLAåœ¨å“åº”é€Ÿåº¦ã€æ„ŸçŸ¥èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.21639",
            "title": "OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models",
            "url": "https://huggingface.co/papers/2601.21639",
            "abstract": "OCRVerse is a novel end-to-end OCR method that unifies text-centric and vision-centric approaches through comprehensive data engineering and a two-stage SFT-RL training framework with domain-specific reward strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t The development of large vision language models drives the demand for managing, and applying massive amounts of multimodal data, making OCR technology, which extracts information from visual images, increasingly popular. However, existing OCR methods primarily focus on recognizing text elements from images or scanned documents (Text-centric OCR), neglecting the identification of visual elements from visually information-dense image sources (Vision-centric OCR), such as charts, web pages and science plots. In reality, these visually information-dense images are widespread on the internet and have significant real-world application value, such as data visualization and web page analysis. In this technical report, we propose OCRVerse, the first holistic OCR method in end-to-end manner that enables unified text-centric OCR and vision-centric OCR. To this end, we constructe comprehensive data engineering to cover a wide range of text-centric documents, such as newspapers, magazines and books, as well as vision-centric rendered composites, including charts, web pages and scientific plots. Moreover, we propose a two-stage SFT-RL multi-domain training method for OCRVerse. SFT directly mixes cross-domain data to train and establish initial domain knowledge, while RL focuses on designing personalized reward strategies for the characteristics of each domain. Specifically, since different domains require various output formats and expected outputs, we provide sufficient flexibility in the RL stage to customize flexible reward signals for each domain, thereby improving cross-domain fusion and avoiding data conflicts. Experimental results demonstrate the effectiveness of OCRVerse, achieving competitive results across text-centric and vision-centric data types, even comparable to large-scale open-source and closed-source models.",
            "score": 41,
            "issue_id": 845,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "acbd589acf4bb214",
            "authors": [
                "Yufeng Zhong",
                "Lei Chen",
                "Xuanle Zhao",
                "Wenkang Han",
                "Liming Zheng",
                "Jing Huang",
                "Deyang Jiang",
                "Yilin Cao",
                "Lin Ma",
                "Zhixiong Zeng"
            ],
            "affiliations": [
                "Meituan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.21639.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#cv",
                    "#open_source",
                    "#dataset",
                    "#science",
                    "#training",
                    "#optimization",
                    "#rl",
                    "#multimodal"
                ],
                "emoji": "ğŸ“„",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸",
                    "desc": "OCRVerse Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ supervised fine-tuning (SFT) Ğ¸ reinforcement learning (RL), Ğ³Ğ´Ğµ SFT ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° RL Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ¸Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° (Ğ³Ğ°Ğ·ĞµÑ‚Ñ‹, Ğ¶ÑƒÑ€Ğ½Ğ°Ğ»Ñ‹, ĞºĞ½Ğ¸Ğ³Ğ¸) Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ (Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹, Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹, Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¾Ğ±Ğ¾Ğ¸Ñ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "OCRVerse: Unifying Text and Vision for Enhanced OCR",
                    "desc": "OCRVerse is an innovative end-to-end Optical Character Recognition (OCR) method that integrates both text-centric and vision-centric approaches. It employs a two-stage training framework combining Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) with tailored reward strategies for different domains. This method allows for effective extraction of information from both traditional text documents and complex visual data like charts and web pages. Experimental results show that OCRVerse performs competitively against existing models, demonstrating its versatility in handling diverse multimodal data."
                },
                "zh": {
                    "title": "OCRVerseï¼šç»Ÿä¸€æ–‡æœ¬ä¸è§†è§‰çš„OCRæ–°æ–¹æ³•",
                    "desc": "OCRVerseæ˜¯ä¸€ç§æ–°é¢–çš„ç«¯åˆ°ç«¯å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰æ–¹æ³•ï¼Œå®ƒé€šè¿‡å…¨é¢çš„æ•°æ®å·¥ç¨‹å’Œä¸¤é˜¶æ®µçš„SFT-RLè®­ç»ƒæ¡†æ¶ï¼Œå°†ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒå’Œä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„æ–¹æ³•ç»Ÿä¸€èµ·æ¥ã€‚ç°æœ‰çš„OCRæ–¹æ³•ä¸»è¦å…³æ³¨ä»å›¾åƒæˆ–æ‰«ææ–‡æ¡£ä¸­è¯†åˆ«æ–‡æœ¬å…ƒç´ ï¼Œè€Œå¿½è§†äº†ä»ä¿¡æ¯å¯†é›†çš„è§†è§‰å›¾åƒæºï¼ˆå¦‚å›¾è¡¨å’Œç½‘é¡µï¼‰ä¸­è¯†åˆ«è§†è§‰å…ƒç´ ã€‚OCRVerseé€šè¿‡æ„å»ºå…¨é¢çš„æ•°æ®å·¥ç¨‹ï¼Œæ¶µç›–äº†å¹¿æ³›çš„æ–‡æœ¬å’Œè§†è§‰æ–‡æ¡£ï¼Œæä¾›äº†è·¨é¢†åŸŸçš„è®­ç»ƒæ–¹æ³•ï¼Œä»¥é€‚åº”ä¸åŒé¢†åŸŸçš„è¾“å‡ºæ ¼å¼å’ŒæœŸæœ›ç»“æœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOCRVerseåœ¨æ–‡æœ¬å’Œè§†è§‰æ•°æ®ç±»å‹ä¸Šéƒ½å–å¾—äº†ç«äº‰åŠ›çš„æ•ˆæœï¼Œç”šè‡³ä¸å¤§å‹å¼€æºå’Œé—­æºæ¨¡å‹ç›¸åª²ç¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.21821",
            "title": "MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods",
            "url": "https://huggingface.co/papers/2601.21821",
            "abstract": "A large-scale multimodal reasoning dataset called MMFineReason is introduced to improve vision language models' performance through high-quality reasoning annotations and demonstrates superior parameter efficiency in fine-tuned models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Vision Language Models (VLMs) have driven significant progress in visual reasoning. However, open-source VLMs still lag behind proprietary systems, largely due to the lack of high-quality reasoning data. Existing datasets offer limited coverage of challenging domains such as STEM diagrams and visual puzzles, and lack consistent, long-form Chain-of-Thought (CoT) annotations essential for eliciting strong reasoning capabilities. To bridge this gap, we introduce MMFineReason, a large-scale multimodal reasoning dataset comprising 1.8M samples and 5.1B solution tokens, featuring high-quality reasoning annotations distilled from Qwen3-VL-235B-A22B-Thinking. The dataset is established via a systematic three-stage pipeline: (1) large-scale data collection and standardization, (2) CoT rationale generation, and (3) comprehensive selection based on reasoning quality and difficulty awareness. The resulting dataset spans STEM problems, visual puzzles, games, and complex diagrams, with each sample annotated with visually grounded reasoning traces. We fine-tune Qwen3-VL-Instruct on MMFineReason to develop MMFineReason-2B/4B/8B versions. Our models establish new state-of-the-art results for their size class. Notably, MMFineReason-4B succesfully surpasses Qwen3-VL-8B-Thinking, and MMFineReason-8B even outperforms Qwen3-VL-30B-A3B-Thinking while approaching Qwen3-VL-32B-Thinking, demonstrating remarkable parameter efficiency. Crucially, we uncover a \"less is more\" phenomenon via our difficulty-aware filtering strategy: a subset of just 7\\% (123K samples) achieves performance comparable to the full dataset. Notably, we reveal a synergistic effect where reasoning-oriented data composition simultaneously boosts general capabilities.",
            "score": 25,
            "issue_id": 842,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "718b1a94a389896b",
            "authors": [
                "Honglin Lin",
                "Zheng Liu",
                "Yun Zhu",
                "Chonghan Qin",
                "Juekai Lin",
                "Xiaoran Shang",
                "Conghui He",
                "Wentao Zhang",
                "Lijun Wu"
            ],
            "affiliations": [
                "OpenDataLab",
                "Peking University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.21821.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#reasoning",
                    "#multimodal",
                    "#open_source",
                    "#training",
                    "#small_models",
                    "#synthetic"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ’Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´ĞµĞ»Ğ°ÑÑ‚ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒĞ¼Ğ½ĞµĞµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ…",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ MMFineReason â€” Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ 1.8M Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ vision language models Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought). Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ±Ñ€Ğ°Ğ½ Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€: ÑĞ±Ğ¾Ñ€ Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ STEM, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼ĞºĞ¸ Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹. ĞŸÑ€Ğ¸ fine-tuning Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Qwen3-VL Ğ½Ğ° MMFineReason ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹Ğ´Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²: Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (4B Ğ¸ 8B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚ Â«Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞµÂ»: Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ²ÑĞµĞ³Ğ¾ 7% Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ÑƒÑ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Unlocking Visual Reasoning with MMFineReason",
                    "desc": "The paper introduces MMFineReason, a large-scale multimodal reasoning dataset designed to enhance the performance of vision language models (VLMs) through high-quality reasoning annotations. It addresses the limitations of existing datasets by providing extensive coverage of challenging domains like STEM diagrams and visual puzzles, along with consistent Chain-of-Thought (CoT) annotations. The dataset consists of 1.8 million samples and 5.1 billion solution tokens, created through a systematic pipeline that ensures high reasoning quality. Fine-tuning models on this dataset demonstrates significant improvements in parameter efficiency, achieving state-of-the-art results and revealing that a smaller, carefully selected subset of data can yield comparable performance to larger datasets."
                },
                "zh": {
                    "title": "æå‡è§†è§‰è¯­è¨€æ¨¡å‹æ€§èƒ½çš„MMFineReasonæ•°æ®é›†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºMMFineReasonçš„å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨ç†æ•°æ®é›†ï¼Œæ—¨åœ¨é€šè¿‡é«˜è´¨é‡çš„æ¨ç†æ³¨é‡Šæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ•°æ®é›†åŒ…å«180ä¸‡æ ·æœ¬å’Œ51äº¿ä¸ªè§£å†³æ–¹æ¡ˆæ ‡è®°ï¼Œæ¶µç›–STEMé—®é¢˜ã€è§†è§‰éš¾é¢˜å’Œå¤æ‚å›¾è¡¨ç­‰é¢†åŸŸã€‚é€šè¿‡ç³»ç»Ÿçš„ä¸‰é˜¶æ®µæµç¨‹å»ºç«‹æ•°æ®é›†ï¼Œç¡®ä¿äº†æ¨ç†è´¨é‡å’Œéš¾åº¦æ„è¯†ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„æ¨¡å‹åœ¨å‚æ•°æ•ˆç‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œéƒ¨åˆ†æ ·æœ¬çš„é€‰æ‹©ç­–ç•¥æ˜¾ç¤ºå‡ºâ€œå°‘å³æ˜¯å¤šâ€çš„ç°è±¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.21420",
            "title": "ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation",
            "url": "https://huggingface.co/papers/2601.21420",
            "abstract": "ConceptMoE dynamically allocates computation by merging similar tokens into concept representations, improving both performance and efficiency in large language models through adaptive processing and reduced attention computation.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models allocate uniform computation across all tokens, ignoring that some sequences are trivially predictable while others require deep reasoning. We introduce ConceptMoE, which dynamically merges semantically similar tokens into concept representations, performing implicit token-level compute allocation. A learnable chunk module identifies optimal boundaries by measuring inter-token similarity, compressing sequences by a target ratio R before they enter the compute-intensive concept model. Crucially, the MoE architecture enables controlled evaluation: we reallocate saved computation to match baseline activated FLOPs (excluding attention map computation) and total parameters, isolating genuine architectural benefits. Under these conditions, ConceptMoE consistently outperforms standard MoE across language and vision-language tasks, achieving +0.9 points on language pretraining, +2.3 points on long context understanding, and +0.6 points on multimodal benchmarks. When converting pretrained MoE during continual training with layer looping, gains reach +5.5 points, demonstrating practical applicability. Beyond performance, ConceptMoE reduces attention computation by up to R^2times and KV cache by Rtimes. At R=2, empirical measurements show prefill speedups reaching 175\\% and decoding speedups up to 117\\% on long sequences. The minimal architectural modifications enable straightforward integration into existing MoE, demonstrating that adaptive concept-level processing fundamentally improves both effectiveness and efficiency of large language models.",
            "score": 22,
            "issue_id": 843,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "ac9a729c6bbb7bea",
            "authors": [
                "Zihao Huang",
                "Jundong Zhou",
                "Xingwei Qu",
                "Qiyang Min",
                "Ge Zhang"
            ],
            "affiliations": [
                "Bytedance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.21420.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#multimodal",
                    "#architecture",
                    "#training"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ConceptMoE â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ĞµĞ¶Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ°, ÑĞ¶Ğ¸Ğ¼Ğ°Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Mixture of Experts Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ°Ğ»Ğ»Ğ¾Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ° KV-ĞºÑÑˆĞ°."
                },
                "en": {
                    "title": "Dynamic Token Merging for Efficient Language Processing",
                    "desc": "ConceptMoE is a novel approach that enhances large language models by dynamically merging similar tokens into concept representations, which allows for more efficient computation. This method addresses the issue of uniform computation allocation by adapting to the complexity of different token sequences, leading to improved performance in various tasks. A learnable chunk module identifies optimal token boundaries, compressing sequences before they undergo intensive processing, which significantly reduces attention computation. Overall, ConceptMoE not only boosts model performance but also accelerates processing speed, making it a valuable advancement in the field of machine learning."
                },
                "zh": {
                    "title": "åŠ¨æ€è®¡ç®—åˆ†é…ï¼Œæå‡è¯­è¨€æ¨¡å‹æ•ˆç‡",
                    "desc": "ConceptMoEé€šè¿‡å°†ç›¸ä¼¼çš„æ ‡è®°åˆå¹¶ä¸ºæ¦‚å¿µè¡¨ç¤ºï¼ŒåŠ¨æ€åˆ†é…è®¡ç®—èµ„æºï¼Œä»è€Œæé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªå¯å­¦ä¹ çš„æ¨¡å—ï¼Œèƒ½å¤Ÿæ ¹æ®æ ‡è®°é—´çš„ç›¸ä¼¼æ€§ç¡®å®šæœ€ä½³è¾¹ç•Œï¼Œå‹ç¼©è¾“å…¥åºåˆ—ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒConceptMoEåœ¨è¯­è¨€å’Œè§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºæ ‡å‡†çš„MoEæ¶æ„ï¼Œæ˜¾è‘—æå‡äº†é¢„è®­ç»ƒå’Œé•¿ä¸Šä¸‹æ–‡ç†è§£çš„æ•ˆæœã€‚æ­¤å¤–ï¼ŒConceptMoEè¿˜å‡å°‘äº†æ³¨æ„åŠ›è®¡ç®—çš„éœ€æ±‚ï¼Œæå‡äº†å¤„ç†é€Ÿåº¦ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.22046",
            "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
            "url": "https://huggingface.co/papers/2601.22046",
            "abstract": "PLANING presents an efficient streaming reconstruction framework that combines explicit geometric primitives with neural Gaussians to achieve high-quality rendering and accurate geometry simultaneously through decoupled optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: https://city-super.github.io/PLANING/ .",
            "score": 19,
            "issue_id": 846,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "e87734e7dba01e32",
            "authors": [
                "Changjian Jiang",
                "Kerui Ren",
                "Xudong Li",
                "Kaiwen Song",
                "Linning Xu",
                "Tao Lu",
                "Junting Dong",
                "Yu Zhang",
                "Bo Dai",
                "Mulin Yu"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "The University of Hong Kong",
                "The University of Science and Technology of China",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.22046.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½",
                    "desc": "PLANING â€” ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ²Ğ½Ñ‹Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ñ‹ Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ°Ğ¼Ğ¸. Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ. PLANING Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸, Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒÑ ÑÑ†ĞµĞ½Ñ‹ Ğ² 5 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ², Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾ÑÑ‚Ğ°Ğ²Ğ°ÑÑÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾Ğ¹ Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½."
                },
                "en": {
                    "title": "Efficient Streaming Reconstruction with PLANING: Quality Meets Speed!",
                    "desc": "PLANING is a novel streaming reconstruction framework that effectively combines explicit geometric shapes with neural Gaussian representations. This approach allows for high-quality rendering and precise geometry to be achieved simultaneously through a process of decoupled optimization. By separating the updates for geometry and appearance, PLANING enhances the stability of streaming reconstruction while minimizing structural redundancy. The framework demonstrates significant improvements in reconstruction metrics and speed, making it suitable for various applications in scene modeling and AI simulations."
                },
                "zh": {
                    "title": "é«˜æ•ˆæµå¼é‡å»ºï¼Œè§£è€¦ä¼˜åŒ–æ–°æ–¹æ³•",
                    "desc": "PLANINGæ˜¯ä¸€ç§é«˜æ•ˆçš„æµå¼é‡å»ºæ¡†æ¶ï¼Œå®ƒç»“åˆäº†æ˜¾å¼å‡ ä½•åŸè¯­å’Œç¥ç»é«˜æ–¯æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶å®ç°é«˜è´¨é‡æ¸²æŸ“å’Œå‡†ç¡®å‡ ä½•çš„é‡å»ºã€‚è¯¥æ–¹æ³•é€šè¿‡è§£è€¦ä¼˜åŒ–ï¼Œä½¿å‡ ä½•å’Œå¤–è§‚çš„å»ºæ¨¡è¿‡ç¨‹ç›¸äº’ç‹¬ç«‹ï¼Œä»è€Œæ”¯æŒåœ¨çº¿åˆå§‹åŒ–å’Œä¼˜åŒ–ç­–ç•¥ã€‚PLANINGåœ¨é‡å»ºè´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨100ç§’å†…é‡å»ºScanNetV2åœºæ™¯ï¼Œé€Ÿåº¦æ¯”2Dé«˜æ–¯ç‚¹äº‘å¿«5å€ä»¥ä¸Šã€‚å…¶ç»“æ„æ¸…æ™°å’Œè®¡ç®—æ•ˆç‡ä½¿å…¶é€‚ç”¨äºå¤§è§„æ¨¡åœºæ™¯å»ºæ¨¡å’Œé€‚åˆæ¨¡æ‹Ÿçš„ç¯å¢ƒï¼Œä¸ºåµŒå…¥å¼äººå·¥æ™ºèƒ½æä¾›äº†è‰¯å¥½çš„åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.21337",
            "title": "Qwen3-ASR Technical Report",
            "url": "https://huggingface.co/papers/2601.21337",
            "abstract": "The Qwen3-ASR family introduces speech recognition models with language identification capabilities and a non-autoregressive forced alignment model, achieving state-of-the-art performance and efficient processing.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we introduce Qwen3-ASR family, which includes two powerful all-in-one speech recognition models and a novel non-autoregressive speech forced alignment model. Qwen3-ASR-1.7B and Qwen3-ASR-0.6B are ASR models that support language identification and ASR for 52 languages and dialects. Both of them leverage large-scale speech training data and the strong audio understanding ability of their foundation model Qwen3-Omni. We conduct comprehensive internal evaluation besides the open-sourced benchmarks as ASR models might differ little on open-sourced benchmark scores but exhibit significant quality differences in real-world scenarios. The experiments reveal that the 1.7B version achieves SOTA performance among open-sourced ASR models and is competitive with the strongest proprietary APIs while the 0.6B version offers the best accuracy-efficiency trade-off. Qwen3-ASR-0.6B can achieve an average TTFT as low as 92ms and transcribe 2000 seconds speech in 1 second at a concurrency of 128. Qwen3-ForcedAligner-0.6B is an LLM based NAR timestamp predictor that is able to align text-speech pairs in 11 languages. Timestamp accuracy experiments show that the proposed model outperforms the three strongest force alignment models and takes more advantages in efficiency and versatility. To further accelerate the community research of ASR and audio understanding, we release these models under the Apache 2.0 license.",
            "score": 18,
            "issue_id": 843,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "b3bb3cb725958ca4",
            "authors": [
                "Xian Shi",
                "Xiong Wang",
                "Zhifang Guo",
                "Yongqi Wang",
                "Pei Zhang",
                "Xinyu Zhang",
                "Zishan Guo",
                "Hongkun Hao",
                "Yu Xi",
                "Baosong Yang",
                "Jin Xu",
                "Jingren Zhou",
                "Junyang Lin"
            ],
            "affiliations": [
                "Qwen Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.21337.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multilingual",
                    "#inference",
                    "#low_resource",
                    "#audio",
                    "#open_source"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ½ĞµĞ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ¡ĞµĞ¼ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Qwen3-ASR Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ° Ğ´Ğ»Ñ 52 ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Qwen3-ASR-1.7B Ğ¸ Qwen3-ASR-0.6B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑ Ñ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ API Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ½ĞµĞ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen3-ForcedAligner-0.6B, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº. Ğ’ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸ĞµĞ¹ Apache 2.0 Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ² ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Speech Recognition with Qwen3-ASR Models",
                    "desc": "The Qwen3-ASR family presents advanced speech recognition models that integrate language identification and a non-autoregressive forced alignment model. These models, Qwen3-ASR-1.7B and Qwen3-ASR-0.6B, are designed to recognize speech in 52 languages and dialects, utilizing extensive training data and the capabilities of the Qwen3-Omni foundation model. Evaluation results indicate that the 1.7B model achieves state-of-the-art performance, while the 0.6B model excels in balancing accuracy and processing efficiency. Additionally, the Qwen3-ForcedAligner-0.6B model demonstrates superior timestamp alignment capabilities, enhancing the efficiency and versatility of speech-to-text applications."
                },
                "zh": {
                    "title": "Qwen3-ASRï¼šé«˜æ•ˆçš„è¯­éŸ³è¯†åˆ«ä¸è¯­è¨€è¯†åˆ«ç»“åˆ",
                    "desc": "Qwen3-ASRç³»åˆ—å¼•å…¥äº†å…·æœ‰è¯­è¨€è¯†åˆ«èƒ½åŠ›çš„è¯­éŸ³è¯†åˆ«æ¨¡å‹å’Œä¸€ç§éè‡ªå›å½’å¼ºåˆ¶å¯¹é½æ¨¡å‹ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½å’Œé«˜æ•ˆçš„å¤„ç†èƒ½åŠ›ã€‚è¯¥ç³»åˆ—åŒ…æ‹¬Qwen3-ASR-1.7Bå’ŒQwen3-ASR-0.6Bä¸¤ä¸ªæ¨¡å‹ï¼Œæ”¯æŒ52ç§è¯­è¨€å’Œæ–¹è¨€çš„è¯­éŸ³è¯†åˆ«ã€‚é€šè¿‡å¤§è§„æ¨¡çš„è¯­éŸ³è®­ç»ƒæ•°æ®å’Œå¼ºå¤§çš„éŸ³é¢‘ç†è§£èƒ½åŠ›ï¼Œè¿™äº›æ¨¡å‹åœ¨çœŸå®åœºæ™¯ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„è´¨é‡å·®å¼‚ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†è¿™äº›æ¨¡å‹ï¼Œä»¥ä¿ƒè¿›è¯­éŸ³è¯†åˆ«å’ŒéŸ³é¢‘ç†è§£é¢†åŸŸçš„ç¤¾åŒºç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.20730",
            "title": "AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts",
            "url": "https://huggingface.co/papers/2601.20730",
            "abstract": "AgentLongBench evaluates large language models as autonomous agents through dynamic environment interactions, revealing challenges in handling high-information-density tool responses compared to memory fragmentation in long conversations.  \t\t\t\t\tAI-generated summary \t\t\t\t The evolution of Large Language Models (LLMs) into autonomous agents necessitates the management of extensive, dynamic contexts. Current benchmarks, however, remain largely static, relying on passive retrieval tasks that fail to simulate the complexities of agent-environment interaction, such as non-linear reasoning and iterative feedback. To address this, we introduce AgentLongBench, which evaluates agents through simulated environment rollouts based on Lateral Thinking Puzzles. This framework generates rigorous interaction trajectories across knowledge-intensive and knowledge-free scenarios. Experiments with state-of-the-art models and memory systems (32K to 4M tokens) expose a critical weakness: while adept at static retrieval, agents struggle with the dynamic information synthesis essential for workflows. Our analysis indicates that this degradation is driven by the minimum number of tokens required to resolve a query. This factor explains why the high information density inherent in massive tool responses poses a significantly greater challenge than the memory fragmentation typical of long-turn dialogues.",
            "score": 17,
            "issue_id": 847,
            "pub_date": "2026-01-28",
            "pub_date_card": {
                "ru": "28 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 28",
                "zh": "1æœˆ28æ—¥"
            },
            "hash": "c668980f764a8142",
            "authors": [
                "Shicheng Fang",
                "Yuxin Wang",
                "XiaoRan Liu",
                "Jiahao Lu",
                "Chuanyuan Tan",
                "Xinchi Chen",
                "Yining Zheng",
                "Xuanjing Huang",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Innovation Institute",
                "Soochow University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.20730.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#long_context",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM: Ğ¾Ñ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "AgentLongBench â€” ÑÑ‚Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ¾Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼Ğ¾Ğº, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸ĞµÑ‘Ğ¼ĞºĞ¸Ñ… Ğ¸ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¾Ñ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸: Ñ…Ğ¾Ñ‚Ñ Ğ¾Ğ½Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ½Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¼ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ². ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ¾Ğ¹ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½ĞµĞµ, Ñ‡ĞµĞ¼ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…."
                },
                "en": {
                    "title": "Evaluating LLMs: From Static Retrieval to Dynamic Interaction Challenges",
                    "desc": "The paper introduces AgentLongBench, a new framework for evaluating large language models (LLMs) as autonomous agents in dynamic environments. It highlights the limitations of current benchmarks that focus on static tasks, which do not adequately reflect the complexities of real-world interactions. Through the use of Lateral Thinking Puzzles, AgentLongBench assesses how well agents can manage information in both knowledge-rich and knowledge-poor contexts. The findings reveal that while LLMs excel at retrieving static information, they struggle with synthesizing dynamic information, particularly when faced with high-density tool responses."
                },
                "zh": {
                    "title": "è¯„ä¼°è‡ªä¸»æ™ºèƒ½ä½“çš„åŠ¨æ€ç¯å¢ƒäº¤äº’èƒ½åŠ›",
                    "desc": "AgentLongBench æ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè‡ªä¸»æ™ºèƒ½ä½“çš„å·¥å…·ï¼Œé‡ç‚¹åœ¨äºåŠ¨æ€ç¯å¢ƒäº¤äº’ä¸­çš„è¡¨ç°ã€‚è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦æ˜¯é™æ€çš„ï¼Œæ— æ³•æœ‰æ•ˆæ¨¡æ‹Ÿæ™ºèƒ½ä½“ä¸ç¯å¢ƒä¹‹é—´å¤æ‚çš„äº’åŠ¨ã€‚é€šè¿‡å¼•å…¥åŸºäºä¾§å‘æ€ç»´éš¾é¢˜çš„æ¨¡æ‹Ÿç¯å¢ƒï¼ŒAgentLongBench èƒ½å¤Ÿç”Ÿæˆä¸¥æ ¼çš„äº¤äº’è½¨è¿¹ï¼Œæ­ç¤ºäº†æ™ºèƒ½ä½“åœ¨å¤„ç†é«˜ä¿¡æ¯å¯†åº¦çš„å·¥å…·å“åº”æ—¶çš„æŒ‘æˆ˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡æ™ºèƒ½ä½“åœ¨é™æ€æ£€ç´¢æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åŠ¨æ€ä¿¡æ¯ç»¼åˆæ–¹é¢å´å­˜åœ¨æ˜¾è‘—çš„å¼±ç‚¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.22154",
            "title": "Exploring Reasoning Reward Model for Agents",
            "url": "https://huggingface.co/papers/2601.22154",
            "abstract": "Agent-RRM, a multi-faceted reward model, provides structured feedback for agentic trajectories through reasoning traces, critiques, and performance scores, with unified feedback integration showing superior performance across diverse benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.",
            "score": 16,
            "issue_id": 842,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "3e45765b00a07c6a",
            "authors": [
                "Kaixuan Fan",
                "Kaituo Feng",
                "Manyuan Zhang",
                "Tianshuo Peng",
                "Zhixun Li",
                "Yilei Jiang",
                "Shuang Chen",
                "Peng Pei",
                "Xunliang Cai",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "MMLab, CUHK",
                "Meituan",
                "SEEM, CUHK"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.22154.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#reasoning",
                    "#benchmark",
                    "#open_source",
                    "#rl",
                    "#optimization",
                    "#agents"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Agent-RRM, Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°ÑĞ¿ĞµĞºÑ‚Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ñ€Ğ°ÑÑĞ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… reward-ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ…ĞµĞ¼Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ (Reagent-U), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ²Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ÑÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° 12 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Agent Learning with Structured Feedback",
                    "desc": "The paper introduces Agent-RRM, a new reward model designed to enhance agentic reinforcement learning by providing structured feedback. This model includes three key components: reasoning traces that detail the agent's thought process, critiques that identify and suggest improvements for reasoning errors, and performance scores that assess overall effectiveness. By integrating these feedback elements through various strategies, the authors demonstrate significant improvements in agent performance across multiple benchmarks. The results indicate that using a comprehensive feedback approach leads to better training outcomes compared to traditional sparse reward methods."
                },
                "zh": {
                    "title": "æ™ºèƒ½ä½“æ¨ç†çš„ç»“æ„åŒ–åé¦ˆæ–°æ¨¡å¼",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºAgent-RRMçš„å¤šé¢å‘å¥–åŠ±æ¨¡å‹ï¼Œæ—¨åœ¨ä¸ºæ™ºèƒ½ä½“çš„æ¨ç†è¿‡ç¨‹æä¾›ç»“æ„åŒ–åé¦ˆã€‚è¯¥æ¨¡å‹é€šè¿‡æ˜ç¡®çš„æ¨ç†è½¨è¿¹ã€èšç„¦çš„æ‰¹è¯„å’Œæ•´ä½“è¯„åˆ†æ¥è¯„ä¼°æ™ºèƒ½ä½“çš„è¡¨ç°ï¼Œå¸®åŠ©æå‡è®­ç»ƒæ•ˆæœã€‚ä¸ä¼ ç»Ÿçš„åŸºäºç»“æœçš„ç¨€ç–å¥–åŠ±æ–¹æ³•ç›¸æ¯”ï¼ŒAgent-RRMèƒ½å¤Ÿæ›´å¥½åœ°åŒºåˆ†ä¸­é—´æ¨ç†è´¨é‡ï¼Œä»è€Œä¼˜åŒ–è®­ç»ƒç»“æœã€‚é€šè¿‡åœ¨12ä¸ªä¸åŒåŸºå‡†ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼Œç ”ç©¶è¡¨æ˜ç»Ÿä¸€åé¦ˆé›†æˆç­–ç•¥Reagent-Uæ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼ŒéªŒè¯äº†è¯¥æ¨ç†å¥–åŠ±æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.21754",
            "title": "Language-based Trial and Error Falls Behind in the Era of Experience",
            "url": "https://huggingface.co/papers/2601.21754",
            "abstract": "A novel framework called SCOUT is introduced that uses lightweight scouts to reduce exploration costs for large language models in nonlinguistic environments, enabling improved performance through supervised fine-tuning and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight \"scouts\" (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption.",
            "score": 13,
            "issue_id": 842,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "49a4d251d0b7ad56",
            "authors": [
                "Haoyu Wang",
                "Guozheng Ma",
                "Shugang Cui",
                "Yilun Kong",
                "Haotian Luo",
                "Li Shen",
                "Mengya Gao",
                "Yichao Wu",
                "Xiaogang Wang",
                "Dacheng Tao"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Sense Time",
                "Sun Yat-sen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.21754.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#rl",
                    "#optimization",
                    "#transfer_learning",
                    "#agents",
                    "#small_models"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ²ĞµĞ´Ñ‡Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·ÑƒĞ¼Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ²ĞµĞ´ĞºĞ¸: ĞºĞ°Ğº Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ°Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ…",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº SCOUT, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²-Ñ€Ğ°Ğ·Ğ²ĞµĞ´Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ²Ğ½Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ². ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ² Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹: Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ LLM Ñ‡ĞµÑ€ĞµĞ· supervised fine-tuning Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ reinforcement learning Ğ´Ğ»Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ SCOUT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ°Ğ¶Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-3B Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Gemini-2.5-Pro Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "SCOUT: Efficient Exploration for LLMs in Nonlinguistic Tasks",
                    "desc": "The paper introduces SCOUT, a framework designed to enhance the performance of large language models (LLMs) in nonlinguistic environments by reducing exploration costs. It identifies that the main challenge for LLMs in these tasks is the high computational expense of trial-and-error exploration. SCOUT employs lightweight scouts, which are smaller models that quickly gather data about the environment, allowing the LLM to learn more efficiently. By using supervised fine-tuning and reinforcement learning on the data collected by scouts, SCOUT significantly improves the LLM's performance while conserving computational resources."
                },
                "zh": {
                    "title": "SCOUTï¼šé™ä½æ¢ç´¢æˆæœ¬ï¼Œæå‡æ¨¡å‹æ€§èƒ½",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSCOUTçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä½¿ç”¨è½»é‡çº§çš„ä¾¦å¯Ÿè€…æ¥é™ä½å¤§å‹è¯­è¨€æ¨¡å‹åœ¨éè¯­è¨€ç¯å¢ƒä¸­çš„æ¢ç´¢æˆæœ¬ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†æ¢ç´¢ä¸åˆ©ç”¨è§£è€¦ï¼Œåˆ©ç”¨å°å‹å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰å¿«é€Ÿæ¢æµ‹ç¯å¢ƒåŠ¨æ€ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚SCOUTé€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¤šè½®å¼ºåŒ–å­¦ä¹ ï¼Œåˆ©ç”¨æ”¶é›†åˆ°çš„è½¨è¿¹æ¥æ¿€æ´»å¤§å‹è¯­è¨€æ¨¡å‹çš„æ½œåœ¨çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSCOUTæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„è¡¨ç°ï¼ŒåŒæ—¶èŠ‚çœäº†çº¦60%çš„GPUè®¡ç®—æ—¶é—´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.16914",
            "title": "LoL: Longer than Longer, Scaling Video Generation to Hour",
            "url": "https://huggingface.co/papers/2601.16914",
            "abstract": "Researchers developed a method to overcome sink-collapse in autoregressive video generation by addressing the conflict between Rotary Position Embedding and multi-head attention mechanisms, enabling real-time streaming of videos up to 12 hours long.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent research in long-form video generation has shifted from bidirectional to autoregressive models, yet these methods commonly suffer from error accumulation and a loss of long-term coherence. While attention sink frames have been introduced to mitigate this performance decay, they often induce a critical failure mode we term sink-collapse: the generated content repeatedly reverts to the sink frame, resulting in abrupt scene resets and cyclic motion patterns. Our analysis reveals that sink-collapse originates from an inherent conflict between the periodic structure of Rotary Position Embedding (RoPE) and the multi-head attention mechanisms prevalent in current generative models. To address it, we propose a lightweight, training-free approach that effectively suppresses this behavior by introducing multi-head RoPE jitter that breaks inter-head attention homogenization and mitigates long-horizon collapse. Extensive experiments show that our method successfully alleviates sink-collapse while preserving generation quality. To the best of our knowledge, this work achieves the first demonstration of real-time, streaming, and infinite-length video generation with little quality decay. As an illustration of this robustness, we generate continuous videos up to 12 hours in length, which, to our knowledge, is among the longest publicly demonstrated results in streaming video generation.",
            "score": 13,
            "issue_id": 856,
            "pub_date": "2026-01-23",
            "pub_date_card": {
                "ru": "23 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 23",
                "zh": "1æœˆ23æ—¥"
            },
            "hash": "a43258d61b107f81",
            "authors": [
                "Justin Cui",
                "Jie Wu",
                "Ming Li",
                "Tao Yang",
                "Xiaojie Li",
                "Rui Wang",
                "Andrew Bai",
                "Yuanhao Ban",
                "Cho-Jui Hsieh"
            ],
            "affiliations": [
                "ByteDance Seed",
                "UCLA",
                "University of Central Florida"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.16914.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#inference",
                    "#architecture",
                    "#video",
                    "#optimization"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ‘ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ°: Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ sink-collapse Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ¸Ğ·-Ğ·Ğ° ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Rotary Position Embedding Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ multi-head attention. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ multi-head RoPE jitter Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ³Ğ´Ğ° ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ÑÑ Ğº Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ°Ğ´Ñ€Ñƒ. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¼ Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡ĞµĞ¹ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ¾ 12 Ñ‡Ğ°ÑĞ¾Ğ² Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Breaking the Cycle: Real-Time Long-Form Video Generation Without Sink-Collapse",
                    "desc": "This paper presents a novel method to tackle the issue of sink-collapse in autoregressive video generation, which is a problem where generated content repeatedly returns to a specific frame, causing unnatural motion. The authors identify that this issue arises from a conflict between Rotary Position Embedding (RoPE) and multi-head attention mechanisms used in generative models. To resolve this, they introduce a technique called multi-head RoPE jitter, which disrupts the uniformity of attention across heads and helps maintain coherence over long video sequences. Their approach enables real-time streaming of videos, achieving impressive lengths of up to 12 hours with minimal quality loss, marking a significant advancement in the field of video generation."
                },
                "zh": {
                    "title": "çªç ´è‡ªå›å½’è§†é¢‘ç”Ÿæˆçš„æ²‰æ²¡å´©æºƒ",
                    "desc": "ç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ç§æ–¹æ³•æ¥å…‹æœè‡ªå›å½’è§†é¢‘ç”Ÿæˆä¸­çš„æ²‰æ²¡å´©æºƒé—®é¢˜ï¼Œè§£å†³äº†æ—‹è½¬ä½ç½®åµŒå…¥ä¸å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¹‹é—´çš„å†²çªã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿå®ç°å®æ—¶æµåª’ä½“è§†é¢‘ç”Ÿæˆï¼Œé•¿åº¦å¯è¾¾12å°æ—¶ã€‚é€šè¿‡å¼•å…¥å¤šå¤´æ—‹è½¬ä½ç½®åµŒå…¥æŠ–åŠ¨ï¼Œç ”ç©¶è€…æœ‰æ•ˆæŠ‘åˆ¶äº†æ²‰æ²¡å´©æºƒç°è±¡ï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆé•¿è§†é¢‘æ—¶è¡¨ç°å‡ºè‰²ï¼ŒæˆåŠŸé¿å…äº†å†…å®¹é‡å¤å’Œåœºæ™¯é‡ç½®çš„é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.22157",
            "title": "Discovering Hidden Gems in Model Repositories",
            "url": "https://huggingface.co/papers/2601.22157",
            "abstract": "Hidden superior models exist in public repositories but are overlooked due to inefficient discovery methods; a multi-armed bandit approach using shared query sets and aggressive elimination significantly accelerates identification of top-performing models.  \t\t\t\t\tAI-generated summary \t\t\t\t Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects efficient market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of \"hidden gems\", unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. Our method retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.",
            "score": 11,
            "issue_id": 848,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "9c405dea26083100",
            "authors": [
                "Jonathan Kahana",
                "Eliahu Horwitz",
                "Yedid Hoshen"
            ],
            "affiliations": [
                "Department of Computer Science, School of Computer Science and Engineering, The Hebrew University of Jerusalem, Israel"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.22157.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#benchmark",
                    "#inference"
                ],
                "emoji": "ğŸ’",
                "ru": {
                    "title": "ĞŸĞ¾Ğ¸ÑĞº ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¶ĞµĞ¼Ñ‡ÑƒĞ¶Ğ¸Ğ½: Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€ÑƒĞºĞ¸Ğ¹ Ğ±Ğ°Ğ½Ğ´Ğ¸Ñ‚",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ² Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ÑÑ… Ğ½Ğ°Ñ…Ğ¾Ğ´ÑÑ‚ÑÑ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ğ½ĞµĞ·Ğ°Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·-Ğ·Ğ° Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 2000 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Â«ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¶ĞµĞ¼Ñ‡ÑƒĞ¶Ğ¸Ğ½Ñ‹Â» â€” Ñ€ĞµĞ´ĞºĞ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ½Ñ‚ÑĞ½Ñ‹, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸. Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±Ñ‹Ğ»Ğ° ÑÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€ÑƒĞºĞ¾Ğ³Ğ¾ Ğ±Ğ°Ğ½Ğ´Ğ¸Ñ‚Ğ°, Ğ° Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Sequential Halving Ğ±Ñ‹Ğ» ÑƒÑĞºĞ¾Ñ€ĞµĞ½ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ°Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²ÑĞµĞ³Ğ¾ 50 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ°, ÑƒÑĞºĞ¾Ñ€Ğ¸Ğ² Ğ¿Ğ¾Ğ¸ÑĞº Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 50 Ñ€Ğ°Ğ·."
                },
                "en": {
                    "title": "Uncovering Hidden Gems: Accelerating Model Discovery with Multi-Armed Bandits",
                    "desc": "This paper addresses the issue of overlooked high-performing machine learning models in public repositories. It reveals that many fine-tuned models, referred to as 'hidden gems', significantly outperform popular models but are rarely used. The authors propose a Multi-Armed Bandit approach to efficiently discover these superior models, utilizing shared query sets and aggressive elimination strategies. Their method dramatically speeds up the identification process, achieving model discovery with over 50 times fewer queries than traditional exhaustive methods."
                },
                "zh": {
                    "title": "å‘ç°éšè—ä¼˜è´¨æ¨¡å‹çš„é«˜æ•ˆæ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å…¬å…±å­˜å‚¨åº“ä¸­å­˜åœ¨çš„éšè—ä¼˜è´¨æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹ç”±äºå‘ç°æ–¹æ³•æ•ˆç‡ä½ä¸‹è€Œè¢«å¿½è§†ã€‚æˆ‘ä»¬é€šè¿‡å¯¹2000å¤šä¸ªæ¨¡å‹çš„è¯„ä¼°ï¼Œå‘ç°äº†è®¸å¤šè¡¨ç°ä¼˜å¼‚ä½†ä¸å—æ¬¢è¿çš„æ¨¡å‹ï¼Œç§°ä¸ºâ€œéšè—å®çŸ³â€ã€‚ç‰¹åˆ«æ˜¯åœ¨Llama-3.1-8Bç³»åˆ—ä¸­ï¼Œæˆ‘ä»¬å‘ç°ä¸€äº›ä¸‹è½½é‡å°‘çš„æ£€æŸ¥ç‚¹åœ¨æ•°å­¦æ€§èƒ½ä¸Šä»83.2%æå‡åˆ°96.0%ï¼Œè€Œæ¨ç†æˆæœ¬æ²¡æœ‰å¢åŠ ã€‚ä¸ºäº†è§£å†³æ¨¡å‹å‘ç°çš„è®¡ç®—å¤æ‚æ€§ï¼Œæˆ‘ä»¬å°†å…¶å»ºæ¨¡ä¸ºå¤šè‡‚èµŒåšæœºé—®é¢˜ï¼Œå¹¶é€šè¿‡å…±äº«æŸ¥è¯¢é›†å’Œæ¿€è¿›çš„æ·˜æ±°ç­–ç•¥åŠ é€Ÿäº†æ¨¡å‹å‘ç°è¿‡ç¨‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.22083",
            "title": "Latent Adversarial Regularization for Offline Preference Optimization",
            "url": "https://huggingface.co/papers/2601.22083",
            "abstract": "GANPO uses latent-space regularization through adversarial divergence minimization to improve language model preference optimization, offering more robust structural feedback than token-level methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Learning from human feedback typically relies on preference optimization that constrains policy updates through token-level regularization. However, preference optimization for language models is particularly challenging because token-space similarity does not imply semantic or behavioral similarity. To address this challenge, we leverage latent-space regularization for language model preference optimization. We introduce GANPO, which achieves latent-space regularization by penalizing divergence between the internal representations of a policy model and a reference model. Given that latent representations are not associated with explicit probability densities, we adopt an adversarial approach inspired by GANs to minimize latent-space divergence. We integrate GANPO as a regularizer into existing offline preference optimization objectives. Experiments across multiple model architectures and tasks show consistent improvements from latent-space regularization. Further, by comparing GANPO-induced inferential biases with those from token-level regularization, we find that GANPO provides more robust structural feedback under distributional shift and noise while maintaining comparable downstream performance with minor computational overhead.",
            "score": 10,
            "issue_id": 856,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "244ce35359b7d545",
            "authors": [
                "Enyi Jiang",
                "Yibo Jacky Zhang",
                "Yinglun Xu",
                "Andreas Haupt",
                "Nancy Amato",
                "Sanmi Koyejo"
            ],
            "affiliations": [
                "Department of Computer Science, Stanford University",
                "Siebel School of Computing and Data Science, University of Illinois at Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.22083.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²",
                    "desc": "GANPO â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ°Ğ´Ğ²ĞµÑ€ÑĞ°Ñ€Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ GAN, Ğ´Ğ»Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¾Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾, Ñ‡ĞµĞ¼ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¿Ğ¾ÑĞºĞ¾Ğ»ÑŒĞºÑƒ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ñƒ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GANPO Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ ÑˆÑƒĞ¼Ğ° Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ´Ğ²Ğ¸Ğ³Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Language Models with Latent-Space Regularization",
                    "desc": "GANPO is a method that enhances language model preference optimization by using latent-space regularization instead of traditional token-level methods. It focuses on minimizing the divergence between the internal representations of a policy model and a reference model, which helps in capturing semantic similarities better. By employing an adversarial approach similar to Generative Adversarial Networks (GANs), GANPO effectively penalizes discrepancies in latent representations. Experiments demonstrate that this approach yields more reliable feedback in challenging conditions while keeping computational costs low and maintaining performance."
                },
                "zh": {
                    "title": "GANPOï¼šæå‡è¯­è¨€æ¨¡å‹çš„æ½œåœ¨ç©ºé—´æ­£åˆ™åŒ–",
                    "desc": "GANPOæ˜¯ä¸€ç§é€šè¿‡å¯¹æŠ—æ€§æ•£åº¦æœ€å°åŒ–æ¥å®ç°æ½œåœ¨ç©ºé—´æ­£åˆ™åŒ–çš„æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„è¯­è¨€æ¨¡å‹çš„åå¥½ä¼˜åŒ–ã€‚ä¸åŸºäºæ ‡è®°çš„æ­£åˆ™åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒGANPOæä¾›äº†æ›´å¼ºçš„ç»“æ„åé¦ˆï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è¯­è¨€æ¨¡å‹æ—¶ã€‚è¯¥æ–¹æ³•é€šè¿‡æƒ©ç½šç­–ç•¥æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹ä¹‹é—´çš„å†…éƒ¨è¡¨ç¤ºæ•£åº¦æ¥å®ç°æ½œåœ¨ç©ºé—´æ­£åˆ™åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒGANPOåœ¨å¤šä¸ªæ¨¡å‹æ¶æ„å’Œä»»åŠ¡ä¸­éƒ½èƒ½å¸¦æ¥ä¸€è‡´çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶åœ¨åˆ†å¸ƒå˜åŒ–å’Œå™ªå£°æƒ…å†µä¸‹è¡¨ç°æ›´ä¸ºç¨³å¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.21590",
            "title": "Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening",
            "url": "https://huggingface.co/papers/2601.21590",
            "abstract": "A theoretically grounded method for improving large language model reasoning performance through distribution sharpening without iterative sampling or external rewards, achieving comparable results to reinforcement learning post-training with significantly reduced computational costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) post-training is a dominant approach for improving the reasoning performance of large language models (LLMs), yet growing evidence suggests that its gains arise primarily from distribution sharpening rather than the acquisition of new capabilities. Recent work has shown that sampling from the power distribution of LLMs using Markov chain Monte Carlo (MCMC) can recover performance comparable to RL post-training without relying on external rewards; however, the high computational cost of MCMC makes such approaches impractical for widespread adoption. In this work, we propose a theoretically grounded alternative that eliminates the need for iterative MCMC. We derive a novel formulation showing that the global power distribution can be approximated by a token-level scaled low-temperature one, where the scaling factor captures future trajectory quality. Leveraging this insight, we introduce a training-free and verifier-free algorithm that sharpens the base model's generative distribution autoregressively. Empirically, we evaluate our method on math, QA, and code tasks across four LLMs, and show that our method matches or surpasses one-shot GRPO without relying on any external rewards, while reducing inference latency by over 10x compared to MCMC-based sampling.",
            "score": 10,
            "issue_id": 848,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "4c4f71a6a4dd1057",
            "authors": [
                "Xiaotong Ji",
                "Rasul Tutunov",
                "Matthieu Zimmer",
                "Haitham Bou Ammar"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab",
                "UCL Centre for Artificial Intelligence"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.21590.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#plp",
                    "#reasoning",
                    "#optimization",
                    "#training",
                    "#math"
                ],
                "emoji": "ğŸ”¥",
                "ru": {
                    "title": "Ğ—Ğ°Ğ¾ÑÑ‚Ñ€ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ñ: ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ¾ÑÑ‚Ñ€ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ±Ñ‹Ğ»ÑŒ Ğ¾Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ğ¸Ğ·-Ğ·Ğ° ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ, Ğ° Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ½Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 10 Ñ€Ğ°Ğ·."
                },
                "en": {
                    "title": "Sharpening Language Model Reasoning Efficiently",
                    "desc": "This paper presents a new method to enhance the reasoning abilities of large language models (LLMs) without the need for iterative sampling or external rewards, which are common in reinforcement learning (RL) post-training. The authors argue that the improvements seen in RL are mainly due to a process called distribution sharpening, rather than the development of new skills. They introduce a novel approach that approximates the global power distribution of LLMs using a simpler, token-level scaled low-temperature distribution, which allows for more efficient processing. Their empirical results demonstrate that this method achieves performance on par with or better than traditional RL methods while significantly reducing computational costs and inference time."
                },
                "zh": {
                    "title": "é€šè¿‡åˆ†å¸ƒé”åŒ–æå‡è¯­è¨€æ¨¡å‹æ¨ç†æ€§èƒ½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç†è®ºåŸºç¡€çš„æ–¹æ³•ï¼Œé€šè¿‡åˆ†å¸ƒé”åŒ–æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æ€§èƒ½ï¼Œè€Œæ— éœ€è¿­ä»£é‡‡æ ·æˆ–å¤–éƒ¨å¥–åŠ±ã€‚è¿™ç§æ–¹æ³•çš„æ•ˆæœä¸å¼ºåŒ–å­¦ä¹ åè®­ç»ƒç›¸å½“ï¼Œä½†è®¡ç®—æˆæœ¬æ˜¾è‘—é™ä½ã€‚æˆ‘ä»¬æ¨å¯¼å‡ºä¸€ç§æ–°å…¬å¼ï¼Œè¡¨æ˜å…¨å±€åŠŸç‡åˆ†å¸ƒå¯ä»¥é€šè¿‡æ ‡è®°çº§åˆ«çš„ä½æ¸©ç¼©æ”¾åˆ†å¸ƒæ¥è¿‘ä¼¼ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— è®­ç»ƒå’Œæ— éªŒè¯çš„ç®—æ³•ï¼Œèƒ½å¤Ÿè‡ªå›å½’åœ°é”åŒ–åŸºç¡€æ¨¡å‹çš„ç”Ÿæˆåˆ†å¸ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.21571",
            "title": "Shaping capabilities with token-level data filtering",
            "url": "https://huggingface.co/papers/2601.21571",
            "abstract": "Token filtering during pretraining effectively reduces unwanted language model capabilities while maintaining alignment, becoming more effective at larger scales and tolerating noisy labels with sufficient compute.  \t\t\t\t\tAI-generated summary \t\t\t\t Current approaches to reducing undesired capabilities in language models are largely post hoc, and can thus be easily bypassed by adversaries. A natural alternative is to shape capabilities during pretraining itself. On the proxy task of removing medical capabilities, we show that the simple intervention of filtering pretraining data is highly effective, robust, and inexpensive at scale. Inspired by work on data attribution, we show that filtering tokens is more effective than filtering documents, achieving the same hit to undesired capabilities at a lower cost to benign ones. Training models spanning two orders of magnitude, we then demonstrate that filtering gets more effective with scale: for our largest models, token filtering leads to a 7000x compute slowdown on the forget domain. We also show that models trained with token filtering can still be aligned on the forget domain. Along the way, we introduce a methodology for labeling tokens with sparse autoencoders and distilling cheap, high-quality classifiers. We also demonstrate that filtering can be robust to noisy labels with sufficient pretraining compute.",
            "score": 8,
            "issue_id": 855,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "c83b6d5dc9da683b",
            "authors": [
                "Neil Rathi",
                "Alec Radford"
            ],
            "affiliations": [
                "Anthropic",
                "Stanford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.21571.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#alignment",
                    "#data",
                    "#security"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²: ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°, Ñ‡ĞµĞ¼ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ€Ğ°ÑÑ‚ĞµÑ‚ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 7000-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°ĞµĞ¼Ğ¾Ğ¼ Ğ´Ğ¾Ğ¼ĞµĞ½Ğµ Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº ÑˆÑƒĞ¼Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Token Filtering: Shaping Language Models from the Start",
                    "desc": "This paper discusses a method for improving language models by filtering tokens during the pretraining phase. By removing specific unwanted capabilities, such as medical knowledge, the authors show that this approach is more effective and cost-efficient than filtering entire documents. The research indicates that as the model size increases, the benefits of token filtering also grow, leading to significant reductions in undesired capabilities without sacrificing performance on benign tasks. Additionally, the authors introduce a new technique for labeling tokens and demonstrate that their method can handle noisy labels effectively with enough computational resources."
                },
                "zh": {
                    "title": "é€šè¿‡ä»¤ç‰Œè¿‡æ»¤æå‡è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åœ¨é¢„è®­ç»ƒé˜¶æ®µé€šè¿‡è¿‡æ»¤ä»¤ç‰Œæ¥å‡å°‘è¯­è¨€æ¨¡å‹çš„ä¸è‰¯èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨å¤§è§„æ¨¡è®­ç»ƒä¸­æ›´æœ‰æ•ˆï¼Œå¹¶ä¸”èƒ½å¤Ÿå®¹å¿å¸¦æœ‰å™ªå£°çš„æ ‡ç­¾ã€‚ä¸è¿‡æ»¤æ–‡æ¡£ç›¸æ¯”ï¼Œè¿‡æ»¤ä»¤ç‰Œåœ¨é™ä½ä¸è‰¯èƒ½åŠ›çš„åŒæ—¶ï¼Œå¯¹è‰¯æ€§èƒ½åŠ›çš„å½±å“æ›´å°ã€‚è®ºæ–‡è¿˜ä»‹ç»äº†ä¸€ç§ä½¿ç”¨ç¨€ç–è‡ªç¼–ç å™¨æ ‡è®°ä»¤ç‰Œçš„æ–¹æ³•ï¼Œå¹¶è¯æ˜äº†åœ¨è¶³å¤Ÿçš„é¢„è®­ç»ƒè®¡ç®—ä¸‹ï¼Œè¿‡æ»¤å¯¹å™ªå£°æ ‡ç­¾å…·æœ‰é²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.21051",
            "title": "Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report",
            "url": "https://huggingface.co/papers/2601.21051",
            "abstract": "A two-stage trained cybersecurity reasoning model achieves competitive performance on specialized tasks while maintaining general capabilities through supervised fine-tuning and reinforcement learning from verifiable rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Foundation-Sec-8B-Reasoning, the first open-source native reasoning model for cybersecurity. Built upon our previously released Foundation-Sec-8B base model (derived from Llama-3.1-8B-Base), the model is trained through a two-stage process combining supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR). Our training leverages proprietary reasoning data spanning cybersecurity analysis, instruction-following, and mathematical reasoning. Evaluation across 10 cybersecurity benchmarks and 10 general-purpose benchmarks demonstrates performance competitive with significantly larger models on cybersecurity tasks while maintaining strong general capabilities. The model shows effective generalization on multi-hop reasoning tasks and strong safety performance when deployed with appropriate system prompts and guardrails. This work demonstrates that domain-specialized reasoning models can achieve strong performance on specialized tasks while maintaining broad general capabilities. We release the model publicly at https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning.",
            "score": 8,
            "issue_id": 842,
            "pub_date": "2026-01-28",
            "pub_date_card": {
                "ru": "28 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 28",
                "zh": "1æœˆ28æ—¥"
            },
            "hash": "089e23bea0d4b7ae",
            "authors": [
                "Zhuoran Yang",
                "Ed Li",
                "Jianliang He",
                "Aman Priyanshu",
                "Baturay Saglam",
                "Paul Kassianik",
                "Sajana Weerawardhena",
                "Anu Vellore",
                "Blaine Nelson",
                "Neusha Javidnia",
                "Arthur Goldblatt",
                "Fraser Burch",
                "Avi Zohary",
                "Assaf Eisenman",
                "Mahdi Sabbaghi",
                "Supriti Vijay",
                "Rahim Dharssi",
                "Dhruv Kedia",
                "Kojin Oshiba",
                "Yaron Singer",
                "Amin Karbasi"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Cisco Systems Inc.",
                "Foundation AI",
                "University of California, San Diego",
                "University of Pennsylvania",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.21051.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#dataset",
                    "#reasoning",
                    "#benchmark",
                    "#open_source",
                    "#training",
                    "#science",
                    "#small_models"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ: Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Foundation-Sec-8B-Reasoning â€” Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Llama-3.1-8B. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ° supervised fine-tuning Ğ½Ğ° proprietary Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ·Ğ°Ñ‚ĞµĞ¼ reinforcement learning Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°Ğ»Ğ°Ñ 8-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ domain-specialized Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Empowering Cybersecurity with Specialized Reasoning Models",
                    "desc": "The paper introduces Foundation-Sec-8B-Reasoning, an open-source reasoning model specifically designed for cybersecurity tasks. It employs a two-stage training approach that combines supervised fine-tuning and reinforcement learning from verifiable rewards to enhance its performance. The model is evaluated on various benchmarks, showing competitive results against larger models while retaining general capabilities. This research highlights the potential of specialized reasoning models to excel in specific domains without sacrificing overall versatility."
                },
                "zh": {
                    "title": "ç½‘ç»œå®‰å…¨æ¨ç†æ¨¡å‹ï¼šä¸“ç”¨ä¸é€šç”¨çš„å®Œç¾ç»“åˆ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFoundation-Sec-8B-Reasoningçš„å¼€æºç½‘ç»œå®‰å…¨æ¨ç†æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œç»“åˆäº†ç½‘ç»œå®‰å…¨åˆ†æã€æŒ‡ä»¤è·Ÿéšå’Œæ•°å­¦æ¨ç†çš„æ•°æ®ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨ç½‘ç»œå®‰å…¨ä»»åŠ¡ä¸Šä¸æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ç«äº‰åŠ›å¼ºï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„é€šç”¨èƒ½åŠ›ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œé¢†åŸŸä¸“ç”¨çš„æ¨ç†æ¨¡å‹èƒ½å¤Ÿåœ¨ç‰¹å®šä»»åŠ¡ä¸Šå–å¾—ä¼˜å¼‚è¡¨ç°ï¼ŒåŒæ—¶å…·å¤‡å¹¿æ³›çš„é€šç”¨èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.18129",
            "title": "Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models",
            "url": "https://huggingface.co/papers/2601.18129",
            "abstract": "A minimal post-training approach using supervised fine-tuning, on-policy distillation, and small-scale reinforcement fine-tuning enables the development of high-quality sovereign language models with reduced resource requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have progressed rapidly; however, most state-of-the-art models are trained and evaluated primarily in high-resource languages such as English and Chinese, and are often developed by a small number of organizations with access to large-scale compute and data. This gatekeeping creates a practical barrier for sovereign settings in which a regional- or national-scale institution or domain owner must retain control and understanding of model weights, training data, and deployment while operating under limited resources and strict transparency constraints. To this end, we identify two core requirements: (1) adoptability, the ability to transform a base model into a general-purpose assistant, and (2) sovereign capability, the ability to perform high-stakes, region-specific tasks (e.g., legal reasoning in local languages and cultural knowledge). We investigate whether these requirements can be achieved without scaling massive instruction corpora or relying on complex preference tuning pipelines and large-scale reinforcement fine-tuning (RFT). We present Typhoon S, a minimal and open post-training recipe that combines supervised fine-tuning, on-policy distillation, and small-scale RFT. Using Thai as a representative case study, we demonstrate that our approach transforms both sovereign-adapted and general-purpose base models into instruction-tuned models with strong general performance. We further show that small-scale RFT with InK-GRPO -- an extension of GRPO that augments the GRPO loss with a next-word prediction loss -- improves Thai legal reasoning and Thai-specific knowledge while preserving general capabilities. Our results suggest that a carefully designed post-training strategy can reduce the required scale of instruction data and computation, providing a practical path toward high-quality sovereign LLMs under academic-scale resources.",
            "score": 8,
            "issue_id": 842,
            "pub_date": "2026-01-26",
            "pub_date_card": {
                "ru": "26 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 26",
                "zh": "1æœˆ26æ—¥"
            },
            "hash": "0030396a5bb56927",
            "authors": [
                "Kunat Pipatanakul",
                "Pittawat Taveekitworachai"
            ],
            "affiliations": [
                "Typhoon, SCB 10X"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.18129.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#reasoning",
                    "#open_source",
                    "#training",
                    "#multilingual",
                    "#optimization",
                    "#low_resource",
                    "#small_models"
                ],
                "emoji": "ğŸ›ï¸",
                "ru": {
                    "title": "Ğ¡ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ supervised fine-tuning, on-policy distillation Ğ¸ small-scale reinforcement learning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ LLM Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ post-training Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° GRPO, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ğ¾Ğµ InK-GRPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ GRPO Ñ loss Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ next-word prediction Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¸Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ‚Ğ¾Ğ² Ñ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Sovereign Language Models with Minimal Resources",
                    "desc": "This paper presents a method for developing high-quality sovereign language models using a minimal post-training approach. It combines supervised fine-tuning, on-policy distillation, and small-scale reinforcement fine-tuning to adapt base models for specific regional tasks. The study focuses on the Thai language, demonstrating that this approach can enhance legal reasoning and cultural knowledge while maintaining general performance. The findings suggest that effective post-training strategies can enable the creation of powerful language models with fewer resources, making them accessible for local institutions."
                },
                "zh": {
                    "title": "é«˜æ•ˆå¼€å‘ä¸»æƒè¯­è¨€æ¨¡å‹çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æœ€å°åŒ–çš„åè®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒã€åœ¨çº¿è’¸é¦å’Œå°è§„æ¨¡å¼ºåŒ–å¾®è°ƒï¼Œèƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹å¼€å‘é«˜è´¨é‡çš„ä¸»æƒè¯­è¨€æ¨¡å‹ã€‚å¤§å¤šæ•°å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ä¸»è¦åœ¨é«˜èµ„æºè¯­è¨€ä¸Šè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œå¯¼è‡´ä¸€äº›åœ°åŒºæˆ–å›½å®¶çš„æœºæ„éš¾ä»¥æŒæ§æ¨¡å‹çš„æƒé‡å’Œè®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬æå‡ºçš„Typhoon Sæ–¹æ³•ï¼Œèƒ½å¤Ÿå°†åŸºç¡€æ¨¡å‹è½¬å˜ä¸ºé€šç”¨åŠ©æ‰‹ï¼Œå¹¶æ»¡è¶³ç‰¹å®šåŒºåŸŸçš„é«˜é£é™©ä»»åŠ¡éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§åè®­ç»ƒç­–ç•¥å¯ä»¥æœ‰æ•ˆå‡å°‘æ‰€éœ€çš„æŒ‡ä»¤æ•°æ®å’Œè®¡ç®—èµ„æºï¼Œæä¾›é«˜è´¨é‡ä¸»æƒè¯­è¨€æ¨¡å‹çš„å¯è¡Œè·¯å¾„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.22069",
            "title": "VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning",
            "url": "https://huggingface.co/papers/2601.22069",
            "abstract": "VTC-R1 enables efficient long-context reasoning by compressing textual traces into compact images and iteratively feeding them back into vision-language models as optical memory, achieving significant speedup without sacrificing performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-context reasoning has significantly empowered large language models (LLMs) to tackle complex tasks, yet it introduces severe efficiency bottlenecks due to the computational complexity. Existing efficient approaches often rely on complex additional training or external models for compression, which limits scalability and discards critical fine-grained information. In this paper, we propose VTC-R1, a new efficient reasoning paradigm that integrates vision-text compression into the reasoning process. Instead of processing lengthy textual traces, VTC-R1 renders intermediate reasoning segments into compact images, which are iteratively fed back into vision-language models as \"optical memory.\" We construct a training dataset based on OpenR1-Math-220K achieving 3.4x token compression and fine-tune representative VLMs-Glyph and Qwen3-VL. Extensive experiments on benchmarks such as MATH500, AIME25, AMC23 and GPQA-D demonstrate that VTC-R1 consistently outperforms standard long-context reasoning. Furthermore, our approach significantly improves inference efficiency, achieving 2.7x speedup in end-to-end latency, highlighting its potential as a scalable solution for reasoning-intensive applications. Our code is available at https://github.com/w-yibo/VTC-R1.",
            "score": 7,
            "issue_id": 842,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "d1a1451f99556cf9",
            "authors": [
                "Yibo Wang",
                "Yongcheng Jing",
                "Shunyu Liu",
                "Hao Guan",
                "Rong-cheng Tu",
                "Chengyu Wang",
                "Jun Huang",
                "Dacheng Tao"
            ],
            "affiliations": [
                "Alibaba Cloud Computing",
                "Nanyang Technical University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.22069.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#benchmark",
                    "#open_source",
                    "#training",
                    "#long_context",
                    "#inference"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°: ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ",
                    "desc": "VTC-R1 â€” Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ°ÑÑ‚ÑÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Â«Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸Â». ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ½Ğ° 3.4x Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ğ°Ğ¶Ğ½ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² 2.7x Ñ€Ğ°Ğ·Ğ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "VTC-R1: Revolutionizing Long-Context Reasoning with Optical Memory",
                    "desc": "The paper introduces VTC-R1, a novel method for enhancing long-context reasoning in large language models (LLMs) by converting textual information into compact images. This approach allows for efficient processing by using these images as 'optical memory' in vision-language models, significantly reducing computational load. Unlike previous methods that require complex training or external models, VTC-R1 achieves a remarkable 3.4x token compression while preserving essential details. Experimental results show that VTC-R1 not only improves reasoning performance but also accelerates inference speed by 2.7 times, making it a promising solution for applications requiring intensive reasoning."
                },
                "zh": {
                    "title": "VTC-R1ï¼šé«˜æ•ˆé•¿ä¸Šä¸‹æ–‡æ¨ç†çš„æ–°æ–¹æ³•",
                    "desc": "VTC-R1æ˜¯ä¸€ç§æ–°é¢–çš„é«˜æ•ˆæ¨ç†èŒƒå¼ï¼Œé€šè¿‡å°†æ–‡æœ¬è½¨è¿¹å‹ç¼©ä¸ºç´§å‡‘çš„å›¾åƒï¼Œæ¥å®ç°é•¿ä¸Šä¸‹æ–‡æ¨ç†ã€‚è¯¥æ–¹æ³•å°†ä¸­é—´æ¨ç†ç‰‡æ®µæ¸²æŸ“ä¸ºå›¾åƒï¼Œå¹¶å°†å…¶ä½œä¸ºâ€œå…‰å­¦è®°å¿†â€è¿­ä»£è¾“å…¥åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»è€Œæ˜¾è‘—æé«˜æ¨ç†æ•ˆç‡ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒVTC-R1åœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†3.4å€çš„ä»¤ç‰Œå‹ç¼©å’Œ2.7å€çš„æ¨ç†é€Ÿåº¦æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVTC-R1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºæ ‡å‡†çš„é•¿ä¸Šä¸‹æ–‡æ¨ç†æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨æ¨ç†å¯†é›†å‹åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.21181",
            "title": "MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2601.21181",
            "abstract": "Multimodal Large Language Models suffer from cross-modal hallucinations where one modality incorrectly influences generation from another, leading to fabricated outputs; this exposes a fundamental deficiency in modality-interaction control. To address this, a training-free method called Modality-Adaptive Decoding (MAD) is proposed that adaptively weights modality-specific decoding branches based on task requirements by leveraging the model's inherent ability to self-assess modality relevance. MAD uses extracted modality probabilities to adaptively weight contrastive decoding branches, enabling the model to focus on relevant information while suppressing cross-modal interference. Extensive experiments on CMM and AVHBench demonstrate that MAD significantly reduces cross-modal hallucinations across multiple audio-visual language models, showing that explicit modality awareness through self-assessment is crucial for robust multimodal reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) suffer from cross-modal hallucinations, where one modality inappropriately influences generation about another, leading to fabricated output. This exposes a more fundamental deficiency in modality-interaction control. To address this, we propose Modality-Adaptive Decoding (MAD), a training-free method that adaptively weights modality-specific decoding branches based on task requirements. MAD leverages the model's inherent ability to self-assess modality relevance by querying which modalities are needed for each task. The extracted modality probabilities are then used to adaptively weight contrastive decoding branches, enabling the model to focus on relevant information while suppressing cross-modal interference. Extensive experiments on CMM and AVHBench demonstrate that MAD significantly reduces cross-modal hallucinations across multiple audio-visual language models (7.8\\% and 2.0\\% improvements for VideoLLaMA2-AV, 8.7\\% and 4.7\\% improvements for Qwen2.5-Omni). Our approach demonstrates that explicit modality awareness through self-assessment is crucial for robust multimodal reasoning, offering a principled extension to existing contrastive decoding methods. Our code is available at https://github.com/top-yun/MAD{https://github.com/top-yun/MAD}",
            "score": 7,
            "issue_id": 842,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "5e6eaaac32ce4fd3",
            "authors": [
                "Sangyun Chung",
                "Se Yeon Kim",
                "Youngchae Chee",
                "Yong Man Ro"
            ],
            "affiliations": [
                "Integrated Vision Language Lab, KAIST, South Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.21181.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video",
                    "#audio",
                    "#inference"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¾Ğ´Ğ½Ğ° Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¾ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Modality-Adaptive Decoding (MAD), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²ĞµÑ‚Ğ²Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº ÑĞ°Ğ¼Ğ¾Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞ²Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Enhancing Multimodal Models with Adaptive Modality Awareness",
                    "desc": "This paper addresses the issue of cross-modal hallucinations in Multimodal Large Language Models (MLLMs), where one type of data (like text) incorrectly influences another (like images), resulting in inaccurate outputs. The authors introduce a novel method called Modality-Adaptive Decoding (MAD), which does not require additional training and dynamically adjusts the importance of different data types based on the task at hand. By utilizing the model's ability to evaluate which modalities are relevant, MAD effectively reduces interference between modalities during the generation process. Experimental results show that MAD significantly improves the accuracy of multimodal reasoning in various audio-visual language models, highlighting the importance of modality awareness in enhancing model performance."
                },
                "zh": {
                    "title": "æ¨¡æ€è‡ªé€‚åº”è§£ç ï¼šæå‡å¤šæ¨¡æ€æ¨¡å‹çš„å‡†ç¡®æ€§",
                    "desc": "å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é¢ä¸´è·¨æ¨¡æ€å¹»è§‰çš„é—®é¢˜ï¼Œå³ä¸€ç§æ¨¡æ€é”™è¯¯åœ°å½±å“å¦ä¸€ç§æ¨¡æ€çš„ç”Ÿæˆï¼Œå¯¼è‡´è™šå‡çš„è¾“å‡ºã€‚è¿™æš´éœ²äº†æ¨¡æ€äº¤äº’æ§åˆ¶çš„åŸºæœ¬ç¼ºé™·ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºæ¨¡æ€è‡ªé€‚åº”è§£ç ï¼ˆMADï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ ¹æ®ä»»åŠ¡éœ€æ±‚è‡ªé€‚åº”åœ°åŠ æƒç‰¹å®šæ¨¡æ€çš„è§£ç åˆ†æ”¯ã€‚MADåˆ©ç”¨æ¨¡å‹è‡ªæˆ‘è¯„ä¼°æ¨¡æ€ç›¸å…³æ€§çš„èƒ½åŠ›ï¼Œæ˜¾è‘—å‡å°‘äº†å¤šä¸ªéŸ³é¢‘-è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„è·¨æ¨¡æ€å¹»è§‰ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.20975",
            "title": "DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents",
            "url": "https://huggingface.co/papers/2601.20975",
            "abstract": "DeepSearchQA presents a 900-prompt benchmark evaluating agents on complex multi-step information-seeking tasks requiring systematic information collation, deduplication, and reasoning about stopping criteria across 17 fields.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce DeepSearchQA, a 900-prompt benchmark for evaluating agents on difficult multi-step information-seeking tasks across 17 different fields. Unlike traditional benchmarks that target single answer retrieval or broad-spectrum factuality, DeepSearchQA features a dataset of challenging, handcrafted tasks designed to evaluate an agent's ability to execute complex search plans to generate exhaustive answer lists. This shift in design explicitly tests three critical, yet under-evaluated capabilities: 1) systematic collation of fragmented information from disparate sources, 2) de-duplication and entity resolution to ensure precision, and 3) the ability to reason about stopping criteria within an open-ended search space. Each task is structured as a causal chain, where discovering information for one step is dependent on the successful completion of the previous one, stressing long-horizon planning and context retention. All tasks are grounded in the open web with objectively verifiable answer sets. Our comprehensive evaluation of state-of-the-art agent architectures reveals significant performance limitations: even the most advanced models struggle to balance high recall with precision. We observe distinct failure modes ranging from premature stopping (under-retrieval) to hedging behaviors, where agents cast an overly wide net of low-confidence answers to artificially boost recall. These findings highlight critical headroom in current agent designs and position DeepSearchQA as an essential diagnostic tool for driving future research toward more robust, deep-research capabilities.",
            "score": 6,
            "issue_id": 842,
            "pub_date": "2026-01-28",
            "pub_date_card": {
                "ru": "28 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 28",
                "zh": "1æœˆ28æ—¥"
            },
            "hash": "f3af40971f095e34",
            "authors": [
                "Nikita Gupta",
                "Riju Chatterjee",
                "Lukas Haas",
                "Connie Tao",
                "Andrew Wang",
                "Chang Liu",
                "Hidekazu Oiwa",
                "Elena Gribovskaya",
                "Jan Ackermann",
                "John Blitzer",
                "Sasha Goldshtein",
                "Dipanjan Das"
            ],
            "affiliations": [
                "Google DeepMind",
                "Google Research",
                "Google Search",
                "Kaggle"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.20975.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸: Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº DeepSearchQA Ñ 900 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸: ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ², Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒĞ¼ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞšĞ°Ğ¶Ğ´Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ° ĞºĞ°Ğº Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ°Ñ Ñ†ĞµĞ¿ÑŒ, Ğ³Ğ´Ğµ ÑƒÑĞ¿ĞµÑ… Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ³Ğ¾, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ: Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ‚Ñ€ÑƒĞ´Ğ¾Ğ¼ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ¾Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑÑ Ñ€ĞµĞ¶Ğ¸Ğ¼Ñ‹ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ° Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ¶Ğ´ĞµĞ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ¾ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ¸Ğ·ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "DeepSearchQA: Elevating AI's Information-Seeking Skills",
                    "desc": "DeepSearchQA is a new benchmark consisting of 900 prompts that assess AI agents on complex, multi-step information-seeking tasks across 17 fields. It focuses on evaluating the agents' abilities to collate fragmented information, deduplicate data, and reason about when to stop searching for answers. Unlike traditional benchmarks that emphasize single-answer retrieval, this benchmark requires agents to create comprehensive answer lists through systematic search strategies. The study reveals that even advanced models face challenges in balancing recall and precision, indicating a need for improvements in AI design for better deep-research capabilities."
                },
                "zh": {
                    "title": "DeepSearchQAï¼šæ¨åŠ¨æ™ºèƒ½ä½“æ·±åº¦ç ”ç©¶èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•",
                    "desc": "DeepSearchQAæ˜¯ä¸€ä¸ªåŒ…å«900ä¸ªæç¤ºçš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ™ºèƒ½ä½“åœ¨å¤æ‚çš„å¤šæ­¥éª¤ä¿¡æ¯æœç´¢ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¿™äº›ä»»åŠ¡æ¶‰åŠ17ä¸ªä¸åŒé¢†åŸŸï¼Œè¦æ±‚æ™ºèƒ½ä½“ç³»ç»Ÿåœ°æ•´åˆä¿¡æ¯ã€å»é‡å’Œæ¨ç†åœæ­¢æ ‡å‡†ã€‚ä¸ä¼ ç»ŸåŸºå‡†ä¸åŒï¼ŒDeepSearchQAä¸“æ³¨äºè¯„ä¼°æ™ºèƒ½ä½“æ‰§è¡Œå¤æ‚æœç´¢è®¡åˆ’çš„èƒ½åŠ›ï¼Œä»¥ç”Ÿæˆè¯¦å°½çš„ç­”æ¡ˆåˆ—è¡¨ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰çš„æ™ºèƒ½ä½“åœ¨é«˜å¬å›ç‡ä¸é«˜ç²¾ç¡®åº¦ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½é™åˆ¶ï¼Œæ­ç¤ºäº†æœªæ¥ç ”ç©¶çš„æ½œåœ¨æ”¹è¿›ç©ºé—´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.21598",
            "title": "Beyond Imitation: Reinforcement Learning for Active Latent Planning",
            "url": "https://huggingface.co/papers/2601.21598",
            "abstract": "Active latent planning method improves reasoning accuracy and efficiency by modeling latent token supervision as conditional VAE and using reinforcement learning with coherence rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning methods fine-tune Large Language Models (LLMs) to substitute discrete language tokens with continuous latent tokens. These methods consume fewer tokens compared to the conventional language CoT reasoning and have the potential to plan in a dense latent space. However, current latent tokens are generally supervised based on imitating language labels. Considering that there can be multiple equivalent but diverse CoT labels for a question, passively imitating an arbitrary one may lead to inferior latent token representations and latent reasoning policies, undermining the potential planning ability and resulting in clear gaps between training and testing. In this work, we emphasize the importance of active planning over the representation space of latent tokens in achieving the optimal latent reasoning policy. So, we propose the Active Latent Planning method (ATP-Latent), which models the supervision process of latent tokens as a conditional variational auto-encoder (VAE) to obtain a smoother latent space. Moreover, to facilitate the most reasonable latent reasoning policy, ATP-Latent conducts reinforcement learning (RL) with an auxiliary coherence reward, which is calculated based on the consistency between VAE-decoded contents of latent tokens, enabling a guided RL process. In experiments on LLaMA-1B, ATP-Latent demonstrates +4.1\\% accuracy and -3.3\\% tokens on four benchmarks compared to advanced baselines. Codes are available on https://github.com/zz1358m/ATP-Latent-master.",
            "score": 5,
            "issue_id": 842,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "8241a0e30425989a",
            "authors": [
                "Zhi Zheng",
                "Wee Sun Lee"
            ],
            "affiliations": [
                "School of Computing, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.21598.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#rl",
                    "#optimization",
                    "#small_models"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ĞºĞ°Ğº ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ (VAE), Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»Ğ°Ğ´ĞºĞ¾Ğµ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ. Ğ”Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ·Ğ° ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼Ñ‹Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 4,1% Ğ¿Ñ€Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 3,3% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Reasoning with Active Latent Planning",
                    "desc": "The paper introduces the Active Latent Planning method (ATP-Latent) to enhance reasoning accuracy and efficiency in machine learning models. It utilizes a conditional variational auto-encoder (VAE) to create a smoother latent space for continuous token representation, moving away from traditional discrete language tokens. By implementing reinforcement learning with coherence rewards, ATP-Latent ensures that the reasoning policy is guided by the consistency of the decoded outputs. Experimental results show that ATP-Latent outperforms existing methods, achieving higher accuracy while using fewer tokens."
                },
                "zh": {
                    "title": "ä¸»åŠ¨æ½œåœ¨è§„åˆ’ï¼šæå‡æ¨ç†æ•ˆç‡ä¸å‡†ç¡®æ€§",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§ä¸»åŠ¨æ½œåœ¨è§„åˆ’æ–¹æ³•ï¼ˆATP-Latentï¼‰ï¼Œæ—¨åœ¨æé«˜æ¨ç†çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†æ½œåœ¨æ ‡è®°çš„ç›‘ç£å»ºæ¨¡ä¸ºæ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼Œå¹¶ç»“åˆå¼ºåŒ–å­¦ä¹ ä¸ä¸€è‡´æ€§å¥–åŠ±ï¼Œä¼˜åŒ–æ½œåœ¨æ¨ç†ç­–ç•¥ã€‚ä¸ä¼ ç»Ÿçš„é“¾å¼æ€ç»´æ¨ç†ç›¸æ¯”ï¼ŒATP-Latentåœ¨ä½¿ç”¨æ›´å°‘çš„æ ‡è®°çš„åŒæ—¶ï¼Œèƒ½å¤Ÿåœ¨æ›´å¯†é›†çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œè§„åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒATP-Latentåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ç›¸è¾ƒäºå…ˆè¿›çš„åŸºçº¿æé«˜äº†4.1%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶å‡å°‘äº†3.3%çš„æ ‡è®°ä½¿ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.22158",
            "title": "One-step Latent-free Image Generation with Pixel Mean Flows",
            "url": "https://huggingface.co/papers/2601.22158",
            "abstract": "Pixel MeanFlow introduces a one-step latent-free image generation method by separating network output space from loss space, achieving strong performance on ImageNet at multiple resolutions.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose \"pixel MeanFlow\" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.",
            "score": 4,
            "issue_id": 846,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "90fea23896dfac9c",
            "authors": [
                "Yiyang Lu",
                "Susie Lu",
                "Qiao Sun",
                "Hanhong Zhao",
                "Zhicheng Jiang",
                "Xianbang Wang",
                "Tianhong Li",
                "Zhengyang Geng",
                "Kaiming He"
            ],
            "affiliations": [
                "Meta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.22158.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#architecture",
                    "#optimization",
                    "#training"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞĞ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Pixel MeanFlow Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ: ÑĞµÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸, Ğ° Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑÑÑ‚ÑÑ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ MeanFlow. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ¼ ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ²ÑĞ·Ğ¸ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ImageNet: 2.22 FID Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ 256x256 Ğ¸ 2.48 FID Ğ´Ğ»Ñ 512x512."
                },
                "en": {
                    "title": "One-Step Image Generation Without Latents!",
                    "desc": "Pixel MeanFlow (pMF) presents a novel approach to image generation that eliminates the need for latent spaces and multi-step sampling. By separating the network output space from the loss space, pMF effectively targets a low-dimensional image manifold while utilizing MeanFlow in the velocity space for loss calculation. This method allows for one-step generation of images, achieving impressive results on ImageNet at various resolutions. The findings suggest that pMF could significantly enhance the capabilities of diffusion and flow-based generative models."
                },
                "zh": {
                    "title": "ä¸€æ­¥ç”Ÿæˆï¼Œæ— éœ€æ½œåœ¨ç©ºé—´çš„å›¾åƒç”Ÿæˆ",
                    "desc": "Pixel MeanFlowï¼ˆpMFï¼‰æå‡ºäº†ä¸€ç§ä¸€æ­¥ç”Ÿæˆå›¾åƒçš„æ–¹æ³•ï¼Œé¿å…äº†ä½¿ç”¨æ½œåœ¨ç©ºé—´ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†ç½‘ç»œè¾“å‡ºç©ºé—´ä¸æŸå¤±ç©ºé—´åˆ†å¼€æ¥å®ç°ï¼Œç½‘ç»œç›®æ ‡ä½äºå‡å®šçš„ä½ç»´å›¾åƒæµå½¢ä¸Šï¼Œè€ŒæŸå¤±åˆ™é€šè¿‡é€Ÿåº¦ç©ºé—´ä¸­çš„MeanFlowå®šä¹‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒpMFåœ¨256x256å’Œ512x512åˆ†è¾¨ç‡ä¸‹çš„ImageNetä¸Šè¡¨ç°å‡ºè‰²ï¼Œåˆ†åˆ«è¾¾åˆ°äº†2.22å’Œ2.48çš„FIDå€¼ã€‚è¿™é¡¹ç ”ç©¶ä¸ºæ— æ½œåœ¨ç©ºé—´çš„æ‰©æ•£/æµç”Ÿæˆæ¨¡å‹çš„è¿›ä¸€æ­¥å‘å±•å¥ å®šäº†åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.22156",
            "title": "Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts",
            "url": "https://huggingface.co/papers/2601.22156",
            "abstract": "HALO enables efficient conversion of Transformer models to RNN-attention hybrid architectures with improved long-context performance using minimal training data.  \t\t\t\t\tAI-generated summary \t\t\t\t Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data",
            "score": 4,
            "issue_id": 846,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "aacbf8bdba443c30",
            "authors": [
                "Yingfa Chen",
                "Zhen Leng Thai",
                "Zihan Zhou",
                "Zhu Zhang",
                "Xingyu Shen",
                "Shuo Wang",
                "Chaojun Xiao",
                "Xu Han",
                "Zhiyuan Liu"
            ],
            "affiliations": [
                "NLP Group, DCST, IAI, BNRIST, Tsinghua University, Beijing, China",
                "OpenBMB"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.22156.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#transfer_learning",
                    "#architecture",
                    "#optimization",
                    "#training",
                    "#inference"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹: ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ² RNN Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ HALO â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğµ Ğ±Ğ»Ğ¾ĞºĞ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ HypeNet â€” Ğ½Ğ¾Ğ²ÑƒÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑÑ…ĞµĞ¼Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ HyPE. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ 2.3 Ğ¼Ğ»Ñ€Ğ´ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµĞ½ĞµĞµ 0.01% Ğ¾Ñ‚ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ÑƒÑ Ñ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ»ÑƒÑ‡ÑˆÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "HALO: Transforming Transformers for Efficient Long-Context Learning",
                    "desc": "HALO is a method that helps change Transformer models into hybrid architectures that use both RNNs and attention mechanisms, making them better at handling long sequences of data. This approach reduces the need for extensive training data, requiring only 2.3 billion tokens instead of the usual 10 billion. The new hybrid model, called HypeNet, benefits from a unique position encoding method that enhances its ability to generalize over longer contexts. As a result, HypeNet performs similarly to the original Transformer models while being more efficient and faster for long-context tasks."
                },
                "zh": {
                    "title": "HALOï¼šé«˜æ•ˆè½¬æ¢Transformeræ¨¡å‹çš„åˆ©å™¨",
                    "desc": "HALOæ˜¯ä¸€ç§å°†Transformeræ¨¡å‹é«˜æ•ˆè½¬æ¢ä¸ºRNN-æ³¨æ„åŠ›æ··åˆæ¶æ„çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä½¿ç”¨æå°‘è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹æé«˜é•¿ä¸Šä¸‹æ–‡æ€§èƒ½ã€‚ä¼ ç»Ÿçš„æ··åˆæ¶æ„åœ¨é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†ç”±äºå¤§è§„æ¨¡é¢„è®­ç»ƒçš„é«˜æˆæœ¬ï¼Œå¯¼è‡´å…¶åº”ç”¨å—åˆ°é™åˆ¶ã€‚HALOé€šè¿‡å‚æ•°è½¬ç§»å’ŒçŸ¥è¯†è’¸é¦ï¼Œèƒ½å¤Ÿä»¥ä»…éœ€2.3Bæ ‡è®°çš„æ•°æ®å°†é¢„è®­ç»ƒçš„softmaxæ³¨æ„åŠ›å—è½¬æ¢ä¸ºRNNå—ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„æ¨¡å‹è½¬æ¢ã€‚æœ€ç»ˆï¼ŒHypeNetæ¶æ„åœ¨é•¿ä¸Šä¸‹æ–‡æ€§èƒ½å’Œæ•ˆç‡ä¸Šè¶…è¶Šäº†åŸå§‹çš„Transformeræ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.22146",
            "title": "FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale",
            "url": "https://huggingface.co/papers/2601.22146",
            "abstract": "Large language models can be pre-trained from scratch using synthetic instruction-response pairs generated from unstructured text corpora, outperforming traditional methods on benchmarks measuring response quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Due to limited supervised training data, large language models (LLMs) are typically pre-trained via a self-supervised \"predict the next word\" objective on a vast amount of unstructured text data. To make the resulting model useful to users, it is further trained on a far smaller amount of \"instruction-tuning\" data comprised of supervised training examples of instructions and responses. To overcome the limited amount of supervised data, we propose a procedure that can transform the knowledge in internet-scale pre-training documents into billions of synthetic instruction and answer training pairs. The resulting dataset, called FineInstructions, uses ~18M instruction templates created from real user-written queries and prompts. These instruction templates are matched to and instantiated with human-written source documents from unstructured pre-training corpora. With \"supervised\" synthetic training data generated at this scale, an LLM can be pre-trained from scratch solely with the instruction-tuning objective, which is far more in-distribution with the expected downstream usage of LLMs (responding to user prompts). We conduct controlled token-for-token training experiments and find pre-training on FineInstructions outperforms standard pre-training and other proposed synthetic pre-training techniques on standard benchmarks measuring free-form response quality. Our resources can be found at https://huggingface.co/fineinstructions .",
            "score": 4,
            "issue_id": 849,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "fc2bed56d0098d41",
            "authors": [
                "Ajay Patel",
                "Colin Raffel",
                "Chris Callison-Burch"
            ],
            "affiliations": [
                "Department of Computer Science, University of Toronto, Toronto, Canada",
                "Department of Computer and Information Science, University of Pennsylvania, Philadelphia, USA",
                "Vector Institute, Toronto, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.22146.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#open_source"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ°Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸Ğ· Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… FineInstructions Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ~18Ğœ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚-Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ¾Ğ². ĞŸÑ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğº Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ½Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° FineInstructions Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing LLM Training with Synthetic Instructions",
                    "desc": "This paper presents a novel approach to pre-training large language models (LLMs) using synthetic instruction-response pairs derived from unstructured text data. By generating a dataset called FineInstructions, which consists of approximately 18 million instruction templates matched with human-written documents, the authors enable LLMs to be trained from scratch with a focus on instruction-tuning. This method allows the models to better align with real user interactions, as it emphasizes responding to prompts rather than just predicting the next word. The results show that LLMs pre-trained on this synthetic dataset significantly outperform those trained using traditional methods on benchmarks that assess response quality."
                },
                "zh": {
                    "title": "åˆæˆæ•°æ®åŠ©åŠ›å¤§å‹è¯­è¨€æ¨¡å‹çš„å´›èµ·",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆåˆæˆçš„æŒ‡ä»¤-å“åº”å¯¹æ¥ä»å¤´å¼€å§‹é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨äº†æ¥è‡ªäº’è”ç½‘çš„å¤§é‡æ— ç»“æ„æ–‡æœ¬æ•°æ®ï¼Œç”Ÿæˆäº†çº¦1800ä¸‡ä¸ªæŒ‡ä»¤æ¨¡æ¿ï¼Œå¹¶ä¸äººç±»ç¼–å†™çš„æ–‡æ¡£ç›¸åŒ¹é…ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥åœ¨æ²¡æœ‰å¤§é‡ç›‘ç£æ•°æ®çš„æƒ…å†µä¸‹ï¼Œä¸“æ³¨äºæŒ‡ä»¤è°ƒä¼˜ç›®æ ‡è¿›è¡Œé¢„è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¿™ç§åˆæˆæ•°æ®çš„é¢„è®­ç»ƒæ–¹æ³•åœ¨å“åº”è´¨é‡çš„æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºä¼ ç»Ÿçš„é¢„è®­ç»ƒæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.21579",
            "title": "KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices",
            "url": "https://huggingface.co/papers/2601.21579",
            "abstract": "KromHC addresses training instability and scalability issues in hyper-connections by using Kronecker products to parametrize residual matrices with reduced parameter complexity.  \t\t\t\t\tAI-generated summary \t\t\t\t The success of Hyper-Connections (HC) in neural networks (NN) has also highlighted issues related to its training instability and restricted scalability. The Manifold-Constrained Hyper-Connections (mHC) mitigate these challenges by projecting the residual connection space onto a Birkhoff polytope, however, it faces two issues: 1) its iterative Sinkhorn-Knopp (SK) algorithm does not always yield exact doubly stochastic residual matrices; 2) mHC incurs a prohibitive O(n^3C) parameter complexity with n as the width of the residual stream and C as the feature dimension. The recently proposed mHC-lite reparametrizes the residual matrix via the Birkhoff-von-Neumann theorem to guarantee double stochasticity, but also faces a factorial explosion in its parameter complexity, O left( nC cdot n! right). To address both challenges, we propose KromHC, which uses the Kronecker products of smaller doubly stochastic matrices to parametrize the residual matrix in mHC. By enforcing manifold constraints across the factor residual matrices along each mode of the tensorized residual stream, KromHC guarantees exact double stochasticity of the residual matrices while reducing parameter complexity to O(n^2C). Comprehensive experiments demonstrate that KromHC matches or even outperforms state-of-the-art (SOTA) mHC variants, while requiring significantly fewer trainable parameters. The code is available at https://github.com/wz1119/KromHC.",
            "score": 4,
            "issue_id": 851,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "3d6157f88d1aaba5",
            "authors": [
                "Wuyang Zhou",
                "Yuxuan Gu",
                "Giorgos Iacovides",
                "Danilo Mandic"
            ],
            "affiliations": [
                "Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.21579.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ“‰",
                "ru": {
                    "title": "ĞšÑ€Ğ¾Ğ½ĞµĞºĞµÑ€Ğ¾Ğ²Ñ‹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ğ¸Ğ¿ĞµÑ€-ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³Ğ¸Ğ¿ĞµÑ€-ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ KromHC â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ĞšÑ€Ğ¾Ğ½ĞµĞºĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ñ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸ĞµĞ¹ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ñ ĞºÑƒĞ±Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ O(nÂ³C) Ğ´Ğ¾ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ O(nÂ²C), Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ KromHC Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "KromHC: Simplifying Hyper-Connections with Kronecker Products",
                    "desc": "KromHC is a novel approach that addresses the training instability and scalability issues found in hyper-connections within neural networks. It utilizes Kronecker products to effectively parametrize residual matrices, significantly reducing the complexity of parameters needed for training. By enforcing manifold constraints on the factor residual matrices, KromHC ensures that the residual matrices remain doubly stochastic while lowering the parameter complexity to O(n^2C). Experimental results show that KromHC not only matches but can also surpass the performance of existing state-of-the-art methods, all while using fewer trainable parameters."
                },
                "zh": {
                    "title": "KromHCï¼šé™ä½è¶…è¿æ¥çš„è®­ç»ƒå¤æ‚åº¦ä¸ä¸ç¨³å®šæ€§",
                    "desc": "KromHCæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è¶…è¿æ¥ï¼ˆHCï¼‰åœ¨ç¥ç»ç½‘ç»œè®­ç»ƒä¸­çš„ä¸ç¨³å®šæ€§å’Œå¯æ‰©å±•æ€§é—®é¢˜ã€‚å®ƒé€šè¿‡ä½¿ç”¨å…‹ç½—å†…å…‹ç§¯å¯¹æ®‹å·®çŸ©é˜µè¿›è¡Œå‚æ•°åŒ–ï¼Œä»è€Œé™ä½å‚æ•°å¤æ‚åº¦ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”ï¼ŒKromHCç¡®ä¿äº†æ®‹å·®çŸ©é˜µçš„åŒéšæœºæ€§ï¼Œå¹¶å°†å‚æ•°å¤æ‚åº¦å‡å°‘åˆ°O(n^2C)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKromHCåœ¨æ€§èƒ½ä¸Šä¸æœ€å…ˆè¿›çš„mHCå˜ä½“ç›¸å½“ï¼Œç”šè‡³æ›´ä¼˜ï¼ŒåŒæ—¶éœ€è¦çš„å¯è®­ç»ƒå‚æ•°æ˜¾è‘—å‡å°‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.21343",
            "title": "Self-Improving Pretraining: using post-trained models to pretrain better models",
            "url": "https://huggingface.co/papers/2601.21343",
            "abstract": "A reinforcement learning-based pretraining method improves language model safety, factuality, and quality by evaluating generations through a combination of model rollouts, original suffixes, and rewritten suffixes.  \t\t\t\t\tAI-generated summary \t\t\t\t Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully curated datasets and applying multiple stages of fine-tuning and alignment. However, even this complex pipeline cannot guarantee the correction of patterns learned during pretraining. Therefore, addressing these issues during pretraining is crucial, as it shapes a model's core behaviors and prevents unsafe or hallucinated outputs from becoming deeply embedded. To tackle this issue, we introduce a new pretraining method that streams documents and uses reinforcement learning (RL) to improve the next K generated tokens at each step. A strong, post-trained model judges candidate generations -- including model rollouts, the original suffix, and a rewritten suffix -- for quality, safety, and factuality. Early in training, the process relies on the original and rewritten suffixes; as the model improves, RL rewards high-quality rollouts. This approach builds higher quality, safer, and more factual models from the ground up. In experiments, our method gives 36.2% and 18.5% relative improvements over standard pretraining in terms of factuality and safety, and up to 86.3% win rate improvements in overall generation quality.",
            "score": 4,
            "issue_id": 843,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "ce5d96e38f006343",
            "authors": [
                "Ellen Xiaoqing Tan",
                "Shehzaad Dhuliawala",
                "Jing Xu",
                "Ping Yu",
                "Sainbayar Sukhbaatar",
                "Jason Weston",
                "Olga Golovneva"
            ],
            "affiliations": [
                "Meta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.21343.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ„Ğ°ĞºĞ¾Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡ÑƒĞ´ÑŒÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµå¼ºĞ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑĞ°Ğ½Ğ½Ñ‹Ğµ ÑÑƒÑ„Ñ„Ğ¸ĞºÑÑ‹. ĞĞ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑĞ°Ğ½Ğ½Ñ‹Ğµ ÑÑƒÑ„Ñ„Ğ¸ĞºÑÑ‹, Ğ° Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ RL Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ°Ğ¼Ğ¾Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ„Ğ°ĞºĞ¾Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 36,2%, Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 18,5% Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° 86,3% Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Reinforcement Learning for Safer and Factual Language Models",
                    "desc": "This paper presents a novel pretraining method for language models that utilizes reinforcement learning (RL) to enhance safety, factuality, and overall quality of generated text. By evaluating candidate outputs through model rollouts and comparing them with original and rewritten suffixes, the method ensures that the model learns to produce safer and more accurate responses from the beginning. The approach shifts from relying solely on curated datasets to integrating RL rewards that prioritize high-quality outputs as the model matures. Experimental results demonstrate significant improvements in factuality, safety, and generation quality compared to traditional pretraining methods."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ æå‡è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ä¸è´¨é‡",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ã€äº‹å®æ€§å’Œæ•´ä½“è´¨é‡ã€‚è¯¥æ–¹æ³•é€šè¿‡è¯„ä¼°ç”Ÿæˆçš„æ–‡æœ¬ï¼ŒåŒ…æ‹¬æ¨¡å‹çš„å›æ»šã€åŸå§‹åç¼€å’Œé‡å†™åç¼€ï¼Œæ¥ä¼˜åŒ–æ¯ä¸€æ­¥ç”Ÿæˆçš„ä¸‹ä¸€ä¸ªKä¸ªæ ‡è®°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨äº‹å®æ€§å’Œå®‰å…¨æ€§æ–¹é¢ç›¸è¾ƒäºæ ‡å‡†é¢„è®­ç»ƒæœ‰36.2%å’Œ18.5%çš„ç›¸å¯¹æå‡ï¼ŒåŒæ—¶åœ¨æ•´ä½“ç”Ÿæˆè´¨é‡ä¸Šæé«˜äº†86.3%çš„èƒœç‡ã€‚é€šè¿‡åœ¨é¢„è®­ç»ƒé˜¶æ®µè§£å†³è¿™äº›é—®é¢˜ï¼Œå¯ä»¥æœ‰æ•ˆé˜²æ­¢ä¸å®‰å…¨æˆ–è™šæ„çš„è¾“å‡ºæ·±å…¥æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.22101",
            "title": "ECO: Quantized Training without Full-Precision Master Weights",
            "url": "https://huggingface.co/papers/2601.22101",
            "abstract": "Error-compensating optimizer eliminates memory overhead from master weights in quantized LLM training while maintaining near-lossless accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, existing approaches still rely on accumulating their updates in high-precision: concretely, gradient updates must be applied to a high-precision weight buffer, known as master weights. This buffer introduces substantial memory overhead, particularly for Sparse Mixture of Experts (SMoE) models, where model parameters and optimizer states dominate memory usage. To address this, we introduce the Error-Compensating Optimizer (ECO), which eliminates master weights by applying updates directly to quantized parameters. ECO quantizes weights after each step and carefully injects the resulting quantization error into the optimizer momentum, forming an error-feedback loop with no additional memory. We prove that, under standard assumptions and a decaying learning rate, ECO converges to a constant-radius neighborhood of the optimum, while naive master-weight removal can incur an error that is inversely proportional to the learning rate. We show empirical results for pretraining small Transformers (30-800M), a Gemma-3 1B model, and a 2.1B parameter Sparse MoE model with FP8 quantization, and fine-tuning DeepSeek-MoE-16B in INT4 precision. Throughout, ECO matches baselines with master weights up to near-lossless accuracy, significantly shifting the static memory vs validation loss Pareto frontier.",
            "score": 3,
            "issue_id": 847,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "4bfb0adcdfce0a91",
            "authors": [
                "Mahdi Nikdan",
                "Amir Zandieh",
                "Dan Alistarh",
                "Vahab Mirrokni"
            ],
            "affiliations": [
                "Google Research",
                "ISTA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.22101.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#training",
                    "#architecture"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ñ ĞºĞ¾Ğ¼Ğ¿ĞµĞ½ÑĞ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… LLM Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Error-Compensating Optimizer (ECO) â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ½Ğ¾Ğ²Ğ¾Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ECO Ğ¸Ğ·Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ğ¾Ñ‚ Ğ±ÑƒÑ„ĞµÑ€Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²ĞµÑĞ¾Ğ² (master weights) Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğº ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ĞºĞ¾Ğ¼Ğ¿ĞµĞ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ²Ğ½ĞµĞ´Ñ€ÑÑ Ğ¸Ñ… Ğ² Ğ¸Ğ¼Ğ¿ÑƒĞ»ÑŒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ECO ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼ÑƒĞ¼Ñƒ Ñ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ¾Ñ‚ 30Ğœ Ğ´Ğ¾ 16B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ FP8 Ğ¸ INT4."
                },
                "en": {
                    "title": "ECO: Optimizing Memory Efficiency in Quantized LLM Training",
                    "desc": "This paper presents the Error-Compensating Optimizer (ECO), which enhances the training of quantized Large Language Models (LLMs) by removing the need for high-precision master weights. Traditional methods require maintaining a master weight buffer, leading to significant memory overhead, especially in Sparse Mixture of Experts (SMoE) models. ECO directly updates quantized parameters and incorporates quantization errors into the optimizer's momentum, creating an efficient error-feedback loop without additional memory costs. The results demonstrate that ECO achieves near-lossless accuracy while reducing memory usage, thus improving the efficiency of LLM training."
                },
                "zh": {
                    "title": "é”™è¯¯è¡¥å¿ä¼˜åŒ–å™¨ï¼šé‡åŒ–è®­ç»ƒçš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é”™è¯¯è¡¥å¿ä¼˜åŒ–å™¨ï¼ˆECOï¼‰ï¼Œæ—¨åœ¨æ¶ˆé™¤é‡åŒ–å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒä¸­çš„ä¸»æƒé‡å†…å­˜å¼€é”€ï¼ŒåŒæ—¶ä¿æŒæ¥è¿‘æ— æŸçš„å‡†ç¡®æ€§ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºé«˜ç²¾åº¦çš„ä¸»æƒé‡ç¼“å†²åŒºæ¥ç´¯ç§¯æ¢¯åº¦æ›´æ–°ï¼Œè¿™åœ¨ç¨€ç–ä¸“å®¶æ¨¡å‹ä¸­ä¼šå¯¼è‡´æ˜¾è‘—çš„å†…å­˜å ç”¨ã€‚ECOé€šè¿‡ç›´æ¥å¯¹é‡åŒ–å‚æ•°åº”ç”¨æ›´æ–°ï¼Œé¿å…äº†ä¸»æƒé‡çš„ä½¿ç”¨ï¼Œå¹¶åœ¨ä¼˜åŒ–å™¨åŠ¨é‡ä¸­æ³¨å…¥é‡åŒ–è¯¯å·®ï¼Œå½¢æˆä¸€ä¸ªæ— é¢å¤–å†…å­˜çš„åé¦ˆå¾ªç¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒECOåœ¨å°å‹Transformerå’Œç¨€ç–MoEæ¨¡å‹çš„é¢„è®­ç»ƒä¸­ï¼Œèƒ½å¤Ÿåœ¨ä¸å¢åŠ å†…å­˜çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸ä¼¼çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.22054",
            "title": "MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources",
            "url": "https://huggingface.co/papers/2601.22054",
            "abstract": "Metric Anything presents a scalable pretraining framework for metric depth estimation that leverages diverse 3D data and sparse metric prompts to achieve superior performance across multiple vision tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.",
            "score": 3,
            "issue_id": 845,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "ce93aacc808fbae3",
            "authors": [
                "Baorui Ma",
                "Jiahui Yang",
                "Donglin Di",
                "Xuancheng Zhang",
                "Jianxun Cui",
                "Hao Li",
                "Yan Xie",
                "Wei Chen"
            ],
            "affiliations": [
                "Li Auto Inc"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.22054.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#cv",
                    "#architecture",
                    "#transfer_learning",
                    "#open_source",
                    "#dataset",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğº Ğ¿ÑƒÑ‚ÑŒ Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Metric Anything â€” Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ 3D Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ â€” Sparse Metric Prompt (Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ°ÑĞºĞ° Ğ½Ğ° ĞºĞ°Ñ€Ñ‚Ğ°Ñ… Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ ÑˆÑƒĞ¼Ğ¾Ğ² Ğ´Ğ°Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ¸ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° 20 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ° Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ñ‡ĞµÑ‚ĞºÑƒÑ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. ĞŸÑ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…: Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, ÑÑƒĞ¿ĞµÑ€Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ, Ñ„ÑŒÑĞ¶ĞµĞ½ Radar-ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Metric Depth Estimation with Scalable Learning",
                    "desc": "Metric Anything introduces a scalable framework for metric depth estimation that utilizes diverse 3D data and sparse metric prompts. This approach addresses challenges like sensor noise and camera biases by employing a Sparse Metric Prompt, which allows the model to learn depth without needing specific prompts or architectures. The framework is trained on a large dataset of image-depth pairs, demonstrating significant improvements in various vision tasks such as depth completion and monocular depth estimation. By leveraging scaling laws similar to those in foundation models, this work paves the way for more efficient metric perception in real-world applications."
                },
                "zh": {
                    "title": "åº¦é‡æ·±åº¦ä¼°è®¡çš„æ–°è·¯å¾„",
                    "desc": "Metric Anything æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨å¤šæ ·çš„ä¸‰ç»´æ•°æ®å’Œç¨€ç–çš„åº¦é‡æç¤ºæ¥æé«˜åº¦é‡æ·±åº¦ä¼°è®¡çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿä»å˜ˆæ‚çš„ä¸‰ç»´æ•°æ®ä¸­å­¦ä¹ åº¦é‡æ·±åº¦ï¼Œè€Œæ— éœ€æ‰‹åŠ¨è®¾è®¡æç¤ºæˆ–ç‰¹å®šäºç›¸æœºçš„å»ºæ¨¡ã€‚æ ¸å¿ƒåˆ›æ–°æ˜¯ç¨€ç–åº¦é‡æç¤ºï¼Œé€šè¿‡éšæœºé®ç½©æ·±åº¦å›¾æ¥åˆ›å»ºï¼Œèƒ½å¤Ÿå°†ç©ºé—´æ¨ç†ä¸ä¼ æ„Ÿå™¨å’Œç›¸æœºåå·®è§£è€¦ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œé¢„è®­ç»ƒæ¨¡å‹åœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç¤ºäº†åº¦é‡æ·±åº¦ä¼°è®¡çš„æ–°è·¯å¾„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.21996",
            "title": "Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units",
            "url": "https://huggingface.co/papers/2601.21996",
            "abstract": "Mechnistic Data Attribution framework traces interpretable units to specific training samples using influence functions, demonstrating causal relationships between data structure and neural circuit formation in language models.  \t\t\t\t\tAI-generated summary \t\t\t\t While Mechanistic Interpretability has identified interpretable circuits in LLMs, their causal origins in training data remain elusive. We introduce Mechanistic Data Attribution (MDA), a scalable framework that employs Influence Functions to trace interpretable units back to specific training samples. Through extensive experiments on the Pythia family, we causally validate that targeted intervention--removing or augmenting a small fraction of high-influence samples--significantly modulates the emergence of interpretable heads, whereas random interventions show no effect. Our analysis reveals that repetitive structural data (e.g., LaTeX, XML) acts as a mechanistic catalyst. Furthermore, we observe that interventions targeting induction head formation induce a concurrent change in the model's in-context learning (ICL) capability. This provides direct causal evidence for the long-standing hypothesis regarding the functional link between induction heads and ICL. Finally, we propose a mechanistic data augmentation pipeline that consistently accelerates circuit convergence across model scales, providing a principled methodology for steering the developmental trajectories of LLMs.",
            "score": 3,
            "issue_id": 848,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "fa48b853d7f59196",
            "authors": [
                "Jianhui Chen",
                "Yuzhang Luo",
                "Liangming Pan"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence, Beijing, China",
                "State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.21996.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#architecture",
                    "#data",
                    "#training",
                    "#interpretability"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğº ÑÑ…ĞµĞ¼Ğ°Ğ¼: Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Mechanistic Data Attribution (MDA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾ Ğº ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼ Ğ¸Ğ· Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ, Ñ‡Ñ‚Ğ¾ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ²Ğ»Ğ¸ÑÑ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ² Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Pythia. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ¸ĞµÑÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ (Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº LaTeX Ğ¸ XML) Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‚ ĞºĞ°Ğº ĞºĞ°Ñ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ½Ğ´ÑƒĞºÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºÑƒ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸ĞµĞ¼ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑÑ…ĞµĞ¼ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "Tracing Data Influence in Language Models",
                    "desc": "The paper presents a new framework called Mechanistic Data Attribution (MDA) that uses Influence Functions to connect interpretable units in language models to specific training samples. This approach allows researchers to understand how certain data influences the formation of neural circuits in these models. The study shows that by selectively removing or adding influential training samples, the emergence of interpretable features can be significantly altered, while random changes do not have the same effect. Additionally, the findings suggest that certain structured data types enhance the model's learning capabilities, providing a causal link between data characteristics and model performance."
                },
                "zh": {
                    "title": "æœºåˆ¶æ•°æ®å½’å› ï¼šæ­ç¤ºè¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ ·æœ¬å½±å“",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æœºåˆ¶æ•°æ®å½’å› æ¡†æ¶ï¼ˆMDAï¼‰ï¼Œåˆ©ç”¨å½±å“å‡½æ•°å°†å¯è§£é‡Šå•å…ƒè¿½æº¯åˆ°ç‰¹å®šçš„è®­ç»ƒæ ·æœ¬ã€‚é€šè¿‡å¯¹Pythiaç³»åˆ—æ¨¡å‹çš„å®éªŒï¼Œæˆ‘ä»¬éªŒè¯äº†æœ‰é’ˆå¯¹æ€§çš„å¹²é¢„æªæ–½ï¼ˆå¦‚å»é™¤æˆ–å¢åŠ é«˜å½±å“æ ·æœ¬ï¼‰æ˜¾è‘—å½±å“å¯è§£é‡Šå¤´çš„å‡ºç°ï¼Œè€Œéšæœºå¹²é¢„åˆ™æ²¡æœ‰æ•ˆæœã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œé‡å¤çš„ç»“æ„æ€§æ•°æ®ï¼ˆå¦‚LaTeXã€XMLï¼‰åœ¨æ¨¡å‹è®­ç»ƒä¸­èµ·åˆ°äº†å‚¬åŒ–å‰‚çš„ä½œç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°é’ˆå¯¹è¯±å¯¼å¤´å½¢æˆçš„å¹²é¢„ä¼šåŒæ—¶æ”¹å˜æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œæä¾›äº†è¯±å¯¼å¤´ä¸ä¸Šä¸‹æ–‡å­¦ä¹ ä¹‹é—´åŠŸèƒ½è”ç³»çš„ç›´æ¥å› æœè¯æ®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.21406",
            "title": "Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation",
            "url": "https://huggingface.co/papers/2601.21406",
            "abstract": "UniMRG enhances unified multimodal models by training them to generate multiple visual representations, improving both understanding and generation capabilities through complementary information capture.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified Multimodal Models (UMMs) integrate both visual understanding and generation within a single framework. Their ultimate aspiration is to create a cycle where understanding and generation mutually reinforce each other. While recent post-training methods have successfully leveraged understanding to enhance generation, the reverse direction of utilizing generation to improve understanding remains largely unexplored. In this work, we propose UniMRG (Unified Multi-Representation Generation), a simple yet effective architecture-agnostic post-training method. UniMRG enhances the understanding capabilities of UMMs by incorporating auxiliary generation tasks. Specifically, we train UMMs to generate multiple intrinsic representations of input images, namely pixel (reconstruction), depth (geometry), and segmentation (structure), alongside standard visual understanding objectives. By synthesizing these diverse representations, UMMs capture complementary information regarding appearance, spatial relations, and structural layout. Consequently, UMMs develop a deeper and more comprehensive understanding of visual inputs. Extensive experiments across diverse UMM architectures demonstrate that our method notably enhances fine-grained perception, reduces hallucinations, and improves spatial understanding, while simultaneously boosting generation capabilities.",
            "score": 3,
            "issue_id": 842,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "381753eb6ef608c3",
            "authors": [
                "Zihan Su",
                "Hongyang Wei",
                "Kangrui Cen",
                "Yong Wang",
                "Guanhua Chen",
                "Chun Yuan",
                "Xiangxiang Chu"
            ],
            "affiliations": [
                "AMAP, Alibaba Group",
                "Shanghai Jiao Tong University",
                "Southern University of Science and Technology",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.21406.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#training"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ’Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğµ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "UniMRG â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹, ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ¸Ñ… Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğµ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ´Ñ€ÑƒĞ³ÑƒÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹, ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚."
                },
                "en": {
                    "title": "Enhancing Understanding and Generation in Unified Multimodal Models with UniMRG",
                    "desc": "UniMRG is a novel approach that enhances unified multimodal models (UMMs) by training them to generate various visual representations. This method focuses on improving both understanding and generation by capturing complementary information from different tasks. By incorporating auxiliary generation tasks, such as pixel reconstruction, depth estimation, and segmentation, UMMs can develop a richer understanding of visual inputs. The results show that UniMRG significantly improves fine-grained perception and reduces errors in visual generation, leading to better overall performance of UMMs."
                },
                "zh": {
                    "title": "UniMRGï¼šæå‡å¤šæ¨¡æ€æ¨¡å‹çš„ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›",
                    "desc": "UniMRGæ˜¯ä¸€ç§å¢å¼ºç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„åè®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆå¤šç§è§†è§‰è¡¨ç¤ºæ¥æå‡ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚è¯¥æ–¹æ³•ç»“åˆäº†è¾…åŠ©ç”Ÿæˆä»»åŠ¡ï¼Œè®­ç»ƒæ¨¡å‹ç”Ÿæˆè¾“å…¥å›¾åƒçš„åƒç´ ã€æ·±åº¦å’Œåˆ†å‰²ç­‰å¤šç§å†…åœ¨è¡¨ç¤ºã€‚é€šè¿‡ç»¼åˆè¿™äº›ä¸åŒçš„è¡¨ç¤ºï¼Œæ¨¡å‹èƒ½å¤Ÿæ•æ‰åˆ°å¤–è§‚ã€ç©ºé—´å…³ç³»å’Œç»“æ„å¸ƒå±€ç­‰äº’è¡¥ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniMRGæ˜¾è‘—æé«˜äº†ç»†ç²’åº¦æ„ŸçŸ¥èƒ½åŠ›ï¼Œå‡å°‘äº†å¹»è§‰ç°è±¡ï¼Œå¹¶æ”¹å–„äº†ç©ºé—´ç†è§£ï¼ŒåŒæ—¶å¢å¼ºäº†ç”Ÿæˆèƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.20465",
            "title": "BMAM: Brain-inspired Multi-Agent Memory Framework",
            "url": "https://huggingface.co/papers/2601.20465",
            "abstract": "BMAM presents a brain-inspired multi-agent memory architecture that decomposes memory into specialized subsystems to address long-term reasoning challenges in language-model-based agents.  \t\t\t\t\tAI-generated summary \t\t\t\t Language-model-based agents operating over extended interaction horizons face persistent challenges in preserving temporally grounded information and maintaining behavioral consistency across sessions, a failure mode we term soul erosion. We present BMAM (Brain-inspired Multi-Agent Memory), a general-purpose memory architecture that models agent memory as a set of functionally specialized subsystems rather than a single unstructured store. Inspired by cognitive memory systems, BMAM decomposes memory into episodic, semantic, salience-aware, and control-oriented components that operate at complementary time scales. To support long-horizon reasoning, BMAM organizes episodic memories along explicit timelines and retrieves evidence by fusing multiple complementary signals. Experiments on the LoCoMo benchmark show that BMAM achieves 78.45 percent accuracy under the standard long-horizon evaluation setting, and ablation analyses confirm that the hippocampus-inspired episodic memory subsystem plays a critical role in temporal reasoning.",
            "score": 3,
            "issue_id": 845,
            "pub_date": "2026-01-28",
            "pub_date_card": {
                "ru": "28 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 28",
                "zh": "1æœˆ28æ—¥"
            },
            "hash": "7d637c5449781625",
            "authors": [
                "Yang Li",
                "Jiaxiang Liu",
                "Yusong Wang",
                "Yujie Wu",
                "Mingkun Xu"
            ],
            "affiliations": [
                "Guangdong Institute of Intelligence Science and Technology, Zhuhai, China",
                "Institute of Science Tokyo",
                "The Hong Kong Polytechnic University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.20465.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#reasoning",
                    "#long_context",
                    "#agents"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ BMAM, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¼Ğ¾Ğ·Ğ³Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹: ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºÑƒÑ, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ, ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹, Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ·Ğ° Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹. Ğ­Ñ‚Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Â«ÑÑ‚Ğ¸Ñ€Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸Â» â€” ĞºĞ¾Ğ³Ğ´Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚ĞµÑ€ÑÑÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ¸ Ğ½Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¾ÑĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ²Ğ´Ğ¾Ğ»ÑŒ ÑĞ²Ğ½Ğ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑˆĞºĞ°Ğ»Ñ‹ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ LoCoMo Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ 78,45% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ¿Ğ¾Ğ´ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ½Ğ° Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Memory for Long-Term Reasoning in AI Agents",
                    "desc": "BMAM introduces a novel memory architecture for language-model-based agents, inspired by how the human brain organizes memory. It divides memory into specialized subsystems, including episodic, semantic, salience-aware, and control-oriented components, to enhance long-term reasoning. This structure helps agents maintain consistent behavior and retain relevant information over extended interactions, addressing the issue of 'soul erosion.' Experimental results demonstrate that BMAM significantly improves accuracy in long-horizon reasoning tasks, highlighting the importance of its episodic memory subsystem."
                },
                "zh": {
                    "title": "è„‘å¯å‘çš„å¤šæ™ºèƒ½ä½“è®°å¿†æ¶æ„ï¼šè§£å†³é•¿æœŸæ¨ç†æŒ‘æˆ˜",
                    "desc": "BMAMï¼ˆè„‘å¯å‘çš„å¤šæ™ºèƒ½ä½“è®°å¿†æ¶æ„ï¼‰æ˜¯ä¸€ç§æ–°å‹çš„è®°å¿†æ¶æ„ï¼Œæ—¨åœ¨è§£å†³åŸºäºè¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“åœ¨é•¿æœŸæ¨ç†ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ¶æ„å°†è®°å¿†åˆ†è§£ä¸ºå¤šä¸ªåŠŸèƒ½ä¸“é—¨çš„å­ç³»ç»Ÿï¼ŒåŒ…æ‹¬æƒ…èŠ‚è®°å¿†ã€è¯­ä¹‰è®°å¿†ã€æ˜¾è‘—æ€§æ„ŸçŸ¥å’Œæ§åˆ¶å¯¼å‘ç»„ä»¶ï¼Œä»¥ä¾¿åœ¨ä¸åŒæ—¶é—´å°ºåº¦ä¸ŠååŒå·¥ä½œã€‚BMAMé€šè¿‡æ˜ç¡®çš„æ—¶é—´çº¿ç»„ç»‡æƒ…èŠ‚è®°å¿†ï¼Œå¹¶é€šè¿‡èåˆå¤šä¸ªäº’è¡¥ä¿¡å·æ¥æ£€ç´¢è¯æ®ï¼Œä»è€Œæ”¯æŒé•¿æœŸæ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBMAMåœ¨LoCoMoåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†78.45%çš„å‡†ç¡®ç‡ï¼Œè¯æ˜äº†å…¶åœ¨æ—¶é—´æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.22143",
            "title": "JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion",
            "url": "https://huggingface.co/papers/2601.22143",
            "abstract": "A lightweight LoRA adaptation of an audio-video diffusion model enables high-quality video dubbing with preserved speaker identity and improved lip synchronization through synthetic multilingual video training.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines.",
            "score": 2,
            "issue_id": 851,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "4bdc1c53b903065d",
            "authors": [
                "Anthony Chen",
                "Naomi Ken Korem",
                "Tavi Halperin",
                "Matan Ben Yosef",
                "Urska Jelercic",
                "Ofir Bibi",
                "Or Patashnik",
                "Daniel Cohen-Or"
            ],
            "affiliations": [
                "Lightricks",
                "Tel Aviv University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.22143.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#training",
                    "#multimodal",
                    "#audio",
                    "#machine_translation",
                    "#video",
                    "#diffusion"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ›Ñ‘Ğ³ĞºĞ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ´ÑƒĞ±Ğ»ÑĞ¶Ğ°",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ lightweight LoRA Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´ÑƒĞ±Ğ»ÑĞ¶Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ° ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ»Ğ¸Ñ†Ğ° Ğ¸ Ğ·Ğ²ÑƒĞºĞ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾ Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³ÑƒĞ± Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´ÑƒĞ±Ğ»ÑĞ¶Ğ°, ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ÑƒĞ± Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Effortless Video Dubbing with Speaker Identity Preservation",
                    "desc": "This paper presents a novel approach to video dubbing using a lightweight LoRA adaptation of an audio-video diffusion model. The method allows for high-quality dubbing while maintaining the speaker's identity and ensuring accurate lip synchronization. By synthesizing multilingual video training data, the model can generate translated audio and facial movements that align with the original video. The results show that this approach outperforms traditional dubbing methods in terms of visual quality and synchronization accuracy."
                },
                "zh": {
                    "title": "è½»é‡çº§LoRAå®ç°é«˜è´¨é‡è§†é¢‘é…éŸ³",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„LoRAé€‚åº”éŸ³è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œç”¨äºé«˜è´¨é‡çš„è§†é¢‘é…éŸ³ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆæˆå¤šè¯­è¨€è§†é¢‘è®­ç»ƒï¼Œèƒ½å¤Ÿä¿æŒè¯´è¯è€…èº«ä»½å’Œæ”¹å–„å˜´å”‡åŒæ­¥ã€‚æˆ‘ä»¬åˆ©ç”¨ç”Ÿæˆæ¨¡å‹åˆæˆé…å¯¹çš„å¤šè¯­è¨€è§†é¢‘ï¼Œå¹¶åœ¨åŒä¸€ç‰‡æ®µå†…è¿›è¡Œè¯­è¨€åˆ‡æ¢ï¼Œä»¥å®ç°éŸ³é¢‘å’Œé¢éƒ¨åŠ¨ä½œçš„åŒæ­¥ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰ä¿çœŸåº¦ã€å˜´å”‡åŒæ­¥å’Œå¯¹å¤æ‚è¿åŠ¨çš„é²æ£’æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„é…éŸ³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.19001",
            "title": "FROST: Filtering Reasoning Outliers with Attention for Efficient Reasoning",
            "url": "https://huggingface.co/papers/2601.19001",
            "abstract": "FROST is an attention-aware method that improves reasoning efficiency by pruning uncritical paths and removing reasoning outliers, leading to reduced token usage and improved accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose FROST, an attention-aware method for efficient reasoning. Unlike traditional approaches, FROST leverages attention weights to prune uncritical reasoning paths, yielding shorter and more reliable reasoning trajectories. Methodologically, we introduce the concept of reasoning outliers and design an attention-based mechanism to remove them. Theoretically, FROST preserves and enhances the model's reasoning capacity while eliminating outliers at the sentence level. Empirically, we validate FROST on four benchmarks using two strong reasoning models (Phi-4-Reasoning and GPT-OSS-20B), outperforming state-of-the-art methods such as TALE and ThinkLess. Notably, FROST achieves an average 69.68% reduction in token usage and a 26.70% improvement in accuracy over the base model. Furthermore, in evaluations of attention outlier metrics, FROST reduces the maximum infinity norm by 15.97% and the average kurtosis by 91.09% compared to the base model. Code is available at https://github.com/robinzixuan/FROST",
            "score": 2,
            "issue_id": 846,
            "pub_date": "2026-01-26",
            "pub_date_card": {
                "ru": "26 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 26",
                "zh": "1æœˆ26æ—¥"
            },
            "hash": "88cc2522f78b2cd5",
            "authors": [
                "Haozheng Luo",
                "Zhuolin Jiang",
                "Md Zahid Hasan",
                "Yan Chen",
                "Soumalya Sarkar"
            ],
            "affiliations": [
                "Department of Computer Science, Northwestern University, Evanston, IL 60208 USA",
                "Department of Electrical and Computer Engineering, Iowa State University, Ames, IA 50011 USA",
                "RTX Technology Research Center (RTRC), East Hartford, CT 06118 USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.19001.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ¸Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "FROST â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ²ĞµÑĞ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ (attention weights) Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑÑ…. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ Ğ½ĞµĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ğ² Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 69.68% Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 26.70%. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ FROST ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸ Ğ´Ğ°Ğ¶Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ´ĞµĞ»Ğ°Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğ¼."
                },
                "en": {
                    "title": "FROST: Pruning for Efficient and Accurate Reasoning",
                    "desc": "FROST is a novel attention-aware method designed to enhance reasoning efficiency in machine learning models. It intelligently prunes uncritical reasoning paths by utilizing attention weights, which leads to shorter and more reliable reasoning processes. The method also introduces a mechanism to identify and eliminate reasoning outliers, thereby preserving the model's reasoning capacity while improving accuracy. Empirical results demonstrate that FROST significantly reduces token usage and enhances performance compared to existing state-of-the-art methods."
                },
                "zh": {
                    "title": "FROSTï¼šé«˜æ•ˆæ¨ç†çš„æ–°æ–¹æ³•",
                    "desc": "FROSTæ˜¯ä¸€ç§å…³æ³¨æ³¨æ„åŠ›çš„æ¨ç†æ–¹æ³•ï¼Œé€šè¿‡ä¿®å‰ªä¸é‡è¦çš„æ¨ç†è·¯å¾„å’Œå»é™¤æ¨ç†å¼‚å¸¸å€¼ï¼Œæé«˜æ¨ç†æ•ˆç‡ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒFROSTåˆ©ç”¨æ³¨æ„åŠ›æƒé‡æ¥ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œä½¿å¾—æ¨ç†è½¨è¿¹æ›´çŸ­ä¸”æ›´å¯é ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†æ¨ç†å¼‚å¸¸å€¼çš„æ¦‚å¿µï¼Œå¹¶è®¾è®¡äº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„æœºåˆ¶æ¥å»é™¤è¿™äº›å¼‚å¸¸å€¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFROSTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæ˜¾è‘—å‡å°‘äº†ä»¤ç‰Œä½¿ç”¨é‡å¹¶æé«˜äº†å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.20103",
            "title": "Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis",
            "url": "https://huggingface.co/papers/2601.20103",
            "abstract": "Researchers developed a comprehensive benchmark for detecting reward hacking in code generation environments, demonstrating that contrastive anomaly detection outperforms isolated classification approaches and revealing challenges with semantically contextualized reward hacks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reinforcement learning for code generation have made robust environments essential to prevent reward hacking. As LLMs increasingly serve as evaluators in code-based RL, their ability to detect reward hacking remains understudied. In this paper, we propose a novel taxonomy of reward exploits spanning across 54 categories and introduce TRACE (Testing Reward Anomalies in Code Environments), a synthetically curated and human-verified benchmark containing 517 testing trajectories. Unlike prior work that evaluates reward hack detection in isolated classification scenarios, we contrast these evaluations with a more realistic, contrastive anomaly detection setup on TRACE. Our experiments reveal that models capture reward hacks more effectively in contrastive settings than in isolated classification settings, with GPT-5.2 with highest reasoning mode achieving the best detection rate at 63%, up from 45% in isolated settings on TRACE. Building on this insight, we demonstrate that state-of-the-art models struggle significantly more with semantically contextualized reward hacks compared to syntactically contextualized ones. We further conduct qualitative analyses of model behaviors, as well as ablation studies showing that the ratio of benign to hacked trajectories and analysis cluster sizes substantially impact detection performance. We release the benchmark and evaluation harness to enable the community to expand TRACE and evaluate their models.",
            "score": 1,
            "issue_id": 854,
            "pub_date": "2026-01-27",
            "pub_date_card": {
                "ru": "27 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 27",
                "zh": "1æœˆ27æ—¥"
            },
            "hash": "9f38aa7c10392735",
            "authors": [
                "Darshan Deshpande",
                "Anand Kannappan",
                "Rebecca Qian"
            ],
            "affiliations": [
                "Patronus AI, California, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.20103.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#plp",
                    "#rl",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº TRACE Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑÑ€ĞµĞ´Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 517 Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ 54 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ ÑĞºÑĞ¿Ğ»Ğ¾Ğ¹Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸: Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ GPT-5.2 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° 63% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ 45% Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ°Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ñ…ÑƒĞ¶Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ñ‡Ğ¸ÑÑ‚Ñ‹Ñ… Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Reward Hack Detection with TRACE Benchmark",
                    "desc": "This paper introduces a new benchmark called TRACE for detecting reward hacking in code generation environments, which is crucial for improving reinforcement learning systems. The authors show that using contrastive anomaly detection methods is more effective than traditional isolated classification approaches for identifying these hacks. They categorize reward exploits into 54 types and provide a dataset of 517 testing trajectories to facilitate research. The findings indicate that while advanced models like GPT-5.2 can detect reward hacks better in contrastive settings, they still face challenges with semantically contextualized hacks compared to syntactically contextualized ones."
                },
                "zh": {
                    "title": "æå‡ä»£ç ç”Ÿæˆç¯å¢ƒä¸­å¥–åŠ±é»‘å®¢æ£€æµ‹çš„æœ‰æ•ˆæ€§",
                    "desc": "ç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œç”¨äºæ£€æµ‹ä»£ç ç”Ÿæˆç¯å¢ƒä¸­çš„å¥–åŠ±é»‘å®¢è¡Œä¸ºã€‚é€šè¿‡å¯¹æ¯”å¼‚å¸¸æ£€æµ‹æ–¹æ³•çš„å®éªŒï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨æ£€æµ‹æ•ˆæœä¸Šä¼˜äºå­¤ç«‹åˆ†ç±»æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¥–åŠ±åˆ©ç”¨åˆ†ç±»æ³•ï¼Œæ¶µç›–54ä¸ªç±»åˆ«ï¼Œå¹¶å¼•å…¥äº†TRACEåŸºå‡†ï¼ŒåŒ…å«517ä¸ªæµ‹è¯•è½¨è¿¹ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œæ¨¡å‹åœ¨è¯­ä¹‰ä¸Šä¸‹æ–‡å¥–åŠ±é»‘å®¢çš„æ£€æµ‹ä¸Šé¢ä¸´æ›´å¤§æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.17690",
            "title": "Segment Length Matters: A Study of Segment Lengths on Audio Fingerprinting Performance",
            "url": "https://huggingface.co/papers/2601.17690",
            "abstract": "Neural audio fingerprinting performance varies with segment length, with short segments (0.5-second) generally providing better retrieval accuracy, and large language models showing promise in recommending optimal segment durations.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio fingerprinting provides an identifiable representation of acoustic signals, which can be later used for identification and retrieval systems. To obtain a discriminative representation, the input audio is usually segmented into shorter time intervals, allowing local acoustic features to be extracted and analyzed. Modern neural approaches typically operate on short, fixed-duration audio segments, yet the choice of segment duration is often made heuristically and rarely examined in depth. In this paper, we study how segment length affects audio fingerprinting performance. We extend an existing neural fingerprinting architecture to adopt various segment lengths and evaluate retrieval accuracy across different segment lengths and query durations. Our results show that short segment lengths (0.5-second) generally achieve better performance. Moreover, we evaluate LLM capacity in recommending the best segment length, which shows that GPT-5-mini consistently gives the best suggestions across five considerations among three studied LLMs. Our findings provide practical guidance for selecting segment duration in large-scale neural audio retrieval systems.",
            "score": 1,
            "issue_id": 843,
            "pub_date": "2026-01-25",
            "pub_date_card": {
                "ru": "25 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 25",
                "zh": "1æœˆ25æ—¥"
            },
            "hash": "f348f70b77a2c114",
            "authors": [
                "Ziling Gong",
                "Yunyan Ouyang",
                "Iram Kamdar",
                "Melody Ma",
                "Hongjie Chen",
                "Franck Dernoncourt",
                "Ryan A. Rossi",
                "Nesreen K. Ahmed"
            ],
            "affiliations": [
                "Adobe Research",
                "Cisco Research",
                "Data Science Institute Columbia University",
                "Dolby Laboratories"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.17690.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#architecture",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ğ° ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°: ĞºĞ»ÑÑ‡ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾Ñ„Ğ¸Ğ½Ğ³ĞµÑ€Ğ¿Ñ€Ğ¸Ğ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾Ñ„Ğ¸Ğ½Ğ³ĞµÑ€Ğ¿Ñ€Ğ¸Ğ½Ñ‚Ğ¾Ğ² â€” ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ğ»Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ÑƒÑ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ·Ğ°Ğ¿Ğ¸ÑÑĞ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ñ‹ (0,5 ÑĞµĞºÑƒĞ½Ğ´Ñ‹) Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ LLM Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ² Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ GPT-mini, Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ´Ğ»Ğ¸Ğ½Ñƒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Short Segments, Better Retrieval: Optimizing Audio Fingerprinting",
                    "desc": "This paper investigates how the length of audio segments impacts the performance of neural audio fingerprinting systems. It finds that shorter segments, specifically 0.5 seconds, tend to yield higher retrieval accuracy compared to longer segments. The authors also explore the use of large language models (LLMs) to recommend optimal segment durations, with GPT-5-mini performing the best in their evaluations. The study provides valuable insights for improving audio retrieval systems by guiding the selection of segment lengths."
                },
                "zh": {
                    "title": "çŸ­ç‰‡æ®µé•¿åº¦æå‡éŸ³é¢‘æŒ‡çº¹è¯†åˆ«æ€§èƒ½",
                    "desc": "æœ¬è®ºæ–‡ç ”ç©¶äº†éŸ³é¢‘æŒ‡çº¹è¯†åˆ«ä¸­ç‰‡æ®µé•¿åº¦å¯¹æ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬å‘ç°ï¼Œè¾ƒçŸ­çš„ç‰‡æ®µï¼ˆ0.5ç§’ï¼‰é€šå¸¸èƒ½æä¾›æ›´å¥½çš„æ£€ç´¢å‡†ç¡®ç‡ã€‚é€šè¿‡æ‰©å±•ç°æœ‰çš„ç¥ç»æŒ‡çº¹æ¶æ„ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸åŒç‰‡æ®µé•¿åº¦å’ŒæŸ¥è¯¢æŒç»­æ—¶é—´ä¸‹çš„æ£€ç´¢å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨èæœ€ä½³ç‰‡æ®µé•¿åº¦æ–¹é¢çš„èƒ½åŠ›ï¼Œç»“æœè¡¨æ˜GPT-5-miniåœ¨å¤šä¸ªè€ƒè™‘å› ç´ ä¸­å§‹ç»ˆæä¾›æœ€ä½³å»ºè®®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.11747",
            "title": "PRISM: Learning Design Knowledge from Data for Stylistic Design Improvement",
            "url": "https://huggingface.co/papers/2601.11747",
            "abstract": "PRISM leverages design data to create a knowledge base for improving graphic designs based on natural language instructions, achieving superior style alignment compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphic design often involves exploring different stylistic directions, which can be time-consuming for non-experts. We address this problem of stylistically improving designs based on natural language instructions. While VLMs have shown initial success in graphic design, their pretrained knowledge on styles is often too general and misaligned with specific domain data. For example, VLMs may associate minimalism with abstract designs, whereas designers emphasize shape and color choices. Our key insight is to leverage design data -- a collection of real-world designs that implicitly capture designer's principles -- to learn design knowledge and guide stylistic improvement. We propose PRISM (PRior-Informed Stylistic Modification) that constructs and applies a design knowledge base through three stages: (1) clustering high-variance designs to capture diversity within a style, (2) summarizing each cluster into actionable design knowledge, and (3) retrieving relevant knowledge during inference to enable style-aware improvement. Experiments on the Crello dataset show that PRISM achieves the highest average rank of 1.49 (closer to 1 is better) over baselines in style alignment. User studies further validate these results, showing that PRISM is consistently preferred by designers.",
            "score": 1,
            "issue_id": 843,
            "pub_date": "2026-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "0852bbbe8a7fb6af",
            "authors": [
                "Huaxiaoyue Wang",
                "Sunav Choudhary",
                "Franck Dernoncourt",
                "Yu Shen",
                "Stefano Petrangeli"
            ],
            "affiliations": [
                "Adobe Research",
                "Cornell University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.11747.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ—Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ PRISM Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ±Ğ°Ğ·Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° (VLM) Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸Ğ¼ĞµÑÑ‚ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ ÑÑ‚Ğ¸Ğ»ÑÑ… Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ PRISM Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ° Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Crello Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñƒ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ĞµÑ€Ğ¾Ğ² Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ PRISM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ‚Ğ¸Ğ»Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "PRISM: Tailoring Graphic Design with Smart Style Insights",
                    "desc": "PRISM is a machine learning framework that enhances graphic design by utilizing a specialized knowledge base derived from real-world design data. It addresses the limitations of existing Visual Language Models (VLMs) that often provide overly general stylistic guidance. By clustering diverse designs, summarizing them into actionable insights, and retrieving relevant knowledge during the design process, PRISM enables more precise stylistic modifications based on natural language instructions. Experimental results demonstrate that PRISM significantly improves style alignment, making it a preferred tool among designers."
                },
                "zh": {
                    "title": "åˆ©ç”¨è®¾è®¡æ•°æ®æå‡å›¾å½¢è®¾è®¡çš„æ™ºèƒ½åŒ–",
                    "desc": "PRISMæ˜¯ä¸€ç§åˆ©ç”¨è®¾è®¡æ•°æ®åˆ›å»ºçŸ¥è¯†åº“çš„æ–¹æ³•ï¼Œæ—¨åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ”¹å–„å›¾å½¢è®¾è®¡ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸‰ä¸ªé˜¶æ®µæ„å»ºå’Œåº”ç”¨è®¾è®¡çŸ¥è¯†åº“ï¼ŒåŒ…æ‹¬èšç±»é«˜æ–¹å·®è®¾è®¡ã€æ€»ç»“æ¯ä¸ªèšç±»çš„å¯æ“ä½œè®¾è®¡çŸ¥è¯†ï¼Œä»¥åŠåœ¨æ¨ç†è¿‡ç¨‹ä¸­æ£€ç´¢ç›¸å…³çŸ¥è¯†ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒPRISMåœ¨é£æ ¼å¯¹é½æ–¹é¢è¡¨ç°æ›´ä¼˜ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶åœ¨Crelloæ•°æ®é›†ä¸Šçš„å¹³å‡æ’åä¸º1.49ï¼Œè¶Šæ¥è¿‘1è¶Šå¥½ã€‚ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†è¿™äº›ç»“æœï¼Œè¡¨æ˜è®¾è®¡å¸ˆæ›´å€¾å‘äºä½¿ç”¨PRISMã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.21872",
            "title": "WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents",
            "url": "https://huggingface.co/papers/2601.21872",
            "abstract": "WebArbiter introduces a reasoning-first WebPRM that formulates reward modeling as text generation to improve web navigation through structured justifications and preference verdicts, outperforming existing baselines in complex web environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Web agents hold great potential for automating complex computer tasks, yet their interactions involve long-horizon, sequential decision-making with irreversible actions. In such settings, outcome-based supervision is sparse and delayed, often rewarding incorrect trajectories and failing to support inference-time scaling. This motivates the use of Process Reward Models (WebPRMs) for web navigation, but existing approaches remain limited: scalar WebPRMs collapse progress into coarse, weakly grounded signals, while checklist-based WebPRMs rely on brittle template matching that fails under layout or semantic changes and often mislabels superficially correct actions as successful, providing little insight or interpretability. To address these challenges, we introduce WebArbiter, a reasoning-first, principle-inducing WebPRM that formulates reward modeling as text generation, producing structured justifications that conclude with a preference verdict and identify the action most conducive to task completion under the current context. Training follows a two-stage pipeline: reasoning distillation equips the model with coherent principle-guided reasoning, and reinforcement learning corrects teacher biases by directly aligning verdicts with correctness, enabling stronger generalization. To support systematic evaluation, we release WebPRMBench, a comprehensive benchmark spanning four diverse web environments with rich tasks and high-quality preference annotations. On WebPRMBench, WebArbiter-7B outperforms the strongest baseline, GPT-5, by 9.1 points. In reward-guided trajectory search on WebArena-Lite, it surpasses the best prior WebPRM by up to 7.2 points, underscoring its robustness and practical value in real-world complex web tasks.",
            "score": 0,
            "issue_id": 842,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "c457284cedf41902",
            "authors": [
                "Yao Zhang",
                "Shijie Tang",
                "Zeyu Li",
                "Zhen Han",
                "Volker Tresp"
            ],
            "affiliations": [
                "LMU Munich",
                "Munich Center for Machine Learning (MCML)",
                "Technical University of Munich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.21872.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#reasoning",
                    "#rlhf",
                    "#benchmark",
                    "#open_source",
                    "#interpretability",
                    "#agents"
                ],
                "emoji": "ğŸ•·ï¸",
                "ru": {
                    "title": "Ğ Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµĞ±-Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "WebArbiter Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¼ Ğ²ĞµÑ€Ğ´Ğ¸ĞºÑ‚Ğ¾Ğ¼ Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑĞºĞ°Ğ»ÑÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ. ĞĞ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ WebPRMBench Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° 7-9 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²ĞµĞ±-Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Web Navigation with Reasoning-First Reward Models",
                    "desc": "WebArbiter presents a novel approach to reward modeling in web navigation by treating it as a text generation task. This method enhances the interpretability of web agents by providing structured justifications and preference verdicts for actions taken. The model is trained using a two-stage pipeline that combines reasoning distillation and reinforcement learning to improve decision-making accuracy. In evaluations, WebArbiter significantly outperforms existing models, demonstrating its effectiveness in complex web environments."
                },
                "zh": {
                    "title": "æ¨ç†é©±åŠ¨çš„ç½‘é¡µå¯¼èˆªæ–°æ–¹æ³•",
                    "desc": "WebArbiter æ˜¯ä¸€ç§ä»¥æ¨ç†ä¸ºå…ˆçš„ WebPRMï¼Œå®ƒå°†å¥–åŠ±å»ºæ¨¡è§†ä¸ºæ–‡æœ¬ç”Ÿæˆï¼Œä»¥é€šè¿‡ç»“æ„åŒ–çš„ç†ç”±å’Œåå¥½è£å†³æ¥æ”¹å–„ç½‘é¡µå¯¼èˆªã€‚åœ¨å¤æ‚çš„ç½‘é¡µç¯å¢ƒä¸­ï¼ŒWebArbiter è¶…è¶Šäº†ç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œæä¾›äº†æ›´å¼ºçš„å¯è§£é‡Šæ€§å’Œå‡†ç¡®æ€§ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸¤é˜¶æ®µçš„è®­ç»ƒæµç¨‹ï¼Œé¦–å…ˆè¿›è¡Œæ¨ç†è’¸é¦ï¼Œç„¶åé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¥çº æ­£æ•™å¸ˆåå·®ï¼Œä»è€Œå®ç°æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº† WebPRMBenchï¼Œä¸€ä¸ªæ¶µç›–å››ä¸ªä¸åŒç½‘é¡µç¯å¢ƒçš„åŸºå‡†ï¼Œä»¥æ”¯æŒç³»ç»Ÿè¯„ä¼°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.21416",
            "title": "Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation",
            "url": "https://huggingface.co/papers/2601.21416",
            "abstract": "Slot-Based Object-Centric Representations outperform global and dense feature representations in robotic manipulation tasks by providing better generalization under visual distribution shifts.  \t\t\t\t\tAI-generated summary \t\t\t\t The generalization capabilities of robotic manipulation policies are heavily influenced by the choice of visual representations. Existing approaches typically rely on representations extracted from pre-trained encoders, using two dominant types of features: global features, which summarize an entire image via a single pooled vector, and dense features, which preserve a patch-wise embedding from the final encoder layer. While widely used, both feature types mix task-relevant and irrelevant information, leading to poor generalization under distribution shifts, such as changes in lighting, textures, or the presence of distractors. In this work, we explore an intermediate structured alternative: Slot-Based Object-Centric Representations (SBOCR), which group dense features into a finite set of object-like entities. This representation permits to naturally reduce the noise provided to the robotic manipulation policy while keeping enough information to efficiently perform the task. We benchmark a range of global and dense representations against intermediate slot-based representations, across a suite of simulated and real-world manipulation tasks ranging from simple to complex. We evaluate their generalization under diverse visual conditions, including changes in lighting, texture, and the presence of distractors. Our findings reveal that SBOCR-based policies outperform dense and global representation-based policies in generalization settings, even without task-specific pretraining. These insights suggest that SBOCR is a promising direction for designing visual systems that generalize effectively in dynamic, real-world robotic environments.",
            "score": 0,
            "issue_id": 845,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "a204556216ae2010",
            "authors": [
                "Alexandre Chapin",
                "Bruno Machado",
                "Emmanuel DellandrÃ©a",
                "Liming Chen"
            ],
            "affiliations": [
                "Ã‰cole Centrale de Lyon"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.21416.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ»Ğ¾Ñ‚Ñ‹ â€” ĞºĞ»ÑÑ‡ Ğº Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ»Ğ¾Ñ‚Ğ¾Ğ² (SBOCR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ² Ğ½Ğ°Ğ±Ğ¾Ñ€ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… Ğ½Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹, Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SBOCR-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ´Ğ²Ğ¸Ğ³Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ, Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ°Ğº Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸."
                },
                "en": {
                    "title": "Enhancing Robotic Manipulation with Slot-Based Object-Centric Representations",
                    "desc": "This paper introduces Slot-Based Object-Centric Representations (SBOCR) as a new approach for visual representation in robotic manipulation tasks. Unlike traditional global and dense features that mix relevant and irrelevant information, SBOCR organizes dense features into distinct object-like slots, reducing noise and enhancing task performance. The authors demonstrate that SBOCR significantly improves generalization under varying visual conditions, such as changes in lighting and textures. Their experiments show that policies using SBOCR outperform those based on global and dense representations, highlighting its potential for effective visual systems in robotics."
                },
                "zh": {
                    "title": "æ§½åŸºç‰©ä½“ä¸­å¿ƒè¡¨ç¤ºæå‡æœºå™¨äººæ“ä½œçš„æ³›åŒ–èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼ŒåŸºäºæ§½çš„ç‰©ä½“ä¸­å¿ƒè¡¨ç¤ºï¼ˆSBOCRï¼‰ç›¸è¾ƒäºå…¨å±€å’Œå¯†é›†ç‰¹å¾è¡¨ç¤ºçš„ä¼˜åŠ¿ã€‚SBOCRé€šè¿‡å°†å¯†é›†ç‰¹å¾åˆ†ç»„ä¸ºæœ‰é™çš„ç‰©ä½“æ ·æœ¬ï¼Œå‡å°‘äº†ä»»åŠ¡ç›¸å…³å’Œæ— å…³ä¿¡æ¯çš„æ··åˆï¼Œä»è€Œæé«˜äº†åœ¨è§†è§‰åˆ†å¸ƒå˜åŒ–ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒSBOCRåœ¨ä¸åŒçš„è§†è§‰æ¡ä»¶ä¸‹è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„å…¨å±€å’Œå¯†é›†ç‰¹å¾è¡¨ç¤ºï¼Œå°¤å…¶æ˜¯åœ¨å…‰ç…§ã€çº¹ç†å˜åŒ–å’Œå¹²æ‰°ç‰©å­˜åœ¨çš„æƒ…å†µä¸‹ã€‚è¯¥æ–¹æ³•ä¸ºè®¾è®¡èƒ½å¤Ÿåœ¨åŠ¨æ€çœŸå®ç¯å¢ƒä¸­æœ‰æ•ˆæ³›åŒ–çš„è§†è§‰ç³»ç»Ÿæä¾›äº†æ–°çš„æ€è·¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.21282",
            "title": "WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models",
            "url": "https://huggingface.co/papers/2601.21282",
            "abstract": "WorldBench is introduced as a video-based benchmark for disentangled evaluation of physical reasoning in generative models, revealing specific failure patterns in current state-of-the-art video world models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in generative foundational models, often termed \"world models,\" have propelled interest in applying them to critical tasks like robotic planning and autonomous system training. For reliable deployment, these models must exhibit high physical fidelity, accurately simulating real-world dynamics. Existing physics-based video benchmarks, however, suffer from entanglement, where a single test simultaneously evaluates multiple physical laws and concepts, fundamentally limiting their diagnostic capability. We introduce WorldBench, a novel video-based benchmark specifically designed for concept-specific, disentangled evaluation, allowing us to rigorously isolate and assess understanding of a single physical concept or law at a time. To make WorldBench comprehensive, we design benchmarks at two different levels: 1) an evaluation of intuitive physical understanding with concepts such as object permanence or scale/perspective, and 2) an evaluation of low-level physical constants and material properties such as friction coefficients or fluid viscosity. When SOTA video-based world models are evaluated on WorldBench, we find specific patterns of failure in particular physics concepts, with all tested models lacking the physical consistency required to generate reliable real-world interactions. Through its concept-specific evaluation, WorldBench offers a more nuanced and scalable framework for rigorously evaluating the physical reasoning capabilities of video generation and world models, paving the way for more robust and generalizable world-model-driven learning.",
            "score": 0,
            "issue_id": 842,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "038067aeb1ba0b16",
            "authors": [
                "Rishi Upadhyay",
                "Howard Zhang",
                "Jim Solomon",
                "Ayush Agrawal",
                "Pranay Boreddy",
                "Shruti Satya Narayana",
                "Yunhao Ba",
                "Alex Wong",
                "Celso M de Melo",
                "Achuta Kadambi"
            ],
            "affiliations": [
                "DEVCOM Army Research Laboratory",
                "Sony AI",
                "University of California, Los Angeles",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.21282.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#video",
                    "#robotics"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ WorldBench â€” Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²: Ğ¾Ğ½Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑ‚Ğ°Ğ½Ğ½Ñ‹, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ñ‚Ñ€ÑƒĞ´Ğ½ÑĞµÑ‚ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºÑƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. WorldBench Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ…: Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ (Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½ÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ°) Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ñ‹ (ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ñ‹ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ, Ğ²ÑĞ·ĞºĞ¾ÑÑ‚ÑŒ). ĞŸÑ€Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¸Ñ€Ğ° Ğ½Ğ° WorldBench Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ñ‹ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ¾Ğ², ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ½Ğ° Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹."
                },
                "en": {
                    "title": "WorldBench: Disentangling Physical Reasoning in Video Models",
                    "desc": "WorldBench is a new benchmark designed to evaluate how well generative models understand physical concepts in videos. Unlike previous benchmarks that mix multiple physical laws, WorldBench allows for testing one concept at a time, making it easier to identify specific weaknesses in these models. The benchmark assesses both intuitive physical understanding and low-level physical properties, revealing that current state-of-the-art models often struggle with maintaining physical consistency. By providing a clearer evaluation framework, WorldBench aims to improve the reliability of models used in applications like robotic planning and autonomous systems."
                },
                "zh": {
                    "title": "WorldBenchï¼šè§£è€¦è¯„ä¼°ç‰©ç†æ¨ç†çš„æ–°åŸºå‡†",
                    "desc": "WorldBench æ˜¯ä¸€ä¸ªåŸºäºè§†é¢‘çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å¯¹ç”Ÿæˆæ¨¡å‹ä¸­çš„ç‰©ç†æ¨ç†è¿›è¡Œè§£è€¦è¯„ä¼°ã€‚å®ƒæ­ç¤ºäº†å½“å‰æœ€å…ˆè¿›è§†é¢‘ä¸–ç•Œæ¨¡å‹åœ¨ç‰¹å®šç‰©ç†æ¦‚å¿µä¸Šçš„å¤±è´¥æ¨¡å¼ã€‚é€šè¿‡è®¾è®¡ä¸¤ä¸ªä¸åŒå±‚æ¬¡çš„åŸºå‡†ï¼ŒWorldBench èƒ½å¤Ÿå•ç‹¬è¯„ä¼°ç‰©ä½“æŒä¹…æ€§ã€æ¯”ä¾‹/è§†è§’ç­‰ç›´è§‚ç‰©ç†ç†è§£ï¼Œä»¥åŠæ‘©æ“¦ç³»æ•°ã€æµä½“ç²˜åº¦ç­‰ä½çº§ç‰©ç†å¸¸æ•°ã€‚è¯¥åŸºå‡†æµ‹è¯•ä¸ºè¯„ä¼°è§†é¢‘ç”Ÿæˆå’Œä¸–ç•Œæ¨¡å‹çš„ç‰©ç†æ¨ç†èƒ½åŠ›æä¾›äº†æ›´ç»†è‡´å’Œå¯æ‰©å±•çš„æ¡†æ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.20381",
            "title": "STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation",
            "url": "https://huggingface.co/papers/2601.20381",
            "abstract": "STORM enhances robotic manipulation by adapting visual foundation models with semantic-aware slots through multi-phase training, improving generalization and control performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual foundation models provide strong perceptual features for robotics, but their dense representations lack explicit object-level structure, limiting robustness and contractility in manipulation tasks. We propose STORM (Slot-based Task-aware Object-centric Representation for robotic Manipulation), a lightweight object-centric adaptation module that augments frozen visual foundation models with a small set of semantic-aware slots for robotic manipulation. Rather than retraining large backbones, STORM employs a multi-phase training strategy: object-centric slots are first stabilized through visual--semantic pretraining using language embeddings, then jointly adapted with a downstream manipulation policy. This staged learning prevents degenerate slot formation and preserves semantic consistency while aligning perception with task objectives. Experiments on object discovery benchmarks and simulated manipulation tasks show that STORM improves generalization to visual distractors, and control performance compared to directly using frozen foundation model features or training object-centric representations end-to-end. Our results highlight multi-phase adaptation as an efficient mechanism for transforming generic foundation model features into task-aware object-centric representations for robotic control.",
            "score": 0,
            "issue_id": 845,
            "pub_date": "2026-01-28",
            "pub_date_card": {
                "ru": "28 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 28",
                "zh": "1æœˆ28æ—¥"
            },
            "hash": "df62552917db8182",
            "authors": [
                "Alexandre Chapin",
                "Emmanuel DellandrÃ©a",
                "Liming Chen"
            ],
            "affiliations": [
                "Ecole Centrale de Lyon"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.20381.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#robotics",
                    "#architecture"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ„Ğ°Ğ·Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ STORM â€” Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ foundation models ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ»Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-Ğ±ÑĞºĞ±Ğ¾Ğ½Ğ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ„Ğ°Ğ·Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑĞ»Ğ¾Ñ‚Ñ‹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ€Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ñ‚Ğ¾Ğ² Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ STORM ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ¼ĞµÑ…Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ÑĞ¼Ñ‹Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² foundation models."
                },
                "en": {
                    "title": "Transforming Visual Models for Smarter Robotic Manipulation",
                    "desc": "STORM is a method that enhances robotic manipulation by integrating visual foundation models with semantic-aware slots. It addresses the limitations of dense representations in these models, which often lack clear object-level structure. By using a multi-phase training approach, STORM stabilizes object-centric slots through visual-semantic pretraining and then adapts them alongside a manipulation policy. This process improves the robot's ability to generalize to new visual scenarios and enhances its control performance in manipulation tasks."
                },
                "zh": {
                    "title": "STORMï¼šæå‡æœºå™¨äººæ“ä½œçš„æ™ºèƒ½è§£å†³æ–¹æ¡ˆ",
                    "desc": "STORMæ˜¯ä¸€ç§å¢å¼ºæœºå™¨äººæ“ä½œèƒ½åŠ›çš„æ–¹æ³•ï¼Œé€šè¿‡å¤šé˜¶æ®µè®­ç»ƒå°†è§†è§‰åŸºç¡€æ¨¡å‹ä¸è¯­ä¹‰æ„ŸçŸ¥æ§½ç»“åˆèµ·æ¥ï¼Œä»è€Œæé«˜äº†æ³›åŒ–èƒ½åŠ›å’Œæ§åˆ¶æ€§èƒ½ã€‚ä¼ ç»Ÿçš„è§†è§‰åŸºç¡€æ¨¡å‹è™½ç„¶æä¾›äº†å¼ºå¤§çš„æ„ŸçŸ¥ç‰¹å¾ï¼Œä½†å…¶å¯†é›†è¡¨ç¤ºç¼ºä¹æ˜ç¡®çš„å¯¹è±¡çº§ç»“æ„ï¼Œé™åˆ¶äº†åœ¨æ“ä½œä»»åŠ¡ä¸­çš„é²æ£’æ€§å’Œå¯æ§æ€§ã€‚STORMé€šè¿‡è½»é‡çº§çš„å¯¹è±¡ä¸­å¿ƒé€‚åº”æ¨¡å—ï¼Œåˆ©ç”¨å°è§„æ¨¡çš„è¯­ä¹‰æ„ŸçŸ¥æ§½æ¥å¢å¼ºå†»ç»“çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œè€Œä¸æ˜¯é‡æ–°è®­ç»ƒå¤§å‹æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSTORMåœ¨å¤„ç†è§†è§‰å¹²æ‰°ç‰©æ—¶çš„æ³›åŒ–èƒ½åŠ›å’Œæ§åˆ¶æ€§èƒ½ä¼˜äºç›´æ¥ä½¿ç”¨å†»ç»“åŸºç¡€æ¨¡å‹ç‰¹å¾æˆ–ç«¯åˆ°ç«¯è®­ç»ƒå¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.18005",
            "title": "Flow-based Extremal Mathematical Structure Discovery",
            "url": "https://huggingface.co/papers/2601.18005",
            "abstract": "FlowBoost is a closed-loop generative framework that combines geometry-aware flow-matching, reward-guided policy optimization, and stochastic local search to efficiently discover extremal geometric structures with improved results over existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t The discovery of extremal structures in mathematics requires navigating vast and nonconvex landscapes where analytical methods offer little guidance and brute-force search becomes intractable. We introduce FlowBoost, a closed-loop generative framework that learns to discover rare and extremal geometric structures by combining three components: (i) a geometry-aware conditional flow-matching model that learns to sample high-quality configurations, (ii) reward-guided policy optimization with action exploration that directly optimizes the generation process toward the objective while maintaining diversity, and (iii) stochastic local search for both training-data generation and final refinement. Unlike prior open-loop approaches, such as PatternBoost that retrains on filtered discrete samples, or AlphaEvolve which relies on frozen Large Language Models (LLMs) as evolutionary mutation operators, FlowBoost enforces geometric feasibility during sampling, and propagates reward signal directly into the generative model, closing the optimization loop and requiring much smaller training sets and shorter training times, and reducing the required outer-loop iterations by orders of magnitude, while eliminating dependence on LLMs. We demonstrate the framework on four geometric optimization problems: sphere packing in hypercubes, circle packing maximizing sum of radii, the Heilbronn triangle problem, and star discrepancy minimization. In several cases, FlowBoost discovers configurations that match or exceed the best known results. For circle packings, we improve the best known lower bounds, surpassing the LLM-based system AlphaEvolve while using substantially fewer computational resources.",
            "score": 0,
            "issue_id": 858,
            "pub_date": "2026-01-25",
            "pub_date_card": {
                "ru": "25 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 25",
                "zh": "1æœˆ25æ—¥"
            },
            "hash": "ad90f58c7a187cee",
            "authors": [
                "Gergely BÃ©rczi",
                "Baran Hashemi",
                "Jonas KlÃ¼ver"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2601.18005.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#architecture",
                    "#math",
                    "#training"
                ],
                "emoji": "ğŸŒŸ",
                "ru": {
                    "title": "Ğ—Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğµ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ",
                    "desc": "FlowBoost Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹: Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğº Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², FlowBoost Ğ·Ğ°ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ†Ğ¸ĞºĞ» Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑƒĞ¿Ğ°ĞºĞ¾Ğ²ĞºĞ¸ ÑÑ„ĞµÑ€, ÑƒĞ¿Ğ°ĞºĞ¾Ğ²ĞºĞ¸ ĞºÑ€ÑƒĞ³Ğ¾Ğ² Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ²Ñ‘Ğ·Ğ´Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑĞºÑ€ĞµĞ¿Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ², Ñ‡ĞµĞ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "FlowBoost: Revolutionizing Geometric Structure Discovery",
                    "desc": "FlowBoost is a novel generative framework designed to efficiently discover rare geometric structures by integrating three key components: a geometry-aware flow-matching model, reward-guided policy optimization, and stochastic local search. This closed-loop approach allows the model to learn and sample high-quality configurations while optimizing the generation process towards specific objectives. Unlike previous methods that rely on large datasets or fixed models, FlowBoost maintains geometric feasibility and directly incorporates reward signals, significantly reducing training time and resource requirements. The framework has shown promising results in various geometric optimization problems, achieving or surpassing the best-known configurations with fewer computational resources."
                },
                "zh": {
                    "title": "FlowBoostï¼šé«˜æ•ˆå‘ç°æç«¯å‡ ä½•ç»“æ„çš„é—­ç¯ç”Ÿæˆæ¡†æ¶",
                    "desc": "FlowBoost æ˜¯ä¸€ä¸ªé—­ç¯ç”Ÿæˆæ¡†æ¶ï¼Œç»“åˆäº†å‡ ä½•æ„ŸçŸ¥æµåŒ¹é…ã€å¥–åŠ±å¼•å¯¼çš„ç­–ç•¥ä¼˜åŒ–å’Œéšæœºå±€éƒ¨æœç´¢ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å‘ç°æç«¯å‡ ä½•ç»“æ„ã€‚è¯¥æ–¹æ³•é€šè¿‡å­¦ä¹ é«˜è´¨é‡é…ç½®çš„æ¡ä»¶æµåŒ¹é…æ¨¡å‹ï¼Œä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹å¹¶ä¿æŒå¤šæ ·æ€§ï¼ŒåŒæ—¶è¿›è¡Œè®­ç»ƒæ•°æ®ç”Ÿæˆå’Œæœ€ç»ˆç²¾ç‚¼ã€‚ä¸ä»¥å¾€çš„å¼€æ”¾å¼æ–¹æ³•ä¸åŒï¼ŒFlowBoost åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­å¼ºåˆ¶æ‰§è¡Œå‡ ä½•å¯è¡Œæ€§ï¼Œå¹¶å°†å¥–åŠ±ä¿¡å·ç›´æ¥ä¼ æ’­åˆ°ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œä»è€Œç¼©çŸ­äº†è®­ç»ƒæ—¶é—´å¹¶å‡å°‘äº†æ‰€éœ€çš„è®­ç»ƒé›†è§„æ¨¡ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªå‡ ä½•ä¼˜åŒ–é—®é¢˜ä¸Šå±•ç¤ºäº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå‘ç°çš„é…ç½®åœ¨å¤šä¸ªæƒ…å†µä¸‹åŒ¹é…æˆ–è¶…è¿‡äº†å·²çŸ¥çš„æœ€ä½³ç»“æœã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-01-29.html",
    "link_next": "2026-02-02.html",
    "link_month": "2026-01.html",
    "short_date_prev": {
        "ru": "29.01",
        "en": "01/29",
        "zh": "1æœˆ29æ—¥"
    },
    "short_date_next": {
        "ru": "02.02",
        "en": "02/02",
        "zh": "2æœˆ2æ—¥"
    },
    "categories": {
        "#dataset": 11,
        "#data": 3,
        "#benchmark": 15,
        "#agents": 6,
        "#cv": 7,
        "#rl": 8,
        "#rlhf": 4,
        "#rag": 0,
        "#plp": 2,
        "#inference": 10,
        "#3d": 1,
        "#audio": 4,
        "#video": 4,
        "#multimodal": 10,
        "#math": 2,
        "#multilingual": 2,
        "#architecture": 15,
        "#healthcare": 0,
        "#training": 22,
        "#robotics": 4,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 13,
        "#transfer_learning": 4,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 13,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 6,
        "#synthetic": 6,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 12,
        "#small_models": 7,
        "#science": 3,
        "#low_resource": 2
    }
}