{
    "date": {
        "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 4",
        "zh": "2æœˆ4æ—¥"
    },
    "time_utc": "2026-02-04 19:35",
    "weekday": 2,
    "issue_id": 907,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2602.01785",
            "title": "CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding",
            "url": "https://huggingface.co/papers/2602.01785",
            "abstract": "Multimodal large language models can effectively understand source code when represented as compressed images, achieving significant token reduction while maintaining or improving performance on code comprehension tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become a critical bottleneck. Currently, these models rely on a text-based paradigm that treats source code as a linear sequence of tokens, which leads to a linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to a fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8x compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4x compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out a shift toward image-modality code representation as a pathway to more efficient inference.",
            "score": 78,
            "issue_id": 892,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "7f6abd8b2889ebbe",
            "authors": [
                "Yuling Shi",
                "Chaoxiang Xie",
                "Zhensu Sun",
                "Yeheng Chen",
                "Chenxu Zhang",
                "Longfei Yun",
                "Chengcheng Wan",
                "Hongyu Zhang",
                "David Lo",
                "Xiaodong Gu"
            ],
            "affiliations": [
                "Beijing Institute of Technology",
                "Chongqing University",
                "East China Normal University",
                "Hohai University",
                "Imperial College London",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "Singapore Management University",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.01785.jpg",
            "data": {
                "categories": [
                    "#plp",
                    "#multimodal",
                    "#inference"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞšĞ¾Ğ´ ĞºĞ°Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ: ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ² Ğ²Ğ¸Ğ´Ğµ ÑĞ¶Ğ°Ñ‚Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² 8 Ñ€Ğ°Ğ· Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ° Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ° Ğ´Ğ°Ğ¶Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ĞºĞ»Ğ¾Ğ½Ğ¾Ğ² ĞºĞ¾Ğ´Ğ° Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒÑ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ ĞºĞ¾Ğ´Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Revolutionizing Code Understanding with Image-Based Compression",
                    "desc": "This paper explores the use of Multimodal Large Language Models (MLLMs) for understanding source code by representing it as compressed images instead of traditional text. By converting code into images, the models can achieve significant token reduction, with up to 8x compression, while still maintaining or improving performance on code comprehension tasks. The study shows that MLLMs can utilize visual features like syntax highlighting to enhance code completion, even under high compression. Overall, the research suggests that using image representation for code can lead to more efficient processing and opens new avenues for optimizing code understanding in large software systems."
                },
                "zh": {
                    "title": "å›¾åƒåŒ–ä»£ç è¡¨ç¤ºï¼Œæå‡ç†è§£æ•ˆç‡ï¼",
                    "desc": "å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èƒ½å¤Ÿé€šè¿‡å°†æºä»£ç è¡¨ç¤ºä¸ºå‹ç¼©å›¾åƒæ¥æœ‰æ•ˆç†è§£ä»£ç ï¼Œè¿™æ ·å¯ä»¥æ˜¾è‘—å‡å°‘ä»¤ç‰Œæ•°é‡ï¼ŒåŒæ—¶åœ¨ä»£ç ç†è§£ä»»åŠ¡ä¸Šä¿æŒæˆ–æé«˜æ€§èƒ½ã€‚ä¼ ç»Ÿçš„æ–‡æœ¬åŸºç¡€æ¨¡å‹åœ¨å¤„ç†æºä»£ç æ—¶ï¼Œéšç€ä¸Šä¸‹æ–‡é•¿åº¦çš„çº¿æ€§å¢åŠ ï¼Œè®¡ç®—æˆæœ¬ä¹Ÿéšä¹‹ä¸Šå‡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå›¾åƒæ¨¡æ€æ›´é€‚åˆå‹ç¼©ï¼Œå› ä¸ºå¯ä»¥é€šè¿‡è°ƒæ•´åˆ†è¾¨ç‡æ¥å‡å°‘åŸå§‹ä»¤ç‰Œæˆæœ¬ï¼ŒåŒæ—¶ä¿æŒå¯è¯†åˆ«æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒMLLMsåœ¨ä»£ç ç†è§£æ–¹é¢å…·æœ‰æ˜¾è‘—çš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨è§†è§‰æç¤ºçš„åˆ©ç”¨å’Œå‹ç¼©æ¯”æ–¹é¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03786",
            "title": "AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration",
            "url": "https://huggingface.co/papers/2602.03786",
            "abstract": "AOrchestra is a framework-agnostic agentic system that uses a tuple-based abstraction to dynamically create specialized task executors, achieving improved performance on complex benchmarks through automated agent creation and resource management.  \t\t\t\t\tAI-generated summary \t\t\t\t Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra",
            "score": 62,
            "issue_id": 893,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "f6bc21af3253a89f",
            "authors": [
                "Jianhao Ruan",
                "Zhihao Xu",
                "Yiran Peng",
                "Fashen Ren",
                "Zhaoyang Yu",
                "Xinbing Liang",
                "Jinyu Xiang",
                "Bang Liu",
                "Chenglin Wu",
                "Yuyu Luo",
                "Jiayi Zhang"
            ],
            "affiliations": [
                "DeepWisdom",
                "ECNU",
                "HKUST(GZ)",
                "RUC",
                "UdeM & Mila"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03786.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ¼",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ñ€Ñ‚ĞµĞ¶Ğ½ÑƒÑ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ñ",
                    "desc": "AOrchestra â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´Ğµ ĞºĞ¾Ñ€Ñ‚ĞµĞ¶Ğ° (Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ, ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ) Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ¦ĞµĞ½Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ‚Ğ¾Ñ€ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ, Ğ¾Ñ‚Ğ±Ğ¸Ñ€Ğ°Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ»ĞµÑ‚Ñƒ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ° Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ¼Ñƒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸. ĞĞ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… (GAIA, SWE-Bench, Terminal-Bench) AOrchestra Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° 16,28% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Gemini-3-Flash."
                },
                "en": {
                    "title": "Dynamic Agent Creation for Enhanced Task Automation",
                    "desc": "AOrchestra is a versatile system designed to enhance task automation by dynamically creating specialized agents for complex tasks. It utilizes a tuple-based abstraction that includes Instruction, Context, Tools, and Model, allowing for flexible and efficient task execution. This framework-agnostic approach reduces the need for extensive human engineering and supports various agent types as executors. By optimizing resource management and adaptability, AOrchestra demonstrates significant performance improvements on challenging benchmarks."
                },
                "zh": {
                    "title": "AOrchestraï¼šåŠ¨æ€åˆ›å»ºæ™ºèƒ½ä»»åŠ¡æ‰§è¡Œå™¨çš„æ¡†æ¶",
                    "desc": "AOrchestraæ˜¯ä¸€ä¸ªä¸æ¡†æ¶æ— å…³çš„æ™ºèƒ½ç³»ç»Ÿï¼Œä½¿ç”¨åŸºäºå…ƒç»„çš„æŠ½è±¡åŠ¨æ€åˆ›å»ºä¸“é—¨çš„ä»»åŠ¡æ‰§è¡Œå™¨ï¼Œä»è€Œåœ¨å¤æ‚åŸºå‡†æµ‹è¯•ä¸­å®ç°æ›´å¥½çš„æ€§èƒ½ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å°†ä»»ä½•ä»£ç†å»ºæ¨¡ä¸ºä¸€ä¸ªå…ƒç»„ï¼ˆæŒ‡ä»¤ã€ä¸Šä¸‹æ–‡ã€å·¥å…·ã€æ¨¡å‹ï¼‰æ¥è§£å†³ç°æœ‰è®¾è®¡ç¼ºä¹åŠ¨æ€æŠ½è±¡è§†å›¾çš„é—®é¢˜ã€‚AOrchestraçš„ä¸­å¿ƒåè°ƒå™¨åœ¨æ¯ä¸€æ­¥å…·ä½“åŒ–è¿™ä¸ªå…ƒç»„ï¼Œç­–åˆ’ä¸ä»»åŠ¡ç›¸å…³çš„ä¸Šä¸‹æ–‡ï¼Œé€‰æ‹©å·¥å…·å’Œæ¨¡å‹ï¼Œå¹¶é€šè¿‡å³æ—¶è‡ªåŠ¨åˆ›å»ºä»£ç†æ¥å§”æ´¾æ‰§è¡Œã€‚é€šè¿‡è¿™ç§è®¾è®¡ï¼ŒAOrchestraå‡å°‘äº†äººå·¥å·¥ç¨‹çš„å·¥ä½œé‡ï¼Œå¹¶æ”¯æŒå¤šç§ä»£ç†ä½œä¸ºä»»åŠ¡æ‰§è¡Œå™¨çš„å³æ’å³ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02103",
            "title": "No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs",
            "url": "https://huggingface.co/papers/2602.02103",
            "abstract": "Research investigates latent planning dynamics in large language models through a probing method called Tele-Lens, revealing limited global planning and enabling improved uncertainty estimation and CoT bypass recognition.  \t\t\t\t\tAI-generated summary \t\t\t\t This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.",
            "score": 57,
            "issue_id": 892,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "cc18842c82f089b7",
            "authors": [
                "Liyan Xu",
                "Mo Yu",
                "Fandong Meng",
                "Jie Zhou"
            ],
            "affiliations": [
                "WeChat AI, Tencent Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02103.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#open_source",
                    "#interpretability",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¡ĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² LLM: Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Tele-Lens. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ LLM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğµ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ³Ğ´Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ÑĞµĞ³Ğ¾ Ğ¿ÑƒÑ‚Ğ¸. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ½ÑƒĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ²Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Unveiling Latent Planning in Language Models with Tele-Lens",
                    "desc": "This paper explores how large language models (LLMs) plan their reasoning processes using a method called Tele-Lens. It finds that LLMs often lack comprehensive global planning, instead relying on short-term, incremental reasoning steps. The study highlights the importance of Chain-of-Thought (CoT) in tasks that require multi-step reasoning, while also suggesting that a few key CoT positions can effectively estimate uncertainty in reasoning paths. Additionally, the research shows that it is possible to recognize when CoT is bypassed without losing performance, enhancing our understanding of LLM dynamics."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ½œåœ¨è§„åˆ’åŠ¨æ€",
                    "desc": "æœ¬ç ”ç©¶é€šè¿‡ä¸€ç§ç§°ä¸ºTele-Lensçš„æ¢æµ‹æ–¹æ³•ï¼Œæ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„æ½œåœ¨è§„åˆ’åŠ¨æ€ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMsåœ¨è¿›è¡Œæ¨ç†æ—¶è¡¨ç°å‡ºæœ‰é™çš„å…¨å±€è§„åˆ’èƒ½åŠ›ï¼Œä¸»è¦ä¾èµ–äºå¢é‡è¿‡æ¸¡è€Œéç²¾ç¡®çš„å…¨å±€è§„åˆ’ã€‚å°½ç®¡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰åœ¨å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡ä¸­ä»ç„¶è‡³å…³é‡è¦ï¼Œä½†æˆ‘ä»¬æå‡ºçš„å‡è®¾è¡¨æ˜ï¼Œå°‘é‡çš„CoTä½ç½®å¯ä»¥æœ‰æ•ˆä»£è¡¨æ•´ä¸ªè·¯å¾„çš„ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†å¦‚ä½•åœ¨ä¸é™ä½æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œè‡ªåŠ¨è¯†åˆ«CoTçš„ç»•è¿‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02660",
            "title": "MARS: Modular Agent with Reflective Search for Automated AI Research",
            "url": "https://huggingface.co/papers/2602.02660",
            "abstract": "MARS is a modular AI research automation framework that uses budget-aware planning, modular construction, and reflective memory to achieve state-of-the-art performance in autonomous machine learning research.  \t\t\t\t\tAI-generated summary \t\t\t\t Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a \"Design-Decompose-Implement\" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard's top methods. Furthermore, the system exhibits qualitative \"Aha!\" moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths.",
            "score": 43,
            "issue_id": 892,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "be555ca2cb8fd2c1",
            "authors": [
                "Jiefeng Chen",
                "Bhavana Dalvi Mishra",
                "Jaehyun Nam",
                "Rui Meng",
                "Tomas Pfister",
                "Jinsung Yoon"
            ],
            "affiliations": [
                "Google Cloud AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02660.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#agents",
                    "#open_source",
                    "#science",
                    "#benchmark"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞœĞ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ñ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ´Ğ»Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ML-Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "MARS â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾, Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ-Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸-Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµĞ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ ML-Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ½Ğ¾Ğ»Ğ¸Ñ‚Ğ½Ñ‹Ñ… ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ ÑĞ²Ğ½Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. MARS Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MLE-Bench Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿ÑƒÑ‚ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ 63% Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ğ¾Ğ² Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²ĞµÑ‚Ğ²ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ¸Ğ´ĞµÑ‚ĞµĞ»ÑŒÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "MARS: Optimizing AI Research with Smart Planning and Modular Design",
                    "desc": "MARS is a framework designed to automate AI research by optimizing the planning and execution of machine learning tasks. It uses budget-aware planning to balance the performance of models with the costs of training them, ensuring efficient resource use. The framework is modular, allowing researchers to break down complex tasks into manageable parts, and it incorporates reflective memory to improve learning from past experiences. MARS has shown to outperform other open-source frameworks in benchmarks, highlighting its effectiveness in generating valuable insights through cross-branch learning."
                },
                "zh": {
                    "title": "MARSï¼šæ™ºèƒ½ç ”ç©¶çš„æ¨¡å—åŒ–æ¡†æ¶",
                    "desc": "MARSæ˜¯ä¸€ä¸ªæ¨¡å—åŒ–çš„äººå·¥æ™ºèƒ½ç ”ç©¶è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é¢„ç®—æ„è¯†è§„åˆ’ã€æ¨¡å—åŒ–æ„å»ºå’Œåæ€è®°å¿†æ¥å®ç°è‡ªä¸»æœºå™¨å­¦ä¹ ç ”ç©¶çš„å…ˆè¿›æ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡æˆæœ¬å—é™çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¥å¹³è¡¡æ€§èƒ½ä¸æ‰§è¡Œè´¹ç”¨ï¼Œç¡®ä¿åœ¨ç ”ç©¶è¿‡ç¨‹ä¸­æœ‰æ•ˆåˆ©ç”¨èµ„æºã€‚MARSé‡‡ç”¨â€œè®¾è®¡-åˆ†è§£-å®ç°â€çš„ç®¡é“ç®¡ç†å¤æ‚çš„ç ”ç©¶åº“ï¼Œå¹¶é€šè¿‡æ¯”è¾ƒåæ€è®°å¿†åˆ†æè§£å†³æ–¹æ¡ˆå·®å¼‚ï¼Œä»¥æç‚¼å‡ºé«˜ä¿¡å·çš„è§è§£ã€‚è¯¥ç³»ç»Ÿåœ¨MLE-Benchä¸Šè¡¨ç°å‡ºè‰²ï¼Œå±•ç°å‡ºè·¨åˆ†æ”¯è½¬ç§»çš„èƒ½åŠ›ï¼Œä½¿63%çš„å­¦ä¹ æ¥è‡ªäºä¸åŒæœç´¢è·¯å¾„çš„æœ‰æ•ˆæ¦‚æ‹¬ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02619",
            "title": "daVinci-Agency: Unlocking Long-Horizon Agency Data-Efficiently",
            "url": "https://huggingface.co/papers/2602.02619",
            "abstract": "Large language models face challenges in long-horizon agentic workflows due to lack of authentic long-dependency training data, which is addressed by leveraging pull request sequences for structured supervision through progressive decomposition, consistency enforcement, and refinement from bug-fix histories.  \t\t\t\t\tAI-generated summary \t\t\t\t While Large Language Models (LLMs) excel at short-term tasks, scaling them to long-horizon agentic workflows remains challenging. The core bottleneck lies in the scarcity of training data that captures authentic long-dependency structures and cross-stage evolutionary dynamics--existing synthesis methods either confine to single-feature scenarios constrained by model distribution, or incur prohibitive human annotation costs, failing to provide scalable, high-quality supervision. We address this by reconceptualizing data synthesis through the lens of real-world software evolution. Our key insight: Pull Request (PR) sequences naturally embody the supervision signals for long-horizon learning. They decompose complex objectives into verifiable submission units, maintain functional coherence across iterations, and encode authentic refinement patterns through bug-fix histories. Building on this, we propose daVinci-Agency, which systematically mines structured supervision from chain-of-PRs through three interlocking mechanisms: (1) progressive task decomposition via continuous commits, (2) long-term consistency enforcement through unified functional objectives, and (3) verifiable refinement from authentic bug-fix trajectories. Unlike synthetic trajectories that treat each step independently, daVinci-Agency's PR-grounded structure inherently preserves the causal dependencies and iterative refinements essential for teaching persistent goal-directed behavior and enables natural alignment with project-level, full-cycle task modeling. The resulting trajectories are substantial--averaging 85k tokens and 116 tool calls--yet remarkably data-efficient: fine-tuning GLM-4.6 on 239 daVinci-Agency samples yields broad improvements across benchmarks, notably achieving a 47% relative gain on Toolathlon. Beyond benchmark performance, our analysis confirms...",
            "score": 42,
            "issue_id": 893,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "b23ddd364497b1ab",
            "authors": [
                "Mohan Jiang",
                "Dayuan Fu",
                "Junhao Shi",
                "Ji Zeng",
                "Weiye Si",
                "Keyu Li",
                "Xuefeng Li",
                "Yang Xiao",
                "Wenjie Li",
                "Dequan Wang",
                "Pengfei Liu"
            ],
            "affiliations": [
                "GAIR",
                "PolyU",
                "SII Open Source",
                "SJTU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02619.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#synthetic",
                    "#alignment",
                    "#data",
                    "#long_context",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞÑ‚ ĞºĞ¾Ğ´Ğ° Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼: ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ pull request Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ daVinci-Agency Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº PR Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ñ€Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°: Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ€ĞµÑ„Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² (239 Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ²) Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Harnessing Pull Requests for Long-Horizon Learning in LLMs",
                    "desc": "This paper addresses the limitations of large language models (LLMs) in handling long-horizon tasks due to insufficient training data that captures long-term dependencies. The authors propose a novel approach called daVinci-Agency, which utilizes pull request sequences from software development as a source of structured supervision. By breaking down complex tasks into manageable units and enforcing consistency through real-world bug-fix histories, the model learns to maintain functional coherence over time. The results show that fine-tuning on this data significantly improves the model's performance on various benchmarks, demonstrating the effectiveness of leveraging authentic software evolution data for training."
                },
                "zh": {
                    "title": "åˆ©ç”¨æ‹‰å–è¯·æ±‚æå‡é•¿ä¾èµ–å­¦ä¹ çš„æ•ˆç‡",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿æ—¶é—´è·¨åº¦çš„ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯å› ä¸ºç¼ºä¹çœŸå®çš„é•¿ä¾èµ–è®­ç»ƒæ•°æ®ã€‚æœ¬æ–‡æå‡ºé€šè¿‡åˆ©ç”¨æ‹‰å–è¯·æ±‚åºåˆ—æ¥è·å–ç»“æ„åŒ–ç›‘ç£ï¼Œä»è€Œè§£å†³è¿™ä¸€é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬é€æ­¥åˆ†è§£å¤æ‚ç›®æ ‡ã€å¼ºåˆ¶ä¸€è‡´æ€§ä»¥åŠä»é”™è¯¯ä¿®å¤å†å²ä¸­æç‚¼ä¿¡æ¯ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„æ¨¡å‹daVinci-Agencyèƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ é•¿æœŸç›®æ ‡å¯¼å‘è¡Œä¸ºï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03796",
            "title": "3D-Aware Implicit Motion Control for View-Adaptive Human Video Generation",
            "url": "https://huggingface.co/papers/2602.03796",
            "abstract": "3DiMo enables view-agnostic human motion control in video generation by training a motion encoder alongside a pretrained video generator to distill driving frames into compact motion tokens that align with the generator's spatial priors.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing methods for human motion control in video generation typically rely on either 2D poses or explicit 3D parametric models (e.g., SMPL) as control signals. However, 2D poses rigidly bind motion to the driving viewpoint, precluding novel-view synthesis. Explicit 3D models, though structurally informative, suffer from inherent inaccuracies (e.g., depth ambiguity and inaccurate dynamics) which, when used as a strong constraint, override the powerful intrinsic 3D awareness of large-scale video generators. In this work, we revisit motion control from a 3D-aware perspective, advocating for an implicit, view-agnostic motion representation that naturally aligns with the generator's spatial priors rather than depending on externally reconstructed constraints. We introduce 3DiMo, which jointly trains a motion encoder with a pretrained video generator to distill driving frames into compact, view-agnostic motion tokens, injected semantically via cross-attention. To foster 3D awareness, we train with view-rich supervision (i.e., single-view, multi-view, and moving-camera videos), forcing motion consistency across diverse viewpoints. Additionally, we use auxiliary geometric supervision that leverages SMPL only for early initialization and is annealed to zero, enabling the model to transition from external 3D guidance to learning genuine 3D spatial motion understanding from the data and the generator's priors. Experiments confirm that 3DiMo faithfully reproduces driving motions with flexible, text-driven camera control, significantly surpassing existing methods in both motion fidelity and visual quality.",
            "score": 41,
            "issue_id": 892,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "84bc62536de9a4a1",
            "authors": [
                "Zhixue Fang",
                "Xu He",
                "Songlin Tang",
                "Haoxian Zhang",
                "Qingfeng Li",
                "Xiaoqiang Liu",
                "Pengfei Wan",
                "Kun Gai"
            ],
            "affiliations": [
                "CASIA",
                "Kling Team, Kuaishou Technology",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03796.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#training",
                    "#architecture",
                    "#multimodal",
                    "#optimization",
                    "#3d"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞĞµÑĞ²Ğ½Ğ¾Ğµ 3D Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ ÑƒĞ³Ğ»Ğ° Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°",
                    "desc": "3DiMo â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 2D Ğ¿Ğ¾Ğ· Ğ¸Ğ»Ğ¸ ÑĞ²Ğ½Ñ‹Ñ… 3D Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ñ‹ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒĞ³Ğ»Ğ¾Ğ² Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½ÑƒÑ 3D ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ Ğ¾Ñ‚ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğº Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "View-Agnostic Motion Control for Enhanced Video Generation",
                    "desc": "3DiMo is a novel approach for controlling human motion in video generation that does not depend on specific viewpoints. It trains a motion encoder alongside a video generator to create compact motion tokens that align with the generator's understanding of space. This method avoids the limitations of 2D poses and explicit 3D models by using a view-agnostic representation, allowing for more flexible and accurate motion synthesis. The model is trained with diverse video inputs to ensure consistent motion across different perspectives, leading to improved motion fidelity and visual quality in generated videos."
                },
                "zh": {
                    "title": "3DiMoï¼šè§†è§’æ— å…³çš„äººä½“è¿åŠ¨æ§åˆ¶æ–°æ–¹æ³•",
                    "desc": "3DiMoæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œç”¨äºåœ¨è§†é¢‘ç”Ÿæˆä¸­å®ç°ä¸è§†è§’æ— å…³çš„äººä½“è¿åŠ¨æ§åˆ¶ã€‚å®ƒé€šè¿‡ä¸é¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆå™¨å…±åŒè®­ç»ƒä¸€ä¸ªè¿åŠ¨ç¼–ç å™¨ï¼Œå°†é©±åŠ¨å¸§æç‚¼ä¸ºç´§å‡‘çš„è¿åŠ¨æ ‡è®°ï¼Œè¿™äº›æ ‡è®°ä¸ç”Ÿæˆå™¨çš„ç©ºé—´å…ˆéªŒç›¸ä¸€è‡´ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œ3DiMoä¸ä¾èµ–äº2Då§¿åŠ¿æˆ–æ˜¾å¼çš„3Dæ¨¡å‹ï¼Œè€Œæ˜¯é‡‡ç”¨éšå¼çš„è¿åŠ¨è¡¨ç¤ºï¼Œä»è€Œé¿å…äº†è§†è§’é™åˆ¶å’Œç»“æ„ä¸å‡†ç¡®çš„é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œ3DiMoåœ¨è¿åŠ¨ä¿çœŸåº¦å’Œè§†è§‰è´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.01630",
            "title": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks",
            "url": "https://huggingface.co/papers/2602.01630",
            "abstract": "Current world models lack unified frameworks despite task-specific advances, necessitating a comprehensive approach integrating interaction, perception, symbolic reasoning, and spatial representation.  \t\t\t\t\tAI-generated summary \t\t\t\t World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models. We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world.",
            "score": 41,
            "issue_id": 892,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "1a3d327b5e3d7ffa",
            "authors": [
                "Bohan Zeng",
                "Kaixin Zhu",
                "Daili Hua",
                "Bozhou Li",
                "Chengzhuo Tong",
                "Yuran Wang",
                "Xinyi Huang",
                "Yifan Dai",
                "Zixiang Zhang",
                "Yifan Yang",
                "Zhou Liu",
                "Hao Liang",
                "Xiaochen Ma",
                "Ruichuan An",
                "Tianyi Bai",
                "Hongcheng Gao",
                "Junbo Niu",
                "Yang Shi",
                "Xinlong Chen",
                "Yue Ding",
                "Minglei Shi",
                "Kai Zeng",
                "Yiwen Tang",
                "Yuanxing Zhang",
                "Pengfei Wan",
                "Xintao Wang",
                "Wentao Zhang"
            ],
            "affiliations": [
                "HKUST",
                "Kling Team, Kuaishou Technology",
                "Peking University",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "Shanghai Jiao Tong University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.01630.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼, Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ñ†ĞµĞ»Ğ¾Ğµ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¸Ñ€Ğ°. Ğ¢Ğ°ĞºĞ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ±Ğ¾Ğ»ĞµĞµ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ…, Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¸Ñ€Ğ°."
                },
                "en": {
                    "title": "Towards a Unified Framework for World Models in AI",
                    "desc": "This paper discusses the need for a unified framework in world models within AI, which currently focus on specific tasks like visual prediction or 3D estimation. It highlights that while these task-specific models improve performance, they often lack a cohesive structure for understanding the world as a whole. The authors propose a comprehensive design specification that integrates key components such as interaction, perception, symbolic reasoning, and spatial representation. The goal is to guide future research towards developing more robust and generalizable world models that can effectively understand and interact with complex environments."
                },
                "zh": {
                    "title": "æ„å»ºç»Ÿä¸€çš„ä¸–ç•Œæ¨¡å‹æ¡†æ¶",
                    "desc": "å½“å‰çš„ä¸–ç•Œæ¨¡å‹åœ¨ä»»åŠ¡ç‰¹å®šçš„è¿›å±•ä¸­ç¼ºä¹ç»Ÿä¸€çš„æ¡†æ¶ï¼Œå› æ­¤éœ€è¦ä¸€ç§ç»¼åˆçš„æ–¹æ³•æ¥æ•´åˆäº¤äº’ã€æ„ŸçŸ¥ã€ç¬¦å·æ¨ç†å’Œç©ºé—´è¡¨ç¤ºã€‚ä¸–ç•Œæ¨¡å‹æ—¨åœ¨é€šè¿‡å¼•å…¥ç‰©ç†åŠ¨æ€å’Œä¸–ç•ŒçŸ¥è¯†æ¥å¢å¼ºå¤§å‹æ¨¡å‹ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿç†è§£ã€é¢„æµ‹å’Œä¸å¤æ‚ç¯å¢ƒäº’åŠ¨ã€‚ç°æœ‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å°†ä¸–ç•ŒçŸ¥è¯†æ³¨å…¥å­¤ç«‹çš„ä»»åŠ¡ä¸­ï¼Œè€Œä¸æ˜¯å»ºç«‹ç»Ÿä¸€çš„å®šä¹‰æˆ–æ¡†æ¶ã€‚æœ¬æ–‡åˆ†æäº†è¿™äº›ç¢ç‰‡åŒ–æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„ä¸–ç•Œæ¨¡å‹è®¾è®¡è§„èŒƒï¼Œä»¥æŒ‡å¯¼æœªæ¥çš„ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03048",
            "title": "CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs",
            "url": "https://huggingface.co/papers/2602.03048",
            "abstract": "CoBA-RL adapts rollout budget allocation for LLM training by evaluating sample training value and optimizing resource distribution through a capability-oriented value function and greedy strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key approach for enhancing LLM reasoning.However, standard frameworks like Group Relative Policy Optimization (GRPO) typically employ a uniform rollout budget, leading to resource inefficiency. Moreover, existing adaptive methods often rely on instance-level metrics, such as task pass rates, failing to capture the model's dynamic learning state. To address these limitations, we propose CoBA-RL, a reinforcement learning algorithm designed to adaptively allocate rollout budgets based on the model's evolving capability. Specifically, CoBA-RL utilizes a Capability-Oriented Value function to map tasks to their potential training gains and employs a heap-based greedy strategy to efficiently self-calibrate the distribution of computational resources to samples with high training value. Extensive experiments demonstrate that our approach effectively orchestrates the trade-off between exploration and exploitation, delivering consistent generalization improvements across multiple challenging benchmarks. These findings underscore that quantifying sample training value and optimizing budget allocation are pivotal for advancing LLM post-training efficiency.",
            "score": 32,
            "issue_id": 892,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "f231912a460b9894",
            "authors": [
                "Zhiyuan Yao",
                "Yi-Kai Zhang",
                "Yuxin Chen",
                "Yueqing Sun",
                "Zishan Xu",
                "Yu Yang",
                "Tianhao Hu",
                "Qi Gu",
                "Hui Su",
                "Xunliang Cai"
            ],
            "affiliations": [
                "Meituan",
                "Nanjing University",
                "National University of Singapore",
                "Shanghai Jiao Tong University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03048.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#optimization",
                    "#training"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "CoBA-RL â€” ÑÑ‚Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¶Ğ°Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ¾Ñ‚ĞºĞ°Ñ‚Ğ¾Ğ² Ğº Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ°Ğ¼ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ LLM Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Optimizing Training Budgets for Smarter LLMs",
                    "desc": "CoBA-RL is a novel reinforcement learning algorithm that optimizes the allocation of rollout budgets during the training of large language models (LLMs). It introduces a Capability-Oriented Value function to assess the potential training gains of different tasks, allowing for a more efficient distribution of computational resources. Unlike traditional methods that use a uniform budget, CoBA-RL employs a greedy strategy to focus on samples that offer the highest training value. The results show that this adaptive approach significantly enhances the model's generalization performance across various benchmarks by effectively balancing exploration and exploitation."
                },
                "zh": {
                    "title": "æ™ºèƒ½åˆ†é…è®­ç»ƒé¢„ç®—ï¼Œæå‡æ¨¡å‹å­¦ä¹ æ•ˆç‡",
                    "desc": "CoBA-RLæ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨æ ¹æ®æ¨¡å‹çš„åŠ¨æ€èƒ½åŠ›è‡ªé€‚åº”åœ°åˆ†é…è®­ç»ƒé¢„ç®—ã€‚å®ƒä½¿ç”¨èƒ½åŠ›å¯¼å‘ä»·å€¼å‡½æ•°æ¥è¯„ä¼°ä»»åŠ¡çš„æ½œåœ¨è®­ç»ƒæ”¶ç›Šï¼Œå¹¶é€šè¿‡è´ªå©ªç­–ç•¥ä¼˜åŒ–è®¡ç®—èµ„æºçš„åˆ†é…ã€‚ä¸ä¼ ç»Ÿçš„å‡åŒ€é¢„ç®—åˆ†é…æ–¹æ³•ä¸åŒï¼ŒCoBA-RLèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨èµ„æºï¼Œæé«˜æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03139",
            "title": "Diversity-Preserved Distribution Matching Distillation for Fast Visual Synthesis",
            "url": "https://huggingface.co/papers/2602.03139",
            "abstract": "A novel distillation framework called DP-DMD is introduced that preserves sample diversity in text-to-image generation by separating the roles of distilled steps, using v-prediction for diversity and standard DMD loss for quality refinement without additional computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Distribution matching distillation (DMD) aligns a multi-step generator with its few-step counterpart to enable high-quality generation under low inference cost. However, DMD tends to suffer from mode collapse, as its reverse-KL formulation inherently encourages mode-seeking behavior, for which existing remedies typically rely on perceptual or adversarial regularization, thereby incurring substantial computational overhead and training instability. In this work, we propose a role-separated distillation framework that explicitly disentangles the roles of distilled steps: the first step is dedicated to preserving sample diversity via a target-prediction (e.g., v-prediction) objective, while subsequent steps focus on quality refinement under the standard DMD loss, with gradients from the DMD objective blocked at the first step. We term this approach Diversity-Preserved DMD (DP-DMD), which, despite its simplicity -- no perceptual backbone, no discriminator, no auxiliary networks, and no additional ground-truth images -- preserves sample diversity while maintaining visual quality on par with state-of-the-art methods in extensive text-to-image experiments.",
            "score": 31,
            "issue_id": 893,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "8ca33a5a8a7e7eb3",
            "authors": [
                "Tianhe Wu",
                "Ruibin Li",
                "Lei Zhang",
                "Kede Ma"
            ],
            "affiliations": [
                "Multimedia-Analytics-Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03139.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#inference",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ”Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ DP-DMD Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° Ğ¼Ğ¾Ğ´ (Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ). ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ¾Ğ»ĞµĞ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ²: Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ğ·Ğ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğº Ñ‡ĞµÑ€ĞµĞ· v-prediction, Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¹ DMD Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹-Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DP-DMD ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¼ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Preserving Diversity in Text-to-Image Generation with DP-DMD",
                    "desc": "The paper introduces a new framework called Diversity-Preserved DMD (DP-DMD) for text-to-image generation that aims to maintain sample diversity while ensuring high-quality outputs. It separates the distillation process into two distinct steps: the first step focuses on preserving diversity using a target-prediction method, while the second step refines quality using the standard DMD loss. This approach addresses the common issue of mode collapse found in traditional DMD methods, which often require complex regularization techniques that can slow down training. DP-DMD achieves impressive results without the need for additional computational resources or complex architectures, making it efficient and effective for generating diverse and high-quality images."
                },
                "zh": {
                    "title": "ä¿æŒå¤šæ ·æ€§çš„é«˜è´¨é‡ç”Ÿæˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è’¸é¦æ¡†æ¶ï¼Œç§°ä¸ºDP-DMDï¼Œæ—¨åœ¨åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­ä¿æŒæ ·æœ¬å¤šæ ·æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†ç¦»è’¸é¦æ­¥éª¤çš„è§’è‰²ï¼Œä½¿ç”¨ç›®æ ‡é¢„æµ‹æ¥å¢å¼ºå¤šæ ·æ€§ï¼ŒåŒæ—¶åˆ©ç”¨æ ‡å‡†DMDæŸå¤±è¿›è¡Œè´¨é‡ä¼˜åŒ–ï¼Œè€Œæ— éœ€é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚DP-DMDçš„ç¬¬ä¸€æ­¥ä¸“æ³¨äºä¿æŒæ ·æœ¬å¤šæ ·æ€§ï¼Œåç»­æ­¥éª¤åˆ™ä¸“æ³¨äºè´¨é‡æå‡ï¼Œé¿å…äº†æ¨¡å¼å´©æºƒçš„é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒDP-DMDåœ¨ä¿æŒè§†è§‰è´¨é‡çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä¿ç•™æ ·æœ¬å¤šæ ·æ€§ï¼Œè¡¨ç°ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03419",
            "title": "SWE-World: Building Software Engineering Agents in Docker-Free Environments",
            "url": "https://huggingface.co/papers/2602.03419",
            "abstract": "A Docker-free framework replaces physical execution environments with learned surrogates for training software engineering agents, enabling efficient training and test-time scaling without costly container setup.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have enabled software engineering agents to tackle complex code modification tasks. Most existing approaches rely on execution feedback from containerized environments, which require dependency-complete setup and physical execution of programs and tests. While effective, this paradigm is resource-intensive and difficult to maintain, substantially complicating agent training and limiting scalability. We propose SWE-World, a Docker-free framework that replaces physical execution environments with a learned surrogate for training and evaluating software engineering agents. SWE-World leverages LLM-based models trained on real agent-environment interaction data to predict intermediate execution outcomes and final test feedback, enabling agents to learn without interacting with physical containerized environments. This design preserves the standard agent-environment interaction loop while eliminating the need for costly environment construction and maintenance during agent optimization and evaluation. Furthermore, because SWE-World can simulate the final evaluation outcomes of candidate trajectories without real submission, it enables selecting the best solution among multiple test-time attempts, thereby facilitating effective test-time scaling (TTS) in software engineering tasks. Experiments on SWE-bench Verified demonstrate that SWE-World raises Qwen2.5-Coder-32B from 6.2\\% to 52.0\\% via Docker-free SFT, 55.0\\% with Docker-free RL, and 68.2\\% with further TTS. The code is available at https://github.com/RUCAIBox/SWE-World",
            "score": 29,
            "issue_id": 893,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "168b68461f26472e",
            "authors": [
                "Shuang Sun",
                "Huatong Song",
                "Lisheng Huang",
                "Jinhao Jiang",
                "Ran Le",
                "Zhihao Lv",
                "Zongchao Chen",
                "Yiwen Hu",
                "Wenyang Luo",
                "Wayne Xin Zhao",
                "Yang Song",
                "Hongteng Xu",
                "Tao Zhang",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "BOSS Zhipin, Beijing, China",
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03419.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#rl",
                    "#plp",
                    "#agents"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ’Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Docker Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ SWE-World â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ±ĞµĞ· Docker, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑÑƒÑ€Ñ€Ğ¾Ğ³Ğ°Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ ĞŸĞ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ LLM-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ñ‚ĞµÑÑ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ±ĞµĞ· Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ²Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ»ÑƒÑ‡ÑˆĞµĞµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº."
                },
                "en": {
                    "title": "SWE-World: Efficient Training for Software Engineering Agents Without Docker",
                    "desc": "This paper introduces SWE-World, a novel framework that eliminates the need for Docker containers in training software engineering agents. Instead of relying on physical execution environments, SWE-World uses learned surrogates to predict execution outcomes and test feedback, making the training process more efficient. By simulating agent-environment interactions, it allows for effective test-time scaling without the overhead of setting up and maintaining complex environments. The results show significant improvements in performance metrics for software engineering tasks, demonstrating the framework's potential to streamline agent training and evaluation."
                },
                "zh": {
                    "title": "æ— Dockeræ¡†æ¶æå‡è½¯ä»¶å·¥ç¨‹ä»£ç†è®­ç»ƒæ•ˆç‡",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSWE-Worldçš„æ— Dockeræ¡†æ¶ï¼Œç”¨äºè®­ç»ƒè½¯ä»¶å·¥ç¨‹ä»£ç†ã€‚è¯¥æ¡†æ¶é€šè¿‡å­¦ä¹ çš„æ›¿ä»£ç¯å¢ƒï¼Œå–ä»£äº†ä¼ ç»Ÿçš„ç‰©ç†æ‰§è¡Œç¯å¢ƒï¼Œä»è€Œæé«˜äº†è®­ç»ƒå’Œæµ‹è¯•çš„æ•ˆç‡ã€‚SWE-Worldåˆ©ç”¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨¡å‹ï¼Œé¢„æµ‹ä¸­é—´æ‰§è¡Œç»“æœå’Œæœ€ç»ˆæµ‹è¯•åé¦ˆï¼Œä½¿ä»£ç†èƒ½å¤Ÿåœ¨ä¸ä¸ç‰©ç†ç¯å¢ƒäº¤äº’çš„æƒ…å†µä¸‹å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSWE-Worldæ˜¾è‘—æé«˜äº†ä»£ç†çš„æ€§èƒ½ï¼Œç®€åŒ–äº†ç¯å¢ƒæ„å»ºå’Œç»´æŠ¤çš„å¤æ‚æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03411",
            "title": "SWE-Master: Unleashing the Potential of Software Engineering Agents via Post-Training",
            "url": "https://huggingface.co/papers/2602.03411",
            "abstract": "SWE-Master presents a reproducible framework for developing software engineering agents through systematic optimization across multiple stages of agent development, achieving superior performance on software task resolution benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t In this technical report, we present SWE-Master, an open-source and fully reproducible post-training framework for building effective software engineering agents. SWE-Master systematically explores the complete agent development pipeline, including teacher-trajectory synthesis and data curation, long-horizon SFT, RL with real execution feedback, and inference framework design. Starting from an open-source base model with limited initial SWE capability, SWE-Master demonstrates how systematical optimization method can elicit strong long-horizon SWE task solving abilities. We evaluate SWE-Master on SWE-bench Verified, a standard benchmark for realistic software engineering tasks. Under identical experimental settings, our approach achieves a resolve rate of 61.4\\% with Qwen2.5-Coder-32B, substantially outperforming existing open-source baselines. By further incorporating test-time scaling~(TTS) with LLM-based environment feedback, SWE-Master reaches 70.8\\% at TTS@8, demonstrating a strong performance potential. SWE-Master provides a practical and transparent foundation for advancing reproducible research on software engineering agents. The code is available at https://github.com/RUCAIBox/SWE-Master.",
            "score": 27,
            "issue_id": 893,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "093e2d66c0cc0ea5",
            "authors": [
                "Huatong Song",
                "Lisheng Huang",
                "Shuang Sun",
                "Jinhao Jiang",
                "Ran Le",
                "Daixuan Cheng",
                "Guoxin Chen",
                "Yiwen Hu",
                "Zongchao Chen",
                "Wayne Xin Zhao",
                "Yang Song",
                "Tao Zhang",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "BOSS Zhipin, Beijing, China",
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03411.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#training",
                    "#benchmark",
                    "#long_context",
                    "#rl",
                    "#plp",
                    "#reasoning",
                    "#agents",
                    "#optimization"
                ],
                "emoji": "ğŸ› ï¸",
                "ru": {
                    "title": "Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸",
                    "desc": "SWE-Master Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµÑÑŒ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ, ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¸ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, ĞºĞ°Ğº Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹ÑĞ²Ğ¸Ñ‚ÑŒ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸. ĞĞ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ SWE-bench Verified Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²: 61,4% Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¸ 70,8% Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Optimizing Software Engineering Agents with SWE-Master",
                    "desc": "SWE-Master is a framework designed to enhance the development of software engineering agents through systematic optimization. It covers the entire agent development process, including data preparation, supervised fine-tuning, and reinforcement learning with real feedback. By starting with a basic model, SWE-Master effectively improves the agent's ability to solve complex software tasks, achieving a high resolve rate on benchmarks. The framework not only demonstrates superior performance compared to existing models but also emphasizes reproducibility in research."
                },
                "zh": {
                    "title": "SWE-Masterï¼šè½¯ä»¶å·¥ç¨‹ä»£ç†çš„ç³»ç»Ÿä¼˜åŒ–æ¡†æ¶",
                    "desc": "SWE-Masteræ˜¯ä¸€ä¸ªå¼€æºçš„å¯é‡å¤æ¡†æ¶ï¼Œç”¨äºé€šè¿‡ç³»ç»Ÿä¼˜åŒ–å¼€å‘è½¯ä»¶å·¥ç¨‹ä»£ç†ã€‚å®ƒæ¶µç›–äº†ä»£ç†å¼€å‘çš„å„ä¸ªé˜¶æ®µï¼ŒåŒ…æ‹¬æ•™å¸ˆè½¨è¿¹åˆæˆã€æ•°æ®æ•´ç†ã€é•¿æ—¶é—´çš„ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ã€‚é€šè¿‡ä»ä¸€ä¸ªåŸºç¡€æ¨¡å‹å¼€å§‹ï¼ŒSWE-Masterå±•ç¤ºäº†å¦‚ä½•é€šè¿‡ç³»ç»ŸåŒ–çš„ä¼˜åŒ–æ–¹æ³•æå‡è½¯ä»¶å·¥ç¨‹ä»»åŠ¡çš„è§£å†³èƒ½åŠ›ã€‚ç»è¿‡è¯„ä¼°ï¼ŒSWE-Masteråœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨è½¯ä»¶å·¥ç¨‹ä»£ç†ç ”ç©¶ä¸­çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03845",
            "title": "Parallel-Probe: Towards Efficient Parallel Thinking via 2D Probing",
            "url": "https://huggingface.co/papers/2602.03845",
            "abstract": "Parallel-Probe is a training-free controller that optimizes parallel thinking by using consensus-based early stopping and deviation-based branch pruning to reduce computational costs while maintaining accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Parallel thinking has emerged as a promising paradigm for reasoning, yet it imposes significant computational burdens. Existing efficiency methods primarily rely on local, per-trajectory signals and lack principled mechanisms to exploit global dynamics across parallel branches. We introduce 2D probing, an interface that exposes the width-depth dynamics of parallel thinking by periodically eliciting intermediate answers from all branches. Our analysis reveals three key insights: non-monotonic scaling across width-depth allocations, heterogeneous reasoning branch lengths, and early stabilization of global consensus. Guided by these insights, we introduce Parallel-Probe, a training-free controller designed to optimize online parallel thinking. Parallel-Probe employs consensus-based early stopping to regulate reasoning depth and deviation-based branch pruning to dynamically adjust width. Extensive experiments across three benchmarks and multiple models demonstrate that Parallel-Probe establishes a superior Pareto frontier for test-time scaling. Compared to standard majority voting, it reduces sequential tokens by up to 35.8% and total token cost by over 25.8% while maintaining competitive accuracy.",
            "score": 21,
            "issue_id": 892,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "3d87cb50da672c0c",
            "authors": [
                "Tong Zheng",
                "Chengsong Huang",
                "Runpeng Dai",
                "Yun He",
                "Rui Liu",
                "Xin Ni",
                "Huiwen Bao",
                "Kaishen Wang",
                "Hongtu Zhu",
                "Jiaxin Huang",
                "Furong Huang",
                "Heng Huang"
            ],
            "affiliations": [
                "City University of Hong Kong",
                "Department of Computer Science, University of Maryland, College, Park",
                "Tongji University",
                "University of North Carolina at Chapel Hill",
                "Washington University in St. Louis"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03845.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "ğŸŒ³",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ° Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑ Ğ¸ Ğ´ĞµĞ²Ğ¸Ğ°Ñ†Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Parallel-Probe â€” ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ 2D-Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ ÑˆĞ¸Ñ€Ğ¸Ğ½Ñ‹ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²ĞµÑ‚Ğ²ĞµĞ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°: ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑ-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ñ€Ğ°Ğ½Ğ½ÑÑ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ´Ğ»Ñ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ° Ğ²ĞµÑ‚Ğ²ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑˆĞ¸Ñ€Ğ¸Ğ½Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° 25.8% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Optimizing Parallel Thinking with Efficiency and Accuracy",
                    "desc": "Parallel-Probe is a novel controller that enhances parallel thinking in machine learning without requiring prior training. It utilizes consensus-based early stopping to determine when to halt reasoning and deviation-based branch pruning to optimize the number of branches used. This approach allows for efficient computation by balancing the depth and width of reasoning processes, leading to significant reductions in token usage while preserving accuracy. Experiments show that Parallel-Probe outperforms traditional methods, achieving better efficiency in resource usage during model inference."
                },
                "zh": {
                    "title": "ä¼˜åŒ–å¹¶è¡Œæ€ç»´çš„æ— è®­ç»ƒæ§åˆ¶å™¨",
                    "desc": "Parallel-Probeæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ§åˆ¶å™¨ï¼Œé€šè¿‡åŸºäºå…±è¯†çš„æå‰åœæ­¢å’ŒåŸºäºåå·®çš„åˆ†æ”¯ä¿®å‰ªæ¥ä¼˜åŒ–å¹¶è¡Œæ€ç»´ï¼Œé™ä½è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨2Dæ¢æµ‹æ¥å£ï¼Œå®šæœŸä»æ‰€æœ‰åˆ†æ”¯è·å–ä¸­é—´ç­”æ¡ˆï¼Œæ­ç¤ºäº†å¹¶è¡Œæ€ç»´çš„å®½åº¦-æ·±åº¦åŠ¨æ€ã€‚ç ”ç©¶å‘ç°ï¼Œå®½åº¦å’Œæ·±åº¦çš„åˆ†é…å…·æœ‰éå•è°ƒç¼©æ”¾ç‰¹æ€§ï¼Œæ¨ç†åˆ†æ”¯é•¿åº¦ä¸å‡åŒ€ï¼Œä»¥åŠå…¨å±€å…±è¯†çš„æ—©æœŸç¨³å®šæ€§ã€‚é€šè¿‡è¿™äº›æ´å¯Ÿï¼ŒParallel-Probeèƒ½å¤Ÿåœ¨æµ‹è¯•æ—¶å®ç°æ›´ä¼˜çš„èµ„æºåˆ†é…ï¼Œæ˜¾è‘—å‡å°‘è®¡ç®—å¼€é”€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03619",
            "title": "Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation",
            "url": "https://huggingface.co/papers/2602.03619",
            "abstract": "DeepResearch report generation is improved through human-preference-aligned rubric generators trained via reinforcement learning with hybrid rewards and enhanced with multi-agent Markov-state workflows.  \t\t\t\t\tAI-generated summary \t\t\t\t Nowadays, training and evaluating DeepResearch-generated reports remain challenging due to the lack of verifiable reward signals. Accordingly, rubric-based evaluation has become a common practice. However, existing approaches either rely on coarse, pre-defined rubrics that lack sufficient granularity, or depend on manually constructed query-specific rubrics that are costly and difficult to scale. In this paper, we propose a pipeline to train human-preference-aligned query-specific rubric generators tailored for DeepResearch report generation. We first construct a dataset of DeepResearch-style queries annotated with human preferences over paired reports, and train rubric generators via reinforcement learning with a hybrid reward combining human preference supervision and LLM-based rubric evaluation. To better handle long-horizon reasoning, we further introduce a Multi-agent Markov-state (MaMs) workflow for report generation. We empirically show that our proposed rubric generators deliver more discriminative and better human-aligned supervision than existing rubric design strategies. Moreover, when integrated into the MaMs training framework, DeepResearch systems equipped with our rubric generators consistently outperform all open-source baselines on the DeepResearch Bench and achieve performance comparable to that of leading closed-source models.",
            "score": 20,
            "issue_id": 892,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "efff6f143b9f417c",
            "authors": [
                "Changze Lv",
                "Jie Zhou",
                "Wentao Zhao",
                "Jingwen Xu",
                "Zisu Huang",
                "Muzhao Tian",
                "Shihan Dou",
                "Tao Gui",
                "Le Tian",
                "Xiao Zhou",
                "Xiaoqing Zheng",
                "Xuanjing Huang",
                "Jie Zhou"
            ],
            "affiliations": [
                "College of Computer Science and Artificial Intelligence, Fudan University",
                "Pattern Recognition Center, WeChat AI, Tencent Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03619.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#benchmark",
                    "#dataset",
                    "#agents"
                ],
                "emoji": "ğŸ“‹",
                "ru": {
                    "title": "Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ñ‹ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ°Ñ ÑÑ…ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ñ‹ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ² Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğµ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°."
                },
                "en": {
                    "title": "Enhancing DeepResearch Reports with Human-Aligned Rubrics",
                    "desc": "This paper presents a novel approach to improve the generation of reports in DeepResearch by using human-preference-aligned rubric generators. These generators are trained through reinforcement learning, utilizing a hybrid reward system that combines human preferences and evaluations from large language models (LLMs). The authors introduce a Multi-agent Markov-state (MaMs) workflow to enhance the reasoning capabilities of the report generation process. The results demonstrate that their method provides superior supervision and performance compared to existing rubric strategies, achieving results on par with top closed-source models."
                },
                "zh": {
                    "title": "æå‡DeepResearchæŠ¥å‘Šç”Ÿæˆçš„è¯„ä¼°è´¨é‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥æ”¹è¿›DeepResearchæŠ¥å‘Šç”Ÿæˆçš„è¯„ä¼°è¿‡ç¨‹ã€‚æˆ‘ä»¬é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒäººç±»åå¥½å¯¹é½çš„è¯„åˆ†æ ‡å‡†ç”Ÿæˆå™¨ï¼Œä»¥ä¾¿æ›´å¥½åœ°è¯„ä¼°ç”Ÿæˆçš„æŠ¥å‘Šã€‚è¯¥æ–¹æ³•ç»“åˆäº†äººç±»åå¥½ç›‘ç£å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯„ä¼°ï¼Œæä¾›äº†æ›´ç»†è‡´çš„è¯„åˆ†æ ‡å‡†ã€‚é€šè¿‡å¼•å…¥å¤šæ™ºèƒ½ä½“é©¬å°”å¯å¤«çŠ¶æ€å·¥ä½œæµï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨DeepResearchåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„å¼€æºæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02380",
            "title": "Unified Personalized Reward Model for Vision Generation",
            "url": "https://huggingface.co/papers/2602.02380",
            "abstract": "UnifiedReward-Flex combines reward modeling with flexible, context-adaptive reasoning to improve visual generation by dynamically constructing hierarchical assessments based on semantic intent and visual evidence.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority.",
            "score": 16,
            "issue_id": 892,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "2ceb123f5e8506c6",
            "authors": [
                "Yibin Wang",
                "Yuhang Zang",
                "Feng Han",
                "Jiazi Bu",
                "Yujie Zhou",
                "Cheng Jin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai AI Lab",
                "Shanghai Innovation Institute",
                "Shanghai Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02380.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#reasoning",
                    "#multimodal",
                    "#alignment",
                    "#cv",
                    "#rlhf"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±ĞºĞ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° UnifiedReward-Flex â€” ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ñ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¼ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ½ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ². ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ÑÑĞ¸Ñ€Ğ¾Ğ²Ğ¾Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ¼ GRPO Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Dynamic Context-Aware Reward Modeling for Enhanced Visual Generation",
                    "desc": "UnifiedReward-Flex is a novel approach that enhances visual generation by integrating reward modeling with adaptable reasoning techniques. It addresses the limitations of traditional reward models, which often use a fixed evaluation system that does not account for specific visual details or human preferences. By dynamically creating hierarchical assessments based on both semantic intent and visual evidence, it allows for more personalized and context-sensitive evaluations. The model is trained through a two-stage process that improves its reasoning capabilities and aligns it more closely with human-like preferences, leading to better performance in generating images and videos."
                },
                "zh": {
                    "title": "çµæ´»è‡ªé€‚åº”çš„è§†è§‰ç”Ÿæˆå¥–åŠ±æ¨¡å‹",
                    "desc": "UnifiedReward-Flex æ˜¯ä¸€ç§ç»“åˆå¥–åŠ±å»ºæ¨¡å’Œçµæ´»ä¸Šä¸‹æ–‡è‡ªé€‚åº”æ¨ç†çš„æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡åŠ¨æ€æ„å»ºåŸºäºè¯­ä¹‰æ„å›¾å’Œè§†è§‰è¯æ®çš„å±‚æ¬¡è¯„ä¼°æ¥æ”¹å–„è§†è§‰ç”Ÿæˆã€‚ç°æœ‰çš„å¥–åŠ±æ¨¡å‹é€šå¸¸é‡‡ç”¨å›ºå®šçš„è¯„ä¼°æ ‡å‡†ï¼Œå¯¼è‡´å¯¹å†…å®¹ç‰¹å®šçš„è§†è§‰çº¿ç´¢ä¸æ•æ„Ÿï¼Œä»è€Œä¸äººç±»çš„ä¸»è§‚åå¥½å­˜åœ¨ç³»ç»Ÿæ€§ä¸ä¸€è‡´ã€‚è¯¥æ¨¡å‹é€šè¿‡è§£é‡Šè¯­ä¹‰æ„å›¾å¹¶åŸºäºè§†è§‰è¯æ®è¿›è¡Œè¯„ä¼°ï¼Œèƒ½å¤Ÿçµæ´»é€‚åº”ä¸åŒçš„ä¸Šä¸‹æ–‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUnifiedReward-Flex åœ¨å›¾åƒå’Œè§†é¢‘åˆæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02444",
            "title": "RANKVIDEO: Reasoning Reranking for Text-to-Video Retrieval",
            "url": "https://huggingface.co/papers/2602.02444",
            "abstract": "RANKVIDEO is a reasoning-based video retrieval system that improves upon traditional two-stage frameworks through explicit query-video pair analysis and a multi-objective training approach.  \t\t\t\t\tAI-generated summary \t\t\t\t Reranking is a critical component of modern retrieval systems, which typically pair an efficient first-stage retriever with a more expressive model to refine results. While large reasoning models have driven rapid progress in text-centric reranking, reasoning-based reranking for video retrieval remains underexplored. To address this gap, we introduce RANKVIDEO, a reasoning-based reranker for video retrieval that explicitly reasons over query-video pairs using video content to assess relevance. RANKVIDEO is trained using a two-stage curriculum consisting of perception-grounded supervised fine-tuning followed by reranking training that combines pointwise, pairwise, and teacher confidence distillation objectives, and is supported by a data synthesis pipeline for constructing reasoning-intensive query-video pairs. Experiments on the large-scale MultiVENT 2.0 benchmark demonstrate that RANKVIDEO consistently improves retrieval performance within a two-stage framework, yielding an average improvement of 31% on nDCG@10 and outperforming text-only and vision-language reranking alternatives, while more efficient.",
            "score": 15,
            "issue_id": 903,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "5acecd139f4bdfb5",
            "authors": [
                "Tyler Skow",
                "Alexander Martin",
                "Benjamin Van Durme",
                "Rama Chellappa",
                "Reno Kriz"
            ],
            "affiliations": [
                "Human Language Technology Center of Excellence",
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02444.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#video",
                    "#reasoning",
                    "#synthetic",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ¾ â€” Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "RANKVIDEO â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ñ‹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ-Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ²Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¼ curriculum-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ğ¼Ğ¸, Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸ĞµĞ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ¾Ğ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ°Ñ€, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MultiVENT 2.0 Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RANKVIDEO ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ° 31% Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ nDCG@10 Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸."
                },
                "en": {
                    "title": "RANKVIDEO: Enhancing Video Retrieval with Reasoning-Based Reranking",
                    "desc": "RANKVIDEO is a novel video retrieval system that enhances traditional methods by focusing on the relationship between query and video pairs. It employs a reasoning-based reranking approach that analyzes video content to determine relevance more effectively. The training process involves a two-stage curriculum that includes supervised fine-tuning and a combination of different training objectives to improve performance. Experiments show that RANKVIDEO significantly boosts retrieval accuracy, outperforming existing text-only and vision-language models while being more efficient."
                },
                "zh": {
                    "title": "åŸºäºæ¨ç†çš„è§†é¢‘æ£€ç´¢æ–°çªç ´",
                    "desc": "RANKVIDEOæ˜¯ä¸€ç§åŸºäºæ¨ç†çš„è§†é¢‘æ£€ç´¢ç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡æ˜¾å¼åˆ†ææŸ¥è¯¢-è§†é¢‘å¯¹å’Œå¤šç›®æ ‡è®­ç»ƒæ–¹æ³•æ¥æ”¹è¿›ä¼ ç»Ÿçš„ä¸¤é˜¶æ®µæ¡†æ¶ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨è§†é¢‘å†…å®¹å¯¹æŸ¥è¯¢-è§†é¢‘å¯¹çš„ç›¸å…³æ€§è¿›è¡Œè¯„ä¼°ï¼Œä»è€Œå®ç°æ›´ç²¾å‡†çš„é‡æ’åºã€‚RANKVIDEOé‡‡ç”¨ä¸¤é˜¶æ®µè¯¾ç¨‹è¿›è¡Œè®­ç»ƒï¼Œé¦–å…ˆè¿›è¡Œæ„ŸçŸ¥åŸºç¡€çš„ç›‘ç£å¾®è°ƒï¼Œç„¶åç»“åˆç‚¹å¯¹ç‚¹ã€å¯¹æ¯”å’Œæ•™å¸ˆä¿¡å¿ƒè’¸é¦ç›®æ ‡è¿›è¡Œé‡æ’åºè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRANKVIDEOåœ¨å¤§å‹MultiVENT 2.0åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ£€ç´¢æ€§èƒ½ï¼Œå¹³å‡æå‡31%çš„nDCG@10ï¼Œä¸”æ•ˆç‡æ›´é«˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02636",
            "title": "WideSeek: Advancing Wide Research via Multi-Agent Scaling",
            "url": "https://huggingface.co/papers/2602.02636",
            "abstract": "Wide Research advances search intelligence through a dedicated benchmark and multi-agent architecture that enables parallel information retrieval under complex constraints.  \t\t\t\t\tAI-generated summary \t\t\t\t Search intelligence is evolving from Deep Research to Wide Research, a paradigm essential for retrieving and synthesizing comprehensive information under complex constraints in parallel. However, progress in this field is impeded by the lack of dedicated benchmarks and optimization methodologies for search breadth. To address these challenges, we take a deep dive into Wide Research from two perspectives: Data Pipeline and Agent Optimization. First, we produce WideSeekBench, a General Broad Information Seeking (GBIS) benchmark constructed via a rigorous multi-phase data pipeline to ensure diversity across the target information volume, logical constraints, and domains. Second, we introduce WideSeek, a dynamic hierarchical multi-agent architecture that can autonomously fork parallel sub-agents based on task requirements. Furthermore, we design a unified training framework that linearizes multi-agent trajectories and optimizes the system using end-to-end RL. Experimental results demonstrate the effectiveness of WideSeek and multi-agent RL, highlighting that scaling the number of agents is a promising direction for advancing the Wide Research paradigm.",
            "score": 12,
            "issue_id": 892,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "6d29fbb787fc804c",
            "authors": [
                "Ziyang Huang",
                "Haolin Ren",
                "Xiaowei Yuan",
                "Jiawei Wang",
                "Zhongtao Jiang",
                "Kun Xu",
                "Shizhu He",
                "Jun Zhao",
                "Kang Liu"
            ],
            "affiliations": [
                "Institute of Automation, Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02636.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#agents",
                    "#rl",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¨Ğ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº: Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Wide Research â€” Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ WideSeekBench â€” ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ WideSeek â€” Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ½Ğ° Ğ»Ğ¸Ğ½ĞµĞ°Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Search Intelligence with Wide Research",
                    "desc": "This paper presents advancements in search intelligence through a new framework called Wide Research, which focuses on retrieving information under complex constraints. It introduces WideSeekBench, a benchmark designed to evaluate General Broad Information Seeking (GBIS) using a multi-phase data pipeline that ensures diverse information retrieval. Additionally, the authors propose WideSeek, a multi-agent architecture that can dynamically create sub-agents to handle various tasks in parallel. The study shows that using reinforcement learning to optimize these agents can significantly enhance the efficiency and effectiveness of information retrieval processes."
                },
                "zh": {
                    "title": "æ¨åŠ¨æœç´¢æ™ºèƒ½çš„å®½ç ”ç©¶æ–°èŒƒå¼",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†å®½ç ”ç©¶ï¼ˆWide Researchï¼‰åœ¨æœç´¢æ™ºèƒ½é¢†åŸŸçš„è¿›å±•ï¼Œæå‡ºäº†ä¸€ç§ä¸“é—¨çš„åŸºå‡†å’Œå¤šæ™ºèƒ½ä½“æ¶æ„ï¼Œä»¥å®ç°å¤æ‚çº¦æŸä¸‹çš„å¹¶è¡Œä¿¡æ¯æ£€ç´¢ã€‚æˆ‘ä»¬å¼€å‘äº†WideSeekBenchï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„å¹¿æ³›ä¿¡æ¯æ£€ç´¢åŸºå‡†ï¼Œç¡®ä¿äº†ç›®æ ‡ä¿¡æ¯çš„å¤šæ ·æ€§å’Œé€»è¾‘çº¦æŸã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†WideSeekï¼Œä¸€ä¸ªåŠ¨æ€çš„åˆ†å±‚å¤šæ™ºèƒ½ä½“æ¶æ„ï¼Œèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡éœ€æ±‚è‡ªä¸»åˆ†å‰å¹¶è¡Œå­æ™ºèƒ½ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWideSeekå’Œå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜å¢åŠ æ™ºèƒ½ä½“æ•°é‡æ˜¯æ¨åŠ¨å®½ç ”ç©¶èŒƒå¼çš„æœ‰å¸Œæœ›æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.21244",
            "title": "Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification",
            "url": "https://huggingface.co/papers/2601.21244",
            "abstract": "LENS framework improves reinforcement learning with verifiable rewards by identifying and removing interference tokens to enhance exploration efficiency and training stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remains constrained by inefficient exploration under limited rollout budgets, leading to low sampling success and unstable training in complex tasks. We find that many exploration failures arise not from problem difficulty, but from a small number of prompt tokens that introduce interference. Building on this insight, we propose the Less Noise Sampling Framework (LENS), which first prompts by identifying and removing interference tokens. then transfers successful rollouts from the purification process to supervise policy optimization on the original noisy prompts, enabling the model to learn to ignore interference in the real-world, noisy prompting settings. Experimental results show that LENS significantly outperforms GRPO, delivering higher performance and faster convergence, with a 3.88% average gain and over 1.6times speedup. Our work highlights the critical role of pruning interference tokens in improving rollout efficiency, offering a new perspective for RLVR research.",
            "score": 12,
            "issue_id": 892,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "91a5d6080fd74dd0",
            "authors": [
                "Yiju Guo",
                "Tianyi Hu",
                "Zexu Sun",
                "Yankai Lin"
            ],
            "affiliations": [
                "Baidu Inc.",
                "Department of Computer Science, Aarhus University",
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.21244.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ§¹",
                "ru": {
                    "title": "ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¾Ñ‚ Ğ¿Ğ¾Ğ¼ĞµÑ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° LENS - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ²ĞµĞ´ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ Ğ¾Ñ‚ĞºĞ°Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ñ€Ğ°Ğ·Ğ²ĞµĞ´ĞºĞ¸ Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ñ‹ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğµ, ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ¼ĞµÑ…Ğ¸, Ğ° Ğ½Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ ÑÑ‚Ğ¸ Ğ¼ĞµÑˆĞ°ÑÑ‰Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ GRPO: Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 3,88% Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² 1,6 Ñ€Ğ°Ğ·Ğ°."
                },
                "en": {
                    "title": "Enhancing Reinforcement Learning by Reducing Interference",
                    "desc": "The LENS framework enhances reinforcement learning by focusing on verifiable rewards and improving exploration efficiency. It identifies and removes interference tokens that hinder the learning process, leading to more stable training outcomes. By purifying the prompts, LENS allows the model to learn effectively from successful rollouts, even in noisy environments. Experimental results demonstrate that LENS achieves better performance and faster convergence compared to traditional methods, highlighting the importance of reducing noise in reinforcement learning tasks."
                },
                "zh": {
                    "title": "å»é™¤å¹²æ‰°ï¼Œæå‡å¼ºåŒ–å­¦ä¹ æ•ˆç‡",
                    "desc": "LENSæ¡†æ¶é€šè¿‡è¯†åˆ«å’Œå»é™¤å¹²æ‰°æ ‡è®°ï¼Œæå‡äº†å¼ºåŒ–å­¦ä¹ ä¸­çš„å¯éªŒè¯å¥–åŠ±ï¼Œä»è€Œå¢å¼ºäº†æ¢ç´¢æ•ˆç‡å’Œè®­ç»ƒç¨³å®šæ€§ã€‚ç ”ç©¶å‘ç°ï¼Œè®¸å¤šæ¢ç´¢å¤±è´¥å¹¶éæºäºé—®é¢˜çš„å¤æ‚æ€§ï¼Œè€Œæ˜¯ç”±äºå°‘é‡å¹²æ‰°æ ‡è®°çš„å½±å“ã€‚LENSé¦–å…ˆé€šè¿‡å»é™¤å¹²æ‰°æ ‡è®°æ¥è¿›è¡Œæç¤ºï¼Œç„¶åå°†æˆåŠŸçš„å›åˆè½¬ç§»åˆ°åŸå§‹çš„å˜ˆæ‚æç¤ºä¸Šï¼Œä»¥ç›‘ç£ç­–ç•¥ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLENSæ˜¾è‘—ä¼˜äºGRPOï¼Œæä¾›äº†æ›´é«˜çš„æ€§èƒ½å’Œæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.01362",
            "title": "Balancing Understanding and Generation in Discrete Diffusion Models",
            "url": "https://huggingface.co/papers/2602.01362",
            "abstract": "XDLM unifies Masked Diffusion Language Models and Uniform-noise Diffusion Language Models through a stationary noise kernel, achieving improved performance in both semantic understanding and generation quality.  \t\t\t\t\tAI-generated summary \t\t\t\t In discrete generative modeling, two dominant paradigms demonstrate divergent capabilities: Masked Diffusion Language Models (MDLM) excel at semantic understanding and zero-shot generalization, whereas Uniform-noise Diffusion Language Models (UDLM) achieve strong few-step generation quality, yet neither attains balanced performance across both dimensions. To address this, we propose XDLM, which bridges the two paradigms via a stationary noise kernel. XDLM offers two key contributions: (1) it provides a principled theoretical unification of MDLM and UDLM, recovering each paradigm as a special case; and (2) an alleviated memory bottleneck enabled by an algebraic simplification of the posterior probabilities. Experiments demonstrate that XDLM advances the Pareto frontier between understanding capability and generation quality. Quantitatively, XDLM surpasses UDLM by 5.4 points on zero-shot text benchmarks and outperforms MDLM in few-step image generation (FID 54.1 vs. 80.8). When scaled to tune an 8B-parameter large language model, XDLM achieves 15.0 MBPP in just 32 steps, effectively doubling the baseline performance. Finally, analysis of training dynamics reveals XDLM's superior potential for long-term scaling. Code is available at https://github.com/MzeroMiko/XDLM",
            "score": 11,
            "issue_id": 892,
            "pub_date": "2026-02-01",
            "pub_date_card": {
                "ru": "1 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 1",
                "zh": "2æœˆ1æ—¥"
            },
            "hash": "ab3da9b2a401c5c0",
            "authors": [
                "Yue Liu",
                "Yuzhong Zhao",
                "Zheyong Xie",
                "Qixiang Ye",
                "Jianbin Jiao",
                "Yao Hu",
                "Shaosheng Cao",
                "Yunfan Liu"
            ],
            "affiliations": [
                "UCAS",
                "Xiaohongshu Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.01362.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼: ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ XDLM â€” ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ´Ğ²Ğµ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ñ€Ğ½Ğ¾Ğµ ÑĞ´Ñ€Ğ¾ ÑˆÑƒĞ¼Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Masked Diffusion Language Models Ğ¸ Uniform-noise Diffusion Language Models ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ»ÑƒÑ‡Ğ°ÑĞ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. XDLM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ĞºĞ°Ğº Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°Ğ»Ğ¾Ğ¼ Ñ‡Ğ¸ÑĞ»Ğµ ÑˆĞ°Ğ³Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ÑƒĞ´Ğ²Ğ°Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° 32 ÑˆĞ°Ğ³Ğ°.)"
                },
                "en": {
                    "title": "XDLM: Bridging Understanding and Generation in Language Models",
                    "desc": "XDLM is a novel framework that combines the strengths of Masked Diffusion Language Models (MDLM) and Uniform-noise Diffusion Language Models (UDLM) using a stationary noise kernel. This unification allows XDLM to enhance both semantic understanding and generation quality, addressing the limitations of each individual model. The framework not only theoretically connects MDLM and UDLM but also simplifies memory usage through algebraic adjustments in posterior probabilities. Experimental results show that XDLM significantly improves performance on zero-shot text tasks and few-step image generation, demonstrating its effectiveness in balancing understanding and generation capabilities."
                },
                "zh": {
                    "title": "XDLMï¼šç»Ÿä¸€ç†è§£ä¸ç”Ÿæˆçš„è¯­è¨€æ¨¡å‹",
                    "desc": "XDLMæ˜¯ä¸€ç§æ–°å‹çš„è¯­è¨€æ¨¡å‹ï¼Œå®ƒå°†æ©è”½æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆMDLMï¼‰å’Œå‡åŒ€å™ªå£°æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆUDLMï¼‰é€šè¿‡ä¸€ä¸ªé™æ€å™ªå£°æ ¸ç»Ÿä¸€èµ·æ¥ã€‚è¯¥æ¨¡å‹åœ¨è¯­ä¹‰ç†è§£å’Œç”Ÿæˆè´¨é‡æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ï¼Œè§£å†³äº†è¿™ä¸¤ç§æ¨¡å‹å„è‡ªçš„ä¸è¶³ã€‚XDLMçš„ä¸¤ä¸ªä¸»è¦è´¡çŒ®æ˜¯ï¼šé¦–å…ˆï¼Œå®ƒæä¾›äº†MDLMå’ŒUDLMçš„ç†è®ºç»Ÿä¸€ï¼›å…¶æ¬¡ï¼Œé€šè¿‡ä»£æ•°ç®€åŒ–ï¼Œç¼“è§£äº†å†…å­˜ç“¶é¢ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒXDLMåœ¨ç†è§£èƒ½åŠ›å’Œç”Ÿæˆè´¨é‡ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03086",
            "title": "Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning",
            "url": "https://huggingface.co/papers/2602.03086",
            "abstract": "Neural Predictor-Corrector framework unifies homotopy methods across multiple domains and outperforms classical approaches through learned policies and amortized training.  \t\t\t\t\tAI-generated summary \t\t\t\t The Homotopy paradigm, a general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework.",
            "score": 10,
            "issue_id": 902,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "7b0d8cc4b69f4d7e",
            "authors": [
                "Jiayao Mai",
                "Bangyan Liao",
                "Zhenjun Zhao",
                "Yingping Zeng",
                "Haoang Li",
                "Javier Civera",
                "Tailin Wu",
                "Yi Zhou",
                "Peidong Liu"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology (Guangzhou)",
                "Hunan University",
                "University of Zaragoza",
                "Westlake University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03086.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#rl",
                    "#training"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑŒ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ğº: ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³Ğ¾Ğ¼Ğ¾Ñ‚Ğ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Neural Predictor-Corrector (NPC) - ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ°Ñæ¡†æ¶Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ³Ğ¾Ğ¼Ğ¾Ñ‚Ğ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ·Ğ°Ğ¼ĞµĞ½ÑÑÑ‚ Ñ€ÑƒÑ‡Ğ½Ñ‹Ğµ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° ÑˆĞ°Ğ³Ğ° Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ² Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ reinforcement learning. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñƒ amortized training Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¾Ğ´Ğ¸Ğ½ Ñ€Ğ°Ğ· Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ° Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ NPC Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ¾Ñ€Ğ½ĞµĞ¹ Ğ¿Ğ¾Ğ»Ğ¸Ğ½Ğ¾Ğ¼Ğ¾Ğ² Ğ¸ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unifying Homotopy Methods with Neural Learning for Enhanced Problem Solving",
                    "desc": "The Neural Predictor-Corrector (NPC) framework integrates homotopy methods from various domains, providing a unified approach to solving complex problems. Unlike traditional methods that depend on manually designed heuristics for decision-making, NPC employs learned policies through reinforcement learning to optimize step sizes and iteration processes. This framework allows for amortized training, meaning it can be trained once on a set of problems and then efficiently applied to new instances. Experiments show that NPC not only generalizes well to unseen problems but also outperforms classical methods in both efficiency and stability."
                },
                "zh": {
                    "title": "ç»Ÿä¸€åŒä¼¦æ–¹æ³•ï¼Œæå‡æœºå™¨å­¦ä¹ æ•ˆç‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç¥ç»é¢„æµ‹-æ ¡æ­£æ¡†æ¶ï¼ˆNeural Predictor-Corrector, NPCï¼‰ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤šé¢†åŸŸçš„åŒä¼¦æ–¹æ³•ï¼Œå¹¶é€šè¿‡å­¦ä¹ ç­–ç•¥å’Œæ‘Šé”€è®­ç»ƒè¶…è¶Šä¼ ç»Ÿæ–¹æ³•ã€‚è¯¥æ¡†æ¶å°†åŒä¼¦é—®é¢˜è§†ä¸ºä¸€ä¸ªæ•´ä½“ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ è‡ªåŠ¨å‘ç°é«˜æ•ˆç­–ç•¥ï¼Œå–ä»£äº†æ‰‹å·¥è®¾è®¡çš„å¯å‘å¼æ–¹æ³•ã€‚NPCé€šè¿‡å°†ç­–ç•¥é€‰æ‹©è§†ä¸ºä¸€ä¸ªåºåˆ—å†³ç­–é—®é¢˜ï¼Œèƒ½å¤Ÿåœ¨å¤šç§ä»»åŠ¡ä¸­å®ç°æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNPCåœ¨å¤„ç†æœªè§å®ä¾‹æ—¶è¡¨ç°å‡ºè‰²ï¼Œä¸”åœ¨æ•ˆç‡å’Œç¨³å®šæ€§ä¸Šå‡ä¼˜äºä¼ ç»Ÿå’Œä¸“ä¸šåŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03798",
            "title": "FullStack-Agent: Enhancing Agentic Full-Stack Web Coding via Development-Oriented Testing and Repository Back-Translation",
            "url": "https://huggingface.co/papers/2602.03798",
            "abstract": "A unified agent system called FullStack-Agent is introduced to assist non-expert users in developing complex interactive websites by addressing full-stack development challenges through enhanced planning, code editing, and self-improving capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Assisting non-expert users to develop complex interactive websites has become a popular task for LLM-powered code agents. However, existing code agents tend to only generate frontend web pages, masking the lack of real full-stack data processing and storage with fancy visual effects. Notably, constructing production-level full-stack web applications is far more challenging than only generating frontend web pages, demanding careful control of data flow, comprehensive understanding of constantly updating packages and dependencies, and accurate localization of obscure bugs in the codebase. To address these difficulties, we introduce FullStack-Agent, a unified agent system for full-stack agentic coding that consists of three parts: (1) FullStack-Dev, a multi-agent framework with strong planning, code editing, codebase navigation, and bug localization abilities. (2) FullStack-Learn, an innovative data-scaling and self-improving method that back-translates crawled and synthesized website repositories to improve the backbone LLM of FullStack-Dev. (3) FullStack-Bench, a comprehensive benchmark that systematically tests the frontend, backend and database functionalities of the generated website. Our FullStack-Dev outperforms the previous state-of-the-art method by 8.7%, 38.2%, and 15.9% on the frontend, backend, and database test cases respectively. Additionally, FullStack-Learn raises the performance of a 30B model by 9.7%, 9.5%, and 2.8% on the three sets of test cases through self-improvement, demonstrating the effectiveness of our approach. The code is released at https://github.com/mnluzimu/FullStack-Agent.",
            "score": 9,
            "issue_id": 894,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "5dfc19ba939c3259",
            "authors": [
                "Zimu Lu",
                "Houxing Ren",
                "Yunqiao Yang",
                "Ke Wang",
                "Zhuofan Zong",
                "Mingjie Zhan",
                "Hongsheng Li"
            ],
            "affiliations": [
                "Ace Robotics",
                "Multimedia Laboratory (MMLab), The Chinese University of Hong Kong",
                "Shenzhen Loop Area Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03798.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#agents",
                    "#training",
                    "#plp"
                ],
                "emoji": "ğŸ—ï¸",
                "ru": {
                    "title": "ĞŸĞ¾Ğ»Ğ½Ğ¾Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²ĞµĞ±-Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° FullStack-Agent - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ½ĞµĞ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ°Ğ¼ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²ĞµĞ±-Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ñ„Ñ€Ğ¾Ğ½Ñ‚ĞµĞ½Ğ´Ğ°, Ğ±ÑĞºĞµĞ½Ğ´Ğ° Ğ¸ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ğµ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ FullStack-Learn, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ LLM Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ FullStack-Bench ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° 8.7-38.2% Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²ĞµĞ±-Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering Non-Experts in Full-Stack Web Development",
                    "desc": "The paper presents FullStack-Agent, a comprehensive system designed to help non-expert users create complex interactive websites by tackling the challenges of full-stack development. It consists of three main components: FullStack-Dev for planning and code editing, FullStack-Learn for self-improvement through data scaling, and FullStack-Bench for benchmarking website functionalities. FullStack-Dev significantly outperforms previous methods in frontend, backend, and database tests, showcasing its advanced capabilities. The system aims to simplify the development process by providing robust tools for managing data flow and debugging, making full-stack development more accessible."
                },
                "zh": {
                    "title": "å…¨æ ˆä»£ç†ç³»ç»Ÿï¼ŒåŠ©åŠ›ç½‘ç«™å¼€å‘ï¼",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFullStack-Agentçš„ç»Ÿä¸€ä»£ç†ç³»ç»Ÿï¼Œæ—¨åœ¨å¸®åŠ©éä¸“ä¸šç”¨æˆ·å¼€å‘å¤æ‚çš„äº’åŠ¨ç½‘ç«™ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å¢å¼ºçš„è§„åˆ’ã€ä»£ç ç¼–è¾‘å’Œè‡ªæˆ‘æ”¹è¿›èƒ½åŠ›ï¼Œè§£å†³äº†å…¨æ ˆå¼€å‘ä¸­çš„æŒ‘æˆ˜ã€‚FullStack-AgentåŒ…æ‹¬ä¸‰ä¸ªéƒ¨åˆ†ï¼šFullStack-Devã€FullStack-Learnå’ŒFullStack-Benchï¼Œåˆ†åˆ«è´Ÿè´£å¤šä»£ç†æ¡†æ¶ã€æ•°æ®è‡ªæˆ‘æå‡å’Œå…¨é¢åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFullStack-Agentåœ¨å‰ç«¯ã€åç«¯å’Œæ•°æ®åº“æµ‹è¯•ä¸­å‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæ˜¾ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03216",
            "title": "Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection",
            "url": "https://huggingface.co/papers/2602.03216",
            "abstract": "Token Sparse Attention enables efficient long-context inference by dynamically compressing and decompressing attention tensors at the token level, achieving significant speedup with minimal accuracy loss.  \t\t\t\t\tAI-generated summary \t\t\t\t The quadratic complexity of attention remains the central bottleneck in long-context inference for large language models. Prior acceleration methods either sparsify the attention map with structured patterns or permanently evict tokens at specific layers, which can retain irrelevant tokens or rely on irreversible early decisions despite the layer-/head-wise dynamics of token importance. In this paper, we propose Token Sparse Attention, a lightweight and dynamic token-level sparsification mechanism that compresses per-head Q, K, V to a reduced token set during attention and then decompresses the output back to the original sequence, enabling token information to be reconsidered in subsequent layers. Furthermore, Token Sparse Attention exposes a new design point at the intersection of token selection and sparse attention. Our approach is fully compatible with dense attention implementations, including Flash Attention, and can be seamlessly composed with existing sparse attention kernels. Experimental results show that Token Sparse Attention consistently improves accuracy-latency trade-off, achieving up to times3.23 attention speedup at 128K context with less than 1% accuracy degradation. These results demonstrate that dynamic and interleaved token-level sparsification is a complementary and effective strategy for scalable long-context inference.",
            "score": 9,
            "issue_id": 895,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "7e3cbfbe9177c763",
            "authors": [
                "Dongwon Jo",
                "Beomseok Kang",
                "Jiwon Song",
                "Jae-Joon Kim"
            ],
            "affiliations": [
                "Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03216.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#long_context",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¿Ğ°Ñ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ inference Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Token Sparse Attention â€” Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¿Ğ°Ñ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Q, K, V Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¾Ñ‚Ğ±Ğ¸Ñ€Ğ°Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… ÑĞ»Ğ¾Ñ‘Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸Ñ‚ÑŒ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Flash Attention, Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² 3.23 Ñ€Ğ°Ğ·Ğ° Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸Ğ· 128K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ½ĞµĞµ 1%. Ğ­Ñ‚Ğ¾Ñ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¸ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰Ğ°ÑÑÑ ÑĞ¿Ğ°Ñ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ inference Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Dynamic Token Sparsification for Efficient Long-Context Inference",
                    "desc": "This paper introduces Token Sparse Attention, a method designed to improve the efficiency of long-context inference in large language models by dynamically compressing and decompressing attention tensors at the token level. Traditional attention mechanisms face challenges due to their quadratic complexity, which limits performance when processing long sequences. Token Sparse Attention addresses this by selectively reducing the number of tokens considered during attention calculations, allowing for a more flexible and efficient use of resources. The results show that this approach can significantly speed up attention processes while maintaining high accuracy, making it a valuable advancement in the field of machine learning."
                },
                "zh": {
                    "title": "åŠ¨æ€ä»¤ç‰Œç¨€ç–æ³¨æ„åŠ›ï¼Œæå‡é•¿ä¸Šä¸‹æ–‡æ¨ç†æ•ˆç‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºToken Sparse Attentionçš„æœºåˆ¶ï¼Œæ—¨åœ¨æé«˜é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„æ•ˆç‡ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨ä»¤ç‰Œçº§åˆ«åŠ¨æ€å‹ç¼©å’Œè§£å‹æ³¨æ„åŠ›å¼ é‡ï¼Œæ˜¾è‘—åŠ å¿«äº†è®¡ç®—é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒå°çš„å‡†ç¡®æ€§æŸå¤±ã€‚ä¸ä»¥å¾€çš„åŠ é€Ÿæ–¹æ³•ä¸åŒï¼ŒToken Sparse Attentionå…è®¸åœ¨åç»­å±‚ä¸­é‡æ–°è€ƒè™‘ä»¤ç‰Œä¿¡æ¯ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„çµæ´»æ€§å’Œæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨128Kä¸Šä¸‹æ–‡ä¸­å®ç°äº†æœ€é«˜3.23å€çš„æ³¨æ„åŠ›åŠ é€Ÿï¼Œä¸”å‡†ç¡®æ€§ä¸‹é™ä¸åˆ°1%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02676",
            "title": "AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process",
            "url": "https://huggingface.co/papers/2602.02676",
            "abstract": "AdaptMMBench presents a comprehensive benchmark for evaluating adaptive multimodal reasoning in Vision-Language Models, measuring reasoning mode selection rationality through dynamic difficulty assessment and multi-dimensional process evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t Adaptive multimodal reasoning has emerged as a promising frontier in Vision-Language Models (VLMs), aiming to dynamically modulate between tool-augmented visual reasoning and text reasoning to enhance both effectiveness and efficiency. However, existing evaluations rely on static difficulty labels and simplistic metrics, which fail to capture the dynamic nature of difficulty relative to varying model capacities. Consequently, they obscure the distinction between adaptive mode selection and general performance while neglecting fine-grained process analyses. In this paper, we propose AdaptMMBench, a comprehensive benchmark for adaptive multimodal reasoning across five domains: real-world, OCR, GUI, knowledge, and math, encompassing both direct perception and complex reasoning tasks. AdaptMMBench utilizes a Matthews Correlation Coefficient (MCC) metric to evaluate the selection rationality of different reasoning modes, isolating this meta-cognition ability by dynamically identifying task difficulties based on models' capability boundaries. Moreover, AdaptMMBench facilitates multi-dimensional process evaluation across key step coverage, tool effectiveness, and computational efficiency. Our evaluation reveals that while adaptive mode selection scales with model capacity, it notably decouples from final accuracy. Conversely, key step coverage aligns with performance, though tool effectiveness remains highly inconsistent across model architectures.",
            "score": 8,
            "issue_id": 892,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "dcf8a0d29dda7fc8",
            "authors": [
                "Xintong Zhang",
                "Xiaowen Zhang",
                "Jongrong Wu",
                "Zhi Gao",
                "Shilin Yan",
                "Zhenxin Diao",
                "Kunpeng Gao",
                "Xuanyan Chen",
                "Yuwei Wu",
                "Yunde Jia",
                "Qing Li"
            ],
            "affiliations": [
                "Alibaba Group",
                "Beijing Key Laboratory of Intelligent Information Technology, School of Computer Science & Technology, Beijing Institute of Technology",
                "Guangdong Laboratory of Machine Perception and Intelligent Computing, Shenzhen MSU-BIT University",
                "State Key Laboratory of General Artificial Intelligence, BIGAI",
                "Xidian University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02676.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ: Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "AdaptMMBench â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Vision-Language Models, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Matthews Correlation Coefficient Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞÑ†ĞµĞ½ĞºĞ° Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑÑ‚ÑŒ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ (Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¸Ñ€, OCR, GUI, Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ°) Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°: Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ², ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ¾ Ğ½Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "AdaptMMBench: Elevating Adaptive Reasoning in Vision-Language Models",
                    "desc": "The paper introduces AdaptMMBench, a new benchmark designed to evaluate adaptive multimodal reasoning in Vision-Language Models (VLMs). It addresses the limitations of existing evaluations that use static difficulty labels and simplistic metrics, which do not reflect the dynamic nature of task difficulty. AdaptMMBench assesses reasoning mode selection rationality using the Matthews Correlation Coefficient (MCC) and evaluates models across five domains, focusing on both direct perception and complex reasoning tasks. The findings indicate that while adaptive mode selection improves with model capacity, it does not necessarily correlate with final accuracy, highlighting the need for a nuanced understanding of model performance."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”å¤šæ¨¡æ€æ¨ç†çš„è¯„ä¼°æ–°åŸºå‡†",
                    "desc": "AdaptMMBench æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­çš„è‡ªé€‚åº”å¤šæ¨¡æ€æ¨ç†ã€‚å®ƒé€šè¿‡åŠ¨æ€éš¾åº¦è¯„ä¼°å’Œå¤šç»´è¿‡ç¨‹è¯„ä¼°æ¥æµ‹é‡æ¨ç†æ¨¡å¼é€‰æ‹©çš„åˆç†æ€§ã€‚è¯¥åŸºå‡†æ¶µç›–äº†äº”ä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬ç°å®ä¸–ç•Œã€OCRã€GUIã€çŸ¥è¯†å’Œæ•°å­¦ï¼Œæ¶‰åŠç›´æ¥æ„ŸçŸ¥å’Œå¤æ‚æ¨ç†ä»»åŠ¡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè‡ªé€‚åº”æ¨¡å¼é€‰æ‹©ä¸æ¨¡å‹èƒ½åŠ›ç›¸å…³ï¼Œä½†ä¸æœ€ç»ˆå‡†ç¡®æ€§å¹¶ä¸ç›´æ¥ç›¸å…³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.00747",
            "title": "Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training",
            "url": "https://huggingface.co/papers/2602.00747",
            "abstract": "DeMix is a framework that uses model merging to predict optimal data ratios for LLM pre-training, decoupling search from training costs to improve mixture discovery efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Determining an effective data mixture is a key factor in Large Language Model (LLM) pre-training, where models must balance general competence with proficiency on hard tasks such as math and code. However, identifying an optimal mixture remains an open challenge, as existing approaches either rely on unreliable tiny-scale proxy experiments or require prohibitively expensive large-scale exploration. To address this, we propose Decouple Searching from Training Mix (DeMix), a novel framework that leverages model merging to predict optimal data ratios. Instead of training proxy models for every sampled mixture, DeMix trains component models on candidate datasets at scale and derives data mixture proxies via weighted model merging. This paradigm decouples search from training costs, enabling evaluation of unlimited sampled mixtures without extra training burden and thus facilitating better mixture discovery through more search trials. Extensive experiments demonstrate that DeMix breaks the trade-off between sufficiency, accuracy and efficiency, obtaining the optimal mixture with higher benchmark performance at lower search cost. Additionally, we release the DeMix Corpora, a comprehensive 22T-token dataset comprising high-quality pre-training data with validated mixtures to facilitate open research. Our code and DeMix Corpora is available at https://github.com/Lucius-lsr/DeMix.",
            "score": 8,
            "issue_id": 892,
            "pub_date": "2026-01-31",
            "pub_date_card": {
                "ru": "31 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 31",
                "zh": "1æœˆ31æ—¥"
            },
            "hash": "ce36bac13dae5835",
            "authors": [
                "Shengrui Li",
                "Fei Zhao",
                "Kaiyan Zhao",
                "Jieying Ye",
                "Haifeng Liu",
                "Fangcheng Shi",
                "Zheyong Xie",
                "Yao Hu",
                "Shaosheng Cao"
            ],
            "affiliations": [
                "NLP Team, Xiaohongshu Inc. Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.00747.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#data",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "ĞŸĞ¾Ğ¸ÑĞº Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¼ĞµÑĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "DeMix â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¼ĞµÑĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ LLM, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¾ĞºÑĞ¸-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ² Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ¼ĞµÑ€Ğ¶ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¢Ğ°ĞºĞ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¼ĞµÑĞ¸ Ğ¾Ñ‚ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğµ Ñ‡Ğ¸ÑĞ»Ğ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² ÑĞ¼ĞµÑĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ DeMix Corpora â€” Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ¾Ğ¼ 22 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Decoupling Search from Training for Optimal Data Mixtures in LLMs",
                    "desc": "DeMix is a framework designed to enhance the efficiency of discovering optimal data mixtures for pre-training Large Language Models (LLMs). It achieves this by using model merging techniques to predict the best data ratios, which allows for extensive mixture evaluation without the need for costly training of proxy models. This decoupling of search from training costs enables researchers to explore a wider range of data mixtures, leading to improved performance on challenging tasks. The framework also introduces the DeMix Corpora, a large dataset that supports further research in this area by providing validated data mixtures."
                },
                "zh": {
                    "title": "è§£è€¦æœç´¢ä¸è®­ç»ƒï¼Œæå‡æ•°æ®æ··åˆæ•ˆç‡",
                    "desc": "DeMixæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œé€šè¿‡æ¨¡å‹åˆå¹¶æ¥é¢„æµ‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢„è®­ç»ƒçš„æœ€ä½³æ•°æ®æ¯”ä¾‹ï¼Œä»è€Œæé«˜æ··åˆå‘ç°çš„æ•ˆç‡ã€‚è¯¥æ–¹æ³•å°†æœç´¢ä¸è®­ç»ƒæˆæœ¬è§£è€¦ï¼Œä½¿å¾—å¯ä»¥åœ¨ä¸å¢åŠ é¢å¤–è®­ç»ƒè´Ÿæ‹…çš„æƒ…å†µä¸‹è¯„ä¼°æ— é™çš„æ ·æœ¬æ··åˆã€‚DeMixé€šè¿‡åŠ æƒæ¨¡å‹åˆå¹¶ï¼Œä»å€™é€‰æ•°æ®é›†ä¸Šè®­ç»ƒç»„ä»¶æ¨¡å‹ï¼Œç”Ÿæˆæ•°æ®æ··åˆä»£ç†ï¼Œé¿å…äº†å¯¹æ¯ä¸ªæ ·æœ¬æ··åˆè¿›è¡Œä»£ç†æ¨¡å‹è®­ç»ƒçš„éœ€æ±‚ã€‚å®éªŒè¡¨æ˜ï¼ŒDeMixåœ¨å……åˆ†æ€§ã€å‡†ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´æ‰“ç ´äº†æƒè¡¡ï¼Œä»¥æ›´ä½çš„æœç´¢æˆæœ¬è·å¾—æ›´é«˜çš„åŸºå‡†æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03747",
            "title": "LIVE: Long-horizon Interactive Video World Modeling",
            "url": "https://huggingface.co/papers/2602.03747",
            "abstract": "LIVE is a long-horizon video world model that uses cycle-consistency and diffusion loss to control error accumulation during extended video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive video world models predict future visual observations conditioned on actions. While effective over short horizons, these models often struggle with long-horizon generation, as small prediction errors accumulate over time. Prior methods alleviate this by introducing pre-trained teacher models and sequence-level distribution matching, which incur additional computational cost and fail to prevent error propagation beyond the training horizon. In this work, we propose LIVE, a Long-horizon Interactive Video world modEl that enforces bounded error accumulation via a novel cycle-consistency objective, thereby eliminating the need for teacher-based distillation. Specifically, LIVE first performs a forward rollout from ground-truth frames and then applies a reverse generation process to reconstruct the initial state. The diffusion loss is subsequently computed on the reconstructed terminal state, providing an explicit constraint on long-horizon error propagation. Moreover, we provide an unified view that encompasses different approaches and introduce progressive training curriculum to stabilize training. Experiments demonstrate that LIVE achieves state-of-the-art performance on long-horizon benchmarks, generating stable, high-quality videos far beyond training rollout lengths.",
            "score": 7,
            "issue_id": 903,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "2c348bc63334000c",
            "authors": [
                "Junchao Huang",
                "Ziyang Ye",
                "Xinting Hu",
                "Tianyu He",
                "Guiyu Zhang",
                "Shaoshuai Shi",
                "Jiang Bian",
                "Li Jiang"
            ],
            "affiliations": [
                "Microsoft Research",
                "Shenzhen Loop Area Institute",
                "The Chinese University of Hong Kong, Shenzhen",
                "The University of Hong Kong",
                "Voyager Research, Didi Chuxing"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03747.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#diffusion",
                    "#long_context",
                    "#optimization",
                    "#video"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ¦Ğ¸ĞºĞ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº: Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "LIVE â€” ÑÑ‚Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ (forward-backward) Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğµ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ğ¼Ğ¸ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ-Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ‚Ğ¾Ñ€Ğ°, Ğ° Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¸Ğ· ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ†Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ LIVE Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "LIVE: Mastering Long-Horizon Video Generation with Cycle-Consistency",
                    "desc": "LIVE is a novel video world model designed to generate long sequences of video while minimizing errors. It uses a cycle-consistency objective to control the accumulation of prediction errors over time, which is a common issue in autoregressive models. By performing a forward rollout and then reconstructing the initial state through a reverse process, LIVE effectively reduces the need for additional teacher models. The introduction of diffusion loss further constrains error propagation, leading to high-quality video generation that surpasses previous methods."
                },
                "zh": {
                    "title": "LIVEï¼šæ§åˆ¶é•¿æ—¶é—´è§†é¢‘ç”Ÿæˆä¸­çš„è¯¯å·®ç§¯ç´¯",
                    "desc": "LIVEæ˜¯ä¸€ç§é•¿æ—¶é—´è§†é¢‘ä¸–ç•Œæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡å¾ªç¯ä¸€è‡´æ€§å’Œæ‰©æ•£æŸå¤±æ¥æ§åˆ¶åœ¨é•¿æ—¶é—´è§†é¢‘ç”Ÿæˆè¿‡ç¨‹ä¸­çš„è¯¯å·®ç§¯ç´¯ã€‚ä¼ ç»Ÿçš„è‡ªå›å½’è§†é¢‘æ¨¡å‹åœ¨çŸ­æ—¶é—´å†…è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨é•¿æ—¶é—´ç”Ÿæˆæ—¶ï¼Œé¢„æµ‹è¯¯å·®ä¼šéšç€æ—¶é—´çš„æ¨ç§»è€Œç´¯ç§¯ã€‚LIVEé€šè¿‡å¼•å…¥æ–°çš„å¾ªç¯ä¸€è‡´æ€§ç›®æ ‡ï¼Œæ¶ˆé™¤äº†å¯¹æ•™å¸ˆæ¨¡å‹çš„ä¾èµ–ï¼Œä»è€Œæœ‰æ•ˆåœ°é™åˆ¶äº†è¯¯å·®çš„ä¼ æ’­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLIVEåœ¨é•¿æ—¶é—´åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿç”Ÿæˆç¨³å®šä¸”é«˜è´¨é‡çš„è§†é¢‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03709",
            "title": "No Shortcuts to Culture: Indonesian Multi-hop Question Answering for Complex Cultural Understanding",
            "url": "https://huggingface.co/papers/2602.03709",
            "abstract": "Multi-hop question answering dataset ID-MoCQA assesses cultural understanding in large language models through Indonesian traditions with diverse reasoning chains.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding culture requires reasoning across context, tradition, and implicit social knowledge, far beyond recalling isolated facts. Yet most culturally focused question answering (QA) benchmarks rely on single-hop questions, which may allow models to exploit shallow cues rather than demonstrate genuine cultural reasoning. In this work, we introduce ID-MoCQA, the first large-scale multi-hop QA dataset for assessing the cultural understanding of large language models (LLMs), grounded in Indonesian traditions and available in both English and Indonesian. We present a new framework that systematically transforms single-hop cultural questions into multi-hop reasoning chains spanning six clue types (e.g., commonsense, temporal, geographical). Our multi-stage validation pipeline, combining expert review and LLM-as-a-judge filtering, ensures high-quality question-answer pairs. Our evaluation across state-of-the-art models reveals substantial gaps in cultural reasoning, particularly in tasks requiring nuanced inference. ID-MoCQA provides a challenging and essential benchmark for advancing the cultural competency of LLMs.",
            "score": 7,
            "issue_id": 898,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "621ab01c0e2d402f",
            "authors": [
                "Vynska Amalia Permadi",
                "Xingwei Tan",
                "Nafise Sadat Moosavi",
                "Nikos Aletras"
            ],
            "affiliations": [
                "Department of Informatics, Universitas Pembangunan Nasional Veteran Yogyakarta, Indonesia",
                "School of Computer Science, University of Sheffield, United Kingdom"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03709.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#benchmark",
                    "#low_resource",
                    "#open_source",
                    "#multilingual"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ñ‹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ID-MoCQA â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ğ¸Ğ½Ğ´Ğ¾Ğ½ĞµĞ·Ğ¸Ğ¹ÑĞºĞ¸Ñ… Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ² Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ ÑˆĞµÑÑ‚ÑŒÑ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº: Ğ·Ğ´Ñ€Ğ°Ğ²Ñ‹Ğ¹ ÑĞ¼Ñ‹ÑĞ», Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ, Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ. ĞœĞ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚. ĞÑ†ĞµĞ½ĞºĞ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ³Ğ¾ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Enhancing Cultural Understanding in AI with Multi-Hop Reasoning",
                    "desc": "The paper introduces ID-MoCQA, a novel multi-hop question answering dataset designed to evaluate the cultural understanding of large language models (LLMs) through the lens of Indonesian traditions. Unlike traditional benchmarks that focus on single-hop questions, ID-MoCQA requires models to engage in complex reasoning across multiple contexts and clues, enhancing the assessment of cultural knowledge. The dataset includes a systematic transformation of questions into multi-hop reasoning chains, ensuring a diverse range of inference types. Evaluation results indicate significant shortcomings in the cultural reasoning capabilities of current state-of-the-art models, highlighting the need for improved cultural competency in AI systems."
                },
                "zh": {
                    "title": "è¯„ä¼°æ–‡åŒ–ç†è§£çš„å¤šè·³é—®ç­”æ•°æ®é›†",
                    "desc": "ID-MoCQAæ˜¯ä¸€ä¸ªå¤šè·³é—®ç­”æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å¯¹å°å°¼æ–‡åŒ–çš„ç†è§£ã€‚ä¸ä¼ ç»Ÿçš„å•è·³é—®ç­”ä¸åŒï¼Œè¯¥æ•°æ®é›†è¦æ±‚æ¨¡å‹é€šè¿‡å¤šç§æ¨ç†é“¾æ¥å›ç­”é—®é¢˜ï¼Œæ¶‰åŠå¸¸è¯†ã€æ—¶é—´å’Œåœ°ç†ç­‰çº¿ç´¢ç±»å‹ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°æ¡†æ¶ï¼Œå°†å•è·³æ–‡åŒ–é—®é¢˜ç³»ç»Ÿåœ°è½¬åŒ–ä¸ºå¤šè·³æ¨ç†é“¾ï¼Œå¹¶é€šè¿‡ä¸“å®¶è¯„å®¡å’Œæ¨¡å‹è¿‡æ»¤ç¡®ä¿é—®é¢˜å’Œç­”æ¡ˆçš„é«˜è´¨é‡ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå½“å‰æ¨¡å‹åœ¨æ–‡åŒ–æ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç»†è‡´æ¨ç†çš„ä»»åŠ¡ä¸­ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03677",
            "title": "Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration",
            "url": "https://huggingface.co/papers/2602.03677",
            "abstract": "Research reveals that instruction tokens act as structural anchors in multimodal large language models, with shallow layers performing non-selective information transfer and deep layers resolving modality competition guided by instruction intent.  \t\t\t\t\tAI-generated summary \t\t\t\t Modality following serves as the capacity of multimodal large language models (MLLMs) to selectively utilize multimodal contexts based on user instructions. It is fundamental to ensuring safety and reliability in real-world deployments. However, the underlying mechanisms governing this decision-making process remain poorly understood. In this paper, we investigate its working mechanism through an information flow lens. Our findings reveal that instruction tokens function as structural anchors for modality arbitration: Shallow attention layers perform non-selective information transfer, routing multimodal cues to these anchors as a latent buffer; Modality competition is resolved within deep attention layers guided by the instruction intent, while MLP layers exhibit semantic inertia, acting as an adversarial force. Furthermore, we identify a sparse set of specialized attention heads that drive this arbitration. Causal interventions demonstrate that manipulating a mere 5% of these critical heads can decrease the modality-following ratio by 60% through blocking, or increase it by 60% through targeted amplification of failed samples. Our work provides a substantial step toward model transparency and offers a principled framework for the orchestration of multimodal information in MLLMs.",
            "score": 5,
            "issue_id": 899,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "f0b2273ebdb7b52e",
            "authors": [
                "Yu Zhang",
                "Mufan Xu",
                "Xuefeng Bai",
                "Kehai chen",
                "Pengfei Zhang",
                "Yang Xiang",
                "Min Zhang"
            ],
            "affiliations": [
                "Harbin Institute of Technology, Harbin",
                "Harbin Institute of Technology, Shenzhen, China",
                "Peng Cheng Laboratory, Shenzhen, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03677.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#interpretability",
                    "#architecture"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ĞºĞ°Ğº ÑĞºĞ¾Ñ€Ñ: ĞºĞ°Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ÑÑ‚ Ğ½ÑƒĞ¶Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‚ ĞºĞ°Ğº ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ ÑĞºĞ¾Ñ€Ñ, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸: Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ Ğ½ĞµĞ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ñƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ ÑĞ»Ğ¾Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ÑÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ñ€ĞµĞ´ĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… attention heads, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹ Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° â€” Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ²ÑĞµĞ³Ğ¾ 5% ÑÑ‚Ğ¸Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ² Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼ Ğ½Ğ° 60%. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² MLLM."
                },
                "en": {
                    "title": "Unlocking the Secrets of Multimodal Instruction Following",
                    "desc": "This paper explores how instruction tokens help multimodal large language models (MLLMs) manage different types of information, like text and images. It shows that shallow layers of the model transfer information without much selection, while deeper layers use the instructions to decide which type of information to focus on. The study identifies specific attention heads that play a crucial role in this decision-making process, revealing that small changes in these heads can significantly affect how well the model follows instructions. Overall, the research enhances our understanding of how MLLMs operate and provides a framework for improving their performance in real-world applications."
                },
                "zh": {
                    "title": "æŒ‡ä»¤ä»¤ç‰Œï¼šå¤šæ¨¡æ€æ¨¡å‹çš„ç»“æ„é”š",
                    "desc": "æœ¬ç ”ç©¶æ­ç¤ºäº†æŒ‡ä»¤ä»¤ç‰Œåœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ç»“æ„æ€§é”šå®šä½œç”¨ã€‚æµ…å±‚ç½‘ç»œæ‰§è¡Œéé€‰æ‹©æ€§çš„ä¿¡æ¯ä¼ é€’ï¼Œè€Œæ·±å±‚ç½‘ç»œåˆ™æ ¹æ®æŒ‡ä»¤æ„å›¾è§£å†³æ¨¡æ€ç«äº‰ã€‚æˆ‘ä»¬å‘ç°ï¼ŒæŒ‡ä»¤ä»¤ç‰Œä½œä¸ºæ¨¡æ€ä»²è£çš„ç»“æ„é”šï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¼•å¯¼ä¿¡æ¯æµåŠ¨ã€‚é€šè¿‡æ“æ§ç‰¹å®šçš„æ³¨æ„åŠ›å¤´ï¼Œæˆ‘ä»¬å¯ä»¥æ˜¾è‘—å½±å“æ¨¡å‹çš„æ¨¡æ€è·Ÿéšèƒ½åŠ›ï¼Œä»è€Œæé«˜æ¨¡å‹çš„é€æ˜åº¦å’Œå¯é æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03647",
            "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
            "url": "https://huggingface.co/papers/2602.03647",
            "abstract": "Search-R2 framework improves language agent reasoning through Actor-Refiner collaboration with targeted interventions and fine-grained reward supervision for better credit assignment in reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a 'cut-and-regenerate' mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead.",
            "score": 5,
            "issue_id": 892,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "c6747bdce6916ea5",
            "authors": [
                "Bowei He",
                "Minda Hu",
                "Zenan Xu",
                "Hongru Wang",
                "Licheng Zong",
                "Yankai Chen",
                "Chen Ma",
                "Xue Liu",
                "Pluto Zhou",
                "Irwin King"
            ],
            "affiliations": [
                "City University of Hong Kong",
                "LLM Department, Tencent",
                "McGill University",
                "Mohamed bin Zayed University of Artificial Intelligence",
                "The Chinese University of Hong Kong",
                "The University of Edinburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03647.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#agents",
                    "#rl",
                    "#rag"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ”Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ°Ñ Ñ€ĞµÑ„Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ° Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Search-R2, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Actor Ğ¸ Refiner ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°Ğ¼Ğ¸. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¿Ñ€Ğ¸Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ğ³Ğ´Ğ° Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ½Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ñ… ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ½Ğ¾Ğ¹ Ğ·Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ, Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑÑ‰ĞµĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Search-R2 Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚."
                },
                "en": {
                    "title": "Enhancing Language Agent Reasoning with Search-R2 Framework",
                    "desc": "The Search-R2 framework enhances the reasoning capabilities of language agents by utilizing a collaborative approach between an Actor and a Meta-Refiner. This method addresses the multi-scale credit assignment problem in reinforcement learning by providing targeted interventions and fine-grained reward supervision. The Actor generates initial reasoning paths, while the Meta-Refiner corrects errors through a 'cut-and-regenerate' process, improving the overall reasoning quality. Experimental results show that Search-R2 outperforms existing models in reasoning accuracy across various question-answering datasets, demonstrating its effectiveness in optimizing agent performance."
                },
                "zh": {
                    "title": "é€šè¿‡æ¼”å‘˜-ä¿®æ­£è€…åä½œæå‡è¯­è¨€ä»£ç†æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSearch-R2çš„æ¡†æ¶ï¼Œé€šè¿‡æ¼”å‘˜-ä¿®æ­£è€…çš„åä½œæ¥æ”¹å–„è¯­è¨€ä»£ç†çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶è§£å†³äº†å¼ºåŒ–å­¦ä¹ ä¸­çš„å¤šå°ºåº¦ä¿¡ç”¨åˆ†é…é—®é¢˜ï¼Œé‡‡ç”¨äº†é’ˆå¯¹æ€§çš„å¹²é¢„å’Œç»†ç²’åº¦çš„å¥–åŠ±ç›‘ç£ã€‚æ¼”å‘˜è´Ÿè´£ç”Ÿæˆåˆæ­¥çš„æ¨ç†è½¨è¿¹ï¼Œè€Œä¿®æ­£è€…åˆ™é€šè¿‡â€œåˆ‡å‰²-å†ç”Ÿæˆâ€æœºåˆ¶æ¥è¯Šæ–­å’Œä¿®å¤é”™è¯¯æ­¥éª¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSearch-R2åœ¨å¤šç§é—®ç­”æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.01053",
            "title": "LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents",
            "url": "https://huggingface.co/papers/2602.01053",
            "abstract": "LRAgent is a KV cache sharing framework for multi-LoRA agents that decomposes cache into shared and adapter-dependent components, reducing memory and compute overhead while maintaining accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Role specialization in multi-LLM agent systems is often realized via multi-LoRA, where agents share a pretrained backbone and differ only through lightweight adapters. Despite sharing base model weights, each agent independently builds and stores its own KV cache for the same long, tool-augmented trajectories, incurring substantial memory and compute overhead. Existing KV cache sharing methods largely overlook this multi-LoRA setting. We observe that, across agents, cache differences are dominated by adapter outputs, while activations from the shared pretrained backbone remain highly similar. Based on this observation, we propose LRAgent, a KV cache sharing framework for multi-LoRA agents that decomposes the cache into a shared base component from the pretrained weights and an adapter-dependent component from LoRA weights. LRAgent reduces memory overhead by sharing the base component and storing the adapter component in its inherent low-rank form, and further reduces compute overhead, enabled by shared-A multi-LoRA architectures, by also sharing the low-rank cache and avoiding redundant computations for contexts already processed by other agents. To efficiently reconstruct adapter contributions at runtime, we introduce Flash-LoRA-Attention, a kernel that reorders attention computation to avoid materializing the low-rank cache to full dimension. LRAgent achieves throughput and time-to-first-token latency close to fully shared caching, while preserving accuracy near the non-shared caching baseline across agentic question-answering benchmarks.",
            "score": 5,
            "issue_id": 895,
            "pub_date": "2026-02-01",
            "pub_date_card": {
                "ru": "1 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 1",
                "zh": "2æœˆ1æ—¥"
            },
            "hash": "d5a1b3c5ea1bc139",
            "authors": [
                "Hyesung Jeon",
                "Hyeongju Ha",
                "Jae-Joon Kim"
            ],
            "affiliations": [
                "Department of Electrical and Computer Engineering, Seoul National University, Seoul, Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.01053.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#inference",
                    "#architecture"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºÑÑˆĞ° Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ LoRA Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ°Ğ¼Ğ¸",
                    "desc": "LRAgent â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ KV ĞºÑÑˆĞ° Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LoRA Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ². ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ĞºÑÑˆĞ° Ğ½Ğ° Ğ´Ğ²Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ¾Ğ±Ñ‰ÑƒÑ Ñ‡Ğ°ÑÑ‚ÑŒ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰ÑƒÑ Ğ¾Ñ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ° Ñ‡Ğ°ÑÑ‚ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑÑ Ğ² Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ²Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Flash-LoRA-Attention â€” ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ´Ñ€Ğ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿ĞµÑ€ĞµÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºÑÑˆĞ° Ğ² Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ĞºĞ»Ğ¸ĞºĞ°, Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ñ… Ğº Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±Ñ‰ĞµĞ¼Ñƒ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ³Ğ¾ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Efficient Cache Sharing for Multi-LoRA Agents",
                    "desc": "LRAgent is a framework designed to optimize memory and computational efficiency in multi-LoRA agent systems by sharing key-value (KV) caches. It separates the cache into two parts: a shared component derived from the pretrained backbone and an adapter-dependent component specific to each agent. This approach minimizes memory usage by allowing agents to share the base cache while storing only the necessary low-rank adapter components. Additionally, LRAgent employs a novel kernel called Flash-LoRA-Attention to streamline attention computations, resulting in faster processing times without sacrificing accuracy."
                },
                "zh": {
                    "title": "LRAgentï¼šé«˜æ•ˆçš„å¤šLoRAä»£ç†ç¼“å­˜å…±äº«æ¡†æ¶",
                    "desc": "LRAgentæ˜¯ä¸€ä¸ªç”¨äºå¤šLoRAä»£ç†çš„KVç¼“å­˜å…±äº«æ¡†æ¶ï¼Œå®ƒå°†ç¼“å­˜åˆ†è§£ä¸ºå…±äº«ç»„ä»¶å’Œé€‚é…å™¨ä¾èµ–ç»„ä»¶ï¼Œä»è€Œå‡å°‘å†…å­˜å’Œè®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å…±äº«çš„é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹æƒé‡ï¼Œé™ä½äº†å†…å­˜å ç”¨ï¼Œå¹¶é€šè¿‡ä»¥ä½ç§©å½¢å¼å­˜å‚¨é€‚é…å™¨ç»„ä»¶ï¼Œè¿›ä¸€æ­¥å‡å°‘è®¡ç®—å¼€é”€ã€‚LRAgentè¿˜å¼•å…¥äº†Flash-LoRA-Attentionå†…æ ¸ï¼Œä¼˜åŒ–äº†æ³¨æ„åŠ›è®¡ç®—ï¼Œé¿å…äº†å°†ä½ç§©ç¼“å­˜æ‰©å±•åˆ°å…¨ç»´åº¦ã€‚é€šè¿‡è¿™äº›åˆ›æ–°ï¼ŒLRAgentåœ¨ä»£ç†é—®ç­”åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ¥è¿‘å®Œå…¨å…±äº«ç¼“å­˜çš„ååé‡å’Œé¦–æ¬¡å“åº”å»¶è¿Ÿï¼ŒåŒæ—¶ä¿æŒäº†æ¥è¿‘éå…±äº«ç¼“å­˜çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.00359",
            "title": "Position: Agentic Evolution is the Path to Evolving LLMs",
            "url": "https://huggingface.co/papers/2602.00359",
            "abstract": "Large language models face limitations in adapting to changing real-world environments, necessitating a new approach called agentic evolution that treats deployment-time improvement as a goal-directed optimization process.  \t\t\t\t\tAI-generated summary \t\t\t\t As Large Language Models (LLMs) move from curated training sets into open-ended real-world environments, a fundamental limitation emerges: static training cannot keep pace with continual deployment environment change. Scaling training-time and inference-time compute improves static capability but does not close this train-deploy gap. We argue that addressing this limitation requires a new scaling axis-evolution. Existing deployment-time adaptation methods, whether parametric fine-tuning or heuristic memory accumulation, lack the strategic agency needed to diagnose failures and produce durable improvements. Our position is that agentic evolution represents the inevitable future of LLM adaptation, elevating evolution itself from a fixed pipeline to an autonomous evolver agent. We instantiate this vision in a general framework, A-Evolve, which treats deployment-time improvement as a deliberate, goal-directed optimization process over persistent system state. We further propose the evolution-scaling hypothesis: the capacity for adaptation scales with the compute allocated to evolution, positioning agentic evolution as a scalable path toward sustained, open-ended adaptation in the real world.",
            "score": 5,
            "issue_id": 906,
            "pub_date": "2026-01-30",
            "pub_date_card": {
                "ru": "30 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 30",
                "zh": "1æœˆ30æ—¥"
            },
            "hash": "473194f48499c7c3",
            "authors": [
                "Minhua Lin",
                "Hanqing Lu",
                "Zhan Shi",
                "Bing He",
                "Rui Mao",
                "Zhiwei Zhang",
                "Zongyu Wu",
                "Xianfeng Tang",
                "Hui Liu",
                "Zhenwei Dai",
                "Xiang Zhang",
                "Suhang Wang",
                "Benoit Dumoulin",
                "Jian Pei"
            ],
            "affiliations": [
                "Amazon",
                "Duke University",
                "The Pennsylvania State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.00359.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#training"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "ĞÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğº Ğ¶Ğ¸Ğ²Ğ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸: Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ½Ğ° Ğ»ĞµÑ‚Ñƒ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸Ñ…ÑÑ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ¾Ğ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°Ğº ĞºĞ°Ğº ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ»ĞµĞ´Ğ¸Ñ‚ÑŒ Ğ·Ğ° Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº A-Evolve, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ°Ğ´ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ĞµĞ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´Ğ²Ğ¸Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñƒ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑ‚Ñ‘Ñ‚ Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼, Ğ²Ñ‹Ğ´ĞµĞ»ÑĞµĞ¼Ñ‹Ğ¼ Ğ½Ğ° ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering LLMs with Autonomous Evolution for Real-World Adaptation",
                    "desc": "This paper introduces the concept of agentic evolution for Large Language Models (LLMs) to address their limitations in adapting to changing real-world environments. It argues that traditional methods like fine-tuning and memory accumulation are insufficient for effective deployment-time adaptation. The authors propose a framework called A-Evolve, which treats improvement as a goal-directed optimization process that allows LLMs to evolve autonomously. They also present the evolution-scaling hypothesis, suggesting that the ability to adapt increases with the computational resources dedicated to the evolution process."
                },
                "zh": {
                    "title": "ä»£ç†è¿›åŒ–ï¼šå¤§å‹è¯­è¨€æ¨¡å‹çš„æœªæ¥é€‚åº”ä¹‹è·¯",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é€‚åº”ä¸æ–­å˜åŒ–çš„ç°å®ç¯å¢ƒæ—¶é¢ä¸´å±€é™æ€§ï¼Œå› æ­¤éœ€è¦ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºä»£ç†è¿›åŒ–ï¼Œå®ƒå°†éƒ¨ç½²æ—¶çš„æ”¹è¿›è§†ä¸ºä¸€ä¸ªç›®æ ‡å¯¼å‘çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚ç°æœ‰çš„éƒ¨ç½²æ—¶é—´é€‚åº”æ–¹æ³•ç¼ºä¹å¿…è¦çš„æˆ˜ç•¥èƒ½åŠ›ï¼Œæ— æ³•æœ‰æ•ˆè¯Šæ–­å¤±è´¥å¹¶äº§ç”ŸæŒä¹…çš„æ”¹è¿›ã€‚æˆ‘ä»¬æå‡ºçš„A-Evolveæ¡†æ¶å°†éƒ¨ç½²æ—¶çš„æ”¹è¿›è§†ä¸ºä¸€ä¸ªæœ‰æ„è¯†çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¼ºè°ƒè¿›åŒ–çš„è‡ªä¸»æ€§ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†è¿›åŒ–æ‰©å±•å‡è®¾ï¼šé€‚åº”èƒ½åŠ›ä¸åˆ†é…ç»™è¿›åŒ–çš„è®¡ç®—èµ„æºæˆæ­£æ¯”ï¼Œä»£ç†è¿›åŒ–æ˜¯å®ç°æŒç»­å¼€æ”¾å¼é€‚åº”çš„å¯æ‰©å±•è·¯å¾„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02537",
            "title": "WorldVQA: Measuring Atomic World Knowledge in Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2602.02537",
            "abstract": "WorldVQA is a benchmark for evaluating the visual world knowledge of multimodal large language models by separating visual knowledge retrieval from reasoning to measure memorized facts.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce WorldVQA, a benchmark designed to evaluate the atomic visual world knowledge of Multimodal Large Language Models (MLLMs). Unlike current evaluations, which often conflate visual knowledge retrieval with reasoning, WorldVQA decouples these capabilities to strictly measure \"what the model memorizes.\" The benchmark assesses the atomic capability of grounding and naming visual entities across a stratified taxonomy, spanning from common head-class objects to long-tail rarities. We expect WorldVQA to serve as a rigorous test for visual factuality, thereby establishing a standard for assessing the encyclopedic breadth and hallucination rates of current and next-generation frontier models.",
            "score": 5,
            "issue_id": 892,
            "pub_date": "2026-01-28",
            "pub_date_card": {
                "ru": "28 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 28",
                "zh": "1æœˆ28æ—¥"
            },
            "hash": "6d9ce4ef7deff235",
            "authors": [
                "Runjie Zhou",
                "Youbo Shao",
                "Haoyu Lu",
                "Bowei Xing",
                "Tongtong Bai",
                "Yujie Chen",
                "Jie Zhao",
                "Lin Sui",
                "Haotian Yao",
                "Zijia Zhao",
                "Hao Yang",
                "Haoning Wu",
                "Zaida Zhou",
                "Jinguo Zhu",
                "Zhiqi Huang",
                "Yiping Bao",
                "Yangyang Liu",
                "Y. Charles",
                "Xinyu Zhou"
            ],
            "affiliations": [
                "Moonshot AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02537.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ½Ğ¸Ğ»Ğ° Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ WorldVQA â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ¼ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ¸Ğ¼ĞµĞ½Ğ½Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ„Ğ°ĞºÑ‚Ñ‹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸Ğ· ÑÑ‚Ñ€Ğ°Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ â€” Ğ¾Ñ‚ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ñ‘Ğ½Ğ½Ñ‹Ñ… ĞºĞ»Ğ°ÑÑĞ¾Ğ² Ğ´Ğ¾ Ñ€ĞµĞ´ĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞ½Ñ†Ğ¸ĞºĞ»Ğ¾Ğ¿ĞµĞ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹ Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "WorldVQA: Measuring Visual Knowledge in AI Models",
                    "desc": "WorldVQA is a new benchmark aimed at evaluating the visual knowledge of Multimodal Large Language Models (MLLMs). It distinguishes between the retrieval of visual knowledge and reasoning, allowing for a clearer assessment of what the model has memorized. The benchmark tests the model's ability to identify and name visual entities across a wide range of categories, from common objects to rare items. By doing so, WorldVQA aims to provide a standard for measuring the accuracy and comprehensiveness of visual knowledge in AI models."
                },
                "zh": {
                    "title": "WorldVQAï¼šè¯„ä¼°è§†è§‰çŸ¥è¯†çš„æ–°æ ‡å‡†",
                    "desc": "WorldVQAæ˜¯ä¸€ä¸ªåŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è§†è§‰ä¸–ç•ŒçŸ¥è¯†ã€‚å®ƒå°†è§†è§‰çŸ¥è¯†æ£€ç´¢ä¸æ¨ç†åˆ†å¼€ï¼Œä»¥ä¸¥æ ¼æµ‹é‡æ¨¡å‹æ‰€è®°å¿†çš„äº‹å®ã€‚è¯¥åŸºå‡†è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒå±‚æ¬¡åˆ†ç±»ä¸­çš„åŸºç¡€èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¸¸è§ç‰©ä½“å’Œç¨€æœ‰ç‰©ä½“çš„è¯†åˆ«ä¸å‘½åã€‚æˆ‘ä»¬å¸Œæœ›WorldVQAèƒ½æˆä¸ºè¯„ä¼°è§†è§‰äº‹å®æ€§çš„ä¸¥æ ¼æµ‹è¯•æ ‡å‡†ï¼Œä»è€Œä¸ºå½“å‰å’Œä¸‹ä¸€ä»£å‰æ²¿æ¨¡å‹çš„ç™¾ç§‘å…¨ä¹¦å¹¿åº¦å’Œå¹»è§‰ç‡æä¾›è¯„ä¼°ä¾æ®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.19103",
            "title": "Glance and Focus Reinforcement for Pan-cancer Screening",
            "url": "https://huggingface.co/papers/2601.19103",
            "abstract": "A reinforcement learning framework with glance and focus models improves pan-cancer screening in CT scans by addressing foreground-background imbalance and reducing false positives through group relative learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists' glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD).",
            "score": 4,
            "issue_id": 892,
            "pub_date": "2026-01-27",
            "pub_date_card": {
                "ru": "27 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 27",
                "zh": "1æœˆ27æ—¥"
            },
            "hash": "e23fd9bb0e094ee5",
            "authors": [
                "Linshan Wu",
                "Jiaxin Zhuang",
                "Hao Chen"
            ],
            "affiliations": [
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.19103.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#cv",
                    "#healthcare"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ”Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ÑĞºÑ€Ğ¸Ğ½Ğ¸Ğ½Ğ³ Ñ€Ğ°ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° GF-Screen Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞºÑ€Ğ¸Ğ½Ğ¸Ğ½Ğ³Ğ° Ñ€Ğ°ĞºĞ° Ğ¿Ğ¾ ĞšĞ¢-ÑĞ½Ğ¸Ğ¼ĞºĞ°Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ‚ĞºĞ°Ğ½ÑĞ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ \"Ğ’Ğ·Ğ³Ğ»ÑĞ´\" Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ñ€Ğ°Ğ¶Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ \"Ğ¤Ğ¾ĞºÑƒÑ\" Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ’Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ \"Ğ’Ğ·Ğ³Ğ»ÑĞ´\" Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ½Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ \"Ğ¤Ğ¾ĞºÑƒÑ\" Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ñ‹ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ³Ñ€ÑƒĞ¿Ğ¿ Ğ¿Ğ¾Ğ´Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ¾Ğ², Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Pan-Cancer Screening with Glance and Focus Models",
                    "desc": "This paper presents GF-Screen, a novel reinforcement learning framework designed to enhance pan-cancer screening in CT scans. It addresses the challenge of foreground-background imbalance by utilizing a Glance model to identify regions with lesions and a Focus model to accurately segment these lesions. The framework employs group relative learning to optimize the Glance model, allowing it to prioritize the most promising sub-volumes for analysis while minimizing false positives. Extensive testing on multiple datasets shows that GF-Screen significantly outperforms existing methods, achieving top results in a major pan-cancer challenge."
                },
                "zh": {
                    "title": "GF-Screenï¼šæå‡CTæ³›ç™Œç—‡ç­›æŸ¥çš„æ™ºèƒ½è§£å†³æ–¹æ¡ˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºGF-Screençš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæ”¹å–„CTæ‰«æä¸­çš„æ³›ç™Œç—‡ç­›æŸ¥ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥â€œç¥è§†â€å’Œâ€œèšç„¦â€æ¨¡å‹ï¼Œè§£å†³äº†å‰æ™¯ä¸èƒŒæ™¯ä¸å¹³è¡¡çš„é—®é¢˜ï¼Œå¹¶é€šè¿‡ç¾¤ä½“ç›¸å¯¹å­¦ä¹ å‡å°‘äº†å‡é˜³æ€§ã€‚GF-Screenåˆ©ç”¨ç¥è§†æ¨¡å‹å®šä½ç—…å˜åŒºåŸŸï¼Œå¹¶é€šè¿‡èšç„¦æ¨¡å‹ç²¾ç¡®åˆ†å‰²ç—…ç¶ï¼Œä¼˜åŒ–äº†æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGF-Screenåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†ç­›æŸ¥çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03806",
            "title": "Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation",
            "url": "https://huggingface.co/papers/2602.03806",
            "abstract": "Offline reinforcement learning method combines contextual bandit learning with partial trajectories to improve multi-turn code generation performance while reducing training costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation. While online RL tends to perform better than offline RL, its higher training cost and instability hinders wide adoption. In this paper, we build on the observation that multi-turn code generation can be formulated as a one-step recoverable Markov decision process and propose contextual bandit learning with offline trajectories (Cobalt), a new method that combines the benefits of online and offline RL. Cobalt first collects code generation trajectories using a reference LLM and divides them into partial trajectories as contextual prompts. Then, during online bandit learning, the LLM is trained to complete each partial trajectory prompt through single-step code generation. Cobalt outperforms two multi-turn online RL baselines based on GRPO and VeRPO, and substantially improves R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 absolute Pass@1 scores on LiveCodeBench. Also, we analyze LLMs' in-context reward hacking behaviors and augment Cobalt training with perturbed trajectories to mitigate this issue. Overall, our results demonstrate Cobalt as a promising solution for iterative decision-making tasks like multi-turn code generation. Our code and data are available at https://github.com/OSU-NLP-Group/cobalt.",
            "score": 3,
            "issue_id": 893,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "23c0a359167c4dfe",
            "authors": [
                "Ziru Chen",
                "Dongdong Chen",
                "Ruinan Jin",
                "Yingbin Liang",
                "Yujia Xie",
                "Huan Sun"
            ],
            "affiliations": [
                "Microsoft",
                "The Ohio State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03806.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#plp"
                ],
                "emoji": "ğŸ’»",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ½Ğ´Ğ¸Ñ‚Ñ‹ Ğ¸ Ğ¾Ñ„Ñ„Ğ»Ğ°Ğ¹Ğ½ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Cobalt, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¹ Ğ±Ğ°Ğ½Ğ´Ğ¸Ñ‚ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° ÑĞ·Ñ‹ĞºĞµ LLM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ĞºĞ°Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ RL Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ĞµÑ‘ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰Ñ‘Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Cobalt: Bridging Offline and Online RL for Code Generation",
                    "desc": "This paper introduces Cobalt, an offline reinforcement learning method that enhances multi-turn code generation by integrating contextual bandit learning with partial trajectories. By treating multi-turn code generation as a recoverable Markov decision process, Cobalt leverages previously collected code generation data to create contextual prompts for training. The method allows for efficient single-step code generation while significantly reducing training costs compared to traditional online reinforcement learning approaches. Experimental results show that Cobalt outperforms existing online RL methods, demonstrating its effectiveness in improving performance on code generation tasks."
                },
                "zh": {
                    "title": "Cobaltï¼šæå‡å¤šè½®ä»£ç ç”Ÿæˆçš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•Cobaltï¼Œæ—¨åœ¨æé«˜å¤šè½®ä»£ç ç”Ÿæˆçš„æ€§èƒ½ï¼ŒåŒæ—¶é™ä½è®­ç»ƒæˆæœ¬ã€‚Cobaltç»“åˆäº†ä¸Šä¸‹æ–‡èµŒåšå­¦ä¹ å’Œéƒ¨åˆ†è½¨è¿¹ï¼Œé€šè¿‡ä½¿ç”¨å‚è€ƒå¤§å‹è¯­è¨€æ¨¡å‹æ”¶é›†ä»£ç ç”Ÿæˆè½¨è¿¹ï¼Œå¹¶å°†å…¶åˆ†å‰²ä¸ºä¸Šä¸‹æ–‡æç¤ºã€‚è¯¥æ–¹æ³•åœ¨åœ¨çº¿èµŒåšå­¦ä¹ ä¸­è®­ç»ƒæ¨¡å‹å®Œæˆæ¯ä¸ªéƒ¨åˆ†è½¨è¿¹æç¤ºï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¤šè½®åœ¨çº¿å¼ºåŒ–å­¦ä¹ åŸºçº¿ã€‚ç ”ç©¶è¿˜åˆ†æäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡ä¸­çš„å¥–åŠ±é»‘å®¢è¡Œä¸ºï¼Œå¹¶é€šè¿‡æ‰°åŠ¨è½¨è¿¹å¢å¼ºCobaltçš„è®­ç»ƒï¼Œä»¥å‡è½»è¿™ä¸€é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03295",
            "title": "POP: Prefill-Only Pruning for Efficient Large Model Inference",
            "url": "https://huggingface.co/papers/2602.03295",
            "abstract": "Stage-aware pruning method for large language and vision-language models that improves efficiency by selectively removing layers during different processing phases while maintaining accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated remarkable capabilities. However, their deployment is hindered by significant computational costs. Existing structured pruning methods, while hardware-efficient, often suffer from significant accuracy degradation. In this paper, we argue that this failure stems from a stage-agnostic pruning approach that overlooks the asymmetric roles between the prefill and decode stages. By introducing a virtual gate mechanism, our importance analysis reveals that deep layers are critical for next-token prediction (decode) but largely redundant for context encoding (prefill). Leveraging this insight, we propose Prefill-Only Pruning (POP), a stage-aware inference strategy that safely omits deep layers during the computationally intensive prefill stage while retaining the full model for the sensitive decode stage. To enable the transition between stages, we introduce independent Key-Value (KV) projections to maintain cache integrity, and a boundary handling strategy to ensure the accuracy of the first generated token. Extensive experiments on Llama-3.1, Qwen3-VL, and Gemma-3 across diverse modalities demonstrate that POP achieves up to 1.37times speedup in prefill latency with minimal performance loss, effectively overcoming the accuracy-efficiency trade-off limitations of existing structured pruning methods.",
            "score": 3,
            "issue_id": 901,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "0010032aca79834a",
            "authors": [
                "Junhui He",
                "Zhihui Fu",
                "Jun Wang",
                "Qingan Li"
            ],
            "affiliations": [
                "OPPO Research Institute",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03295.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ° ÑĞ»Ğ¾ĞµĞ² Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ¿Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€Ğ¾Ğ»Ğ¸ ÑĞ»Ğ¾ĞµĞ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ ÑĞ»Ğ¾Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°, Ğ½Ğ¾ Ğ¸Ğ·Ğ»Ğ¸ÑˆĞ½Ğ¸ Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Prefill-Only Pruning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ ÑĞ»Ğ¾Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ¿Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Llama-3.1 Ğ¸ Qwen3-VL Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² 1.37 Ñ€Ğ°Ğ·Ğ° Ğ±ĞµĞ· Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Efficient Layer Pruning for Enhanced Model Performance",
                    "desc": "This paper presents a novel stage-aware pruning method for large language models (LLMs) and vision-language models (VLMs) that enhances computational efficiency without sacrificing accuracy. The authors identify that traditional pruning techniques often degrade performance because they do not consider the different roles of model layers during the prefill and decode stages. By implementing a Prefill-Only Pruning (POP) strategy, they selectively remove deep layers during the prefill phase, which is less sensitive to accuracy, while keeping the full model intact for the decode phase, where precision is crucial. Their experiments show that this approach can significantly speed up prefill latency by up to 1.37 times, demonstrating a successful balance between efficiency and model performance."
                },
                "zh": {
                    "title": "é˜¶æ®µæ„ŸçŸ¥å‰ªæï¼šæå‡æ•ˆç‡ä¸å‡†ç¡®æ€§çš„å¹³è¡¡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é˜¶æ®µæ„ŸçŸ¥çš„å‰ªææ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ã€‚é€šè¿‡åœ¨ä¸åŒå¤„ç†é˜¶æ®µé€‰æ‹©æ€§åœ°ç§»é™¤å±‚ï¼Œä¿æŒäº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†è™šæ‹Ÿé—¨æœºåˆ¶ï¼Œåˆ†æäº†æ·±å±‚åœ¨ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æå‡ºäº†ä»…åœ¨é¢„å¡«å……é˜¶æ®µå‰ªæçš„ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é¢„å¡«å……å»¶è¿Ÿä¸Šå®ç°äº†é«˜è¾¾1.37å€çš„åŠ é€Ÿï¼ŒåŒæ—¶æ€§èƒ½æŸå¤±æœ€å°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02419",
            "title": "SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration",
            "url": "https://huggingface.co/papers/2602.02419",
            "abstract": "SafeGround is a uncertainty-aware framework for GUI grounding models that uses distribution-aware uncertainty quantification and calibration to enable risk-aware predictions with controlled false discovery rates.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38% percentage points over Gemini-only inference.",
            "score": 3,
            "issue_id": 893,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "c6a8e53e2ac59cfd",
            "authors": [
                "Qingni Wang",
                "Yue Fan",
                "Xin Eric Wang"
            ],
            "affiliations": [
                "University of California, Santa Barbara",
                "University of California, Santa Cruz"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02419.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#inference",
                    "#agents",
                    "#security"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "SafeGround â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ Ğ½Ğ° ÑĞºÑ€Ğ°Ğ½Ğµ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ñ€Ğ¸ÑĞºĞ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ’Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¹ (FDR). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SafeGround Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ½Ğ° 5,38% Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ GUI."
                },
                "en": {
                    "title": "Enhancing GUI Grounding with Uncertainty Awareness",
                    "desc": "SafeGround is a framework designed to improve the reliability of GUI grounding models by incorporating uncertainty awareness. It uses a method for quantifying uncertainty that captures how spread out the model's predictions are, which helps in understanding the risk of making incorrect decisions. The framework also includes a calibration process that sets a decision threshold to control the rate of false discoveries during testing. By applying SafeGround to various models, the results show significant improvements in accuracy and better differentiation between correct and incorrect predictions."
                },
                "zh": {
                    "title": "SafeGroundï¼šæå‡GUIå®šä½æ¨¡å‹çš„å¯é æ€§ä¸å‡†ç¡®æ€§",
                    "desc": "SafeGroundæ˜¯ä¸€ä¸ªå…³æ³¨ä¸ç¡®å®šæ€§çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰å®šä½æ¨¡å‹çš„å¯é æ€§ã€‚å®ƒé€šè¿‡åˆ†å¸ƒæ„ŸçŸ¥çš„ä¸ç¡®å®šæ€§é‡åŒ–å’Œæ ¡å‡†ï¼Œèƒ½å¤Ÿåœ¨æµ‹è¯•å‰è¿›è¡Œé£é™©æ„è¯†çš„é¢„æµ‹ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•æ‰æ¨¡å‹è¾“å‡ºçš„éšæœºæ ·æœ¬çš„ç©ºé—´åˆ†æ•£æ€§ï¼Œæ¥é‡åŒ–ä¸ç¡®å®šæ€§ï¼Œå¹¶åœ¨æµ‹è¯•æ—¶è®¾å®šå…·æœ‰ç»Ÿè®¡ä¿è¯çš„é”™è¯¯å‘ç°ç‡ï¼ˆFDRï¼‰æ§åˆ¶çš„å†³ç­–é˜ˆå€¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSafeGroundåœ¨å¤šä¸ªGUIå®šä½æ¨¡å‹ä¸Šæ˜¾è‘—æé«˜äº†ç³»ç»Ÿçº§å‡†ç¡®æ€§ï¼Œæœ€å¤šå¯æå‡5.38ä¸ªç™¾åˆ†ç‚¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.01753",
            "title": "ObjEmbed: Towards Universal Multimodal Object Embeddings",
            "url": "https://huggingface.co/papers/2602.01753",
            "abstract": "ObjEmbed is a novel multimodal language-model embedding approach that decomposes images into regional embeddings for improved object-level visual understanding and retrieval tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.",
            "score": 3,
            "issue_id": 892,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "453d05bbcb5933c8",
            "authors": [
                "Shenghao Fu",
                "Yukun Su",
                "Fengyun Rao",
                "Jing Lyu",
                "Xiaohua Xie",
                "Wei-Shi Zheng"
            ],
            "affiliations": [
                "Guangdong Province Key Laboratory of Information Security Technology, China",
                "Independent Researcher",
                "Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China",
                "Pazhou Laboratory (Huangpu), China",
                "Peng Cheng Laboratory, China",
                "School of Computer Science and Engineering, Sun Yat-sen University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.01753.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#multimodal",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ±ÑŠĞµĞºÑ‚Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "ObjEmbed â€” ÑÑ‚Ğ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ²Ğ° Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑĞ¼Ñ‹ÑĞ»Ñƒ Ğ¸ IoU-ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ visual grounding, Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ²ÑĞµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ forward pass Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° 18 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Enhancing Visual Understanding with Regional Embeddings",
                    "desc": "ObjEmbed is a new approach in multimodal language modeling that enhances the understanding of images by breaking them down into regional embeddings. This method addresses the challenge of aligning specific image regions with their corresponding textual descriptions, which is crucial for tasks like visual grounding and image retrieval. By generating both object embeddings for semantic matching and IoU embeddings for localization quality, ObjEmbed improves the accuracy of object retrieval. Its efficient encoding allows for simultaneous processing of all objects and the full image, leading to superior performance across various benchmarks."
                },
                "zh": {
                    "title": "ObjEmbedï¼šæå‡è§†è§‰ç†è§£çš„å¤šæ¨¡æ€åµŒå…¥æ–¹æ³•",
                    "desc": "ObjEmbedæ˜¯ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åµŒå…¥æ–¹æ³•ï¼Œå®ƒå°†å›¾åƒåˆ†è§£ä¸ºåŒºåŸŸåµŒå…¥ï¼Œä»¥æé«˜å¯¹è±¡çº§è§†è§‰ç†è§£å’Œæ£€ç´¢ä»»åŠ¡çš„æ•ˆæœã€‚è¯¥æ–¹æ³•è§£å†³äº†å›¾åƒåŒºåŸŸä¸ç‰¹å®šçŸ­è¯­ä¹‹é—´çš„ç»†ç²’åº¦å¯¹é½é—®é¢˜ï¼Œæ”¯æŒè§†è§‰å®šä½ã€å±€éƒ¨å›¾åƒæ£€ç´¢å’Œå…¨å±€å›¾åƒæ£€ç´¢ç­‰å¤šç§è§†è§‰ç†è§£ä»»åŠ¡ã€‚ObjEmbedå…·æœ‰ä¸‰ä¸ªå…³é”®ç‰¹æ€§ï¼šå¯¹è±¡å¯¼å‘è¡¨ç¤ºã€é€šç”¨æ€§å’Œé«˜æ•ˆç¼–ç ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†åŒºåŸŸçº§å’Œå›¾åƒçº§ä»»åŠ¡ã€‚é€šè¿‡åœ¨18ä¸ªä¸åŒåŸºå‡†ä¸Šçš„ä¼˜è¶Šè¡¨ç°ï¼Œè¯æ˜äº†å…¶å¼ºå¤§çš„è¯­ä¹‰åŒºåˆ†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03837",
            "title": "Accelerating Scientific Research with Gemini: Case Studies and Common Techniques",
            "url": "https://huggingface.co/papers/2602.03837",
            "abstract": "Advanced AI models demonstrate capability in supporting expert-level mathematical discovery and scientific research through collaborative approaches involving proof verification and automated code execution.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google's Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer. While the majority of our results stem from this interactive, conversational methodology, we also highlight specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a \"neuro-symbolic\" loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery.",
            "score": 2,
            "issue_id": 892,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "477d690a8c841188",
            "authors": [
                "David P. Woodruff",
                "Vincent Cohen-Addad",
                "Lalit Jain",
                "Jieming Mao",
                "Song Zuo",
                "MohammadHossein Bateni",
                "Simina Branzei",
                "Michael P. Brenner",
                "Lin Chen",
                "Ying Feng",
                "Lance Fortnow",
                "Gang Fu",
                "Ziyi Guan",
                "Zahra Hadizadeh",
                "Mohammad T. Hajiaghayi",
                "Mahdi JafariRaviz",
                "Adel Javanmard",
                "Karthik C. S.",
                "Ken-ichi Kawarabayashi",
                "Ravi Kumar",
                "Silvio Lattanzi",
                "Euiwoong Lee",
                "Yi Li",
                "Ioannis Panageas",
                "Dimitris Paparas",
                "Benjamin Przybocki",
                "Bernardo Subercaseaux",
                "Ola Svensson",
                "Shayan Taherijam",
                "Xuan Wu",
                "Eylon Yogev",
                "Morteza Zadimoghaddam",
                "Samson Zhou",
                "Vahab Mirrokni"
            ],
            "affiliations": [
                "Bar-Ilan University",
                "Carnegie Mellon University",
                "EPFL",
                "Google Research",
                "Harvard University",
                "Illinois Institute of Technology",
                "MIT",
                "Nanyang Technological University",
                "National Institute of Informatics, Tokyo",
                "Purdue University",
                "Rutgers University",
                "Texas A&M University",
                "The University of Tokyo",
                "University of California, Irvine",
                "University of Maryland, College Park",
                "University of Michigan",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03837.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#science",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "AI ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ñ‚Ğ½Ñ‘Ñ€: Ğ¾Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ¿Ğ¾Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¾Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº Google Gemini, Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ² Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸ĞºĞµ Ğ¸ ÑĞ¼ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ, Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ°Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ½Ğ¾ Ğ¸ Ğ² Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…â€”Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ½ĞºĞ¸Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°Ñ… Ğ¸ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ†Ğ¸ĞºĞ»Ğ°Ñ… Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ°Ğº Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ñ‚Ğ½Ñ‘Ñ€Ğ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ, Ğ° Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "AI: A Collaborative Partner in Scientific Discovery",
                    "desc": "This paper explores how advanced AI models, particularly Google's Gemini, can assist researchers in making significant mathematical discoveries and conducting scientific research. It presents case studies where AI has helped solve open problems and generate new proofs in fields like theoretical computer science, economics, and physics. The authors identify effective collaboration techniques between humans and AI, such as iterative refinement and problem decomposition. Additionally, they showcase innovative uses of AI, including its role as a rigorous reviewer and its ability to autonomously verify complex proofs through code execution."
                },
                "zh": {
                    "title": "AIä¸äººç±»åˆä½œï¼Œæ¨åŠ¨ç§‘å­¦å‘ç°æ–°çºªå…ƒ",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å…ˆè¿›çš„äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨æ•°å­¦å‘ç°å’Œç§‘å­¦ç ”ç©¶ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯ä¸ç ”ç©¶äººå‘˜çš„åˆä½œã€‚ç ”ç©¶è¡¨æ˜ï¼Œè°·æ­Œçš„Geminiæ¨¡å‹èƒ½å¤Ÿå¸®åŠ©è§£å†³å¼€æ”¾æ€§é—®é¢˜ã€åé©³çŒœæƒ³å¹¶ç”Ÿæˆæ–°çš„è¯æ˜ã€‚é€šè¿‡æ¡ˆä¾‹ç ”ç©¶ï¼Œæç‚¼å‡ºæœ‰æ•ˆçš„äººæœºåä½œæŠ€æœ¯ï¼Œå¦‚è¿­ä»£ä¼˜åŒ–ã€é—®é¢˜åˆ†è§£å’Œè·¨å­¦ç§‘çŸ¥è¯†è½¬ç§»ã€‚è®ºæ–‡å¼ºè°ƒï¼ŒAIä¸ä»…æ˜¯è‡ªåŠ¨åŒ–å·¥å…·ï¼Œæ›´æ˜¯ç§‘å­¦å‘ç°è¿‡ç¨‹ä¸­çš„çœŸæ­£åˆä½œä¼™ä¼´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03454",
            "title": "Contextualized Visual Personalization in Vision-Language Models",
            "url": "https://huggingface.co/papers/2602.03454",
            "abstract": "CoViP addresses contextualized visual personalization by treating personalized image captioning as a core task and improving capabilities through reinforcement-learning-based post-training and caption-augmented generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent progress in vision-language models (VLMs), existing approaches often fail to generate personalized responses based on the user's specific experiences, as they lack the ability to associate visual inputs with a user's accumulated visual-textual context. We newly formalize this challenge as contextualized visual personalization, which requires the visual recognition and textual retrieval of personalized visual experiences by VLMs when interpreting new images. To address this issue, we propose CoViP, a unified framework that treats personalized image captioning as a core task for contextualized visual personalization and improves this capability through reinforcement-learning-based post-training and caption-augmented generation. We further introduce diagnostic evaluations that explicitly rule out textual shortcut solutions and verify whether VLMs truly leverage visual context. Extensive experiments demonstrate that existing open-source and proprietary VLMs exhibit substantial limitations, while CoViP not only improves personalized image captioning but also yields holistic gains across downstream personalization tasks. These results highlight CoViP as a crucial stage for enabling robust and generalizable contextualized visual personalization.",
            "score": 2,
            "issue_id": 892,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "62f7fb59d0b1e6db",
            "authors": [
                "Yeongtak Oh",
                "Sangwon Yu",
                "Junsung Park",
                "Han Cheol Moon",
                "Jisoo Mok",
                "Sungroh Yoon"
            ],
            "affiliations": [
                "Daegu Gyeongbuk Institute of Science and Technology, Daegu, South Korea",
                "Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea",
                "Interdisciplinary Program in Artificial Intelligence, Seoul National University, Seoul, Korea",
                "Samsung Electronics, South Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03454.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#multimodal",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "ğŸ“¸",
                "ru": {
                    "title": "ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ vision-language",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° CoViP Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ¸Ñ€ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑĞ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¸ÑĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑĞ¼ĞµĞ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Enhancing Personalization in Image Captioning with CoViP",
                    "desc": "CoViP is a framework designed to enhance personalized image captioning by integrating contextualized visual personalization. It addresses the limitations of current vision-language models (VLMs) that struggle to generate responses tailored to individual user experiences. By employing reinforcement learning for post-training and augmenting generation with captions, CoViP improves the model's ability to connect visual inputs with a user's unique context. The framework also includes diagnostic evaluations to ensure that VLMs effectively utilize visual context rather than relying on shortcuts, demonstrating significant improvements in personalized tasks."
                },
                "zh": {
                    "title": "CoViPï¼šå®ç°ä¸Šä¸‹æ–‡åŒ–çš„è§†è§‰ä¸ªæ€§åŒ–",
                    "desc": "CoViPæ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸ªæ€§åŒ–å›¾åƒæè¿°æ¥å®ç°ä¸Šä¸‹æ–‡åŒ–çš„è§†è§‰ä¸ªæ€§åŒ–ã€‚å®ƒåˆ©ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡ŒåæœŸè®­ç»ƒï¼Œå¹¶é€šè¿‡å¢å¼ºæè¿°ç”Ÿæˆæ¥æå‡èƒ½åŠ›ã€‚è¯¥æ–¹æ³•è§£å†³äº†ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆä¸ªæ€§åŒ–å“åº”æ—¶çš„ä¸è¶³ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ç»“åˆç”¨æˆ·çš„è§†è§‰å’Œæ–‡æœ¬èƒŒæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoViPåœ¨ä¸ªæ€§åŒ–å›¾åƒæè¿°å’Œå…¶ä»–ä¸ªæ€§åŒ–ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºæ˜¾è‘—çš„æ”¹è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02914",
            "title": "FaceLinkGen: Rethinking Identity Leakage in Privacy-Preserving Face Recognition with Identity Extraction",
            "url": "https://huggingface.co/papers/2602.02914",
            "abstract": "FaceLinkGen attack demonstrates that current privacy-preserving face recognition methods fail to protect identity information despite pixel-level distortion metrics suggesting adequate protection.  \t\t\t\t\tAI-generated summary \t\t\t\t Transformation-based privacy-preserving face recognition (PPFR) aims to verify identities while hiding facial data from attackers and malicious service providers. Existing evaluations mostly treat privacy as resistance to pixel-level reconstruction, measured by PSNR and SSIM. We show that this reconstruction-centric view fails. We present FaceLinkGen, an identity extraction attack that performs linkage/matching and face regeneration directly from protected templates without recovering original pixels. On three recent PPFR systems, FaceLinkGen reaches over 98.5\\% matching accuracy and above 96\\% regeneration success, and still exceeds 92\\% matching and 94\\% regeneration in a near zero knowledge setting. These results expose a structural gap between pixel distortion metrics, which are widely used in PPFR evaluation, and real privacy. We show that visual obfuscation leaves identity information broadly exposed to both external intruders and untrusted service providers.",
            "score": 2,
            "issue_id": 892,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "8962edb941993f54",
            "authors": [
                "Wenqi Guo",
                "Shan Du"
            ],
            "affiliations": [
                "Department of CMPS, University of British Columbia, Kelowna, Canada",
                "Weathon Software, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02914.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#benchmark",
                    "#cv",
                    "#security"
                ],
                "emoji": "ğŸ”“",
                "ru": {
                    "title": "ĞŸĞ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ½Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ»Ğ¸Ñ†",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ°Ñ‚Ğ°ĞºĞ° FaceLinkGen, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¸Ñ† - Ğ¾Ğ½Ğ¸ Ğ½Ğµ Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ğ°ÑÑ‚ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ñ‹ Ğ½Ğ° ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ (PSNR, SSIM), Ğ½Ğ¾ ÑÑ‚Ğ¾ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¹ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ñ€ĞµĞ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ»Ğ¸Ñ† Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ¸Ğ· Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ² Ğ±ĞµĞ· Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ 98.5% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ„ÑƒÑĞºĞ°Ñ†Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ°Ñ‚Ğ°Ğº."
                },
                "en": {
                    "title": "FaceLinkGen: Unmasking the Flaws in Privacy-Preserving Face Recognition",
                    "desc": "The FaceLinkGen attack reveals that current privacy-preserving face recognition (PPFR) methods do not adequately protect individual identities, despite appearing effective based on pixel-level distortion metrics like PSNR and SSIM. This paper introduces FaceLinkGen, an attack that can extract and regenerate identities from protected face templates without needing to reconstruct the original pixel data. The results demonstrate that FaceLinkGen achieves over 98.5% matching accuracy and more than 96% regeneration success across various PPFR systems. This highlights a significant disconnect between traditional pixel distortion evaluations and actual privacy protection, showing that visual obfuscation techniques still leave identity information vulnerable to unauthorized access."
                },
                "zh": {
                    "title": "æ­ç¤ºéšç§ä¿æŠ¤äººè„¸è¯†åˆ«çš„ç»“æ„æ€§ç¼ºé™·",
                    "desc": "FaceLinkGenæ”»å‡»è¡¨æ˜ï¼Œå½“å‰çš„éšç§ä¿æŠ¤äººè„¸è¯†åˆ«æ–¹æ³•åœ¨ä¿æŠ¤èº«ä»½ä¿¡æ¯æ–¹é¢å­˜åœ¨ç¼ºé™·ï¼Œå°½ç®¡åƒç´ çº§å¤±çœŸæŒ‡æ ‡çœ‹ä¼¼æä¾›äº†è¶³å¤Ÿçš„ä¿æŠ¤ã€‚ç°æœ‰è¯„ä¼°ä¸»è¦å°†éšç§è§†ä¸ºå¯¹åƒç´ çº§é‡å»ºçš„æŠµæŠ—ï¼Œä½¿ç”¨PSNRå’ŒSSIMè¿›è¡Œæµ‹é‡ã€‚æˆ‘ä»¬æå‡ºçš„FaceLinkGenæ”»å‡»èƒ½å¤Ÿç›´æ¥ä»å—ä¿æŠ¤çš„æ¨¡æ¿ä¸­è¿›è¡Œèº«ä»½é“¾æ¥/åŒ¹é…å’Œäººè„¸å†ç”Ÿï¼Œè€Œæ— éœ€æ¢å¤åŸå§‹åƒç´ ã€‚åœ¨ä¸‰ç§æœ€æ–°çš„éšç§ä¿æŠ¤äººè„¸è¯†åˆ«ç³»ç»Ÿä¸­ï¼ŒFaceLinkGençš„åŒ¹é…å‡†ç¡®ç‡è¶…è¿‡98.5%ï¼Œå†ç”ŸæˆåŠŸç‡è¶…è¿‡96%ï¼Œå¹¶ä¸”åœ¨æ¥è¿‘é›¶çŸ¥è¯†çš„æƒ…å†µä¸‹ä»ç„¶è¶…è¿‡92%çš„åŒ¹é…å’Œ94%çš„å†ç”ŸæˆåŠŸç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02751",
            "title": "Scaling Small Agents Through Strategy Auctions",
            "url": "https://huggingface.co/papers/2602.02751",
            "abstract": "Small language models struggle with complex tasks but can be effectively coordinated through a marketplace-inspired framework that reduces costs and improves performance through strategic bidding and self-improvement mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Small language models are increasingly viewed as a promising, cost-effective approach to agentic AI, with proponents claiming they are sufficiently capable for agentic workflows. However, while smaller agents can closely match larger ones on simple tasks, it remains unclear how their performance scales with task complexity, when large models become necessary, and how to better leverage small agents for long-horizon workloads. In this work, we empirically show that small agents' performance fails to scale with task complexity on deep search and coding tasks, and we introduce Strategy Auctions for Workload Efficiency (SALE), an agent framework inspired by freelancer marketplaces. In SALE, agents bid with short strategic plans, which are scored by a systematic cost-value mechanism and refined via a shared auction memory, enabling per-task routing and continual self-improvement without training a separate router or running all models to completion. Across deep search and coding tasks of varying complexity, SALE reduces reliance on the largest agent by 53%, lowers overall cost by 35%, and consistently improves upon the largest agent's pass@1 with only a negligible overhead beyond executing the final trace. In contrast, established routers that rely on task descriptions either underperform the largest agent or fail to reduce cost -- often both -- underscoring their poor fit for agentic workflows. These results suggest that while small agents may be insufficient for complex workloads, they can be effectively \"scaled up\" through coordinated task allocation and test-time self-improvement. More broadly, they motivate a systems-level view of agentic AI in which performance gains come less from ever-larger individual models and more from market-inspired coordination mechanisms that organize heterogeneous agents into efficient, adaptive ecosystems.",
            "score": 2,
            "issue_id": 905,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "ebdb4addd8141534",
            "authors": [
                "Lisa Alazraki",
                "William F. Shen",
                "Yoram Bachrach",
                "Akhil Mathur"
            ],
            "affiliations": [
                "Imperial College London",
                "Meta Superintelligence Labs",
                "University of Cambridge"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02751.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#agents",
                    "#training"
                ],
                "emoji": "ğŸª",
                "ru": {
                    "title": "ĞœĞ°Ñ€ĞºĞµÑ‚Ğ¿Ğ»ĞµĞ¹Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: ĞºĞ°Ğº ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ SALE (Strategy Auctions for Workload Efficiency) â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ñ„Ñ€Ğ¸Ğ»Ğ°Ğ½Ñ-Ğ¼Ğ°Ñ€ĞºĞµÑ‚Ğ¿Ğ»ĞµĞ¹ÑĞ¾Ğ², Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ»Ğ°Ğ½Ñ‹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ°ÑƒĞºÑ†Ğ¸Ğ¾Ğ½Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ‰ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ°ÑƒĞºÑ†Ğ¸Ğ¾Ğ½Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñƒ Â«ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ-Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÂ» Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ SALE ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ ÑĞ°Ğ¼Ğ¾Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 53%, ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° 35% Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ, Ñ‡ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Empowering Small Models through Strategic Coordination",
                    "desc": "This paper discusses the limitations of small language models when handling complex tasks and introduces a new framework called Strategy Auctions for Workload Efficiency (SALE). SALE allows small agents to bid on tasks using strategic plans, which are evaluated based on cost and value, promoting self-improvement and efficient task allocation. The framework significantly reduces the need for larger models, cutting costs by 35% and improving performance on complex tasks. The findings suggest that instead of relying solely on larger models, a coordinated approach using smaller agents can enhance overall efficiency in agentic AI workflows."
                },
                "zh": {
                    "title": "å°å‹ä»£ç†çš„å¸‚åœºåŒ–åè°ƒæå‡æ€§èƒ½",
                    "desc": "å°å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ï¼Œä½†é€šè¿‡ä¸€ç§å—å¸‚åœºå¯å‘çš„æ¡†æ¶å¯ä»¥æœ‰æ•ˆåè°ƒå®ƒä»¬ï¼Œä»è€Œé™ä½æˆæœ¬å¹¶æé«˜æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºç­–ç•¥æ‹å–çš„å·¥ä½œè´Ÿè½½æ•ˆç‡æ¡†æ¶ï¼Œå…è®¸ä»£ç†é€šè¿‡çŸ­æœŸæˆ˜ç•¥è®¡åˆ’è¿›è¡Œç«æ ‡ï¼Œå¹¶é€šè¿‡ç³»ç»Ÿçš„æˆæœ¬-ä»·å€¼æœºåˆ¶è¿›è¡Œè¯„åˆ†å’Œä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ·±åº¦æœç´¢å’Œç¼–ç ä»»åŠ¡ä¸­æ˜¾è‘—å‡å°‘äº†å¯¹å¤§å‹ä»£ç†çš„ä¾èµ–ï¼Œå¹¶é™ä½äº†æ•´ä½“æˆæœ¬ï¼ŒåŒæ—¶åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†æœ€å¤§çš„ä»£ç†ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡å°å‹ä»£ç†åœ¨å¤æ‚å·¥ä½œè´Ÿè½½ä¸­å¯èƒ½ä¸è¶³ï¼Œä½†é€šè¿‡åè°ƒä»»åŠ¡åˆ†é…å’Œè‡ªæˆ‘æ”¹è¿›ï¼Œå¯ä»¥æœ‰æ•ˆæå‡å…¶æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.01212",
            "title": "SimpleGPT: Improving GPT via A Simple Normalization Strategy",
            "url": "https://huggingface.co/papers/2602.01212",
            "abstract": "SimpleNorm normalization strategy stabilizes activation scales and enables larger stable learning rates in Transformer models by reducing Hessian spectral norm, leading to improved training performance.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we revisit Transformer optimization through the lens of second-order geometry and establish a direct connection between architectural design, activation scale, the Hessian matrix, and the maximum tolerable learning rate. We introduce a simple normalization strategy, termed SimpleNorm, which stabilizes intermediate activation scales by construction. Then, by analyzing the Hessian of the loss with respect to network activations, we theoretically show that SimpleNorm significantly reduces the spectral norm of the Hessian, thereby permitting larger stable learning rates. We validate our theoretical findings through extensive experiments on large GPT models at parameter scales 1B, 1.4B, 7B and 8B. Empirically, SimpleGPT, our SimpleNorm-based network, tolerates learning rates 3times-10times larger than standard convention, consistently demonstrates strong optimization stability, and achieves substantially better performance than well-established baselines. Specifically, when training 7B-scale models for 60K steps, SimpleGPT achieves a training loss that is 0.08 lower than that of LLaMA2 with QKNorm, reducing the loss from 2.290 to 2.208. Our source code will be released at https://github.com/Ocram7/SimpleGPT.",
            "score": 2,
            "issue_id": 896,
            "pub_date": "2026-02-01",
            "pub_date_card": {
                "ru": "1 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 1",
                "zh": "2æœˆ1æ—¥"
            },
            "hash": "caf81558dc906d12",
            "authors": [
                "Marco Chen",
                "Xianbiao Qi",
                "Yelin He",
                "Jiaquan Ye",
                "Rong Xiao"
            ],
            "affiliations": [
                "Intellifusion Inc.",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.01212.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#architecture",
                    "#optimization",
                    "#math"
                ],
                "emoji": "ğŸ“ˆ",
                "ru": {
                    "title": "ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·: Ğ¿ÑƒÑ‚ÑŒ Ğº ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ SimpleNorm â€” Ğ¿Ñ€Ğ¾ÑÑ‚ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ñ‹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ² Transformer Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SimpleNorm ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ½Ğ¾Ñ€Ğ¼Ñƒ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ“ĞµÑÑĞ¸Ğ°Ğ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… GPT Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (Ğ¾Ñ‚ 1B Ğ´Ğ¾ 8B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ SimpleGPT ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ĞµĞ½ Ğ¿Ñ€Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑÑ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² 3-10 Ñ€Ğ°Ğ· Ğ²Ñ‹ÑˆĞµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸, Ñ‡ĞµĞ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ½Ğ° 0.08 Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ LLaMA2 Ğ¿Ñ€Ğ¸ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞµ 7B Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Unlocking Higher Learning Rates with SimpleNorm",
                    "desc": "This paper introduces a new normalization technique called SimpleNorm, which helps stabilize the activation scales in Transformer models. By focusing on the Hessian matrix's spectral norm, SimpleNorm allows for larger and more stable learning rates during training. The authors demonstrate that their method leads to improved optimization stability and better performance on large-scale models. Extensive experiments show that models using SimpleNorm can tolerate learning rates that are 3 to 10 times higher than traditional methods, resulting in lower training loss compared to existing models."
                },
                "zh": {
                    "title": "SimpleNormï¼šæå‡Transformerå­¦ä¹ ç‡çš„ç¨³å®šæ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å½’ä¸€åŒ–ç­–ç•¥ï¼Œç§°ä¸ºSimpleNormï¼Œæ—¨åœ¨ç¨³å®šTransformeræ¨¡å‹ä¸­çš„æ¿€æ´»å°ºåº¦ã€‚é€šè¿‡åˆ†ææŸå¤±å‡½æ•°çš„HessiançŸ©é˜µï¼Œç ”ç©¶è¡¨æ˜SimpleNormæ˜¾è‘—é™ä½äº†Hessiançš„è°±èŒƒæ•°ï¼Œä»è€Œå…è®¸ä½¿ç”¨æ›´å¤§çš„ç¨³å®šå­¦ä¹ ç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºSimpleNormçš„SimpleGPTç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæ‰¿å—æ¯”ä¼ ç»Ÿæ–¹æ³•é«˜å‡º3åˆ°10å€çš„å­¦ä¹ ç‡ï¼Œå¹¶ä¸”ä¼˜åŒ–ç¨³å®šæ€§æ˜¾è‘—å¢å¼ºã€‚æœ€ç»ˆï¼ŒSimpleGPTåœ¨è®­ç»ƒ7Bè§„æ¨¡æ¨¡å‹æ—¶ï¼ŒæŸå¤±å€¼æ¯”LLaMA2ä½0.08ï¼Œè¡¨ç°å‡ºæ›´ä¼˜çš„è®­ç»ƒæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03238",
            "title": "The Necessity of a Unified Framework for LLM-Based Agent Evaluation",
            "url": "https://huggingface.co/papers/2602.03238",
            "abstract": "Large Language Models have advanced general-purpose agents, but current evaluation benchmarks suffer from confounding factors and lack of standardization, necessitating a unified framework for rigorous assessment.  \t\t\t\t\tAI-generated summary \t\t\t\t With the advent of Large Language Models (LLMs), general-purpose agents have seen fundamental advancements. However, evaluating these agents presents unique challenges that distinguish them from static QA benchmarks. We observe that current agent benchmarks are heavily confounded by extraneous factors, including system prompts, toolset configurations, and environmental dynamics. Existing evaluations often rely on fragmented, researcher-specific frameworks where the prompt engineering for reasoning and tool usage varies significantly, making it difficult to attribute performance gains to the model itself. Additionally, the lack of standardized environmental data leads to untraceable errors and non-reproducible results. This lack of standardization introduces substantial unfairness and opacity into the field. We propose that a unified evaluation framework is essential for the rigorous advancement of agent evaluation. To this end, we introduce a proposal aimed at standardizing agent evaluation.",
            "score": 1,
            "issue_id": 895,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "1a9ff02f62d1b11b",
            "authors": [
                "Pengyu Zhu",
                "Li Sun",
                "Philip S. Yu",
                "Sen Su"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "Chongqing University of Posts and Telecommunications",
                "University of Illinois Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03238.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ñ‡ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ñ‚Ñ‹, ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ° Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ñ‚Ñ€ÑƒĞ´Ğ½ÑĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¢ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ¾Ğ·Ğ½ĞµĞ½Ğ½Ñ‹Ğµ, ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğº Ğ¸Ğ½Ğ¶Ğ¸Ğ½Ğ¸Ñ€Ğ¸Ğ½Ğ³Ñƒ Ğ¿Ñ€Ğ¾Ğ¼Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½ĞµĞ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM."
                },
                "en": {
                    "title": "Standardizing Evaluation for Fair AI Agents",
                    "desc": "This paper discusses the challenges in evaluating Large Language Models (LLMs) used as general-purpose agents. It highlights that current benchmarks are affected by various confounding factors, such as different prompts and tool configurations, which complicate the assessment of model performance. The authors argue that the lack of a standardized evaluation framework leads to unfairness and non-reproducible results in the field. To address these issues, they propose a unified framework for rigorous and consistent evaluation of agent performance."
                },
                "zh": {
                    "title": "ç»Ÿä¸€è¯„ä¼°æ¡†æ¶ï¼Œæ¨åŠ¨æ™ºèƒ½ä½“è¯„ä¼°çš„ä¸¥æ ¼è¿›å±•",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨åŠ¨äº†é€šç”¨æ™ºèƒ½ä½“çš„è¿›æ­¥ï¼Œä½†å½“å‰çš„è¯„ä¼°åŸºå‡†å­˜åœ¨æ··æ·†å› ç´ å’Œç¼ºä¹æ ‡å‡†åŒ–çš„é—®é¢˜ã€‚è¿™äº›è¯„ä¼°é¢ä¸´çš„æŒ‘æˆ˜ä¸é™æ€é—®ç­”åŸºå‡†ä¸åŒï¼Œä¸»è¦å—åˆ°ç³»ç»Ÿæç¤ºã€å·¥å…·é…ç½®å’Œç¯å¢ƒåŠ¨æ€ç­‰å¤–éƒ¨å› ç´ çš„å½±å“ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•å¾€å¾€ä¾èµ–äºåˆ†æ•£çš„ã€ç ”ç©¶è€…ç‰¹å®šçš„æ¡†æ¶ï¼Œå¯¼è‡´éš¾ä»¥å°†æ€§èƒ½æå‡å½’å› äºæ¨¡å‹æœ¬èº«ã€‚æ­¤å¤–ï¼Œç¼ºä¹æ ‡å‡†åŒ–çš„ç¯å¢ƒæ•°æ®ä½¿å¾—é”™è¯¯éš¾ä»¥è¿½è¸ªï¼Œç»“æœä¹Ÿéš¾ä»¥é‡å¤ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02494",
            "title": "MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training",
            "url": "https://huggingface.co/papers/2602.02494",
            "abstract": "MEG-XL demonstrates improved brain-to-text decoding performance through extended pre-training with 2.5-minute MEG context, significantly outperforming previous models with less contextual data.  \t\t\t\t\tAI-generated summary \t\t\t\t Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL .",
            "score": 1,
            "issue_id": 901,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "5fd183a0041f2728",
            "authors": [
                "Dulhan Jayalath",
                "Oiwi Parker Jones"
            ],
            "affiliations": [
                "PNPL, University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02494.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#transfer_learning",
                    "#long_context",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ”Ğ¾Ğ»Ğ³Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ â€” ĞºĞ»ÑÑ‡ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ·Ğ³Ğ°",
                    "desc": "MEG-XL â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ¸Ğ· ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¼Ğ¾Ğ·Ğ³Ğ°, Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ² 2.5 Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾ÑĞ½Ñ†ĞµÑ„Ğ°Ğ»Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ² 5-300 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞµ, Ñ‡ĞµĞ¼ Ğ² Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ…. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ·Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ² Ğ¸Ğ· ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¼Ğ¾Ğ·Ğ³Ğ° MEG-XL Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ 1 Ñ‡Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ 50 Ñ‡Ğ°ÑĞ¾Ğ², Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼. Ğ”Ğ¾Ğ»Ğ³Ğ¾ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑÑ‚ÑÑ Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½ĞµĞ²Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ğ¼Ğ¾Ğ·Ğ³-ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€."
                },
                "en": {
                    "title": "Unlocking Speech with Extended Neural Context",
                    "desc": "MEG-XL is a machine learning model that enhances brain-to-text decoding by using a longer context of 2.5 minutes of MEG data for pre-training. This approach allows the model to learn better statistical patterns across different subjects, which is crucial for decoding natural speech that occurs over extended periods. By fine-tuning on word decoding tasks, MEG-XL achieves performance comparable to models trained on much larger datasets, demonstrating its efficiency. The findings suggest that utilizing longer contexts during pre-training significantly improves the model's ability to generalize and transfer knowledge to specific tasks."
                },
                "zh": {
                    "title": "é•¿ä¸Šä¸‹æ–‡é¢„è®­ç»ƒï¼Œæå‡è„‘-æ–‡æœ¬è§£ç æ€§èƒ½",
                    "desc": "MEG-XLæ˜¯ä¸€ç§æ”¹è¿›çš„è„‘-æ–‡æœ¬è§£ç æ¨¡å‹ï¼Œé€šè¿‡ä½¿ç”¨2.5åˆ†é’Ÿçš„MEGä¸Šä¸‹æ–‡è¿›è¡Œæ‰©å±•é¢„è®­ç»ƒï¼Œæ˜¾è‘—æé«˜äº†è§£ç æ€§èƒ½ã€‚ä¸ä¹‹å‰åªä½¿ç”¨å‡ ç§’ä¸Šä¸‹æ–‡çš„æ¨¡å‹ç›¸æ¯”ï¼ŒMEG-XLèƒ½å¤Ÿæ•æ‰æ›´é•¿çš„ç¥ç»ä¸Šä¸‹æ–‡ï¼Œä»è€Œæé«˜æ•°æ®æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨è„‘æ•°æ®çš„å•è¯è§£ç ä»»åŠ¡ä¸Šï¼Œä½¿ç”¨æ›´å°‘çš„æ•°æ®ï¼ˆä¾‹å¦‚1å°æ—¶å¯¹æ¯”50å°æ—¶ï¼‰è¾¾åˆ°äº†ä¸ç›‘ç£å­¦ä¹ ç›¸å½“çš„æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé•¿ä¸Šä¸‹æ–‡çš„é¢„è®­ç»ƒæœ‰åŠ©äºæ›´å¥½åœ°åˆ©ç”¨ç¥ç»ä¸Šä¸‹æ–‡ï¼Œä»è€Œæå‡è§£ç æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02405",
            "title": "Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning",
            "url": "https://huggingface.co/papers/2602.02405",
            "abstract": "Distribution Aligned Imitation Learning (DAIL) improves LLM reasoning by transforming expert solutions into in-distribution traces and using contrastive learning to focus on expert methodologies, achieving significant performance gains with minimal expert data.  \t\t\t\t\tAI-generated summary \t\t\t\t Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization.",
            "score": 1,
            "issue_id": 903,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "cb88331c5eda8f68",
            "authors": [
                "Ethan Mendes",
                "Jungsoo Park",
                "Alan Ritter"
            ],
            "affiliations": [
                "Georgia Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02405.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#training",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Distribution Aligned Imitation Learning (DAIL), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ÑÑÑ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾ÑĞºĞ¾Ğ»ÑŒĞºÑƒ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹, Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑÑ… Ğ¸ Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DAIL Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ (10-25% Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ°), Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² 2-4 Ñ€Ğ°Ğ·Ğ° Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ· Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµĞ½ĞµĞµ 1000 Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Bridging the Gap: Expert Insights for Better AI Reasoning",
                    "desc": "Distribution Aligned Imitation Learning (DAIL) enhances the reasoning abilities of large language models (LLMs) by converting expert solutions into in-distribution reasoning traces. This method uses contrastive learning to emphasize expert techniques, allowing the model to learn effectively from limited expert data. DAIL addresses the challenge of traditional imitation learning, which often fails due to the out-of-distribution nature of expert solutions. The approach demonstrates significant performance improvements, achieving up to 25% gains in model accuracy while improving reasoning efficiency and enabling better generalization to new tasks."
                },
                "zh": {
                    "title": "åˆ†å¸ƒå¯¹é½ï¼Œæå‡æ¨ç†èƒ½åŠ›ï¼",
                    "desc": "åˆ†å¸ƒå¯¹é½æ¨¡ä»¿å­¦ä¹ ï¼ˆDAILï¼‰é€šè¿‡å°†ä¸“å®¶è§£å†³æ–¹æ¡ˆè½¬åŒ–ä¸ºç¬¦åˆåˆ†å¸ƒçš„æ¨ç†è½¨è¿¹ï¼Œæ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¯¹æ¯”å­¦ä¹ ï¼Œä¸“æ³¨äºä¸“å®¶çš„æ–¹æ³•è®ºï¼Œä»è€Œåœ¨ä½¿ç”¨æœ€å°‘çš„ä¸“å®¶æ•°æ®æ—¶å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚DAILçš„ä¸¤æ­¥æ³•é¦–å…ˆå°†ä¸“å®¶è§£å†³æ–¹æ¡ˆè½¬åŒ–ä¸ºè¯¦ç»†çš„æ¨ç†è½¨è¿¹ï¼Œç„¶ååº”ç”¨å¯¹æ¯”ç›®æ ‡æ¥å¼ºåŒ–å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒDAILèƒ½å¤Ÿåˆ©ç”¨å°‘äº1000ä¸ªé«˜è´¨é‡çš„ä¸“å®¶è§£å†³æ–¹æ¡ˆï¼Œåœ¨Qwen2.5-Instructå’ŒQwen3æ¨¡å‹ä¸Šå®ç°10-25%çš„æ€§èƒ½æå‡ï¼Œå¹¶æé«˜æ¨ç†æ•ˆç‡2åˆ°4å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02220",
            "title": "LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation",
            "url": "https://huggingface.co/papers/2602.02220",
            "abstract": "HieraNav presents a multi-granularity, open-vocabulary navigation task with LangMap benchmark that enables agents to follow natural language instructions across different semantic levels in 3D environments.  \t\t\t\t\tAI-generated summary \t\t\t\t The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: scene, room, region, and instance. To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation. Project: https://bo-miao.github.io/LangMap",
            "score": 1,
            "issue_id": 901,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "1528f5b2c41137b4",
            "authors": [
                "Bo Miao",
                "Weijia Liu",
                "Jun Luo",
                "Lachlan Shinnick",
                "Jian Liu",
                "Thomas Hamilton-Smith",
                "Yuhe Yang",
                "Zijie Wu",
                "Vanja Videnovic",
                "Feras Dayoub",
                "Anton van den Hengel"
            ],
            "affiliations": [
                "AIML, Adelaide University",
                "Breaker Industries",
                "East China Normal University",
                "NERC-RVC, Hunan University",
                "Singapore University of Technology and Design",
                "University Western Australia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02220.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#multimodal",
                    "#benchmark",
                    "#dataset",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "ğŸ—ºï¸",
                "ru": {
                    "title": "Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ ÑĞ·Ñ‹ĞºÑƒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… 3D-ÑÑ†ĞµĞ½Ğ°Ñ…",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° HieraNav Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² 3D-Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ…: ÑÑ†ĞµĞ½Ğ°, ĞºĞ¾Ğ¼Ğ½Ğ°Ñ‚Ğ°, Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½ Ğ¸ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ LangMap Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑĞºĞ°Ğ½Ğ¾Ğ² Ğ¿Ğ¾Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ 414 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 18K Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ¸Ğ»ÑÑ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ¼Ğ°Ğ»Ñ‹Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼, Ğ´Ğ°Ğ»ÑŒĞ½Ğ¸Ğ¼ Ñ†ĞµĞ»ÑĞ¼ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Navigating Language: HieraNav and LangMap for AI Agents",
                    "desc": "HieraNav introduces a new navigation task that allows AI agents to understand and follow natural language instructions at different levels of detail in 3D spaces. The LangMap benchmark provides a rich dataset with various semantic levels, including scene, room, region, and instance, enabling comprehensive evaluation of navigation tasks. It features high-quality annotations for over 414 object categories and includes more than 18,000 tasks, enhancing the agents' ability to interpret instructions. The study shows that while context and memory improve navigation success, challenges remain with complex goals and multi-goal scenarios."
                },
                "zh": {
                    "title": "HieraNavï¼šå¤šç²’åº¦è‡ªç„¶è¯­è¨€å¯¼èˆªçš„æœªæ¥",
                    "desc": "HieraNavæ˜¯ä¸€ä¸ªå¤šç²’åº¦ã€å¼€æ”¾è¯æ±‡çš„å¯¼èˆªä»»åŠ¡ï¼Œæ—¨åœ¨è®©æ™ºèƒ½ä½“æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤åœ¨3Dç¯å¢ƒä¸­è¿›è¡Œå¯¼èˆªã€‚è¯¥ä»»åŠ¡é€šè¿‡LangMapåŸºå‡†æµ‹è¯•å®ç°ï¼Œæ¶µç›–äº†åœºæ™¯ã€æˆ¿é—´ã€åŒºåŸŸå’Œå®ä¾‹å››ä¸ªè¯­ä¹‰å±‚æ¬¡ã€‚LangMapæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†ï¼ŒåŸºäºçœŸå®çš„3Då®¤å†…æ‰«æï¼Œæä¾›äº†ä¸°å¯Œçš„äººç±»éªŒè¯æ³¨é‡Šå’Œä»»åŠ¡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¸°å¯Œçš„ä¸Šä¸‹æ–‡å’Œè®°å¿†å¯ä»¥æé«˜å¯¼èˆªæˆåŠŸç‡ï¼Œä½†åœ¨å¤„ç†é•¿å°¾ã€å°å‹ã€ä¾èµ–ä¸Šä¸‹æ–‡å’Œè¿œç¨‹ç›®æ ‡æ—¶ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.01405",
            "title": "Feedback by Design: Understanding and Overcoming User Feedback Barriers in Conversational Agents",
            "url": "https://huggingface.co/papers/2602.01405",
            "abstract": "High-quality feedback is essential for effective human-AI interaction. It bridges knowledge gaps, corrects digressions, and shapes system behavior; both during interaction and throughout model development. Yet despite its importance, human feedback to AI is often infrequent and low quality. This gap motivates a critical examination of human feedback during interactions with AIs. To understand and overcome the challenges preventing users from giving high-quality feedback, we conducted two studies examining feedback dynamics between humans and conversational agents (CAs). Our formative study, through the lens of Grice's maxims, identified four Feedback Barriers -- Common Ground, Verifiability, Communication, and Informativeness -- that prevent high-quality feedback by users. Building on these findings, we derive three design desiderata and show that systems incorporating scaffolds aligned with these desiderata enabled users to provide higher-quality feedback. Finally, we detail a call for action to the broader AI community for advances in Large Language Models capabilities to overcome Feedback Barriers.",
            "score": 1,
            "issue_id": 905,
            "pub_date": "2026-02-01",
            "pub_date_card": {
                "ru": "1 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 1",
                "zh": "2æœˆ1æ—¥"
            },
            "hash": "149d6974be7e2108",
            "authors": [
                "Nikhil Sharma",
                "Zheng Zhang",
                "Daniel Lee",
                "Namita Krishnan",
                "Guang-Jie Ren",
                "Ziang Xiao",
                "Yunyao Li"
            ],
            "affiliations": [
                "Adobe Inc.",
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.01405.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ AI",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğº ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ´Ğ²Ğ° ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ° Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑÑŒ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ñ… Ğ“Ñ€Ğ°Ğ¹ÑĞ°: Ğ¾Ğ±Ñ‰ĞµĞµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ, ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑÑ‹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ² Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼."
                },
                "en": {
                    "title": "Bridging the Feedback Gap for Better AI Interactions",
                    "desc": "This paper explores the importance of high-quality human feedback in improving interactions with AI systems, particularly conversational agents. It identifies four key barriers that hinder users from providing effective feedback: Common Ground, Verifiability, Communication, and Informativeness. The authors conducted studies to understand these barriers and proposed design principles to help AI systems facilitate better feedback from users. They advocate for advancements in Large Language Models to address these challenges and enhance the overall quality of human-AI interactions."
                },
                "zh": {
                    "title": "æå‡äººæœºäº¤äº’ä¸­çš„åé¦ˆè´¨é‡",
                    "desc": "é«˜è´¨é‡çš„åé¦ˆå¯¹äººæœºäº¤äº’è‡³å…³é‡è¦ï¼Œå®ƒå¯ä»¥å¼¥è¡¥çŸ¥è¯†å·®è·ã€çº æ­£åå·®å¹¶å¡‘é€ ç³»ç»Ÿè¡Œä¸ºã€‚å°½ç®¡åé¦ˆçš„é‡è¦æ€§æ˜¾è€Œæ˜“è§ï¼Œä½†ç”¨æˆ·å¯¹äººå·¥æ™ºèƒ½çš„åé¦ˆå¾€å¾€ä¸å¤Ÿé¢‘ç¹ä¸”è´¨é‡è¾ƒä½ã€‚æœ¬æ–‡é€šè¿‡ä¸¤é¡¹ç ”ç©¶æ¢è®¨äº†äººç±»ä¸å¯¹è¯ä»£ç†ä¹‹é—´çš„åé¦ˆåŠ¨æ€ï¼Œè¯†åˆ«å‡ºå››ä¸ªåé¦ˆéšœç¢ï¼šå…±åŒåŸºç¡€ã€å¯éªŒè¯æ€§ã€æ²Ÿé€šå’Œä¿¡æ¯é‡ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæå‡ºäº†ä¸‰ä¸ªè®¾è®¡è¦æ±‚ï¼Œå¹¶å±•ç¤ºäº†ç¬¦åˆè¿™äº›è¦æ±‚çš„ç³»ç»Ÿå¦‚ä½•å¸®åŠ©ç”¨æˆ·æä¾›æ›´é«˜è´¨é‡çš„åé¦ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.00682",
            "title": "RecGOAT: Graph Optimal Adaptive Transport for LLM-Enhanced Multimodal Recommendation with Dual Semantic Alignment",
            "url": "https://huggingface.co/papers/2602.00682",
            "abstract": "A novel dual semantic alignment framework for LLM-enhanced multimodal recommendation that addresses representational divergence between large models and recommendation systems through graph attention networks and cross-modal contrastive learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal recommendation systems typically integrates user behavior with multimodal data from items, thereby capturing more accurate user preferences. Concurrently, with the rise of large models (LMs), multimodal recommendation is increasingly leveraging their strengths in semantic understanding and contextual reasoning. However, LM representations are inherently optimized for general semantic tasks, while recommendation models rely heavily on sparse user/item unique identity (ID) features. Existing works overlook the fundamental representational divergence between large models and recommendation systems, resulting in incompatible multimodal representations and suboptimal recommendation performance. To bridge this gap, we propose RecGOAT, a novel yet simple dual semantic alignment framework for LLM-enhanced multimodal recommendation, which offers theoretically guaranteed alignment capability. RecGOAT first employs graph attention networks to enrich collaborative semantics by modeling item-item, user-item, and user-user relationships, leveraging user/item LM representations and interaction history. Furthermore, we design a dual-granularity progressive multimodality-ID alignment framework, which achieves instance-level and distribution-level semantic alignment via cross-modal contrastive learning (CMCL) and optimal adaptive transport (OAT), respectively. Theoretically, we demonstrate that the unified representations derived from our alignment framework exhibit superior semantic consistency and comprehensiveness. Extensive experiments on three public benchmarks show that our RecGOAT achieves state-of-the-art performance, empirically validating our theoretical insights. Additionally, the deployment on a large-scale online advertising platform confirms the model's effectiveness and scalability in industrial recommendation scenarios. Code available at https://github.com/6lyc/RecGOAT-LLM4Rec.",
            "score": 1,
            "issue_id": 900,
            "pub_date": "2026-01-31",
            "pub_date_card": {
                "ru": "31 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 31",
                "zh": "1æœˆ31æ—¥"
            },
            "hash": "193acf3b8bfd59bf",
            "authors": [
                "Yuecheng Li",
                "Hengwei Ju",
                "Zeyu Song",
                "Wei Yang",
                "Chi Lu",
                "Peng Jiang",
                "Kun Gai"
            ],
            "affiliations": [
                "Fudan University",
                "Kuaishou Technology",
                "Unaffiliated",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.00682.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ“Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° RecGOAT â€” Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ LLM Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°, Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ ID-Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ². Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ°Ñ ÑÑ…ĞµĞ¼Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ‚Ñ€Ñ‘Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğµ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ñ€ĞµĞºĞ»Ğ°Ğ¼Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing Recommendations with Dual Semantic Alignment",
                    "desc": "This paper introduces RecGOAT, a dual semantic alignment framework designed to improve multimodal recommendation systems by addressing the differences in representation between large language models (LLMs) and traditional recommendation systems. It utilizes graph attention networks to enhance collaborative semantics by modeling relationships among users and items, while also leveraging LLM representations. The framework employs cross-modal contrastive learning and optimal adaptive transport to achieve both instance-level and distribution-level semantic alignment. Experimental results demonstrate that RecGOAT outperforms existing methods, confirming its effectiveness in real-world recommendation scenarios."
                },
                "zh": {
                    "title": "åŒè¯­ä¹‰å¯¹é½ï¼Œæå‡å¤šæ¨¡æ€æ¨èæ•ˆæœ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŒè¯­ä¹‰å¯¹é½æ¡†æ¶RecGOATï¼Œç”¨äºå¢å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ¨¡æ€æ¨èç³»ç»Ÿã€‚è¯¥æ¡†æ¶é€šè¿‡å›¾æ³¨æ„åŠ›ç½‘ç»œå’Œè·¨æ¨¡æ€å¯¹æ¯”å­¦ä¹ ï¼Œè§£å†³äº†å¤§æ¨¡å‹ä¸æ¨èç³»ç»Ÿä¹‹é—´çš„è¡¨ç¤ºå·®å¼‚é—®é¢˜ã€‚RecGOATé€šè¿‡å»ºæ¨¡ç”¨æˆ·ä¸ç‰©å“ä¹‹é—´çš„å…³ç³»ï¼Œä¸°å¯Œäº†åä½œè¯­ä¹‰ï¼Œå¹¶å®ç°äº†å®ä¾‹çº§å’Œåˆ†å¸ƒçº§çš„è¯­ä¹‰å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRecGOATåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†å…¶ç†è®ºæœ‰æ•ˆæ€§å’Œåœ¨å·¥ä¸šæ¨èåœºæ™¯ä¸­çš„å¯æ‰©å±•æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.00398",
            "title": "MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers",
            "url": "https://huggingface.co/papers/2602.00398",
            "abstract": "MemoryLLM decouples feed-forward networks from self-attention in transformers, enabling context-free token-wise neural retrieval memory that improves inference efficiency through pre-computed lookups.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding how transformer components operate in LLMs is important, as it is at the core of recent technological advances in artificial intelligence. In this work, we revisit the challenges associated with interpretability of feed-forward modules (FFNs) and propose MemoryLLM, which aims to decouple FFNs from self-attention and enables us to study the decoupled FFNs as context-free token-wise neural retrieval memory. In detail, we investigate how input tokens access memory locations within FFN parameters and the importance of FFN memory across different downstream tasks. MemoryLLM achieves context-free FFNs by training them in isolation from self-attention directly using the token embeddings. This approach allows FFNs to be pre-computed as token-wise lookups (ToLs), enabling on-demand transfer between VRAM and storage, additionally enhancing inference efficiency. We also introduce Flex-MemoryLLM, positioning it between a conventional transformer design and MemoryLLM. This architecture bridges the performance gap caused by training FFNs with context-free token-wise embeddings.",
            "score": 1,
            "issue_id": 905,
            "pub_date": "2026-01-30",
            "pub_date_card": {
                "ru": "30 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 30",
                "zh": "1æœˆ30æ—¥"
            },
            "hash": "980b87a1189170b8",
            "authors": [
                "Ajay Jaiswal",
                "Lauren Hannah",
                "Han-Byul Kim",
                "Duc Hoang",
                "Arnav Kundu",
                "Mehrdad Farajtabar",
                "Minsik Cho"
            ],
            "affiliations": [
                "Apple"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.00398.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ’¾",
                "ru": {
                    "title": "ĞŸĞ°Ğ¼ÑÑ‚ÑŒ Ğ±ĞµĞ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°: Ğ¾Ñ‚Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ FFN Ğ¾Ñ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ MemoryLLM â€” Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑĞ²ÑĞ·Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ (FFN) Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ‚ÑŒ FFN ĞºĞ°Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ Ğ¿Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾ÑĞºĞ¾Ğ»ÑŒĞºÑƒ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ, ĞºĞ°Ğº Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² FFN. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼ (ToLs) Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Flex-MemoryLLM â€” Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑ€Ğ°Ğ²Ğ½Ğ¾Ğ²ĞµÑˆĞ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğ¹ MemoryLLM."
                },
                "en": {
                    "title": "Decoupling Memory for Efficient Inference in Transformers",
                    "desc": "MemoryLLM is a novel approach that separates feed-forward networks (FFNs) from self-attention mechanisms in transformers. This separation allows FFNs to function as context-free token-wise neural retrieval memory, which enhances the efficiency of inference through pre-computed lookups. The study explores how input tokens can access specific memory locations within FFN parameters, highlighting the significance of FFN memory for various downstream tasks. Additionally, the introduction of Flex-MemoryLLM offers a middle ground between traditional transformer designs and MemoryLLM, addressing performance issues related to context-free embeddings."
                },
                "zh": {
                    "title": "è§£è€¦å‰é¦ˆç½‘ç»œï¼Œæå‡æ¨ç†æ•ˆç‡",
                    "desc": "MemoryLLMé€šè¿‡å°†å‰é¦ˆç½‘ç»œä¸è‡ªæ³¨æ„åŠ›è§£è€¦ï¼Œæä¾›äº†ä¸€ç§æ— ä¸Šä¸‹æ–‡çš„åŸºäºä»¤ç‰Œçš„ç¥ç»æ£€ç´¢è®°å¿†ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—å‰é¦ˆç½‘ç»œèƒ½å¤Ÿç‹¬ç«‹äºè‡ªæ³¨æ„åŠ›è¿›è¡Œè®­ç»ƒï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå‰é¦ˆç½‘ç»œåœ¨ä¸åŒä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è®°å¿†é‡è¦æ€§ï¼Œä»¥åŠè¾“å…¥ä»¤ç‰Œå¦‚ä½•è®¿é—®å‰é¦ˆç½‘ç»œå‚æ•°ä¸­çš„è®°å¿†ä½ç½®ã€‚Flex-MemoryLLMæ¶æ„åˆ™åœ¨ä¼ ç»Ÿå˜æ¢å™¨è®¾è®¡ä¸MemoryLLMä¹‹é—´æ¶èµ·äº†æ¡¥æ¢ï¼Œç¼©å°äº†å› ä½¿ç”¨æ— ä¸Šä¸‹æ–‡ä»¤ç‰ŒåµŒå…¥è€Œé€ æˆçš„æ€§èƒ½å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03817",
            "title": "Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion",
            "url": "https://huggingface.co/papers/2602.03817",
            "abstract": "A fusion framework called FINCH combines audio and spatiotemporal predictors for bioacoustic classification by adaptively weighting evidence based on reliability estimates, outperforming fixed-weight methods and audio-only approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Many machine learning systems have access to multiple sources of evidence for the same prediction target, yet these sources often differ in reliability and informativeness across inputs. In bioacoustic classification, species identity may be inferred both from the acoustic signal and from spatiotemporal context such as location and season; while Bayesian inference motivates multiplicative evidence combination, in practice we typically only have access to discriminative predictors rather than calibrated generative models. We introduce Fusion under INdependent Conditional Hypotheses (FINCH), an adaptive log-linear evidence fusion framework that integrates a pre-trained audio classifier with a structured spatiotemporal predictor. FINCH learns a per-sample gating function that estimates the reliability of contextual information from uncertainty and informativeness statistics. The resulting fusion family contains the audio-only classifier as a special case and explicitly bounds the influence of contextual evidence, yielding a risk-contained hypothesis class with an interpretable audio-only fallback. Across benchmarks, FINCH consistently outperforms fixed-weight fusion and audio-only baselines, improving robustness and error trade-offs even when contextual information is weak in isolation. We achieve state-of-the-art performance on CBI and competitive or improved performance on several subsets of BirdSet using a lightweight, interpretable, evidence-based approach. Code is available: \\href{https://anonymous.4open.science/r/birdnoise-85CD/README.md{anonymous-repository}}",
            "score": 0,
            "issue_id": 892,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "7300014e4792982e",
            "authors": [
                "Oscar Ovanger",
                "Levi Harris",
                "Timothy H. Keitt"
            ],
            "affiliations": [
                "1",
                "2"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03817.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ¦",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ·Ğ²ÑƒĞºĞ° Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ FINCH â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¸Ğ¾Ğ°ĞºÑƒÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¼. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²ĞµĞ½Ñ‚Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ° Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ log-Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµÑĞ¾Ğ² Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ€ĞµĞ·ĞµÑ€Ğ²Ğ½Ñ‹Ğ¼ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FINCH Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ²ĞµÑĞ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ñ‡Ğ¸ÑÑ‚Ñ‹Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… CBI Ğ¸ BirdSet."
                },
                "en": {
                    "title": "Adaptive Fusion for Enhanced Bioacoustic Classification",
                    "desc": "The paper presents FINCH, a novel framework for bioacoustic classification that combines audio data with spatiotemporal predictors. It adaptively weighs the evidence from these sources based on their reliability, improving upon traditional fixed-weight methods. By using a gating function that assesses the uncertainty and informativeness of contextual information, FINCH enhances the robustness of predictions. The framework not only outperforms audio-only classifiers but also provides a clear fallback option, making it interpretable and effective even when contextual data is weak."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”è¯æ®èåˆï¼Œæå‡ç”Ÿç‰©å£°å­¦åˆ†ç±»æ€§èƒ½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºFINCHçš„èåˆæ¡†æ¶ï¼Œç”¨äºç”Ÿç‰©å£°å­¦åˆ†ç±»ã€‚è¯¥æ¡†æ¶ç»“åˆäº†éŸ³é¢‘å’Œæ—¶ç©ºé¢„æµ‹å™¨ï¼Œé€šè¿‡æ ¹æ®å¯é æ€§ä¼°è®¡è‡ªé€‚åº”åŠ æƒè¯æ®ï¼Œä»è€Œè¶…è¶Šäº†å›ºå®šæƒé‡æ–¹æ³•å’Œä»…ä½¿ç”¨éŸ³é¢‘çš„æ–¹æ³•ã€‚FINCHèƒ½å¤Ÿæ ¹æ®ä¸ç¡®å®šæ€§å’Œä¿¡æ¯é‡ç»Ÿè®¡æ¥å­¦ä¹ æ¯ä¸ªæ ·æœ¬çš„é—¨æ§å‡½æ•°ï¼Œè¯„ä¼°ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å¯é æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFINCHåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæå‡äº†æ¨¡å‹çš„é²æ£’æ€§å’Œé”™è¯¯æƒè¡¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03320",
            "title": "MedSAM-Agent: Empowering Interactive Medical Image Segmentation with Multi-turn Agentic Reinforcement Learning",
            "url": "https://huggingface.co/papers/2602.03320",
            "abstract": "MedSAM-Agent reformulates medical image segmentation as a multi-step decision-making process using hybrid prompting and a two-stage training pipeline with process rewards to improve autonomous reasoning and optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Medical image segmentation is evolving from task-specific models toward generalizable frameworks. Recent research leverages Multi-modal Large Language Models (MLLMs) as autonomous agents, employing reinforcement learning with verifiable reward (RLVR) to orchestrate specialized tools like the Segment Anything Model (SAM). However, these approaches often rely on single-turn, rigid interaction strategies and lack process-level supervision during training, which hinders their ability to fully exploit the dynamic potential of interactive tools and leads to redundant actions. To bridge this gap, we propose MedSAM-Agent, a framework that reformulates interactive segmentation as a multi-step autonomous decision-making process. First, we introduce a hybrid prompting strategy for expert-curated trajectory generation, enabling the model to internalize human-like decision heuristics and adaptive refinement strategies. Furthermore, we develop a two-stage training pipeline that integrates multi-turn, end-to-end outcome verification with a clinical-fidelity process reward design to promote interaction parsimony and decision efficiency. Extensive experiments across 6 medical modalities and 21 datasets demonstrate that MedSAM-Agent achieves state-of-the-art performance, effectively unifying autonomous medical reasoning with robust, iterative optimization. Code is available https://github.com/CUHK-AIM-Group/MedSAM-Agent{here}.",
            "score": 0,
            "issue_id": 901,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "51c688c84d1de0db",
            "authors": [
                "Shengyuan Liu",
                "Liuxin Bao",
                "Qi Yang",
                "Wanting Geng",
                "Boyun Zheng",
                "Chenxin Li",
                "Wenting Chen",
                "Houwen Peng",
                "Yixuan Yuan"
            ],
            "affiliations": [
                "Chinese University of Hong Kong, Hong Kong SAR, China",
                "Dalian University of Technology, Dalian, China",
                "Hunyuan Group, Tencent",
                "Institute of Automation, the Chinese Academy of Sciences, Beijing, China",
                "Stanford University, Stanford, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03320.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#healthcare",
                    "#rl",
                    "#cv",
                    "#rlhf",
                    "#science",
                    "#training",
                    "#agents",
                    "#reasoning",
                    "#open_source"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "MedSAM-Agent Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ ÑĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° 21 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 6 Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing Medical Image Segmentation with Autonomous Decision-Making",
                    "desc": "MedSAM-Agent transforms medical image segmentation into a multi-step decision-making process, enhancing the model's ability to reason and optimize autonomously. It utilizes a hybrid prompting strategy to generate expert-curated trajectories, allowing the model to learn human-like decision-making and refinement techniques. The framework features a two-stage training pipeline that combines multi-turn outcome verification with a clinical-fidelity process reward system, improving interaction efficiency and reducing redundant actions. Extensive testing across various medical modalities shows that MedSAM-Agent achieves leading performance, effectively integrating autonomous reasoning with iterative optimization."
                },
                "zh": {
                    "title": "MedSAM-Agentï¼šåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–°æ€è·¯",
                    "desc": "MedSAM-Agentå°†åŒ»å­¦å›¾åƒåˆ†å‰²é‡æ–°å®šä¹‰ä¸ºä¸€ä¸ªå¤šæ­¥éª¤çš„å†³ç­–è¿‡ç¨‹ï¼Œåˆ©ç”¨æ··åˆæç¤ºå’Œä¸¤é˜¶æ®µè®­ç»ƒç®¡é“æ¥æé«˜è‡ªä¸»æ¨ç†å’Œä¼˜åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥ä¸“å®¶ç­–åˆ’çš„è½¨è¿¹ç”Ÿæˆç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå†…åŒ–ç±»ä¼¼äººç±»çš„å†³ç­–å¯å‘å¼å’Œè‡ªé€‚åº”ä¼˜åŒ–ç­–ç•¥ã€‚å®ƒè¿˜ç»“åˆäº†å¤šè½®ã€ç«¯åˆ°ç«¯çš„ç»“æœéªŒè¯ä¸ä¸´åºŠä¿çœŸåº¦çš„è¿‡ç¨‹å¥–åŠ±è®¾è®¡ï¼Œä»¥ä¿ƒè¿›äº¤äº’çš„ç®€çº¦æ€§å’Œå†³ç­–æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMedSAM-Agentåœ¨å…­ç§åŒ»å­¦æ¨¡æ€å’Œ21ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæœ‰æ•ˆåœ°å°†è‡ªä¸»åŒ»å­¦æ¨ç†ä¸å¼ºå¤§çš„è¿­ä»£ä¼˜åŒ–ç›¸ç»“åˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.01519",
            "title": "You Need an Encoder for Native Position-Independent Caching",
            "url": "https://huggingface.co/papers/2602.01519",
            "abstract": "Native position-independent caching enhances LLM inference efficiency by reintroducing encoders and developing a caching system that reduces latency while maintaining accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t The Key-Value (KV) cache of Large Language Models (LLMs) is prefix-based, making it highly inefficient for processing contexts retrieved in arbitrary order. Position-Independent Caching (PIC) has been proposed to enable KV reuse without positional constraints; however, existing approaches often incur substantial accuracy degradation, limiting their practical adoption. To address this issue, we propose native PIC by reintroducing the encoder to prevalent decoder-only LLMs and explicitly training it to support PIC. We further develop COMB, a PIC-aware caching system that integrates seamlessly with existing inference frameworks. Experimental results show that COMB reduces Time-to-First-Token (TTFT) by 51-94% and increases throughput by 3times with comparable accuracy. Furthermore, the quality improvement when using DeepSeek-V2-Lite-Chat demonstrates the applicability of COMB to other types of decoder-only LLMs. Our code is available at https://github.com/shijuzhao/Comb.",
            "score": 0,
            "issue_id": 901,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "70f393ac8a1a2604",
            "authors": [
                "Shiju Zhao",
                "Junhao Hu",
                "Jiaqi Zheng",
                "Guihai Chen"
            ],
            "affiliations": [
                "School of Computer Science, Peking University, China",
                "State Key Laboratory for Novel Software Technology, Nanjing University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.01519.jpg",
            "data": {
                "categories": [],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞŸĞ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğµ ĞºĞµÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ³Ğ¾ ĞºĞµÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (PIC) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Key-Value ĞºĞµÑˆ Ğ±ĞµĞ· ÑƒÑ‡Ñ‘Ñ‚Ğ° Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ÑÑ‚ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€-only Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ ÑĞ²Ğ½Ñ‹Ğ¼ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ ĞµĞ³Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ PIC. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ COMB, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ°Ğ¼Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾ ÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ½Ğ° 51-94% Ğ¸ Ñ‚Ñ€Ñ‘Ñ…ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Boosting LLM Efficiency with Native Position-Independent Caching",
                    "desc": "This paper presents a novel approach to improve the efficiency of Large Language Models (LLMs) during inference by implementing native position-independent caching (PIC). The authors reintroduce encoders into decoder-only LLMs, allowing for key-value (KV) cache reuse without being limited by positional constraints. They develop a new caching system called COMB, which integrates with existing frameworks and significantly reduces latency while maintaining accuracy. Experimental results indicate that COMB can decrease Time-to-First-Token (TTFT) by up to 94% and triple throughput, demonstrating its effectiveness across various decoder-only LLMs."
                },
                "zh": {
                    "title": "æå‡LLMæ¨ç†æ•ˆç‡çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æœ¬åœ°ä½ç½®æ— å…³ç¼“å­˜ï¼ˆPICï¼‰æ–¹æ³•ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ•ˆç‡ã€‚é€šè¿‡é‡æ–°å¼•å…¥ç¼–ç å™¨å¹¶å¼€å‘ä¸€ä¸ªç¼“å­˜ç³»ç»Ÿï¼Œå‡å°‘äº†å»¶è¿Ÿï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚ç°æœ‰çš„åŸºäºå‰ç¼€çš„é”®å€¼ç¼“å­˜æ•ˆç‡ä½ä¸‹ï¼Œè€Œæœ¬ç ”ç©¶çš„COMBç³»ç»Ÿèƒ½å¤Ÿåœ¨ä¸å—ä½ç½®é™åˆ¶çš„æƒ…å†µä¸‹é‡ç”¨ç¼“å­˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCOMBåœ¨ä¿æŒç›¸ä¼¼å‡†ç¡®åº¦çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†é¦–æ¬¡ç”Ÿæˆæ—¶é—´ï¼Œå¹¶æé«˜äº†ååé‡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-02-03.html",
    "link_next": "2026-02-05.html",
    "link_month": "2026-02.html",
    "short_date_prev": {
        "ru": "03.02",
        "en": "02/03",
        "zh": "2æœˆ3æ—¥"
    },
    "short_date_next": {
        "ru": "05.02",
        "en": "02/05",
        "zh": "2æœˆ5æ—¥"
    },
    "categories": {
        "#dataset": 6,
        "#data": 2,
        "#benchmark": 20,
        "#agents": 15,
        "#cv": 8,
        "#rl": 11,
        "#rlhf": 4,
        "#rag": 1,
        "#plp": 5,
        "#inference": 6,
        "#3d": 2,
        "#audio": 1,
        "#video": 4,
        "#multimodal": 14,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 7,
        "#healthcare": 2,
        "#training": 22,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 3,
        "#reasoning": 14,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 2,
        "#optimization": 15,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 5,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 10,
        "#small_models": 1,
        "#science": 4,
        "#low_resource": 1
    }
}