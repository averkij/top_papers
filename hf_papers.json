{
    "date": {
        "ru": "19 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 19",
        "zh": "11æœˆ19æ—¥"
    },
    "time_utc": "2024-11-19 06:14",
    "weekday": 1,
    "issue_id": 654,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.10640",
            "title": "BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices",
            "url": "https://huggingface.co/papers/2411.10640",
            "abstract": "The emergence and growing popularity of multimodal large language models (MLLMs) have significant potential to enhance various aspects of daily life, from improving communication to facilitating learning and problem-solving. Mobile phones, as essential daily companions, represent the most effective and accessible deployment platform for MLLMs, enabling seamless integration into everyday tasks. However, deploying MLLMs on mobile phones presents challenges due to limitations in memory size and computational capability, making it difficult to achieve smooth and real-time processing without extensive optimization. In this paper, we present BlueLM-V-3B, an algorithm and system co-design approach specifically tailored for the efficient deployment of MLLMs on mobile platforms. To be specific, we redesign the dynamic resolution scheme adopted by mainstream MLLMs and implement system optimization for hardware-aware deployment to optimize model inference on mobile phones. BlueLM-V-3B boasts the following key highlights: (1) Small Size: BlueLM-V-3B features a language model with 2.7B parameters and a vision encoder with 400M parameters. (2) Fast Speed: BlueLM-V-3B achieves a generation speed of 24.4 token/s on the MediaTek Dimensity 9300 processor with 4-bit LLM weight quantization. (3) Strong Performance: BlueLM-V-3B has attained the highest average score of 66.1 on the OpenCompass benchmark among models with leq 4B parameters and surpassed a series of models with much larger parameter sizes (e.g., MiniCPM-V-2.6, InternVL2-8B).",
            "score": 10,
            "issue_id": 652,
            "pub_date": "2024-11-16",
            "pub_date_card": {
                "ru": "16 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 16",
                "zh": "11æœˆ16æ—¥"
            },
            "hash": "0366549d4347dfd2",
            "authors": [
                "Xudong Lu",
                "Yinghao Chen",
                "Cheng Chen",
                "Hui Tan",
                "Boheng Chen",
                "Yina Xie",
                "Rui Hu",
                "Guanxin Tan",
                "Renshou Wu",
                "Yan Hu",
                "Yi Zeng",
                "Lei Wu",
                "Liuyang Bian",
                "Zhaoxiong Wang",
                "Long Liu",
                "Yanzhou Yang",
                "Han Xiao",
                "Aojun Zhou",
                "Yafei Wen",
                "Xiaoxin Chen",
                "Shuai Ren",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CUHK MMLab",
                "vivo AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.10640.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#architecture",
                    "#small_models",
                    "#multimodal",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "BlueLM-V-3B: ĞœĞ¾Ñ‰ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ²Ğ°ÑˆĞµĞ¼ ĞºĞ°Ñ€Ğ¼Ğ°Ğ½Ğµ",
                    "desc": "BlueLM-V-3B - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½Ğ¾Ğ², Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ¼ĞµĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ (3,1 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²), Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ (24,4 Ñ‚Ğ¾ĞºĞµĞ½Ğ°/Ñ) Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. BlueLM-V-3B Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ˜Ğ˜-Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹ Ğ² Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½ÑƒÑ Ğ¶Ğ¸Ğ·Ğ½ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Efficient MLLMs for Mobile: BlueLM-V-3B Unleashed!",
                    "desc": "This paper introduces BlueLM-V-3B, a multimodal large language model (MLLM) designed for efficient deployment on mobile devices. It addresses the challenges of limited memory and computational power by optimizing model inference through a redesigned dynamic resolution scheme and hardware-aware system optimizations. BlueLM-V-3B features a compact architecture with 2.7 billion parameters for the language model and 400 million for the vision encoder, ensuring fast processing speeds. The model achieves impressive performance metrics, outperforming larger models on benchmarks while maintaining a generation speed of 24.4 tokens per second."
                },
                "zh": {
                    "title": "é«˜æ•ˆéƒ¨ç½²å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„åˆ›æ–°æ–¹æ¡ˆ",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºBlueLM-V-3Bçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œæ—¨åœ¨é«˜æ•ˆåœ°åœ¨ç§»åŠ¨å¹³å°ä¸Šéƒ¨ç½²ã€‚è¯¥æ¨¡å‹å…·æœ‰2.7äº¿å‚æ•°çš„è¯­è¨€æ¨¡å‹å’Œ4äº¿å‚æ•°çš„è§†è§‰ç¼–ç å™¨ï¼Œèƒ½å¤Ÿåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šå®ç°å¿«é€Ÿç”Ÿæˆã€‚é€šè¿‡é‡æ–°è®¾è®¡åŠ¨æ€åˆ†è¾¨ç‡æ–¹æ¡ˆå’Œè¿›è¡Œç¡¬ä»¶ä¼˜åŒ–ï¼ŒBlueLM-V-3Båœ¨MediaTek Dimensity 9300å¤„ç†å™¨ä¸Šè¾¾åˆ°äº†æ¯ç§’24.4ä¸ªæ ‡è®°çš„ç”Ÿæˆé€Ÿåº¦ã€‚è¯¥æ¨¡å‹åœ¨OpenCompassåŸºå‡†æµ‹è¯•ä¸­è·å¾—äº†66.1çš„æœ€é«˜å¹³å‡åˆ†ï¼Œè¶…è¶Šäº†è®¸å¤šå‚æ•°æ›´å¤§çš„æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.11504",
            "title": "Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering",
            "url": "https://huggingface.co/papers/2411.11504",
            "abstract": "The evolution of machine learning has increasingly prioritized the development of powerful models and more scalable supervision signals. However, the emergence of foundation models presents significant challenges in providing effective supervision signals necessary for further enhancing their capabilities. Consequently, there is an urgent need to explore novel supervision signals and technical approaches. In this paper, we propose verifier engineering, a novel post-training paradigm specifically designed for the era of foundation models. The core of verifier engineering involves leveraging a suite of automated verifiers to perform verification tasks and deliver meaningful feedback to foundation models. We systematically categorize the verifier engineering process into three essential stages: search, verify, and feedback, and provide a comprehensive review of state-of-the-art research developments within each stage. We believe that verifier engineering constitutes a fundamental pathway toward achieving Artificial General Intelligence.",
            "score": 3,
            "issue_id": 654,
            "pub_date": "2024-11-18",
            "pub_date_card": {
                "ru": "18 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 18",
                "zh": "11æœˆ18æ—¥"
            },
            "hash": "a48ecb5e8a1da0ae",
            "authors": [
                "Xinyan Guan",
                "Yanjiang Liu",
                "Xinyu Lu",
                "Boxi Cao",
                "Ben He",
                "Xianpei Han",
                "Le Sun",
                "Jie Lou",
                "Bowen Yu",
                "Yaojie Lu",
                "Hongyu Lin"
            ],
            "affiliations": [
                "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.11504.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#survey",
                    "#training",
                    "#agi"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ’ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ 'Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸' - Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½ Ğ½Ğ° Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¿Ğ¾Ğ¸ÑĞº, Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑÑ‡Ğ¸Ñ‚Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑ‚Ğ°Ñ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ½Ğ° Ğ¿ÑƒÑ‚Ğ¸ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "Unlocking Foundation Models with Verifier Engineering",
                    "desc": "This paper discusses the challenges of providing effective supervision signals for foundation models in machine learning. It introduces a new approach called verifier engineering, which uses automated verifiers to enhance the capabilities of these models. The process is divided into three stages: search, verify, and feedback, each aimed at improving model performance. The authors argue that this method is crucial for advancing towards Artificial General Intelligence."
                },
                "zh": {
                    "title": "éªŒè¯å™¨å·¥ç¨‹ï¼šè¿ˆå‘äººå·¥é€šç”¨æ™ºèƒ½çš„æ–°è·¯å¾„",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åœ¨åŸºç¡€æ¨¡å‹æ—¶ä»£ï¼Œå¦‚ä½•æä¾›æœ‰æ•ˆçš„ç›‘ç£ä¿¡å·ä»¥æå‡æ¨¡å‹èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åè®­ç»ƒèŒƒå¼â€”â€”éªŒè¯å™¨å·¥ç¨‹ï¼Œæ—¨åœ¨åˆ©ç”¨è‡ªåŠ¨åŒ–éªŒè¯å™¨è¿›è¡ŒéªŒè¯ä»»åŠ¡ï¼Œå¹¶ä¸ºåŸºç¡€æ¨¡å‹æä¾›æœ‰æ„ä¹‰çš„åé¦ˆã€‚éªŒè¯å™¨å·¥ç¨‹çš„è¿‡ç¨‹åˆ†ä¸ºä¸‰ä¸ªå…³é”®é˜¶æ®µï¼šæœç´¢ã€éªŒè¯å’Œåé¦ˆï¼Œå¹¶å¯¹æ¯ä¸ªé˜¶æ®µçš„æœ€æ–°ç ”ç©¶è¿›å±•è¿›è¡Œäº†ç³»ç»Ÿæ€§å›é¡¾ã€‚æˆ‘ä»¬è®¤ä¸ºï¼ŒéªŒè¯å™¨å·¥ç¨‹æ˜¯å®ç°äººå·¥é€šç”¨æ™ºèƒ½çš„é‡è¦é€”å¾„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.10669",
            "title": "Awaker2.5-VL: Stably Scaling MLLMs with Parameter-Efficient Mixture of Experts",
            "url": "https://huggingface.co/papers/2411.10669",
            "abstract": "As the research of Multimodal Large Language Models (MLLMs) becomes popular, an advancing MLLM model is typically required to handle various textual and visual tasks (e.g., VQA, Detection, OCR, and ChartQA) simultaneously for real-world applications. However, due to the significant differences in representation and distribution among data from various tasks, simply mixing data of all tasks together leads to the well-known``multi-task conflict\" issue, resulting in performance degradation across various tasks. To address this issue, we propose Awaker2.5-VL, a Mixture of Experts~(MoE) architecture suitable for MLLM, which acquires the multi-task capabilities through multiple sparsely activated experts. To speed up the training and inference of Awaker2.5-VL, each expert in our model is devised as a low-rank adaptation (LoRA) structure. Extensive experiments on multiple latest benchmarks demonstrate the effectiveness of Awaker2.5-VL. The code and model weight are released in our Project Page: https://github.com/MetabrainAGI/Awaker.",
            "score": 3,
            "issue_id": 654,
            "pub_date": "2024-11-16",
            "pub_date_card": {
                "ru": "16 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 16",
                "zh": "11æœˆ16æ—¥"
            },
            "hash": "f1319420ae85759e",
            "authors": [
                "Jinqiang Long",
                "Yanqi Dai",
                "Guoxing Yang",
                "Hongpeng Lin",
                "Nanyi Fei",
                "Yizhao Gao",
                "Zhiwu Lu"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Metabrain AGI Lab, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.10669.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#multimodal",
                    "#benchmark",
                    "#optimization",
                    "#training",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Awaker2.5-VL: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ°Ñ MLLM Ğ±ĞµĞ· ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Awaker2.5-VL - Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM), Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğµ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (MoE). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ (LoRA) Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Awaker2.5-VL Ğ½Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Awaker2.5-VL: Mastering Multimodal Tasks with Expert Precision",
                    "desc": "This paper introduces Awaker2.5-VL, a new model designed to improve the performance of Multimodal Large Language Models (MLLMs) on various tasks like visual question answering and detection. The authors address the 'multi-task conflict' problem, which occurs when different tasks interfere with each other due to their diverse data representations. Awaker2.5-VL uses a Mixture of Experts (MoE) architecture, where only a subset of experts is activated for each task, allowing for better specialization and efficiency. Additionally, the model incorporates low-rank adaptation (LoRA) to enhance training speed and inference performance, showing promising results in extensive experiments."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€ä»»åŠ¡çš„ä¸“å®¶æ··åˆè§£å†³æ–¹æ¡ˆ",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºAwaker2.5-VLçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œæ—¨åœ¨åŒæ—¶å¤„ç†æ–‡æœ¬å’Œè§†è§‰ä»»åŠ¡ï¼Œå¦‚è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ã€æ£€æµ‹ã€å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰å’Œå›¾è¡¨é—®ç­”ï¼ˆChartQAï¼‰ã€‚ä¸ºäº†å…‹æœå¤šä»»åŠ¡å†²çªé—®é¢˜ï¼ŒAwaker2.5-VLé‡‡ç”¨äº†ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„ï¼Œé€šè¿‡å¤šä¸ªç¨€ç–æ¿€æ´»çš„ä¸“å®¶æ¥å®ç°å¤šä»»åŠ¡èƒ½åŠ›ã€‚æ¯ä¸ªä¸“å®¶è¢«è®¾è®¡ä¸ºä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ç»“æ„ï¼Œä»¥åŠ é€Ÿæ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†ã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒAwaker2.5-VLåœ¨å¤šä¸ªæœ€æ–°åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.11767",
            "title": "Drowning in Documents: Consequences of Scaling Reranker Inference",
            "url": "https://huggingface.co/papers/2411.11767",
            "abstract": "Rerankers, typically cross-encoders, are often used to re-score the documents retrieved by cheaper initial IR systems. This is because, though expensive, rerankers are assumed to be more effective. We challenge this assumption by measuring reranker performance for full retrieval, not just re-scoring first-stage retrieval. Our experiments reveal a surprising trend: the best existing rerankers provide diminishing returns when scoring progressively more documents and actually degrade quality beyond a certain limit. In fact, in this setting, rerankers can frequently assign high scores to documents with no lexical or semantic overlap with the query. We hope that our findings will spur future research to improve reranking.",
            "score": 3,
            "issue_id": 652,
            "pub_date": "2024-11-18",
            "pub_date_card": {
                "ru": "18 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 18",
                "zh": "11æœˆ18æ—¥"
            },
            "hash": "3fa06087787bc8d4",
            "authors": [
                "Mathew Jacob",
                "Erik Lindgren",
                "Matei Zaharia",
                "Michael Carbin",
                "Omar Khattab",
                "Andrew Drozdov"
            ],
            "affiliations": [
                "Databricks",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.11767.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#data"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ² Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² (rerankers) Ğ² Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒÑ…ÑƒĞ´ÑˆĞ°ĞµÑ‚ÑÑ. Ğ‘Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ğ³Ğ¾, Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€Ğ¸ÑĞ²Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ±ĞµĞ· Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ»Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ Ğ¿Ğ¾Ğ´ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğµ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ½Ğ°Ğ´ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°."
                },
                "en": {
                    "title": "Rethinking Rerankers: Diminishing Returns in Document Scoring",
                    "desc": "This paper investigates the effectiveness of rerankers, specifically cross-encoders, in the context of information retrieval (IR). Traditionally, rerankers are believed to enhance the quality of document scoring after an initial retrieval phase. However, the authors find that as more documents are scored, the performance of these rerankers diminishes and can even lead to poorer results. The study highlights that rerankers may assign high scores to irrelevant documents, suggesting a need for improved methods in reranking processes."
                },
                "zh": {
                    "title": "é‡æ’åºå™¨çš„æœ‰æ•ˆæ€§éœ€é‡æ–°å®¡è§†",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†é‡æ’åºå™¨ï¼ˆé€šå¸¸æ˜¯äº¤å‰ç¼–ç å™¨ï¼‰åœ¨ä¿¡æ¯æ£€ç´¢ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬é€šè¿‡æµ‹é‡é‡æ’åºå™¨åœ¨å®Œæ•´æ£€ç´¢ä¸­çš„è¡¨ç°ï¼ŒæŒ‘æˆ˜äº†å®ƒä»¬åœ¨åˆæ­¥æ£€ç´¢åé‡æ–°è¯„åˆ†çš„å‡è®¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„é‡æ’åºå™¨åœ¨è¯„åˆ†è¶Šæ¥è¶Šå¤šçš„æ–‡æ¡£æ—¶ï¼Œæ•ˆæœé€æ¸å‡å¼±ï¼Œç”šè‡³åœ¨æŸä¸ªé™åº¦åè´¨é‡ä¸‹é™ã€‚æˆ‘ä»¬çš„å‘ç°å¸Œæœ›èƒ½æ¿€åŠ±æœªæ¥çš„ç ”ç©¶ï¼Œä»¥æ”¹è¿›é‡æ’åºæŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.07641",
            "title": "Top-$nÏƒ$: Not All Logits Are You Need",
            "url": "https://huggingface.co/papers/2411.07641",
            "abstract": "Large language models (LLMs) typically employ greedy decoding or low-temperature sampling for reasoning tasks, reflecting a perceived trade-off between diversity and accuracy. We challenge this convention by introducing top-nsigma, a novel sampling method that operates directly on pre-softmax logits by leveraging a statistical threshold. Our key insight is that logits naturally separate into a Gaussian-distributed noisy region and a distinct informative region, enabling efficient token filtering without complex probability manipulations. Unlike existing methods (e.g., top-p, min-p) that inadvertently include more noise tokens at higher temperatures, top-nsigma maintains a stable sampling space regardless of temperature scaling. We also provide a theoretical analysis of top-nsigma to better understand its behavior. The extensive experimental results across four reasoning-focused datasets demonstrate that our method not only outperforms existing sampling approaches but also surpasses greedy decoding, while maintaining consistent performance even at high temperatures.",
            "score": 3,
            "issue_id": 651,
            "pub_date": "2024-11-12",
            "pub_date_card": {
                "ru": "12 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 12",
                "zh": "11æœˆ12æ—¥"
            },
            "hash": "d3439bf0c336ac57",
            "authors": [
                "Chenxia Tang",
                "Jianchun Liu",
                "Hongli Xu",
                "Liusheng Huang"
            ],
            "affiliations": [
                "School of Computer Science and Technology, University of Science and Technology of China",
                "Suzhou Institute for Advanced Research, University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.07641.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Top-nsigma: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ top-nsigma. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ ÑĞ¾Ñ„Ñ‚Ğ¼Ğ°ĞºÑĞ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Top-nsigma Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¶Ğ°Ğ´Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Top-NSigma: Enhancing Sampling for Better Reasoning in LLMs",
                    "desc": "This paper introduces a new sampling method called top-nsigma for large language models (LLMs) that improves reasoning tasks. Unlike traditional methods that use greedy decoding or low-temperature sampling, top-nsigma filters tokens based on their pre-softmax logits, which are divided into informative and noisy regions. This approach allows for better token selection without the complications of probability adjustments, ensuring a stable sampling process across different temperature settings. Experimental results show that top-nsigma outperforms existing methods and maintains high performance even when using higher temperatures."
                },
                "zh": {
                    "title": "çªç ´ä¼ ç»Ÿï¼Œæå‡æ¨ç†æ€§èƒ½çš„top-nsigmaæ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é‡‡æ ·æ–¹æ³•top-nsigmaï¼Œæ—¨åœ¨æ”¹å–„å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•ç›´æ¥åœ¨é¢„è½¯æœ€å¤§å€¼çš„logitsä¸Šæ“ä½œï¼Œé€šè¿‡ç»Ÿè®¡é˜ˆå€¼æ¥è¿›è¡Œæœ‰æ•ˆçš„ä»¤ç‰Œè¿‡æ»¤ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œlogitså¯ä»¥è‡ªç„¶åœ°åˆ†ä¸ºé«˜æ–¯åˆ†å¸ƒçš„å™ªå£°åŒºåŸŸå’Œä¿¡æ¯ä¸°å¯Œçš„åŒºåŸŸï¼Œä»è€Œé¿å…äº†å¤æ‚çš„æ¦‚ç‡æ“ä½œã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œtop-nsigmaåœ¨å¤šä¸ªæ¨ç†æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„é‡‡æ ·æ–¹æ³•å’Œè´ªå©ªè§£ç ï¼Œä¸”åœ¨é«˜æ¸©åº¦ä¸‹ä»èƒ½ä¿æŒç¨³å®šçš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.10510",
            "title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion Transformers",
            "url": "https://huggingface.co/papers/2411.10510",
            "abstract": "Diffusion Transformers (DiT) have emerged as powerful generative models for various tasks, including image, video, and speech synthesis. However, their inference process remains computationally expensive due to the repeated evaluation of resource-intensive attention and feed-forward modules. To address this, we introduce SmoothCache, a model-agnostic inference acceleration technique for DiT architectures. SmoothCache leverages the observed high similarity between layer outputs across adjacent diffusion timesteps. By analyzing layer-wise representation errors from a small calibration set, SmoothCache adaptively caches and reuses key features during inference. Our experiments demonstrate that SmoothCache achieves 8% to 71% speed up while maintaining or even improving generation quality across diverse modalities. We showcase its effectiveness on DiT-XL for image generation, Open-Sora for text-to-video, and Stable Audio Open for text-to-audio, highlighting its potential to enable real-time applications and broaden the accessibility of powerful DiT models.",
            "score": 1,
            "issue_id": 654,
            "pub_date": "2024-11-15",
            "pub_date_card": {
                "ru": "15 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 15",
                "zh": "11æœˆ15æ—¥"
            },
            "hash": "991f548fec1ec8c9",
            "authors": [
                "Joseph Liu",
                "Joshua Geddes",
                "Ziyu Guo",
                "Haomiao Jiang",
                "Mahesh Kumar Nandwana"
            ],
            "affiliations": [
                "Queens University",
                "Roblox"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.10510.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#multimodal",
                    "#video",
                    "#optimization",
                    "#audio",
                    "#diffusion"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "SmoothCache: Ğ‘Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ñ ÑƒĞ¼Ğ½Ñ‹Ğ¼ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ DiT",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SmoothCache - Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Diffusion Transformers (DiT). SmoothCache Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ»Ğ¾ĞµĞ² Ğ½Ğ° ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ°Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºÑÑˆĞ¸Ñ€ÑƒĞµÑ‚ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾ĞµĞ² Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SmoothCache Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ 8% Ğ´Ğ¾ 71% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "Accelerating Diffusion Transformers with SmoothCache",
                    "desc": "This paper presents SmoothCache, a technique designed to speed up the inference process of Diffusion Transformers (DiT), which are used for generating images, videos, and audio. The traditional inference method is slow because it requires many evaluations of complex attention and feed-forward layers. SmoothCache improves efficiency by caching and reusing similar outputs from adjacent diffusion timesteps, reducing the need for repeated calculations. Experiments show that this method can accelerate inference by 8% to 71% while maintaining or enhancing the quality of the generated content."
                },
                "zh": {
                    "title": "SmoothCacheï¼šåŠ é€Ÿæ‰©æ•£å˜æ¢å™¨çš„æ¨ç†è¿‡ç¨‹",
                    "desc": "æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œå¹¿æ³›åº”ç”¨äºå›¾åƒã€è§†é¢‘å’Œè¯­éŸ³åˆæˆç­‰ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ¨ç†è¿‡ç¨‹è®¡ç®—å¼€é”€è¾ƒå¤§ï¼Œå› ä¸ºéœ€è¦é‡å¤è¯„ä¼°èµ„æºå¯†é›†å‹çš„æ³¨æ„åŠ›å’Œå‰é¦ˆæ¨¡å—ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SmoothCacheï¼Œè¿™æ˜¯ä¸€ç§ä¸æ¨¡å‹æ— å…³çš„æ¨ç†åŠ é€ŸæŠ€æœ¯ï¼Œåˆ©ç”¨ç›¸é‚»æ‰©æ•£æ—¶é—´æ­¥ä¹‹é—´å±‚è¾“å‡ºçš„é«˜åº¦ç›¸ä¼¼æ€§ã€‚é€šè¿‡åˆ†æå°å‹æ ¡å‡†é›†ä¸­çš„å±‚çº§è¡¨ç¤ºè¯¯å·®ï¼ŒSmoothCacheè‡ªé€‚åº”åœ°ç¼“å­˜å’Œé‡ç”¨å…³é”®ç‰¹å¾ï¼Œä»è€Œåœ¨ä¿æŒæˆ–æé«˜ç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†8%åˆ°71%çš„é€Ÿåº¦æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.10499",
            "title": "FitDiT: Advancing the Authentic Garment Details for High-fidelity Virtual Try-on",
            "url": "https://huggingface.co/papers/2411.10499",
            "abstract": "Although image-based virtual try-on has made considerable progress, emerging approaches still encounter challenges in producing high-fidelity and robust fitting images across diverse scenarios. These methods often struggle with issues such as texture-aware maintenance and size-aware fitting, which hinder their overall effectiveness. To address these limitations, we propose a novel garment perception enhancement technique, termed FitDiT, designed for high-fidelity virtual try-on using Diffusion Transformers (DiT) allocating more parameters and attention to high-resolution features. First, to further improve texture-aware maintenance, we introduce a garment texture extractor that incorporates garment priors evolution to fine-tune garment feature, facilitating to better capture rich details such as stripes, patterns, and text. Additionally, we introduce frequency-domain learning by customizing a frequency distance loss to enhance high-frequency garment details. To tackle the size-aware fitting issue, we employ a dilated-relaxed mask strategy that adapts to the correct length of garments, preventing the generation of garments that fill the entire mask area during cross-category try-on. Equipped with the above design, FitDiT surpasses all baselines in both qualitative and quantitative evaluations. It excels in producing well-fitting garments with photorealistic and intricate details, while also achieving competitive inference times of 4.57 seconds for a single 1024x768 image after DiT structure slimming, outperforming existing methods.",
            "score": 1,
            "issue_id": 654,
            "pub_date": "2024-11-15",
            "pub_date_card": {
                "ru": "15 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 15",
                "zh": "11æœˆ15æ—¥"
            },
            "hash": "b142b4be26ef6147",
            "authors": [
                "Boyuan Jiang",
                "Xiaobin Hu",
                "Donghao Luo",
                "Qingdong He",
                "Chengming Xu",
                "Jinlong Peng",
                "Jiangning Zhang",
                "Chengjie Wang",
                "Yunsheng Wu",
                "Yanwei Fu"
            ],
            "affiliations": [
                "Fudan University",
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.10499.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#3d",
                    "#diffusion"
                ],
                "emoji": "ğŸ‘š",
                "ru": {
                    "title": "FitDiT: Ğ’Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ° Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ¸ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ FitDiT, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…. FitDiT ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€Ğ° Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ğ½ĞºĞ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°ÑĞºĞ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "FitDiT: Revolutionizing Virtual Try-On with High-Fidelity Garment Perception",
                    "desc": "This paper presents FitDiT, a new technique for improving virtual try-on systems using Diffusion Transformers. It addresses challenges in producing realistic and well-fitting images by enhancing garment perception through a specialized texture extractor and frequency-domain learning. The method also incorporates a dilated-relaxed mask strategy to ensure garments fit correctly without oversizing. FitDiT demonstrates superior performance in generating high-fidelity images with intricate details while maintaining efficient processing times."
                },
                "zh": {
                    "title": "FitDiTï¼šé«˜ä¿çœŸè™šæ‹Ÿè¯•ç©¿çš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æœè£…æ„ŸçŸ¥å¢å¼ºæŠ€æœ¯ï¼Œç§°ä¸ºFitDiTï¼Œæ—¨åœ¨æé«˜è™šæ‹Ÿè¯•ç©¿çš„é«˜ä¿çœŸåº¦ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰åˆ†é…æ›´å¤šå‚æ•°å’Œæ³¨æ„åŠ›äºé«˜åˆ†è¾¨ç‡ç‰¹å¾ï¼Œä»¥è§£å†³çº¹ç†æ„ŸçŸ¥ç»´æŠ¤å’Œå°ºå¯¸æ„ŸçŸ¥é€‚é…çš„é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†æœè£…çº¹ç†æå–å™¨å’Œé¢‘åŸŸå­¦ä¹ ï¼Œå¢å¼ºäº†æœè£…ç»†èŠ‚çš„æ•æ‰èƒ½åŠ›ï¼Œå¹¶é‡‡ç”¨æ‰©å¼ æ”¾æ¾æ©ç ç­–ç•¥æ¥é€‚åº”æœè£…çš„æ­£ç¡®é•¿åº¦ã€‚FitDiTåœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­å‡è¶…è¶Šäº†æ‰€æœ‰åŸºçº¿ï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰çœŸå®æ„Ÿå’Œå¤æ‚ç»†èŠ‚çš„åˆèº«æœè£…ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-18.html",
    "link_next": "2024-11-20.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "18.11",
        "en": "11/18",
        "zh": "11æœˆ18æ—¥"
    },
    "short_date_next": {
        "ru": "20.11",
        "en": "11/20",
        "zh": "11æœˆ20æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªæ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œç§°ä¸ºLLaVA-o1ï¼Œæ—¨åœ¨è¿›è¡Œè‡ªä¸»å¤šé˜¶æ®µæ¨ç†ã€‚ä¸é“¾å¼æ€ç»´æç¤ºä¸åŒï¼ŒLLaVA-o1ç‹¬ç«‹è¿›è¡Œæ€»ç»“ã€è§†è§‰è§£é‡Šã€é€»è¾‘æ¨ç†å’Œç»“è®ºç”Ÿæˆã€‚è¿™ç§ç»“æ„åŒ–æ–¹æ³•ä½¿å…¶åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸Šå–å¾—æ˜¾è‘—ç²¾åº¦æå‡ã€‚ç ”ç©¶å›¢é˜Ÿç¼–åˆ¶äº†LLaVA-o1-100kæ•°æ®é›†ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ¨ç†æ—¶é˜¶æ®µçº§æŸæœç´¢æ–¹æ³•ï¼Œä»¥å®ç°æœ‰æ•ˆçš„æ¨ç†æ—¶æ‰©å±•ã€‚ç»“æœæ˜¾ç¤ºï¼ŒLLaVA-o1åœ¨å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†å¤šä¸ªå¤§å‹å’Œå°é—­æºæ¨¡å‹ã€‚",
        "title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªæ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œç§°ä¸ºLLaVA-o1ï¼Œæ—¨åœ¨è¿›è¡Œè‡ªä¸»å¤šé˜¶æ®µæ¨ç†ã€‚ä¸é“¾å¼æ€ç»´æç¤ºä¸åŒï¼ŒLLaVA-o1ç‹¬ç«‹è¿›è¡Œæ€»ç»“ã€è§†è§‰è§£é‡Šã€é€»è¾‘æ¨ç†å’Œç»“è®ºç”Ÿæˆã€‚è¿™ç§ç»“æ„åŒ–æ–¹æ³•ä½¿å…¶åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸Šå–å¾—æ˜¾è‘—ç²¾åº¦æå‡ã€‚ç ”ç©¶å›¢é˜Ÿç¼–åˆ¶äº†LLaVA-o1-100kæ•°æ®é›†ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ¨ç†æ—¶é˜¶æ®µçº§æŸæœç´¢æ–¹æ³•ï¼Œä»¥å®ç°æœ‰æ•ˆçš„æ¨ç†æ—¶æ‰©å±•ã€‚ç»“æœæ˜¾ç¤ºï¼ŒLLaVA-o1åœ¨å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†å¤šä¸ªå¤§å‹å’Œå°é—­æºæ¨¡å‹ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« gÃ¨ xÄ«n de shÃ¬ juÃ© yÇ” yÃ¡n mÃ³ xÃ¬ng (VLM)ï¼ŒchÄ“ng wÃ©i LLaVA-o1ï¼ŒzhÇ yÇn jÃ¬n xÃ­ng zÃ¬ zhÇ” duÅ jiÄ“ duÃ n tuÃ­ lÇ. yÇ” liÃ n shÃ¬ sÄ« wÃ©i tÃ­ shÃ¬ bÃ¹ tÃ³ngï¼ŒLLaVA-o1 dÃº lÃ¬ jÃ¬n xÃ­ng zÇ’ng jiÄ›ï¼ŒshÃ¬ juÃ© jiÄ› shÃ¬ï¼ŒluÃ³ ji tuÃ­ lÇ hÃ© jiÃ© lÃ¹n shÄ“ng chÃ©ng. zhÃ¨ zhÇ’ng jiÄ“ gÃ²u huÃ  fÇ shÇ qÃ­ zÃ i tuÃ­ lÇ mÃ¬ jÄ« xÃ­ng rÃ¨n wÃ¹ shÃ ng qÇ” dÃ© xiÇn zhÃ¹ jÄ«ng dÃ¹ tÃ­ shÄ“ng. yÃ¡n jiÅ« tuÃ¡n duÃ¬ biÄn zhÃ¬ le LLaVA-o1-100k shÃ¹ jÃ¹ jÃ­ï¼ŒbÃ¬ng tÃ­ chÅ« le yÄ« zhÇ’ng tuÃ­ lÇ shÃ­ jiÄ“ duÃ n jÃ­ shÃ¹ sÅu suÇ’ fÇï¼ŒyÇ shÃ­ xiÃ n yÃ¡n jiÅ« shÃ­ kuÃ² zhÇn. jiÃ© guÇ’ xiÇn shÃ¬ï¼ŒLLaVA-o1 zÃ i duÅ mÃ³ shuÃ i tuÃ­ lÇ jÄ« zhÇ”n cÃ¨ shÃ¬ zhÅng biÇo xiÃ n chÅ« sÃ¨ï¼ŒchÄo yuÃ¨ le duÅ gÃ¨ dÃ  xÃ­ng hÃ© fÄ“ng bÃ¬ yuÃ¡n mÃ³ xÃ¬ng.",
        "vocab": "[\n    {\"word\": \"è§†è§‰è¯­è¨€æ¨¡å‹\", \"pinyin\": \"shÃ¬juÃ© yÇ”yÃ¡n mÃ³xÃ­ng\", \"trans\": \"visual language model\"},\n    {\"word\": \"è‡ªä¸»\", \"pinyin\": \"zÃ¬zhÇ”\", \"trans\": \"autonomous\"},\n    {\"word\": \"å¤šé˜¶æ®µ\", \"pinyin\": \"duÅ jiÄ“duÃ n\", \"trans\": \"multi-stage\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ«lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"é“¾å¼\", \"pinyin\": \"liÃ nshÃ¬\", \"trans\": \"chain-like\"},\n    {\"word\": \"æç¤º\", \"pinyin\": \"tÃ­shÃ¬\", \"trans\": \"prompt\"},\n    {\"word\": \"ç‹¬ç«‹\", \"pinyin\": \"dÃºlÃ¬\", \"trans\": \"independent\"},\n    {\"word\": \"æ€»ç»“\", \"pinyin\": \"zÇ’ngjiÃ©\", \"trans\": \"summary\"},\n    {\"word\": \"è§†è§‰è§£é‡Š\", \"pinyin\": \"shÃ¬juÃ© jiÄ›shÃ¬\", \"trans\": \"visual explanation\"},\n    {\"word\": \"é€»è¾‘æ¨ç†\", \"pinyin\": \"luÃ³ji tuÄ«lÇ\", \"trans\": \"logical reasoning\"},\n    {\"word\": \"ç»“è®ºç”Ÿæˆ\", \"pinyin\": \"jiÃ©lÃ¹n shÄ“ngchÃ©ng\", \"trans\": \"conclusion generation\"},\n    {\"word\": \"ç»“æ„åŒ–\", \"pinyin\": \"jiÃ©gÃ²uhuÃ \", \"trans\": \"structured\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄngfÇ\", \"trans\": \"method\"},\n    {\"word\": \"ç²¾åº¦\", \"pinyin\": \"jÄ«ngdÃ¹\", \"trans\": \"accuracy\"},\n    {\"word\": \"æå‡\", \"pinyin\": \"tÃ­shÄ“ng\", \"trans\": \"improvement\"},\n    {\"word\": \"ç ”ç©¶å›¢é˜Ÿ\", \"pinyin\": \"yÃ¡njiÅ« tuÃ¡nduÃ¬\", \"trans\": \"research team\"},\n    {\"word\": \"ç¼–åˆ¶\", \"pinyin\": \"biÄnzhÃ¬\", \"trans\": \"compile\"},\n    {\"word\": \"æ•°æ®é›†\", \"pinyin\": \"shÃ¹jÃ¹jÃ­\", \"trans\": \"dataset\"},\n    {\"word\": \"æ¨ç†æ—¶\", \"pinyin\": \"tuÄ«lÇ shÃ­\", \"trans\": \"during reasoning\"},\n    {\"word\": \"é˜¶æ®µçº§\", \"pinyin\": \"jiÄ“duÃ n jÃ­\", \"trans\": \"stage-level\"},\n    {\"word\": \"æŸæœç´¢\", \"pinyin\": \"shÃ¹ sÅusuÇ’\", \"trans\": \"beam search\"},\n    {\"word\": \"æ‰©å±•\", \"pinyin\": \"kuÃ²zhÇn\", \"trans\": \"expansion\"},\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³shÃ¬\", \"trans\": \"multimodal\"},\n    {\"word\": \"åŸºå‡†æµ‹è¯•\", \"pinyin\": \"jÄ«zhÇ”n cÃ¨shÃ¬\", \"trans\": \"benchmark test\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇoxiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"å‡ºè‰²\", \"pinyin\": \"chÅ«sÃ¨\", \"trans\": \"outstanding\"},\n    {\"word\": \"è¶…è¶Š\", \"pinyin\": \"chÄoyuÃ¨\", \"trans\": \"surpass\"},\n    {\"word\": \"å¤§å‹\", \"pinyin\": \"dÃ xÃ­ng\", \"trans\": \"large-scale\"},\n    {\"word\": \"å°é—­æº\", \"pinyin\": \"fÄ“ngbÃ¬ yuÃ¡n\", \"trans\": \"closed-source\"}\n]",
        "trans": "This article introduces a new visual language model (VLM) called LLaVA-o1, designed for autonomous multi-stage reasoning. Unlike chain-of-thought prompting, LLaVA-o1 independently performs summarization, visual explanation, logical reasoning, and conclusion generation. This structured approach achieves significant accuracy improvements in reasoning-intensive tasks. The research team compiled the LLaVA-o1-100k dataset and proposed a stage-level beam search method during inference to achieve effective reasoning-time expansion. The results show that LLaVA-o1 performs exceptionally well in multimodal reasoning benchmarks, outperforming several large and closed-source models.",
        "update_ts": "2024-11-18 09:12"
    }
}