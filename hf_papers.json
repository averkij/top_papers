{
    "date": {
        "ru": "23 декабря",
        "en": "December 23",
        "zh": "12月23日"
    },
    "time_utc": "2024-12-23 06:14",
    "weekday": 0,
    "issue_id": 1261,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.15119",
            "title": "Parallelized Autoregressive Visual Generation",
            "url": "https://huggingface.co/papers/2412.15119",
            "abstract": "Autoregressive models have emerged as a powerful approach for visual generation but suffer from slow inference speed due to their sequential token-by-token prediction process. In this paper, we propose a simple yet effective approach for parallelized autoregressive visual generation that improves generation efficiency while preserving the advantages of autoregressive modeling. Our key insight is that parallel generation depends on visual token dependencies-tokens with weak dependencies can be generated in parallel, while strongly dependent adjacent tokens are difficult to generate together, as their independent sampling may lead to inconsistencies. Based on this observation, we develop a parallel generation strategy that generates distant tokens with weak dependencies in parallel while maintaining sequential generation for strongly dependent local tokens. Our approach can be seamlessly integrated into standard autoregressive models without modifying the architecture or tokenizer. Experiments on ImageNet and UCF-101 demonstrate that our method achieves a 3.6x speedup with comparable quality and up to 9.5x speedup with minimal quality degradation across both image and video generation tasks. We hope this work will inspire future research in efficient visual generation and unified autoregressive modeling. Project page: https://epiphqny.github.io/PAR-project.",
            "score": 14,
            "issue_id": 1258,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 декабря",
                "en": "December 19",
                "zh": "12月19日"
            },
            "hash": "0933582baa02f7a6",
            "authors": [
                "Yuqing Wang",
                "Shuhuai Ren",
                "Zhijie Lin",
                "Yujin Han",
                "Haoyuan Guo",
                "Zhenheng Yang",
                "Difan Zou",
                "Jiashi Feng",
                "Xihui Liu"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Peking University",
                "University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.15119.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#video",
                    "#cv",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение авторегрессионной генерации изображений и видео без потери качества",
                    "desc": "Статья предлагает метод параллельного авторегрессионного генерирования визуального контента, который ускоряет процесс, сохраняя преимущества авторегрессионного моделирования. Авторы разработали стратегию, которая генерирует удаленные токены с слабыми зависимостями параллельно, но сохраняет последовательное генерирование для сильно зависимых локальных токенов. Метод легко интегрируется в стандартные авторегрессионные модели без изменения архитектуры или токенизатора. Эксперименты показали ускорение до 3.6 раз с сопоставимым качеством и до 9.5 раз с минимальной деградацией качества для задач генерации изображений и видео."
                },
                "en": {
                    "title": "Speeding Up Visual Generation with Smart Token Parallelization",
                    "desc": "This paper addresses the slow inference speed of autoregressive models used for visual generation, which typically generate images or videos one token at a time. The authors propose a new method that allows for parallel generation of visual tokens, focusing on the dependencies between tokens to determine which can be generated simultaneously. By identifying weakly dependent tokens that can be generated in parallel, while keeping strongly dependent tokens in a sequential order, the method enhances efficiency without altering the existing model architecture. Experiments show significant speed improvements in generating images and videos, making this approach a promising direction for future research in visual generation."
                },
                "zh": {
                    "title": "并行自回归生成，提升视觉生成效率",
                    "desc": "自回归模型在视觉生成中表现出色，但由于逐个预测的过程，推理速度较慢。本文提出了一种简单有效的并行自回归视觉生成方法，旨在提高生成效率，同时保留自回归建模的优点。我们的关键见解是，视觉标记之间的依赖关系决定了并行生成的可能性，弱依赖的标记可以并行生成，而强依赖的标记则难以一起生成。实验结果表明，我们的方法在图像和视频生成任务中实现了3.6倍的速度提升，且质量保持相当，甚至在某些情况下可达到9.5倍的速度提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.13649",
            "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
            "url": "https://huggingface.co/papers/2412.13649",
            "abstract": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context generation. Despite the numerous efforts in this area, the optimization for the decoding phase is generally ignored. However, we believe such optimization is crucial, especially for long-output generation tasks based on the following two observations: (i) Excessive compression during the prefill phase, which requires specific full context impairs the comprehension of the reasoning task; (ii) Deviation of heavy hitters occurs in the reasoning tasks with long outputs. Therefore, SCOPE, a simple yet efficient framework that separately performs KV cache optimization during the prefill and decoding phases, is introduced. Specifically, the KV cache during the prefill phase is preserved to maintain the essential information, while a novel strategy based on sliding is proposed to select essential heavy hitters for the decoding phase. Memory usage and memory transfer are further optimized using adaptive and discontinuous strategies. Extensive experiments on LongGenBench show the effectiveness and generalization of SCOPE and its compatibility as a plug-in to other prefill-only KV compression methods.",
            "score": 9,
            "issue_id": 1258,
            "pub_date": "2024-12-18",
            "pub_date_card": {
                "ru": "18 декабря",
                "en": "December 18",
                "zh": "12月18日"
            },
            "hash": "885d11532659dd95",
            "authors": [
                "Jialong Wu",
                "Zhenglin Wang",
                "Linhai Zhang",
                "Yilong Lai",
                "Yulan He",
                "Deyu Zhou"
            ],
            "affiliations": [
                "Department of Informatics, Kings College London, UK",
                "School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China",
                "The Alan Turing Institute, UK"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.13649.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#long_context",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективная оптимизация KV-кэша для длинных контекстов в LLM",
                    "desc": "Статья представляет SCOPE - фреймворк для оптимизации KV-кэша в моделях LLM при генерации длинных текстов. Авторы предлагают раздельную оптимизацию для этапов prefill и decoding, сохраняя важную информацию на первом этапе и выбирая ключевые элементы на втором. Используются адаптивные и прерывистые стратегии для оптимизации использования памяти. Эксперименты на LongGenBench показывают эффективность и обобщаемость SCOPE, а также его совместимость с другими методами сжатия KV-кэша."
                },
                "en": {
                    "title": "Optimizing KV Caches for Enhanced Long-Context Generation",
                    "desc": "This paper addresses the limitations of Key-Value (KV) caches in large language models (LLMs) when generating long outputs. It highlights that optimizing the decoding phase is essential, as excessive compression during the prefill phase can hinder reasoning tasks. The proposed framework, SCOPE, optimizes KV cache usage by preserving crucial information during prefill and employing a sliding strategy to select important data during decoding. Experimental results demonstrate that SCOPE improves memory efficiency and can be integrated with existing KV compression methods."
                },
                "zh": {
                    "title": "优化KV缓存，提升长输出生成效率",
                    "desc": "本文提出了一种名为SCOPE的框架，旨在优化长输出生成任务中的KV缓存。研究表明，在预填充阶段过度压缩会影响推理任务的理解，因此需要保留关键信息。SCOPE通过在预填充和解码阶段分别优化KV缓存，采用滑动策略选择重要的重击项，从而提高解码效率。实验结果表明，SCOPE在LongGenBench上表现出色，并且可以作为其他KV压缩方法的插件使用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.16145",
            "title": "Offline Reinforcement Learning for LLM Multi-Step Reasoning",
            "url": "https://huggingface.co/papers/2412.16145",
            "abstract": "Improving the multi-step reasoning ability of large language models (LLMs) with offline reinforcement learning (RL) is essential for quickly adapting them to complex tasks. While Direct Preference Optimization (DPO) has shown promise in aligning LLMs with human preferences, it is less suitable for multi-step reasoning tasks because (1) DPO relies on paired preference data, which is not readily available for multi-step reasoning tasks, and (2) it treats all tokens uniformly, making it ineffective for credit assignment in multi-step reasoning tasks, which often come with sparse reward. In this work, we propose OREO (Offline Reasoning Optimization), an offline RL method for enhancing LLM multi-step reasoning. Building on insights from previous works of maximum entropy reinforcement learning, it jointly learns a policy model and value function by optimizing the soft Bellman Equation. We show in principle that it reduces the need to collect pairwise data and enables better credit assignment. Empirically, OREO surpasses existing offline learning methods on multi-step reasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH) and embodied agent control (ALFWorld). The approach can be extended to a multi-iteration framework when additional resources are available. Furthermore, the learned value function can be leveraged to guide the tree search for free, which can further boost performance during test time.",
            "score": 7,
            "issue_id": 1260,
            "pub_date": "2024-12-20",
            "pub_date_card": {
                "ru": "20 декабря",
                "en": "December 20",
                "zh": "12月20日"
            },
            "hash": "5779a845f782fb45",
            "authors": [
                "Huaijie Wang",
                "Shibo Hao",
                "Hanze Dong",
                "Shenao Zhang",
                "Yilin Bao",
                "Ziran Yang",
                "Yi Wu"
            ],
            "affiliations": [
                "Northwestern University",
                "Salesforce Research",
                "Tsinghua University",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.16145.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rl",
                    "#optimization",
                    "#math",
                    "#rlhf",
                    "#agents",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "OREO: Оптимизация многошаговых рассуждений для языковых моделей",
                    "desc": "В этой статье представлен метод OREO (Offline Reasoning Optimization) для улучшения способностей больших языковых моделей (LLM) к многошаговым рассуждениям с помощью обучения с подкреплением в офлайн-режиме. OREO совместно обучает модель политики и функцию ценности, оптимизируя мягкое уравнение Беллмана. Метод превосходит существующие офлайн-методы обучения на задачах многошагового рассуждения, включая математические задачи и управление агентами. OREO также может быть расширен до многоитерационной структуры и использован для направления древовидного поиска во время тестирования."
                },
                "en": {
                    "title": "OREO: Enhancing Multi-Step Reasoning in LLMs with Offline RL",
                    "desc": "This paper introduces OREO, an offline reinforcement learning method designed to improve the multi-step reasoning capabilities of large language models (LLMs). Unlike Direct Preference Optimization, which struggles with multi-step tasks due to its reliance on paired preference data and uniform token treatment, OREO effectively addresses these challenges by optimizing the soft Bellman Equation. The method enhances credit assignment and reduces the need for extensive data collection, leading to superior performance on reasoning benchmarks. Additionally, OREO's learned value function can be utilized to enhance search strategies during testing, further improving outcomes."
                },
                "zh": {
                    "title": "OREO：提升大型语言模型的多步推理能力",
                    "desc": "本论文提出了一种名为OREO的离线强化学习方法，旨在提高大型语言模型（LLMs）的多步推理能力。与直接偏好优化（DPO）不同，OREO不依赖于成对的偏好数据，适用于多步推理任务。该方法通过优化软贝尔曼方程，联合学习策略模型和价值函数，从而改善了奖励稀疏情况下的信用分配问题。实验结果表明，OREO在数学推理和智能体控制等多步推理基准测试中优于现有的离线学习方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.15322",
            "title": "Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis",
            "url": "https://huggingface.co/papers/2412.15322",
            "abstract": "We propose to synthesize high-quality and synchronized audio, given video and optional text conditions, using a novel multimodal joint training framework MMAudio. In contrast to single-modality training conditioned on (limited) video data only, MMAudio is jointly trained with larger-scale, readily available text-audio data to learn to generate semantically aligned high-quality audio samples. Additionally, we improve audio-visual synchrony with a conditional synchronization module that aligns video conditions with audio latents at the frame level. Trained with a flow matching objective, MMAudio achieves new video-to-audio state-of-the-art among public models in terms of audio quality, semantic alignment, and audio-visual synchronization, while having a low inference time (1.23s to generate an 8s clip) and just 157M parameters. MMAudio also achieves surprisingly competitive performance in text-to-audio generation, showing that joint training does not hinder single-modality performance. Code and demo are available at: https://hkchengrex.github.io/MMAudio",
            "score": 4,
            "issue_id": 1258,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 декабря",
                "en": "December 19",
                "zh": "12月19日"
            },
            "hash": "9d724184f50de930",
            "authors": [
                "Ho Kei Cheng",
                "Masato Ishii",
                "Akio Hayakawa",
                "Takashi Shibuya",
                "Alexander Schwing",
                "Yuki Mitsufuji"
            ],
            "affiliations": [
                "Sony AI",
                "Sony Group Corporation",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.15322.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#audio",
                    "#inference",
                    "#video",
                    "#multimodal",
                    "#synthetic"
                ],
                "emoji": "🎵",
                "ru": {
                    "title": "MMAudio: Революция в синтезе аудио по видео и тексту",
                    "desc": "MMAudio - это новая мультимодальная система для синтеза высококачественного и синхронизированного аудио на основе видео и опционального текста. Она использует совместное обучение на аудио-текстовых данных и условный модуль синхронизации для улучшения соответствия видео и звука. MMAudio достигает нового уровня качества в задаче генерации аудио по видео, превосходя существующие модели по качеству звука, семантическому соответствию и синхронизации. Модель также показывает хорошие результаты в генерации аудио по тексту."
                },
                "en": {
                    "title": "MMAudio: High-Quality Audio Synthesis with Video and Text Integration",
                    "desc": "This paper introduces MMAudio, a novel framework for generating high-quality audio that is synchronized with video and can also utilize text inputs. Unlike traditional methods that rely solely on video data, MMAudio leverages a larger dataset of text-audio pairs to enhance the semantic alignment of the generated audio. The framework includes a conditional synchronization module that ensures audio is aligned with video at the frame level, improving overall coherence. With a flow matching objective, MMAudio sets new benchmarks in audio quality and synchronization while maintaining efficient performance with a low number of parameters."
                },
                "zh": {
                    "title": "MMAudio：高质量音频合成的新方法",
                    "desc": "我们提出了一种新的多模态联合训练框架MMAudio，用于合成高质量和同步的音频，基于视频和可选的文本条件。与仅依赖视频数据的单模态训练不同，MMAudio结合了大规模的文本-音频数据进行联合训练，以生成语义对齐的高质量音频样本。此外，我们通过条件同步模块在帧级别上对视频条件和音频潜在特征进行对齐，从而提高音频与视频的同步性。MMAudio在音频质量、语义对齐和音频-视觉同步方面达到了新的公共模型的最佳水平，同时推理时间低（生成8秒片段仅需1.23秒），参数量仅为157M。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.14590",
            "title": "MixLLM: LLM Quantization with Global Mixed-precision between Output-features and Highly-efficient System Design",
            "url": "https://huggingface.co/papers/2412.14590",
            "abstract": "Quantization has become one of the most effective methodologies to compress LLMs into smaller size. However, the existing quantization solutions still show limitations of either non-negligible accuracy drop or system inefficiency. In this paper, we make a comprehensive analysis of the general quantization principles on their effect to the triangle of accuracy, memory consumption and system efficiency. We propose MixLLM that explores the new optimization space of mixed-precision quantization between output features based on the insight that different output features matter differently in the model. MixLLM identifies the output features with high salience in the global view rather than within each single layer, effectively assigning the larger bit-width to output features that need it most to achieve good accuracy with low memory consumption. We present the sweet spot of quantization configuration of algorithm-system co-design that leads to high accuracy and system efficiency. To address the system challenge, we design the two-step dequantization to make use of the int8 Tensor Core easily and fast data type conversion to reduce dequantization overhead significantly, and present the software pipeline to overlap the memory access, dequantization and the MatMul to the best. Extensive experiments show that with only 10% more bits, the PPL increasement can be reduced from about 0.5 in SOTA to within 0.2 for Llama 3.1 70B, while on average MMLU-Pro improves by 0.93 over the SOTA of three popular models. In addition to its superior accuracy, MixLLM also achieves state-of-the-art system efficiency.",
            "score": 2,
            "issue_id": 1260,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 декабря",
                "en": "December 19",
                "zh": "12月19日"
            },
            "hash": "3a5b6d590eec2c6e",
            "authors": [
                "Zhen Zheng",
                "Xiaonan Song",
                "Chuanjie Liu"
            ],
            "affiliations": [
                "Microsoft"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.14590.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "MixLLM: Оптимальное квантование для эффективных языковых моделей",
                    "desc": "Статья представляет новый метод квантования больших языковых моделей под названием MixLLM. Авторы предлагают использовать смешанную точность квантования для выходных признаков, основываясь на их важности в глобальном контексте модели. MixLLM оптимизирует баланс между точностью, потреблением памяти и эффективностью системы. Эксперименты показывают, что MixLLM достигает лучшей точности и эффективности по сравнению с существующими методами квантования для популярных языковых моделей."
                },
                "en": {
                    "title": "MixLLM: Optimizing Quantization for Accuracy and Efficiency in LLMs",
                    "desc": "This paper discusses a new method called MixLLM for quantizing large language models (LLMs) to make them smaller and more efficient. The authors analyze how different quantization techniques affect accuracy, memory use, and system performance. MixLLM uses mixed-precision quantization, assigning more bits to important output features, which helps maintain accuracy while reducing memory consumption. The proposed method also includes a two-step dequantization process to improve speed and efficiency, resulting in better performance compared to existing solutions."
                },
                "zh": {
                    "title": "MixLLM：高效的混合精度量化方案",
                    "desc": "量化技术已成为压缩大型语言模型（LLMs）的一种有效方法，但现有的量化方案在准确性和系统效率上仍存在局限性。本文对量化原则进行了全面分析，探讨了准确性、内存消耗和系统效率之间的关系。我们提出了MixLLM，利用混合精度量化优化输出特征，确保重要特征获得更高的位宽，从而在保持良好准确性的同时降低内存消耗。通过设计两步反量化和优化软件管道，MixLLM在准确性和系统效率上都达到了最先进的水平。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.16112",
            "title": "CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up",
            "url": "https://huggingface.co/papers/2412.16112",
            "abstract": "Diffusion Transformers (DiT) have become a leading architecture in image generation. However, the quadratic complexity of attention mechanisms, which are responsible for modeling token-wise relationships, results in significant latency when generating high-resolution images. To address this issue, we aim at a linear attention mechanism in this paper that reduces the complexity of pre-trained DiTs to linear. We begin our exploration with a comprehensive summary of existing efficient attention mechanisms and identify four key factors crucial for successful linearization of pre-trained DiTs: locality, formulation consistency, high-rank attention maps, and feature integrity. Based on these insights, we introduce a convolution-like local attention strategy termed CLEAR, which limits feature interactions to a local window around each query token, and thus achieves linear complexity. Our experiments indicate that, by fine-tuning the attention layer on merely 10K self-generated samples for 10K iterations, we can effectively transfer knowledge from a pre-trained DiT to a student model with linear complexity, yielding results comparable to the teacher model. Simultaneously, it reduces attention computations by 99.5% and accelerates generation by 6.3 times for generating 8K-resolution images. Furthermore, we investigate favorable properties in the distilled attention layers, such as zero-shot generalization cross various models and plugins, and improved support for multi-GPU parallel inference. Models and codes are available here: https://github.com/Huage001/CLEAR.",
            "score": 2,
            "issue_id": 1259,
            "pub_date": "2024-12-20",
            "pub_date_card": {
                "ru": "20 декабря",
                "en": "December 20",
                "zh": "12月20日"
            },
            "hash": "c17ca50dc03ea86c",
            "authors": [
                "Songhua Liu",
                "Zhenxiong Tan",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.16112.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#transfer_learning",
                    "#training",
                    "#inference",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "CLEAR: Ускорение DiT без потери качества",
                    "desc": "Статья представляет новый метод линейного внимания для Diffusion Transformers (DiT), называемый CLEAR. Этот подход снижает сложность предобученных DiT с квадратичной до линейной, сохраняя при этом качество генерации изображений. CLEAR использует локальное внимание, подобное свёрточным операциям, ограничивая взаимодействие признаков локальным окном вокруг каждого токена запроса. Эксперименты показывают, что fine-tuning слоя внимания на всего 10 тысячах сгенерированных образцов позволяет эффективно передать знания от предобученной модели к модели с линейной сложностью."
                },
                "en": {
                    "title": "Linearizing Attention for Faster Image Generation with CLEAR",
                    "desc": "This paper presents a new approach to improve the efficiency of Diffusion Transformers (DiT) in image generation by introducing a linear attention mechanism. The authors identify key factors necessary for linearizing DiTs, such as locality and feature integrity, and propose a local attention strategy called CLEAR. This method significantly reduces the computational complexity of attention mechanisms, achieving a 99.5% reduction in attention computations and a 6.3 times speedup in generating high-resolution images. Additionally, the study shows that the distilled model retains performance comparable to the original DiT while enabling better generalization and multi-GPU support."
                },
                "zh": {
                    "title": "线性注意力，快速生成高分辨率图像！",
                    "desc": "本文提出了一种新的线性注意力机制，旨在解决扩散变换器（DiT）在生成高分辨率图像时的延迟问题。我们总结了现有的高效注意力机制，并确定了成功线性化预训练DiT的四个关键因素。基于这些见解，我们引入了一种名为CLEAR的局部注意力策略，限制特征交互在每个查询标记周围的局部窗口内，从而实现线性复杂度。实验结果表明，通过对注意力层进行微调，我们可以有效地将知识从预训练的DiT转移到学生模型，同时显著减少计算量并加速生成过程。"
                }
            }
        }
    ],
    "link_prev": "2024-12-20.html",
    "link_next": "2024-12-24.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "20.12",
        "en": "12/20",
        "zh": "12月20日"
    },
    "short_date_next": {
        "ru": "24.12",
        "en": "12/24",
        "zh": "12月24日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 5,
        "#3d": 0,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 1,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了Qwen2.5，一系列大型语言模型。与前一版本相比，Qwen2.5在预训练和后训练阶段都有显著改进。预训练数据集从7万亿个标记扩展到18万亿个，增强了常识、专业知识和推理能力。后训练使用了超过100万个样本的监督微调和多阶段强化学习，提升了人类偏好和长文本生成等能力。Qwen2.5系列提供多种尺寸和版本，包括基础模型和指令微调模型，以及量化版本。",
        "title": "Qwen2.5 Technical Report",
        "pinyin": "Zhè piān wénzhāng jièshào le Qwen2.5, yī xìliè dàxíng yǔyán móxíng. Yǔ qián yī bǎnběn xiāngbǐ, Qwen2.5 zài yùxùnliàn hé hòuxùnliàn jiēduàn dōu yǒu xiǎnzhù gǎijìn. Yùxùnliàn shùjùjí cóng 7 wàn yì gè biāojì kuòzhǎn dào 18 wàn yì gè, zēngqiáng le chángshí, zhuānxiàng zhīshì hé tuīlǐ nénglì. Hòuxùnliàn shǐyòng le chāoguò 100 wàn gè yàngbǎn de jiàndū wēitiáo hé duō jiēduàn qiángzhù xuéxí, tíshēng le rénlèi piānfú hé cháng wénběn shēngchéng děng nénglì. Qwen2.5 xìliè tígōng duōzhǒng chǐcùn hé bǎnběn, bāokuò jīchǔ móxíng hé zhǐlǐng wēitiáo móxíng, yǐjià liàngzhù bǎnběn.",
        "vocab": "[{'word': '篇', 'pinyin': 'piān', 'trans': 'piece of writing'}, {'word': '介绍', 'pinyin': 'jièshào', 'trans': 'introduce'}, {'word': '语言模型', 'pinyin': 'yǔyán móxíng', 'trans': 'language model'}, {'word': '前一版本', 'pinyin': 'qián yī bǎnběn', 'trans': 'previous version'}, {'word': '相比', 'pinyin': 'xiāngbǐ', 'trans': 'compare'}, {'word': '预训练', 'pinyin': 'yù xùnliàn', 'trans': 'pre-training'}, {'word': '后训练', 'pinyin': 'hòu xùnliàn', 'trans': 'post-training'}, {'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'}, {'word': '改进', 'pinyin': 'gǎijìn', 'trans': 'improvement'}, {'word': '标记', 'pinyin': 'biāojì', 'trans': 'token'}, {'word': '扩展', 'pinyin': 'kuòzhǎn', 'trans': 'expand'}, {'word': '常识', 'pinyin': 'chángshí', 'trans': 'common sense'}, {'word': '专业知识', 'pinyin': 'zhuānyè zhīshi', 'trans': 'professional knowledge'}, {'word': '推理', 'pinyin': 'tuīlǐ', 'trans': 'reasoning'}, {'word': '能力', 'pinyin': 'nénglì', 'trans': 'ability'}, {'word': '监督', 'pinyin': 'jiàndū', 'trans': 'supervised'}, {'word': '微调', 'pinyin': 'wēitiáo', 'trans': 'fine-tuning'}, {'word': '多阶段', 'pinyin': 'duō jiēduàn', 'trans': 'multi-stage'}, {'word': '强化学习', 'pinyin': 'qiángjià xuéxí', 'trans': 'reinforcement learning'}, {'word': '提升', 'pinyin': 'tíshēng', 'trans': 'enhance'}, {'word': '人类偏好', 'pinyin': 'rénlèi piānhào', 'trans': 'human preference'}, {'word': '长文本', 'pinyin': 'cháng wénběn', 'trans': 'long text'}, {'word': '生成', 'pinyin': 'shēngchéng', 'trans': 'generation'}, {'word': '尺寸', 'pinyin': 'chǐcùn', 'trans': 'size'}, {'word': '基础模型', 'pinyin': 'jīchǔ móxíng', 'trans': 'base model'}, {'word': '指令', 'pinyin': 'zhǐlìng', 'trans': 'instruction'}, {'word': '量化', 'pinyin': 'liànghuà', 'trans': 'quantization'}]",
        "trans": "This article introduces Qwen2.5, a series of large language models. Compared to the previous version, Qwen2.5 has significant improvements in both the pre-training and post-training stages. The pre-training dataset has been expanded from 7 trillion tokens to 18 trillion tokens, enhancing common sense, professional knowledge, and reasoning capabilities. Post-training involved supervised fine-tuning with over 1 million samples and multi-stage reinforcement learning, improving human preference and long text generation capabilities. The Qwen2.5 series offers various sizes and versions, including base models, instruction-tuned models, and quantized versions.",
        "update_ts": "2024-12-22 12:38"
    }
}