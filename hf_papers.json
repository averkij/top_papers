{
    "date": {
        "ru": "20 февраля",
        "en": "February 20",
        "zh": "2月20日"
    },
    "time_utc": "2025-02-20 04:12",
    "weekday": 3,
    "issue_id": 2310,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.13144",
            "title": "RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based Reinforcement Learning",
            "url": "https://huggingface.co/papers/2502.13144",
            "abstract": "Existing end-to-end autonomous driving (AD) algorithms typically follow the Imitation Learning (IL) paradigm, which faces challenges such as causal confusion and the open-loop gap. In this work, we establish a 3DGS-based closed-loop Reinforcement Learning (RL) training paradigm. By leveraging 3DGS techniques, we construct a photorealistic digital replica of the real physical world, enabling the AD policy to extensively explore the state space and learn to handle out-of-distribution scenarios through large-scale trial and error. To enhance safety, we design specialized rewards that guide the policy to effectively respond to safety-critical events and understand real-world causal relationships. For better alignment with human driving behavior, IL is incorporated into RL training as a regularization term. We introduce a closed-loop evaluation benchmark consisting of diverse, previously unseen 3DGS environments. Compared to IL-based methods, RAD achieves stronger performance in most closed-loop metrics, especially 3x lower collision rate. Abundant closed-loop results are presented at https://hgao-cv.github.io/RAD.",
            "score": 13,
            "issue_id": 2309,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 февраля",
                "en": "February 18",
                "zh": "2月18日"
            },
            "hash": "0db330614af75888",
            "authors": [
                "Hao Gao",
                "Shaoyu Chen",
                "Bo Jiang",
                "Bencheng Liao",
                "Yiang Shi",
                "Xiaoyang Guo",
                "Yuechuan Pu",
                "Haoran Yin",
                "Xiangyu Li",
                "Xinbang Zhang",
                "Ying Zhang",
                "Wenyu Liu",
                "Qian Zhang",
                "Xinggang Wang"
            ],
            "affiliations": [
                "Horizon Robotics",
                "Huazhong University of Science & Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.13144.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#alignment",
                    "#3d",
                    "#games",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "Революция в автономном вождении: обучение с подкреплением в фотореалистичных 3D-мирах",
                    "desc": "Статья представляет новый подход к автономному вождению, основанный на обучении с подкреплением (RL) с использованием фотореалистичных 3D-моделей окружающей среды. Авторы разработали систему RAD, которая позволяет политике автономного вождения исследовать различные сценарии и учиться справляться с нестандартными ситуациями через масштабные эксперименты. Для повышения безопасности были разработаны специальные функции вознаграждения, а для лучшего соответствия человеческому поведению при вождении было включено обучение по имитации (IL) в качестве регуляризации. Результаты показывают, что RAD превосходит методы, основанные только на IL, особенно в снижении частоты столкновений."
                },
                "en": {
                    "title": "Revolutionizing Autonomous Driving with Closed-Loop Reinforcement Learning",
                    "desc": "This paper presents a new approach to autonomous driving using a closed-loop Reinforcement Learning (RL) paradigm, addressing limitations of traditional Imitation Learning (IL). By creating a realistic 3D digital environment, the model can explore various driving scenarios and learn from trial and error, improving its ability to handle unexpected situations. The authors introduce specialized rewards to enhance safety by teaching the model to react appropriately to critical events and understand causal relationships in driving. Additionally, they incorporate IL as a regularization term to better align the model's behavior with human driving patterns, resulting in significantly improved performance metrics, including a lower collision rate."
                },
                "zh": {
                    "title": "闭环强化学习提升自主驾驶安全性与性能",
                    "desc": "现有的端到端自主驾驶算法通常采用模仿学习（IL）方法，但面临因果混淆和开放环路差距等挑战。本文提出了一种基于3DGS的闭环强化学习（RL）训练范式，通过构建真实物理世界的逼真数字复制品，使自主驾驶策略能够广泛探索状态空间，并通过大规模试错学习处理分布外场景。为了提高安全性，我们设计了专门的奖励机制，引导策略有效应对安全关键事件，并理解现实世界的因果关系。同时，为了更好地与人类驾驶行为对齐，我们将模仿学习作为正则化项融入到强化学习训练中。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.13922",
            "title": "LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization",
            "url": "https://huggingface.co/papers/2502.13922",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities through pretraining and alignment. However, superior short-context LLMs may underperform in long-context scenarios due to insufficient long-context alignment. This alignment process remains challenging due to the impracticality of human annotation for extended contexts and the difficulty in balancing short- and long-context performance. To address these challenges, we introduce LongPO, that enables short-context LLMs to self-evolve to excel on long-context tasks by internally transferring short-context capabilities. LongPO harnesses LLMs to learn from self-generated short-to-long preference data, comprising paired responses generated for identical instructions with long-context inputs and their compressed short-context counterparts, respectively. This preference reveals capabilities and potentials of LLMs cultivated during short-context alignment that may be diminished in under-aligned long-context scenarios. Additionally, LongPO incorporates a short-to-long KL constraint to mitigate short-context performance decline during long-context alignment. When applied to Mistral-7B-Instruct-v0.2 from 128K to 512K context lengths, LongPO fully retains short-context performance and largely outperforms naive SFT and DPO in both long- and short-context tasks. Specifically, \\ourMethod-trained models can achieve results on long-context benchmarks comparable to, or even surpassing, those of superior LLMs (e.g., GPT-4-128K) that involve extensive long-context annotation and larger parameter scales.",
            "score": 11,
            "issue_id": 2309,
            "pub_date": "2025-02-19",
            "pub_date_card": {
                "ru": "19 февраля",
                "en": "February 19",
                "zh": "2月19日"
            },
            "hash": "93cf8365ba7edb80",
            "authors": [
                "Guanzheng Chen",
                "Xin Li",
                "Michael Qizhe Shieh",
                "Lidong Bing"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Lab, 310023, Hangzhou, China",
                "National University of Singapore",
                "Shanda AI Research Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.13922.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#transfer_learning",
                    "#benchmark",
                    "#long_context",
                    "#architecture",
                    "#rlhf"
                ],
                "emoji": "📏",
                "ru": {
                    "title": "LongPO: Самоэволюция языковых моделей для работы с длинным контекстом",
                    "desc": "Статья представляет новый метод LongPO для улучшения работы языковых моделей с длинным контекстом. LongPO позволяет моделям, обученным на коротких контекстах, самостоятельно адаптироваться к длинным контекстам, используя самогенерируемые данные о предпочтениях. Метод включает ограничение KL для сохранения производительности на коротких контекстах. Эксперименты показывают, что LongPO превосходит наивные методы обучения на длинных и коротких контекстах."
                },
                "en": {
                    "title": "Empowering Short-Context LLMs for Long-Context Mastery",
                    "desc": "This paper presents LongPO, a method designed to enhance the performance of short-context Large Language Models (LLMs) on long-context tasks. The challenge arises from the difficulty of aligning LLMs for long contexts due to the lack of human-annotated data and the need to balance performance across different context lengths. LongPO allows LLMs to learn from their own generated data, creating a preference model that helps them adapt their short-context skills to long-context scenarios. The results show that models trained with LongPO maintain their short-context performance while significantly improving their long-context capabilities, even rivaling more advanced models like GPT-4."
                },
                "zh": {
                    "title": "LongPO：短上下文模型的长上下文自我演化",
                    "desc": "大型语言模型（LLMs）在预训练和对齐方面表现出色，但在长上下文场景中可能表现不佳。为了解决这个问题，本文提出了LongPO方法，使短上下文的LLMs能够自我演化，以在长上下文任务中表现出色。LongPO通过生成短到长的偏好数据，帮助模型学习如何在长上下文中保持短上下文的能力。实验结果表明，使用LongPO的模型在长上下文基准测试中表现优异，甚至可以与更强大的LLMs相媲美。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.13965",
            "title": "Autellix: An Efficient Serving Engine for LLM Agents as General Programs",
            "url": "https://huggingface.co/papers/2502.13965",
            "abstract": "Large language model (LLM) applications are evolving beyond simple chatbots into dynamic, general-purpose agentic programs, which scale LLM calls and output tokens to help AI agents reason, explore, and solve complex tasks. However, existing LLM serving systems ignore dependencies between programs and calls, missing significant opportunities for optimization. Our analysis reveals that programs submitted to LLM serving engines experience long cumulative wait times, primarily due to head-of-line blocking at both the individual LLM request and the program. To address this, we introduce Autellix, an LLM serving system that treats programs as first-class citizens to minimize their end-to-end latencies. Autellix intercepts LLM calls submitted by programs, enriching schedulers with program-level context. We propose two scheduling algorithms-for single-threaded and distributed programs-that preempt and prioritize LLM calls based on their programs' previously completed calls. Our evaluation demonstrates that across diverse LLMs and agentic workloads, Autellix improves throughput of programs by 4-15x at the same latency compared to state-of-the-art systems, such as vLLM.",
            "score": 7,
            "issue_id": 2310,
            "pub_date": "2025-02-19",
            "pub_date_card": {
                "ru": "19 февраля",
                "en": "February 19",
                "zh": "2月19日"
            },
            "hash": "f64be1cc6aba8b4c",
            "authors": [
                "Michael Luo",
                "Xiaoxiang Shi",
                "Colin Cai",
                "Tianjun Zhang",
                "Justin Wong",
                "Yichuan Wang",
                "Chi Wang",
                "Yanping Huang",
                "Zhifeng Chen",
                "Joseph E. Gonzalez",
                "Ion Stoica"
            ],
            "affiliations": [
                "Google DeepMind",
                "Shanghai Jiao Tong University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.13965.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#agents",
                    "#architecture"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Autellix: Революция в обслуживании LLM для агентных программ",
                    "desc": "Статья представляет Autellix - систему обслуживания больших языковых моделей (LLM), оптимизирующую выполнение агентных программ. Autellix рассматривает программы как объекты первого класса, перехватывая вызовы LLM и обогащая планировщики контекстом на уровне программ. Предложены два алгоритма планирования для однопоточных и распределенных программ, которые приоритизируют вызовы LLM на основе ранее выполненных вызовов. Эксперименты показывают, что Autellix улучшает пропускную способность программ в 4-15 раз при той же задержке по сравнению с современными системами."
                },
                "en": {
                    "title": "Autellix: Optimizing LLM Serving for Enhanced Throughput",
                    "desc": "This paper discusses the evolution of large language models (LLMs) from simple chatbots to more complex, agentic programs that can perform a variety of tasks. It identifies a significant issue in current LLM serving systems, which overlook the dependencies between different programs and their calls, leading to inefficiencies and long wait times. The authors introduce Autellix, a new LLM serving system that optimizes these processes by treating programs as first-class entities and enhancing scheduling with program-level context. Their proposed scheduling algorithms significantly improve the throughput of LLM programs, achieving 4-15 times better performance compared to existing systems while maintaining similar latency."
                },
                "zh": {
                    "title": "优化LLM调用，提升程序性能的Autellix",
                    "desc": "这篇论文介绍了一种新的大型语言模型（LLM）服务系统，名为Autellix。Autellix通过将程序视为第一类公民，优化了LLM调用的调度，从而减少了整体延迟。研究表明，现有的LLM服务系统忽视了程序与调用之间的依赖关系，导致了长时间的等待。通过引入新的调度算法，Autellix在多种LLM和任务负载下，提升了程序的吞吐量，效果显著。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.13347",
            "title": "Craw4LLM: Efficient Web Crawling for LLM Pretraining",
            "url": "https://huggingface.co/papers/2502.13347",
            "abstract": "Web crawl is a main source of large language models' (LLMs) pretraining data, but the majority of crawled web pages are discarded in pretraining due to low data quality. This paper presents Crawl4LLM, an efficient web crawling method that explores the web graph based on the preference of LLM pretraining. Specifically, it leverages the influence of a webpage in LLM pretraining as the priority score of the web crawler's scheduler, replacing the standard graph connectivity based priority. Our experiments on a web graph containing 900 million webpages from a commercial search engine's index demonstrate the efficiency of Crawl4LLM in obtaining high-quality pretraining data. With just 21% URLs crawled, LLMs pretrained on Crawl4LLM data reach the same downstream performances of previous crawls, significantly reducing the crawling waste and alleviating the burdens on websites. Our code is publicly available at https://github.com/cxcscmu/Crawl4LLM.",
            "score": 6,
            "issue_id": 2310,
            "pub_date": "2025-02-19",
            "pub_date_card": {
                "ru": "19 февраля",
                "en": "February 19",
                "zh": "2月19日"
            },
            "hash": "dd055f606e1ddfe2",
            "authors": [
                "Shi Yu",
                "Zhiyuan Liu",
                "Chenyan Xiong"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, Tsinghua University",
                "School of Computer Science, Carnegie Mellon University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.13347.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#graphs",
                    "#open_source",
                    "#data"
                ],
                "emoji": "🕷️",
                "ru": {
                    "title": "Умный веб-краулинг для эффективного обучения языковых моделей",
                    "desc": "Статья представляет Crawl4LLM - эффективный метод веб-краулинга для предобучения больших языковых моделей (LLM). Метод использует влияние веб-страницы на предобучение LLM в качестве приоритета для планировщика краулера, заменяя стандартный подход на основе связности графа. Эксперименты на графе из 900 миллионов страниц показали эффективность Crawl4LLM в получении качественных данных для предобучения. При использовании всего 21% URL модели достигают тех же результатов, что и при предыдущих подходах к краулингу."
                },
                "en": {
                    "title": "Crawl Smart: Boosting LLMs with Efficient Web Crawling",
                    "desc": "This paper introduces Crawl4LLM, a novel web crawling technique designed to enhance the quality of pretraining data for large language models (LLMs). Instead of relying on traditional methods that prioritize web page connectivity, Crawl4LLM uses a priority score based on the potential influence of a webpage on LLM performance. The method was tested on a vast web graph with 900 million pages, showing that it can achieve comparable downstream performance with only 21% of the URLs crawled. This approach not only improves data quality but also minimizes the environmental impact of web crawling by reducing unnecessary data collection."
                },
                "zh": {
                    "title": "高效爬虫，提升LLM预训练数据质量",
                    "desc": "本论文提出了一种名为Crawl4LLM的高效网络爬虫方法，旨在提高大语言模型（LLM）预训练数据的质量。该方法通过网页在LLM预训练中的影响力作为优先级评分，优化了爬虫调度器的工作，而不是依赖于传统的图连接性优先级。实验结果表明，Crawl4LLM在仅爬取21%的网址的情况下，能够获得与之前爬取相同的下游性能，显著减少了爬取浪费。此方法不仅提高了数据质量，还减轻了对网站的负担。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12143",
            "title": "Small Models Struggle to Learn from Strong Reasoners",
            "url": "https://huggingface.co/papers/2502.12143",
            "abstract": "Large language models (LLMs) excel in complex reasoning tasks, and distilling their reasoning capabilities into smaller models has shown promise. However, we uncover an interesting phenomenon, which we term the Small Model Learnability Gap: small models (leq3B parameters) do not consistently benefit from long chain-of-thought (CoT) reasoning or distillation from larger models. Instead, they perform better when fine-tuned on shorter, simpler reasoning chains that better align with their intrinsic learning capacity. To address this, we propose Mix Distillation, a simple yet effective strategy that balances reasoning complexity by combining long and short CoT examples or reasoning from both larger and smaller models. Our experiments demonstrate that Mix Distillation significantly improves small model reasoning performance compared to training on either data alone. These findings highlight the limitations of direct strong model distillation and underscore the importance of adapting reasoning complexity for effective reasoning capability transfer.",
            "score": 6,
            "issue_id": 2309,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "5abea4fb025815c2",
            "authors": [
                "Yuetai Li",
                "Xiang Yue",
                "Zhangchen Xu",
                "Fengqing Jiang",
                "Luyao Niu",
                "Bill Yuchen Lin",
                "Bhaskar Ramasubramanian",
                "Radha Poovendran"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "University of Washington",
                "Western Washington University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12143.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#transfer_learning",
                    "#optimization",
                    "#reasoning",
                    "#small_models"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Адаптация сложности рассуждений для эффективного обучения малых языковых моделей",
                    "desc": "Исследование показывает, что маленькие языковые модели (до 3 млрд параметров) не всегда успешно обучаются на длинных цепочках рассуждений или при дистилляции от больших моделей. Авторы обнаружили, что такие модели лучше обучаются на более коротких и простых цепочках рассуждений. Для решения этой проблемы предложен метод Mix Distillation, сочетающий длинные и короткие примеры рассуждений. Эксперименты показывают, что этот подход значительно улучшает способность маленьких моделей к рассуждениям."
                },
                "en": {
                    "title": "Bridging the Learnability Gap for Small Models with Mix Distillation",
                    "desc": "This paper explores the challenges faced by small language models (with 3 billion parameters or fewer) in learning from complex reasoning tasks. It identifies a phenomenon called the Small Model Learnability Gap, where these smaller models do not gain advantages from long chain-of-thought (CoT) reasoning or direct distillation from larger models. Instead, they perform better when trained on shorter, simpler reasoning chains that match their learning abilities. To improve their performance, the authors propose a method called Mix Distillation, which combines both long and short reasoning examples, leading to better reasoning outcomes for small models."
                },
                "zh": {
                    "title": "混合蒸馏：提升小模型推理能力的有效策略",
                    "desc": "大型语言模型在复杂推理任务中表现出色，但我们发现小模型（参数小于等于3亿）在长链推理或从大模型蒸馏中并不总是受益。相反，它们在短小简单的推理链上微调时表现更好，这与它们的学习能力更为契合。为了解决这个问题，我们提出了混合蒸馏（Mix Distillation）策略，通过结合长短推理示例或来自大模型和小模型的推理，平衡推理复杂性。实验表明，混合蒸馏显著提高了小模型的推理性能，强调了直接强模型蒸馏的局限性，并突出了适应推理复杂性的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.13233",
            "title": "SearchRAG: Can Search Engines Be Helpful for LLM-based Medical Question Answering?",
            "url": "https://huggingface.co/papers/2502.13233",
            "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in general domains but often struggle with tasks requiring specialized knowledge. Conventional Retrieval-Augmented Generation (RAG) techniques typically retrieve external information from static knowledge bases, which can be outdated or incomplete, missing fine-grained clinical details essential for accurate medical question answering. In this work, we propose SearchRAG, a novel framework that overcomes these limitations by leveraging real-time search engines. Our method employs synthetic query generation to convert complex medical questions into search-engine-friendly queries and utilizes uncertainty-based knowledge selection to filter and incorporate the most relevant and informative medical knowledge into the LLM's input. Experimental results demonstrate that our method significantly improves response accuracy in medical question answering tasks, particularly for complex questions requiring detailed and up-to-date knowledge.",
            "score": 5,
            "issue_id": 2310,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 февраля",
                "en": "February 18",
                "zh": "2月18日"
            },
            "hash": "58503159cb9ed740",
            "authors": [
                "Yucheng Shi",
                "Tianze Yang",
                "Canyu Chen",
                "Quanzheng Li",
                "Tianming Liu",
                "Xiang Li",
                "Ninghao Liu"
            ],
            "affiliations": [
                "Illinois Institute of Technology",
                "Massachusetts General Hospital and Harvard Medical School",
                "University of Georgia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.13233.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#synthetic",
                    "#healthcare",
                    "#science"
                ],
                "emoji": "🩺",
                "ru": {
                    "title": "SearchRAG: Точные медицинские ответы с помощью актуального поиска",
                    "desc": "Эта статья представляет SearchRAG - новый метод для улучшения ответов больших языковых моделей на медицинские вопросы. В отличие от традиционных подходов извлечения информации, SearchRAG использует поисковые системы в реальном времени для получения актуальных данных. Метод включает генерацию синтетических запросов и отбор релевантной информации на основе оценки неопределенности. Эксперименты показывают, что SearchRAG значительно повышает точность ответов на сложные медицинские вопросы, требующие детальных и современных знаний."
                },
                "en": {
                    "title": "Enhancing Medical Q&A with Real-Time Search and Smart Querying",
                    "desc": "This paper introduces SearchRAG, a new framework designed to enhance the performance of Large Language Models (LLMs) in medical question answering. Unlike traditional Retrieval-Augmented Generation (RAG) methods that rely on static knowledge bases, SearchRAG utilizes real-time search engines to access current and detailed medical information. The framework employs synthetic query generation to transform complex medical inquiries into queries suitable for search engines, and it uses uncertainty-based knowledge selection to ensure that only the most relevant information is included. Experimental results indicate that SearchRAG significantly boosts the accuracy of LLM responses, especially for intricate medical questions that demand precise and updated knowledge."
                },
                "zh": {
                    "title": "实时搜索提升医疗问答准确性",
                    "desc": "大型语言模型在一般领域表现出色，但在需要专业知识的任务中常常遇到困难。传统的检索增强生成（RAG）技术通常从静态知识库中检索外部信息，这些信息可能过时或不完整，缺乏准确医疗问答所需的细节。我们提出了一种新框架SearchRAG，通过利用实时搜索引擎来克服这些限制。实验结果表明，我们的方法在医疗问答任务中显著提高了响应准确性，尤其是对于需要详细和最新知识的复杂问题。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.13943",
            "title": "AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence",
            "url": "https://huggingface.co/papers/2502.13943",
            "abstract": "Current approaches for training Process Reward Models (PRMs) often involve breaking down responses into multiple reasoning steps using rule-based techniques, such as using predefined placeholder tokens or setting the reasoning step's length into a fixed size. These approaches overlook the fact that specific words do not typically mark true decision points in a text. To address this, we propose AdaptiveStep, a method that divides reasoning steps based on the model's confidence in predicting the next word. This division method provides more decision-making information at each step, enhancing downstream tasks, such as reward model learning. Moreover, our method does not require manual annotation. We demonstrate its effectiveness through experiments with AdaptiveStep-trained PRMs in mathematical reasoning and code generation tasks. Experimental results indicate that the outcome PRM achieves state-of-the-art Best-of-N performance, surpassing greedy search strategy with token-level value-guided decoding, while also reducing construction costs by over 30% compared to existing open-source PRMs. In addition, we provide a thorough analysis and case study on the PRM's performance, transferability, and generalization capabilities.",
            "score": 2,
            "issue_id": 2310,
            "pub_date": "2025-02-19",
            "pub_date_card": {
                "ru": "19 февраля",
                "en": "February 19",
                "zh": "2月19日"
            },
            "hash": "41ab630e56147df2",
            "authors": [
                "Yuliang Liu",
                "Junjie Lu",
                "Zhaoling Chen",
                "Chaofeng Qu",
                "Jason Klein Liu",
                "Chonghan Liu",
                "Zefan Cai",
                "Yunhui Xia",
                "Li Zhao",
                "Jiang Bian",
                "Chuheng Zhang",
                "Wei Shen",
                "Zhouhan Lin"
            ],
            "affiliations": [
                "MSRA",
                "Nanjing University",
                "Shanghai Jiaotong University",
                "UW-Madison",
                "University of Technology Sydney"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.13943.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#plp",
                    "#training",
                    "#open_source",
                    "#math",
                    "#transfer_learning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "AdaptiveStep: умное разбиение на шаги для эффективного обучения PRM",
                    "desc": "Статья представляет новый метод AdaptiveStep для обучения моделей вознаграждения процессов (PRM). Вместо разбиения ответов на шаги фиксированной длины, AdaptiveStep использует уверенность модели в предсказании следующего слова для определения границ шагов рассуждения. Этот подход улучшает качество информации на каждом шаге и не требует ручной разметки. Эксперименты показали, что PRM, обученные с помощью AdaptiveStep, достигают лучших результатов в задачах математических рассуждений и генерации кода, превосходя существующие методы."
                },
                "en": {
                    "title": "AdaptiveStep: Smarter Reasoning for Better Reward Models",
                    "desc": "This paper introduces AdaptiveStep, a novel method for training Process Reward Models (PRMs) that improves the way reasoning steps are defined. Instead of relying on fixed-length steps or predefined tokens, AdaptiveStep adjusts the reasoning process based on the model's confidence in predicting the next word. This approach enhances the decision-making information available at each step, leading to better performance in tasks like reward model learning. Experimental results show that PRMs trained with AdaptiveStep outperform traditional methods in mathematical reasoning and code generation, while also being more cost-effective."
                },
                "zh": {
                    "title": "自适应步骤：提升奖励模型的决策能力",
                    "desc": "本文提出了一种新的训练过程奖励模型（PRM）的方法，称为AdaptiveStep。该方法通过根据模型对下一个单词预测的信心来划分推理步骤，从而提供更丰富的决策信息。与传统的基于规则的方法不同，AdaptiveStep不需要手动标注，且在数学推理和代码生成任务中表现出色。实验结果表明，使用AdaptiveStep训练的PRM在性能上超过了现有的开源PRM，并且构建成本降低了30%以上。"
                }
            }
        }
    ],
    "link_prev": "2025-02-19.html",
    "link_next": "2025-02-21.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "19.02",
        "en": "02/19",
        "zh": "2月19日"
    },
    "short_date_next": {
        "ru": "21.02",
        "en": "02/21",
        "zh": "2月21日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 2,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 1,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 3,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了现有端到端语音大语言模型（LLMs）通常依赖大规模标注数据进行训练，而数据高效训练尚未深入探讨。文章聚焦于语音和文本之间的两个基本问题：表示空间差距和序列长度不一致。作者提出了Soundwave，它使用高效的训练策略和新颖的架构来解决这些问题。结果显示，Soundwave在使用五十分之一的训练数据时，在语音翻译和AIR-Bench语音任务中优于先进的Qwen2-Audio。进一步分析表明，Soundwave在对话中仍保持其智能。项目的代码可在https://github.com/FreedomIntelligence/Soundwave找到。",
        "title": "Soundwave: Less is More for Speech-Text Alignment in LLMs",
        "pinyin": "这篇文章讨论了现有端到端语音大语言模型（LLMs）通常依赖大规模标注数据进行训练，而数据高效训练尚未深入探讨。\nZhè piān wénzhāng tǎolùn le xiànyǒu duānduān yǔyīn dà yǔyán móxíng (LLMs) tōngcháng yīlài dà guīmó biāozhù shùjù jìnxíng xùnliàn, ér shùjù gāoxiào xùnliàn shàngwèi shēnrù tàntào.\n\n文章聚焦于语音和文本之间的两个基本问题：表示空间差距和序列长度不一致。\nWénzhāng jùjiāo yú yǔyīn hé wénběn zhījiān de liǎng gè jīběn wèntí: biǎoshì kōngjiān chājù hé xùliè chángdù bù yīzhì.\n\n作者提出了Soundwave，它使用高效的训练策略和新颖的架构来解决这些问题。\nZuòzhě tíchū le Soundwave, tā shǐyòng gāoxiào de xùnliàn cèlüè hé xīnyǐng de jiàgòu lái jiějué zhèxiē wèntí.\n\n结果显示，Soundwave在使用五十分之一的训练数据时，在语音翻译和AIR-Bench语音任务中优于先进的Qwen2-Audio。\nJiégǔo xiǎnshì, Soundwave zài shǐyòng wǔshí fēn zhī yī de xùnliàn shùjù shí, zài yǔyīn fānyì hé AIR-Bench yǔyīn rènwù zhōng yōuyú xiānjìn de Qwen2-Audio.\n\n进一步分析表明，Soundwave在对话中仍保持其智能。\nJìn yībù fēnxiǎn biǎomíng, Soundwave zài duìhuà zhōng réng bǎochí qí zhìnéng.\n\n项目的代码可在https://github.com/FreedomIntelligence/Soundwave找到。\nXiàngmù de dàimǎ kě zài https://github.com/FreedomIntelligence/Soundwave zhǎo dào.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"端到端\", \"pinyin\": \"duān dào duān\", \"trans\": \"end-to-end\"},\n    {\"word\": \"语音\", \"pinyin\": \"yǔ yīn\", \"trans\": \"speech\"},\n    {\"word\": \"大语言模型\", \"pinyin\": \"dà yǔ yán mó xíng\", \"trans\": \"large language model\"},\n    {\"word\": \"依赖\", \"pinyin\": \"yī lài\", \"trans\": \"rely on\"},\n    {\"word\": \"大规模\", \"pinyin\": \"dà guī mó\", \"trans\": \"large-scale\"},\n    {\"word\": \"标注\", \"pinyin\": \"biāo zhù\", \"trans\": \"annotate\"},\n    {\"word\": \"数据\", \"pinyin\": \"shù jù\", \"trans\": \"data\"},\n    {\"word\": \"进行\", \"pinyin\": \"jìn xíng\", \"trans\": \"carry out\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùn liàn\", \"trans\": \"train\"},\n    {\"word\": \"高效\", \"pinyin\": \"gāo xiào\", \"trans\": \"efficient\"},\n    {\"word\": \"深入\", \"pinyin\": \"shēn rù\", \"trans\": \"in-depth\"},\n    {\"word\": \"探讨\", \"pinyin\": \"tàn tǎo\", \"trans\": \"explore\"},\n    {\"word\": \"聚焦\", \"pinyin\": \"jù jiāo\", \"trans\": \"focus on\"},\n    {\"word\": \"基本\", \"pinyin\": \"jī běn\", \"trans\": \"basic\"},\n    {\"word\": \"问题\", \"pinyin\": \"wèn tí\", \"trans\": \"problem\"},\n    {\"word\": \"表示\", \"pinyin\": \"biǎo shì\", \"trans\": \"represent\"},\n    {\"word\": \"空间\", \"pinyin\": \"kōng jiān\", \"trans\": \"space\"},\n    {\"word\": \"差距\", \"pinyin\": \"chā jù\", \"trans\": \"gap\"},\n    {\"word\": \"序列\", \"pinyin\": \"xù liè\", \"trans\": \"sequence\"},\n    {\"word\": \"长度\", \"pinyin\": \"cháng dù\", \"trans\": \"length\"},\n    {\"word\": \"不一致\", \"pinyin\": \"bù yī zhì\", \"trans\": \"inconsistent\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"策略\", \"pinyin\": \"cè lüè\", \"trans\": \"strategy\"},\n    {\"word\": \"架构\", \"pinyin\": \"jià gòu\", \"trans\": \"architecture\"},\n    {\"word\": \"解决\", \"pinyin\": \"jiě jué\", \"trans\": \"solve\"},\n    {\"word\": \"结果\", \"pinyin\": \"jié guǒ\", \"trans\": \"result\"},\n    {\"word\": \"显示\", \"pinyin\": \"xiǎn shì\", \"trans\": \"show\"},\n    {\"word\": \"优于\", \"pinyin\": \"yōu yú\", \"trans\": \"superior to\"},\n    {\"word\": \"先进\", \"pinyin\": \"xiān jìn\", \"trans\": \"advanced\"},\n    {\"word\": \"分析\", \"pinyin\": \"fēn xī\", \"trans\": \"analyze\"},\n    {\"word\": \"表明\", \"pinyin\": \"biǎo míng\", \"trans\": \"indicate\"},\n    {\"word\": \"保持\", \"pinyin\": \"bǎo chí\", \"trans\": \"maintain\"},\n    {\"word\": \"智能\", \"pinyin\": \"zhì néng\", \"trans\": \"intelligence\"},\n    {\"word\": \"项目\", \"pinyin\": \"xiàng mù\", \"trans\": \"project\"},\n    {\"word\": \"代码\", \"pinyin\": \"dài mǎ\", \"trans\": \"code\"}\n]",
        "trans": "This article discusses how existing end-to-end speech large language models (LLMs) typically rely on large-scale annotated data for training, while data-efficient training has not been deeply explored. The article focuses on two fundamental issues between speech and text: the representation space gap and the inconsistency in sequence length. The authors propose Soundwave, which employs efficient training strategies and novel architecture to address these problems. The results show that Soundwave outperforms the advanced Qwen2-Audio in speech translation and AIR-Bench speech tasks while using one-fiftieth of the training data. Further analysis indicates that Soundwave retains its intelligence in conversations. The project's code can be found at https://github.com/FreedomIntelligence/Soundwave.",
        "update_ts": "2025-02-19 09:11"
    }
}