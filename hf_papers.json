{
    "date": {
        "ru": "8 Ğ¸ÑĞ»Ñ",
        "en": "July 8",
        "zh": "7æœˆ8æ—¥"
    },
    "time_utc": "2025-07-08 03:46",
    "weekday": 1,
    "issue_id": 4693,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.03724",
            "title": "MemOS: A Memory OS for AI System",
            "url": "https://huggingface.co/papers/2507.03724",
            "abstract": "MemOS is proposed as a memory operating system for Large Language Models to enhance memory management, enabling efficient storage and retrieval, and facilitating continual learning and personalized modeling.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have become an essential infrastructure for Artificial General Intelligence (AGI), yet their lack of well-defined memory management systems hinders the development of long-context reasoning, continual personalization, and knowledge consistency.Existing models mainly rely on static parameters and short-lived contextual states, limiting their ability to track user preferences or update knowledge over extended periods.While Retrieval-Augmented Generation (RAG) introduces external knowledge in plain text, it remains a stateless workaround without lifecycle control or integration with persistent representations.Recent work has modeled the training and inference cost of LLMs from a memory hierarchy perspective, showing that introducing an explicit memory layer between parameter memory and external retrieval can substantially reduce these costs by externalizing specific knowledge. Beyond computational efficiency, LLMs face broader challenges arising from how information is distributed over time and context, requiring systems capable of managing heterogeneous knowledge spanning different temporal scales and sources. To address this challenge, we propose MemOS, a memory operating system that treats memory as a manageable system resource. It unifies the representation, scheduling, and evolution of plaintext, activation-based, and parameter-level memories, enabling cost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates both memory content and metadata such as provenance and versioning. MemCubes can be composed, migrated, and fused over time, enabling flexible transitions between memory types and bridging retrieval with parameter-based learning. MemOS establishes a memory-centric system framework that brings controllability, plasticity, and evolvability to LLMs, laying the foundation for continual learning and personalized modeling.",
            "score": 12,
            "issue_id": 4693,
            "pub_date": "2025-07-04",
            "pub_date_card": {
                "ru": "4 Ğ¸ÑĞ»Ñ",
                "en": "July 4",
                "zh": "7æœˆ4æ—¥"
            },
            "hash": "5a64c779be945671",
            "authors": [
                "Zhiyu Li",
                "Shichao Song",
                "Chenyang Xi",
                "Hanyu Wang",
                "Chen Tang",
                "Simin Niu",
                "Ding Chen",
                "Jiawei Yang",
                "Chunyu Li",
                "Qingchen Yu",
                "Jihao Zhao",
                "Yezhaohui Wang",
                "Peng Liu",
                "Zehao Lin",
                "Pengyuan Wang",
                "Jiahao Huo",
                "Tianyi Chen",
                "Kai Chen",
                "Kehang Li",
                "Zhen Tao",
                "Junpeng Ren",
                "Huayi Lai",
                "Hao Wu",
                "Bo Tang",
                "Zhenren Wang",
                "Zhaoxin Fan",
                "Ningyu Zhang",
                "Linfeng Zhang",
                "Junchi Yan",
                "Mingchuan Yang",
                "Tong Xu",
                "Wei Xu",
                "Huajun Chen",
                "Haofeng Wang",
                "Hongkang Yang",
                "Wentao Zhang",
                "Zhi-Qin John Xu",
                "Siheng Chen",
                "Feiyu Xiong"
            ],
            "affiliations": [
                "Beihang University",
                "Institute for Advanced Algorithms Research, Shanghai",
                "MemTensor (Shanghai) Technology Co., Ltd.",
                "Peking University",
                "Renmin University of China",
                "Research Institute of China Telecom",
                "Shanghai Jiao Tong University",
                "Tongji University",
                "University of Science and Technology of China",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.03724.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#agi",
                    "#rag",
                    "#long_context",
                    "#optimization",
                    "#data"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "MemOS: Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "MemOS - ÑÑ‚Ğ¾ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. MemOS Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ MemCube ĞºĞ°Ğº Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ñ‹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰ĞµĞ¹ ĞºĞ°Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚, Ñ‚Ğ°Ğº Ğ¸ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ LLM."
                },
                "en": {
                    "title": "MemOS: Revolutionizing Memory Management for LLMs",
                    "desc": "MemOS is a proposed memory operating system designed to improve memory management in Large Language Models (LLMs). It addresses the limitations of existing models that rely on static parameters and short-term context by introducing a structured memory layer that enhances storage and retrieval capabilities. The system utilizes MemCubes, which encapsulate memory content along with metadata, allowing for flexible transitions between different memory types. This approach not only increases computational efficiency but also supports continual learning and personalized modeling by managing knowledge across various temporal scales."
                },
                "zh": {
                    "title": "MemOSï¼šä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æä¾›æ™ºèƒ½å†…å­˜ç®¡ç†",
                    "desc": "MemOSæ˜¯ä¸€ç§ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è®¾è®¡çš„å†…å­˜æ“ä½œç³»ç»Ÿï¼Œæ—¨åœ¨æ”¹å–„å†…å­˜ç®¡ç†ã€‚å®ƒé€šè¿‡ç»Ÿä¸€è¡¨ç¤ºã€è°ƒåº¦å’Œæ¼”å˜ä¸åŒç±»å‹çš„å†…å­˜ï¼Œæ”¯æŒé«˜æ•ˆçš„å­˜å‚¨å’Œæ£€ç´¢ã€‚MemOSå¼•å…¥äº†MemCubeä½œä¸ºåŸºæœ¬å•å…ƒï¼Œå°è£…äº†å†…å­˜å†…å®¹å’Œå…ƒæ•°æ®ï¼Œå…è®¸çµæ´»çš„å†…å­˜ç±»å‹è½¬æ¢ã€‚è¯¥ç³»ç»Ÿä¸ºLLMsæä¾›äº†å¯æ§æ€§ã€å¯å¡‘æ€§å’Œå¯æ¼”åŒ–æ€§ï¼Œä¿ƒè¿›äº†æŒç»­å­¦ä¹ å’Œä¸ªæ€§åŒ–å»ºæ¨¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.03253",
            "title": "RefineX: Learning to Refine Pre-training Data at Scale from\n  Expert-Guided Programs",
            "url": "https://huggingface.co/papers/2507.03253",
            "abstract": "RefineX is a scalable framework for improving the quality of large language model pre-training data through programmatic editing, yielding better performance than alternative methods across various downstream tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The foundational capabilities of large language models (LLMs) are deeply influenced by the quality of their pre-training corpora. However, enhancing data quality at scale remains a significant challenge, primarily due to the trade-off between refinement effectiveness and processing efficiency. While rule-based filtering remains the dominant paradigm, it typically operates at the document level and lacks the granularity needed to refine specific content within documents. Inspired by emerging work such as ProX, we propose RefineX, a novel framework for large-scale, surgical refinement of pre-training data through programmatic editing tasks. RefineX enables efficient and fine-grained data refinement while reliably preserving the diversity and naturalness of raw text. The core strength of RefineX lies in distilling high-quality, expert-guided end-to-end refinement results into minimal edit-based deletion programs. This high-precision distillation pipeline is used to train an efficient and reliable refine model that can systematically improve every instance in the corpus at scale. We evaluate RefineX across from-scratch pre-training at multiple model scales and find that it consistently outperforms models trained on raw, filtered, or alternatively refined data across diverse downstream tasks. On the 750M model, RefineX yields 2.6%-7.2% average gains on lighteval tasks, and achieves comparable performance using significantly fewer training tokens. Further analysis shows that RefineX reliably enhances text quality with both high efficiency and precision, outperforming prior approaches such as end-to-end generation and Prox-C. These results position RefineX as a scalable, effective, and reliable solution for optimizing pre-training data in modern LLM pipelines.",
            "score": 8,
            "issue_id": 4693,
            "pub_date": "2025-07-04",
            "pub_date_card": {
                "ru": "4 Ğ¸ÑĞ»Ñ",
                "en": "July 4",
                "zh": "7æœˆ4æ—¥"
            },
            "hash": "6f3d1aa17a4188e7",
            "authors": [
                "Baolong Bi",
                "Shenghua Liu",
                "Xingzhang Ren",
                "Dayiheng Liu",
                "Junyang Lin",
                "Yiwei Wang",
                "Lingrui Mei",
                "Junfeng Fang",
                "Jiafeng Guo",
                "Xueqi Cheng"
            ],
            "affiliations": [
                "Alibaba Group",
                "Institute of Computing Technology, Chinese Academy of Sciences",
                "National University of Singapore",
                "University of California, Merced"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.03253.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#data"
                ],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "RefineX: Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ˜Ğ˜",
                    "desc": "RefineX - ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°. RefineX Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€ Ğ² ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RefineX, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼."
                },
                "en": {
                    "title": "RefineX: Precision Editing for Superior Language Model Training",
                    "desc": "RefineX is a new framework designed to enhance the quality of pre-training data for large language models (LLMs) through targeted programmatic editing. It addresses the challenge of improving data quality at scale by allowing for precise modifications rather than broad document-level changes. This method preserves the diversity and naturalness of the text while ensuring efficient processing. Evaluations show that models trained with RefineX consistently outperform those trained on raw or traditionally refined data across various tasks, demonstrating its effectiveness in optimizing pre-training data."
                },
                "zh": {
                    "title": "RefineXï¼šæå‡é¢„è®­ç»ƒæ•°æ®è´¨é‡çš„å¯æ‰©å±•æ¡†æ¶",
                    "desc": "RefineXæ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç¨‹åºåŒ–ç¼–è¾‘æé«˜å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæ•°æ®çš„è´¨é‡ã€‚è¯¥æ¡†æ¶è§£å†³äº†æ•°æ®è´¨é‡æå‡ä¸å¤„ç†æ•ˆç‡ä¹‹é—´çš„æƒè¡¡é—®é¢˜ï¼Œèƒ½å¤Ÿè¿›è¡Œé«˜æ•ˆä¸”ç»†è‡´çš„æ•°æ®ç²¾ç‚¼ã€‚RefineXé€šè¿‡æœ€å°åŒ–ç¼–è¾‘çš„åˆ é™¤ç¨‹åºï¼Œæç‚¼å‡ºé«˜è´¨é‡çš„ä¸“å®¶æŒ‡å¯¼çš„ç«¯åˆ°ç«¯ç²¾ç‚¼ç»“æœï¼Œä»è€Œç³»ç»Ÿæ€§åœ°æ”¹å–„è¯­æ–™åº“ä¸­çš„æ¯ä¸ªå®ä¾‹ã€‚å®éªŒè¡¨æ˜ï¼ŒRefineXåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä½¿ç”¨åŸå§‹ã€è¿‡æ»¤æˆ–å…¶ä»–ç²¾ç‚¼æ•°æ®è®­ç»ƒçš„æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.03483",
            "title": "BMMR: A Large-Scale Bilingual Multimodal Multi-Discipline Reasoning\n  Dataset",
            "url": "https://huggingface.co/papers/2507.03483",
            "abstract": "A large-scale dataset and verification tool are introduced for assessing and improving cross-disciplinary reasoning capabilities in multimodal models.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce BMMR, a large-scale bilingual, multimodal, multi-disciplinary reasoning dataset for the community to develop and evaluate large multimodal models (LMMs). BMMR comprises 110k college-level questions spanning 300 UNESCO-defined subjects, spanning diverse formats-multiple-choice, fill-in-the-blank, and open-ended QA-and sourced from both print and digital media such as books, exams, and quizzes. All data are curated and filtered via a human-in-the-loop and scalable framework, and each instance is paired with a high-quality reasoning path. The dataset is organized into two parts: BMMR-Eval that comprises 20,458 high-quality instances to comprehensively assess LMMs' knowledge and reasoning across multiple disciplines in both Chinese and English; and BMMR-Train that contains 88,991 instances to support further research and development, extending the current focus on mathematical reasoning to diverse disciplines and domains. In addition, we propose the process-based multi-discipline verifier (i.e., BMMR-Verifier) for accurate and fine-grained evaluation of reasoning paths. Extensive experiments on 24 models reveal that (i) even SOTA models (e.g., o3 and Gemini-2.5-Pro) leave substantial headroom on BMMR-Eval; (ii) reasoning models exhibit discipline bias and outperform LMMs only on specific subjects; (iii) open-source models still trail their proprietary counterparts; and (iv) fine-tuning on BMMR-Train narrows this gap. Additionally, we conduct reasoning-chain analyses using BMMR-Verifier and other in-depth studies, uncovering the challenges LMMs currently face in multidisciplinary reasoning. We will release the data, and we hope our work can offer insights and contributions to the community.",
            "score": 7,
            "issue_id": 4693,
            "pub_date": "2025-07-04",
            "pub_date_card": {
                "ru": "4 Ğ¸ÑĞ»Ñ",
                "en": "July 4",
                "zh": "7æœˆ4æ—¥"
            },
            "hash": "a916ca78a2bd6196",
            "authors": [
                "Zhiheng Xi",
                "Guanyu Li",
                "Yutao Fan",
                "Honglin Guo",
                "Yufang Liu",
                "Xiaoran Fan",
                "Jiaqi Liu",
                "Jingchao Ding",
                "Wangmeng Zuo",
                "Zhenfei Yin",
                "Lei Bai",
                "Tao Ji",
                "Tao Gui",
                "Qi Zhang",
                "Xuanjing Huang"
            ],
            "affiliations": [
                "East China Normal University",
                "Fudan University",
                "Harbin Institute of Technology",
                "Oxford",
                "Shanghai AI Laboratory",
                "University of Sydney",
                "Yimudata"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.03483.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#data"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ BMMR - Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ…. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 110 Ñ‚Ñ‹ÑÑÑ‡ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚ÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ğ¾ 300 Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ°Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚ĞµÑÑ‚Ñ‹, Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ¼ Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ BMMR-Verifier Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning with BMMR Dataset",
                    "desc": "This paper presents BMMR, a comprehensive dataset designed to enhance the reasoning abilities of large multimodal models (LMMs) across various disciplines. It includes 110,000 college-level questions from 300 subjects, formatted in multiple-choice, fill-in-the-blank, and open-ended styles, sourced from both print and digital media. The dataset is divided into BMMR-Eval for evaluation and BMMR-Train for training, with a focus on improving reasoning in diverse domains beyond just mathematics. Additionally, the authors introduce BMMR-Verifier, a tool for detailed assessment of reasoning paths, revealing significant gaps in current models' performance and highlighting the need for further research in multidisciplinary reasoning."
                },
                "zh": {
                    "title": "æ¨åŠ¨å¤šæ¨¡æ€æ¨¡å‹çš„è·¨å­¦ç§‘æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†BMMRï¼Œä¸€ä¸ªå¤§è§„æ¨¡çš„åŒè¯­ã€å¤šæ¨¡æ€ã€å¤šå­¦ç§‘æ¨ç†æ•°æ®é›†ï¼Œæ—¨åœ¨å¸®åŠ©å¼€å‘å’Œè¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ã€‚è¯¥æ•°æ®é›†åŒ…å«110,000ä¸ªå¤§å­¦æ°´å¹³çš„é—®é¢˜ï¼Œæ¶µç›–300ä¸ªè”åˆå›½æ•™ç§‘æ–‡ç»„ç»‡å®šä¹‰çš„å­¦ç§‘ï¼Œé—®é¢˜å½¢å¼å¤šæ ·ï¼ŒåŒ…æ‹¬é€‰æ‹©é¢˜ã€å¡«ç©ºé¢˜å’Œå¼€æ”¾å¼é—®ç­”ã€‚æ•°æ®ç»è¿‡äººå·¥ç­›é€‰å’Œè¿‡æ»¤ï¼Œå¹¶ä¸ºæ¯ä¸ªå®ä¾‹é…å¤‡é«˜è´¨é‡çš„æ¨ç†è·¯å¾„ï¼Œåˆ†ä¸ºBMMR-Evalå’ŒBMMR-Trainä¸¤éƒ¨åˆ†ï¼Œä»¥æ”¯æŒå¤šå­¦ç§‘çŸ¥è¯†å’Œæ¨ç†çš„è¯„ä¼°ä¸ç ”ç©¶ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†åŸºäºè¿‡ç¨‹çš„å¤šå­¦ç§‘éªŒè¯å™¨ï¼ˆBMMR-Verifierï¼‰ï¼Œç”¨äºå¯¹æ¨ç†è·¯å¾„è¿›è¡Œå‡†ç¡®å’Œç»†è‡´çš„è¯„ä¼°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.05163",
            "title": "4DSloMo: 4D Reconstruction for High Speed Scene with Asynchronous\n  Capture",
            "url": "https://huggingface.co/papers/2507.05163",
            "abstract": "A high-speed 4D capturing system using low FPS cameras with asynchronous capture and video-diffusion-based artifact correction enhances reconstruction quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Reconstructing fast-dynamic scenes from multi-view videos is crucial for high-speed motion analysis and realistic 4D reconstruction. However, the majority of 4D capture systems are limited to frame rates below 30 FPS (frames per second), and a direct 4D reconstruction of high-speed motion from low FPS input may lead to undesirable results. In this work, we propose a high-speed 4D capturing system only using low FPS cameras, through novel capturing and processing modules. On the capturing side, we propose an asynchronous capture scheme that increases the effective frame rate by staggering the start times of cameras. By grouping cameras and leveraging a base frame rate of 25 FPS, our method achieves an equivalent frame rate of 100-200 FPS without requiring specialized high-speed cameras. On processing side, we also propose a novel generative model to fix artifacts caused by 4D sparse-view reconstruction, as asynchrony reduces the number of viewpoints at each timestamp. Specifically, we propose to train a video-diffusion-based artifact-fix model for sparse 4D reconstruction, which refines missing details, maintains temporal consistency, and improves overall reconstruction quality. Experimental results demonstrate that our method significantly enhances high-speed 4D reconstruction compared to synchronous capture.",
            "score": 2,
            "issue_id": 4693,
            "pub_date": "2025-07-07",
            "pub_date_card": {
                "ru": "7 Ğ¸ÑĞ»Ñ",
                "en": "July 7",
                "zh": "7æœˆ7æ—¥"
            },
            "hash": "e1f4c8e83495db53",
            "authors": [
                "Yutian Chen",
                "Shi Guo",
                "Tianshuo Yang",
                "Lihe Ding",
                "Xiuyuan Yu",
                "Jinwei Gu",
                "Tianfan Xue"
            ],
            "affiliations": [
                "NVIDIA",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.05163.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#3d",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ’Ñ‹ÑĞ¾ĞºĞ¾ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ½Ğ°Ñ 4D-ÑÑŠĞµĞ¼ĞºĞ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ 4D-ÑÑŠĞµĞ¼ĞºĞ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ¾Ğ¹ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ¾Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñƒ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ´Ğ¾ 100-200 FPS Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑÑŠĞµĞ¼ĞºĞ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ĞºĞ°Ğ¼ĞµÑ€. Ğ”Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ², Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾ Ğ¼Ğ°Ğ»Ğ¾Ğ¼Ñƒ Ñ‡Ğ¸ÑĞ»Ñƒ Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ², Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑÑŠĞµĞ¼ĞºĞ¾Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing 4D Capture: High-Speed Reconstruction with Low FPS Cameras",
                    "desc": "This paper presents a novel system for capturing high-speed 4D scenes using low frame rate cameras. It introduces an asynchronous capture technique that effectively increases the frame rate by staggering the start times of multiple cameras, achieving rates of 100-200 FPS from a base of 25 FPS. Additionally, the authors propose a video-diffusion-based generative model to correct artifacts in the sparse 4D reconstruction, ensuring better detail and temporal consistency. Experimental results show that this approach significantly improves the quality of high-speed 4D reconstructions compared to traditional synchronous methods."
                },
                "zh": {
                    "title": "ä½å¸§ç‡ç›¸æœºå®ç°é«˜é€Ÿåº¦4Dé‡å»ºçš„åˆ›æ–°æ–¹æ¡ˆ",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é«˜é€Ÿåº¦çš„4Dæ•æ‰ç³»ç»Ÿï¼Œåˆ©ç”¨ä½å¸§ç‡ç›¸æœºè¿›è¡Œå¼‚æ­¥æ•æ‰å’Œè§†é¢‘æ‰©æ•£åŸºç¡€çš„ä¼ªå½±ä¿®æ­£ï¼Œä»è€Œæé«˜é‡å»ºè´¨é‡ã€‚ä¼ ç»Ÿçš„4Dæ•æ‰ç³»ç»Ÿé€šå¸¸å¸§ç‡ä½äº30 FPSï¼Œç›´æ¥ä»ä½å¸§ç‡è¾“å…¥è¿›è¡Œé«˜é€Ÿåº¦è¿åŠ¨çš„4Dé‡å»ºä¼šå¯¼è‡´ä¸ç†æƒ³çš„ç»“æœã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¼‚æ­¥æ•æ‰æ–¹æ¡ˆï¼Œå°†ç›¸æœºçš„å¯åŠ¨æ—¶é—´é”™å¼€ï¼Œæå‡äº†æœ‰æ•ˆå¸§ç‡ï¼Œè¾¾åˆ°100-200 FPSçš„æ•ˆæœã€‚å¤„ç†æ–¹é¢ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè§†é¢‘æ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹ï¼Œä¿®å¤4Dç¨€ç–è§†å›¾é‡å»ºä¸­äº§ç”Ÿçš„ä¼ªå½±ï¼Œæ˜¾è‘—æ”¹å–„äº†é‡å»ºçš„ç»†èŠ‚å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.04036",
            "title": "PresentAgent: Multimodal Agent for Presentation Video Generation",
            "url": "https://huggingface.co/papers/2507.04036",
            "abstract": "A multimodal agent transforms documents into detailed presentation videos with audio, evaluated using a comprehensive framework involving vision-language models.  \t\t\t\t\tAI-generated summary \t\t\t\t We present PresentAgent, a multimodal agent that transforms long-form documents into narrated presentation videos. While existing approaches are limited to generating static slides or text summaries, our method advances beyond these limitations by producing fully synchronized visual and spoken content that closely mimics human-style presentations. To achieve this integration, PresentAgent employs a modular pipeline that systematically segments the input document, plans and renders slide-style visual frames, generates contextual spoken narration with large language models and Text-to-Speech models, and seamlessly composes the final video with precise audio-visual alignment. Given the complexity of evaluating such multimodal outputs, we introduce PresentEval, a unified assessment framework powered by Vision-Language Models that comprehensively scores videos across three critical dimensions: content fidelity, visual clarity, and audience comprehension through prompt-based evaluation. Our experimental validation on a curated dataset of 30 document-presentation pairs demonstrates that PresentAgent approaches human-level quality across all evaluation metrics. These results highlight the significant potential of controllable multimodal agents in transforming static textual materials into dynamic, effective, and accessible presentation formats. Code will be available at https://github.com/AIGeeksGroup/PresentAgent.",
            "score": 0,
            "issue_id": 4693,
            "pub_date": "2025-07-05",
            "pub_date_card": {
                "ru": "5 Ğ¸ÑĞ»Ñ",
                "en": "July 5",
                "zh": "7æœˆ5æ—¥"
            },
            "hash": "79b10f5eed3bd7e4",
            "authors": [
                "Jingwei Shi",
                "Zeyu Zhang",
                "Biao Wu",
                "Yanjie Liang",
                "Meng Fang",
                "Ling Chen",
                "Yang Zhao"
            ],
            "affiliations": [
                "AI Geeks, Australia",
                "Australian Artificial Intelligence Institute, Australia",
                "La Trobe University, Australia",
                "University of Liverpool, United Kingdom"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.04036.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#agents",
                    "#optimization",
                    "#dataset",
                    "#benchmark",
                    "#games",
                    "#interpretability"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PresentAgent - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¾Ğ·Ğ²ÑƒÑ‡ĞºĞ¾Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ°Ğ¹Ğ´Ğ¾Ğ², Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½Ğ¾Ğ²ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº PresentEval Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ PresentAgent Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ Ğº ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸."
                },
                "en": {
                    "title": "Transforming Text into Engaging Videos with PresentAgent",
                    "desc": "PresentAgent is a multimodal agent designed to convert long documents into engaging presentation videos with synchronized audio. Unlike traditional methods that only create static slides or text summaries, this approach generates dynamic visual and spoken content that resembles human presentations. It utilizes a modular pipeline for document segmentation, slide rendering, and narration generation, ensuring high-quality audio-visual alignment. The effectiveness of PresentAgent is evaluated using PresentEval, a framework that assesses video quality based on content fidelity, visual clarity, and audience comprehension, demonstrating its potential to enhance the accessibility of information."
                },
                "zh": {
                    "title": "å°†æ–‡æ¡£è½¬åŒ–ä¸ºç”ŸåŠ¨æ¼”ç¤ºçš„æ™ºèƒ½ä½“",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPresentAgentçš„å¤šæ¨¡æ€æ™ºèƒ½ä½“ï¼Œå®ƒèƒ½å¤Ÿå°†é•¿ç¯‡æ–‡æ¡£è½¬åŒ–ä¸ºå¸¦æœ‰æ—ç™½çš„æ¼”ç¤ºè§†é¢‘ã€‚ä¸ç°æœ‰æ–¹æ³•ä»…èƒ½ç”Ÿæˆé™æ€å¹»ç¯ç‰‡æˆ–æ–‡æœ¬æ‘˜è¦ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆä¸äººç±»æ¼”ç¤ºé£æ ¼ç›¸ä¼¼çš„åŒæ­¥è§†è§‰å’Œè¯­éŸ³å†…å®¹ã€‚PresentAgenté‡‡ç”¨æ¨¡å—åŒ–æµç¨‹ï¼Œç³»ç»Ÿåœ°å¯¹è¾“å…¥æ–‡æ¡£è¿›è¡Œåˆ†æ®µï¼Œè§„åˆ’å’Œæ¸²æŸ“å¹»ç¯ç‰‡é£æ ¼çš„è§†è§‰æ¡†æ¶ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³çš„æ—ç™½ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†PresentEvalè¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡è§†è§‰-è¯­è¨€æ¨¡å‹å¯¹è§†é¢‘è¿›è¡Œå…¨é¢è¯„åˆ†ï¼ŒéªŒè¯äº†PresentAgentåœ¨å†…å®¹çœŸå®æ€§ã€è§†è§‰æ¸…æ™°åº¦å’Œè§‚ä¼—ç†è§£åŠ›ç­‰æ–¹é¢æ¥è¿‘äººç±»æ°´å¹³çš„è´¨é‡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-07-07.html",
    "link_next": "2025-07-09.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "07.07",
        "en": "07/07",
        "zh": "7æœˆ7æ—¥"
    },
    "short_date_next": {
        "ru": "09.07",
        "en": "07/09",
        "zh": "7æœˆ9æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 3,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}