{
    "date": {
        "ru": "7 января",
        "en": "January 7",
        "zh": "1月7日"
    },
    "time_utc": "2025-01-07 03:17",
    "weekday": 1,
    "issue_id": 1527,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.02976",
            "title": "STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution",
            "url": "https://huggingface.co/papers/2501.02976",
            "abstract": "Image diffusion models have been adapted for real-world video super-resolution to tackle over-smoothing issues in GAN-based methods. However, these models struggle to maintain temporal consistency, as they are trained on static images, limiting their ability to capture temporal dynamics effectively. Integrating text-to-video (T2V) models into video super-resolution for improved temporal modeling is straightforward. However, two key challenges remain: artifacts introduced by complex degradations in real-world scenarios, and compromised fidelity due to the strong generative capacity of powerful T2V models (e.g., CogVideoX-5B). To enhance the spatio-temporal quality of restored videos, we introduce~\\name (Spatial-Temporal Augmentation with T2V models for Real-world video super-resolution), a novel approach that leverages T2V models for real-world video super-resolution, achieving realistic spatial details and robust temporal consistency. Specifically, we introduce a Local Information Enhancement Module (LIEM) before the global attention block to enrich local details and mitigate degradation artifacts. Moreover, we propose a Dynamic Frequency (DF) Loss to reinforce fidelity, guiding the model to focus on different frequency components across diffusion steps. Extensive experiments demonstrate~\\name~outperforms state-of-the-art methods on both synthetic and real-world datasets.",
            "score": 1,
            "issue_id": 1527,
            "pub_date": "2025-01-06",
            "pub_date_card": {
                "ru": "6 января",
                "en": "January 6",
                "zh": "1月6日"
            },
            "hash": "13ac412646c508f5",
            "authors": [
                "Rui Xie",
                "Yinhong Liu",
                "Penghao Zhou",
                "Chen Zhao",
                "Jun Zhou",
                "Kai Zhang",
                "Zhenyu Zhang",
                "Jian Yang",
                "Zhenheng Yang",
                "Ying Tai"
            ],
            "affiliations": [
                "ByteDance",
                "Nanjing University",
                "Southwest University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02976.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#diffusion",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Качественное суперразрешение видео с помощью T2V моделей",
                    "desc": "Представлена новая методика STAR для суперразрешения видео в реальных условиях с использованием моделей text-to-video. Предложен модуль LIEM для улучшения локальных деталей и устранения артефактов деградации. Введена функция потерь Dynamic Frequency для усиления точности восстановления на разных частотах. Эксперименты показывают превосходство STAR над современными методами на синтетических и реальных датасетах."
                },
                "en": {
                    "title": "Enhancing Video Quality with T2V Models for Real-World Super-Resolution",
                    "desc": "This paper presents a new method called Spatial-Temporal Augmentation with T2V models for Real-world video super-resolution, which aims to improve video quality by addressing issues of over-smoothing and temporal consistency. Traditional image diffusion models struggle with video because they are designed for static images, leading to challenges in capturing motion dynamics. The proposed approach incorporates a Local Information Enhancement Module to enhance local details and reduce artifacts, along with a Dynamic Frequency Loss to maintain fidelity across different frequency components. Experimental results show that this method outperforms existing techniques in both synthetic and real-world scenarios, providing better spatial and temporal quality in restored videos."
                },
                "zh": {
                    "title": "提升视频超分辨率的时空一致性",
                    "desc": "本文提出了一种新方法，名为~\\name~，用于提高真实世界视频超分辨率的时空质量。该方法结合了文本到视频（T2V）模型，以解决传统生成对抗网络（GAN）方法中的过平滑问题。通过引入局部信息增强模块（LIEM）和动态频率损失（DF Loss），该方法能够有效改善视频的局部细节和时间一致性。实验结果表明，~\\name~在合成和真实世界数据集上均优于现有的最先进方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.02157",
            "title": "Personalized Graph-Based Retrieval for Large Language Models",
            "url": "https://huggingface.co/papers/2501.02157",
            "abstract": "As large language models (LLMs) evolve, their ability to deliver personalized and context-aware responses offers transformative potential for improving user experiences. Existing personalization approaches, however, often rely solely on user history to augment the prompt, limiting their effectiveness in generating tailored outputs, especially in cold-start scenarios with sparse data. To address these limitations, we propose Personalized Graph-based Retrieval-Augmented Generation (PGraphRAG), a framework that leverages user-centric knowledge graphs to enrich personalization. By directly integrating structured user knowledge into the retrieval process and augmenting prompts with user-relevant context, PGraphRAG enhances contextual understanding and output quality. We also introduce the Personalized Graph-based Benchmark for Text Generation, designed to evaluate personalized text generation tasks in real-world settings where user history is sparse or unavailable. Experimental results show that PGraphRAG significantly outperforms state-of-the-art personalization methods across diverse tasks, demonstrating the unique advantages of graph-based retrieval for personalization.",
            "score": 1,
            "issue_id": 1527,
            "pub_date": "2025-01-04",
            "pub_date_card": {
                "ru": "4 января",
                "en": "January 4",
                "zh": "1月4日"
            },
            "hash": "65e3736cfc1e3295",
            "authors": [
                "Steven Au",
                "Cameron J. Dimacali",
                "Ojasmitha Pedirappagari",
                "Namyong Park",
                "Franck Dernoncourt",
                "Yu Wang",
                "Nikos Kanakaris",
                "Hanieh Deilamsalehy",
                "Ryan A. Rossi",
                "Nesreen K. Ahmed"
            ],
            "affiliations": [
                "Adobe Research",
                "Cisco AI Research",
                "Meta AI",
                "University of California Santa Cruz",
                "University of Oregon",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.02157.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#optimization",
                    "#graphs",
                    "#multimodal",
                    "#benchmark",
                    "#games"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "Графы знаний на службе персонализации языковых моделей",
                    "desc": "Статья представляет новый подход к персонализации ответов больших языковых моделей (LLM) под названием PGraphRAG. В отличие от существующих методов, полагающихся на историю пользователя, PGraphRAG использует ориентированные на пользователя графы знаний для обогащения контекста. Этот метод улучшает понимание контекста и качество генерируемых ответов, особенно в сценариях с ограниченными данными о пользователе. Экспериментальные результаты показывают, что PGraphRAG превосходит современные методы персонализации в различных задачах."
                },
                "en": {
                    "title": "Revolutionizing Personalization with Graph-based Retrieval",
                    "desc": "This paper introduces a new framework called Personalized Graph-based Retrieval-Augmented Generation (PGraphRAG) that enhances the personalization of large language models (LLMs). Unlike traditional methods that depend only on user history, PGraphRAG utilizes user-centric knowledge graphs to provide richer context for generating responses. By integrating structured user information into the retrieval process, it improves the model's understanding and the quality of its outputs, especially in situations where user data is limited. The authors also present a benchmark for evaluating personalized text generation, showing that PGraphRAG outperforms existing methods in various tasks."
                },
                "zh": {
                    "title": "个性化图谱提升生成质量",
                    "desc": "随着大型语言模型的发展，它们在提供个性化和上下文感知的响应方面展现出巨大的潜力。现有的个性化方法通常仅依赖用户历史数据来增强提示，这在数据稀疏的冷启动场景中效果有限。为了解决这些问题，我们提出了个性化图谱检索增强生成（PGraphRAG）框架，利用以用户为中心的知识图谱来丰富个性化。实验结果表明，PGraphRAG在多种任务中显著优于现有的个性化方法，展示了基于图谱的检索在个性化中的独特优势。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.03006",
            "title": "TransPixar: Advancing Text-to-Video Generation with Transparency",
            "url": "https://huggingface.co/papers/2501.03006",
            "abstract": "Text-to-video generative models have made significant strides, enabling diverse applications in entertainment, advertising, and education. However, generating RGBA video, which includes alpha channels for transparency, remains a challenge due to limited datasets and the difficulty of adapting existing models. Alpha channels are crucial for visual effects (VFX), allowing transparent elements like smoke and reflections to blend seamlessly into scenes. We introduce TransPixar, a method to extend pretrained video models for RGBA generation while retaining the original RGB capabilities. TransPixar leverages a diffusion transformer (DiT) architecture, incorporating alpha-specific tokens and using LoRA-based fine-tuning to jointly generate RGB and alpha channels with high consistency. By optimizing attention mechanisms, TransPixar preserves the strengths of the original RGB model and achieves strong alignment between RGB and alpha channels despite limited training data. Our approach effectively generates diverse and consistent RGBA videos, advancing the possibilities for VFX and interactive content creation.",
            "score": 1,
            "issue_id": 1527,
            "pub_date": "2025-01-06",
            "pub_date_card": {
                "ru": "6 января",
                "en": "January 6",
                "zh": "1月6日"
            },
            "hash": "e85e5fa9a03d5d04",
            "authors": [
                "Luozhou Wang",
                "Yijun Li",
                "Zhifei Chen",
                "Jui-Hsien Wang",
                "Zhifei Zhang",
                "He Zhang",
                "Zhe Lin",
                "Yingcong Chen"
            ],
            "affiliations": [
                "Adobe Research",
                "HKUST",
                "HKUST(GZ)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.03006.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#training",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "TransPixar: Прорыв в генерации RGBA-видео для визуальных эффектов",
                    "desc": "TransPixar - это новый метод генерации RGBA-видео, расширяющий возможности предобученных видеомоделей. Он использует архитектуру диффузионного трансформера (DiT) и токены, специфичные для альфа-канала, для совместной генерации RGB и альфа-каналов с высокой согласованностью. Метод применяет тонкую настройку на основе LoRA и оптимизирует механизмы внимания для сохранения сильных сторон исходной RGB-модели. TransPixar эффективно генерирует разнообразные и согласованные RGBA-видео, открывая новые возможности для создания визуальных эффектов и интерактивного контента."
                },
                "en": {
                    "title": "TransPixar: Bridging RGB and Alpha for Enhanced Video Generation",
                    "desc": "This paper presents TransPixar, a novel method for generating RGBA videos, which include transparency information crucial for visual effects. The challenge lies in the limited datasets and the need to adapt existing models to handle alpha channels effectively. TransPixar utilizes a diffusion transformer architecture and incorporates alpha-specific tokens, allowing it to generate both RGB and alpha channels simultaneously. By optimizing attention mechanisms and employing LoRA-based fine-tuning, TransPixar achieves high consistency between RGB and alpha outputs, enhancing the quality of video generation for applications in VFX and interactive media."
                },
                "zh": {
                    "title": "TransPixar：生成高质量RGBA视频的新方法",
                    "desc": "本文介绍了一种名为TransPixar的方法，旨在生成包含透明通道的RGBA视频。传统的视频生成模型在处理透明效果时面临挑战，TransPixar通过扩展预训练模型来解决这一问题。该方法利用扩散变换器架构，结合特定的透明通道标记，并通过LoRA微调实现RGB和透明通道的高一致性生成。最终，TransPixar在有限的数据集上优化了注意力机制，成功生成多样且一致的RGBA视频，推动了视觉特效和互动内容创作的可能性。"
                }
            }
        }
    ],
    "link_prev": "2025-01-06.html",
    "link_next": "2025-01-08.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "06.01",
        "en": "01/06",
        "zh": "1月6日"
    },
    "short_date_next": {
        "ru": "08.01",
        "en": "01/08",
        "zh": "1月8日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "This article introduces EnerVerse, a framework for generating future space in robotic tasks. It uses attention mechanisms for consistent space modeling and a memory context for long sequence generation. The FAV space enhances robot observation and adaptability. A data engine with 4DGS improves data quality and diversity. Experiments show it boosts performance in long-range robotic tasks.",
        "title": "EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation",
        "pinyin": "Sure, here is the pinyin transcription for the given text:\n\nZhè wénzhāng jièshào EnerVerse, yīgè kuàngjià yǐnqǐ wèilái kōngjiān zài jīqirén rénwù zhōng. Tā shǐyòng zhùyì jīzhì wèi yuáncháng kōngjiān móxíng hé yīgè jìyì qūjiàn wèi cháng xùliè shēngchéng. FAV kōngjiān zēngqiáng jīqirén guānchá hé shìyìngxìng. Yīgè shùjù yǐnqíng yǒu 4DGS gǎishàn shùjù zhìliàng hé duōyàngxìng. Shíyàn xiǎnshì tā zēngqiáng xiàoguǒ zài chángqī jīqirén rénwù zhōng.\n\nPlease note that the pinyin transcription is based on the pronunciation of the Chinese characters that would be used to translate the English text. The actual translation might vary slightly depending on the context and specific terminology used.",
        "vocab": "[\n    {\"word\": \"framework\", \"pinyin\": \"kuàngjià\", \"trans\": \"框架\"},\n    {\"word\": \"generating\", \"pinyin\": \"shēngchéng\", \"trans\": \"生成\"},\n    {\"word\": \"future\", \"pinyin\": \"wèilái\", \"trans\": \"未来\"},\n    {\"word\": \"space\", \"pinyin\": \"kōngjiān\", \"trans\": \"空间\"},\n    {\"word\": \"robotic\", \"pinyin\": \"jīqirén\", \"trans\": \"机器人\"},\n    {\"word\": \"tasks\", \"pinyin\": \"rènwù\", \"trans\": \"任务\"},\n    {\"word\": \"attention\", \"pinyin\": \"zhùyì\", \"trans\": \"注意\"},\n    {\"word\": \"mechanisms\", \"pinyin\": \"jīzhì\", \"trans\": \"机制\"},\n    {\"word\": \"consistent\", \"pinyin\": \"wúguǒ\", \"trans\": \"一致\"},\n    {\"word\": \"modeling\", \"pinyin\": \"móxíng\", \"trans\": \"建模\"},\n    {\"word\": \"memory\", \"pinyin\": \"jìyì\", \"trans\": \"记忆\"},\n    {\"word\": \"context\", \"pinyin\": \"qǔwén\", \"trans\": \"上下文\"},\n    {\"word\": \"sequence\", \"pinyin\": \"xùliè\", \"trans\": \"序列\"},\n    {\"word\": \"generation\", \"pinyin\": \"shēngchéng\", \"trans\": \"生成\"},\n    {\"word\": \"FAV\", \"pinyin\": \"Fēi-Ēi-Wēi\", \"trans\": \"FAV\"},\n    {\"word\": \"enhances\", \"pinyin\": \"zēngqiáng\", \"trans\": \"增强\"},\n    {\"word\": \"observation\", \"pinyin\": \"guānchá\", \"trans\": \"观察\"},\n    {\"word\": \"adaptability\", \"pinyin\": \"shìyìngxìng\", \"trans\": \"适应性\"},\n    {\"word\": \"engine\", \"pinyin\": \"yǐnqíng\", \"trans\": \"引擎\"},\n    {\"word\": \"4DGS\", \"pinyin\": \"Sì-Dī-Jī-Ēs\", \"trans\": \"4DGS\"},\n    {\"word\": \"improves\", \"pinyin\": \"gǎishàn\", \"trans\": \"改善\"},\n    {\"word\": \"quality\", \"pinyin\": \"zhìliàng\", \"trans\": \"质量\"},\n    {\"word\": \"diversity\", \"pinyin\": \"duōyàngxìng\", \"trans\": \"多样性\"},\n    {\"word\": \"experiments\", \"pinyin\": \"shíyàn\", \"trans\": \"实验\"},\n    {\"word\": \"show\", \"pinyin\": \"xiǎnshì\", \"trans\": \"显示\"},\n    {\"word\": \"boosts\", \"pinyin\": \"zēngqiáng\", \"trans\": \"增强\"},\n    {\"word\": \"performance\", \"pinyin\": \"biǎoxiàn\", \"trans\": \"表现\"},\n    {\"word\": \"long-range\", \"pinyin\": \"chángyuǎn\", \"trans\": \"长远\"}\n]",
        "trans": "This article introduces EnerVerse, a framework designed to generate future space in robotic tasks. It employs attention mechanisms to ensure consistent space modeling and utilizes a memory context for generating long sequences. The FAV space enhances robot observation capabilities and adaptability. Additionally, a data engine equipped with 4DGS improves the quality and diversity of data. Experiments demonstrate that it significantly boosts performance in long-range robotic tasks.",
        "update_ts": "2025-01-06 09:11"
    }
}