{
    "date": {
        "ru": "11 июля",
        "en": "July 11",
        "zh": "7月11日"
    },
    "time_utc": "2025-07-11 04:30",
    "weekday": 4,
    "issue_id": 4763,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.07966",
            "title": "Scaling RL to Long Videos",
            "url": "https://huggingface.co/papers/2507.07966",
            "abstract": "A framework for scaling vision-language models to long videos using reinforcement learning, achieving strong performance on various reasoning tasks with a specialized training infrastructure.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 52K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves strong performance on long video QA benchmarks such as VideoMME. It also outperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal reasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on our LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent performance gains as the number of input video frames scales. LongVILA-R1 marks a firm step towards long video reasoning in VLMs. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames / around 256k tokens).",
            "score": 49,
            "issue_id": 4761,
            "pub_date": "2025-07-10",
            "pub_date_card": {
                "ru": "10 июля",
                "en": "July 10",
                "zh": "7月10日"
            },
            "hash": "4ab23da398f0e8d8",
            "authors": [
                "Yukang Chen",
                "Wei Huang",
                "Baifeng Shi",
                "Qinghao Hu",
                "Hanrong Ye",
                "Ligeng Zhu",
                "Zhijian Liu",
                "Pavlo Molchanov",
                "Jan Kautz",
                "Xiaojuan Qi",
                "Sifei Liu",
                "Hongxu Yin",
                "Yao Lu",
                "Song Han"
            ],
            "affiliations": [
                "HKU",
                "MIT",
                "NVIDIA",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07966.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#reasoning",
                    "#multimodal",
                    "#rl",
                    "#open_source",
                    "#training",
                    "#dataset",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Революция в понимании длинных видео с помощью ИИ",
                    "desc": "Представлена полноценная система для масштабирования моделей визуально-языкового понимания на длинные видео с использованием обучения с подкреплением. Система включает большой датасет LongVideo-Reason с 52 тысячами пар вопрос-ответ по длинным видео, двухэтапный процесс обучения с цепочкой рассуждений и RL, а также специализированную инфраструктуру обучения MR-SP. Модель LongVILA-R1-7B демонстрирует высокую производительность на различных задачах рассуждения по длинным видео. Предложенный подход позволяет эффективно масштабировать визуально-языковые модели для работы с длинными видео."
                },
                "en": {
                    "title": "Scaling Vision-Language Models for Long Video Reasoning",
                    "desc": "This paper presents a framework designed to enhance vision-language models (VLMs) for reasoning tasks involving long videos. It introduces a large dataset called LongVideo-Reason, which contains 52,000 question-answer pairs related to long videos, facilitating high-quality reasoning across various domains. The framework employs a two-stage training process that combines chain-of-thought supervised fine-tuning with reinforcement learning, optimizing the model's performance. Additionally, it features a specialized training infrastructure, Multi-modal Reinforcement Sequence Parallelism, which significantly accelerates the training process for long video reasoning tasks."
                },
                "zh": {
                    "title": "长视频推理的新突破",
                    "desc": "本文提出了一种框架，用于通过强化学习将视觉-语言模型（VLMs）扩展到长视频推理。我们整合了三个关键组件：一个包含52K长视频问答对的大规模数据集LongVideo-Reason，一个两阶段的训练流程，以及一个名为多模态强化序列并行（MR-SP）的训练基础设施。实验结果表明，LongVILA-R1-7B在长视频问答基准上表现优异，并在时间推理、目标和目的推理、空间推理等方面超越了其他模型。我们的系统在长视频强化学习训练中实现了高达2.1倍的加速，标志着在VLMs中进行长视频推理的坚实一步。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07999",
            "title": "Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and\n  Methodology",
            "url": "https://huggingface.co/papers/2507.07999",
            "abstract": "TreeBench evaluates visual grounded reasoning through subtle target detection, traceable evidence, and second-order reasoning, while TreeVGR enhances this with joint localization and reasoning using reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically referencing visual regions, just like human \"thinking with images\". However, no benchmark exists to evaluate these capabilities holistically. To bridge this gap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a diagnostic benchmark built on three principles: (1) focused visual perception of subtle targets in complex scenes, (2) traceable evidence via bounding box evaluation, and (3) second-order reasoning to test object interactions and spatial hierarchies beyond simple object localization. Prioritizing images with dense objects, we initially sample 1K high-quality images from SA-1B, and incorporate eight LMM experts to manually annotate questions, candidate options, and answers for each image. After three stages of quality control, TreeBench consists of 405 challenging visual question-answering pairs, even the most advanced models struggle with this benchmark, where none of them reach 60% accuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR (Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to supervise localization and reasoning jointly with reinforcement learning, enabling accurate localizations and explainable reasoning pathways. Initialized from Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and TreeBench (+13.4), proving traceability is key to advancing vision-grounded reasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR.",
            "score": 24,
            "issue_id": 4761,
            "pub_date": "2025-07-10",
            "pub_date_card": {
                "ru": "10 июля",
                "en": "July 10",
                "zh": "7月10日"
            },
            "hash": "e52c2296896d713c",
            "authors": [
                "Haochen Wang",
                "Xiangtai Li",
                "Zilong Huang",
                "Anran Wang",
                "Jiacong Wang",
                "Tao Zhang",
                "Jiani Zheng",
                "Sule Bai",
                "Zijian Kang",
                "Jiashi Feng",
                "Zhuochen Wang",
                "Zhaoxiang Zhang"
            ],
            "affiliations": [
                "ByteDance",
                "NLPR, MAIS, CASIA",
                "UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07999.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#training",
                    "#interpretability",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Отслеживаемые доказательства - ключ к улучшению визуального ИИ",
                    "desc": "TreeBench - это диагностический бенчмарк для оценки визуального обоснованного рассуждения, основанный на трех принципах: восприятие сложных сцен, отслеживаемые доказательства и рассуждения второго порядка. Он состоит из 405 сложных пар вопросов и ответов по изображениям, с которыми даже самые продвинутые модели справляются с трудом. TreeVGR - это парадигма обучения, использующая обучение с подкреплением для совместного контроля локализации и рассуждений. Инициализированная на основе Qwen2.5-VL-7B, она улучшает результаты на нескольких бенчмарках, доказывая важность отслеживаемости для развития визуально-обоснованных рассуждений."
                },
                "en": {
                    "title": "Enhancing Visual Grounded Reasoning with TreeBench and TreeVGR",
                    "desc": "This paper introduces TreeBench, a benchmark designed to evaluate visual grounded reasoning by focusing on subtle target detection, traceable evidence, and second-order reasoning. It highlights the need for a comprehensive assessment tool as existing models struggle with complex visual tasks, achieving less than 60% accuracy on the benchmark. TreeVGR is proposed as an enhancement that uses reinforcement learning to improve joint localization and reasoning, demonstrating significant performance gains over existing models. The research emphasizes the importance of traceability in developing advanced visual reasoning capabilities."
                },
                "zh": {
                    "title": "提升视觉推理的可追溯性",
                    "desc": "本文提出了TreeBench，一个用于评估视觉基础推理的基准，侧重于复杂场景中微妙目标的检测、可追溯证据和二阶推理。TreeVGR则通过强化学习增强了这一过程，实现了定位和推理的联合训练。研究表明，现有的先进模型在TreeBench基准上表现不佳，准确率未超过60%。通过引入可追溯性，TreeVGR显著提升了模型在视觉基础推理任务中的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07998",
            "title": "PyVision: Agentic Vision with Dynamic Tooling",
            "url": "https://huggingface.co/papers/2507.07998",
            "abstract": "PyVision, an interactive framework, enables LLMs to autonomously create and refine Python-based tools for visual reasoning, achieving significant performance improvements across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs are increasingly deployed as agents, systems capable of planning, reasoning, and dynamically calling external tools. However, in visual reasoning, prior approaches largely remain limited by predefined workflows and static toolsets. In this report, we present PyVision, an interactive, multi-turn framework that enables MLLMs to autonomously generate, execute, and refine Python-based tools tailored to the task at hand, unlocking flexible and interpretable problem-solving. We develop a taxonomy of the tools created by PyVision and analyze their usage across a diverse set of benchmarks. Quantitatively, PyVision achieves consistent performance gains, boosting GPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini. These results point to a broader shift: dynamic tooling allows models not just to use tools, but to invent them, advancing toward more agentic visual reasoning.",
            "score": 10,
            "issue_id": 4761,
            "pub_date": "2025-07-10",
            "pub_date_card": {
                "ru": "10 июля",
                "en": "July 10",
                "zh": "7月10日"
            },
            "hash": "ab8504b49800fe67",
            "authors": [
                "Shitian Zhao",
                "Haoquan Zhang",
                "Shaoheng Lin",
                "Ming Li",
                "Qilong Wu",
                "Kaipeng Zhang",
                "Chen Wei"
            ],
            "affiliations": [
                "CUHK",
                "NUS",
                "Rice University",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07998.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#interpretability",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "🔧",
                "ru": {
                    "title": "PyVision: LLM создают инструменты для визуального анализа",
                    "desc": "PyVision - это интерактивный фреймворк, позволяющий большим языковым моделям (LLM) автономно создавать и совершенствовать инструменты на Python для визуального анализа. Фреймворк использует многоэтапный подход, где модель генерирует, выполняет и улучшает инструменты под конкретную задачу. PyVision значительно повышает производительность LLM на различных бенчмарках, например, улучшая результаты GPT-4.1 на 7.8% в V* и Claude-4.0-Sonnet на 31.1% в VLMsAreBlind-mini. Это представляет собой шаг к более гибкому и интерпретируемому решению задач в области компьютерного зрения."
                },
                "en": {
                    "title": "Empowering LLMs with Dynamic Tool Creation for Visual Reasoning",
                    "desc": "PyVision is an innovative framework that empowers large language models (LLMs) to autonomously create and improve Python tools for visual reasoning tasks. Unlike previous methods that relied on fixed workflows, PyVision allows for dynamic tool generation and execution, enhancing flexibility in problem-solving. The framework has been evaluated across various benchmarks, showing significant performance improvements, such as a 7.8% increase for GPT-4.1 and a 31.1% boost for Claude-4.0-Sonnet. This advancement signifies a shift towards more agentic capabilities in visual reasoning, where models can not only utilize existing tools but also invent new ones."
                },
                "zh": {
                    "title": "动态工具，智能推理的新纪元",
                    "desc": "PyVision是一个交互式框架，允许大型语言模型（LLMs）自主创建和改进基于Python的视觉推理工具。与以往的静态工具集不同，PyVision支持多轮交互，使模型能够根据具体任务灵活生成和执行工具。研究表明，PyVision在多个基准测试中显著提高了性能，例如GPT-4.1在V*上提升了7.8%。这些结果表明，动态工具的使用使模型不仅能够使用工具，还能发明新工具，推动视觉推理的进步。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07202",
            "title": "A Survey on Long-Video Storytelling Generation: Architectures,\n  Consistency, and Cinematic Quality",
            "url": "https://huggingface.co/papers/2507.07202",
            "abstract": "Despite the significant progress that has been made in video generative models, existing state-of-the-art methods can only produce videos lasting 5-16 seconds, often labeled \"long-form videos\". Furthermore, videos exceeding 16 seconds struggle to maintain consistent character appearances and scene layouts throughout the narrative. In particular, multi-subject long videos still fail to preserve character consistency and motion coherence. While some methods can generate videos up to 150 seconds long, they often suffer from frame redundancy and low temporal diversity. Recent work has attempted to produce long-form videos featuring multiple characters, narrative coherence, and high-fidelity detail. We comprehensively studied 32 papers on video generation to identify key architectural components and training strategies that consistently yield these qualities. We also construct a comprehensive novel taxonomy of existing methods and present comparative tables that categorize papers by their architectural designs and performance characteristics.",
            "score": 2,
            "issue_id": 4762,
            "pub_date": "2025-07-09",
            "pub_date_card": {
                "ru": "9 июля",
                "en": "July 9",
                "zh": "7月9日"
            },
            "hash": "c7dc5888e8a06c13",
            "authors": [
                "Mohamed Elmoghany",
                "Ryan Rossi",
                "Seunghyun Yoon",
                "Subhojyoti Mukherjee",
                "Eslam Bakr",
                "Puneet Mathur",
                "Gang Wu",
                "Viet Dac Lai",
                "Nedim Lipka",
                "Ruiyi Zhang",
                "Varun Manjunatha",
                "Chien Nguyen",
                "Daksh Dangi",
                "Abel Salinas",
                "Mohammad Taesiri",
                "Hongjie Chen",
                "Xiaolei Huang",
                "Joe Barrow",
                "Nesreen Ahmed",
                "Hoda Eldardiry",
                "Namyong Park",
                "Yu Wang",
                "Jaemin Cho",
                "Anh Totti Nguyen",
                "Zhengzhong Tu",
                "Thien Nguyen",
                "Dinesh Manocha",
                "Mohamed Elhoseiny",
                "Franck Dernoncourt"
            ],
            "affiliations": [
                "Adobe Research",
                "Auburn University",
                "Cisco",
                "Dolby Labs",
                "Independent Researcher",
                "KAUST",
                "Meta AI",
                "Pattern Data",
                "Texas A&M University",
                "UNC Chapel Hill",
                "University of Maryland, College Park",
                "University of Memphis",
                "University of Oregon",
                "University of Southern California",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07202.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#architecture",
                    "#video",
                    "#training"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Прорыв в генерации длинных видео: анализ ключевых компонентов и стратегий",
                    "desc": "Эта статья посвящена анализу современных методов генерации видео с помощью машинного обучения. Авторы изучили 32 научные работы, чтобы выявить ключевые архитектурные компоненты и стратегии обучения, позволяющие создавать длительные видео с несколькими персонажами и связным сюжетом. В статье отмечается, что существующие модели генеративного ИИ способны создавать видео длительностью только 5-16 секунд, при этом более длинные видео страдают от проблем с согласованностью персонажей и сцен. Авторы представляют новую таксономию существующих методов и сравнительные таблицы, классифицирующие работы по их архитектурным особенностям и характеристикам производительности."
                },
                "en": {
                    "title": "Unlocking the Future of Long-Form Video Generation",
                    "desc": "This paper reviews the current state of video generative models, highlighting their limitations in producing long-form videos that exceed 16 seconds. It identifies issues such as character consistency and motion coherence, particularly in videos featuring multiple subjects. The authors analyze 32 existing studies to pinpoint effective architectural components and training strategies that enhance video quality. Additionally, they propose a new taxonomy to classify these methods based on their designs and performance metrics, aiming to guide future research in this area."
                },
                "zh": {
                    "title": "提升视频生成的连贯性与多样性",
                    "desc": "尽管视频生成模型取得了显著进展，但现有的最先进方法只能生成持续5到16秒的视频，通常被称为“长视频”。超过16秒的视频在角色外观和场景布局的一致性方面存在困难，尤其是多角色长视频在角色一致性和运动连贯性方面仍然存在问题。虽然一些方法可以生成长达150秒的视频，但它们往往面临帧冗余和时间多样性不足的问题。我们对32篇视频生成论文进行了全面研究，识别出关键的架构组件和训练策略，并构建了一个新的现有方法分类法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07136",
            "title": "LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+\n  FPS",
            "url": "https://huggingface.co/papers/2507.07136",
            "abstract": "LangSplatV2 enhances 3D text querying speed and accuracy by replacing the heavyweight decoder with a sparse coefficient field and efficient CUDA optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce LangSplatV2, which achieves high-dimensional feature splatting at 476.2 FPS and 3D open-vocabulary text querying at 384.6 FPS for high-resolution images, providing a 42 times speedup and a 47 times boost over LangSplat respectively, along with improved query accuracy. LangSplat employs Gaussian Splatting to embed 2D CLIP language features into 3D, significantly enhancing speed and learning a precise 3D language field with SAM semantics. Such advancements in 3D language fields are crucial for applications that require language interaction within complex scenes. However, LangSplat does not yet achieve real-time inference performance (8.2 FPS), even with advanced A100 GPUs, severely limiting its broader application. In this paper, we first conduct a detailed time analysis of LangSplat, identifying the heavyweight decoder as the primary speed bottleneck. Our solution, LangSplatV2 assumes that each Gaussian acts as a sparse code within a global dictionary, leading to the learning of a 3D sparse coefficient field that entirely eliminates the need for a heavyweight decoder. By leveraging this sparsity, we further propose an efficient sparse coefficient splatting method with CUDA optimization, rendering high-dimensional feature maps at high quality while incurring only the time cost of splatting an ultra-low-dimensional feature. Our experimental results demonstrate that LangSplatV2 not only achieves better or competitive query accuracy but is also significantly faster. Codes and demos are available at our project page: https://langsplat-v2.github.io.",
            "score": 2,
            "issue_id": 4763,
            "pub_date": "2025-07-09",
            "pub_date_card": {
                "ru": "9 июля",
                "en": "July 9",
                "zh": "7月9日"
            },
            "hash": "f35bfc7d12aabb90",
            "authors": [
                "Wanhua Li",
                "Yujie Zhao",
                "Minghan Qin",
                "Yang Liu",
                "Yuanhao Cai",
                "Chuang Gan",
                "Hanspeter Pfister"
            ],
            "affiliations": [
                "Harvard University",
                "Johns Hopkins University",
                "MIT-IBM Watson AI Lab",
                "Tsinghua University",
                "UMass Amherst",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07136.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#optimization",
                    "#inference",
                    "#data"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "LangSplatV2: Революция в скорости и точности 3D текстовых запросов",
                    "desc": "LangSplatV2 - это усовершенствованная версия системы для трехмерных текстовых запросов. Она заменяет тяжеловесный декодер на разреженное поле коэффициентов и использует оптимизацию CUDA. Это позволяет достичь скорости 476.2 кадров в секунду для сплаттинга высокоразмерных признаков и 384.6 кадров в секунду для текстовых запросов в трехмерном пространстве. LangSplatV2 демонстрирует значительное ускорение и повышение точности по сравнению с предыдущей версией."
                },
                "en": {
                    "title": "Speeding Up 3D Text Querying with LangSplatV2",
                    "desc": "LangSplatV2 is a machine learning model that improves the speed and accuracy of 3D text querying by replacing the traditional heavyweight decoder with a more efficient sparse coefficient field. It achieves impressive performance metrics, processing high-dimensional features at 476.2 frames per second (FPS) and 3D text queries at 384.6 FPS, marking a significant speedup compared to its predecessor, LangSplat. The model utilizes Gaussian Splatting to effectively integrate 2D language features into a 3D context, enhancing the precision of language interactions in complex scenes. Despite these advancements, LangSplatV2 still struggles to reach real-time inference speeds, which limits its practical applications in dynamic environments."
                },
                "zh": {
                    "title": "LangSplatV2：提升 3D 文本查询速度与准确性",
                    "desc": "LangSplatV2 是一种新型的 3D 文本查询方法，通过用稀疏系数场替代传统的重型解码器，显著提高了查询速度和准确性。该方法在高分辨率图像上实现了每秒 476.2 帧的高维特征喷溅和每秒 384.6 帧的 3D 开放词汇文本查询，速度提升达 42 倍，准确性也有显著提高。LangSplatV2 利用高斯喷溅技术将 2D CLIP 语言特征嵌入 3D，学习精确的 3D 语言场，适用于复杂场景中的语言交互应用。尽管 LangSplatV2 显著提升了性能，但在实时推理方面仍有待改进。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07982",
            "title": "Geometry Forcing: Marrying Video Diffusion and 3D Representation for\n  Consistent World Modeling",
            "url": "https://huggingface.co/papers/2507.07982",
            "abstract": "Videos inherently represent 2D projections of a dynamic 3D world. However, our analysis suggests that video diffusion models trained solely on raw video data often fail to capture meaningful geometric-aware structure in their learned representations. To bridge this gap between video diffusion models and the underlying 3D nature of the physical world, we propose Geometry Forcing, a simple yet effective method that encourages video diffusion models to internalize latent 3D representations. Our key insight is to guide the model's intermediate representations toward geometry-aware structure by aligning them with features from a pretrained geometric foundation model. To this end, we introduce two complementary alignment objectives: Angular Alignment, which enforces directional consistency via cosine similarity, and Scale Alignment, which preserves scale-related information by regressing unnormalized geometric features from normalized diffusion representation. We evaluate Geometry Forcing on both camera view-conditioned and action-conditioned video generation tasks. Experimental results demonstrate that our method substantially improves visual quality and 3D consistency over the baseline methods. Project page: https://GeometryForcing.github.io.",
            "score": 1,
            "issue_id": 4762,
            "pub_date": "2025-07-10",
            "pub_date_card": {
                "ru": "10 июля",
                "en": "July 10",
                "zh": "7月10日"
            },
            "hash": "fbe6e1954d8e9c30",
            "authors": [
                "Haoyu Wu",
                "Diankun Wu",
                "Tianyu He",
                "Junliang Guo",
                "Yang Ye",
                "Yueqi Duan",
                "Jiang Bian"
            ],
            "affiliations": [
                "Microsoft Research",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07982.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#video",
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "🧊",
                "ru": {
                    "title": "Geometry Forcing: внедрение 3D-геометрии в видео-диффузионные модели",
                    "desc": "Статья представляет метод Geometry Forcing для улучшения видео-диффузионных моделей. Авторы предлагают выравнивать промежуточные представления модели с признаками из предобученной геометрической модели-основы. Метод включает два вида выравнивания: угловое для согласованности направлений и масштабное для сохранения информации о масштабе. Эксперименты показывают, что Geometry Forcing значительно улучшает визуальное качество и 3D-согласованность генерируемых видео."
                },
                "en": {
                    "title": "Bridging 2D Videos to 3D Understanding with Geometry Forcing",
                    "desc": "This paper addresses the limitations of video diffusion models that do not effectively capture the 3D structure of the world from 2D video data. The authors introduce a technique called Geometry Forcing, which helps these models learn geometric representations by aligning their intermediate features with those from a pretrained geometric foundation model. They propose two alignment objectives: Angular Alignment, which ensures directional consistency, and Scale Alignment, which maintains scale information. The results show that Geometry Forcing significantly enhances the visual quality and 3D consistency of generated videos compared to existing methods."
                },
                "zh": {
                    "title": "提升视频模型的几何感知能力",
                    "desc": "本论文提出了一种名为几何强制（Geometry Forcing）的方法，旨在改善视频扩散模型在学习表示时对三维几何结构的捕捉能力。我们发现，仅使用原始视频数据训练的模型往往无法有效捕捉到有意义的几何信息。通过将模型的中间表示与预训练的几何基础模型的特征对齐，我们引入了两个互补的对齐目标：角度对齐和尺度对齐，以增强模型的几何感知能力。实验结果表明，几何强制方法在视频生成任务中显著提高了视觉质量和三维一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07484",
            "title": "Machine Bullshit: Characterizing the Emergent Disregard for Truth in\n  Large Language Models",
            "url": "https://huggingface.co/papers/2507.07484",
            "abstract": "Machine bullshit, characterized by LLMs' indifference to truth, is quantified and analyzed through a new framework, revealing that RLHF and CoT prompting exacerbate certain bullshit forms.  \t\t\t\t\tAI-generated summary \t\t\t\t Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to statements made without regard to their truth value. While previous work has explored large language model (LLM) hallucination and sycophancy, we propose machine bullshit as an overarching conceptual framework that can allow researchers to characterize the broader phenomenon of emergent loss of truthfulness in LLMs and shed light on its underlying mechanisms. We introduce the Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and propose a complementary taxonomy analyzing four qualitative forms of bullshit: empty rhetoric, paltering, weasel words, and unverified claims. We conduct empirical evaluations on the Marketplace dataset, the Political Neutrality dataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI assistants) explicitly designed to evaluate machine bullshit. Our results demonstrate that model fine-tuning with reinforcement learning from human feedback (RLHF) significantly exacerbates bullshit and inference-time chain-of-thought (CoT) prompting notably amplify specific bullshit forms, particularly empty rhetoric and paltering. We also observe prevalent machine bullshit in political contexts, with weasel words as the dominant strategy. Our findings highlight systematic challenges in AI alignment and provide new insights toward more truthful LLM behavior.",
            "score": 1,
            "issue_id": 4763,
            "pub_date": "2025-07-10",
            "pub_date_card": {
                "ru": "10 июля",
                "en": "July 10",
                "zh": "7月10日"
            },
            "hash": "c35c3791aec951d3",
            "authors": [
                "Kaiqu Liang",
                "Haimin Hu",
                "Xuandong Zhao",
                "Dawn Song",
                "Thomas L. Griffiths",
                "Jaime Fernández Fisac"
            ],
            "affiliations": [
                "Princeton University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07484.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#ethics",
                    "#benchmark",
                    "#hallucinations",
                    "#rlhf",
                    "#training"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Проблема правдивости в LLM: анализ машинного буллшита",
                    "desc": "В статье рассматривается концепция \"машинного буллшита\", когда LLMs генерируют утверждения без учета их истинности. Исследователи вводят новый индекс буллшита, чтобы количественно оценить безразличие LLMs к правде, и предлагают таксономию, анализирующую четыре формы буллшита: пустая риторика, увиливание, слова-лазейки и непроверенные утверждения. Эксперименты показывают, что обучение с подкреплением от обратной связи человека (RLHF) и использование цепочки рассуждений (CoT) усиливают некоторые формы буллшита. Результаты подчеркивают проблемы в согласовании AI и предлагают новые пути к более правдивому поведению LLM."
                },
                "en": {
                    "title": "Quantifying Machine Bullshit: A New Framework for LLM Truthfulness",
                    "desc": "This paper introduces the concept of 'machine bullshit' to describe how large language models (LLMs) can generate statements without regard for their truthfulness. It presents a new framework that includes the Bullshit Index, a metric designed to quantify this indifference to truth. The authors analyze four types of machine bullshit: empty rhetoric, paltering, weasel words, and unverified claims, and evaluate these through various datasets. The findings indicate that techniques like reinforcement learning from human feedback (RLHF) and chain-of-thought prompting can worsen the generation of certain types of bullshit, particularly in political contexts."
                },
                "zh": {
                    "title": "揭示机器胡说的真相",
                    "desc": "本文提出了一个新的框架来量化和分析大型语言模型（LLM）在生成内容时对真相的漠视，称之为“机器胡说”。我们引入了“胡说指数”，这是一个新的指标，用于量化LLM对真相的无动于衷，并分析了四种胡说的定性形式：空洞修辞、模棱两可、狡猾用词和未经验证的声明。研究表明，使用人类反馈的强化学习（RLHF）会显著加剧胡说现象，而推理时的思维链（CoT）提示则特别放大了空洞修辞和模棱两可的表现。我们的发现揭示了AI对齐中的系统性挑战，并为实现更真实的LLM行为提供了新的见解。"
                }
            }
        }
    ],
    "link_prev": "2025-07-10.html",
    "link_next": "2025-07-14.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "10.07",
        "en": "07/10",
        "zh": "7月10日"
    },
    "short_date_next": {
        "ru": "14.07",
        "en": "07/14",
        "zh": "7月14日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 1,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}