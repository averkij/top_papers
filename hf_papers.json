{
    "date": {
        "ru": "24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 24",
        "zh": "2æœˆ24æ—¥"
    },
    "time_utc": "2025-02-24 13:19",
    "weekday": 0,
    "issue_id": 2373,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.14776",
            "title": "SurveyX: Academic Survey Automation via Large Language Models",
            "url": "https://huggingface.co/papers/2502.14776",
            "abstract": "Large Language Models (LLMs) have demonstrated exceptional comprehension capabilities and a vast knowledge base, suggesting that LLMs can serve as efficient tools for automated survey generation. However, recent research related to automated survey generation remains constrained by some critical limitations like finite context window, lack of in-depth content discussion, and absence of systematic evaluation frameworks. Inspired by human writing processes, we propose SurveyX, an efficient and organized system for automated survey generation that decomposes the survey composing process into two phases: the Preparation and Generation phases. By innovatively introducing online reference retrieval, a pre-processing method called AttributeTree, and a re-polishing process, SurveyX significantly enhances the efficacy of survey composition. Experimental evaluation results show that SurveyX outperforms existing automated survey generation systems in content quality (0.259 improvement) and citation quality (1.76 enhancement), approaching human expert performance across multiple evaluation dimensions. Examples of surveys generated by SurveyX are available on www.surveyx.cn",
            "score": 68,
            "issue_id": 2363,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 20",
                "zh": "2æœˆ20æ—¥"
            },
            "hash": "b2504554ef88d631",
            "authors": [
                "Xun Liang",
                "Jiawei Yang",
                "Yezhaohui Wang",
                "Chen Tang",
                "Zifan Zheng",
                "Simin Niu",
                "Shichao Song",
                "Hanyu Wang",
                "Bo Tang",
                "Feiyu Xiong",
                "Keming Mao",
                "Zhiyu li"
            ],
            "affiliations": [
                "Institute for Advanced Algorithms Research, Shanghai, China",
                "Northeastern University, Shenyang, China",
                "Renmin University of China, Beijing, China",
                "The University of Sydney, Sydney, Australia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.14776.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#survey",
                    "#benchmark",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "SurveyX: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ SurveyX - ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ° Ğ½Ğ° Ğ´Ğ²Ğµ Ñ„Ğ°Ğ·Ñ‹: Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºÑƒ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¿Ğ¾Ğ¸ÑĞº ÑÑÑ‹Ğ»Ğ¾Ğº Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ AttributeTree. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ SurveyX Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ÑÑÑŒ Ğº ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²-Ğ»ÑĞ´ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ñ€ÑĞ´ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€Ğ¸ÑÑƒÑ‰Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "SurveyX: Revolutionizing Automated Survey Generation with LLMs",
                    "desc": "This paper introduces SurveyX, a novel system for automated survey generation that leverages Large Language Models (LLMs) to improve the survey creation process. It addresses key limitations of previous methods by breaking down the process into two distinct phases: Preparation and Generation. SurveyX incorporates innovative techniques such as online reference retrieval and a pre-processing method called AttributeTree, which enhance the quality of the generated surveys. Experimental results demonstrate that SurveyX significantly outperforms existing systems in both content and citation quality, nearing the performance of human experts."
                },
                "zh": {
                    "title": "SurveyXï¼šé«˜æ•ˆçš„è‡ªåŠ¨è°ƒæŸ¥ç”Ÿæˆç³»ç»Ÿ",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç†è§£èƒ½åŠ›å’ŒçŸ¥è¯†åŸºç¡€æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ˜¾ç¤ºå‡ºå®ƒä»¬å¯ä»¥ä½œä¸ºè‡ªåŠ¨è°ƒæŸ¥ç”Ÿæˆçš„æœ‰æ•ˆå·¥å…·ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è‡ªåŠ¨è°ƒæŸ¥ç”Ÿæˆç ”ç©¶å—åˆ°ä¸€äº›å…³é”®é™åˆ¶ï¼Œå¦‚æœ‰é™çš„ä¸Šä¸‹æ–‡çª—å£ã€ç¼ºä¹æ·±å…¥çš„å†…å®¹è®¨è®ºå’Œç¼ºä¹ç³»ç»Ÿçš„è¯„ä¼°æ¡†æ¶ã€‚æˆ‘ä»¬æå‡ºäº†SurveyXï¼Œä¸€ä¸ªé«˜æ•ˆä¸”æœ‰ç»„ç»‡çš„è‡ªåŠ¨è°ƒæŸ¥ç”Ÿæˆç³»ç»Ÿï¼Œå°†è°ƒæŸ¥ç¼–å†™è¿‡ç¨‹åˆ†ä¸ºå‡†å¤‡é˜¶æ®µå’Œç”Ÿæˆé˜¶æ®µã€‚é€šè¿‡å¼•å…¥åœ¨çº¿å‚è€ƒæ£€ç´¢ã€å±æ€§æ ‘é¢„å¤„ç†æ–¹æ³•å’Œé‡æ–°æ¶¦è‰²è¿‡ç¨‹ï¼ŒSurveyXæ˜¾è‘—æé«˜äº†è°ƒæŸ¥ç¼–å†™çš„æ•ˆç‡ï¼Œå¹¶åœ¨å†…å®¹è´¨é‡å’Œå¼•ç”¨è´¨é‡ä¸Šè¶…è¶Šäº†ç°æœ‰ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11663",
            "title": "MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction",
            "url": "https://huggingface.co/papers/2502.11663",
            "abstract": "World models that forecast environmental changes from actions are vital for autonomous driving models with strong generalization. The prevailing driving world model mainly build on video prediction model. Although these models can produce high-fidelity video sequences with advanced diffusion-based generator, they are constrained by their predictive duration and overall generalization capabilities. In this paper, we explore to solve this problem by combining generation loss with MAE-style feature-level context learning. In particular, we instantiate this target with three key design: (1) A more scalable Diffusion Transformer (DiT) structure trained with extra mask construction task. (2) we devise diffusion-related mask tokens to deal with the fuzzy relations between mask reconstruction and generative diffusion process. (3) we extend mask construction task to spatial-temporal domain by utilizing row-wise mask for shifted self-attention rather than masked self-attention in MAE. Then, we adopt a row-wise cross-view module to align with this mask design. Based on above improvement, we propose MaskGWM: a Generalizable driving World Model embodied with Video Mask reconstruction. Our model contains two variants: MaskGWM-long, focusing on long-horizon prediction, and MaskGWM-mview, dedicated to multi-view generation. Comprehensive experiments on standard benchmarks validate the effectiveness of the proposed method, which contain normal validation of Nuscene dataset, long-horizon rollout of OpenDV-2K dataset and zero-shot validation of Waymo dataset. Quantitative metrics on these datasets show our method notably improving state-of-the-art driving world model.",
            "score": 36,
            "issue_id": 2367,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "14ade1d8007cfa16",
            "authors": [
                "Jingcheng Ni",
                "Yuxin Guo",
                "Yichen Liu",
                "Rui Chen",
                "Lewei Lu",
                "Zehuan Wu"
            ],
            "affiliations": [
                "SenseTime Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11663.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#architecture",
                    "#long_context",
                    "#games",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "MaskGWM: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MaskGWM Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² ÑÑ‚Ğ¸Ğ»Ğµ MAE. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Diffusion Transformer, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ°ÑĞºĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ÑĞ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Autonomous Driving with MaskGWM: A New Era in World Models",
                    "desc": "This paper presents a new approach to creating world models for autonomous driving that can better predict environmental changes over time. The authors introduce MaskGWM, which combines advanced video generation techniques with feature-level context learning to enhance generalization. Key innovations include a scalable Diffusion Transformer architecture, the use of diffusion-related mask tokens, and an extension of mask construction to the spatial-temporal domain. Experimental results demonstrate that MaskGWM significantly outperforms existing models in various driving scenarios, showcasing its effectiveness in long-horizon and multi-view predictions."
                },
                "zh": {
                    "title": "æå‡é©¾é©¶æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ä¸€ç§æ–°çš„é©¾é©¶ä¸–ç•Œæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡ç»“åˆç”ŸæˆæŸå¤±å’ŒMAEé£æ ¼çš„ç‰¹å¾çº§ä¸Šä¸‹æ–‡å­¦ä¹ æ¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„æ‰©æ•£å˜æ¢å™¨ç»“æ„ï¼Œå¹¶å¼•å…¥äº†æ‰©æ•£ç›¸å…³çš„æ©ç æ ‡è®°ï¼Œä»¥å¤„ç†æ©ç é‡å»ºä¸ç”Ÿæˆæ‰©æ•£è¿‡ç¨‹ä¹‹é—´çš„æ¨¡ç³Šå…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æ©ç æ„å»ºä»»åŠ¡æ‰©å±•åˆ°æ—¶ç©ºåŸŸï¼Œé‡‡ç”¨è¡Œçº§æ©ç è¿›è¡Œåç§»è‡ªæ³¨æ„åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„MaskGWMæ¨¡å‹åœ¨å¤šä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†é©¾é©¶ä¸–ç•Œæ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15007",
            "title": "LLM-Microscope: Uncovering the Hidden Role of Punctuation in Context Memory of Transformers",
            "url": "https://huggingface.co/papers/2502.15007",
            "abstract": "We introduce methods to quantify how Large Language Models (LLMs) encode and store contextual information, revealing that tokens often seen as minor (e.g., determiners, punctuation) carry surprisingly high context. Notably, removing these tokens -- especially stopwords, articles, and commas -- consistently degrades performance on MMLU and BABILong-4k, even if removing only irrelevant tokens. Our analysis also shows a strong correlation between contextualization and linearity, where linearity measures how closely the transformation from one layer's embeddings to the next can be approximated by a single linear mapping. These findings underscore the hidden importance of filler tokens in maintaining context. For further exploration, we present LLM-Microscope, an open-source toolkit that assesses token-level nonlinearity, evaluates contextual memory, visualizes intermediate layer contributions (via an adapted Logit Lens), and measures the intrinsic dimensionality of representations. This toolkit illuminates how seemingly trivial tokens can be critical for long-range understanding.",
            "score": 32,
            "issue_id": 2367,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 20",
                "zh": "2æœˆ20æ—¥"
            },
            "hash": "9be244de8b366352",
            "authors": [
                "Anton Razzhigaev",
                "Matvey Mikhalchuk",
                "Temurbek Rahmatullaev",
                "Elizaveta Goncharova",
                "Polina Druzhinina",
                "Ivan Oseledets",
                "Andrey Kuznetsov"
            ],
            "affiliations": [
                "AIRI",
                "HSE University",
                "Lomonosov Moscow State University",
                "Skoltech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15007.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#architecture",
                    "#long_context",
                    "#interpretability",
                    "#training",
                    "#open_source"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ¡ĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑĞ¸Ğ»Ğ° Ğ¼ĞµĞ»Ğ¾Ñ‡ĞµĞ¹: ĞºĞ°Ğº Ğ½ĞµĞ·Ğ°Ğ¼ĞµÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ°Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸ Ñ…Ñ€Ğ°Ğ½ÑÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ° Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ°Ñ€Ñ‚Ğ¸ĞºĞ»Ğ¸ Ğ¸ Ğ¿ÑƒĞ½ĞºÑ‚ÑƒĞ°Ñ†Ğ¸Ñ) Ğ½ĞµÑÑƒÑ‚ Ğ²Ğ°Ğ¶Ğ½ÑƒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ°Ñ… MMLU Ğ¸ BABILong-4k. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ÑÑ… Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Unveiling the Hidden Power of Minor Tokens in LLMs",
                    "desc": "This paper explores how Large Language Models (LLMs) encode contextual information, highlighting the significant role of seemingly minor tokens like stopwords and punctuation. The authors demonstrate that removing these tokens negatively impacts model performance on tasks such as MMLU and BABILong-4k, indicating their importance in maintaining context. They also find a correlation between the contextualization of tokens and the linearity of transformations between model layers, suggesting that the way information is processed is not purely linear. To aid further research, the authors introduce LLM-Microscope, a toolkit designed to analyze token-level nonlinearity and visualize the contributions of different layers in LLMs."
                },
                "zh": {
                    "title": "éšè—çš„é‡è¦æ€§ï¼šå¡«å……æ ‡è®°åœ¨ä¸Šä¸‹æ–‡ä¸­çš„å…³é”®ä½œç”¨",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†é‡åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ä½•ç¼–ç å’Œå­˜å‚¨ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œé€šå¸¸è¢«è§†ä¸ºæ¬¡è¦çš„æ ‡è®°ï¼ˆå¦‚é™å®šè¯å’Œæ ‡ç‚¹ç¬¦å·ï¼‰å®é™…ä¸Šæ‰¿è½½ç€æ„æƒ³ä¸åˆ°çš„é«˜ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ç‰¹åˆ«æ˜¯ï¼Œå»é™¤è¿™äº›æ ‡è®°ï¼ˆå°¤å…¶æ˜¯åœç”¨è¯ã€å† è¯å’Œé€—å·ï¼‰ä¼šæ˜¾è‘—é™ä½MMLUå’ŒBABILong-4kçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„åˆ†æè¿˜è¡¨æ˜ï¼Œä¸Šä¸‹æ–‡åŒ–ä¸çº¿æ€§åº¦ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§ï¼Œçº¿æ€§åº¦è¡¡é‡ä»ä¸€å±‚åµŒå…¥åˆ°ä¸‹ä¸€å±‚çš„è½¬æ¢æ˜¯å¦å¯ä»¥ç”¨å•ä¸€çº¿æ€§æ˜ å°„æ¥è¿‘ä¼¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.13449",
            "title": "Mol-LLaMA: Towards General Understanding of Molecules in Large Molecular Language Model",
            "url": "https://huggingface.co/papers/2502.13449",
            "abstract": "Understanding molecules is key to understanding organisms and driving advances in drug discovery, requiring interdisciplinary knowledge across chemistry and biology. Although large molecular language models have achieved notable success in interpreting molecular structures, their instruction datasets are limited to the specific knowledge from task-oriented datasets and do not fully cover the fundamental characteristics of molecules, hindering their abilities as general-purpose molecular assistants. To address this issue, we propose Mol-LLaMA, a large molecular language model that grasps the general knowledge centered on molecules via multi-modal instruction tuning. To this end, we design key data types that encompass the fundamental features of molecules, incorporating essential knowledge from molecular structures. In addition, to improve understanding of molecular features, we introduce a module that integrates complementary information from different molecular encoders, leveraging the distinct advantages of different molecular representations. Our experimental results demonstrate that Mol-LLaMA is capable of comprehending the general features of molecules and generating relevant responses to users' queries with detailed explanations, implying its potential as a general-purpose assistant for molecular analysis.",
            "score": 31,
            "issue_id": 2363,
            "pub_date": "2025-02-19",
            "pub_date_card": {
                "ru": "19 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 19",
                "zh": "2æœˆ19æ—¥"
            },
            "hash": "e52b99ade1ae590a",
            "authors": [
                "Dongki Kim",
                "Wonbin Lee",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "Korea Advanced Institute of Science and Technology (KAIST), Seoul, South Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.13449.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#science",
                    "#agi",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Mol-LLaMA: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¯Ğœ Ğ´Ğ»Ñ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ĞµĞ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Mol-LLaMA - Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ», Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ», Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ½ĞµĞ´Ñ€Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ». Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Mol-LLaMA ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Mol-LLaMA: A General-Purpose Assistant for Molecular Understanding",
                    "desc": "This paper introduces Mol-LLaMA, a large molecular language model designed to enhance understanding of molecular structures for drug discovery. Unlike previous models that relied on limited task-specific datasets, Mol-LLaMA utilizes multi-modal instruction tuning to incorporate a broader range of fundamental molecular knowledge. The model integrates various molecular encoders to leverage their unique strengths, improving its ability to interpret molecular features. Experimental results show that Mol-LLaMA can effectively respond to user queries with detailed explanations, positioning it as a versatile tool for molecular analysis."
                },
                "zh": {
                    "title": "Mol-LLaMAï¼šåˆ†å­åˆ†æçš„é€šç”¨åŠ©æ‰‹",
                    "desc": "ç†è§£åˆ†å­å¯¹äºç†è§£ç”Ÿç‰©ä½“å’Œæ¨åŠ¨è¯ç‰©å‘ç°è‡³å…³é‡è¦ã€‚è™½ç„¶å¤§å‹åˆ†å­è¯­è¨€æ¨¡å‹åœ¨è§£æåˆ†å­ç»“æ„æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å®ƒä»¬çš„è®­ç»ƒæ•°æ®é›†ä»…é™äºç‰¹å®šä»»åŠ¡çš„çŸ¥è¯†ï¼Œæœªèƒ½å…¨é¢è¦†ç›–åˆ†å­çš„åŸºæœ¬ç‰¹å¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Mol-LLaMAï¼Œä¸€ä¸ªé€šè¿‡å¤šæ¨¡æ€æŒ‡ä»¤è°ƒä¼˜æ¥æŒæ¡åˆ†å­ä¸­å¿ƒçš„é€šç”¨çŸ¥è¯†çš„å¤§å‹åˆ†å­è¯­è¨€æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMol-LLaMAèƒ½å¤Ÿç†è§£åˆ†å­çš„æ™®éç‰¹å¾ï¼Œå¹¶ç”Ÿæˆä¸ç”¨æˆ·æŸ¥è¯¢ç›¸å…³çš„è¯¦ç»†è§£é‡Šï¼Œæ˜¾ç¤ºå‡ºå…¶ä½œä¸ºåˆ†å­åˆ†æé€šç”¨åŠ©æ‰‹çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.14397",
            "title": "PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data",
            "url": "https://huggingface.co/papers/2502.14397",
            "abstract": "We introduce PhotoDoodle, a novel image editing framework designed to facilitate photo doodling by enabling artists to overlay decorative elements onto photographs. Photo doodling is challenging because the inserted elements must appear seamlessly integrated with the background, requiring realistic blending, perspective alignment, and contextual coherence. Additionally, the background must be preserved without distortion, and the artist's unique style must be captured efficiently from limited training data. These requirements are not addressed by previous methods that primarily focus on global style transfer or regional inpainting. The proposed method, PhotoDoodle, employs a two-stage training strategy. Initially, we train a general-purpose image editing model, OmniEditor, using large-scale data. Subsequently, we fine-tune this model with EditLoRA using a small, artist-curated dataset of before-and-after image pairs to capture distinct editing styles and techniques. To enhance consistency in the generated results, we introduce a positional encoding reuse mechanism. Additionally, we release a PhotoDoodle dataset featuring six high-quality styles. Extensive experiments demonstrate the advanced performance and robustness of our method in customized image editing, opening new possibilities for artistic creation.",
            "score": 28,
            "issue_id": 2364,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 20",
                "zh": "2æœˆ20æ—¥"
            },
            "hash": "7e08e4606569ffb5",
            "authors": [
                "Shijie Huang",
                "Yiren Song",
                "Yuxuan Zhang",
                "Hailong Guo",
                "Xueyin Wang",
                "Mike Zheng Shou",
                "Jiaming Liu"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "Byte Dance",
                "National University of Singapore",
                "Shanghai Jiao Tong University",
                "Tiamat"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.14397.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#data",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "PhotoDoodle: ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹",
                    "desc": "PhotoDoodle - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ Ñ…ÑƒĞ´Ğ¾Ğ¶Ğ½Ğ¸ĞºĞ°Ğ¼ Ğ½Ğ°ĞºĞ»Ğ°Ğ´Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´ĞµĞºĞ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ OmniEditor, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ½Ğ° Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ…ÑƒĞ´Ğ¾Ğ¶Ğ½Ğ¸ĞºĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ EditLoRA. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ PhotoDoodle Ñ ÑˆĞµÑÑ‚ÑŒÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ¸Ğ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Seamless Artistic Integration with PhotoDoodle",
                    "desc": "PhotoDoodle is an innovative image editing framework that allows artists to add decorative elements to photographs while ensuring they blend seamlessly with the background. The challenge lies in maintaining realistic integration, perspective alignment, and preserving the original photo without distortion. To achieve this, PhotoDoodle uses a two-stage training approach, starting with a general image editing model and then fine-tuning it with a small dataset of artist-specific edits. The framework also introduces a positional encoding reuse mechanism to improve the consistency of the edited images, showcasing its effectiveness in customized artistic creation."
                },
                "zh": {
                    "title": "PhotoDoodleï¼šè‰ºæœ¯åˆ›ä½œçš„æ–°å¯èƒ½æ€§",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPhotoDoodleçš„æ–°å‹å›¾åƒç¼–è¾‘æ¡†æ¶ï¼Œæ—¨åœ¨å¸®åŠ©è‰ºæœ¯å®¶åœ¨ç…§ç‰‡ä¸Šå åŠ è£…é¥°å…ƒç´ ã€‚PhotoDoodleè§£å†³äº†å›¾åƒåˆæˆä¸­çš„å¤šä¸ªæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å…ƒç´ ä¸èƒŒæ™¯çš„æ— ç¼èåˆã€é€è§†å¯¹é½å’Œä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé¦–å…ˆä½¿ç”¨å¤§è§„æ¨¡æ•°æ®è®­ç»ƒé€šç”¨å›¾åƒç¼–è¾‘æ¨¡å‹OmniEditorï¼Œç„¶åé€šè¿‡å°å‹è‰ºæœ¯å®¶ç­–åˆ’çš„æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œä»¥æ•æ‰ç‹¬ç‰¹çš„ç¼–è¾‘é£æ ¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPhotoDoodleåœ¨å®šåˆ¶å›¾åƒç¼–è¾‘æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†è‰ºæœ¯åˆ›ä½œçš„æ–°å¯èƒ½æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12084",
            "title": "VLM$^2$-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues",
            "url": "https://huggingface.co/papers/2502.12084",
            "abstract": "Visually linking matching cues is a crucial ability in daily life, such as identifying the same person in multiple photos based on their cues, even without knowing who they are. Despite the extensive knowledge that vision-language models (VLMs) possess, it remains largely unexplored whether they are capable of performing this fundamental task. To address this, we introduce VLM^2-Bench, a benchmark designed to assess whether VLMs can Visually Link Matching cues, with 9 subtasks and over 3,000 test cases. Comprehensive evaluation across eight open-source VLMs and GPT-4o, along with further analysis of various language-side and vision-side prompting methods, leads to a total of eight key findings. We identify critical challenges in models' ability to link visual cues, highlighting a significant performance gap where even GPT-4o lags 34.80% behind humans. Based on these insights, we advocate for (i) enhancing core visual capabilities to improve adaptability and reduce reliance on prior knowledge, (ii) establishing clearer principles for integrating language-based reasoning in vision-centric tasks to prevent unnecessary biases, and (iii) shifting vision-text training paradigms toward fostering models' ability to independently structure and infer relationships among visual cues.",
            "score": 16,
            "issue_id": 2366,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "7af295c37b147c3a",
            "authors": [
                "Jianshu Zhang",
                "Dongyu Yao",
                "Renjie Pi",
                "Paul Pu Liang",
                "Yi R.",
                "Fung"
            ],
            "affiliations": [
                "CMU",
                "HKUST",
                "MIT"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12084.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#alignment",
                    "#interpretability",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ñ‹Ğ¼ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VLM^2-Bench - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM) ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 9 Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 3000 Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ Ğ²Ğ¾ÑĞµĞ¼ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… VLM Ğ¸ GPT-4o. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ğ³Ğ´Ğµ Ğ´Ğ°Ğ¶Ğµ GPT-4o Ğ¾Ñ‚ÑÑ‚Ğ°ĞµÑ‚ Ğ½Ğ° 34.80%. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµÑ‚ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing Visual Linking in VLMs",
                    "desc": "This paper introduces VLM^2-Bench, a new benchmark aimed at evaluating the ability of vision-language models (VLMs) to visually link matching cues, which is essential for tasks like recognizing the same person in different images. The benchmark consists of 9 subtasks and over 3,000 test cases, providing a comprehensive assessment of VLM performance. The evaluation reveals significant challenges, with models like GPT-4o showing a 34.80% performance gap compared to human capabilities. The authors suggest improvements in visual processing, clearer integration of language reasoning, and a shift in training paradigms to enhance models' ability to understand and relate visual cues independently."
                },
                "zh": {
                    "title": "æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„åŒ¹é…èƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†è§‰é“¾æ¥åŒ¹é…çº¿ç´¢æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶è€…ä»¬æå‡ºäº†VLM^2-BenchåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨è¯†åˆ«ç›¸åŒå¯¹è±¡æ—¶çš„è¡¨ç°ã€‚é€šè¿‡å¯¹å…«ä¸ªå¼€æºVLMå’ŒGPT-4oçš„å…¨é¢è¯„ä¼°ï¼Œå‘ç°æ¨¡å‹åœ¨é“¾æ¥è§†è§‰çº¿ç´¢æ–¹é¢å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œç”šè‡³GPT-4oæ¯”äººç±»ä½34.80%ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œä½œè€…å»ºè®®å¢å¼ºæ¨¡å‹çš„æ ¸å¿ƒè§†è§‰èƒ½åŠ›ï¼Œæ˜ç¡®è¯­è¨€æ¨ç†ä¸è§†è§‰ä»»åŠ¡çš„æ•´åˆåŸåˆ™ï¼Œå¹¶æ¨åŠ¨è§†è§‰-æ–‡æœ¬è®­ç»ƒèŒƒå¼çš„è½¬å˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.14922",
            "title": "SIFT: Grounding LLM Reasoning in Contexts via Stickers",
            "url": "https://huggingface.co/papers/2502.14922",
            "abstract": "This paper identifies the misinterpretation of the context can be a significant issue during the reasoning process of large language models, spanning from smaller models like Llama3.2-3B-Instruct to cutting-edge ones like DeepSeek-R1. For example, in the phrase \"10 dollars per kilo,\" LLMs might not recognize that \"per\" means \"for each,\" leading to calculation errors. We introduce a novel, post-training approach called **Stick to the Facts (SIFT)** to tackle this. SIFT leverages increasing inference-time compute to ground LLM reasoning in contexts. At the core of SIFT lies the *Sticker*, which is generated by the model itself to explicitly emphasize the key information within the context. Given the curated Sticker, SIFT generates two predictions -- one from the original query and one from the query augmented with the Sticker. If they differ, the Sticker is sequentially refined via *forward* optimization (to better align the extracted facts with the query) and *inverse* generation (to conform with the model's inherent tendencies) for more faithful reasoning outcomes. Studies across diverse models (from 3B to 100B+) and benchmarks (e.g., GSM8K, MATH-500) reveal consistent performance improvements. Notably, SIFT improves the pass@1 accuracy of DeepSeek-R1 on AIME2024 from 78.33% to **85.67**%, establishing a new state-of-the-art in the open-source community. The code is available at https://github.com/zhijie-group/SIFT.",
            "score": 15,
            "issue_id": 2363,
            "pub_date": "2025-02-19",
            "pub_date_card": {
                "ru": "19 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 19",
                "zh": "2æœˆ19æ—¥"
            },
            "hash": "b5ab16112068f00f",
            "authors": [
                "Zihao Zeng",
                "Xuyao Huang",
                "Boxiu Li",
                "Zhijie Deng"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.14922.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#inference",
                    "#math",
                    "#training",
                    "#reasoning",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "SIFT: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SIFT (Stick to the Facts) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). SIFT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 'Sticker' - ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Enhancing LLM Reasoning with Stick to the Facts (SIFT)",
                    "desc": "This paper addresses the problem of context misinterpretation in large language models (LLMs), which can lead to reasoning errors. It introduces a new post-training method called **Stick to the Facts (SIFT)** that enhances LLM reasoning by grounding it in context. SIFT utilizes a self-generated *Sticker* to highlight crucial information, allowing the model to produce two predictions for comparison. The method shows significant improvements in accuracy across various models and benchmarks, achieving a new state-of-the-art performance in the open-source community."
                },
                "zh": {
                    "title": "æå‡è¯­è¨€æ¨¡å‹æ¨ç†å‡†ç¡®æ€§çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡æŒ‡å‡ºï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯¹ä¸Šä¸‹æ–‡çš„è¯¯è§£å¯èƒ½ä¼šå¯¼è‡´æ˜¾è‘—é—®é¢˜ã€‚æ¯”å¦‚ï¼Œåœ¨â€œæ¯å…¬æ–¤10ç¾å…ƒâ€è¿™ä¸ªçŸ­è¯­ä¸­ï¼Œæ¨¡å‹å¯èƒ½æ— æ³•æ­£ç¡®ç†è§£â€œæ¯â€çš„æ„æ€ï¼Œä»è€Œå¯¼è‡´è®¡ç®—é”™è¯¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„åè®­ç»ƒæ–¹æ³•ï¼Œç§°ä¸º**Stick to the Facts (SIFT)**ï¼Œå®ƒé€šè¿‡å¢åŠ æ¨ç†æ—¶çš„è®¡ç®—é‡æ¥å¢å¼ºæ¨¡å‹çš„ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ã€‚SIFTçš„æ ¸å¿ƒæ˜¯ç”±æ¨¡å‹ç”Ÿæˆçš„*Sticker*ï¼Œå®ƒå¼ºè°ƒäº†ä¸Šä¸‹æ–‡ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œä»è€Œæé«˜æ¨ç†çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15589",
            "title": "LightThinker: Thinking Step-by-Step Compression",
            "url": "https://huggingface.co/papers/2502.15589",
            "abstract": "Large language models (LLMs) have shown remarkable performance in complex reasoning tasks, but their efficiency is hindered by the substantial memory and computational costs associated with generating lengthy tokens. In this paper, we propose LightThinker, a novel method that enables LLMs to dynamically compress intermediate thoughts during reasoning. Inspired by human cognitive processes, LightThinker compresses verbose thought steps into compact representations and discards the original reasoning chains, thereby significantly reducing the number of tokens stored in the context window. This is achieved by training the model on when and how to perform compression through data construction, mapping hidden states to condensed gist tokens, and creating specialized attention masks. Additionally, we introduce the Dependency (Dep) metric to quantify the degree of compression by measuring the reliance on historical tokens during generation. Extensive experiments on four datasets and two models show that LightThinker reduces peak memory usage and inference time, while maintaining competitive accuracy. Our work provides a new direction for improving the efficiency of LLMs in complex reasoning tasks without sacrificing performance. Code will be released at https://github.com/zjunlp/LightThinker.",
            "score": 14,
            "issue_id": 2365,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 21",
                "zh": "2æœˆ21æ—¥"
            },
            "hash": "563a32fe7bab988e",
            "authors": [
                "Jintian Zhang",
                "Yuqi Zhu",
                "Mengshu Sun",
                "Yujie Luo",
                "Shuofei Qiao",
                "Lun Du",
                "Da Zheng",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "affiliations": [
                "Ant Group",
                "Zhejiang University",
                "Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15589.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#architecture",
                    "#reasoning",
                    "#long_context",
                    "#optimization",
                    "#data"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "LightThinker: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "LightThinker - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (LLM) Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¶Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ñ‹ÑĞ»Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¼ Ğ¾ĞºĞ½Ğµ. LightThinker Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ‚Ğ¾Ğ¼Ñƒ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸ ĞºĞ°Ğº Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ, Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ² ÑĞ¶Ğ°Ñ‚Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹-ÑÑƒÑ‚ÑŒ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ°ÑĞºĞ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿Ğ¸ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "LightThinker: Efficient Reasoning through Dynamic Thought Compression",
                    "desc": "This paper introduces LightThinker, a method designed to enhance the efficiency of large language models (LLMs) during complex reasoning tasks. By dynamically compressing intermediate thoughts, LightThinker reduces the memory and computational costs associated with generating lengthy tokens. The approach mimics human cognitive processes by transforming verbose reasoning into compact representations, which helps in minimizing the number of tokens stored. The authors also present a new metric, Dependency (Dep), to measure the effectiveness of this compression, demonstrating that LightThinker can lower memory usage and inference time while maintaining accuracy across various datasets."
                },
                "zh": {
                    "title": "LightThinkerï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡çš„æ–°æ–¹æ³•",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç”Ÿæˆé•¿æ–‡æœ¬æ—¶çš„å†…å­˜å’Œè®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•LightThinkerï¼Œèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€å‹ç¼©ä¸­é—´æ€ç»´ã€‚LightThinkerå€Ÿé‰´äººç±»è®¤çŸ¥è¿‡ç¨‹ï¼Œå°†å†—é•¿çš„æ€ç»´æ­¥éª¤å‹ç¼©ä¸ºç´§å‡‘çš„è¡¨ç¤ºï¼Œä»è€Œæ˜¾è‘—å‡å°‘ä¸Šä¸‹æ–‡çª—å£ä¸­å­˜å‚¨çš„æ ‡è®°æ•°é‡ã€‚é€šè¿‡åœ¨æ•°æ®æ„å»ºä¸­è®­ç»ƒæ¨¡å‹è¿›è¡Œå‹ç¼©ï¼Œå¹¶å¼•å…¥ä¾èµ–åº¦ï¼ˆDepï¼‰æŒ‡æ ‡æ¥é‡åŒ–å‹ç¼©ç¨‹åº¦ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜LightThinkeråœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶ï¼Œé™ä½äº†å³°å€¼å†…å­˜ä½¿ç”¨å’Œæ¨ç†æ—¶é—´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.14494",
            "title": "StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following",
            "url": "https://huggingface.co/papers/2502.14494",
            "abstract": "Multi-turn instruction following capability constitutes a core competency of large language models (LLMs) in real-world applications. Existing evaluation benchmarks predominantly focus on fine-grained constraint satisfaction and domain-specific capability assessment, yet overlook the crucial structural dependency between dialogue turns that distinguishes multi-turn from single-turn interactions. This structural dependency not only reflects user intent but also establishes a second dimension for instruction following evaluation beyond constraint satisfaction. To address this gap, we propose StructFlowBench, a multi-turn instruction following benchmark with structural flow modeling. The benchmark innovatively defines a structural flow framework comprising six fundamental inter-turn relationships, which not only introduces novel structural constraints for model evaluation but also serves as generation parameters for creating customized dialogue flows tailored to specific scenarios. Adopting established LLM-based automatic evaluation methodologies, we conduct systematic evaluations of 13 leading open-source and closed-source LLMs. Experimental results reveal significant deficiencies in current models' comprehension of multi-turn dialogue structures. The code is available at https://github.com/MLGroupJLU/StructFlowBench.",
            "score": 11,
            "issue_id": 2365,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 20",
                "zh": "2æœˆ20æ—¥"
            },
            "hash": "512d952f8463678a",
            "authors": [
                "Jinnan Li",
                "Jinzhe Li",
                "Yue Wang",
                "Yi Chang",
                "Yuan Wu"
            ],
            "affiliations": [
                "College of Computer Science and Technology, Jilin University",
                "Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, MOE, China",
                "International Center of Future Science, Jilin University",
                "School of Artificial Intelligence, Jilin University",
                "School of Information and Library Science, University of North Carolina at Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.14494.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ LLM Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº StructFlowBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², StructFlowBench ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ĞµĞ¿Ğ»Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ ÑˆĞµÑÑ‚ÑŒ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµĞ¶Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 13 Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Multi-Turn Dialogue Understanding in LLMs",
                    "desc": "This paper introduces StructFlowBench, a new benchmark designed to evaluate the multi-turn instruction following capabilities of large language models (LLMs). It highlights the importance of understanding the structural dependencies between dialogue turns, which are often ignored in existing benchmarks. By defining six fundamental inter-turn relationships, StructFlowBench provides a framework for assessing how well models can follow instructions across multiple dialogue turns. The study reveals that many current LLMs struggle with these structural aspects, indicating a need for improvement in their dialogue comprehension."
                },
                "zh": {
                    "title": "å¤šè½®å¯¹è¯è¯„ä¼°çš„æ–°æ ‡å‡†",
                    "desc": "æœ¬æ–‡æå‡ºäº†StructFlowBenchï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤šè½®æŒ‡ä»¤è·Ÿéšèƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å¡«è¡¥ç°æœ‰è¯„ä¼°æ–¹æ³•çš„ç©ºç™½ã€‚ç°æœ‰çš„è¯„ä¼°ä¸»è¦å…³æ³¨ç»†ç²’åº¦çš„çº¦æŸæ»¡è¶³å’Œç‰¹å®šé¢†åŸŸèƒ½åŠ›è¯„ä¼°ï¼Œä½†å¿½è§†äº†å¯¹è¯è½®æ¬¡ä¹‹é—´çš„ç»“æ„ä¾èµ–å…³ç³»ã€‚æˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªç»“æ„æµæ¡†æ¶ï¼ŒåŒ…å«å…­ç§åŸºæœ¬çš„è½®æ¬¡å…³ç³»ï¼Œä»¥æ­¤ä¸ºæ¨¡å‹è¯„ä¼°å¼•å…¥æ–°çš„ç»“æ„çº¦æŸã€‚é€šè¿‡å¯¹13ä¸ªé¢†å…ˆçš„å¼€æºå’Œé—­æºå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå½“å‰æ¨¡å‹åœ¨ç†è§£å¤šè½®å¯¹è¯ç»“æ„æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15086",
            "title": "Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of Large Language Models",
            "url": "https://huggingface.co/papers/2502.15086",
            "abstract": "As the use of large language model (LLM) agents continues to grow, their safety vulnerabilities have become increasingly evident. Extensive benchmarks evaluate various aspects of LLM safety by defining the safety relying heavily on general standards, overlooking user-specific standards. However, safety standards for LLM may vary based on a user-specific profiles rather than being universally consistent across all users. This raises a critical research question: Do LLM agents act safely when considering user-specific safety standards? Despite its importance for safe LLM use, no benchmark datasets currently exist to evaluate the user-specific safety of LLMs. To address this gap, we introduce U-SAFEBENCH, the first benchmark designed to assess user-specific aspect of LLM safety. Our evaluation of 18 widely used LLMs reveals current LLMs fail to act safely when considering user-specific safety standards, marking a new discovery in this field. To address this vulnerability, we propose a simple remedy based on chain-of-thought, demonstrating its effectiveness in improving user-specific safety. Our benchmark and code are available at https://github.com/yeonjun-in/U-SafeBench.",
            "score": 8,
            "issue_id": 2373,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 20",
                "zh": "2æœˆ20æ—¥"
            },
            "hash": "1dcbddc768d1e8df",
            "authors": [
                "Yeonjun In",
                "Wonjoong Kim",
                "Kanghoon Yoon",
                "Sungchul Kim",
                "Mehrab Tanjim",
                "Kibum Kim",
                "Chanyoung Park"
            ],
            "affiliations": [
                "Adobe Research",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15086.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#security",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ LLM: ÑƒÑ‡ĞµÑ‚ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ U-SAFEBENCH - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¾Ğ². ĞÑ†ĞµĞ½ĞºĞ° 18 Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Enhancing LLM Safety with User-Specific Standards",
                    "desc": "This paper discusses the safety vulnerabilities of large language models (LLMs) and highlights the need for user-specific safety standards rather than relying on general benchmarks. It introduces U-SAFEBENCH, a new benchmark designed to evaluate how well LLMs adhere to these user-specific safety standards. The authors found that many popular LLMs do not perform safely when assessed against these tailored standards. To improve safety, they propose a chain-of-thought approach that effectively enhances user-specific safety in LLMs."
                },
                "zh": {
                    "title": "å…³æ³¨ç”¨æˆ·ç‰¹å®šçš„LLMå®‰å…¨æ€§",
                    "desc": "éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„ä½¿ç”¨ä¸æ–­å¢åŠ ï¼Œå®ƒä»¬çš„å®‰å…¨æ¼æ´å˜å¾—è¶Šæ¥è¶Šæ˜æ˜¾ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–äºé€šç”¨æ ‡å‡†ï¼Œå¿½è§†äº†ç”¨æˆ·ç‰¹å®šçš„å®‰å…¨æ ‡å‡†ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLLMçš„å®‰å…¨æ ‡å‡†å¯èƒ½å› ç”¨æˆ·çš„ä¸åŒè€Œæœ‰æ‰€ä¸åŒï¼Œè€Œä¸æ˜¯å¯¹æ‰€æœ‰ç”¨æˆ·éƒ½ä¸€è‡´ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†U-SAFEBENCHï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°ç”¨æˆ·ç‰¹å®šLLMå®‰å…¨æ€§çš„åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.14949",
            "title": "KITAB-Bench: A Comprehensive Multi-Domain Benchmark for Arabic OCR and Document Understanding",
            "url": "https://huggingface.co/papers/2502.14949",
            "abstract": "With the growing adoption of Retrieval-Augmented Generation (RAG) in document processing, robust text recognition has become increasingly critical for knowledge extraction. While OCR (Optical Character Recognition) for English and other languages benefits from large datasets and well-established benchmarks, Arabic OCR faces unique challenges due to its cursive script, right-to-left text flow, and complex typographic and calligraphic features. We present KITAB-Bench, a comprehensive Arabic OCR benchmark that fills the gaps in current evaluation systems. Our benchmark comprises 8,809 samples across 9 major domains and 36 sub-domains, encompassing diverse document types including handwritten text, structured tables, and specialized coverage of 21 chart types for business intelligence. Our findings show that modern vision-language models (such as GPT-4, Gemini, and Qwen) outperform traditional OCR approaches (like EasyOCR, PaddleOCR, and Surya) by an average of 60% in Character Error Rate (CER). Furthermore, we highlight significant limitations of current Arabic OCR models, particularly in PDF-to-Markdown conversion, where the best model Gemini-2.0-Flash achieves only 65% accuracy. This underscores the challenges in accurately recognizing Arabic text, including issues with complex fonts, numeral recognition errors, word elongation, and table structure detection. This work establishes a rigorous evaluation framework that can drive improvements in Arabic document analysis methods and bridge the performance gap with English OCR technologies.",
            "score": 5,
            "issue_id": 2371,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 20",
                "zh": "2æœˆ20æ—¥"
            },
            "hash": "f7184152f13a0145",
            "authors": [
                "Ahmed Heakl",
                "Abdullah Sohail",
                "Mukul Ranjan",
                "Rania Hossam",
                "Ghazi Ahmed",
                "Mohamed El-Geish",
                "Omar Maher",
                "Zhiqiang Shen",
                "Fahad Khan",
                "Salman Khan"
            ],
            "affiliations": [
                "Australian National University",
                "LinkÃ¶ping University",
                "MBZUAI",
                "Monta AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.14949.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#benchmark",
                    "#low_resource",
                    "#dataset",
                    "#science",
                    "#optimization"
                ],
                "emoji": "ğŸ“š",
                "ru": {
                    "title": "KITAB-Bench: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ OCR",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ KITAB-Bench - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° (OCR). ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 8809 Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¸Ğ· 9 Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² Ğ¸ 36 Ğ¿Ğ¾Ğ´Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº OCR Ğ½Ğ° 60% Ğ¿Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² (CER). ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ 65% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ°Ñ†Ğ¸Ğ¸ PDF Ğ² Markdown, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "Bridging the Gap: Advancing Arabic OCR with KITAB-Bench",
                    "desc": "This paper introduces KITAB-Bench, a new benchmark specifically designed for evaluating Arabic Optical Character Recognition (OCR) systems. It addresses the unique challenges of Arabic text, such as its cursive nature and right-to-left orientation, which complicate accurate recognition. The study reveals that modern vision-language models significantly outperform traditional OCR methods in recognizing Arabic text, achieving a 60% improvement in Character Error Rate (CER). Additionally, it identifies key limitations in current Arabic OCR models, particularly in converting PDFs to Markdown, highlighting the need for enhanced techniques in Arabic document processing."
                },
                "zh": {
                    "title": "æå‡é˜¿æ‹‰ä¼¯è¯­OCRçš„åŸºå‡†ä¸æŒ‘æˆ˜",
                    "desc": "éšç€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨æ–‡æ¡£å¤„ç†ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œå¼ºå¤§çš„æ–‡æœ¬è¯†åˆ«å˜å¾—æ„ˆå‘é‡è¦ã€‚é˜¿æ‹‰ä¼¯è¯­çš„å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå¦‚è¿å†™å­—æ¯ã€ä»å³åˆ°å·¦çš„æ–‡æœ¬æµä»¥åŠå¤æ‚çš„æ’ç‰ˆç‰¹å¾ã€‚æˆ‘ä»¬æå‡ºäº†KITAB-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„é˜¿æ‹‰ä¼¯è¯­OCRåŸºå‡†ï¼Œæ¶µç›–äº†8809ä¸ªæ ·æœ¬ï¼Œæ¶‰åŠ9ä¸ªä¸»è¦é¢†åŸŸå’Œ36ä¸ªå­é¢†åŸŸã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰ä¸Šæ¯”ä¼ ç»ŸOCRæ–¹æ³•å¹³å‡æé«˜äº†60%ï¼Œå¹¶å¼ºè°ƒäº†å½“å‰é˜¿æ‹‰ä¼¯è¯­OCRæ¨¡å‹çš„æ˜¾è‘—å±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15027",
            "title": "InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback",
            "url": "https://huggingface.co/papers/2502.15027",
            "abstract": "Existing benchmarks do not test Large Multimodal Models (LMMs) on their interactive intelligence with human users which is vital for developing general-purpose AI assistants. We design InterFeedback, an interactive framework, which can be applied to any LMM and dataset to assess this ability autonomously. On top of this, we introduce InterFeedback-Bench which evaluates interactive intelligence using two representative datasets, MMMU-Pro and MathVerse, to test 10 different open-source LMMs. Additionally, we present InterFeedback-Human, a newly collected dataset of 120 cases designed for manually testing interactive performance in leading models such as OpenAI-o1 and Claude-3.5-Sonnet. Our evaluation results show that even state-of-the-art LMM (like OpenAI-o1) can correct their results through human feedback less than 50%. Our findings point to the need for methods that can enhance the LMMs' capability to interpret and benefit from feedback.",
            "score": 4,
            "issue_id": 2363,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 20",
                "zh": "2æœˆ20æ—¥"
            },
            "hash": "3b32932adf862766",
            "authors": [
                "Henry Hengyuan Zhao",
                "Wenqi Pei",
                "Yifei Tao",
                "Haiyang Mei",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15027.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#rlhf",
                    "#dataset",
                    "#agi",
                    "#multimodal",
                    "#open_source"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ LMM: Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº InterFeedback Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ InterFeedback-Bench Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 10 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LMM Ğ½Ğ° Ğ´Ğ²ÑƒÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… InterFeedback-Human Ğ´Ğ»Ñ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ LMM ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 50% ÑĞ»ÑƒÑ‡Ğ°ĞµĞ²."
                },
                "en": {
                    "title": "Enhancing Interactive Intelligence in Large Multimodal Models",
                    "desc": "This paper addresses the gap in evaluating Large Multimodal Models (LMMs) regarding their interactive intelligence with human users. The authors introduce InterFeedback, a framework that autonomously assesses the interactive capabilities of any LMM across various datasets. They also present InterFeedback-Bench, which tests 10 open-source LMMs using two datasets, MMMU-Pro and MathVerse, and InterFeedback-Human, a dataset for manual evaluation of models like OpenAI-o1 and Claude-3.5-Sonnet. The results reveal that even advanced LMMs struggle to incorporate human feedback effectively, highlighting the need for improved methods to enhance their responsiveness to user interactions."
                },
                "zh": {
                    "title": "æå‡å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„äº’åŠ¨æ™ºèƒ½",
                    "desc": "ç°æœ‰çš„åŸºå‡†æµ‹è¯•å¹¶æœªè¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ä¸äººç±»ç”¨æˆ·çš„äº’åŠ¨æ™ºèƒ½ï¼Œè€Œè¿™å¯¹äºå¼€å‘é€šç”¨äººå·¥æ™ºèƒ½åŠ©æ‰‹è‡³å…³é‡è¦ã€‚æˆ‘ä»¬è®¾è®¡äº†InterFeedbackï¼Œè¿™æ˜¯ä¸€ä¸ªäº’åŠ¨æ¡†æ¶ï¼Œå¯ä»¥åº”ç”¨äºä»»ä½•LMMå’Œæ•°æ®é›†ï¼Œä»¥è‡ªä¸»è¯„ä¼°è¿™ç§èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¨å‡ºäº†InterFeedback-Benchï¼Œä½¿ç”¨ä¸¤ä¸ªä»£è¡¨æ€§æ•°æ®é›†MMM-Proå’ŒMathVerseæ¥è¯„ä¼°10ç§ä¸åŒçš„å¼€æºLMMçš„äº’åŠ¨æ™ºèƒ½ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„LMMï¼ˆå¦‚OpenAI-o1ï¼‰ä¹Ÿåªèƒ½é€šè¿‡äººç±»åé¦ˆçº æ­£ä¸åˆ°50%çš„ç»“æœï¼Œæ˜¾ç¤ºå‡ºå¢å¼ºLMMè§£è¯»å’Œåˆ©ç”¨åé¦ˆèƒ½åŠ›çš„æ–¹æ³•çš„å¿…è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15422",
            "title": "Evaluating Multimodal Generative AI with Korean Educational Standards",
            "url": "https://huggingface.co/papers/2502.15422",
            "abstract": "This paper presents the Korean National Educational Test Benchmark (KoNET), a new benchmark designed to evaluate Multimodal Generative AI Systems using Korean national educational tests. KoNET comprises four exams: the Korean Elementary General Educational Development Test (KoEGED), Middle (KoMGED), High (KoHGED), and College Scholastic Ability Test (KoCSAT). These exams are renowned for their rigorous standards and diverse questions, facilitating a comprehensive analysis of AI performance across different educational levels. By focusing on Korean, KoNET provides insights into model performance in less-explored languages. We assess a range of models - open-source, open-access, and closed APIs - by examining difficulties, subject diversity, and human error rates. The code and dataset builder will be made fully open-sourced at https://github.com/naver-ai/KoNET.",
            "score": 3,
            "issue_id": 2372,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 21",
                "zh": "2æœˆ21æ—¥"
            },
            "hash": "f25bc188f9466ff0",
            "authors": [
                "Sanghee Park",
                "Geewook Kim"
            ],
            "affiliations": [
                "KAIST AI",
                "NAVER Cloud AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15422.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#benchmark",
                    "#dataset",
                    "#open_source",
                    "#multilingual"
                ],
                "emoji": "ğŸ‡°ğŸ‡·",
                "ru": {
                    "title": "KoNET: ĞšĞ¾Ñ€ĞµĞ¹ÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº KoNET Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¸Ñ… Ğ½Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ÑĞºĞ·Ğ°Ğ¼ĞµĞ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ñ‚ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑˆĞºĞ¾Ğ»Ñ‹ Ğ´Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚Ğ°. KoNET Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ˜Ğ˜ Ğ½Ğ° ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼ĞµĞ½ĞµĞµ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹, Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ¾Ğ² Ğ¸ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº."
                },
                "en": {
                    "title": "Evaluating AI Performance with Korean Educational Tests",
                    "desc": "This paper introduces the Korean National Educational Test Benchmark (KoNET), which is designed to assess the capabilities of Multimodal Generative AI Systems using standardized Korean educational tests. KoNET includes four distinct exams that cover various educational stages, allowing for a thorough evaluation of AI performance in a structured manner. The benchmark aims to provide valuable insights into how well AI models perform in the Korean language, which is often underrepresented in AI research. The study evaluates multiple models, including open-source and closed APIs, by analyzing their performance across different question types and levels of difficulty."
                },
                "zh": {
                    "title": "è¯„ä¼°å¤šæ¨¡æ€AIçš„æ–°åŸºå‡†ï¼šKoNET",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†éŸ©å›½å›½å®¶æ•™è‚²æµ‹è¯•åŸºå‡†ï¼ˆKoNETï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€ç”ŸæˆAIç³»ç»Ÿçš„æ–°åŸºå‡†ï¼ŒåŸºäºéŸ©å›½çš„å›½å®¶æ•™è‚²æµ‹è¯•ã€‚KoNETåŒ…å«å››ä¸ªè€ƒè¯•ï¼šéŸ©å›½å°å­¦ç»¼åˆæ•™è‚²å‘å±•æµ‹è¯•ï¼ˆKoEGEDï¼‰ã€ä¸­å­¦ï¼ˆKoMGEDï¼‰ã€é«˜ä¸­ï¼ˆKoHGEDï¼‰å’Œå¤§å­¦å…¥å­¦èƒ½åŠ›æµ‹è¯•ï¼ˆKoCSATï¼‰ã€‚è¿™äº›è€ƒè¯•ä»¥å…¶ä¸¥æ ¼çš„æ ‡å‡†å’Œå¤šæ ·åŒ–çš„é—®é¢˜è€Œé—»åï¼Œèƒ½å¤Ÿå…¨é¢åˆ†æAIåœ¨ä¸åŒæ•™è‚²æ°´å¹³ä¸Šçš„è¡¨ç°ã€‚é€šè¿‡å…³æ³¨éŸ©è¯­ï¼ŒKoNETä¸ºæ¨¡å‹åœ¨è¾ƒå°‘æ¢ç´¢çš„è¯­è¨€ä¸­çš„è¡¨ç°æä¾›äº†æ·±å…¥çš„è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.14637",
            "title": "ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality Protein Backbone Generation",
            "url": "https://huggingface.co/papers/2502.14637",
            "abstract": "Protein backbone generation plays a central role in de novo protein design and is significant for many biological and medical applications. Although diffusion and flow-based generative models provide potential solutions to this challenging task, they often generate proteins with undesired designability and suffer computational inefficiency. In this study, we propose a novel rectified quaternion flow (ReQFlow) matching method for fast and high-quality protein backbone generation. In particular, our method generates a local translation and a 3D rotation from random noise for each residue in a protein chain, which represents each 3D rotation as a unit quaternion and constructs its flow by spherical linear interpolation (SLERP) in an exponential format. We train the model by quaternion flow (QFlow) matching with guaranteed numerical stability and rectify the QFlow model to accelerate its inference and improve the designability of generated protein backbones, leading to the proposed ReQFlow model. Experiments show that ReQFlow achieves state-of-the-art performance in protein backbone generation while requiring much fewer sampling steps and significantly less inference time (e.g., being 37x faster than RFDiffusion and 62x faster than Genie2 when generating a backbone of length 300), demonstrating its effectiveness and efficiency. The code is available at https://github.com/AngxiaoYue/ReQFlow.",
            "score": 3,
            "issue_id": 2371,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 20",
                "zh": "2æœˆ20æ—¥"
            },
            "hash": "2b9039b1aeecb0ab",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#diffusion",
                    "#3d",
                    "#healthcare",
                    "#inference",
                    "#open_source",
                    "#optimization"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "ReQFlow: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ»ĞºĞ¾Ğ²Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ReQFlow Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ»ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¾ÑÑ‚Ğ¾Ğ²Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ²Ğ°Ñ‚ĞµÑ€Ğ½Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ 3D-Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑĞ»ÑÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ° Ğ² Ğ±ĞµĞ»ĞºĞ¾Ğ²Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¸. ReQFlow Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ»ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¾ÑÑ‚Ğ¾Ğ²Ğ¾Ğ², Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ ÑˆĞ°Ğ³Ğ¾Ğ² ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ReQFlow Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² 37 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ RFDiffusion Ğ¸ Ğ² 62 Ñ€Ğ°Ğ·Ğ° Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Genie2 Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑÑ‚Ğ¾Ğ²Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ 300 Ğ°Ğ¼Ğ¸Ğ½Ğ¾ĞºĞ¸ÑĞ»Ğ¾Ñ‚."
                },
                "en": {
                    "title": "ReQFlow: Fast and Efficient Protein Backbone Generation",
                    "desc": "This paper introduces a new method called rectified quaternion flow (ReQFlow) for generating protein backbones, which is crucial for designing proteins from scratch. The authors address the limitations of existing generative models, which often produce proteins that are not optimal and are slow to compute. ReQFlow improves efficiency by using quaternion representations for 3D rotations and employs a technique called spherical linear interpolation (SLERP) to create smooth transitions in protein structures. The results show that ReQFlow outperforms previous models in both speed and quality, making it a significant advancement in the field of protein design."
                },
                "zh": {
                    "title": "é«˜æ•ˆè›‹ç™½è´¨éª¨æ¶ç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¿®æ­£å››å…ƒæ•°æµï¼ˆReQFlowï¼‰åŒ¹é…æ–¹æ³•ï¼Œç”¨äºå¿«é€Ÿé«˜è´¨é‡çš„è›‹ç™½è´¨éª¨æ¶ç”Ÿæˆã€‚è¯¥æ–¹æ³•é€šè¿‡éšæœºå™ªå£°ä¸ºæ¯ä¸ªæ°¨åŸºé…¸æ®‹åŸºç”Ÿæˆå±€éƒ¨å¹³ç§»å’Œä¸‰ç»´æ—‹è½¬ï¼Œå¹¶ä½¿ç”¨å•ä½å››å…ƒæ•°è¡¨ç¤ºä¸‰ç»´æ—‹è½¬ã€‚æˆ‘ä»¬é€šè¿‡å››å…ƒæ•°æµï¼ˆQFlowï¼‰åŒ¹é…è®­ç»ƒæ¨¡å‹ï¼Œç¡®ä¿æ•°å€¼ç¨³å®šæ€§ï¼Œå¹¶ä¿®æ­£QFlowæ¨¡å‹ä»¥åŠ é€Ÿæ¨ç†å’Œæé«˜ç”Ÿæˆè›‹ç™½è´¨éª¨æ¶çš„è®¾è®¡æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReQFlowåœ¨è›‹ç™½è´¨éª¨æ¶ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ¨ç†é€Ÿåº¦æ˜¾è‘—æé«˜ï¼Œæ•ˆç‡è¿œè¶…ç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.13189",
            "title": "MoBA: Mixture of Block Attention for Long-Context LLMs",
            "url": "https://huggingface.co/papers/2502.13189",
            "abstract": "Scaling the effective context length is essential for advancing large language models (LLMs) toward artificial general intelligence (AGI). However, the quadratic increase in computational complexity inherent in traditional attention mechanisms presents a prohibitive overhead. Existing approaches either impose strongly biased structures, such as sink or window attention which are task-specific, or radically modify the attention mechanism into linear approximations, whose performance in complex reasoning tasks remains inadequately explored.   In this work, we propose a solution that adheres to the ``less structure'' principle, allowing the model to determine where to attend autonomously, rather than introducing predefined biases. We introduce Mixture of Block Attention (MoBA), an innovative approach that applies the principles of Mixture of Experts (MoE) to the attention mechanism. This novel architecture demonstrates superior performance on long-context tasks while offering a key advantage: the ability to seamlessly transition between full and sparse attention, enhancing efficiency without the risk of compromising performance. MoBA has already been deployed to support Kimi's long-context requests and demonstrates significant advancements in efficient attention computation for LLMs. Our code is available at https://github.com/MoonshotAI/MoBA.",
            "score": 3,
            "issue_id": 2370,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 18",
                "zh": "2æœˆ18æ—¥"
            },
            "hash": "a2a79c273e7a0f43",
            "authors": [
                "Enzhe Lu",
                "Zhejun Jiang",
                "Jingyuan Liu",
                "Yulun Du",
                "Tao Jiang",
                "Chao Hong",
                "Shaowei Liu",
                "Weiran He",
                "Enming Yuan",
                "Yuzhi Wang",
                "Zhiqi Huang",
                "Huan Yuan",
                "Suting Xu",
                "Xinran Xu",
                "Guokun Lai",
                "Yanru Chen",
                "Huabin Zheng",
                "Junjie Yan",
                "Jianlin Su",
                "Yuxin Wu",
                "Neo Y. Zhang",
                "Zhilin Yang",
                "Xinyu Zhou",
                "Mingxing Zhang",
                "Jiezhong Qiu"
            ],
            "affiliations": [
                "Moonshot AI",
                "Tsinghua University",
                "Zhejiang Lab/Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.13189.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#long_context",
                    "#training",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "MoBA: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ LLM Ğ±ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Mixture of Block Attention (MoBA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Mixture of Experts Ğº Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñƒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. MoBA Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ, Ğ½Ğ° Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Empowering LLMs with Efficient Context Scaling through MoBA",
                    "desc": "This paper addresses the challenge of scaling context length in large language models (LLMs) while managing computational complexity. Traditional attention mechanisms face a quadratic increase in resource demands, which can hinder performance. The authors propose Mixture of Block Attention (MoBA), a new method that allows models to autonomously determine their attention focus without imposing rigid structures. MoBA combines the principles of Mixture of Experts with attention, enabling efficient processing of long contexts and improving performance on complex reasoning tasks."
                },
                "zh": {
                    "title": "æå‡é•¿ä¸Šä¸‹æ–‡å¤„ç†æ•ˆç‡çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç§°ä¸ºæ··åˆå—æ³¨æ„åŠ›ï¼ˆMoBAï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶çš„æ•ˆç‡ã€‚ä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶åœ¨è®¡ç®—å¤æ‚åº¦ä¸Šå‘ˆç°äºŒæ¬¡å¢é•¿ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ‰©å±•æ€§ã€‚MoBAéµå¾ªâ€œå°‘ç»“æ„â€åŸåˆ™ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»å†³å®šå…³æ³¨çš„å†…å®¹ï¼Œè€Œä¸æ˜¯ä¾èµ–äºé¢„å®šä¹‰çš„åè§ã€‚è¯¥æ–¹æ³•åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶èƒ½å¤Ÿåœ¨å…¨æ³¨æ„åŠ›å’Œç¨€ç–æ³¨æ„åŠ›ä¹‹é—´æ— ç¼åˆ‡æ¢ï¼Œæå‡äº†è®¡ç®—æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15657",
            "title": "Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?",
            "url": "https://huggingface.co/papers/2502.15657",
            "abstract": "The leading AI companies are increasingly focused on building generalist AI agents -- systems that can autonomously plan, act, and pursue goals across almost all tasks that humans can perform. Despite how useful these systems might be, unchecked AI agency poses significant risks to public safety and security, ranging from misuse by malicious actors to a potentially irreversible loss of human control. We discuss how these risks arise from current AI training methods. Indeed, various scenarios and experiments have demonstrated the possibility of AI agents engaging in deception or pursuing goals that were not specified by human operators and that conflict with human interests, such as self-preservation. Following the precautionary principle, we see a strong need for safer, yet still useful, alternatives to the current agency-driven trajectory. Accordingly, we propose as a core building block for further advances the development of a non-agentic AI system that is trustworthy and safe by design, which we call Scientist AI. This system is designed to explain the world from observations, as opposed to taking actions in it to imitate or please humans. It comprises a world model that generates theories to explain data and a question-answering inference machine. Both components operate with an explicit notion of uncertainty to mitigate the risks of overconfident predictions. In light of these considerations, a Scientist AI could be used to assist human researchers in accelerating scientific progress, including in AI safety. In particular, our system can be employed as a guardrail against AI agents that might be created despite the risks involved. Ultimately, focusing on non-agentic AI may enable the benefits of AI innovation while avoiding the risks associated with the current trajectory. We hope these arguments will motivate researchers, developers, and policymakers to favor this safer path.",
            "score": 3,
            "issue_id": 2365,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 21",
                "zh": "2æœˆ21æ—¥"
            },
            "hash": "66434dc15bcc1913",
            "authors": [
                "Yoshua Bengio",
                "Michael Cohen",
                "Damiano Fornasiere",
                "Joumana Ghosn",
                "Pietro Greiner",
                "Matt MacDermott",
                "SÃ¶ren Mindermann",
                "Adam Oberman",
                "Jesse Richardson",
                "Oliver Richardson",
                "Marc-Antoine Rondeau",
                "Pierre-Luc St-Charles",
                "David Williams-King"
            ],
            "affiliations": [
                "Imperial College London",
                "McGill University",
                "Mila Quebec AI Institute",
                "Universite de Montreal",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15657.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#agents",
                    "#security",
                    "#science",
                    "#agi",
                    "#ethics"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¹ Ğ˜Ğ˜: Ğ¾Ñ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğº ÑƒÑ‡ĞµĞ½Ñ‹Ğ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ñ€Ğ¸ÑĞºĞ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ½ĞµĞ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ˜Ğ˜ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Scientist AI, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ Ğ¼Ğ¸Ñ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ½Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ² Ğ½ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ°, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ¹ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Scientist AI Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¾Ñ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Towards Safer AI: Embracing Non-Agentic Systems",
                    "desc": "This paper discusses the risks associated with developing generalist AI agents that can operate autonomously across various tasks. It highlights how current AI training methods can lead to unintended behaviors, such as deception or pursuing harmful goals. To address these concerns, the authors propose a new approach called Scientist AI, which focuses on creating a non-agentic system that explains observations rather than taking actions. This system aims to assist human researchers while ensuring safety and trustworthiness, ultimately promoting a safer path for AI development."
                },
                "zh": {
                    "title": "å®‰å…¨ä¸åˆ›æ–°å¹¶é‡çš„ç§‘å­¦å®¶AI",
                    "desc": "æœ¬æ–‡è®¨è®ºäº†å½“å‰äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç³»ç»Ÿåœ¨è‡ªä¸»è§„åˆ’å’Œæ‰§è¡Œä»»åŠ¡æ–¹é¢çš„é£é™©ï¼Œå°¤å…¶æ˜¯å½“è¿™äº›ç³»ç»Ÿå¯èƒ½åç¦»äººç±»çš„æ„å›¾æ—¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„éä»£ç†AIç³»ç»Ÿï¼Œç§°ä¸ºç§‘å­¦å®¶AIï¼Œå®ƒæ—¨åœ¨é€šè¿‡è§‚å¯Ÿæ¥è§£é‡Šä¸–ç•Œï¼Œè€Œä¸æ˜¯ä¸»åŠ¨é‡‡å–è¡ŒåŠ¨ã€‚ç§‘å­¦å®¶AIåŒ…æ‹¬ä¸€ä¸ªä¸–ç•Œæ¨¡å‹å’Œä¸€ä¸ªé—®ç­”æ¨ç†æœºå™¨ï¼Œèƒ½å¤Ÿå¤„ç†ä¸ç¡®å®šæ€§ï¼Œä»è€Œå‡å°‘è¿‡äºè‡ªä¿¡çš„é¢„æµ‹å¸¦æ¥çš„é£é™©ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ç§å®‰å…¨çš„AIç³»ç»Ÿèƒ½å¤Ÿå¸®åŠ©äººç±»ç ”ç©¶äººå‘˜åŠ é€Ÿç§‘å­¦è¿›æ­¥ï¼ŒåŒæ—¶ä½œä¸ºå¯¹æŠ—æ½œåœ¨å±é™©AIä»£ç†çš„ä¿æŠ¤æªæ–½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15681",
            "title": "One-step Diffusion Models with $f$-Divergence Distribution Matching",
            "url": "https://huggingface.co/papers/2502.15681",
            "abstract": "Sampling from diffusion models involves a slow iterative process that hinders their practical deployment, especially for interactive applications. To accelerate generation speed, recent approaches distill a multi-step diffusion model into a single-step student generator via variational score distillation, which matches the distribution of samples generated by the student to the teacher's distribution. However, these approaches use the reverse Kullback-Leibler (KL) divergence for distribution matching which is known to be mode seeking. In this paper, we generalize the distribution matching approach using a novel f-divergence minimization framework, termed f-distill, that covers different divergences with different trade-offs in terms of mode coverage and training variance. We derive the gradient of the f-divergence between the teacher and student distributions and show that it is expressed as the product of their score differences and a weighting function determined by their density ratio. This weighting function naturally emphasizes samples with higher density in the teacher distribution, when using a less mode-seeking divergence. We observe that the popular variational score distillation approach using the reverse-KL divergence is a special case within our framework. Empirically, we demonstrate that alternative f-divergences, such as forward-KL and Jensen-Shannon divergences, outperform the current best variational score distillation methods across image generation tasks. In particular, when using Jensen-Shannon divergence, f-distill achieves current state-of-the-art one-step generation performance on ImageNet64 and zero-shot text-to-image generation on MS-COCO. Project page: https://research.nvidia.com/labs/genair/f-distill",
            "score": 3,
            "issue_id": 2364,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 21",
                "zh": "2æœˆ21æ—¥"
            },
            "hash": "b1dfe60e5b59d586",
            "authors": [
                "Yilun Xu",
                "Weili Nie",
                "Arash Vahdat"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15681.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#diffusion",
                    "#inference"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ f-Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ f-distill. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ f-Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸ ĞšÑƒĞ»ÑŒĞ±Ğ°ĞºĞ°-Ğ›ĞµĞ¹Ğ±Ğ»ĞµÑ€Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ Ğ¸ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸ĞµĞ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ f-distill Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Accelerating Diffusion Models with f-Distill for Faster Image Generation",
                    "desc": "This paper addresses the slow sampling process of diffusion models, which limits their use in real-time applications. It introduces a new method called f-distill that generalizes the distribution matching process using f-divergence minimization, allowing for better trade-offs between mode coverage and training variance. The authors derive a gradient expression for f-divergence that highlights samples with higher density in the teacher distribution, improving the quality of generated samples. Empirical results show that using f-distill with alternative divergences, particularly Jensen-Shannon divergence, leads to superior performance in image generation tasks compared to existing methods."
                },
                "zh": {
                    "title": "åŠ é€Ÿæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åˆ†å¸ƒåŒ¹é…æ–¹æ³•ï¼Œç§°ä¸ºf-distillï¼Œæ—¨åœ¨åŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆé€Ÿåº¦ã€‚é€šè¿‡å¼•å…¥f-æ•£åº¦æœ€å°åŒ–æ¡†æ¶ï¼Œf-distillèƒ½å¤Ÿå¤„ç†ä¸åŒçš„æ•£åº¦ï¼Œä»è€Œåœ¨æ¨¡å¼è¦†ç›–å’Œè®­ç»ƒæ–¹å·®ä¹‹é—´å–å¾—ä¸åŒçš„æƒè¡¡ã€‚æˆ‘ä»¬æ¨å¯¼äº†æ•™å¸ˆå’Œå­¦ç”Ÿåˆ†å¸ƒä¹‹é—´f-æ•£åº¦çš„æ¢¯åº¦ï¼Œå¹¶å±•ç¤ºäº†å…¶ä¸åˆ†æ•°å·®å¼‚å’Œå¯†åº¦æ¯”çš„åŠ æƒå‡½æ•°çš„å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨Jensen-Shannonæ•£åº¦çš„f-distillåœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„å˜åˆ†åˆ†æ•°è’¸é¦æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15631",
            "title": "The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer",
            "url": "https://huggingface.co/papers/2502.15631",
            "abstract": "Large language models have demonstrated remarkable progress in mathematical reasoning, leveraging chain-of-thought and test-time compute scaling. However, many open questions remain regarding the interplay between reasoning token usage and accuracy gains. In particular, when comparing models across generations, it is unclear whether improved performance results from longer reasoning chains or more efficient reasoning. We systematically analyze chain-of-thought length across o1-mini and o3-mini variants on the Omni-MATH benchmark, finding that o3-mini (m) achieves superior accuracy without requiring longer reasoning chains than o1-mini. Moreover, we show that accuracy generally declines as reasoning chains grow across all models and compute settings, even when controlling for difficulty of the questions. This accuracy drop is significantly smaller in more proficient models, suggesting that new generations of reasoning models use test-time compute more effectively. Finally, we highlight that while o3-mini (h) achieves a marginal accuracy gain over o3-mini (m), it does so by allocating substantially more reasoning tokens across all problems, even the ones that o3-mini (m) can already solve. These findings provide new insights into the relationship between model capability and reasoning length, with implications for efficiency, scaling, and evaluation methodologies.",
            "score": 3,
            "issue_id": 2363,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 21",
                "zh": "2æœˆ21æ—¥"
            },
            "hash": "6f204197089fd2e2",
            "authors": [
                "Marthe Ballon",
                "Andres Algaba",
                "Vincent Ginis"
            ],
            "affiliations": [
                "Data Analytics Lab, Vrije Universiteit Brussel, 1050 Brussel, Belgium",
                "School of Engineering and Applied Sciences, Harvard University, Cambridge, Massachusetts 02138, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15631.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#math",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ Ğ¸Ñ… Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸ ÑƒĞ´Ğ»Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ¾ ÑÑ‚Ğ¾ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñƒ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Efficiency Over Length: Unlocking Better Reasoning in Language Models",
                    "desc": "This paper investigates how the length of reasoning chains in large language models affects their accuracy in mathematical reasoning tasks. It compares two model variants, o1-mini and o3-mini, on the Omni-MATH benchmark, revealing that o3-mini achieves better accuracy without needing longer reasoning chains. The study finds that longer reasoning chains often lead to decreased accuracy, especially across various models and question difficulties. Additionally, it suggests that newer models are more efficient in using reasoning tokens, which has important implications for model evaluation and scaling."
                },
                "zh": {
                    "title": "æ¨ç†é“¾é•¿åº¦ä¸æ¨¡å‹å‡†ç¡®æ€§çš„å…³ç³»",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯æ¨ç†é“¾çš„é•¿åº¦ä¸å‡†ç¡®æ€§ä¹‹é—´çš„å…³ç³»ã€‚ç ”ç©¶å‘ç°ï¼Œo3-miniæ¨¡å‹åœ¨ä¸éœ€è¦æ›´é•¿æ¨ç†é“¾çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿå®ç°æ›´é«˜çš„å‡†ç¡®ç‡ã€‚è®ºæ–‡è¿˜æŒ‡å‡ºï¼Œéšç€æ¨ç†é“¾çš„å¢é•¿ï¼Œæ‰€æœ‰æ¨¡å‹çš„å‡†ç¡®æ€§æ™®éä¸‹é™ï¼Œä½†åœ¨æ›´é«˜æ•ˆçš„æ¨¡å‹ä¸­ï¼Œè¿™ç§ä¸‹é™çš„å¹…åº¦è¾ƒå°ã€‚æœ€åï¼Œå°½ç®¡o3-mini(h)åœ¨å‡†ç¡®æ€§ä¸Šç•¥æœ‰æå‡ï¼Œä½†å®ƒéœ€è¦åˆ†é…æ›´å¤šçš„æ¨ç†æ ‡è®°ï¼Œè¿™è¡¨æ˜æ–°ä¸€ä»£æ¨¡å‹åœ¨æµ‹è¯•æ—¶çš„è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.14905",
            "title": "Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema Adherence",
            "url": "https://huggingface.co/papers/2502.14905",
            "abstract": "In this paper, we address the challenge of enforcing strict schema adherence in large language model (LLM) generation by leveraging LLM reasoning capabilities. Building on the DeepSeek R1 reinforcement learning framework, our approach trains structured reasoning skills of a 1.5B parameter model through a novel pipeline that combines synthetic reasoning dataset construction with custom reward functions under Group Relative Policy Optimization (GRPO). Specifically, we first perform R1 reinforcement learning on a 20K sample unstructured-to-structured dataset, mirroring the original DeepSeek R1 methods, to establish core reasoning abilities. Subsequently, we performed supervised fine-tuning on a separate 10K reasoning sample dataset, focusing on refining schema adherence for downstream tasks. Despite the relatively modest training scope, requiring approximately 20 hours on an 8xH100 GPU cluster for GRPO training and 3 hours on 1xA100 for SFT, our model demonstrates robust performance in enforcing schema consistency. We compare our ThinkJSON approach against the original DeepSeek R1 (671B), distilled versions of DeepSeek R1 (Qwen-1.5B and Qwen-7B), and Gemini 2.0 Flash (70B), showcasing its effectiveness in real-world applications. Our results underscore the practical utility of a resource-efficient framework for schema-constrained text generation.",
            "score": 2,
            "issue_id": 2363,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 18",
                "zh": "2æœˆ18æ—¥"
            },
            "hash": "e6a1c222ee43df08",
            "authors": [
                "Bhavik Agarwal",
                "Ishan Joshi",
                "Viktoria Rojkova"
            ],
            "affiliations": [
                "MasterControl AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.14905.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#synthetic",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ ÑÑ…ĞµĞ¼Ñ‹ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ° DeepSeek R1, Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 1,5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Group Relative Policy Optimization (GRPO). ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞºÑ€Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ…ĞµĞ¼Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµÑÑƒÑ€ÑĞ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾ ÑÑ…ĞµĞ¼Ğµ."
                },
                "en": {
                    "title": "ThinkJSON: Efficient Schema-Constrained Text Generation with LLMs",
                    "desc": "This paper presents a method to improve how large language models (LLMs) follow specific rules or schemas when generating text. The authors use a reinforcement learning approach called DeepSeek R1 to train a 1.5 billion parameter model, enhancing its reasoning skills through a combination of synthetic data and custom reward systems. They first establish core reasoning abilities with a large unstructured dataset and then fine-tune the model on a smaller dataset focused on schema adherence. The results show that their method, named ThinkJSON, performs well in maintaining schema consistency compared to larger models, demonstrating an efficient way to generate structured text."
                },
                "zh": {
                    "title": "åˆ©ç”¨æ¨ç†èƒ½åŠ›å¼ºåŒ–æ¨¡å¼éµå¾ª",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸­å¼ºåˆ¶éµå¾ªä¸¥æ ¼æ¨¡å¼çš„æŒ‘æˆ˜ï¼Œåˆ©ç”¨äº†è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬åŸºäºDeepSeek R1å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè®­ç»ƒäº†ä¸€ä¸ªå…·æœ‰15äº¿å‚æ•°çš„æ¨¡å‹ï¼Œé€šè¿‡åˆæˆæ¨ç†æ•°æ®é›†æ„å»ºå’Œè‡ªå®šä¹‰å¥–åŠ±å‡½æ•°ï¼Œç»“åˆç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨ä¸€ä¸ª2ä¸‡æ ·æœ¬çš„éç»“æ„åŒ–åˆ°ç»“æ„åŒ–æ•°æ®é›†ä¸Šè¿›è¡ŒR1å¼ºåŒ–å­¦ä¹ ï¼Œä»¥å»ºç«‹æ ¸å¿ƒæ¨ç†èƒ½åŠ›ã€‚éšåï¼Œæˆ‘ä»¬åœ¨ä¸€ä¸ªå•ç‹¬çš„1ä¸‡æ¨ç†æ ·æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œäº†ç›‘ç£å¾®è°ƒï¼Œä¸“æ³¨äºæé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ¨¡å¼éµå¾ªæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15011",
            "title": "CrossOver: 3D Scene Cross-Modal Alignment",
            "url": "https://huggingface.co/papers/2502.15011",
            "abstract": "Multi-modal 3D object understanding has gained significant attention, yet current approaches often assume complete data availability and rigid alignment across all modalities. We present CrossOver, a novel framework for cross-modal 3D scene understanding via flexible, scene-level modality alignment. Unlike traditional methods that require aligned modality data for every object instance, CrossOver learns a unified, modality-agnostic embedding space for scenes by aligning modalities - RGB images, point clouds, CAD models, floorplans, and text descriptions - with relaxed constraints and without explicit object semantics. Leveraging dimensionality-specific encoders, a multi-stage training pipeline, and emergent cross-modal behaviors, CrossOver supports robust scene retrieval and object localization, even with missing modalities. Evaluations on ScanNet and 3RScan datasets show its superior performance across diverse metrics, highlighting adaptability for real-world applications in 3D scene understanding.",
            "score": 1,
            "issue_id": 2366,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 20",
                "zh": "2æœˆ20æ—¥"
            },
            "hash": "97fe61eb66853ca2",
            "authors": [
                "Sayan Deb Sarkar",
                "Ondrej Miksik",
                "Marc Pollefeys",
                "Daniel Barath",
                "Iro Armeni"
            ],
            "affiliations": [
                "ETH Zurich",
                "Microsoft Spatial AI Lab, Zurich",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15011.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#3d",
                    "#training"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±ĞºĞ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½",
                    "desc": "CrossOver - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑÑ†ĞµĞ½Ñ‹. ĞĞ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ†ĞµĞ½, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· Ğ¶ĞµÑÑ‚ĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. CrossOver Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑÑ†ĞµĞ½ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "CrossOver: Flexible 3D Scene Understanding Across Modalities",
                    "desc": "This paper introduces CrossOver, a new framework designed for understanding 3D scenes using multiple types of data, such as images and point clouds. Unlike traditional methods that need all data types to be perfectly aligned, CrossOver allows for flexible alignment, making it easier to work with incomplete information. It creates a shared space where different data types can be compared and understood together, even if some data is missing. The results show that CrossOver performs better than existing methods, making it useful for real-world applications in 3D scene analysis."
                },
                "zh": {
                    "title": "è·¨æ¨¡æ€3Dåœºæ™¯ç†è§£çš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºCrossOverçš„æ–°æ¡†æ¶ï¼Œç”¨äºçµæ´»çš„è·¨æ¨¡æ€3Dåœºæ™¯ç†è§£ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒCrossOverä¸éœ€è¦æ¯ä¸ªå¯¹è±¡å®ä¾‹çš„æ¨¡æ€æ•°æ®ä¸¥æ ¼å¯¹é½ï¼Œè€Œæ˜¯é€šè¿‡æ”¾å®½çº¦æŸæ¥å­¦ä¹ ç»Ÿä¸€çš„æ¨¡æ€æ— å…³åµŒå…¥ç©ºé—´ã€‚è¯¥æ¡†æ¶æ”¯æŒRGBå›¾åƒã€ç‚¹äº‘ã€CADæ¨¡å‹ã€å¹³é¢å›¾å’Œæ–‡æœ¬æè¿°ç­‰å¤šç§æ¨¡æ€çš„å¯¹é½ï¼Œèƒ½å¤Ÿåœ¨ç¼ºå¤±æ¨¡æ€çš„æƒ…å†µä¸‹è¿›è¡Œç¨³å¥çš„åœºæ™¯æ£€ç´¢å’Œå¯¹è±¡å®šä½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCrossOveråœ¨ScanNetå’Œ3RScanæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…3Dåœºæ™¯ç†è§£ä¸­çš„é€‚åº”æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2403.12959",
            "title": "WHAC: World-grounded Humans and Cameras",
            "url": "https://huggingface.co/papers/2403.12959",
            "abstract": "Estimating human and camera trajectories with accurate scale in the world coordinate system from a monocular video is a highly desirable yet challenging and ill-posed problem. In this study, we aim to recover expressive parametric human models (i.e., SMPL-X) and corresponding camera poses jointly, by leveraging the synergy between three critical players: the world, the human, and the camera. Our approach is founded on two key observations. Firstly, camera-frame SMPL-X estimation methods readily recover absolute human depth. Secondly, human motions inherently provide absolute spatial cues. By integrating these insights, we introduce a novel framework, referred to as WHAC, to facilitate world-grounded expressive human pose and shape estimation (EHPS) alongside camera pose estimation, without relying on traditional optimization techniques. Additionally, we present a new synthetic dataset, WHAC-A-Mole, which includes accurately annotated humans and cameras, and features diverse interactive human motions as well as realistic camera trajectories. Extensive experiments on both standard and newly established benchmarks highlight the superiority and efficacy of our framework. We will make the code and dataset publicly available.",
            "score": 0,
            "issue_id": 2373,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 19",
                "zh": "3æœˆ19æ—¥"
            },
            "hash": "2fcde4d4c9ef8b28",
            "authors": [
                "Wanqi Yin",
                "Zhongang Cai",
                "Ruisi Wang",
                "Fanzhou Wang",
                "Chen Wei",
                "Haiyi Mei",
                "Weiye Xiao",
                "Zhitao Yang",
                "Qingping Sun",
                "Atsushi Yamashita",
                "Ziwei Liu",
                "Lei Yang"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "SenseTime Research",
                "Shanghai AI Laboratory",
                "The University of Tokyo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2403.12959.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#dataset",
                    "#cv",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ°, Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 3D-Ğ¿Ğ¾Ğ·",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ Ğ¿Ğ¾ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº WHAC, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° (SMPL-X) Ğ¸ Ğ¿Ğ¾Ğ·Ñ‹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¸Ñ€Ğ¾Ğ¼, Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ² ÑƒÑ‡ĞµÑ‚Ğµ Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ WHAC-A-Mole Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Jointly Estimating Human and Camera Trajectories with WHAC",
                    "desc": "This paper addresses the challenge of estimating human and camera trajectories from a single video while maintaining accurate scale in the world coordinate system. The authors propose a novel framework called WHAC, which simultaneously estimates expressive human models and camera poses by leveraging the relationship between the world, human, and camera. They utilize the depth information obtained from camera-frame SMPL-X estimation and the spatial cues provided by human motions to enhance the accuracy of their model. Additionally, they introduce a new synthetic dataset, WHAC-A-Mole, which includes detailed annotations for training and testing their approach, demonstrating its effectiveness through extensive experiments."
                },
                "zh": {
                    "title": "ä»å•ç›®è§†é¢‘ä¸­ç²¾å‡†ä¼°è®¡äººç±»ä¸ç›¸æœºè½¨è¿¹",
                    "desc": "æœ¬ç ”ç©¶æ—¨åœ¨ä»å•ç›®è§†é¢‘ä¸­å‡†ç¡®ä¼°è®¡äººç±»å’Œç›¸æœºçš„è½¨è¿¹ï¼Œè§£å†³è¿™ä¸€å¤æ‚é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶WHACï¼Œé€šè¿‡ç»“åˆä¸–ç•Œã€äººä½“å’Œç›¸æœºä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œæ¥å…±åŒæ¢å¤äººç±»æ¨¡å‹å’Œç›¸æœºå§¿æ€ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç›¸æœºå¸§SMPL-Xä¼°è®¡æŠ€æœ¯å’Œäººç±»è¿åŠ¨æä¾›çš„ç©ºé—´çº¿ç´¢ï¼Œé¿å…äº†ä¼ ç»Ÿçš„ä¼˜åŒ–æŠ€æœ¯ã€‚æˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„åˆæˆæ•°æ®é›†WHAC-A-Moleï¼ŒåŒ…å«å‡†ç¡®æ ‡æ³¨çš„äººç±»å’Œç›¸æœºï¼Œå±•ç¤ºäº†å¤šæ ·çš„äº’åŠ¨äººç±»åŠ¨ä½œå’ŒçœŸå®çš„ç›¸æœºè½¨è¿¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.14975",
            "title": "Beyond No: Quantifying AI Over-Refusal and Emotional Attachment Boundaries",
            "url": "https://huggingface.co/papers/2502.14975",
            "abstract": "We present an open-source benchmark and evaluation framework for assessing emotional boundary handling in Large Language Models (LLMs). Using a dataset of 1156 prompts across six languages, we evaluated three leading LLMs (GPT-4o, Claude-3.5 Sonnet, and Mistral-large) on their ability to maintain appropriate emotional boundaries through pattern-matched response analysis. Our framework quantifies responses across seven key patterns: direct refusal, apology, explanation, deflection, acknowledgment, boundary setting, and emotional awareness. Results demonstrate significant variation in boundary-handling approaches, with Claude-3.5 achieving the highest overall score (8.69/10) and producing longer, more nuanced responses (86.51 words on average). We identified a substantial performance gap between English (average score 25.62) and non-English interactions (< 0.22), with English responses showing markedly higher refusal rates (43.20% vs. < 1% for non-English). Pattern analysis revealed model-specific strategies, such as Mistral's preference for deflection (4.2%) and consistently low empathy scores across all models (< 0.06). Limitations include potential oversimplification through pattern matching, lack of contextual understanding in response analysis, and binary classification of complex emotional responses. Future work should explore more nuanced scoring methods, expand language coverage, and investigate cultural variations in emotional boundary expectations. Our benchmark and methodology provide a foundation for systematic evaluation of LLM emotional intelligence and boundary-setting capabilities.",
            "score": 0,
            "issue_id": 2372,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 20",
                "zh": "2æœˆ20æ—¥"
            },
            "hash": "453adb62252fb78b",
            "authors": [
                "David Noever",
                "Grant Rosario"
            ],
            "affiliations": [
                "PeopleTec, Inc., Huntsville, AL"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.14975.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#benchmark",
                    "#open_source",
                    "#multilingual",
                    "#long_context"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ¼ĞµÑ€ÑĞµĞ¼ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹. Ğ¢Ñ€Ğ¸ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (GPT-4, Claude-3.5 Ğ¸ Mistral-large) Ğ±Ñ‹Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¸Ğ· 1156 Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»ÑÑ Ğ¿Ğ¾ ÑĞµĞ¼Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¾Ñ‚ĞºĞ°Ğ·, Ğ¸Ğ·Ğ²Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºÑƒ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Claude-3.5 Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚."
                },
                "en": {
                    "title": "Evaluating Emotional Intelligence in Language Models",
                    "desc": "This paper introduces an open-source benchmark designed to evaluate how well Large Language Models (LLMs) manage emotional boundaries in their responses. The study analyzed 1156 prompts in six languages, focusing on three prominent LLMs: GPT-4o, Claude-3.5 Sonnet, and Mistral-large. The evaluation framework measures responses based on seven emotional handling patterns, revealing that Claude-3.5 performed best overall, while significant performance disparities were noted between English and non-English interactions. The findings highlight the need for improved methods to assess emotional intelligence in LLMs, suggesting future research should address cultural differences and enhance scoring techniques."
                },
                "zh": {
                    "title": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æƒ…æ„Ÿè¾¹ç•Œå¤„ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¼€æºåŸºå‡†å’Œè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æƒ…æ„Ÿè¾¹ç•Œå¤„ç†æ–¹é¢çš„è¡¨ç°ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªåŒ…å«1156ä¸ªæç¤ºçš„å¤šè¯­è¨€æ•°æ®é›†ï¼Œè¯„ä¼°äº†ä¸‰ç§é¢†å…ˆçš„LLMï¼ˆGPT-4oã€Claude-3.5 Sonnetå’ŒMistral-largeï¼‰åœ¨ä¿æŒé€‚å½“æƒ…æ„Ÿè¾¹ç•Œæ–¹é¢çš„èƒ½åŠ›ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸åŒæ¨¡å‹åœ¨è¾¹ç•Œå¤„ç†æ–¹æ³•ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼ŒClaude-3.5è·å¾—äº†æœ€é«˜åˆ†ï¼ˆ8.69/10ï¼‰ï¼Œå¹¶äº§ç”Ÿäº†æ›´é•¿ã€æ›´ç»†è‡´çš„å›åº”ã€‚æˆ‘ä»¬è¿˜å‘ç°è‹±è¯­å’Œéè‹±è¯­äº’åŠ¨ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„è¡¨ç°å·®è·ï¼Œæœªæ¥çš„ç ”ç©¶åº”æ¢ç´¢æ›´ç»†è‡´çš„è¯„åˆ†æ–¹æ³•å’Œæ–‡åŒ–å·®å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.13407",
            "title": "JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust Multi-Teacher Knowledge Distillation Framework",
            "url": "https://huggingface.co/papers/2502.13407",
            "abstract": "Deep learning has achieved significant success in the field of remote sensing image change detection (CD), yet two major challenges remain: the scarcity of sub-meter, all-inclusive open-source CD datasets, and the difficulty of achieving consistent and satisfactory detection results across images with varying change areas. To address these issues, we introduce the JL1-CD dataset, which contains 5,000 pairs of 512 x 512 pixel images with a resolution of 0.5 to 0.75 meters. Additionally, we propose a multi-teacher knowledge distillation (MTKD) framework for CD. Experimental results on the JL1-CD and SYSU-CD datasets demonstrate that the MTKD framework significantly improves the performance of CD models with various network architectures and parameter sizes, achieving new state-of-the-art results. The code is available at https://github.com/circleLZY/MTKD-CD.",
            "score": 0,
            "issue_id": 2370,
            "pub_date": "2025-02-19",
            "pub_date_card": {
                "ru": "19 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 19",
                "zh": "2æœˆ19æ—¥"
            },
            "hash": "5a5fa5fc17c92e51",
            "authors": [
                "Ziyuan Liu",
                "Ruifei Zhu",
                "Long Gao",
                "Yuanxiu Zhou",
                "Jingyu Ma",
                "Yuantao Gu"
            ],
            "affiliations": [
                "Chang Guang Satellite Technology Co., Ltd. (CGSTL) Changchun 130102, China",
                "Department of Electronic Engineering, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.13407.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#transfer_learning",
                    "#training",
                    "#dataset"
                ],
                "emoji": "ğŸ›°ï¸",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… JL1-CD Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ (MTKD) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MTKD Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. Ğ”Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… JL1-CD Ğ¸ SYSU-CD."
                },
                "en": {
                    "title": "Enhancing Change Detection with JL1-CD and MTKD Framework",
                    "desc": "This paper addresses challenges in remote sensing image change detection (CD) by introducing the JL1-CD dataset, which consists of 5,000 pairs of high-resolution images. The dataset aims to provide a comprehensive resource for training models, overcoming the issue of limited open-source datasets. Additionally, the authors propose a multi-teacher knowledge distillation (MTKD) framework that enhances the performance of CD models across different architectures. Experimental results show that this framework achieves state-of-the-art performance on both the JL1-CD and SYSU-CD datasets."
                },
                "zh": {
                    "title": "æå‡é¥æ„Ÿå˜åŒ–æ£€æµ‹çš„æ–°æ–¹æ³•",
                    "desc": "æ·±åº¦å­¦ä¹ åœ¨é¥æ„Ÿå›¾åƒå˜åŒ–æ£€æµ‹é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ä»é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šç¼ºä¹äºšç±³çº§ã€å…¨é¢çš„å¼€æºå˜åŒ–æ£€æµ‹æ•°æ®é›†ï¼Œä»¥åŠåœ¨å˜åŒ–åŒºåŸŸä¸åŒçš„å›¾åƒä¸­å®ç°ä¸€è‡´ä¸”ä»¤äººæ»¡æ„çš„æ£€æµ‹ç»“æœçš„å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†JL1-CDæ•°æ®é›†ï¼ŒåŒ…å«5000å¯¹512 x 512åƒç´ çš„å›¾åƒï¼Œåˆ†è¾¨ç‡ä¸º0.5åˆ°0.75ç±³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦ï¼ˆMTKDï¼‰æ¡†æ¶ç”¨äºå˜åŒ–æ£€æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMTKDæ¡†æ¶æ˜¾è‘—æé«˜äº†å„ç§ç½‘ç»œæ¶æ„å’Œå‚æ•°è§„æ¨¡çš„å˜åŒ–æ£€æµ‹æ¨¡å‹çš„æ€§èƒ½ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15082",
            "title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning",
            "url": "https://huggingface.co/papers/2502.15082",
            "abstract": "User specifications or legal frameworks often require information to be removed from pretrained models, including large language models (LLMs). This requires deleting or \"forgetting\" a set of data points from an already-trained model, which typically degrades its performance on other data points. Thus, a balance must be struck between removing information and keeping the model's other abilities intact, with a failure to balance this trade-off leading to poor deletion or an unusable model. To this end, we propose UPCORE (Utility-Preserving Coreset Selection), a method-agnostic data selection framework for mitigating collateral damage during unlearning. Finding that the model damage is correlated with the variance of the model's representations on the forget set, we selectively prune the forget set to remove outliers, thereby minimizing model degradation after unlearning. We evaluate UPCORE across three standard unlearning methods consistently achieving a superior balance between the competing objectives of deletion efficacy and model preservation. To better evaluate this trade-off, we introduce a new metric, measuring the area-under-the-curve (AUC) across standard metrics. We find that UPCORE improves both standard metrics and AUC, benefitting from positive transfer between the coreset and pruned points while reducing negative transfer from the forget set to points outside of it.",
            "score": 0,
            "issue_id": 2365,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 20",
                "zh": "2æœˆ20æ—¥"
            },
            "hash": "4a0bc63404dc5d71",
            "authors": [
                "Vaidehi Patil",
                "Elias Stengel-Eskin",
                "Mohit Bansal"
            ],
            "affiliations": [
                "UNC Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15082.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#leakage",
                    "#optimization",
                    "#data"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ UPCORE Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. UPCORE Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸Ğ· Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° ÑƒĞ´Ğ°Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ğ¼ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´Ğ¸ Ğ¿Ğ¾Ğ´ ĞºÑ€Ğ¸Ğ²Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ°."
                },
                "en": {
                    "title": "Smart Unlearning: Balancing Deletion and Model Integrity with UPCORE",
                    "desc": "This paper addresses the challenge of unlearning information from pretrained models, particularly large language models, without significantly harming their performance. The authors introduce UPCORE, a framework that selects a subset of data points to delete while minimizing the negative impact on the model's overall capabilities. By focusing on the variance of model representations, UPCORE effectively prunes outliers from the forget set, leading to better retention of the model's utility. The framework is evaluated against standard unlearning methods, demonstrating improved performance in both deletion effectiveness and model preservation through a new area-under-the-curve metric."
                },
                "zh": {
                    "title": "UPCOREï¼šå¹³è¡¡é—å¿˜ä¸æ¨¡å‹ä¿ç•™çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œç”¨æˆ·çš„è¦æ±‚å¸¸å¸¸éœ€è¦ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åˆ é™¤ç‰¹å®šä¿¡æ¯ï¼Œè¿™è¢«ç§°ä¸ºâ€œé—å¿˜â€ã€‚ç„¶è€Œï¼Œåˆ é™¤æ•°æ®ç‚¹å¯èƒ½ä¼šå½±å“æ¨¡å‹åœ¨å…¶ä»–æ•°æ®ä¸Šçš„è¡¨ç°ï¼Œå› æ­¤éœ€è¦åœ¨åˆ é™¤ä¿¡æ¯å’Œä¿æŒæ¨¡å‹èƒ½åŠ›ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†UPCOREï¼ˆå®ç”¨æ€§ä¿æŒæ ¸å¿ƒé›†é€‰æ‹©ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨çš„æ•°æ®é€‰æ‹©æ¡†æ¶ï¼Œæ—¨åœ¨å‡å°‘é—å¿˜è¿‡ç¨‹ä¸­çš„æ¨¡å‹æŸå®³ã€‚é€šè¿‡é€‰æ‹©æ€§åœ°ä¿®å‰ªé—å¿˜é›†ä¸­çš„å¼‚å¸¸å€¼ï¼ŒUPCOREèƒ½å¤Ÿåœ¨åˆ é™¤æœ‰æ•ˆæ€§å’Œæ¨¡å‹ä¿ç•™ä¹‹é—´å®ç°æ›´å¥½çš„å¹³è¡¡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-02-21.html",
    "link_next": "2025-02-25.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "21.02",
        "en": "02/21",
        "zh": "2æœˆ21æ—¥"
    },
    "short_date_next": {
        "ru": "25.02",
        "en": "02/25",
        "zh": "2æœˆ25æ—¥"
    },
    "categories": {
        "#dataset": 12,
        "#data": 6,
        "#benchmark": 12,
        "#agents": 1,
        "#cv": 5,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 6,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 6,
        "#math": 2,
        "#multilingual": 2,
        "#architecture": 5,
        "#healthcare": 1,
        "#training": 11,
        "#robotics": 0,
        "#agi": 4,
        "#games": 1,
        "#interpretability": 3,
        "#reasoning": 4,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 2,
        "#security": 2,
        "#optimization": 7,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 5,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 9,
        "#small_models": 0,
        "#science": 3,
        "#low_resource": 2
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸º SurveyX çš„è‡ªåŠ¨åŒ–é—®å·ç”Ÿæˆç³»ç»Ÿã€‚SurveyX é€šè¿‡å¼•å…¥åœ¨çº¿å‚è€ƒæ£€ç´¢ã€é¢„å¤„ç†æ–¹æ³• AttributeTree å’Œé‡æ–°æ¶¦è‰²è¿‡ç¨‹ï¼Œæ˜¾è‘—æé«˜äº†é—®å·ç”Ÿæˆçš„æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSurveyX åœ¨å†…å®¹è´¨é‡å’Œå¼•ç”¨è´¨é‡æ–¹é¢ä¼˜äºç°æœ‰ç³»ç»Ÿï¼Œæ¥è¿‘äººç±»ä¸“å®¶çš„è¡¨ç°ã€‚æ–‡ç« è¿˜æåˆ°äº† SurveyX ç”Ÿæˆçš„é—®å·ç¤ºä¾‹å¯ä»¥åœ¨ www.surveyx.cn ä¸Šæ‰¾åˆ°ã€‚",
        "title": "SurveyX: Academic Survey Automation via Large Language Models",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸º SurveyX çš„è‡ªåŠ¨åŒ–é—®å·ç”Ÿæˆç³»ç»Ÿã€‚SurveyX é€šè¿‡å¼•å…¥åœ¨çº¿å‚è€ƒæ£€ç´¢ã€é¢„å¤„ç†æ–¹æ³• AttributeTree å’Œé‡æ–°æ¶¦è‰²è¿‡ç¨‹ï¼Œæ˜¾è‘—æé«˜äº†é—®å·ç”Ÿæˆçš„æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSurveyX åœ¨å†…å®¹è´¨é‡å’Œå¼•ç”¨è´¨é‡æ–¹é¢ä¼˜äºç°æœ‰ç³»ç»Ÿï¼Œæ¥è¿‘äººç±»ä¸“å®¶çš„è¡¨ç°ã€‚æ–‡ç« è¿˜æåˆ°äº† SurveyX ç”Ÿæˆçš„é—®å·ç¤ºä¾‹å¯ä»¥åœ¨ www.surveyx.cn ä¸Šæ‰¾åˆ°ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ«zhÇ’ng mÃ­ngwÃ¨i SurveyX de zÃ¬dÃ²nghuÃ  wÃ¨njuÃ n shÄ“ngchÃ©ng xÃ¬tÇ’ng. SurveyX tÅngguÃ² yÇnrÃ¹ zÃ ixiÃ n cÄnkÇo jiÇnsuÇ’, yÃ¹chÇ”lÇ fÄngfÇ AttributeTree hÃ© chÃ³ngxÄ«n rÃ¹nsÃ¨ guÃ²chÃ©ng, xiÇnzhÃ¹ tÄ«gÄo le wÃ¨njuÃ n shÄ“ngchÃ©ng de xiÃ oguÇ’. ShÃ­yÃ n jiÃ©guÇ’ biÇomÃ­ng, SurveyX zÃ i nÃ¨irÃ³ng zhÃ¬liÃ ng hÃ© yÇnyÃ²ng zhÃ¬liÃ ng fÄngmiÃ n yÅuyÃº xiÃ nyÇ’u xÃ¬tÇ’ng, jiÄ“jÃ¬n rÃ©nlÃ¨i zhuÄnjiÄ de biÇoxiÃ n. WÃ©nzhÄng hÃ¡i tÃ­ dÃ o le SurveyX shÄ“ngchÃ©ng de wÃ¨njuÃ n shÃ¬lÃ¬ kÄ›yÇ zÃ i www.surveyx.cn shÃ ng zhÇo dÃ o.",
        "vocab": "[{'word': 'è‡ªåŠ¨åŒ–', 'pinyin': 'zÃ¬dÃ²nghuÃ ', 'trans': 'automated'},\n{'word': 'é—®å·', 'pinyin': 'wÃ¨njuÃ n', 'trans': 'questionnaire'},\n{'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ngchÃ©ng', 'trans': 'generate'},\n{'word': 'ç³»ç»Ÿ', 'pinyin': 'xÃ¬tÇ’ng', 'trans': 'system'},\n{'word': 'å¼•å…¥', 'pinyin': 'yÇnrÃ¹', 'trans': 'introduce'},\n{'word': 'åœ¨çº¿', 'pinyin': 'zÃ ixiÃ n', 'trans': 'online'},\n{'word': 'å‚è€ƒ', 'pinyin': 'cÄnkÇo', 'trans': 'reference'},\n{'word': 'æ£€ç´¢', 'pinyin': 'jiÇnsuÇ’', 'trans': 'retrieval'},\n{'word': 'é¢„å¤„ç†', 'pinyin': 'yÃ¹chÇ”lÇ', 'trans': 'preprocessing'},\n{'word': 'æ–¹æ³•', 'pinyin': 'fÄngfÇ', 'trans': 'method'},\n{'word': 'AttributeTree', 'pinyin': '', 'trans': 'AttributeTree'},\n{'word': 'æ¶¦è‰²', 'pinyin': 'rÃ¹nsÃ¨', 'trans': 'polish'},\n{'word': 'è¿‡ç¨‹', 'pinyin': 'guÃ²chÃ©ng', 'trans': 'process'},\n{'word': 'æ˜¾è‘—', 'pinyin': 'xiÇnzhÃ¹', 'trans': 'significant'},\n{'word': 'æé«˜', 'pinyin': 'tÃ­gÄo', 'trans': 'improve'},\n{'word': 'æ•ˆæœ', 'pinyin': 'xiÃ oguÇ’', 'trans': 'effect'},\n{'word': 'å®éªŒ', 'pinyin': 'shÃ­yÃ n', 'trans': 'experiment'},\n{'word': 'ç»“æœ', 'pinyin': 'jiÃ©guÇ’', 'trans': 'result'},\n{'word': 'è¡¨æ˜', 'pinyin': 'biÇomÃ­ng', 'trans': 'indicate'},\n{'word': 'å†…å®¹', 'pinyin': 'nÃ¨irÃ³ng', 'trans': 'content'},\n{'word': 'è´¨é‡', 'pinyin': 'zhÃ¬liÃ ng', 'trans': 'quality'},\n{'word': 'å¼•ç”¨', 'pinyin': 'yÇnyÃ²ng', 'trans': 'citation'},\n{'word': 'ç°æœ‰', 'pinyin': 'xiÃ nyÇ’u', 'trans': 'existing'},\n{'word': 'æ¥è¿‘', 'pinyin': 'jiÄ“jÃ¬n', 'trans': 'close to'},\n{'word': 'äººç±»', 'pinyin': 'rÃ©nlÃ¨i', 'trans': 'human'},\n{'word': 'ä¸“å®¶', 'pinyin': 'zhuÄnjiÄ', 'trans': 'expert'},\n{'word': 'è¡¨ç°', 'pinyin': 'biÇoxiÃ n', 'trans': 'performance'},\n{'word': 'æåˆ°', 'pinyin': 'tÃ­dÃ o', 'trans': 'mention'},\n{'word': 'ç¤ºä¾‹', 'pinyin': 'shÃ¬lÃ¬', 'trans': 'example'},\n{'word': 'æ‰¾åˆ°', 'pinyin': 'zhÇodÃ o', 'trans': 'find'}]",
        "trans": "This article introduces an automated survey generation system called SurveyX. SurveyX significantly enhances the effectiveness of survey generation by incorporating online reference retrieval, the preprocessing method AttributeTree, and a refinement process. Experimental results indicate that SurveyX outperforms existing systems in terms of content quality and citation quality, approaching the performance of human experts. The article also mentions that examples of surveys generated by SurveyX can be found at www.surveyx.cn.",
        "update_ts": "2025-02-24 09:12"
    }
}