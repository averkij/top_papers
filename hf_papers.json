{
    "date": {
        "ru": "26 июня",
        "en": "June 26",
        "zh": "6月26日"
    },
    "time_utc": "2025-06-26 05:13",
    "weekday": 3,
    "issue_id": 4496,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.18095",
            "title": "ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image\n  Generation",
            "url": "https://huggingface.co/papers/2506.18095",
            "abstract": "ShareGPT-4o-Image and Janus-4o enable open research in photorealistic, instruction-aligned image generation through a large dataset and multimodal model.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal generative models have unlocked photorealistic, instruction-aligned image generation, yet leading systems like GPT-4o-Image remain proprietary and inaccessible. To democratize these capabilities, we present ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and 46K text-and-image-to-image data, all synthesized using GPT-4o's image generation capabilities for distilling its advanced image generation abilities. Leveraging this dataset, we develop Janus-4o, a multimodal large language model capable of both text-to-image and text-and-image-to-image generation. Janus-4o not only significantly improves text-to-image generation over its predecessor, Janus-Pro, but also newly supports text-and-image-to-image generation. Notably, it achieves impressive performance in text-and-image-to-image generation from scratch, using only 91K synthetic samples and 6 hours of training on an 8 A800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will foster open research in photorealistic, instruction-aligned image generation.",
            "score": 11,
            "issue_id": 4496,
            "pub_date": "2025-06-22",
            "pub_date_card": {
                "ru": "22 июня",
                "en": "June 22",
                "zh": "6月22日"
            },
            "hash": "ea0d767800ce404b",
            "authors": [
                "Junying Chen",
                "Zhenyang Cai",
                "Pengcheng Chen",
                "Shunian Chen",
                "Ke Ji",
                "Xidong Wang",
                "Yunjin Yang",
                "Benyou Wang"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18095.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#open_source",
                    "#multimodal",
                    "#synthetic"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Демократизация фотореалистичной генерации изображений с помощью открытых данных и моделей",
                    "desc": "Статья представляет ShareGPT-4o-Image - первый набор данных, содержащий 45 тысяч примеров генерации изображений по тексту и 46 тысяч примеров генерации изображений по тексту и изображению, созданных с помощью GPT-4o. На основе этого датасета разработана мультимодальная языковая модель Janus-4o, способная генерировать изображения как по тексту, так и по тексту с изображением. Janus-4o значительно улучшает генерацию изображений по сравнению с предшественником Janus-Pro и достигает впечатляющих результатов в генерации изображений по тексту и изображению, используя всего 91 тысячу синтетических примеров. Авторы надеются, что публикация ShareGPT-4o-Image и Janus-4o будет способствовать открытым исследованиям в области фотореалистичной генерации изображений, согласованной с инструкциями."
                },
                "en": {
                    "title": "Democratizing Photorealistic Image Generation with Open Datasets and Models",
                    "desc": "This paper introduces ShareGPT-4o-Image, a comprehensive dataset designed to enhance photorealistic image generation aligned with user instructions. It includes 45,000 text-to-image and 46,000 text-and-image-to-image samples, all generated using the advanced capabilities of GPT-4o. The authors also present Janus-4o, a multimodal large language model that improves upon previous models by enabling both text-to-image and text-and-image-to-image generation. With only 91,000 synthetic samples and minimal training time, Janus-4o demonstrates significant advancements in generating high-quality images, promoting open research in this field."
                },
                "zh": {
                    "title": "开放研究，真实感图像生成的新纪元",
                    "desc": "本论文介绍了ShareGPT-4o-Image数据集和Janus-4o模型，旨在推动开放研究在真实感图像生成领域的发展。ShareGPT-4o-Image包含45K文本到图像和46K文本与图像到图像的数据，利用GPT-4o的图像生成能力进行合成。Janus-4o是一个多模态大语言模型，能够进行文本到图像和文本与图像到图像的生成，显著提升了生成效果。我们希望这些工具能够促进真实感、指令对齐的图像生成研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.19697",
            "title": "Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large\n  Language Models",
            "url": "https://huggingface.co/papers/2506.19697",
            "abstract": "Outlier-Safe Pre-Training improves large language model quantization performance by preventing extreme activation outliers through innovative training techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Extreme activation outliers in Large Language Models (LLMs) critically degrade quantization performance, hindering efficient on-device deployment. While channel-wise operations and adaptive gradient scaling are recognized causes, practical mitigation remains challenging. We introduce Outlier-Safe Pre-Training (OSP), a practical guideline that proactively prevents outlier formation rather than relying on post-hoc mitigation. OSP combines three key innovations: (1) the Muon optimizer, eliminating privileged bases while maintaining training efficiency; (2) Single-Scale RMSNorm, preventing channel-wise amplification; and (3) a learnable embedding projection, redistributing activation magnitudes originating from embedding matrices. We validate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is the first production-scale LLM trained without such outliers. Under aggressive 4-bit quantization, our OSP model achieves a 35.7 average score across 10 benchmarks (compared to 26.5 for an Adam-trained model), with only a 2% training overhead. Remarkably, OSP models exhibit near-zero excess kurtosis (0.04) compared to extreme values (1818.56) in standard models, fundamentally altering LLM quantization behavior. Our work demonstrates that outliers are not inherent to LLMs but are consequences of training strategies, paving the way for more efficient LLM deployment. The source code and pretrained checkpoints are available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.",
            "score": 10,
            "issue_id": 4495,
            "pub_date": "2025-06-24",
            "pub_date_card": {
                "ru": "24 июня",
                "en": "June 24",
                "zh": "6月24日"
            },
            "hash": "3a6d3af4578d26f6",
            "authors": [
                "Jungwoo Park",
                "Taewhoo Lee",
                "Chanwoong Yoon",
                "Hyeon Hwang",
                "Jaewoo Kang"
            ],
            "affiliations": [
                "AIGEN Sciences",
                "Korea University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.19697.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Безопасное предобучение LLM для эффективного квантования",
                    "desc": "Статья представляет новый метод предварительного обучения больших языковых моделей (LLM), называемый Outlier-Safe Pre-Training (OSP). OSP предотвращает образование экстремальных выбросов активации, которые ухудшают производительность квантования моделей. Метод включает в себя три ключевые инновации: оптимизатор Muon, однопараметрическую нормализацию RMSNorm и обучаемую проекцию эмбеддингов. Эксперименты показывают, что OSP значительно улучшает производительность 4-битного квантования LLM, делая их более эффективными для развертывания на устройствах."
                },
                "en": {
                    "title": "Preventing Outliers for Better LLM Performance",
                    "desc": "This paper presents Outlier-Safe Pre-Training (OSP), a novel approach to enhance the quantization performance of large language models (LLMs) by preventing extreme activation outliers during training. The authors identify that these outliers significantly impair the efficiency of LLMs when deployed on devices, and propose a proactive strategy rather than relying on post-training fixes. OSP incorporates three innovations: the Muon optimizer for efficient training, Single-Scale RMSNorm to control channel-wise amplification, and a learnable embedding projection to manage activation magnitudes. The results show that OSP-trained models achieve superior performance in quantization benchmarks while maintaining low training overhead, indicating that outliers can be effectively managed through improved training techniques."
                },
                "zh": {
                    "title": "创新训练技术，提升模型量化性能",
                    "desc": "本论文提出了一种名为Outlier-Safe Pre-Training（OSP）的新方法，旨在改善大型语言模型（LLM）的量化性能。OSP通过创新的训练技术，主动防止极端激活异常值的形成，从而提高模型在设备上的高效部署。该方法结合了三项关键创新：Muon优化器、单尺度RMSNorm和可学习的嵌入投影，显著降低了训练过程中的异常值。实验结果表明，OSP模型在4位量化下的表现优于传统模型，展示了训练策略对异常值的影响。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18403",
            "title": "The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs",
            "url": "https://huggingface.co/papers/2506.18403",
            "abstract": "The Debugging Decay Index (DDI) quantifies and optimizes the effectiveness of iterative AI debugging by predicting intervention points to revive and enhance debugging capability.  \t\t\t\t\tAI-generated summary \t\t\t\t The effectiveness of AI debugging follows a predictable exponential decay pattern; most models lose 60-80% of their debugging capability within just 2-3 attempts, despite iterative debugging being a critical capability for practical code generation systems. We introduce the Debugging Decay Index (DDI), a mathematical framework that quantifies when debugging becomes ineffective and predicts intervention points. Our strategic fresh start approach shifts from exploitation to exploration at strategic points in the debugging process, demonstrating that well-timed interventions can rescue the effectiveness of debugging. DDI reveals a fundamental limitation in current AI debugging and provides the first quantitative framework for optimising iterative code generation strategies.",
            "score": 2,
            "issue_id": 4494,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 июня",
                "en": "June 23",
                "zh": "6月23日"
            },
            "hash": "30aa788d94afe45d",
            "authors": [
                "Muntasir Adnan",
                "Carlos C. N. Kuhn"
            ],
            "affiliations": [
                "Open Source Institute, University of Canberra, Bruce, Canberra, Australia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18403.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#math"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Оптимизация отладки ИИ: измерение и преодоление затухания эффективности",
                    "desc": "Статья представляет Индекс Затухания Отладки (DDI), который количественно оценивает эффективность итеративной отладки ИИ. Авторы обнаружили, что способность моделей к отладке быстро снижается, следуя экспоненциальному паттерну затухания. DDI позволяет предсказать оптимальные точки вмешательства для восстановления эффективности отладки. Предложенный подход стратегического свежего старта демонстрирует, что своевременные вмешательства могут значительно улучшить процесс отладки кода."
                },
                "en": {
                    "title": "Reviving AI Debugging with Strategic Interventions",
                    "desc": "The Debugging Decay Index (DDI) is a new method that measures how quickly AI debugging abilities decline over time. It shows that most AI models lose a significant portion of their debugging skills after just a few attempts, which is a major issue for systems that generate code. By using DDI, developers can identify the best moments to intervene and improve the debugging process, shifting from refining existing solutions to exploring new ones. This approach not only highlights a key limitation in current AI debugging methods but also offers a way to enhance the effectiveness of iterative code generation."
                },
                "zh": {
                    "title": "优化AI调试的关键：调试衰减指数",
                    "desc": "调试衰减指数（DDI）量化并优化了迭代AI调试的有效性，通过预测干预点来恢复和增强调试能力。研究表明，AI调试的有效性遵循可预测的指数衰减模式，大多数模型在仅仅2-3次尝试后就会失去60-80%的调试能力。我们提出的DDI框架可以量化调试失效的时机，并预测干预点，从而在调试过程中实现从利用到探索的战略转变。DDI揭示了当前AI调试的基本局限性，并提供了优化迭代代码生成策略的首个定量框架。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.19502",
            "title": "MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility\n  Applications",
            "url": "https://huggingface.co/papers/2506.19502",
            "abstract": "MATE, a multimodal accessibility multi-agent system, converts data into understandable formats based on user needs, supporting various disabilities and integrating with institutional technologies.  \t\t\t\t\tAI-generated summary \t\t\t\t Accessibility remains a critical concern in today's society, as many technologies are not developed to support the full range of user needs. Existing multi-agent systems (MAS) often cannot provide comprehensive assistance for users in need due to the lack of customization stemming from closed-source designs. Consequently, individuals with disabilities frequently encounter significant barriers when attempting to interact with digital environments. We introduce MATE, a multimodal accessibility MAS, which performs the modality conversions based on the user's needs. The system is useful for assisting people with disabilities by ensuring that data will be converted to an understandable format. For instance, if the user cannot see well and receives an image, the system converts this image to its audio description. MATE can be applied to a wide range of domains, industries, and areas, such as healthcare, and can become a useful assistant for various groups of users. The system supports multiple types of models, ranging from LLM API calling to using custom machine learning (ML) classifiers. This flexibility ensures that the system can be adapted to various needs and is compatible with a wide variety of hardware. Since the system is expected to run locally, it ensures the privacy and security of sensitive information. In addition, the framework can be effectively integrated with institutional technologies (e.g., digital healthcare service) for real-time user assistance. Furthermore, we introduce ModCon-Task-Identifier, a model that is capable of extracting the precise modality conversion task from the user input. Numerous experiments show that ModCon-Task-Identifier consistently outperforms other LLMs and statistical models on our custom data. Our code and data are publicly available at https://github.com/AlgazinovAleksandr/Multi-Agent-MATE.",
            "score": 1,
            "issue_id": 4494,
            "pub_date": "2025-06-24",
            "pub_date_card": {
                "ru": "24 июня",
                "en": "June 24",
                "zh": "6月24日"
            },
            "hash": "85d844ff6061cc93",
            "authors": [
                "Aleksandr Algazinov",
                "Matt Laing",
                "Paul Laban"
            ],
            "affiliations": [
                "Dept. of Comp. Sci. & Tech. Tsinghua University Beijing, China",
                "Dept. of Psych. & Cog. Sci. Tsinghua University Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.19502.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#agents",
                    "#multimodal",
                    "#ethics",
                    "#open_source"
                ],
                "emoji": "♿",
                "ru": {
                    "title": "MATE: Интеллектуальная система для преодоления барьеров доступности",
                    "desc": "MATE - это мультимодальная система мультиагентов для обеспечения доступности, которая преобразует данные в понятные форматы в зависимости от потребностей пользователя. Система поддерживает различные виды инвалидности и может интегрироваться с институциональными технологиями. MATE использует широкий спектр моделей, от вызовов API больших языковых моделей до пользовательских классификаторов машинного обучения. Система включает в себя ModCon-Task-Identifier - модель, способную точно определять задачу преобразования модальности из пользовательского ввода."
                },
                "en": {
                    "title": "Empowering Accessibility Through Intelligent Data Conversion",
                    "desc": "MATE is a multimodal accessibility multi-agent system designed to convert data into formats that are understandable for users with disabilities. It addresses the limitations of existing multi-agent systems by providing customizable solutions that adapt to individual user needs. The system can perform tasks like converting images to audio descriptions, making digital content more accessible. Additionally, MATE integrates with institutional technologies and ensures user privacy by running locally, while its ModCon-Task-Identifier model excels in identifying specific modality conversion tasks."
                },
                "zh": {
                    "title": "MATE：为每个人提供无障碍的智能助手",
                    "desc": "MATE是一个多模态无障碍多代理系统，旨在根据用户需求将数据转换为可理解的格式，以支持各种残疾人士。该系统通过执行模态转换，帮助用户更好地与数字环境互动，例如将图像转换为音频描述，以满足视觉障碍者的需求。MATE具有灵活性，支持多种模型和硬件，确保能够适应不同的用户需求，并保护敏感信息的隐私和安全。通过与机构技术的有效集成，MATE能够提供实时的用户支持，提升无障碍服务的质量。"
                }
            }
        }
    ],
    "link_prev": "2025-06-25.html",
    "link_next": "2025-06-27.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "25.06",
        "en": "06/25",
        "zh": "6月25日"
    },
    "short_date_next": {
        "ru": "27.06",
        "en": "06/27",
        "zh": "6月27日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 1,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}