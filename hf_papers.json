{
    "date": {
        "ru": "19 мая",
        "en": "May 19",
        "zh": "5月19日"
    },
    "time_utc": "2025-05-19 09:13",
    "weekday": 0,
    "issue_id": 3829,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.09388",
            "title": "Qwen3 Technical Report",
            "url": "https://huggingface.co/papers/2505.09388",
            "abstract": "In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.",
            "score": 68,
            "issue_id": 3823,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 мая",
                "en": "May 14",
                "zh": "5月14日"
            },
            "hash": "69a0f87bb5460e8d",
            "authors": [
                "An Yang",
                "Anfeng Li",
                "Baosong Yang",
                "Beichen Zhang",
                "Binyuan Hui",
                "Bo Zheng",
                "Bowen Yu",
                "Chang Gao",
                "Chengen Huang",
                "Chenxu Lv",
                "Chujie Zheng",
                "Dayiheng Liu",
                "Fan Zhou",
                "Fei Huang",
                "Feng Hu",
                "Hao Ge",
                "Haoran Wei",
                "Huan Lin",
                "Jialong Tang",
                "Jian Yang",
                "Jianhong Tu",
                "Jianwei Zhang",
                "Jianxin Yang",
                "Jiaxi Yang",
                "Jing Zhou",
                "Jingren Zhou",
                "Junyang Lin",
                "Kai Dang",
                "Keqin Bao",
                "Kexin Yang",
                "Le Yu",
                "Lianghao Deng",
                "Mei Li",
                "Mingfeng Xue",
                "Mingze Li",
                "Pei Zhang",
                "Peng Wang",
                "Qin Zhu",
                "Rui Men",
                "Ruize Gao",
                "Shixuan Liu",
                "Shuang Luo",
                "Tianhao Li",
                "Tianyi Tang",
                "Wenbiao Yin",
                "Xingzhang Ren",
                "Xinyu Wang",
                "Xinyu Zhang",
                "Xuancheng Ren",
                "Yang Fan",
                "Yang Su",
                "Yichang Zhang",
                "Yinger Zhang",
                "Yu Wan",
                "Yuqiong Liu",
                "Zekun Wang",
                "Zeyu Cui",
                "Zhenru Zhang",
                "Zhipeng Zhou",
                "Zihan Qiu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.09388.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#agi",
                    "#reasoning",
                    "#multilingual",
                    "#benchmark",
                    "#architecture",
                    "#training",
                    "#open_source"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Qwen3: Единая модель для мышления и быстрых ответов",
                    "desc": "Qwen3 - это новое семейство больших языковых моделей (LLM), разработанное для улучшения производительности, эффективности и многоязычных возможностей. Ключевой инновацией Qwen3 является интеграция режима мышления и режима без мышления в единую структуру, что позволяет динамически переключаться между ними. Модель вводит механизм бюджета мышления, позволяющий адаптивно распределять вычислительные ресурсы во время вывода. Qwen3 достигает передовых результатов в различных бенчмарках и поддерживает 119 языков и диалектов."
                },
                "en": {
                    "title": "Qwen3: Unifying Thinking and Efficiency in Language Models",
                    "desc": "Qwen3 is the latest version of the Qwen model family, featuring large language models that enhance performance, efficiency, and multilingual capabilities. It includes both dense and Mixture-of-Expert (MoE) architectures with a wide range of parameters, from 0.6 to 235 billion. A notable innovation is the integration of thinking and non-thinking modes, allowing for seamless dynamic switching based on user needs, which improves response times and reasoning capabilities. Additionally, Qwen3 supports 119 languages, significantly increasing its accessibility and effectiveness in diverse applications, while also providing a thinking budget mechanism for optimized resource allocation during inference."
                },
                "zh": {
                    "title": "Qwen3：统一思维与响应的智能语言模型",
                    "desc": "本文介绍了Qwen3，这是Qwen模型系列的最新版本。Qwen3包含一系列大型语言模型，旨在提高性能、效率和多语言能力。其创新之处在于将思维模式和非思维模式整合到一个统一框架中，实现动态模式切换，适应用户查询。Qwen3还引入了思维预算机制，允许用户在推理过程中自适应分配计算资源，从而在任务复杂性基础上平衡延迟和性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11409",
            "title": "Visual Planning: Let's Think Only with Images",
            "url": "https://huggingface.co/papers/2505.11409",
            "abstract": "Recent advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have substantially enhanced machine reasoning across diverse tasks. However, these models predominantly rely on pure text as the medium for both expressing and structuring reasoning, even when visual information is present. In this work, we argue that language may not always be the most natural or effective modality for reasoning, particularly in tasks involving spatial and geometrical information. Motivated by this, we propose a new paradigm, Visual Planning, which enables planning through purely visual representations, independent of text. In this paradigm, planning is executed via sequences of images that encode step-by-step inference in the visual domain, akin to how humans sketch or visualize future actions. We introduce a novel reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models, leading to substantial improvements in planning in a selection of representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our visual planning paradigm outperforms all other planning variants that conduct reasoning in the text-only space. Our results establish Visual Planning as a viable and promising alternative to language-based reasoning, opening new avenues for tasks that benefit from intuitive, image-based inference.",
            "score": 10,
            "issue_id": 3825,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 мая",
                "en": "May 16",
                "zh": "5月16日"
            },
            "hash": "67875b7838a7b7ea",
            "authors": [
                "Yi Xu",
                "Chengzu Li",
                "Han Zhou",
                "Xingchen Wan",
                "Caiqi Zhang",
                "Anna Korhonen",
                "Ivan Vulić"
            ],
            "affiliations": [
                "Google",
                "Language Technology Lab, University of Cambridge",
                "University College London"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11409.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#reasoning",
                    "#multimodal",
                    "#training",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Визуальное планирование: новый подход к машинному рассуждению без слов",
                    "desc": "Статья представляет новую парадигму под названием 'Визуальное планирование', которая позволяет осуществлять планирование с помощью чисто визуальных представлений, без использования текста. Авторы предлагают фреймворк обучения с подкреплением VPRL, усиленный методом GRPO для дообучения больших моделей компьютерного зрения. Эксперименты показывают, что визуальное планирование превосходит текстовые методы рассуждений в задачах визуальной навигации. Результаты открывают новые возможности для задач, требующих интуитивного, основанного на изображениях вывода."
                },
                "en": {
                    "title": "Visual Planning: Reasoning Beyond Text",
                    "desc": "This paper introduces a new approach called Visual Planning, which focuses on using visual representations for reasoning instead of relying solely on text. The authors argue that for tasks involving spatial and geometrical information, visual reasoning can be more effective. They present a reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), which enhances planning capabilities in visual navigation tasks. The results show that this visual approach outperforms traditional text-based reasoning methods, suggesting a shift towards image-based inference in machine learning applications."
                },
                "zh": {
                    "title": "视觉规划：超越文本的推理新范式",
                    "desc": "最近，大型语言模型（LLMs）和多模态扩展（MLLMs）的进展显著提升了机器推理能力。然而，这些模型主要依赖纯文本来表达和构建推理，即使在存在视觉信息的情况下。我们提出了一种新的范式，称为视觉规划，允许通过纯视觉表示进行规划，而不依赖文本。我们的研究表明，视觉规划在处理空间和几何信息的任务中，比基于语言的推理更有效。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.07675",
            "title": "Simple Semi-supervised Knowledge Distillation from Vision-Language\n  Models via texttt{D}ual-texttt{H}ead\n  texttt{O}ptimization",
            "url": "https://huggingface.co/papers/2505.07675",
            "abstract": "Vision-language models (VLMs) have achieved remarkable success across diverse tasks by leveraging rich textual information with minimal labeled data. However, deploying such large models remains challenging, particularly in resource-constrained environments. Knowledge distillation (KD) offers a well-established solution to this problem; however, recent KD approaches from VLMs often involve multi-stage training or additional tuning, increasing computational overhead and optimization complexity. In this paper, we propose texttt{D}ual-texttt{H}ead texttt{O}ptimization (texttt{DHO}) -- a simple yet effective KD framework that transfers knowledge from VLMs to compact, task-specific models in semi-supervised settings. Specifically, we introduce dual prediction heads that independently learn from labeled data and teacher predictions, and propose to linearly combine their outputs during inference. We observe that DHO mitigates gradient conflicts between supervised and distillation signals, enabling more effective feature learning than single-head KD baselines. As a result, extensive experiments show that DHO consistently outperforms baselines across multiple domains and fine-grained datasets. Notably, on ImageNet, it achieves state-of-the-art performance, improving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively, while using fewer parameters.",
            "score": 8,
            "issue_id": 3828,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 мая",
                "en": "May 12",
                "zh": "5月12日"
            },
            "hash": "73f4f4dd13e67a25",
            "authors": [
                "Seongjae Kang",
                "Dong Bok Lee",
                "Hyungjoon Jang",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "KAIST",
                "VUNO Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.07675.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#optimization",
                    "#transfer_learning",
                    "#training",
                    "#cv",
                    "#data"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "DHO: Эффективная дистилляция знаний для компактных моделей компьютерного зрения",
                    "desc": "Статья представляет новый метод дистилляции знаний под названием Dual-Head Optimization (DHO) для эффективной передачи знаний от крупных визуально-языковых моделей (VLM) к компактным моделям для конкретных задач. DHO использует две независимые головы предсказания, которые обучаются на размеченных данных и предсказаниях учителя соответственно. Этот подход позволяет избежать конфликтов градиентов между сигналами обучения с учителем и без него, что приводит к более эффективному обучению признаков. Эксперименты показывают, что DHO превосходит базовые методы на различных наборах данных, достигая лучших результатов на ImageNet при использовании меньшего количества параметров."
                },
                "en": {
                    "title": "Streamlining Knowledge Distillation with Dual-Head Optimization",
                    "desc": "This paper introduces a new framework called Dual-Head Optimization (DHO) for knowledge distillation from vision-language models (VLMs) to smaller, task-specific models. DHO uses two prediction heads that learn from both labeled data and the predictions of a larger teacher model, which helps to reduce conflicts in learning signals. The method simplifies the distillation process, avoiding the complexity of multi-stage training while still improving feature learning. Experiments show that DHO outperforms existing methods, achieving better accuracy on datasets like ImageNet with fewer parameters."
                },
                "zh": {
                    "title": "DHO：高效的知识蒸馏框架",
                    "desc": "本文提出了一种名为DHO的知识蒸馏框架，旨在将视觉语言模型（VLMs）的知识转移到紧凑的任务特定模型中。DHO通过引入双预测头，分别从标记数据和教师预测中独立学习，并在推理时线性组合它们的输出。实验结果表明，DHO有效缓解了监督信号和蒸馏信号之间的梯度冲突，从而实现了比单头知识蒸馏基线更有效的特征学习。最终，DHO在多个领域和细粒度数据集上均表现优异，尤其在ImageNet上，使用更少的参数实现了3%的准确率提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11107",
            "title": "Group Think: Multiple Concurrent Reasoning Agents Collaborating at Token\n  Level Granularity",
            "url": "https://huggingface.co/papers/2505.11107",
            "abstract": "Recent advances in large language models (LLMs) have demonstrated the power of reasoning through self-generated chains of thought. Multiple reasoning agents can collaborate to raise joint reasoning quality above individual outcomes. However, such agents typically interact in a turn-based manner, trading increased latency for improved quality. In this paper, we propose Group Think--a single LLM that acts as multiple concurrent reasoning agents, or thinkers. With shared visibility into each other's partial generation progress, Group Think introduces a new concurrent-reasoning paradigm in which multiple reasoning trajectories adapt dynamically to one another at the token level. For example, a reasoning thread may shift its generation mid-sentence upon detecting that another thread is better positioned to continue. This fine-grained, token-level collaboration enables Group Think to reduce redundant reasoning and improve quality while achieving significantly lower latency. Moreover, its concurrent nature allows for efficient utilization of idle computational resources, making it especially suitable for edge inference, where very small batch size often underutilizes local~GPUs. We give a simple and generalizable modification that enables any existing LLM to perform Group Think on a local GPU. We also present an evaluation strategy to benchmark reasoning latency and empirically demonstrate latency improvements using open-source LLMs that were not explicitly trained for Group Think. We hope this work paves the way for future LLMs to exhibit more sophisticated and more efficient collaborative behavior for higher quality generation.",
            "score": 6,
            "issue_id": 3828,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 мая",
                "en": "May 16",
                "zh": "5月16日"
            },
            "hash": "b2ea7382367e75ba",
            "authors": [
                "Chan-Jan Hsu",
                "Davide Buffelli",
                "Jamie McGowan",
                "Feng-Ting Liao",
                "Yi-Chang Chen",
                "Sattar Vakili",
                "Da-shan Shiu"
            ],
            "affiliations": [
                "MediaTek Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11107.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#reasoning",
                    "#optimization",
                    "#agents",
                    "#training",
                    "#open_source",
                    "#inference"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Коллективное мышление: параллельные рассуждения в одной языковой модели",
                    "desc": "В статье представлен метод Group Think, позволяющий большой языковой модели (LLM) действовать как несколько параллельных рассуждающих агентов. Эти агенты могут динамически адаптировать свои рассуждения на уровне токенов, что позволяет снизить избыточность и повысить качество при значительно меньшей задержке. Метод особенно подходит для периферийных вычислений, где часто недоиспользуются локальные GPU. Авторы предлагают простую модификацию, позволяющую любой существующей LLM выполнять Group Think на локальном GPU."
                },
                "en": {
                    "title": "Group Think: Collaborative Reasoning for Faster, Smarter LLMs",
                    "desc": "This paper introduces Group Think, a novel approach that allows a single large language model (LLM) to function as multiple reasoning agents working together simultaneously. By enabling these agents to share visibility into each other's progress, they can dynamically adjust their reasoning paths at the token level, enhancing the overall quality of the output. This concurrent reasoning reduces redundancy and improves efficiency, leading to lower latency compared to traditional turn-based interactions. The authors also provide a method to adapt existing LLMs for Group Think, demonstrating its effectiveness through empirical evaluations."
                },
                "zh": {
                    "title": "Group Think：提升推理质量的新方法",
                    "desc": "本文提出了一种名为Group Think的新方法，它利用单个大型语言模型（LLM）作为多个并发推理代理。通过共享彼此的生成进度，这种方法允许推理线程在生成过程中动态适应，从而减少冗余推理并提高生成质量。Group Think的并发特性使得计算资源得到更有效的利用，特别适合边缘推理场景。我们还提供了一种简单的修改方法，使现有的LLM能够在本地GPU上实现Group Think。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11427",
            "title": "Mergenetic: a Simple Evolutionary Model Merging Library",
            "url": "https://huggingface.co/papers/2505.11427",
            "abstract": "Model merging allows combining the capabilities of existing models into a new one - post hoc, without additional training. This has made it increasingly popular thanks to its low cost and the availability of libraries that support merging on consumer GPUs. Recent work shows that pairing merging with evolutionary algorithms can boost performance, but no framework currently supports flexible experimentation with such strategies in language models. We introduce Mergenetic, an open-source library for evolutionary model merging. Mergenetic enables easy composition of merging methods and evolutionary algorithms while incorporating lightweight fitness estimators to reduce evaluation costs. We describe its design and demonstrate that Mergenetic produces competitive results across tasks and languages using modest hardware.",
            "score": 5,
            "issue_id": 3828,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 мая",
                "en": "May 16",
                "zh": "5月16日"
            },
            "hash": "759eb3fbdec85844",
            "authors": [
                "Adrian Robert Minut",
                "Tommaso Mencattini",
                "Andrea Santilli",
                "Donato Crisostomi",
                "Emanuele Rodolà"
            ],
            "affiliations": [
                "Ecole Polytechnique Fédérale de Lausanne",
                "Sapienza University of Rome"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11427.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#open_source",
                    "#training"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "Эволюционное слияние языковых моделей без переобучения",
                    "desc": "Статья представляет библиотеку Mergenetic для эволюционного объединения языковых моделей. Она позволяет комбинировать существующие модели без дополнительного обучения, используя эволюционные алгоритмы. Mergenetic поддерживает гибкое экспериментирование с различными стратегиями объединения и включает легковесные оценщики качества для снижения вычислительных затрат. Результаты показывают конкурентоспособность подхода на различных задачах и языках при использовании скромных вычислительных ресурсов."
                },
                "en": {
                    "title": "Mergenetic: Evolving Better Models Through Merging",
                    "desc": "This paper presents Mergenetic, an open-source library designed for evolutionary model merging in machine learning. Model merging allows the combination of existing models into a new one without the need for additional training, making it cost-effective and efficient. Mergenetic enhances this process by integrating evolutionary algorithms, which can improve model performance. The library also includes lightweight fitness estimators to minimize evaluation costs, demonstrating competitive results across various tasks and languages using standard hardware."
                },
                "zh": {
                    "title": "Mergenetic：进化模型合并的新选择",
                    "desc": "模型合并是一种将现有模型的能力结合成新模型的方法，无需额外训练。这种方法因其低成本和支持消费者GPU的库而越来越受欢迎。最近的研究表明，将合并与进化算法结合可以提高性能，但目前没有框架支持在语言模型中灵活实验这些策略。我们介绍了Mergenetic，这是一个开源库，用于进化模型合并，能够轻松组合合并方法和进化算法，同时引入轻量级的适应度评估器以降低评估成本。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.10962",
            "title": "MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective\n  Search and Data Curation",
            "url": "https://huggingface.co/papers/2505.10962",
            "abstract": "Automated Theorem Proving (ATP) in formal languages remains a formidable challenge in AI, demanding rigorous logical deduction and navigating vast search spaces. While large language models (LLMs) have shown promising performance, existing stepwise provers often suffer from biased search guidance, leading to inefficiencies and suboptimal proof strategies. This paper introduces the Multi-Perspective Search Prover (MPS-Prover), a novel stepwise ATP system designed to overcome these limitations. MPS-Prover incorporates two key innovations: a highly effective post-training data curation strategy that prunes approximately 40% of redundant training data without sacrificing performance, and a multi-perspective tree search mechanism. This search integrates a learned critic model with strategically designed heuristic rules to diversify tactic selection, prevent getting trapped in unproductive states, and enhance search robustness. Extensive evaluations demonstrate that MPS-Prover achieves state-of-the-art performance on multiple challenging benchmarks, including miniF2F and ProofNet, outperforming prior 7B parameter models. Furthermore, our analyses reveal that MPS-Prover generates significantly shorter and more diverse proofs compared to existing stepwise and whole-proof methods, highlighting its efficiency and efficacy. Our work advances the capabilities of LLM-based formal reasoning and offers a robust framework and a comprehensive analysis for developing more powerful theorem provers.",
            "score": 5,
            "issue_id": 3825,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 мая",
                "en": "May 16",
                "zh": "5月16日"
            },
            "hash": "07990204af30ff71",
            "authors": [
                "Zhenwen Liang",
                "Linfeng Song",
                "Yang Li",
                "Tao Yang",
                "Feng Zhang",
                "Haitao Mi",
                "Dong Yu"
            ],
            "affiliations": [
                "Tencent AI Lab",
                "Tencent LLM Department"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.10962.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#reasoning",
                    "#data",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Многоперспективный поиск для прорыва в автоматическом доказательстве теорем",
                    "desc": "Эта статья представляет инновационную систему автоматического доказательства теорем под названием MPS-Prover. Система использует две ключевые инновации: эффективную стратегию отбора данных после обучения и механизм поиска с множественными перспективами. MPS-Prover достигает наилучших результатов на нескольких сложных эталонных тестах, превосходя предыдущие модели с 7 миллиардами параметров. Анализ показывает, что MPS-Prover генерирует значительно более короткие и разнообразные доказательства по сравнению с существующими методами."
                },
                "en": {
                    "title": "Revolutionizing Theorem Proving with Multi-Perspective Search",
                    "desc": "This paper presents the Multi-Perspective Search Prover (MPS-Prover), a new system for Automated Theorem Proving (ATP) that addresses inefficiencies in existing stepwise provers. MPS-Prover utilizes a post-training data curation strategy to eliminate redundant training data, improving performance without loss of quality. It also features a multi-perspective tree search that combines a learned critic model with heuristic rules to enhance search diversity and prevent unproductive paths. The results show that MPS-Prover not only achieves state-of-the-art performance on various benchmarks but also produces shorter and more diverse proofs than previous models."
                },
                "zh": {
                    "title": "多视角搜索，提升定理证明效率",
                    "desc": "自动定理证明（ATP）在形式语言中仍然是人工智能中的一大挑战，需要严格的逻辑推理和广泛的搜索空间。虽然大型语言模型（LLMs）表现出良好的性能，但现有的逐步证明器常常受到偏见搜索指导的影响，导致效率低下和次优的证明策略。本文介绍了一种新颖的逐步ATP系统——多视角搜索证明器（MPS-Prover），旨在克服这些局限性。MPS-Prover结合了高效的后训练数据整理策略和多视角树搜索机制，显著提高了证明的效率和多样性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11152",
            "title": "Learning Dense Hand Contact Estimation from Imbalanced Data",
            "url": "https://huggingface.co/papers/2505.11152",
            "abstract": "Hands are essential to human interaction, and understanding contact between hands and the world can promote comprehensive understanding of their function. Recently, there have been growing number of hand interaction datasets that cover interaction with object, other hand, scene, and body. Despite the significance of the task and increasing high-quality data, how to effectively learn dense hand contact estimation remains largely underexplored. There are two major challenges for learning dense hand contact estimation. First, there exists class imbalance issue from hand contact datasets where majority of samples are not in contact. Second, hand contact datasets contain spatial imbalance issue with most of hand contact exhibited in finger tips, resulting in challenges for generalization towards contacts in other hand regions. To tackle these issues, we present a framework that learns dense HAnd COntact estimation (HACO) from imbalanced data. To resolve the class imbalance issue, we introduce balanced contact sampling, which builds and samples from multiple sampling groups that fairly represent diverse contact statistics for both contact and non-contact samples. Moreover, to address the spatial imbalance issue, we propose vertex-level class-balanced (VCB) loss, which incorporates spatially varying contact distribution by separately reweighting loss contribution of each vertex based on its contact frequency across dataset. As a result, we effectively learn to predict dense hand contact estimation with large-scale hand contact data without suffering from class and spatial imbalance issue. The codes will be released.",
            "score": 2,
            "issue_id": 3822,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 мая",
                "en": "May 16",
                "zh": "5月16日"
            },
            "hash": "caa702fa71c24606",
            "authors": [
                "Daniel Sungho Jung",
                "Kyoung Mu Lee"
            ],
            "affiliations": [
                "IPAI, Dept. of ECE & ASRI, Seoul National University, Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11152.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#data"
                ],
                "emoji": "🖐️",
                "ru": {
                    "title": "Точная оценка контактов рук: преодоление дисбаланса данных",
                    "desc": "Эта статья представляет новый подход к оценке плотного контакта рук с окружающей средой, что важно для понимания взаимодействия человека с миром. Авторы разработали фреймворк HACO для обучения на несбалансированных данных, решая проблемы классового и пространственного дисбаланса в наборах данных о контактах рук. Они предложили метод сбалансированной выборки контактов и функцию потерь VCB, учитывающую пространственное распределение контактов на поверхности руки. Результаты показывают эффективность предложенного подхода для точного предсказания плотных контактов рук на основе крупномасштабных данных."
                },
                "en": {
                    "title": "Enhancing Hand Contact Estimation with Balanced Learning Techniques",
                    "desc": "This paper addresses the challenge of estimating dense hand contact in various interactions, which is crucial for understanding hand functionality. It identifies two main issues: class imbalance, where most samples do not involve contact, and spatial imbalance, where contact is primarily at the fingertips. To overcome these challenges, the authors propose a framework called HACO that utilizes balanced contact sampling to ensure diverse representation of contact data. Additionally, they introduce a vertex-level class-balanced loss to adjust the learning process based on the frequency of contact across different hand regions, leading to improved predictions in dense hand contact estimation."
                },
                "zh": {
                    "title": "提升手部接触估计的准确性",
                    "desc": "这篇论文探讨了手部接触估计的重要性，尤其是在与物体、其他手、场景和身体的互动中。尽管已有大量高质量的数据集，但如何有效学习密集的手部接触估计仍然是一个未被充分研究的问题。论文提出了一种新的框架，称为HACO，旨在解决类不平衡和空间不平衡的问题。通过引入平衡接触采样和顶点级类平衡损失，研究者们成功地提高了手部接触估计的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11140",
            "title": "Scaling Reasoning can Improve Factuality in Large Language Models",
            "url": "https://huggingface.co/papers/2505.11140",
            "abstract": "Recent studies on large language model (LLM) reasoning capabilities have demonstrated promising improvements in model performance by leveraging a lengthy thinking process and additional computational resources during inference, primarily in tasks involving mathematical reasoning (Muennighoff et al., 2025). However, it remains uncertain if longer reasoning chains inherently enhance factual accuracy, particularly beyond mathematical contexts. In this work, we thoroughly examine LLM reasoning within complex open-domain question-answering (QA) scenarios. We initially distill reasoning traces from advanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then fine-tune a variety of models ranging from smaller, instruction-tuned variants to larger architectures based on Qwen2.5. To enrich reasoning traces, we introduce factual information from knowledge graphs in the form of paths into our reasoning traces. Our experimental setup includes four baseline approaches and six different instruction-tuned models evaluated across a benchmark of six datasets, encompassing over 22.6K questions. Overall, we carry out 168 experimental runs and analyze approximately 1.7 million reasoning traces. Our findings indicate that, within a single run, smaller reasoning models achieve noticeable improvements in factual accuracy compared to their original instruction-tuned counterparts. Moreover, our analysis demonstrates that adding test-time compute and token budgets factual accuracy consistently improves by 2-8%, further confirming the effectiveness of test-time scaling for enhancing performance and consequently improving reasoning accuracy in open-domain QA tasks. We release all the experimental artifacts for further research.",
            "score": 1,
            "issue_id": 3827,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 мая",
                "en": "May 16",
                "zh": "5月16日"
            },
            "hash": "29d6b0a8040db2ff",
            "authors": [
                "Mike Zhang",
                "Johannes Bjerva",
                "Russa Biswas"
            ],
            "affiliations": [
                "Department of Computer Science Aalborg University Copenhagen, Denmark"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11140.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#graphs",
                    "#dataset",
                    "#benchmark",
                    "#inference",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Длительные рассуждения улучшают точность ответов языковых моделей",
                    "desc": "Исследование посвящено изучению влияния длительного процесса рассуждений на точность ответов больших языковых моделей (LLM) в задачах открытого вопросно-ответного поиска. Авторы провели эксперименты с различными моделями, обогащая цепочки рассуждений фактической информацией из графов знаний. Результаты показывают, что меньшие модели рассуждений достигают заметных улучшений в фактической точности по сравнению с исходными инструктированными аналогами. Добавление вычислительных ресурсов и увеличение лимита токенов во время тестирования последовательно улучшает фактическую точность на 2-8%."
                },
                "en": {
                    "title": "Enhancing LLM Reasoning with Knowledge and Compute",
                    "desc": "This paper investigates the reasoning capabilities of large language models (LLMs) in open-domain question-answering tasks. It analyzes how longer reasoning processes and additional computational resources can impact factual accuracy, especially beyond mathematical reasoning. The authors fine-tune various models and incorporate knowledge graph information to enhance reasoning traces. Their experiments reveal that smaller models can achieve better factual accuracy than larger, instruction-tuned models, and that increasing computational resources during inference can further improve performance."
                },
                "zh": {
                    "title": "提升推理准确性的关键在于模型与资源的结合",
                    "desc": "本研究探讨了大型语言模型（LLM）在复杂开放领域问答（QA）场景中的推理能力。我们从先进的推理模型中提取推理轨迹，并对多种模型进行微调，以提高其推理准确性。通过引入知识图谱中的事实信息，我们丰富了推理轨迹，并在多个数据集上进行了广泛的实验。结果表明，较小的推理模型在事实准确性上有显著提升，而在测试时增加计算资源和令牌预算也能进一步提高准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11049",
            "title": "GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning",
            "url": "https://huggingface.co/papers/2505.11049",
            "abstract": "To enhance the safety of VLMs, this paper introduces a novel reasoning-based VLM guard model dubbed GuardReasoner-VL. The core idea is to incentivize the guard model to deliberatively reason before making moderation decisions via online RL. First, we construct GuardReasoner-VLTrain, a reasoning corpus with 123K samples and 631K reasoning steps, spanning text, image, and text-image inputs. Then, based on it, we cold-start our model's reasoning ability via SFT. In addition, we further enhance reasoning regarding moderation through online RL. Concretely, to enhance diversity and difficulty of samples, we conduct rejection sampling followed by data augmentation via the proposed safety-aware data concatenation. Besides, we use a dynamic clipping parameter to encourage exploration in early stages and exploitation in later stages. To balance performance and token efficiency, we design a length-aware safety reward that integrates accuracy, format, and token cost. Extensive experiments demonstrate the superiority of our model. Remarkably, it surpasses the runner-up by 19.27% F1 score on average. We release data, code, and models (3B/7B) of GuardReasoner-VL at https://github.com/yueliu1999/GuardReasoner-VL/",
            "score": 1,
            "issue_id": 3829,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 мая",
                "en": "May 16",
                "zh": "5月16日"
            },
            "hash": "bedca054f1392a71",
            "authors": [
                "Yue Liu",
                "Shengfang Zhai",
                "Mingzhe Du",
                "Yulin Chen",
                "Tri Cao",
                "Hongcheng Gao",
                "Cheng Wang",
                "Xinfeng Li",
                "Kun Wang",
                "Junfeng Fang",
                "Jiaheng Zhang",
                "Bryan Hooi"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11049.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#rl",
                    "#dataset",
                    "#reasoning",
                    "#open_source",
                    "#training"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Рассуждающий страж для безопасных визуально-языковых моделей",
                    "desc": "Эта статья представляет GuardReasoner-VL - новую модель-охранник для визуально-языковых моделей (VLM), использующую рассуждения для повышения безопасности. Модель обучается с помощью онлайн-обучения с подкреплением (RL) на специально созданном корпусе GuardReasoner-VLTrain, содержащем 123 тысячи образцов с рассуждениями. Авторы применяют несколько техник для улучшения обучения, включая отбор образцов, аугментацию данных и динамическую настройку параметров исследования. Эксперименты показывают, что GuardReasoner-VL превосходит другие модели, улучшая F1-меру на 19.27% в среднем."
                },
                "en": {
                    "title": "GuardReasoner-VL: Enhancing VLM Safety through Reasoning and Reinforcement Learning",
                    "desc": "This paper presents GuardReasoner-VL, a new model designed to improve the safety of Vision-Language Models (VLMs) by incorporating reasoning into moderation decisions. The model is trained using a large reasoning corpus that includes diverse text and image inputs, allowing it to learn from 123K samples and 631K reasoning steps. To enhance its reasoning capabilities, the model employs supervised fine-tuning (SFT) and online reinforcement learning (RL), which helps it adapt and improve over time. The results show that GuardReasoner-VL significantly outperforms existing models, achieving a 19.27% higher F1 score on average, demonstrating its effectiveness in ensuring safer VLM operations."
                },
                "zh": {
                    "title": "推理驱动的安全保护：GuardReasoner-VL",
                    "desc": "为了提高视觉语言模型（VLM）的安全性，本文提出了一种新的基于推理的VLM保护模型，称为GuardReasoner-VL。该模型通过在线强化学习（RL）激励保护模型在做出审查决策之前进行深思熟虑的推理。我们构建了一个包含123K样本和631K推理步骤的推理语料库，并通过监督微调（SFT）来冷启动模型的推理能力。此外，我们通过在线RL进一步增强了审查推理的能力，最终实验结果显示该模型在F1分数上平均超越了第二名19.27%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11011",
            "title": "Humans expect rationality and cooperation from LLM opponents in\n  strategic games",
            "url": "https://huggingface.co/papers/2505.11011",
            "abstract": "As Large Language Models (LLMs) integrate into our social and economic interactions, we need to deepen our understanding of how humans respond to LLMs opponents in strategic settings. We present the results of the first controlled monetarily-incentivised laboratory experiment looking at differences in human behaviour in a multi-player p-beauty contest against other humans and LLMs. We use a within-subject design in order to compare behaviour at the individual level. We show that, in this environment, human subjects choose significantly lower numbers when playing against LLMs than humans, which is mainly driven by the increased prevalence of `zero' Nash-equilibrium choices. This shift is mainly driven by subjects with high strategic reasoning ability. Subjects who play the zero Nash-equilibrium choice motivate their strategy by appealing to perceived LLM's reasoning ability and, unexpectedly, propensity towards cooperation. Our findings provide foundational insights into the multi-player human-LLM interaction in simultaneous choice games, uncover heterogeneities in both subjects' behaviour and beliefs about LLM's play when playing against them, and suggest important implications for mechanism design in mixed human-LLM systems.",
            "score": 1,
            "issue_id": 3828,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 мая",
                "en": "May 16",
                "zh": "5月16日"
            },
            "hash": "0f12554b6b3b2b83",
            "authors": [
                "Darija Barak",
                "Miguel Costa-Gomes"
            ],
            "affiliations": [
                "School of Economics University of Edinburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11011.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rlhf",
                    "#games"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Люди vs ИИ: новые стратегии в играх с искусственным интеллектом",
                    "desc": "Это исследование изучает поведение людей в стратегической игре p-beauty contest против других людей и больших языковых моделей (LLM). Эксперимент показал, что участники выбирают значительно меньшие числа при игре против LLM, чем против людей, что объясняется более частым выбором равновесия Нэша 'ноль'. Такое поведение в основном наблюдается у участников с высокими способностями к стратегическому мышлению. Результаты дают важные insights о взаимодействии человека и LLM в играх с одновременным выбором и имеют значение для проектирования механизмов в смешанных человеко-LLM системах."
                },
                "en": {
                    "title": "Understanding Human Behavior in Games Against LLMs",
                    "desc": "This paper investigates how humans behave when competing against Large Language Models (LLMs) in strategic games, specifically in a multi-player p-beauty contest. The study reveals that participants tend to choose lower numbers when playing against LLMs compared to human opponents, influenced by the perception of LLMs' reasoning capabilities. The results indicate that individuals with strong strategic reasoning are more likely to adopt a 'zero' Nash-equilibrium strategy, believing it aligns with the LLM's cooperative tendencies. These findings highlight the complexities of human-LLM interactions and their implications for designing effective systems that integrate both human and LLM participants."
                },
                "zh": {
                    "title": "人类与大型语言模型的战略互动新视角",
                    "desc": "本研究探讨了人类在与大型语言模型（LLMs）进行战略互动时的行为差异。通过一个受控的实验，我们发现人类在与LLMs对战时选择的数字显著低于与其他人类对战时的选择。这种现象主要是由于高战略推理能力的参与者更倾向于选择零纳什均衡策略。我们的发现为人类与LLMs在多玩家同时选择游戏中的互动提供了基础性见解，并揭示了参与者行为和对LLMs游戏方式的信念差异。"
                }
            }
        }
    ],
    "link_prev": "2025-05-16.html",
    "link_next": "2025-05-20.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "16.05",
        "en": "05/16",
        "zh": "5月16日"
    },
    "short_date_next": {
        "ru": "20.05",
        "en": "05/20",
        "zh": "5月20日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 3,
        "#benchmark": 4,
        "#agents": 1,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 9,
        "#robotics": 0,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 7,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章介绍了Qwen3，这是Qwen模型系列的最新版本。Qwen3包括多种大型语言模型，旨在提高性能、效率和多语言能力。它包含密集和混合专家架构，参数规模从0.6到2350亿不等。Qwen3的创新之处在于将思考模式和非思考模式结合在一个框架中，消除了切换模型的需要。它还引入了思考预算机制，允许用户根据任务复杂性动态分配计算资源。",
        "title": "Qwen3 Technical Report",
        "pinyin": "这篇文章介绍了Qwen3，这是Qwen模型系列的最新版本。\nZhè piān wénzhāng jièshào le Qwen3, zhè shì Qwen móxíng xìliè de zuìxīn bǎnběn.\n\nQwen3包括多种大型语言模型，旨在提高性能、效率和多语言能力。\nQwen3 bāokuò duōzhǒng dàxíng yǔyán móxíng, zhǐ zài tígāo xìngnéng, xiàolǜ hé duōyǔyán nénglì.\n\n它包含密集和混合专家架构，参数规模从0.6到2350亿不等。\nTā bāohán mìjí hé hùnhé zhuānjiā jiàgòu, cānshù guīmó cóng 0.6 dào 2350 yì bùděng.\n\nQwen3的创新之处在于将思考模式和非思考模式结合在一个框架中，消除了切换模型的需要。\nQwen3 de chuàngxīn zhī chù zài yú jiāng sīkǎo móshì hé fēi sīkǎo móshì jiéhé zài yīgè kuàngjià zhōng, xiāochú le qiēhuàn móxíng de xūyào.\n\n它还引入了思考预算机制，允许用户根据任务复杂性动态分配计算资源。\nTā hái yǐnrù le sīkǎo yùsuàn jīzhì, yǔnxǔ yònghù gēnjù rènwù fùzáxìng dòngtài fēnpèi jìsuàn zīyuán.",
        "vocab": "[\n    {\"word\": \"系列\", \"pinyin\": \"xìliè\", \"trans\": \"series\"},\n    {\"word\": \"版本\", \"pinyin\": \"bǎnběn\", \"trans\": \"version\"},\n    {\"word\": \"旨在\", \"pinyin\": \"zhǐzài\", \"trans\": \"aim to\"},\n    {\"word\": \"效率\", \"pinyin\": \"xiàolǜ\", \"trans\": \"efficiency\"},\n    {\"word\": \"多语言\", \"pinyin\": \"duōyǔyán\", \"trans\": \"multilingual\"},\n    {\"word\": \"能力\", \"pinyin\": \"nénglì\", \"trans\": \"ability\"},\n    {\"word\": \"包含\", \"pinyin\": \"bāohán\", \"trans\": \"contain\"},\n    {\"word\": \"密集\", \"pinyin\": \"mìjí\", \"trans\": \"dense\"},\n    {\"word\": \"混合\", \"pinyin\": \"hùnhé\", \"trans\": \"hybrid\"},\n    {\"word\": \"专家\", \"pinyin\": \"zhuānjiā\", \"trans\": \"expert\"},\n    {\"word\": \"架构\", \"pinyin\": \"jiàgòu\", \"trans\": \"architecture\"},\n    {\"word\": \"参数\", \"pinyin\": \"cānshǔ\", \"trans\": \"parameter\"},\n    {\"word\": \"规模\", \"pinyin\": \"guīmó\", \"trans\": \"scale\"},\n    {\"word\": \"创新\", \"pinyin\": \"chuàngxīn\", \"trans\": \"innovation\"},\n    {\"word\": \"之处\", \"pinyin\": \"zhīchù\", \"trans\": \"place\"},\n    {\"word\": \"思考\", \"pinyin\": \"sīkǎo\", \"trans\": \"think\"},\n    {\"word\": \"模式\", \"pinyin\": \"móshì\", \"trans\": \"mode\"},\n    {\"word\": \"结合\", \"pinyin\": \"jiéhé\", \"trans\": \"combine\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàngjià\", \"trans\": \"framework\"},\n    {\"word\": \"消除\", \"pinyin\": \"xiāochú\", \"trans\": \"eliminate\"},\n    {\"word\": \"切换\", \"pinyin\": \"qiēhuàn\", \"trans\": \"switch\"},\n    {\"word\": \"需要\", \"pinyin\": \"xūyào\", \"trans\": \"need\"},\n    {\"word\": \"引入\", \"pinyin\": \"yǐnrù\", \"trans\": \"introduce\"},\n    {\"word\": \"预算\", \"pinyin\": \"yùsuàn\", \"trans\": \"budget\"},\n    {\"word\": \"机制\", \"pinyin\": \"jīzhì\", \"trans\": \"mechanism\"},\n    {\"word\": \"允许\", \"pinyin\": \"yǔnxǔ\", \"trans\": \"allow\"},\n    {\"word\": \"根据\", \"pinyin\": \"gēnjù\", \"trans\": \"according to\"},\n    {\"word\": \"任务\", \"pinyin\": \"rènwù\", \"trans\": \"task\"},\n    {\"word\": \"复杂性\", \"pinyin\": \"fùzáxìng\", \"trans\": \"complexity\"},\n    {\"word\": \"动态\", \"pinyin\": \"dòngtài\", \"trans\": \"dynamic\"},\n    {\"word\": \"分配\", \"pinyin\": \"fēnpèi\", \"trans\": \"allocate\"},\n    {\"word\": \"计算\", \"pinyin\": \"jìsuàn\", \"trans\": \"compute\"},\n    {\"word\": \"资源\", \"pinyin\": \"zīyuán\", \"trans\": \"resources\"}\n]",
        "trans": "This article introduces Qwen3, the latest version in the Qwen model series. Qwen3 includes a variety of large language models aimed at enhancing performance, efficiency, and multilingual capabilities. It features dense and mixture-of-experts architectures, with parameter sizes ranging from 0.6 to 2350 billion. The innovation of Qwen3 lies in combining thinking and non-thinking modes within a single framework, eliminating the need to switch models. It also introduces a thinking budget mechanism, allowing users to dynamically allocate computational resources based on the complexity of the task.",
        "update_ts": "2025-05-19 09:13"
    }
}