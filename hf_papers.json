{
    "date": {
        "ru": "15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 15",
        "zh": "12æœˆ15æ—¥"
    },
    "time_utc": "2025-12-15 09:35",
    "weekday": 0,
    "issue_id": 57,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2512.11558",
            "title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry",
            "url": "https://huggingface.co/papers/2512.11558",
            "abstract": "DentalGPT, a specialized dental multimodal large language model, achieves superior performance in disease classification and dental VQA tasks through high-quality domain knowledge injection and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.",
            "score": 33,
            "issue_id": 51,
            "pub_date": "2025-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "4d36c99c02abd4b4",
            "authors": [
                "Zhenyang Cai",
                "Jiaming Zhang",
                "Junjie Zhao",
                "Ziyi Zeng",
                "Yanchao Li",
                "Jingyi Liang",
                "Junying Chen",
                "Yunjin Yang",
                "Jiajun You",
                "Shuzhi Deng",
                "Tongfei Wang",
                "Wanting Chen",
                "Chunxiu Hao",
                "Ruiqi Xie",
                "Zhenwei Wen",
                "Xiangyi Feng",
                "Zou Ting",
                "Jin Zou Lin",
                "Jianquan Li",
                "Guangjun Yu",
                "Liangyi Chen",
                "Junwen Wang",
                "Shan Jiang",
                "Benyou Wang"
            ],
            "affiliations": [
                "Beijing Institute of Collaborative Innovation",
                "Division of Applied Oral Sciences & Community Dental Care Faculty of Dentistry, The University of Hong Kong",
                "Freedom AI",
                "National Health Data Institute, Shenzhen",
                "Shenzhen Institute of Big Data",
                "Shenzhen Loop Area Institute",
                "Shenzhen Stomatology Hospital (Pingshan) of Southern Medical University",
                "State Key Laboratory of Membrane Biology, Beijing Key Laboratory of Cardiometabolic Molecular Medicine, Institute of Molecular Medicine, National Biomedical Imaging Center, School of Future Technology, Peking University",
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.11558.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#benchmark",
                    "#multimodal",
                    "#science",
                    "#reasoning",
                    "#open_source",
                    "#synthetic",
                    "#training",
                    "#rl",
                    "#dataset"
                ],
                "emoji": "ğŸ¦·",
                "ru": {
                    "title": "Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸",
                    "desc": "DentalGPT â€” ÑÑ‚Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ² ÑÑ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ 120k ÑÑ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»Ğ¾ÑÑ‚Ğ¸ Ñ€Ñ‚Ğ°. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (reinforcement learning) Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ»Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ (7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²), DentalGPT Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ ÑÑ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "DentalGPT: Revolutionizing Dental Diagnosis with Multimodal AI",
                    "desc": "DentalGPT is a specialized multimodal large language model designed for the dental field, which excels in disease classification and visual question answering (VQA) tasks. It was developed using a large annotated dataset of over 120,000 dental images paired with detailed descriptions, enhancing its ability to understand complex dental visuals. The model incorporates reinforcement learning to improve its reasoning capabilities, allowing for more accurate diagnoses. Evaluations show that DentalGPT outperforms many existing models, demonstrating the effectiveness of combining high-quality dental data with advanced training techniques."
                },
                "zh": {
                    "title": "DentalGPTï¼šç‰™ç§‘é¢†åŸŸçš„æ™ºèƒ½åŠ©æ‰‹",
                    "desc": "DentalGPTæ˜¯ä¸€ç§ä¸“é—¨é’ˆå¯¹ç‰™ç§‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æ³¨å…¥é«˜è´¨é‡çš„é¢†åŸŸçŸ¥è¯†å’Œå¼ºåŒ–å­¦ä¹ ï¼Œæ˜¾è‘—æå‡äº†ç–¾ç—…åˆ†ç±»å’Œç‰™ç§‘è§†è§‰é—®ç­”ä»»åŠ¡çš„è¡¨ç°ã€‚ä¸ºäº†å…‹æœç°æœ‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•æ‰ç»†è‡´ç‰™ç§‘è§†è§‰ç»†èŠ‚å’Œæ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼ŒDentalGPTæ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ç‰™ç§‘æ³¨é‡Šå¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡12ä¸‡å¼ ç‰™ç§‘å›¾åƒåŠå…¶è¯¦ç»†æè¿°ã€‚è¯¥æ¨¡å‹åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œæ˜¾è‘—å¢å¼ºäº†å¯¹ç‰™ç§‘çŠ¶å†µçš„è§†è§‰ç†è§£ï¼Œåç»­çš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µè¿›ä¸€æ­¥æå‡äº†å…¶å¤šæ¨¡æ€å¤æ‚æ¨ç†èƒ½åŠ›ã€‚ç»¼åˆè¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒDentalGPTåœ¨ç–¾ç—…åˆ†ç±»å’Œç‰™ç§‘è§†è§‰é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†è®¸å¤šæœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå°½ç®¡å…¶å‚æ•°ä»…ä¸º70äº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.11749",
            "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
            "url": "https://huggingface.co/papers/2512.11749",
            "abstract": "SVG-T2I, a scaled SVG framework, enables high-quality text-to-image synthesis directly in the Visual Foundation Model feature domain, achieving competitive performance in generative tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.",
            "score": 23,
            "issue_id": 51,
            "pub_date": "2025-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "3b2603c9f5e073db",
            "authors": [
                "Minglei Shi",
                "Haolin Wang",
                "Borui Zhang",
                "Wenzhao Zheng",
                "Bohan Zeng",
                "Ziyang Yuan",
                "Xiaoshi Wu",
                "Yuanxing Zhang",
                "Huan Yang",
                "Xintao Wang",
                "Pengfei Wan",
                "Kun Gai",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "affiliations": [
                "Department of Automation, Tsinghua University",
                "Kling Team, Kuaishou Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.11749.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#open_source",
                    "#training",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Visual Foundation Model",
                    "desc": "SVG-T2I â€” ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Visual Foundation Model. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ pipeline Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ĞµĞ³Ğ¾ Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ VFM-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… GenEval (0.75) Ğ¸ DPG-Bench (85.78), Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ Ğ¼Ğ¾Ñ‰ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ VFM Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²ÑĞµ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ pipeline-Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "SVG-T2I: Bridging Text and Image with VFM Power",
                    "desc": "SVG-T2I is a new framework that enhances text-to-image synthesis by utilizing the Visual Foundation Model (VFM) feature domain. It focuses on generating high-quality images from text descriptions while leveraging self-supervised representations for better performance. The framework demonstrates competitive results in generative tasks, achieving notable scores on evaluation benchmarks. By open-sourcing the project, it aims to promote further research in the area of representation-driven visual generation."
                },
                "zh": {
                    "title": "é«˜è´¨é‡æ–‡æœ¬åˆ°å›¾åƒåˆæˆçš„æ–°è·¯å¾„",
                    "desc": "SVG-T2Iæ˜¯ä¸€ä¸ªæ‰©å±•çš„SVGæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨è§†è§‰åŸºç¡€æ¨¡å‹ç‰¹å¾åŸŸä¸­ç›´æ¥å®ç°é«˜è´¨é‡çš„æ–‡æœ¬åˆ°å›¾åƒåˆæˆã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ ‡å‡†çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£ç®¡é“ï¼Œè¾¾åˆ°äº†ç«äº‰åŠ›çš„æ€§èƒ½ï¼ŒéªŒè¯äº†è§†è§‰åŸºç¡€æ¨¡å‹åœ¨ç”Ÿæˆä»»åŠ¡ä¸­çš„å†…åœ¨è¡¨ç¤ºèƒ½åŠ›ã€‚å°½ç®¡åœ¨è§†è§‰åŸºç¡€æ¨¡å‹è¡¨ç¤ºç©ºé—´ä¸­è®­ç»ƒå¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç ”ç©¶ä»ç„¶è¾ƒå°‘ï¼Œä½†SVG-T2Iä¸ºæ­¤æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬å°†é¡¹ç›®å®Œå…¨å¼€æºï¼ŒåŒ…æ‹¬è‡ªç¼–ç å™¨å’Œç”Ÿæˆæ¨¡å‹ï¼Œä»¥åŠå®ƒä»¬çš„è®­ç»ƒã€æ¨ç†ã€è¯„ä¼°ç®¡é“å’Œé¢„è®­ç»ƒæƒé‡ï¼Œä»¥ä¿ƒè¿›åŸºäºè¡¨ç¤ºçš„è§†è§‰ç”Ÿæˆçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.11799",
            "title": "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties",
            "url": "https://huggingface.co/papers/2512.11799",
            "abstract": "V-RGBX is an end-to-end framework for intrinsic-aware video editing that combines video inverse rendering, photorealistic synthesis, and keyframe-based editing to produce consistent and physically plausible edits.  \t\t\t\t\tAI-generated summary \t\t\t\t Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.",
            "score": 22,
            "issue_id": 51,
            "pub_date": "2025-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "fa4bfef8afa61334",
            "authors": [
                "Ye Fang",
                "Tong Wu",
                "Valentin Deschaintre",
                "Duygu Ceylan",
                "Iliyan Georgiev",
                "Chun-Hao Paul Huang",
                "Yiwei Hu",
                "Xuelin Chen",
                "Tuanfeng Yang Wang"
            ],
            "affiliations": [
                "Adobe Research",
                "Fudan University",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.11799.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² ÑÑ†ĞµĞ½Ñ‹",
                    "desc": "V-RGBX â€” ÑÑ‚Ğ¾ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹: Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² ÑÑ†ĞµĞ½Ñ‹ (Ğ°Ğ»ÑŒĞ±ĞµĞ´Ğ¾, Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸, Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ñ‹ Ğ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğµ), Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ÑÑ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»ÑĞ±Ñ‹Ğ¼Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ²ÑĞµĞ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğµ ÑÑ†ĞµĞ½."
                },
                "en": {
                    "title": "Revolutionizing Video Editing with Intrinsic Awareness",
                    "desc": "V-RGBX is a novel framework designed for intrinsic-aware video editing, integrating video inverse rendering, photorealistic synthesis, and keyframe-based editing. It uniquely understands intrinsic scene properties like albedo and material, allowing for realistic video generation and manipulation. The framework employs an interleaved conditioning mechanism that facilitates intuitive editing based on user-selected keyframes, ensuring edits are consistent and physically plausible. V-RGBX outperforms previous methods in producing high-quality, temporally coherent videos, making it effective for various applications such as object appearance editing and scene relighting."
                },
                "zh": {
                    "title": "å†…åœ¨æ„ŸçŸ¥è§†é¢‘ç¼–è¾‘çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "V-RGBXæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œä¸“æ³¨äºå†…åœ¨æ„ŸçŸ¥çš„è§†é¢‘ç¼–è¾‘ã€‚å®ƒç»“åˆäº†è§†é¢‘é€†æ¸²æŸ“ã€é€¼çœŸçš„åˆæˆå’ŒåŸºäºå…³é”®å¸§çš„ç¼–è¾‘ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸€è‡´ä¸”ç‰©ç†ä¸Šåˆç†çš„ç¼–è¾‘æ•ˆæœã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯äº¤é”™æ¡ä»¶æœºåˆ¶ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡é€‰æ‹©å…³é”®å¸§è¿›è¡Œç›´è§‚çš„ã€åŸºäºç‰©ç†çš„ç¼–è¾‘ã€‚V-RGBXåœ¨å¤šä¸ªåº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬ç‰©ä½“å¤–è§‚ç¼–è¾‘å’Œåœºæ™¯é‡ç…§æ˜ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.08269",
            "title": "EgoX: Egocentric Video Generation from a Single Exocentric Video",
            "url": "https://huggingface.co/papers/2512.08269",
            "abstract": "EgoX framework generates egocentric videos from exocentric inputs using video diffusion models with LoRA adaptation, unified conditioning, and geometry-guided self-attention for coherence and visual fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.",
            "score": 22,
            "issue_id": 53,
            "pub_date": "2025-12-09",
            "pub_date_card": {
                "ru": "9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 9",
                "zh": "12æœˆ9æ—¥"
            },
            "hash": "3bb2b096dc745e11",
            "authors": [
                "Taewoong Kang",
                "Kinam Kim",
                "Dohyeon Kim",
                "Minho Park",
                "Junha Hyung",
                "Jaegul Choo"
            ],
            "affiliations": [
                "KAIST AI",
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.08269.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#diffusion",
                    "#video",
                    "#multimodal",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "ĞÑ‚ Ğ²Ğ·Ğ³Ğ»ÑĞ´Ğ° ÑĞ±Ğ¾ĞºÑƒ Ğº Ğ²Ğ·Ğ³Ğ»ÑĞ´Ñƒ Ğ¸Ğ·Ğ½ÑƒÑ‚Ñ€Ğ¸: Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "EgoX â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ñ‚ Ñ‚Ñ€ĞµÑ‚ÑŒĞµĞ³Ğ¾ Ğ»Ğ¸Ñ†Ğ° Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ LoRA Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¾Ğ±Ğ¾Ğ¸Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ self-attention, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ ÑÑ†ĞµĞ½Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ñ‚Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ·Ñ€ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Transforming Perspectives: From Exocentric to Immersive Egocentric Videos",
                    "desc": "The EgoX framework is designed to create egocentric videos from exocentric inputs by utilizing advanced video diffusion models. It employs LoRA adaptation to enhance the model's performance while integrating a unified conditioning strategy that merges different video perspectives. A geometry-guided self-attention mechanism is implemented to focus on relevant spatial areas, ensuring that the generated videos maintain visual coherence and fidelity. This innovative approach allows for the effective synthesis of first-person videos, even in challenging scenarios with varying camera angles and limited overlap."
                },
                "zh": {
                    "title": "EgoXï¼šä»å¤–éƒ¨è§†è§’ç”Ÿæˆè‡ªæˆ‘ä¸­å¿ƒè§†é¢‘çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "EgoXæ¡†æ¶é€šè¿‡è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆè‡ªæˆ‘ä¸­å¿ƒçš„è§†é¢‘ï¼Œä½¿ç”¨LoRAé€‚åº”ã€ç»Ÿä¸€æ¡ä»¶å’Œå‡ ä½•å¼•å¯¼çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥ç¡®ä¿ä¸€è‡´æ€§å’Œè§†è§‰çœŸå®æ„Ÿã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå°†å¤–éƒ¨è§†è§’ï¼ˆç¬¬ä¸‰äººç§°ï¼‰è§†é¢‘è½¬æ¢ä¸ºè‡ªæˆ‘è§†è§’ï¼ˆç¬¬ä¸€äººç§°ï¼‰è§†é¢‘ï¼Œå°½ç®¡é¢ä¸´æå¤§çš„ç›¸æœºå§¿æ€å˜åŒ–å’Œè§†è§’é‡å ä¸è¶³çš„æŒ‘æˆ˜ã€‚EgoXåˆ©ç”¨å¤§è§„æ¨¡è§†é¢‘æ‰©æ•£æ¨¡å‹çš„é¢„è®­ç»ƒæ—¶ç©ºçŸ¥è¯†ï¼Œå¹¶é€šè¿‡è½»é‡çº§çš„LoRAé€‚åº”å¼•å…¥ç»Ÿä¸€çš„æ¡ä»¶ç­–ç•¥ï¼Œç»“åˆå¤–éƒ¨å’Œè‡ªæˆ‘ä¸­å¿ƒçš„å…ˆéªŒä¿¡æ¯ã€‚æœ€ç»ˆï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆä¸€è‡´ä¸”çœŸå®çš„è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„å¯æ‰©å±•æ€§å’Œé²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.10411",
            "title": "Sliding Window Attention Adaptation",
            "url": "https://huggingface.co/papers/2512.10411",
            "abstract": "Sliding Window Attention Adaptation (SWAA) enables Transformer-based Large Language Models (LLMs) to use sliding window attention without retraining, recovering long-context performance through a combination of adaptation techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving \"sink\" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation",
            "score": 10,
            "issue_id": 54,
            "pub_date": "2025-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "e30d3f22fa97adbb",
            "authors": [
                "Yijiong Yu",
                "Jiale Liu",
                "Qingyun Wu",
                "Huazheng Wang",
                "Ji Pei"
            ],
            "affiliations": [
                "DeepSolution",
                "Oregon State University",
                "Penn State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.10411.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#open_source",
                    "#optimization"
                ],
                "emoji": "ğŸªŸ",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ (SWAA) Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ±Ñ‹Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… Ğ¸Ğ·-Ğ·Ğ° Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ¾Ğ¼. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ¿ÑÑ‚Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº: Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²-ÑĞºĞ¾Ñ€ĞµĞ¹, Ñ‡ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾ĞµĞ² Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ¸ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰Ğ¸Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… Ğ¿Ñ€Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Efficient Long-Context Adaptation for Transformers with SWAA",
                    "desc": "The paper introduces Sliding Window Attention Adaptation (SWAA), a method that allows Transformer-based Large Language Models (LLMs) to efficiently use sliding window attention without the need for retraining. This approach addresses the challenge of long-context performance degradation when switching from full attention to sliding window attention during inference. SWAA combines several techniques, including selective application of sliding window attention, preservation of specific tokens, and interleaving different attention layers, to enhance adaptation. The results demonstrate that while adapting these models is complex, effective combinations of methods can restore their long-context capabilities while maintaining efficiency."
                },
                "zh": {
                    "title": "æ»‘åŠ¨çª—å£æ³¨æ„åŠ›é€‚åº”ï¼šæå‡é•¿ä¸Šä¸‹æ–‡æ€§èƒ½çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æ»‘åŠ¨çª—å£æ³¨æ„åŠ›é€‚åº”ï¼ˆSWAAï¼‰ä½¿åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿåœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ä½¿ç”¨æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼Œä»è€Œé€šè¿‡é€‚åº”æŠ€æœ¯ç»„åˆæ¢å¤é•¿ä¸Šä¸‹æ–‡æ€§èƒ½ã€‚è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—å¤æ‚åº¦éšç€è¾“å…¥é•¿åº¦çš„å¢åŠ è€Œå‘ˆå¹³æ–¹å¢é•¿ï¼Œå¯¼è‡´é•¿ä¸Šä¸‹æ–‡æ¨ç†æˆæœ¬é«˜æ˜‚ã€‚SWAAæå‡ºäº†ä¸€ç³»åˆ—å®ç”¨æ–¹æ³•ï¼ŒåŒ…æ‹¬åœ¨é¢„å¡«å……æœŸé—´åº”ç”¨æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ã€ä¿ç•™â€œæ²‰æ²¡â€æ ‡è®°ã€äº¤é”™å…¨æ³¨æ„åŠ›å’Œæ»‘åŠ¨çª—å£æ³¨æ„åŠ›å±‚ã€é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ä»¥åŠå¾®è°ƒï¼Œä»¥å®ç°æ›´å¥½çš„é€‚åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ»‘åŠ¨çª—å£æ³¨æ„åŠ›çš„é€‚åº”æ˜¯å¯è¡Œçš„ï¼Œä½†å¹¶éç®€å•ï¼Œç‰¹å®šçš„ååŒç»„åˆèƒ½å¤Ÿæœ‰æ•ˆæ¢å¤åŸå§‹çš„é•¿ä¸Šä¸‹æ–‡æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.11253",
            "title": "PersonaLive! Expressive Portrait Image Animation for Live Streaming",
            "url": "https://huggingface.co/papers/2512.11253",
            "abstract": "PersonaLive is a diffusion-based framework for real-time portrait animation that enhances speed and efficiency through multi-stage training, hybrid implicit signals, appearance distillation, and autoregressive micro-chunk streaming.  \t\t\t\t\tAI-generated summary \t\t\t\t Current diffusion-based portrait animation models predominantly focus on enhancing visual quality and expression realism, while overlooking generation latency and real-time performance, which restricts their application range in the live streaming scenario. We propose PersonaLive, a novel diffusion-based framework towards streaming real-time portrait animation with multi-stage training recipes. Specifically, we first adopt hybrid implicit signals, namely implicit facial representations and 3D implicit keypoints, to achieve expressive image-level motion control. Then, a fewer-step appearance distillation strategy is proposed to eliminate appearance redundancy in the denoising process, greatly improving inference efficiency. Finally, we introduce an autoregressive micro-chunk streaming generation paradigm equipped with a sliding training strategy and a historical keyframe mechanism to enable low-latency and stable long-term video generation. Extensive experiments demonstrate that PersonaLive achieves state-of-the-art performance with up to 7-22x speedup over prior diffusion-based portrait animation models.",
            "score": 8,
            "issue_id": 51,
            "pub_date": "2025-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "63e3852998e651d2",
            "authors": [
                "Zhiyuan Li",
                "Chi-Man Pun",
                "Chen Fang",
                "Jue Wang",
                "Xiaodong Cun"
            ],
            "affiliations": [
                "Dzine.ai",
                "GVC Lab, Great Bay University",
                "University of Macau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.11253.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#3d",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ² 7-22 Ñ€Ğ°Ğ·Ğ°",
                    "desc": "PersonaLive â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸, Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¸ĞºÑ€Ğ¾-Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ÑƒÑ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Real-Time Portrait Animation Revolutionized with PersonaLive",
                    "desc": "PersonaLive is a cutting-edge framework designed for real-time portrait animation using diffusion models. It focuses on improving the speed and efficiency of animation generation while maintaining high visual quality. The framework employs multi-stage training, hybrid implicit signals for better motion control, and a unique appearance distillation method to streamline the denoising process. Additionally, it introduces an autoregressive micro-chunk streaming approach that allows for low-latency video generation, achieving significant speed improvements over existing models."
                },
                "zh": {
                    "title": "PersonaLiveï¼šå®æ—¶è‚–åƒåŠ¨ç”»çš„æ–°çªç ´",
                    "desc": "PersonaLive æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„å®æ—¶è‚–åƒåŠ¨ç”»æ¡†æ¶ï¼Œé€šè¿‡å¤šé˜¶æ®µè®­ç»ƒã€æ··åˆéšå¼ä¿¡å·ã€å¤–è§‚è’¸é¦å’Œè‡ªå›å½’å¾®å—æµæ¥æé«˜é€Ÿåº¦å’Œæ•ˆç‡ã€‚è¯¥æ¡†æ¶è§£å†³äº†å½“å‰è‚–åƒåŠ¨ç”»æ¨¡å‹åœ¨å®æ—¶æ€§èƒ½ä¸Šçš„ä¸è¶³ï¼Œä½¿å…¶æ›´é€‚åˆç›´æ’­åœºæ™¯ã€‚æˆ‘ä»¬é‡‡ç”¨æ··åˆéšå¼ä¿¡å·æ¥å®ç°å›¾åƒçº§è¿åŠ¨æ§åˆ¶ï¼Œå¹¶æå‡ºäº†å‡å°‘å¤–è§‚å†—ä½™çš„è’¸é¦ç­–ç•¥ï¼Œä»è€Œæ˜¾è‘—æé«˜æ¨ç†æ•ˆç‡ã€‚æœ€ç»ˆï¼ŒPersonaLive å®ç°äº†æ¯”ä»¥å¾€æ¨¡å‹å¿« 7-22 å€çš„æ€§èƒ½ï¼Œå±•ç°äº†å…¶åœ¨å®æ—¶è§†é¢‘ç”Ÿæˆä¸­çš„ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.11792",
            "title": "Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation",
            "url": "https://huggingface.co/papers/2512.11792",
            "abstract": "SAM2VideoX improves realistic motion generation in video models by integrating structure-preserving priors from an autoregressive model into a bidirectional diffusion model with novel feature fusion and local alignment techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60\\% on VBench, 21-22\\% lower FVD, and 71.4\\% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51\\%, surpassing REPA (92.91\\%) by 2.60\\%, and reduce FVD to 360.57, a 21.20\\% and 22.46\\% improvement over REPA- and LoRA-finetuning, respectively. The project website can be found at https://sam2videox.github.io/ .",
            "score": 5,
            "issue_id": 51,
            "pub_date": "2025-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "94f116052c2d1ea3",
            "authors": [
                "Yang Fei",
                "George Stoica",
                "Jingyuan Liu",
                "Qifeng Chen",
                "Ranjay Krishna",
                "Xiaojuan Wang",
                "Benlin Liu"
            ],
            "affiliations": [
                "Adobe",
                "Georgia Tech",
                "HKUST",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.11792.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#diffusion",
                    "#architecture",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸: Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ SAM2VideoX, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾-ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ² Ğ¸Ğ· Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ SAM2 Ğ² Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CogVideoX. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Local Gram Flow loss Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑĞ¾Ñ‡Ğ»ĞµĞ½Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ»ÑĞ´Ğ¸ Ğ¸ Ğ¶Ğ¸Ğ²Ğ¾Ñ‚Ğ½Ñ‹Ğµ, Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ½Ñ‹Ñ… Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ: +2.60% Ğ½Ğ° VBench, ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ FVD Ğ½Ğ° 21-22% Ğ¸ 71.4% Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ»ÑĞ´ÑŒĞ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Realistic Motion Generation in Video Models",
                    "desc": "SAM2VideoX enhances video generation by combining structure-preserving motion priors from an autoregressive model with a bidirectional diffusion model. It introduces a bidirectional feature fusion module to capture global motion patterns and a Local Gram Flow loss to ensure local feature alignment. This approach addresses the challenges of generating realistic motion for articulated and deformable objects, which traditional methods struggle with. Experimental results demonstrate significant improvements in motion fidelity and human preference compared to existing models."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘ç”Ÿæˆçš„çœŸå®è¿åŠ¨è¡¨ç°",
                    "desc": "SAM2VideoXé€šè¿‡å°†è‡ªå›å½’æ¨¡å‹ä¸­çš„ç»“æ„ä¿æŒå…ˆéªŒä¸åŒå‘æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œæ”¹å–„äº†è§†é¢‘æ¨¡å‹ä¸­çš„çœŸå®è¿åŠ¨ç”Ÿæˆã€‚è¯¥æ–¹æ³•å¼•å…¥äº†æ–°çš„ç‰¹å¾èåˆå’Œå±€éƒ¨å¯¹é½æŠ€æœ¯ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„è¿åŠ¨æ—¢çœŸå®åˆä¿æŒç»“æ„ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹å–å¾—äº†ä¸€å®šè¿›å±•ï¼Œä½†åœ¨ç”Ÿæˆå…³èŠ‚å’Œå¯å˜å½¢ç‰©ä½“ï¼ˆå¦‚äººç±»å’ŒåŠ¨ç‰©ï¼‰çš„çœŸå®è¿åŠ¨æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAM2VideoXåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆè§†é¢‘çš„è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.11464",
            "title": "Exploring MLLM-Diffusion Information Transfer with MetaCanvas",
            "url": "https://huggingface.co/papers/2512.11464",
            "abstract": "MetaCanvas leverages multimodal large language models as latent-space planners to enhance precise and structured image and video generation, outperforming global-conditioning methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.",
            "score": 5,
            "issue_id": 51,
            "pub_date": "2025-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "337ee6656cf51654",
            "authors": [
                "Han Lin",
                "Xichen Pan",
                "Ziqi Huang",
                "Ji Hou",
                "Jialiang Wang",
                "Weifeng Chen",
                "Zecheng He",
                "Felix Juefei-Xu",
                "Junzhe Sun",
                "Zhipeng Fan",
                "Ali Thabet",
                "Mohit Bansal",
                "Chu Wang"
            ],
            "affiliations": [
                "Meta Superintelligence Labs",
                "Nanyang Technological University",
                "New York University",
                "UNC Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.11464.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#reasoning",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¸ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° MetaCanvas â€” Ğ»ĞµĞ³ĞºĞ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ MLLMs Ñ€Ğ¾Ğ»ÑŒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ², Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ¸Ğ¼ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ĞµÑ‰Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿ÑƒÑ‚ÑŒ ÑĞ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "Empowering Image and Video Generation with Latent-Space Planning",
                    "desc": "MetaCanvas is a novel framework that utilizes multimodal large language models (MLLMs) as planners in latent spaces to improve the generation of images and videos. Unlike traditional methods that rely on global text encoders, MetaCanvas allows MLLMs to engage in reasoning and planning directly within spatial and spatiotemporal contexts. This approach enhances the precision and structure of generated content, addressing the limitations of existing global-conditioning techniques. Through extensive evaluation across various tasks, MetaCanvas demonstrates superior performance, highlighting its potential to bridge the gap between multimodal understanding and generation."
                },
                "zh": {
                    "title": "MetaCanvasï¼šæå‡å›¾åƒè§†é¢‘ç”Ÿæˆçš„æ½œåœ¨ç©ºé—´è§„åˆ’è€…",
                    "desc": "MetaCanvas æ˜¯ä¸€ä¸ªè½»é‡çº§æ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½œä¸ºæ½œåœ¨ç©ºé—´è§„åˆ’è€…ï¼Œæå‡å›¾åƒå’Œè§†é¢‘ç”Ÿæˆçš„ç²¾ç¡®æ€§å’Œç»“æ„åŒ–ã€‚ä¼ ç»Ÿçš„å¤šæ¨¡æ€å­¦ä¹ é€šå¸¸å°†å¼ºå¤§çš„è¯­è¨€æ¨¡å‹ç®€åŒ–ä¸ºå…¨å±€æ–‡æœ¬ç¼–ç å™¨ï¼Œå¯¼è‡´å…¶æ¨ç†å’Œè§„åˆ’èƒ½åŠ›æœªè¢«å……åˆ†åˆ©ç”¨ã€‚MetaCanvas å…è®¸ MLLMs ç›´æ¥åœ¨ç©ºé—´å’Œæ—¶ç©ºæ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ¨ç†å’Œè§„åˆ’ï¼Œå¹¶ä¸æ‰©æ•£ç”Ÿæˆå™¨ç´§å¯†ç»“åˆã€‚é€šè¿‡åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¿›è¡Œå®è¯è¯„ä¼°ï¼ŒMetaCanvas æ˜¾ç¤ºå‡ºåœ¨ç”Ÿæˆç²¾ç¡®å¸ƒå±€å’Œå¼ºå±æ€§ç»‘å®šæ–¹é¢çš„ä¼˜åŠ¿ï¼Œè¡¨æ˜å°† MLLMs è§†ä¸ºæ½œåœ¨ç©ºé—´è§„åˆ’è€…æ˜¯ç¼©å°å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä¹‹é—´å·®è·çš„æœ‰æ•ˆæ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.06818",
            "title": "MeshSplatting: Differentiable Rendering with Opaque Meshes",
            "url": "https://huggingface.co/papers/2512.06818",
            "abstract": "MeshSplatting, a mesh-based reconstruction method, enhances novel view synthesis by optimizing geometry and appearance through differentiable rendering, improving quality and efficiency over existing techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.",
            "score": 4,
            "issue_id": 55,
            "pub_date": "2025-12-07",
            "pub_date_card": {
                "ru": "7 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 7",
                "zh": "12æœˆ7æ—¥"
            },
            "hash": "38a05e08f469d8ee",
            "authors": [
                "Jan Held",
                "Sanghyun Son",
                "Renaud Vandeghen",
                "Daniel Rebain",
                "Matheus Gadelha",
                "Yi Zhou",
                "Anthony Cioppa",
                "Ming C. Lin",
                "Marc Van Droogenbroeck",
                "Andrea Tagliasacchi"
            ],
            "affiliations": [
                "Adobe Research",
                "Simon Fraser University",
                "University of British Columbia",
                "University of LiÃ¨ge",
                "University of Maryland",
                "University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.06818.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#cv"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ¡ĞµÑ‚ĞºĞ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ²: Ğ¼Ğ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ¾Ğ¼ Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ¹",
                    "desc": "MeshSplatting â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞµÑ‚Ğ¾Ğº Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ². ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½ÑƒÑ Ñ‚Ñ€Ğ¸Ğ°Ğ½Ğ³ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ”ĞµĞ»Ğ¾Ğ½Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° 0.69 Ğ´Ğ‘ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ PSNR Ğ¸ Ğ´Ğ²ÑƒĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ³Ğ»Ğ°Ğ´ĞºĞ¸Ğµ, Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ°Ğ¼Ğ¸ AR/VR Ğ¸ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing 3D Rendering with MeshSplatting",
                    "desc": "MeshSplatting is a new method for creating 3D models that improves how we generate images from different viewpoints. It uses a technique called differentiable rendering to optimize both the shape and appearance of the models, making them look better and faster than older methods. This approach connects points in a way that keeps the surfaces smooth and consistent, which is important for applications like augmented reality and video games. By achieving higher image quality and faster training times, MeshSplatting effectively combines advanced neural rendering with real-time graphics."
                },
                "zh": {
                    "title": "MeshSplattingï¼šæå‡æ–°è§†è§’åˆæˆçš„ç½‘æ ¼é‡å»ºæ–¹æ³•",
                    "desc": "MeshSplattingæ˜¯ä¸€ç§åŸºäºç½‘æ ¼çš„é‡å»ºæ–¹æ³•ï¼Œé€šè¿‡å¯å¾®æ¸²æŸ“ä¼˜åŒ–å‡ ä½•å½¢çŠ¶å’Œå¤–è§‚ï¼Œä»è€Œæå‡æ–°è§†è§’åˆæˆçš„è´¨é‡å’Œæ•ˆç‡ã€‚è¯¥æ–¹æ³•é€šè¿‡é™åˆ¶çš„å¾·åŠ³å†…ä¸‰è§’å‰–åˆ†æ¥å¼ºåˆ¶è¿æ¥æ€§ï¼Œå¹¶æ”¹å–„è¡¨é¢ä¸€è‡´æ€§ï¼Œç”Ÿæˆå¹³æ»‘ä¸”è§†è§‰è´¨é‡é«˜çš„ç½‘æ ¼ï¼Œèƒ½å¤Ÿåœ¨å®æ—¶3Då¼•æ“ä¸­é«˜æ•ˆæ¸²æŸ“ã€‚ä¸å½“å‰æœ€å…ˆè¿›çš„MiLoæ–¹æ³•ç›¸æ¯”ï¼ŒMeshSplattingåœ¨Mip-NeRF360æ•°æ®é›†ä¸Šæé«˜äº†0.69 dBçš„PSNRï¼ŒåŒæ—¶è®­ç»ƒé€Ÿåº¦æé«˜äº†2å€ï¼Œå†…å­˜ä½¿ç”¨å‡å°‘äº†2å€ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆåœ°å°†ç¥ç»æ¸²æŸ“ä¸äº¤äº’å¼3Då›¾å½¢ç»“åˆï¼Œå®ç°æ— ç¼çš„å®æ—¶åœºæ™¯äº¤äº’ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.10605",
            "title": "LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator",
            "url": "https://huggingface.co/papers/2512.10605",
            "abstract": "A general-purpose language-driven framework for robots, LEO-RobotAgent, enhances human-robot interaction and task planning using large language models across various robot types and tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose LEO-RobotAgent, a general-purpose language-driven intelligent agent framework for robots. Under this framework, LLMs can operate different types of robots to complete unpredictable complex tasks across various scenarios. This framework features strong generalization, robustness, and efficiency. The application-level system built around it can fully enhance bidirectional human-robot intent understanding and lower the threshold for human-robot interaction. Regarding robot task planning, the vast majority of existing studies focus on the application of large models in single-task scenarios and for single robot types. These algorithms often have complex structures and lack generalizability. Thus, the proposed LEO-RobotAgent framework is designed with a streamlined structure as much as possible, enabling large models to independently think, plan, and act within this clear framework. We provide a modular and easily registrable toolset, allowing large models to flexibly call various tools to meet different requirements. Meanwhile, the framework incorporates a human-robot interaction mechanism, enabling the algorithm to collaborate with humans like a partner. Experiments have verified that this framework can be easily adapted to mainstream robot platforms including unmanned aerial vehicles (UAVs), robotic arms, and wheeled robot, and efficiently execute a variety of carefully designed tasks with different complexity levels. Our code is available at https://github.com/LegendLeoChen/LEO-RobotAgent.",
            "score": 2,
            "issue_id": 57,
            "pub_date": "2025-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "61e63e51edb593ff",
            "authors": [
                "Lihuang Chen",
                "Xiangyu Luo",
                "Jun Meng"
            ],
            "affiliations": [
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.10605.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#robotics",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ LEO-RobotAgent â€” ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ´Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ… â€” Ğ±ĞµÑĞ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ½Ğ¸ĞºĞ°Ñ…, Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ°Ñ… Ğ¸ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-Ñ€Ğ¾Ğ±Ğ¾Ñ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡Ğ°ĞµÑ‚ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Empowering Robots with Language for Smarter Interaction and Task Planning",
                    "desc": "LEO-RobotAgent is a versatile framework that uses large language models (LLMs) to improve how robots interact with humans and plan tasks. It allows different types of robots to handle complex and unpredictable tasks in various environments. The framework is designed to be efficient and robust, making it easier for robots to understand human intentions and collaborate effectively. By simplifying the structure of task planning, LEO-RobotAgent enhances the adaptability of robots across multiple platforms, such as UAVs and robotic arms."
                },
                "zh": {
                    "title": "é€šç”¨è¯­è¨€é©±åŠ¨çš„æœºå™¨äººæ™ºèƒ½æ¡†æ¶",
                    "desc": "LEO-RobotAgentæ˜¯ä¸€ä¸ªé€šç”¨çš„è¯­è¨€é©±åŠ¨æ™ºèƒ½ä»£ç†æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºäººæœºäº¤äº’å’Œä»»åŠ¡è§„åˆ’ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥æ“ä½œä¸åŒç±»å‹çš„æœºå™¨äººï¼Œå®Œæˆå¤æ‚çš„ä»»åŠ¡ã€‚å®ƒå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€é²æ£’æ€§å’Œé«˜æ•ˆæ€§ï¼Œèƒ½å¤Ÿé™ä½äººæœºäº¤äº’çš„é—¨æ§›ã€‚é€šè¿‡æ¨¡å—åŒ–çš„å·¥å…·é›†ï¼ŒLEO-RobotAgentå¯ä»¥çµæ´»è°ƒç”¨å„ç§å·¥å…·ï¼Œé€‚åº”ä¸åŒçš„éœ€æ±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.11393",
            "title": "The N-Body Problem: Parallel Execution from Single-Person Egocentric Video",
            "url": "https://huggingface.co/papers/2512.11393",
            "abstract": "A model learns to parallelize tasks from a single egocentric video by addressing spatial and object conflicts, achieving improved action coverage and reduced collisions.  \t\t\t\t\tAI-generated summary \t\t\t\t Humans can intuitively parallelise complex activities, but can a model learn this from observing a single person? Given one egocentric video, we introduce the N-Body Problem: how N individuals, can hypothetically perform the same set of tasks observed in this video. The goal is to maximise speed-up, but naive assignment of video segments to individuals often violates real-world constraints, leading to physically impossible scenarios like two people using the same object or occupying the same space. To address this, we formalise the N-Body Problem and propose a suite of metrics to evaluate both performance (speed-up, task coverage) and feasibility (spatial collisions, object conflicts and causal constraints). We then introduce a structured prompting strategy that guides a Vision-Language Model (VLM) to reason about the 3D environment, object usage, and temporal dependencies to produce a viable parallel execution. On 100 videos from EPIC-Kitchens and HD-EPIC, our method for N = 2 boosts action coverage by 45% over a baseline prompt for Gemini 2.5 Pro, while simultaneously slashing collision rates, object and causal conflicts by 55%, 45% and 55% respectively.",
            "score": 1,
            "issue_id": 51,
            "pub_date": "2025-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "ee646b596d383e5a",
            "authors": [
                "Zhifan Zhu",
                "Yifei Huang",
                "Yoichi Sato",
                "Dima Damen"
            ],
            "affiliations": [
                "The University of Tokyo",
                "University of Bristol"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.11393.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ‘¥",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸Ğ· ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Vision-Language Model Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ½Ğ° 45% Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ»Ğ»Ğ¸Ğ·Ğ¸Ğ¹, ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° 55%, 45% Ğ¸ 55% ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾."
                },
                "en": {
                    "title": "Learning to Parallelize Tasks from Egocentric Video",
                    "desc": "This paper presents a method for a model to learn how to parallelize tasks observed in a single egocentric video. It introduces the N-Body Problem, which involves multiple individuals performing tasks while avoiding conflicts such as spatial collisions and object usage overlaps. The authors propose metrics to evaluate both the efficiency of task execution and the feasibility of the proposed actions. By using a structured prompting strategy with a Vision-Language Model, the model significantly improves action coverage and reduces conflicts in task execution."
                },
                "zh": {
                    "title": "ä»è§†é¢‘ä¸­å­¦ä¹ ä»»åŠ¡å¹¶è¡ŒåŒ–çš„æ™ºèƒ½æ¨¡å‹",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ¨¡å‹ï¼Œèƒ½å¤Ÿä»å•ä¸ªè‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ä¸­å­¦ä¹ å¦‚ä½•å¹¶è¡ŒåŒ–ä»»åŠ¡ï¼Œè§£å†³ç©ºé—´å’Œç‰©ä½“å†²çªé—®é¢˜ã€‚æˆ‘ä»¬å®šä¹‰äº†Nä½“é—®é¢˜ï¼Œæ¢è®¨Nä¸ªä¸ªä½“å¦‚ä½•åœ¨è§‚å¯Ÿåˆ°çš„ä»»åŠ¡ä¸­æœ‰æ•ˆåœ°å¹¶è¡Œæ‰§è¡Œã€‚é€šè¿‡å¼•å…¥ä¸€å¥—è¯„ä¼°æŒ‡æ ‡ï¼Œæˆ‘ä»¬ä¸ä»…å…³æ³¨ä»»åŠ¡çš„è¦†ç›–ç‡å’Œé€Ÿåº¦æå‡ï¼Œè¿˜è€ƒè™‘äº†å®é™…æ‰§è¡Œä¸­çš„å¯è¡Œæ€§ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªè§†é¢‘ä¸Šæ˜¾è‘—æé«˜äº†åŠ¨ä½œè¦†ç›–ç‡ï¼Œå¹¶å‡å°‘äº†å†²çªå’Œç¢°æ’çš„å‘ç”Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.10715",
            "title": "CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images",
            "url": "https://huggingface.co/papers/2512.10715",
            "abstract": "Uncertainty estimation in landmark-based segmentation of chest X-rays using hybrid neural network architectures improves reliability and robustness in clinical deployment.  \t\t\t\t\tAI-generated summary \t\t\t\t Uncertainty estimation is essential for the safe clinical deployment of medical image segmentation systems, enabling the identification of unreliable predictions and supporting human oversight. While prior work has largely focused on pixel-level uncertainty, landmark-based segmentation offers inherent topological guarantees yet remains underexplored from an uncertainty perspective. In this work, we study uncertainty estimation for anatomical landmark-based segmentation on chest X-rays. Inspired by hybrid neural network architectures that combine standard image convolutional encoders with graph-based generative decoders, and leveraging their variational latent space, we derive two complementary measures: (i) latent uncertainty, captured directly from the learned distribution parameters, and (ii) predictive uncertainty, obtained by generating multiple stochastic output predictions from latent samples. Through controlled corruption experiments we show that both uncertainty measures increase with perturbation severity, reflecting both global and local degradation. We demonstrate that these uncertainty signals can identify unreliable predictions by comparing with manual ground-truth, and support out-of-distribution detection on the CheXmask dataset. More importantly, we release CheXmask-U (huggingface.co/datasets/mcosarinsky/CheXmask-U), a large scale dataset of 657,566 chest X-ray landmark segmentations with per-node uncertainty estimates, enabling researchers to account for spatial variations in segmentation quality when using these anatomical masks. Our findings establish uncertainty estimation as a promising direction to enhance robustness and safe deployment of landmark-based anatomical segmentation methods in chest X-ray. A fully working interactive demo of the method is available at huggingface.co/spaces/matiasky/CheXmask-U and the source code at github.com/mcosarinsky/CheXmask-U.",
            "score": 1,
            "issue_id": 56,
            "pub_date": "2025-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "9dd547fc5e9ff57c",
            "authors": [
                "Matias Cosarinsky",
                "Nicolas Gaggion",
                "Rodrigo Echeveste",
                "Enzo Ferrante"
            ],
            "affiliations": [
                "APOLO Biotech, Buenos Aires, Argentina",
                "Laboratory of Applied Artificial Intelligence, Institute of Computer Sciences, CONICET - Universidad de Buenos Aires, Argentina",
                "Research Institute for Signals, Systems and Computational Intelligence, sinc(i), CONICET - Universidad Nacional del Litoral, Argentina",
                "Weizmann Institute of Science, Rehovot, Israel"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.10715.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#healthcare",
                    "#architecture",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ«",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ½Ñ‚Ğ³ĞµĞ½Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ… Ğ³Ñ€ÑƒĞ´Ğ½Ğ¾Ğ¹ ĞºĞ»ĞµÑ‚ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ñ… ÑĞ²Ñ‘Ñ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğ° Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ñ… Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ° Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ· Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ğ° ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑ‚ÑƒÑ‚ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ‚ĞµĞ¿ĞµĞ½Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ¿ÑƒÑÑ‚Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ CheXmask-U Ñ 657,566 ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑƒĞ·Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ² ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ."
                },
                "en": {
                    "title": "Enhancing Reliability in Chest X-ray Segmentation through Uncertainty Estimation",
                    "desc": "This paper focuses on improving the reliability of landmark-based segmentation in chest X-rays by incorporating uncertainty estimation. It introduces a hybrid neural network architecture that combines convolutional encoders with graph-based decoders to derive two types of uncertainty: latent uncertainty and predictive uncertainty. The study shows that these uncertainty measures can effectively identify unreliable predictions and enhance out-of-distribution detection. Additionally, the authors provide a large dataset, CheXmask-U, which includes uncertainty estimates for anatomical landmark segmentations, facilitating better assessment of segmentation quality."
                },
                "zh": {
                    "title": "æå‡èƒ¸éƒ¨Xå…‰åˆ†å‰²å¯é æ€§çš„å…³é”®åœ¨äºä¸ç¡®å®šæ€§ä¼°è®¡",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºè§£å‰–æ ‡å¿—çš„èƒ¸éƒ¨Xå…‰å›¾åƒåˆ†å‰²ä¸­çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆç¥ç»ç½‘ç»œæ¶æ„ï¼Œç»“åˆäº†æ ‡å‡†å›¾åƒå·ç§¯ç¼–ç å™¨å’ŒåŸºäºå›¾çš„ç”Ÿæˆè§£ç å™¨ï¼Œä»¥æé«˜åˆ†å‰²çš„å¯é æ€§å’Œç¨³å¥æ€§ã€‚é€šè¿‡å¼•å…¥æ½œåœ¨ä¸ç¡®å®šæ€§å’Œé¢„æµ‹ä¸ç¡®å®šæ€§ä¸¤ç§äº’è¡¥åº¦é‡ï¼Œæˆ‘ä»¬èƒ½å¤Ÿè¯†åˆ«ä¸å¯é çš„é¢„æµ‹å¹¶æ”¯æŒä¸´åºŠåº”ç”¨ä¸­çš„äººå·¥ç›‘ç£ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªåŒ…å«657,566ä¸ªèƒ¸éƒ¨Xå…‰æ ‡å¿—åˆ†å‰²åŠå…¶ä¸ç¡®å®šæ€§ä¼°è®¡çš„å¤§å‹æ•°æ®é›†ï¼Œä¿ƒè¿›äº†ç›¸å…³ç ”ç©¶çš„å¼€å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.06951",
            "title": "Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge",
            "url": "https://huggingface.co/papers/2512.06951",
            "abstract": "A vision-action policy using correlated noise for flow matching and learnable mixed-layer attention wins the 2025 BEHAVIOR Challenge with high performance across diverse household tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making.   Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules.   Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.",
            "score": 1,
            "issue_id": 51,
            "pub_date": "2025-12-07",
            "pub_date_card": {
                "ru": "7 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 7",
                "zh": "12æœˆ7æ—¥"
            },
            "hash": "7a391e30872f5e27",
            "authors": [
                "Ilia Larchenko",
                "Gleb Zarin",
                "Akash Karnatak"
            ],
            "affiliations": [
                "Independent Researchers"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.06951.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#benchmark",
                    "#cv",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞšĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑˆÑƒĞ¼ Ğ´Ğ»Ñ Ğ³Ğ»Ğ°Ğ´ĞºĞ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ² Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾-Ğ¼Ğ¾Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹Ğ¸Ğ³Ñ€Ğ°Ğ»Ğ° ĞºĞ¾Ğ½ĞºÑƒÑ€Ñ BEHAVIOR Challenge 2025 Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¸Ğ· 50 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ±Ñ‹Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Pi0.5 Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ° Ğ´Ğ»Ñ flow matching, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ»Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ³Ğ»Ğ°Ğ´ĞºĞ¸Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ñ‘Ğ² Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 26% q-score Ğ½Ğ° Ğ²ÑĞµÑ… 50 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ²ÑƒÑ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Winning with Correlated Noise and Attention in Household Tasks",
                    "desc": "This paper presents a vision-action policy that achieved first place in the 2025 BEHAVIOR Challenge, which involved complex household tasks requiring advanced manipulation and decision-making. The authors introduce correlated noise for flow matching, enhancing training efficiency and enabling smoother action sequences through correlation-aware inpainting. They also implement learnable mixed-layer attention and System 2 stage tracking to resolve ambiguities during task execution. The proposed method demonstrates a significant performance improvement, achieving a 26% q-score across a diverse set of 50 tasks."
                },
                "zh": {
                    "title": "ç›¸å…³å™ªå£°ä¸æ··åˆå±‚æ³¨æ„åŠ›çš„åˆ›æ–°ç­–ç•¥",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è§†è§‰-åŠ¨ä½œç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨2025å¹´BEHAVIORæŒ‘æˆ˜èµ›ä¸­è·å¾—ç¬¬ä¸€åï¼Œå±•ç¤ºäº†åœ¨50ä¸ªå¤šæ ·åŒ–å®¶åº­ä»»åŠ¡ä¸­çš„é«˜æ€§èƒ½ã€‚è¿™äº›ä»»åŠ¡éœ€è¦åŒæ‰‹æ“ä½œã€å¯¼èˆªå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥å†³ç­–ã€‚æˆ‘ä»¬æå‡ºçš„ä¸»è¦åˆ›æ–°æ˜¯ä½¿ç”¨ç›¸å…³å™ªå£°è¿›è¡ŒæµåŒ¹é…ï¼Œè¿™æé«˜äº†è®­ç»ƒæ•ˆç‡å¹¶å®ç°äº†å¹³æ»‘çš„åŠ¨ä½œåºåˆ—ã€‚é€šè¿‡å¤šæ ·æœ¬æµåŒ¹é…å’Œå¯å­¦ä¹ çš„æ··åˆå±‚æ³¨æ„åŠ›ï¼Œæˆ‘ä»¬æœ‰æ•ˆåœ°è§£å†³äº†æ¨¡ç³Šæ€§é—®é¢˜ï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µåº”ç”¨äº†åŠ¨ä½œå‹ç¼©å’Œç‰¹å®šæŒ‘æˆ˜çš„ä¿®æ­£è§„åˆ™ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-12-12.html",
    "link_next": "2025-12-16.html",
    "link_month": "2025-12.html",
    "short_date_prev": {
        "ru": "12.12",
        "en": "12/12",
        "zh": "12æœˆ12æ—¥"
    },
    "short_date_next": {
        "ru": "16.12",
        "en": "12/16",
        "zh": "12æœˆ16æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 6,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 0,
        "#video": 6,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 5,
        "#healthcare": 2,
        "#training": 6,
        "#robotics": 2,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}