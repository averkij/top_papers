{
    "date": "13 –æ–∫—Ç—è–±—Ä—è",
    "time_utc": "2024-10-13 16:13",
    "issue_id": 83,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.07484",
            "title": "WALL-E: World Alignment by Rule Learning Improves World Model-based LLM Agents",
            "url": "https://huggingface.co/papers/2410.07484",
            "abstract": "Can large language models (LLMs) directly serve as powerful world models for model-based agents? While the gaps between the prior knowledge of LLMs and the specified environment's dynamics do exist, our study reveals that the gaps can be bridged by aligning an LLM with its deployed environment and such \"world alignment\" can be efficiently achieved by rule learning on LLMs. Given the rich prior knowledge of LLMs, only a few additional rules suffice to align LLM predictions with the specified environment dynamics. To this end, we propose a neurosymbolic approach to learn these rules gradient-free through LLMs, by inducing, updating, and pruning rules based on comparisons of agent-explored trajectories and world model predictions. The resulting world model is composed of the LLM and the learned rules. Our embodied LLM agent \"WALL-E\" is built upon model-predictive control (MPC). By optimizing look-ahead actions based on the precise world model, MPC significantly improves exploration and learning efficiency. Compared to existing LLM agents, WALL-E's reasoning only requires a few principal rules rather than verbose buffered trajectories being included in the LLM input. On open-world challenges in Minecraft and ALFWorld, WALL-E achieves higher success rates than existing methods, with lower costs on replanning time and the number of tokens used for reasoning. In Minecraft, WALL-E exceeds baselines by 15-30% in success rate while costing 8-20 fewer replanning rounds and only 60-80% of tokens. In ALFWorld, its success rate surges to a new record high of 95% only after 6 iterations.",
            "score": 38,
            "issue_id": 59,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∫–∞—á–µ—Å—Ç–≤–µ –º–æ—â–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∏—Ä–∞ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–µ–π—Ä–æ—Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª, –ø–æ–∑–≤–æ–ª—è—é—â–∏—Ö —Å–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è LLM —Å –¥–∏–Ω–∞–º–∏–∫–æ–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ä–µ–¥—ã. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç WALL-E –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª—å–Ω–æ-–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É—é—â–µ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ (MPC) –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–µ–π—Å—Ç–≤–∏–π. –í —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö –≤ Minecraft –∏ ALFWorld WALL-E –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.",
                "tags": [
                    "#worldModelAlignment",
                    "#neurosymbolicLearning",
                    "#modelPredictiveControl"
                ],
                "categories": [
                    "#rl",
                    "#nlp",
                    "#embodiedAI",
                    "#planning",
                    "#reasoning"
                ],
                "emoji": "ü§ñ",
                "title": "LLM –∫–∞–∫ –º–æ–¥–µ–ª—å –º–∏—Ä–∞: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª–∞–º –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è —Å –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥–æ–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.08196",
            "title": "MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code",
            "url": "https://huggingface.co/papers/2410.08196",
            "abstract": "Code has been shown to be effective in enhancing the mathematical reasoning abilities of large language models due to its precision and accuracy. Previous works involving continued mathematical pretraining often include code that utilizes math-related packages, which are primarily designed for fields such as engineering, machine learning, signal processing, or module testing, rather than being directly focused on mathematical reasoning. In this paper, we introduce a novel method for generating mathematical code accompanied with corresponding reasoning steps for continued pretraining. Our approach begins with the construction of a high-quality mathematical continued pretraining dataset by incorporating math-related web data, code using mathematical packages, math textbooks, and synthetic data. Next, we construct reasoning steps by extracting LaTeX expressions, the conditions needed for the expressions, and the results of the expressions from the previously collected dataset. Based on this extracted information, we generate corresponding code to accurately capture the mathematical reasoning process. Appending the generated code to each reasoning step results in data consisting of paired natural language reasoning steps and their corresponding code. Combining this data with the original dataset results in a 19.2B-token high-performing mathematical pretraining corpus, which we name MathCode-Pile. Training several popular base models with this corpus significantly improves their mathematical abilities, leading to the creation of the MathCoder2 family of models. All of our data processing and training code is open-sourced, ensuring full transparency and easy reproducibility of the entire data collection and training pipeline. The code is released at https://github.com/mathllm/MathCoder2 .",
            "score": 36,
            "issue_id": 52,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–¥–∞ —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º–∏ —à–∞–≥–∞–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç MathCode-Pile –æ–±—ä–µ–º–æ–º 19.2 –º–ª—Ä–¥ —Ç–æ–∫–µ–Ω–æ–≤, –≤–∫–ª—é—á–∞—é—â–∏–π –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤–µ–±-–¥–∞–Ω–Ω—ã–µ, –∫–æ–¥ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫, —É—á–µ–±–Ω–∏–∫–∏ –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–π LaTeX, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —É—Å–ª–æ–≤–∏–π –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∞ –∑–∞—Ç–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ –∫–æ–¥–∞. –û–±—É—á–µ–Ω–∏–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —ç—Ç–æ–º –∫–æ—Ä–ø—É—Å–µ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏–ª–æ –∏—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏.",
                "tags": [
                    "#MathCode-Pile",
                    "#–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ_—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ",
                    "#–≥–µ–Ω–µ—Ä–∞—Ü–∏—è_–∫–æ–¥–∞"
                ],
                "categories": [
                    "#nlp",
                    "#dataset",
                    "#code",
                    "#benchmark",
                    "#rag"
                ],
                "emoji": "üßÆ",
                "title": "–£–ª—É—á—à–µ–Ω–∏–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –∫–æ–¥–∞"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.03450",
            "title": "MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents",
            "url": "https://huggingface.co/papers/2410.03450",
            "abstract": "MLLM agents demonstrate potential for complex embodied tasks by retrieving multimodal task-relevant trajectory data. However, current retrieval methods primarily focus on surface-level similarities of textual or visual cues in trajectories, neglecting their effectiveness for the specific task at hand. To address this issue, we propose a novel method, MLLM as ReTriever (MART), which enhances the performance of embodied agents by utilizing interaction data to fine-tune an MLLM retriever based on preference learning, such that the retriever fully considers the effectiveness of trajectories and prioritize them for unseen tasks. We also introduce Trajectory Abstraction, a mechanism that leverages MLLMs' summarization capabilities to represent trajectories with fewer tokens while preserving key information, enabling agents to better comprehend milestones in the trajectory. Experimental results across various environments demonstrate our method significantly improves task success rates in unseen scenes compared to baseline methods. This work presents a new paradigm for multimodal retrieval in embodied agents, by fine-tuning a general-purpose MLLM as the retriever to assess trajectory effectiveness. All benchmark task sets and simulator code modifications for action and observation spaces will be released.",
            "score": 29,
            "issue_id": 51,
            "pub_date": "2024-10-04",
            "pub_date_ru": "4 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ MART –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –≤–æ–ø–ª–æ—â–µ–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM). MART –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ MLLM-—Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ–∑–∏—Ä–æ–≤–∞—Ç—å —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á. –¢–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç—Å—è –º–µ—Ö–∞–Ω–∏–∑–º –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ MLLM –ø–æ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —ç—Ç–∞–ø–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –≤ –Ω–æ–≤—ã—Ö —Å—Ü–µ–Ω–∞—Ö –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.",
                "tags": [
                    "#–≤–æ–ø–ª–æ—â–µ–Ω–Ω—ã–µ–ê–≥–µ–Ω—Ç—ã",
                    "#–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π–†–µ—Ç—Ä–∏–≤–µ—Ä",
                    "#–∞–±—Å—Ç—Ä–∞–∫—Ü–∏—è–¢—Ä–∞–µ–∫—Ç–æ—Ä–∏–π"
                ],
                "categories": [
                    "#rl",
                    "#nlp",
                    "#cv",
                    "#multimodal",
                    "#embodiedAI"
                ],
                "emoji": "ü§ñ",
                "title": "MART: –£–ª—É—á—à–µ–Ω–∏–µ –≤–æ–ø–ª–æ—â–µ–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ —Ç–æ–Ω–∫—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ä–µ—Ç—Ä–∏–≤–µ—Ä–æ–≤"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05265",
            "title": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers in LLMs",
            "url": "https://huggingface.co/papers/2410.05265",
            "abstract": "Quantization is essential for deploying Large Language Models (LLMs) by enhancing memory efficiency and inference speed. Existing methods for activation quantization mainly address channel-wise outliers, often neglecting token-wise outliers, leading to reliance on costly per-token dynamic quantization. To address this, we introduce PrefixQuant, a novel technique that isolates outlier tokens offline without re-training. Specifically, PrefixQuant identifies high-frequency outlier tokens and prefixes them in the KV cache, preventing the generation of outlier tokens during inference and simplifying quantization. To our knowledge, PrefixQuant is the first to enable efficient per-tensor static quantization to outperform expensive per-token dynamic quantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and 4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization achieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5 common-sense reasoning tasks, outperforming previous per-token dynamic quantization methods like QuaRot with 0.98 perplexity improvement and +5.98 points accuracy. Additionally, the inference speed of W4A4 quantized models using PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot models by 1.2x to 1.3x. Our code is available at https://github.com/ChenMnZ/PrefixQuant.",
            "score": 26,
            "issue_id": 51,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "PrefixQuant - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –∫–æ—Ç–æ—Ä—ã–π –∏–∑–æ–ª–∏—Ä—É–µ—Ç –≤—ã–±—Ä–æ—Å—ã –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç–æ–∫–µ–Ω–æ–≤ –æ—Ñ–ª–∞–π–Ω –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. –û–Ω –ø—Ä–µ—Ñ–∏–∫—Å–∏—Ä—É–µ—Ç —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è —Ç–æ–∫–µ–Ω—ã-–≤—ã–±—Ä–æ—Å—ã –≤ KV-–∫—ç—à–µ, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç–µ–Ω–∑–æ—Ä–æ–≤. PrefixQuant –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –≤—ã–≤–æ–¥–∞. –ú–µ—Ç–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –≤ –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏.",
                "tags": [
                    "#–∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ_–∞–∫—Ç–∏–≤–∞—Ü–∏–π",
                    "#–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è_LLM",
                    "#—Ç–æ–∫–µ–Ω—ã_–≤—ã–±—Ä–æ—Å—ã"
                ],
                "categories": [
                    "#nlp",
                    "#benchmark",
                    "#code",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "üóúÔ∏è",
                "title": "PrefixQuant: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ LLM –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07869",
            "title": "Benchmarking Agentic Workflow Generation",
            "url": "https://huggingface.co/papers/2410.07869",
            "abstract": "Large Language Models (LLMs), with their exceptional ability to handle a wide range of tasks, have driven significant advancements in tackling reasoning and planning tasks, wherein decomposing complex problems into executable workflows is a crucial step in this process. Existing workflow evaluation frameworks either focus solely on holistic performance or suffer from limitations such as restricted scenario coverage, simplistic workflow structures, and lax evaluation standards. To this end, we introduce WorFBench, a unified workflow generation benchmark with multi-faceted scenarios and intricate graph workflow structures. Additionally, we present WorFEval, a systemic evaluation protocol utilizing subsequence and subgraph matching algorithms to accurately quantify the LLM agent's workflow generation capabilities. Through comprehensive evaluations across different types of LLMs, we discover distinct gaps between the sequence planning capabilities and graph planning capabilities of LLM agents, with even GPT-4 exhibiting a gap of around 15%. We also train two open-source models and evaluate their generalization abilities on held-out tasks. Furthermore, we observe that the generated workflows can enhance downstream tasks, enabling them to achieve superior performance with less time during inference. Code and dataset will be available at https://github.com/zjunlp/WorFBench.",
            "score": 16,
            "issue_id": 54,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç WorFBench - –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç WorFEval - –ø—Ä–æ—Ç–æ–∫–æ–ª –æ—Ü–µ–Ω–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º—ã —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ–¥–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏ –ø–æ–¥–≥—Ä–∞—Ñ–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ –º–æ–¥–µ–ª–µ–π –≤ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏ –≥—Ä–∞—Ñ–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –º–æ–≥—É—Ç —É–ª—É—á—à–∏—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö –∑–∞–¥–∞—á.",
                "tags": [
                    "#workflow_generation",
                    "#benchmark_evaluation",
                    "#llm_capabilities"
                ],
                "categories": [
                    "#nlp",
                    "#benchmark",
                    "#rag",
                    "#code",
                    "#dataset"
                ],
                "emoji": "üß†",
                "title": "WorFBench: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –Ø–ú –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.08159",
            "title": "DART: Denoising Autoregressive Transformer for Scalable Text-to-Image Generation",
            "url": "https://huggingface.co/papers/2410.08159",
            "abstract": "Diffusion models have become the dominant approach for visual generation. They are trained by denoising a Markovian process that gradually adds noise to the input. We argue that the Markovian property limits the models ability to fully utilize the generation trajectory, leading to inefficiencies during training and inference. In this paper, we propose DART, a transformer-based model that unifies autoregressive (AR) and diffusion within a non-Markovian framework. DART iteratively denoises image patches spatially and spectrally using an AR model with the same architecture as standard language models. DART does not rely on image quantization, enabling more effective image modeling while maintaining flexibility. Furthermore, DART seamlessly trains with both text and image data in a unified model. Our approach demonstrates competitive performance on class-conditioned and text-to-image generation tasks, offering a scalable, efficient alternative to traditional diffusion models. Through this unified framework, DART sets a new benchmark for scalable, high-quality image synthesis.",
            "score": 15,
            "issue_id": 57,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "DART - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –¥–∏—Ñ—Ñ—É–∑–∏—é –≤ —Ä–∞–º–∫–∞—Ö –Ω–µ–º–∞—Ä–∫–æ–≤—Å–∫–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –ø–∞—Ç—á–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∫–∞–∫ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º, —Ç–∞–∫ –∏ –≤ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–º –∏–∑–º–µ—Ä–µ–Ω–∏–∏. DART –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –º–æ–∂–µ—Ç –æ–±—É—á–∞—Ç—å—Å—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å DART –≤ –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ –∫–ª–∞—Å—Å—É –∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é.",
                "tags": [
                    "#diffusion",
                    "#non-markovian",
                    "#patch-denoising"
                ],
                "categories": [
                    "#cv",
                    "#nlp",
                    "#multimodal",
                    "#transformers"
                ],
                "emoji": "üé®",
                "title": "DART: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.08164",
            "title": "Agent S: An Open Agentic Framework that Uses Computers Like a Human",
            "url": "https://huggingface.co/papers/2410.08164",
            "abstract": "We present Agent S, an open agentic framework that enables autonomous interaction with computers through a Graphical User Interface (GUI), aimed at transforming human-computer interaction by automating complex, multi-step tasks. Agent S aims to address three key challenges in automating computer tasks: acquiring domain-specific knowledge, planning over long task horizons, and handling dynamic, non-uniform interfaces. To this end, Agent S introduces experience-augmented hierarchical planning, which learns from external knowledge search and internal experience retrieval at multiple levels, facilitating efficient task planning and subtask execution. In addition, it employs an Agent-Computer Interface (ACI) to better elicit the reasoning and control capabilities of GUI agents based on Multimodal Large Language Models (MLLMs). Evaluation on the OSWorld benchmark shows that Agent S outperforms the baseline by 9.37% on success rate (an 83.6% relative improvement) and achieves a new state-of-the-art. Comprehensive analysis highlights the effectiveness of individual components and provides insights for future improvements. Furthermore, Agent S demonstrates broad generalizability to different operating systems on a newly-released WindowsAgentArena benchmark. Code available at https://github.com/simular-ai/Agent-S.",
            "score": 15,
            "issue_id": 52,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "Agent S - —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è –∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–∞—è –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º–∏ —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –°–∏—Å—Ç–µ–º–∞ —Ä–µ—à–∞–µ—Ç —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∑–∞–¥–∞—á: –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞–Ω–∏–π, –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∑–∞–¥–∞—á –∏ —Ä–∞–±–æ—Ç–∞ —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º–∏. Agent S –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, —É—Å–∏–ª–µ–Ω–Ω–æ–µ –æ–ø—ã—Ç–æ–º, –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å Agent-Computer Interface –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å GUI –Ω–∞ –æ—Å–Ω–æ–≤–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.",
                "tags": [
                    "#AgentS",
                    "#ExperienceAugmentedPlanning",
                    "#GUIAutomation"
                ],
                "categories": [
                    "#nlp",
                    "#rl",
                    "#benchmark",
                    "#code",
                    "#interaction"
                ],
                "emoji": "ü§ñ",
                "title": "Agent S: –ê–≤—Ç–æ–Ω–æ–º–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–º"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.08207",
            "title": "DICE: Discrete Inversion Enabling Controllable Editing for Multinomial Diffusion and Masked Generative Models",
            "url": "https://huggingface.co/papers/2410.08207",
            "abstract": "Discrete diffusion models have achieved success in tasks like image generation and masked language modeling but face limitations in controlled content editing. We introduce DICE (Discrete Inversion for Controllable Editing), the first approach to enable precise inversion for discrete diffusion models, including multinomial diffusion and masked generative models. By recording noise sequences and masking patterns during the reverse diffusion process, DICE enables accurate reconstruction and flexible editing of discrete data without the need for predefined masks or attention manipulation. We demonstrate the effectiveness of DICE across both image and text domains, evaluating it on models such as VQ-Diffusion, Paella, and RoBERTa. Our results show that DICE preserves high data fidelity while enhancing editing capabilities, offering new opportunities for fine-grained content manipulation in discrete spaces. For project webpage, see https://hexiaoxiao-cs.github.io/DICE/.",
            "score": 15,
            "issue_id": 50,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "DICE (Discrete Inversion for Controllable Editing) - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç–æ—á–Ω–æ–π –∏–Ω–≤–µ—Ä—Å–∏–∏ –¥–ª—è –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å —Ç–æ—á–Ω—É—é —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –∏ –≥–∏–±–∫–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –º–∞—Å–∫–∞—Ö –∏–ª–∏ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è—Ö —Å –≤–Ω–∏–º–∞–Ω–∏–µ–º. DICE —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –º–æ–¥–µ–ª—è–º–∏ –∫–∞–∫ –≤ –æ–±–ª–∞—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Ç–∞–∫ –∏ —Ç–µ–∫—Å—Ç–∞, –≤–∫–ª—é—á–∞—è VQ-Diffusion, Paella –∏ RoBERTa. –ú–µ—Ç–æ–¥ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ —É–ª—É—á—à–µ–Ω–∏–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.",
                "tags": [
                    "#–¥–∏—Å–∫—Ä–µ—Ç–Ω–∞—è_–¥–∏—Ñ—Ñ—É–∑–∏—è",
                    "#–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ_—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ",
                    "#–∏–Ω–≤–µ—Ä—Å–∏—è_–º–æ–¥–µ–ª–µ–π"
                ],
                "categories": [
                    "#nlp",
                    "#cv",
                    "#editing",
                    "#diffusion"
                ],
                "emoji": "üéõÔ∏è",
                "title": "DICE: –¢–æ—á–Ω–∞—è –∏–Ω–≤–µ—Ä—Å–∏—è –¥–ª—è –≥–∏–±–∫–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07303",
            "title": "Rectified Diffusion: Straightness Is Not Your Need in Rectified Flow",
            "url": "https://huggingface.co/papers/2410.07303",
            "abstract": "Diffusion models have greatly improved visual generation but are hindered by slow generation speed due to the computationally intensive nature of solving generative ODEs. Rectified flow, a widely recognized solution, improves generation speed by straightening the ODE path. Its key components include: 1) using the diffusion form of flow-matching, 2) employing boldsymbol v-prediction, and 3) performing rectification (a.k.a. reflow). In this paper, we argue that the success of rectification primarily lies in using a pretrained diffusion model to obtain matched pairs of noise and samples, followed by retraining with these matched noise-sample pairs. Based on this, components 1) and 2) are unnecessary. Furthermore, we highlight that straightness is not an essential training target for rectification; rather, it is a specific case of flow-matching models. The more critical training target is to achieve a first-order approximate ODE path, which is inherently curved for models like DDPM and Sub-VP. Building on this insight, we propose Rectified Diffusion, which generalizes the design space and application scope of rectification to encompass the broader category of diffusion models, rather than being restricted to flow-matching models. We validate our method on Stable Diffusion v1-5 and Stable Diffusion XL. Our method not only greatly simplifies the training procedure of rectified flow-based previous works (e.g., InstaFlow) but also achieves superior performance with even lower training cost. Our code is available at https://github.com/G-U-N/Rectified-Diffusion.",
            "score": 11,
            "issue_id": 51,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Rectified Diffusion, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ —É—Å–ø–µ—Ö —Ä–µ–∫—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –ø–∞—Ä —à—É–º–∞ –∏ –æ–±—Ä–∞–∑—Ü–æ–≤. –ú–µ—Ç–æ–¥ –æ–±–æ–±—â–∞–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –æ–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ä–µ–∫—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ –±–æ–ª–µ–µ —à–∏—Ä–æ–∫—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. Rectified Diffusion —É–ø—Ä–æ—â–∞–µ—Ç –ø—Ä–æ—Ü–µ–¥—É—Ä—É –æ–±—É—á–µ–Ω–∏—è –∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –º–µ–Ω—å—à–∏—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ.",
                "tags": [
                    "#RectifiedDiffusion",
                    "#–ì–µ–Ω–µ—Ä–∞—Ü–∏—è–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
                    "#–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–û–î–£"
                ],
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#optimization",
                    "#code"
                ],
                "emoji": "üñºÔ∏è",
                "title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ —Ä–µ–∫—Ç–∏—Ñ–∏–∫–∞—Ü–∏—é"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.04751",
            "title": "Intriguing Properties of Large Language and Vision Models",
            "url": "https://huggingface.co/papers/2410.04751",
            "abstract": "Recently, large language and vision models (LLVMs) have received significant attention and development efforts due to their remarkable generalization performance across a wide range of tasks requiring perception and cognitive abilities. A key factor behind their success is their simple architecture, which consists of a vision encoder, a projector, and a large language model (LLM). Despite their achievements in advanced reasoning tasks, their performance on fundamental perception-related tasks (e.g., MMVP) remains surprisingly low. This discrepancy raises the question of how LLVMs truly perceive images and exploit the advantages of the vision encoder. To address this, we systematically investigate this question regarding several aspects: permutation invariance, robustness, math reasoning, alignment preserving and importance, by evaluating the most common LLVM's families (i.e., LLaVA) across 10 evaluation benchmarks. Our extensive experiments reveal several intriguing properties of current LLVMs: (1) they internally process the image in a global manner, even when the order of visual patch sequences is randomly permuted; (2) they are sometimes able to solve math problems without fully perceiving detailed numerical information; (3) the cross-modal alignment is overfitted to complex reasoning tasks, thereby, causing them to lose some of the original perceptual capabilities of their vision encoder; (4) the representation space in the lower layers (<25%) plays a crucial role in determining performance and enhancing visual understanding. Lastly, based on the above observations, we suggest potential future directions for building better LLVMs and constructing more challenging evaluation benchmarks.",
            "score": 10,
            "issue_id": 52,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è –∫—Ä—É–ø–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (LLVM) —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∏—Ö –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ —Ä–∞–±–æ—Ç—ã LLVM, –≤–∫–ª—é—á–∞—è –∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫ –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∞–º, —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å, –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ LLVM –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ, –º–æ–≥—É—Ç —Ä–µ—à–∞—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è —á–∏—Å–ª–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∏ —á—Ç–æ –Ω–∏–∂–Ω–∏–µ —Å–ª–æ–∏ –º–æ–¥–µ–ª–∏ –∏–≥—Ä–∞—é—Ç –∫–ª—é—á–µ–≤—É—é —Ä–æ–ª—å –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ù–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç—Å—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è LLVM –∏ —Å–æ–∑–¥–∞–Ω–∏—è –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–æ–≤.",
                "tags": [
                    "#LLVM",
                    "#–í–æ—Å–ø—Ä–∏—è—Ç–∏–µ–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
                    "#–ê–Ω–∞–ª–∏–∑–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã"
                ],
                "categories": [
                    "#cv",
                    "#nlp",
                    "#benchmark",
                    "#multimodal",
                    "#analysis"
                ],
                "emoji": "üîç",
                "title": "–†–∞—Å–∫—Ä—ã–≤–∞—è —Ç–∞–π–Ω—ã –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤–æ-–≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.08151",
            "title": "Progressive Autoregressive Video Diffusion Models",
            "url": "https://huggingface.co/papers/2410.08151",
            "abstract": "Current frontier video diffusion models have demonstrated remarkable results at generating high-quality videos. However, they can only generate short video clips, normally around 10 seconds or 240 frames, due to computation limitations during training. In this work, we show that existing models can be naturally extended to autoregressive video diffusion models without changing the architectures. Our key idea is to assign the latent frames with progressively increasing noise levels rather than a single noise level, which allows for fine-grained condition among the latents and large overlaps between the attention windows. Such progressive video denoising allows our models to autoregressively generate video frames without quality degradation or abrupt scene changes. We present state-of-the-art results on long video generation at 1 minute (1440 frames at 24 FPS). Videos from this paper are available at https://desaixie.github.io/pa-vdm/.",
            "score": 9,
            "issue_id": 51,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞—é—â–∏–µ—Å—è —É—Ä–æ–≤–Ω–∏ —à—É–º–∞ –∫ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–º –∫–∞–¥—Ä–∞–º, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –±–æ–ª–µ–µ —Ç–æ–Ω–∫–∏–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É –Ω–∏–º–∏. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∏–¥–µ–æ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –¥–æ 1 –º–∏–Ω—É—Ç—ã (1440 –∫–∞–¥—Ä–æ–≤ –ø—Ä–∏ 24 FPS) –±–µ–∑ —É—Ö—É–¥—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏–ª–∏ —Ä–µ–∑–∫–∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å—Ü–µ–Ω—ã. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø–µ—Ä–µ–¥–æ–≤–æ–π —É—Ä–æ–≤–µ–Ω—å –≤ –æ–±–ª–∞—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ.",
                "tags": [
                    "#VideoGeneration",
                    "#DiffusionModels",
                    "#LongVideoSynthesis"
                ],
                "categories": [
                    "#cv",
                    "#video",
                    "#diffusion"
                ],
                "emoji": "üé¨",
                "title": "–ü—Ä–æ—Ä—ã–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ: –º–∏–Ω—É—Ç–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05248",
            "title": "SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe",
            "url": "https://huggingface.co/papers/2410.05248",
            "abstract": "To induce desired behaviors in large language models (LLMs) for interaction-driven tasks, the instruction-tuning stage typically trains LLMs on instruction-response pairs using the next-token prediction (NTP) loss. Previous work aiming to improve instruction-tuning performance often emphasizes the need for higher-quality supervised fine-tuning (SFT) datasets, which typically involves expensive data filtering with proprietary LLMs or labor-intensive data generation by human annotators. However, these approaches do not fully leverage the datasets' intrinsic properties, resulting in high computational and labor costs, thereby limiting scalability and performance gains. In this paper, we propose SFTMix, a novel recipe that elevates instruction-tuning performance beyond the conventional NTP paradigm, without the need for well-curated datasets. Observing that LLMs exhibit uneven confidence across the semantic representation space, we argue that examples with different confidence levels should play distinct roles during the instruction-tuning process. Based on this insight, SFTMix leverages training dynamics to identify examples with varying confidence levels, then applies a Mixup-based regularization to mitigate overfitting on confident examples while propagating supervision signals to improve learning on relatively unconfident ones. This approach enables SFTMix to significantly outperform NTP across a wide range of instruction-following and healthcare domain-specific SFT tasks, demonstrating its adaptability to diverse LLM families and scalability to datasets of any size. Comprehensive ablation studies further verify the robustness of SFTMix's design choices, underscoring its versatility in consistently enhancing performance across different LLMs and datasets in broader natural language processing applications.",
            "score": 8,
            "issue_id": 52,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "SFTMix - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏–Ω–∞–º–∏–∫—É –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–∏–º–µ—Ä–æ–≤ —Å —Ä–∞–∑–Ω—ã–º–∏ —É—Ä–æ–≤–Ω—è–º–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –ó–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ Mixup –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ —É–≤–µ—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö –∏ —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –º–µ–Ω–µ–µ —É–≤–µ—Ä–µ–Ω–Ω—ã—Ö. SFTMix –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö.",
                "tags": [
                    "#instructionTuning",
                    "#SFTMix",
                    "#mixupRegularization"
                ],
                "categories": [
                    "#nlp",
                    "#dataset",
                    "#benchmark",
                    "#code",
                    "#rag"
                ],
                "emoji": "üîÄ",
                "title": "SFTMix: —É–ª—É—á—à–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LLM –±–µ–∑ –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–µ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.06154",
            "title": "GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models",
            "url": "https://huggingface.co/papers/2410.06154",
            "abstract": "In this work, we propose a novel method (GLOV) enabling Large Language Models (LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to enhance downstream vision tasks. Our GLOV meta-prompts an LLM with the downstream task description, querying it for suitable VLM prompts (e.g., for zero-shot classification with CLIP). These prompts are ranked according to a purity measure obtained through a fitness function. In each respective optimization step, the ranked prompts are fed as in-context examples (with their accuracies) to equip the LLM with the knowledge of the type of text prompts preferred by the downstream VLM. Furthermore, we also explicitly steer the LLM generation process in each optimization step by specifically adding an offset difference vector of the embeddings from the positive and negative solutions found by the LLM, in previous optimization steps, to the intermediate layer of the network for the next generation step. This offset vector steers the LLM generation toward the type of language preferred by the downstream VLM, resulting in enhanced performance on the downstream vision tasks. We comprehensively evaluate our GLOV on 16 diverse datasets using two families of VLMs, i.e., dual-encoder (e.g., CLIP) and encoder-decoder (e.g., LLaVa) models -- showing that the discovered solutions can enhance the recognition performance by up to 15.0% and 57.5% (3.8% and 21.6% on average) for these models.",
            "score": 7,
            "issue_id": 54,
            "pub_date": "2024-10-08",
            "pub_date_ru": "8 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ GLOV, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –≤ –∫–∞—á–µ—Å—Ç–≤–µ –Ω–µ—è–≤–Ω—ã—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VLM) —Å —Ü–µ–ª—å—é —É–ª—É—á—à–µ–Ω–∏—è –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è. GLOV –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–∞-–ø—Ä–æ–º–ø—Ç—ã –¥–ª—è LLM —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º —Ü–µ–ª–µ–≤–æ–π –∑–∞–¥–∞—á–∏, –∑–∞–ø—Ä–∞—à–∏–≤–∞—è –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è VLM. –≠—Ç–∏ –ø—Ä–æ–º–ø—Ç—ã —Ä–∞–Ω–∂–∏—Ä—É—é—Ç—Å—è –ø–æ –º–µ—Ä–µ —á–∏—Å—Ç–æ—Ç—ã, –ø–æ–ª—É—á–µ–Ω–Ω–æ–π —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ –ø—Ä–∏–≥–æ–¥–Ω–æ—Å—Ç–∏. –ú–µ—Ç–æ–¥ —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞–µ—Ç —è–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ LLM –ø—É—Ç–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–∞ —Å–º–µ—â–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–≥–æ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–µ—à–µ–Ω–∏—è—Ö.",
                "tags": [
                    "#GLOV",
                    "#–º–µ—Ç–∞–ø—Ä–æ–º–ø—Ç—ã",
                    "#–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—èVLM"
                ],
                "categories": [
                    "#nlp",
                    "#cv",
                    "#prompts",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "üîÆ",
                "title": "GLOV: LLM –∫–∞–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.06508",
            "title": "Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning",
            "url": "https://huggingface.co/papers/2410.06508",
            "abstract": "Monte Carlo Tree Search (MCTS) has recently emerged as a powerful technique for enhancing the reasoning capabilities of LLMs. Techniques such as SFT or DPO have enabled LLMs to distill high-quality behaviors from MCTS, improving their reasoning performance. However, existing distillation methods underutilize the rich trajectory information generated by MCTS, limiting the potential for improvements in LLM reasoning. In this paper, we propose AlphaLLM-CPL, a novel pairwise training framework that enables LLMs to self-improve through MCTS behavior distillation. AlphaLLM-CPL efficiently leverages MCTS trajectories via two key innovations: (1) AlphaLLM-CPL constructs stepwise trajectory pairs from child nodes sharing the same parent in the search tree, providing step-level information for more effective MCTS behavior distillation. (2) AlphaLLM-CPL introduces curriculum preference learning, dynamically adjusting the training sequence of trajectory pairs in each offline training epoch to prioritize critical learning steps and mitigate overfitting. Experimental results on mathematical reasoning tasks demonstrate that AlphaLLM-CPL significantly outperforms previous MCTS behavior distillation methods, substantially boosting the reasoning capabilities of LLMs.",
            "score": 7,
            "issue_id": 51,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "AlphaLLM-CPL - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–æ–∏—Å–∫–∞ –ø–æ –¥–µ—Ä–µ–≤—É –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ (MCTS). –û–Ω —É–ª—É—á—à–∞–µ—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è LLM, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏–∑–≤–ª–µ–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π MCTS. AlphaLLM-CPL –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–ø–∞—Ä–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —É–∑–ª–æ–≤ –¥–µ—Ä–µ–≤–∞ –ø–æ–∏—Å–∫–∞ –∏ –∫—É—Ä—Ä–∏–∫—É–ª—è—Ä–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π LLM –∫ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.",
                "tags": [
                    "#MCTS",
                    "#AlphaLLM",
                    "#CurriculumLearning"
                ],
                "categories": [
                    "#nlp",
                    "#rl",
                    "#reasoning",
                    "#distillation"
                ],
                "emoji": "üß†",
                "title": "AlphaLLM-CPL: –£—Å–∏–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM —á–µ—Ä–µ–∑ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é MCTS"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07041",
            "title": "Emergent properties with repeated examples",
            "url": "https://huggingface.co/papers/2410.07041",
            "abstract": "We study the performance of transformers as a function of the number of repetitions of training examples with algorithmically generated datasets. On three problems of mathematics: the greatest common divisor, modular multiplication, and matrix eigenvalues, we show that for a fixed number of training steps, models trained on smaller sets of repeated examples outperform models trained on larger sets of single-use examples. We also demonstrate that two-set training - repeated use of a small random subset of examples, along normal sampling on the rest of the training set - provides for faster learning and better performance. This highlights that the benefits of repetition can outweigh those of data diversity. These datasets and problems provide a controlled setting to shed light on the still poorly understood interplay between generalization and memorization in deep learning.",
            "score": 6,
            "issue_id": 50,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –ª—É—á—à–µ –æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ –º–µ–Ω—å—à–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–º–∏—Å—è –ø—Ä–∏–º–µ—Ä–∞–º–∏, —á–µ–º –Ω–∞ –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å –Ω–∞ —Ç—Ä–µ—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö: –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –Ω–∞–∏–±–æ–ª—å—à–µ–≥–æ –æ–±—â–µ–≥–æ –¥–µ–ª–∏—Ç–µ–ª—è, –º–æ–¥—É–ª—å–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ –∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Å–æ—á–µ—Ç–∞—é—â–µ–≥–æ –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–∞–ª–æ–≥–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –æ–±—ã—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–æ–π –∏–∑ –æ—Å—Ç–∞–ª—å–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –º–æ–≥—É—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç—å –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –≥–ª—É–±–æ–∫–æ–º –æ–±—É—á–µ–Ω–∏–∏.",
                "tags": [
                    "#–ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ–¥–∞–Ω–Ω—ã—Ö",
                    "#–∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–µ–Ω–∞–±–æ—Ä—ã–¥–∞–Ω–Ω—ã—Ö",
                    "#–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ–∑–∞–¥–∞—á–∏"
                ],
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#math",
                    "#transformers"
                ],
                "emoji": "üîÅ",
                "title": "–ü–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ - –º–∞—Ç—å —É—á–µ–Ω–∏—è –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05210",
            "title": "Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving Vision-Linguistic Compositionality",
            "url": "https://huggingface.co/papers/2410.05210",
            "abstract": "In this paper, we propose a new method to enhance compositional understanding in pre-trained vision and language models (VLMs) without sacrificing performance in zero-shot multi-modal tasks. Traditional fine-tuning approaches often improve compositional reasoning at the cost of degrading multi-modal capabilities, primarily due to the use of global hard negative (HN) loss, which contrasts global representations of images and texts. This global HN loss pushes HN texts that are highly similar to the original ones, damaging the model's multi-modal representations. To overcome this limitation, we propose Fine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard negative loss and selective calibrated regularization. These innovations provide fine-grained negative supervision while preserving the model's representational integrity. Our extensive evaluations across diverse benchmarks for both compositionality and multi-modal tasks show that FSC-CLIP not only achieves compositionality on par with state-of-the-art models but also retains strong multi-modal capabilities. Code is available at: https://github.com/ytaek-oh/fsc-clip.",
            "score": 6,
            "issue_id": 50,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ (VLM) –±–µ–∑ —É—â–µ—Ä–±–∞ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç Fine-grained Selective Calibrated CLIP (FSC-CLIP), –∫–æ—Ç–æ—Ä—ã–π –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –ª–æ–∫–∞–ª—å–Ω—É—é –ø–æ—Ç–µ—Ä—é –∂–µ—Å—Ç–∫–∏—Ö –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ —Å–µ–ª–µ–∫—Ç–∏–≤–Ω—É—é –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—É—é —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é. FSC-CLIP –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π –Ω–∞–¥–∑–æ—Ä, —Å–æ—Ö—Ä–∞–Ω—è—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –º–æ–¥–µ–ª–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ FSC-CLIP –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º —Å–∏–ª—å–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏.",
                "tags": [
                    "#CLIP",
                    "#compositionality",
                    "#fine-grained-learning"
                ],
                "categories": [
                    "#nlp",
                    "#cv",
                    "#multimodal",
                    "#code",
                    "#benchmark"
                ],
                "emoji": "üß©",
                "title": "–£–ª—É—á—à–µ–Ω–∏–µ –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05603",
            "title": "Everything Everywhere All at Once: LLMs can In-Context Learn Multiple Tasks in Superposition",
            "url": "https://huggingface.co/papers/2410.05603",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable in-context learning (ICL) capabilities. In this study, we explore a surprising phenomenon related to ICL: LLMs can perform multiple, computationally distinct ICL tasks simultaneously, during a single inference call, a capability we term \"task superposition\". We provide empirical evidence of this phenomenon across various LLM families and scales and show that this phenomenon emerges even if we train the model to in-context learn one task at a time. We offer theoretical explanations that this capability is well within the expressive power of transformers. We also explore how LLMs internally compose task vectors during superposition. Furthermore, we show that larger models can solve more ICL tasks in parallel, and better calibrate their output distribution. Our findings offer insights into the latent capabilities of LLMs, further substantiate the perspective of \"LLMs as superposition of simulators\", and raise questions about the mechanisms enabling simultaneous task execution.",
            "score": 5,
            "issue_id": 57,
            "pub_date": "2024-10-08",
            "pub_date_ru": "8 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤—ã–ø–æ–ª–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ, –Ω–∞–∑–≤–∞–Ω–Ω—É—é '—Å—É–ø–µ—Ä–ø–æ–∑–∏—Ü–∏–µ–π –∑–∞–¥–∞—á'. –≠—Ç–æ —è–≤–ª–µ–Ω–∏–µ –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–µ–º–µ–π—Å—Ç–≤–∞—Ö LLM –∏ –º–∞—Å—à—Ç–∞–±–∞—Ö, –¥–∞–∂–µ –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á–∏ –ø–æ –æ–¥–Ω–æ–π. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ —ç—Ç–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ–π –º–æ—â–Ω–æ—Å—Ç–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç, –∫–∞–∫ LLM –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ —Å–æ—Å—Ç–∞–≤–ª—è—é—Ç –≤–µ–∫—Ç–æ—Ä—ã –∑–∞–¥–∞—á –≤–æ –≤—Ä–µ–º—è —Å—É–ø–µ—Ä–ø–æ–∑–∏—Ü–∏–∏.",
                "tags": [
                    "#—Å—É–ø–µ—Ä–ø–æ–∑–∏—Ü–∏—è_–∑–∞–¥–∞—á",
                    "#–æ–±—É—á–µ–Ω–∏–µ_–≤_–∫–æ–Ω—Ç–µ–∫—Å—Ç–µ",
                    "#–≤–µ–∫—Ç–æ—Ä—ã_–∑–∞–¥–∞—á"
                ],
                "categories": [
                    "#nlp",
                    "#inference",
                    "#transformers",
                    "#in_context_learning"
                ],
                "emoji": "üß†",
                "title": "–°—É–ø–µ—Ä–ø–æ–∑–∏—Ü–∏—è –∑–∞–¥–∞—á: —Å–∫—Ä—ã—Ç–∞—è —Å–∏–ª–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07137",
            "title": "Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates",
            "url": "https://huggingface.co/papers/2410.07137",
            "abstract": "Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and MT-Bench, have become popular for evaluating language models due to their cost-effectiveness and scalability compared to human evaluation. Achieving high win rates on these benchmarks can significantly boost the promotional impact of newly released language models. This promotional benefit may motivate tricks, such as manipulating model output length or style to game win rates, even though several mechanisms have been developed to control length and disentangle style to reduce gameability. Nonetheless, we show that even a \"null model\" that always outputs a constant response (irrelevant to input instructions) can cheat automatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on AlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench. Moreover, the crafted cheating outputs are transferable because we assume that the instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are private and cannot be accessed. While our experiments are primarily proof-of-concept, an adversary could use LLMs to generate more imperceptible cheating responses, unethically benefiting from high win rates and promotional impact. Our findings call for the development of anti-cheating mechanisms for reliable automatic benchmarks. The code is available at https://github.com/sail-sg/Cheating-LLM-Benchmarks.",
            "score": 5,
            "issue_id": 56,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –æ–±–º–∞–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, –∫–∞–∫ –¥–∞–∂–µ '–Ω—É–ª–µ–≤–∞—è –º–æ–¥–µ–ª—å', –≤—Å–µ–≥–¥–∞ –≤—ã–¥–∞—é—â–∞—è –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ –æ—Ç–≤–µ—Ç, –º–æ–∂–µ—Ç –¥–æ—Å—Ç–∏—á—å –≤—ã—Å–æ–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ç–∞–∫–∏–µ —Ç—Ä—é–∫–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –¥–ª—è –Ω–µ—ç—Ç–∏—á–Ω–æ–≥–æ –ø–æ–≤—ã—à–µ–Ω–∏—è —Ä–µ–π—Ç–∏–Ω–≥–∞ –º–æ–¥–µ–ª–µ–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –∑–∞—â–∏—Ç—ã –æ—Ç –æ–±–º–∞–Ω–∞ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤.",
                "tags": [
                    "#–æ–±–º–∞–Ω_–±–µ–Ω—á–º–∞—Ä–∫–æ–≤",
                    "#–æ—Ü–µ–Ω–∫–∞_—è–∑—ã–∫–æ–≤—ã—Ö_–º–æ–¥–µ–ª–µ–π",
                    "#–Ω—É–ª–µ–≤–∞—è_–º–æ–¥–µ–ª—å"
                ],
                "categories": [
                    "#nlp",
                    "#benchmark",
                    "#dataset",
                    "#code",
                    "#rag"
                ],
                "emoji": "üïµÔ∏è",
                "title": "–û—Å—Ç–æ—Ä–æ–∂–Ω–æ: –¥–∞–∂–µ '–Ω—É–ª–µ–≤–∞—è –º–æ–¥–µ–ª—å' –º–æ–∂–µ—Ç –æ–±–º–∞–Ω—É—Ç—å –±–µ–Ω—á–º–∞—Ä–∫–∏ –Ø–ú!"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.08115",
            "title": "Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System",
            "url": "https://huggingface.co/papers/2410.08115",
            "abstract": "Large Language Model (LLM) based multi-agent systems (MAS) show remarkable potential in collaborative problem-solving, yet they still face critical challenges: low communication efficiency, poor scalability, and a lack of effective parameter-updating optimization methods. We present Optima, a novel framework that addresses these issues by significantly enhancing both communication efficiency and task effectiveness in LLM-based MAS through LLM training. Optima employs an iterative generate, rank, select, and train paradigm with a reward function balancing task performance, token efficiency, and communication readability. We explore various RL algorithms, including Supervised Fine-Tuning, Direct Preference Optimization, and their hybrid approaches, providing insights into their effectiveness-efficiency trade-offs. We integrate Monte Carlo Tree Search-inspired techniques for DPO data generation, treating conversation turns as tree nodes to explore diverse interaction paths. Evaluated on common multi-agent tasks, including information-asymmetric question answering and complex reasoning, Optima shows consistent and substantial improvements over single-agent baselines and vanilla MAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than 10\\% tokens on tasks requiring heavy information exchange. Moreover, Optima's efficiency gains open new possibilities for leveraging inference-compute more effectively, leading to improved inference-time scaling laws. By addressing fundamental challenges in LLM-based MAS, Optima shows the potential towards scalable, efficient, and effective MAS (https://chenweize1998.github.io/optima-project-page).",
            "score": 5,
            "issue_id": 50,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "Optima - –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞, —É–ª—É—á—à–∞—é—â–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –∏ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è, –æ—Ç–±–æ—Ä–∞ –∏ –æ–±—É—á–µ–Ω–∏—è —Å —Ñ—É–Ω–∫—Ü–∏–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –±–∞–ª–∞–Ω—Å–∏—Ä—É—é—â–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —Ç–æ–∫–µ–Ω-—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ —á–∏—Ç–∞–µ–º–æ—Å—Ç—å. –°–∏—Å—Ç–µ–º–∞ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç —Ç–µ—Ö–Ω–∏–∫–∏, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ Monte Carlo Tree Search, –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è. Optima –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ–¥–Ω–æ–∞–≥–µ–Ω—Ç–Ω—ã–º–∏ –±–∞–∑–æ–≤—ã–º–∏ –ª–∏–Ω–∏—è–º–∏ –∏ –æ–±—ã—á–Ω—ã–º–∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ Llama 3 8B.",
                "tags": [
                    "#–º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã–µ_—Å–∏—Å—Ç–µ–º—ã",
                    "#–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è_–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏",
                    "#–º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å_LLM"
                ],
                "categories": [
                    "#nlp",
                    "#rl",
                    "#benchmark",
                    "#code",
                    "#rag"
                ],
                "emoji": "ü§ñ",
                "title": "Optima: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.08049",
            "title": "Scaling Up Your Kernels: Large Kernel Design in ConvNets towards Universal Representations",
            "url": "https://huggingface.co/papers/2410.08049",
            "abstract": "This paper proposes the paradigm of large convolutional kernels in designing modern Convolutional Neural Networks (ConvNets). We establish that employing a few large kernels, instead of stacking multiple smaller ones, can be a superior design strategy. Our work introduces a set of architecture design guidelines for large-kernel ConvNets that optimize their efficiency and performance. We propose the UniRepLKNet architecture, which offers systematical architecture design principles specifically crafted for large-kernel ConvNets, emphasizing their unique ability to capture extensive spatial information without deep layer stacking. This results in a model that not only surpasses its predecessors with an ImageNet accuracy of 88.0%, an ADE20K mIoU of 55.6%, and a COCO box AP of 56.4% but also demonstrates impressive scalability and performance on various modalities such as time-series forecasting, audio, point cloud, and video recognition. These results indicate the universal modeling abilities of large-kernel ConvNets with faster inference speed compared with vision transformers. Our findings reveal that large-kernel ConvNets possess larger effective receptive fields and a higher shape bias, moving away from the texture bias typical of smaller-kernel CNNs. All codes and models are publicly available at https://github.com/AILab-CVC/UniRepLKNet promoting further research and development in the community.",
            "score": 4,
            "issue_id": 51,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —è–¥–µ—Ä –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö (ConvNets). –ê–≤—Ç–æ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–æ–ª—å—à–∏—Ö —è–¥–µ—Ä –≤–º–µ—Å—Ç–æ —Å—Ç–µ–∫–∞ –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –º–µ–ª–∫–∏—Ö –º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –û–Ω–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É UniRepLKNet, –∫–æ—Ç–æ—Ä–∞—è –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è ConvNets —Å –±–æ–ª—å—à–∏–º–∏ —è–¥—Ä–∞–º–∏, –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—è –∏—Ö —É–Ω–∏–∫–∞–ª—å–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—Ç—å –æ–±—à–∏—Ä–Ω—É—é –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –±–µ–∑ –≥–ª—É–±–æ–∫–æ–≥–æ –Ω–∞—Å–ª–æ–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ —ç—Ç–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –¥—Ä—É–≥–∏—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è—Ö, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è ConvNets —Å –±–æ–ª—å—à–∏–º–∏ —è–¥—Ä–∞–º–∏ –ø—Ä–∏ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏ –≤—ã–≤–æ–¥–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å vision transformers.",
                "tags": [
                    "#LargeConvolutionalKernels",
                    "#UniRepLKNet",
                    "#EffectiveReceptiveField"
                ],
                "categories": [
                    "#cv",
                    "#architecture",
                    "#architecture",
                    "#benchmark",
                    "#code"
                ],
                "emoji": "üîç",
                "title": "–ë–æ–ª—å—à–∏–µ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —è–¥—Ä–∞ - –∫–ª—é—á –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º ConvNets"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.06293",
            "title": "Accelerated Preference Optimization for Large Language Model Alignment",
            "url": "https://huggingface.co/papers/2410.06293",
            "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal tool for aligning large language models (LLMs) with human preferences. Direct Preference Optimization (DPO), one of the most popular approaches, formulates RLHF as a policy optimization problem without explicitly estimating the reward function. It overcomes the stability and efficiency issues of two-step approaches, which typically involve first estimating the reward function and then optimizing the policy via proximal policy optimization (PPO). Since RLHF is essentially an optimization problem, and it is well-known that momentum techniques can accelerate optimization both theoretically and empirically, a natural question arises: Can RLHF be accelerated by momentum? This paper answers this question in the affirmative. In detail, we first show that the iterative preference optimization method can be viewed as a proximal point method. Based on this observation, we propose a general Accelerated Preference Optimization (APO) framework, which unifies many existing preference optimization algorithms and employs Nesterov's momentum technique to speed up the alignment of LLMs. Theoretically, we demonstrate that APO can achieve a faster convergence rate than the standard iterative preference optimization methods, including DPO and Self-Play Preference Optimization (SPPO). Empirically, we show the superiority of APO over DPO, iterative DPO, and other strong baselines for RLHF on the AlpacaEval 2.0 benchmark.",
            "score": 2,
            "issue_id": 61,
            "pub_date": "2024-10-08",
            "pub_date_ru": "8 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —É—á–µ—Ç–æ–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π - Accelerated Preference Optimization (APO). APO –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Ç–µ—Ö–Ω–∏–∫–µ –º–æ–º–µ–Ω—Ç–∞ –ù–µ—Å—Ç–µ—Ä–æ–≤–∞ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –∏ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏ –ø–æ–∫–∞–∑–∞–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ APO –Ω–∞–¥ —Ç–∞–∫–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –∫–∞–∫ DPO –∏ SPPO. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ AlpacaEval 2.0 –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞.",
                "tags": [
                    "#RLHF",
                    "#APO",
                    "#–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è_–ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π"
                ],
                "categories": [
                    "#nlp",
                    "#rl",
                    "#benchmark",
                    "#code",
                    "#algo"
                ],
                "emoji": "üöÄ",
                "title": "APO: –£—Å–∫–æ—Ä–µ–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –¥–ª—è RLHF"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.04808",
            "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
            "url": "https://huggingface.co/papers/2410.04808",
            "abstract": "In spite of the outstanding performance, Neural Architecture Search (NAS) is criticized for massive computation. Recently, Zero-shot NAS has emerged as a promising approach by exploiting Zero-cost (ZC) proxies, which markedly reduce computational demands. Despite this, existing ZC proxies heavily rely on expert knowledge and incur significant trial-and-error costs. Particularly in NLP tasks, most existing ZC proxies fail to surpass the performance of the naive baseline. To address these challenges, we introduce a novel framework, LPZero, which is the first to automatically design ZC proxies for various tasks, achieving higher ranking consistency than human-designed proxies. Specifically, we model the ZC proxy as a symbolic equation and incorporate a unified proxy search space that encompasses existing ZC proxies, which are composed of a predefined set of mathematical symbols. To heuristically search for the best ZC proxy, LPZero incorporates genetic programming to find the optimal symbolic composition. We propose a Rule-based Pruning Strategy (RPS), which preemptively eliminates unpromising proxies, thereby mitigating the risk of proxy degradation. Extensive experiments on FlexiBERT, GPT-2, and LLaMA-7B demonstrate LPZero's superior ranking ability and performance on downstream tasks compared to current approaches.",
            "score": 2,
            "issue_id": 56,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "LPZero - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∫—Å–∏ —Å –Ω—É–ª–µ–≤–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç—å—é (ZC) –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –Ω–µ–π—Ä–æ–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (NAS). –û–Ω –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç ZC-–ø—Ä–æ–∫—Å–∏ –∫–∞–∫ —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–æ–µ —É—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏ –≤–∫–ª—é—á–∞–µ—Ç –µ–¥–∏–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ –ø—Ä–æ–∫—Å–∏. LPZero –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–µ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø—Ä–∞–≤–∏–ª-–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–π –æ–±—Ä–µ–∑–∫–∏ (RPS) –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –±–µ—Å–ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ FlexiBERT, GPT-2 –∏ LLaMA-7B –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å LPZero –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏.",
                "tags": [
                    "#–Ω–µ–π—Ä–æ–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π_–ø–æ–∏—Å–∫",
                    "#—Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–æ–µ_—É—Ä–∞–≤–Ω–µ–Ω–∏–µ",
                    "#–≥–µ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–µ_–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ"
                ],
                "categories": [
                    "#nlp",
                    "#benchmark",
                    "#code",
                    "#rag",
                    "#dataset"
                ],
                "emoji": "üß¨",
                "title": "LPZero: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∫—Å–∏ —Å –Ω—É–ª–µ–≤–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç—å—é –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –Ω–µ–π—Ä–æ–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07707",
            "title": "MotionGS: Exploring Explicit Motion Guidance for Deformable 3D Gaussian Splatting",
            "url": "https://huggingface.co/papers/2410.07707",
            "abstract": "Dynamic scene reconstruction is a long-term challenge in the field of 3D vision. Recently, the emergence of 3D Gaussian Splatting has provided new insights into this problem. Although subsequent efforts rapidly extend static 3D Gaussian to dynamic scenes, they often lack explicit constraints on object motion, leading to optimization difficulties and performance degradation. To address the above issues, we propose a novel deformable 3D Gaussian splatting framework called MotionGS, which explores explicit motion priors to guide the deformation of 3D Gaussians. Specifically, we first introduce an optical flow decoupling module that decouples optical flow into camera flow and motion flow, corresponding to camera movement and object motion respectively. Then the motion flow can effectively constrain the deformation of 3D Gaussians, thus simulating the motion of dynamic objects. Additionally, a camera pose refinement module is proposed to alternately optimize 3D Gaussians and camera poses, mitigating the impact of inaccurate camera poses. Extensive experiments in the monocular dynamic scenes validate that MotionGS surpasses state-of-the-art methods and exhibits significant superiority in both qualitative and quantitative results. Project page: https://ruijiezhu94.github.io/MotionGS_page",
            "score": 2,
            "issue_id": 53,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "MotionGS - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –¥–µ—Ñ–æ—Ä–º–∏—Ä—É–µ–º–æ–≥–æ 3D-—Å–ø–ª–∞—Ç—Ç–∏–Ω–≥–∞ –≥–∞—É—Å—Å–∏–∞–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —è–≤–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –º–æ–¥—É–ª—å –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –æ–ø—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ—Ç–æ–∫–∞, —Ä–∞–∑–¥–µ–ª—è—é—â–∏–π –µ–≥–æ –Ω–∞ –ø–æ—Ç–æ–∫ –∫–∞–º–µ—Ä—ã –∏ –ø–æ—Ç–æ–∫ –¥–≤–∏–∂–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤. –¢–∞–∫–∂–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–æ–¥—É–ª—å —É—Ç–æ—á–Ω–µ–Ω–∏—è –ø–æ–ª–æ–∂–µ–Ω–∏—è –∫–∞–º–µ—Ä—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ 3D-–≥–∞—É—Å—Å–∏–∞–Ω–æ–≤ –∏ –ø–æ–∑ –∫–∞–º–µ—Ä—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ MotionGS –Ω–∞–¥ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –≤ –∑–∞–¥–∞—á–µ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—ã—Ö –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω.",
                "tags": [
                    "#3DGaussianSplatting",
                    "#DynamicSceneReconstruction",
                    "#OpticalFlowDecoupling"
                ],
                "categories": [
                    "#cv",
                    "#3d",
                    "#video",
                    "#3d",
                    "#reconstruction"
                ],
                "emoji": "üé•",
                "title": "MotionGS: –†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω —Å –ø–æ–º–æ—â—å—é –¥–µ—Ñ–æ—Ä–º–∏—Ä—É–µ–º—ã—Ö 3D-–≥–∞—É—Å—Å–∏–∞–Ω–æ–≤"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05269",
            "title": "Data Advisor: Dynamic Data Curation for Safety Alignment of Large Language Models",
            "url": "https://huggingface.co/papers/2410.05269",
            "abstract": "Data is a crucial element in large language model (LLM) alignment. Recent studies have explored using LLMs for efficient data collection. However, LLM-generated data often suffers from quality issues, with underrepresented or absent aspects and low-quality datapoints. To address these problems, we propose Data Advisor, an enhanced LLM-based method for generating data that takes into account the characteristics of the desired dataset. Starting from a set of pre-defined principles in hand, Data Advisor monitors the status of the generated data, identifies weaknesses in the current dataset, and advises the next iteration of data generation accordingly. Data Advisor can be easily integrated into existing data generation methods to enhance data quality and coverage. Experiments on safety alignment of three representative LLMs (i.e., Mistral, Llama2, and Falcon) demonstrate the effectiveness of Data Advisor in enhancing model safety against various fine-grained safety issues without sacrificing model utility.",
            "score": 1,
            "issue_id": 60,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "Data Advisor - —ç—Ç–æ —É–ª—É—á—à–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –≤—ã—è–≤–ª—è–µ—Ç —Å–ª–∞–±—ã–µ –º–µ—Å—Ç–∞ –≤ —Ç–µ–∫—É—â–µ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö –∏ –¥–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. Data Advisor –ª–µ–≥–∫–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∏—Ö –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –æ—Ö–≤–∞—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å Data Advisor –≤ –ø–æ–≤—ã—à–µ–Ω–∏–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –±–µ–∑ —É—â–µ—Ä–±–∞ –¥–ª—è –∏—Ö –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏.",
                "tags": [
                    "#DataAdvisor",
                    "#LLMDataGeneration",
                    "#ModelSafetyAlignment"
                ],
                "categories": [
                    "#nlp",
                    "#dataset",
                    "#benchmark",
                    "#rag"
                ],
                "emoji": "üß†",
                "title": "Data Advisor: —É–º–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LLM"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05629",
            "title": "Vector-ICL: In-context Learning with Continuous Vector Representations",
            "url": "https://huggingface.co/papers/2410.05629",
            "abstract": "Large language models (LLMs) have shown remarkable in-context learning (ICL) capabilities on textual data. We explore whether these capabilities can be extended to continuous vectors from diverse domains, obtained from black-box pretrained encoders. By aligning input data with an LLM's embedding space through lightweight projectors, we observe that LLMs can effectively process and learn from these projected vectors, which we term Vector-ICL. In particular, we find that pretraining projectors with general language modeling objectives enables Vector-ICL, while task-specific finetuning further enhances performance. In our experiments across various tasks and modalities, including text reconstruction, numerical function regression, text classification, summarization, molecule captioning, time-series classification, graph classification, and fMRI decoding, Vector-ICL often surpasses both few-shot ICL and domain-specific model or tuning. We further conduct analyses and case studies, indicating the potential of LLMs to process vector representations beyond traditional token-based paradigms.",
            "score": 1,
            "issue_id": 58,
            "pub_date": "2024-10-08",
            "pub_date_ru": "8 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –æ–±—É—á–µ–Ω–∏—é –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–∞ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ Vector-ICL, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç LLM —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏ —É—á–∏—Ç—å—Å—è –Ω–∞ –ø—Ä–æ–µ—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–∞—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Vector-ICL —á–∞—Å—Ç–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∫–∞–∫ few-shot ICL, —Ç–∞–∫ –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –∏ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è—Ö. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª LLM –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–¥–∏–≥–º.",
                "tags": [
                    "#Vector-ICL",
                    "#CrossModalLearning",
                    "#EmbeddingAlignment"
                ],
                "categories": [
                    "#nlp",
                    "#cv",
                    "#rl",
                    "#benchmark",
                    "#code"
                ],
                "emoji": "üß†",
                "title": "LLM –æ—Å–≤–∞–∏–≤–∞—é—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –º–∏—Ä: –æ—Ç —Ç–µ–∫—Å—Ç–∞ –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º –¥–∞–Ω–Ω—ã–º"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.03437",
            "title": "Zebra: In-Context and Generative Pretraining for Solving Parametric PDEs",
            "url": "https://huggingface.co/papers/2410.03437",
            "abstract": "Solving time-dependent parametric partial differential equations (PDEs) is challenging, as models must adapt to variations in parameters such as coefficients, forcing terms, and boundary conditions. Data-driven neural solvers either train on data sampled from the PDE parameters distribution in the hope that the model generalizes to new instances or rely on gradient-based adaptation and meta-learning to implicitly encode the dynamics from observations. This often comes with increased inference complexity. Inspired by the in-context learning capabilities of large language models (LLMs), we introduce Zebra, a novel generative auto-regressive transformer designed to solve parametric PDEs without requiring gradient adaptation at inference. By leveraging in-context information during both pre-training and inference, Zebra dynamically adapts to new tasks by conditioning on input sequences that incorporate context trajectories or preceding states. This approach enables Zebra to flexibly handle arbitrarily sized context inputs and supports uncertainty quantification through the sampling of multiple solution trajectories. We evaluate Zebra across a variety of challenging PDE scenarios, demonstrating its adaptability, robustness, and superior performance compared to existing approaches.",
            "score": 1,
            "issue_id": 58,
            "pub_date": "2024-10-04",
            "pub_date_ru": "4 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "Zebra - —ç—Ç–æ –Ω–æ–≤—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —É—Ä–∞–≤–Ω–µ–Ω–∏–π –≤ —á–∞—Å—Ç–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö (–î–£–ß–ü). –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, Zebra –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞, –∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω–æ–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ú–æ–¥–µ–ª—å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ –Ω–æ–≤—ã–º –∑–∞–¥–∞—á–∞–º, –æ–±—É—Å–ª–æ–≤–ª–∏–≤–∞—è –≤—Ö–æ–¥–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –≤–∫–ª—é—á–∞—é—â–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∏–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è. Zebra –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –î–£–ß–ü.",
                "tags": [
                    "#–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ_–î–£–ß–ü",
                    "#–æ–±—É—á–µ–Ω–∏–µ_–≤_–∫–æ–Ω—Ç–µ–∫—Å—Ç–µ",
                    "#—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã_–¥–ª—è_–î–£–ß–ü"
                ],
                "categories": [
                    "#nlp",
                    "#rag",
                    "#code",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ü¶ì",
                "title": "Zebra: —Ä–µ—à–µ–Ω–∏–µ –î–£–ß–ü —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ"
            }
        }
    ]
}