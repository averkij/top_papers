{
    "date": {
        "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 11",
        "zh": "12æœˆ11æ—¥"
    },
    "time_utc": "2025-12-11 23:22",
    "weekday": 3,
    "issue_id": 19,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2512.09363",
            "title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation",
            "url": "https://huggingface.co/papers/2512.09363",
            "abstract": "StereoWorld generates high-quality stereo video from monocular input using a pretrained video generator with geometry-aware regularization and spatio-temporal tiling.  \t\t\t\t\tAI-generated summary \t\t\t\t The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.",
            "score": 44,
            "issue_id": 5,
            "pub_date": "2025-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "2cc4deef0344b429",
            "authors": [
                "Ke Xing",
                "Longfei Li",
                "Yuyang Yin",
                "Hanwen Liang",
                "Guixun Luo",
                "Chen Fang",
                "Jue Wang",
                "Konstantinos N. Plataniotis",
                "Xiaojie Jin",
                "Yao Zhao",
                "Yunchao Wei"
            ],
            "affiliations": [
                "Beijing Jiaotong University",
                "Dzine AI",
                "University of Toronto",
                "Visual Intelligence + International Joint Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.09363.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#dataset",
                    "#3d",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ğ»Ğ°Ğ·Ğ° Ğº Ğ´Ğ²ÑƒĞ¼: ÑĞ¸Ğ½Ñ‚ĞµĞ· ÑÑ‚ĞµÑ€ĞµĞ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ",
                    "desc": "StereoWorld â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ»Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑÑ‚ĞµÑ€ĞµĞ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚ĞµÑ€ĞµĞ¾-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹. Ğ”Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ·Ğ°Ğ¸Ñ‡Ğ½Ğ°Ñ ÑÑ…ĞµĞ¼Ğ° Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 11 Ğ¼Ğ»Ğ½ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² ÑÑ‚ĞµÑ€ĞµĞ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑĞ¸Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Transforming Monocular Video into Stunning Stereo Experiences",
                    "desc": "StereoWorld is a novel framework that transforms single-camera (monocular) video into high-quality stereo video. It utilizes a pretrained video generator and incorporates geometry-aware regularization to maintain accurate 3D structures in the output. The method also employs spatio-temporal tiling to enhance the efficiency and resolution of the generated videos. Extensive testing shows that StereoWorld significantly improves visual quality and geometric consistency compared to previous techniques."
                },
                "zh": {
                    "title": "StereoWorldï¼šé«˜è´¨é‡ç«‹ä½“è§†é¢‘ç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "StereoWorld æ˜¯ä¸€ä¸ªå°†å•ç›®è§†é¢‘è½¬æ¢ä¸ºé«˜è´¨é‡ç«‹ä½“è§†é¢‘çš„æ¡†æ¶ã€‚å®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆå™¨ï¼Œå¹¶ç»“åˆå‡ ä½•æ„ŸçŸ¥çš„æ­£åˆ™åŒ–æ¥ç¡®ä¿ä¸‰ç»´ç»“æ„çš„å‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶è¿˜é‡‡ç”¨æ—¶ç©ºå¹³é“ºæ–¹æ¡ˆï¼Œä»¥å®ç°é«˜æ•ˆçš„é«˜åˆ†è¾¨ç‡åˆæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStereoWorld åœ¨è§†è§‰ä¿çœŸåº¦å’Œå‡ ä½•ä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºä¹‹å‰çš„æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.08560",
            "title": "BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain",
            "url": "https://huggingface.co/papers/2512.08560",
            "abstract": "An automated framework uses unsupervised decomposition and natural language descriptions to identify and explain visual representations in human brain fMRI data.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains a long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As a result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported.",
            "score": 30,
            "issue_id": 5,
            "pub_date": "2025-12-09",
            "pub_date_card": {
                "ru": "9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 9",
                "zh": "12æœˆ9æ—¥"
            },
            "hash": "92db147f3e19f7de",
            "authors": [
                "Navve Wasserman",
                "Matias Cosarinsky",
                "Yuval Golbari",
                "Aude Oliva",
                "Antonio Torralba",
                "Tamar Rott Shaham",
                "Michal Irani"
            ],
            "affiliations": [
                "Massachusetts Institute of Technology",
                "Weizmann Institute of Science"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.08560.jpg",
            "data": {
                "categories": [
                    "#interpretability"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ„Ñ€Ğ¾Ğ²ĞºĞ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ğ¼Ğ¾Ğ·Ğ³Ğ° Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹Ğº",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… fMRI Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ·Ğ³Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ², Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½ Ñ‡ĞµÑ€ĞµĞ· ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞµĞ³Ğ¾ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒÑÑ‚. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸ÑĞ²Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹ÑĞ²Ğ¸Ğ» Ñ‚Ñ‹ÑÑÑ‡Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ°Ğ½ĞµĞµ Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ñ€Ğµ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ·Ğ³Ğ°."
                },
                "en": {
                    "title": "Automated Insights into Brain Visual Representations",
                    "desc": "This paper presents an automated framework that utilizes unsupervised decomposition techniques to analyze fMRI data from the human brain. The framework identifies and explains visual representations by discovering interpretable patterns in brain activity and linking them to natural images that evoke these patterns. It employs a systematic approach to validate these findings, assigning reliability scores to ensure the robustness of the explanations. Ultimately, this method uncovers a vast array of visual concepts encoded in the brain, including previously unreported fine-grained representations."
                },
                "zh": {
                    "title": "è‡ªåŠ¨åŒ–æ¡†æ¶æ­ç¤ºäººè„‘è§†è§‰è¡¨å¾çš„å¥¥ç§˜",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œç”¨äºè¯†åˆ«å’Œè§£é‡Šäººè„‘fMRIæ•°æ®ä¸­çš„è§†è§‰è¡¨å¾ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ— ç›‘ç£åˆ†è§£æ–¹æ³•ï¼Œé¦–å…ˆå‘ç°fMRIæ´»åŠ¨ä¸­çš„å¯è§£é‡Šæ¨¡å¼ã€‚æ¥ç€ï¼Œé€šè¿‡è¯†åˆ«æœ€èƒ½å¼•å‘è¿™äº›æ¨¡å¼çš„è‡ªç„¶å›¾åƒï¼Œå¹¶ç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°ï¼Œæ¥è§£é‡Šæ¯ä¸ªæ¨¡å¼ã€‚æœ€ç»ˆï¼Œè¯¥æ–¹æ³•æ­ç¤ºäº†æ•°åƒç§å¯è§£é‡Šçš„æ¨¡å¼ï¼Œæ¶µç›–äº†è®¸å¤šä¸åŒçš„è§†è§‰æ¦‚å¿µï¼ŒåŒ…æ‹¬ä»¥å‰æœªæŠ¥å‘Šçš„ç»†ç²’åº¦è¡¨å¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.09824",
            "title": "Composing Concepts from Images and Videos via Concept-prompt Binding",
            "url": "https://huggingface.co/papers/2512.09824",
            "abstract": "Bind & Compose uses Diffusion Transformers with hierarchical binders and temporal strategies to accurately compose complex visual concepts from images and videos.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.",
            "score": 23,
            "issue_id": 1,
            "pub_date": "2025-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "809a434567c434bc",
            "authors": [
                "Xianghao Kong",
                "Zeyu Zhang",
                "Yuwei Guo",
                "Zhuoran Zhao",
                "Songchun Zhang",
                "Anyi Rao"
            ],
            "affiliations": [
                "CUHK",
                "HKUST",
                "HKUST(GZ)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.09824.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#diffusion",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºÑƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Bind & Compose, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Diffusion Transformers Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Diversify-and-Absorb, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğº Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼, Ğ¸ÑĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ñ…. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ²ÑƒÑ…Ğ²ĞµÑ‚Ğ²Ğ¸ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸."
                },
                "en": {
                    "title": "Revolutionizing Visual Composition with Bind & Compose",
                    "desc": "Bind & Compose is a novel approach that enhances visual concept composition by utilizing Diffusion Transformers with a hierarchical binder structure. This method allows for the effective binding of visual concepts to prompt tokens, enabling the integration of complex elements from both images and videos. To improve accuracy, it incorporates a Diversify-and-Absorb Mechanism that filters out irrelevant details during training. Additionally, a Temporal Disentanglement Strategy is employed to separately model video concepts, resulting in improved consistency and quality in the generated visual outputs."
                },
                "zh": {
                    "title": "çµæ´»ç»„åˆè§†è§‰æ¦‚å¿µçš„æ–°æ–¹æ³•",
                    "desc": "Bind & Compose æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£å˜æ¢å™¨å’Œåˆ†å±‚ç»‘å®šå™¨æ¥å‡†ç¡®åœ°ä»å›¾åƒå’Œè§†é¢‘ä¸­ç»„åˆå¤æ‚çš„è§†è§‰æ¦‚å¿µã€‚è¯¥æ–¹æ³•é€šè¿‡å°†è§†è§‰æ¦‚å¿µä¸ç›¸åº”çš„æç¤ºæ ‡è®°ç»‘å®šï¼Œå¹¶ä»ä¸åŒæ¥æºç»„åˆç›®æ ‡æç¤ºï¼Œå®ç°çµæ´»çš„è§†è§‰æ¦‚å¿µç»„åˆã€‚ä¸ºäº†æé«˜æ¦‚å¿µä¸æ ‡è®°çš„ç»‘å®šå‡†ç¡®æ€§ï¼Œè®¾è®¡äº†ä¸€ç§å¤šæ ·åŒ–ä¸å¸æ”¶æœºåˆ¶ï¼Œä½¿ç”¨é¢å¤–çš„å¸æ”¶æ ‡è®°æ¥æ¶ˆé™¤ä¸æ¦‚å¿µæ— å…³çš„ç»†èŠ‚å½±å“ã€‚æ­¤å¤–ï¼Œæå‡ºäº†æ—¶é—´è§£è€¦ç­–ç•¥ï¼Œå°†è§†é¢‘æ¦‚å¿µçš„è®­ç»ƒè¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œä»¥å¢å¼ºå›¾åƒå’Œè§†é¢‘æ¦‚å¿µä¹‹é—´çš„å…¼å®¹æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.09247",
            "title": "OmniPSD: Layered PSD Generation with Diffusion Transformer",
            "url": "https://huggingface.co/papers/2512.09247",
            "abstract": "OmniPSD, a diffusion framework within the Flux ecosystem, enables text-to-PSD generation and image-to-PSD decomposition, achieving high-fidelity results with transparency awareness.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion models have greatly improved image generation and editing, yet generating or reconstructing layered PSD files with transparent alpha channels remains highly challenging. We propose OmniPSD, a unified diffusion framework built upon the Flux ecosystem that enables both text-to-PSD generation and image-to-PSD decomposition through in-context learning. For text-to-PSD generation, OmniPSD arranges multiple target layers spatially into a single canvas and learns their compositional relationships through spatial attention, producing semantically coherent and hierarchically structured layers. For image-to-PSD decomposition, it performs iterative in-context editing, progressively extracting and erasing textual and foreground components to reconstruct editable PSD layers from a single flattened image. An RGBA-VAE is employed as an auxiliary representation module to preserve transparency without affecting structure learning. Extensive experiments on our new RGBA-layered dataset demonstrate that OmniPSD achieves high-fidelity generation, structural consistency, and transparency awareness, offering a new paradigm for layered design generation and decomposition with diffusion transformers.",
            "score": 20,
            "issue_id": 1,
            "pub_date": "2025-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "cf1c01fa0bd2895a",
            "authors": [
                "Cheng Liu",
                "Yiren Song",
                "Haofan Wang",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Lovart AI",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.09247.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#multimodal",
                    "#diffusion",
                    "#dataset",
                    "#cv"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº ÑĞ»Ğ¾ÑĞ¼: Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ PSD Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "OmniPSD â€” ÑÑ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Flux, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ PSD-Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ¾ÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸. Ğ”Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ñ…Ğ¾Ğ»ÑÑ‚Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ÑĞ²ÑĞ·ÑĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸. Ğ”Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ in-context editing, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¸ ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… ÑĞ»Ğ¾Ñ‘Ğ² Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ¾ÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ’ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ RGBA-VAE ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ»ÑŒÑ„Ğ°-ĞºĞ°Ğ½Ğ°Ğ»Ğ° Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Layered Design with OmniPSD",
                    "desc": "OmniPSD is a novel diffusion framework designed for generating and decomposing PSD files, which are commonly used in graphic design. It utilizes advanced techniques like spatial attention to create semantically coherent layers from text prompts and to extract layers from flat images. The framework incorporates an RGBA-VAE to maintain transparency while learning the structure of the images. Through extensive testing, OmniPSD has shown to produce high-quality results, making it a significant advancement in layered design generation and editing."
                },
                "zh": {
                    "title": "OmniPSDï¼šé«˜ä¿çœŸåˆ†å±‚è®¾è®¡ç”Ÿæˆä¸åˆ†è§£çš„æ–°èŒƒå¼",
                    "desc": "OmniPSDæ˜¯ä¸€ä¸ªåŸºäºFluxç”Ÿæ€ç³»ç»Ÿçš„æ‰©æ•£æ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°æ–‡æœ¬åˆ°PSDçš„ç”Ÿæˆå’Œå›¾åƒåˆ°PSDçš„åˆ†è§£ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å¤„ç†å…·æœ‰é€æ˜é€šé“çš„åˆ†å±‚PSDæ–‡ä»¶ã€‚å¯¹äºæ–‡æœ¬åˆ°PSDçš„ç”Ÿæˆï¼ŒOmniPSDåˆ©ç”¨ç©ºé—´æ³¨æ„åŠ›å°†å¤šä¸ªç›®æ ‡å±‚å®‰æ’åœ¨ä¸€ä¸ªç”»å¸ƒä¸Šï¼Œå­¦ä¹ å®ƒä»¬çš„ç»„åˆå…³ç³»ã€‚å¯¹äºå›¾åƒåˆ°PSDçš„åˆ†è§£ï¼Œå®ƒé€šè¿‡è¿­ä»£çš„ä¸Šä¸‹æ–‡ç¼–è¾‘ï¼Œä»å•ä¸€çš„æ‰å¹³å›¾åƒä¸­é€æ­¥æå–å’Œæ“¦é™¤æ–‡æœ¬åŠå‰æ™¯ç»„ä»¶ï¼Œé‡å»ºå¯ç¼–è¾‘çš„PSDå±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.08829",
            "title": "InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models",
            "url": "https://huggingface.co/papers/2512.08829",
            "abstract": "InfiniteVL, a linear-complexity VLM architecture combining sliding window attention and Gated DeltaNet, achieves competitive performance with less data and faster inference than leading Transformer-based models.  \t\t\t\t\tAI-generated summary \t\t\t\t Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.",
            "score": 12,
            "issue_id": 3,
            "pub_date": "2025-12-09",
            "pub_date_card": {
                "ru": "9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 9",
                "zh": "12æœˆ9æ—¥"
            },
            "hash": "c51218d9db2e2c65",
            "authors": [
                "Hongyuan Tao",
                "Bencheng Liao",
                "Shaoyu Chen",
                "Haoran Yin",
                "Qian Zhang",
                "Wenyu Liu",
                "Xinggang Wang"
            ],
            "affiliations": [
                "Horizon Robotics",
                "Huazhong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.08829.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#inference",
                    "#architecture",
                    "#long_context",
                    "#optimization",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ‘ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ€Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ InfiniteVL â€” Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞµ Ğ¾ĞºĞ¾Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ¾ ÑĞ»Ğ¾ÑĞ¼Ğ¸ Gated DeltaNet Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ĞºĞ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ‚Ğ¸Ğ¿Ğ° OCR Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ, Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¸ fine-tuning Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµĞ½ĞµĞµ 2% Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. InfiniteVL Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² 3.6 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ÑƒÑ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ 24 FPS Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸."
                },
                "en": {
                    "title": "InfiniteVL: Fast and Efficient Vision-Language Understanding",
                    "desc": "InfiniteVL is a new architecture for Vision-Language Models (VLMs) that combines sliding window attention with Gated DeltaNet to achieve linear complexity. This model addresses the limitations of traditional window-based and linear attention methods, particularly in handling long sequences and information-rich tasks. It employs a unique three-stage training strategy that allows it to perform well with significantly less training data than other leading models. As a result, InfiniteVL not only matches the performance of top Transformer-based VLMs but also offers faster inference speeds and better memory retention."
                },
                "zh": {
                    "title": "InfiniteVLï¼šé«˜æ•ˆçš„è§†è§‰è¯­è¨€æ¨¡å‹",
                    "desc": "InfiniteVLæ˜¯ä¸€ç§çº¿æ€§å¤æ‚åº¦çš„è§†è§‰è¯­è¨€æ¨¡å‹æ¶æ„ï¼Œç»“åˆäº†æ»‘åŠ¨çª—å£æ³¨æ„åŠ›å’ŒGated DeltaNetï¼Œèƒ½å¤Ÿåœ¨æ•°æ®é‡è¾ƒå°‘å’Œæ¨ç†é€Ÿåº¦æ›´å¿«çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°ä¸é¢†å…ˆçš„Transformeræ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬è’¸é¦é¢„è®­ç»ƒã€æŒ‡ä»¤è°ƒä¼˜å’Œé•¿åºåˆ—å¾®è°ƒï¼Œå…‹æœäº†ä¼ ç»Ÿçª—å£æ³¨æ„åŠ›å’Œçº¿æ€§æ³¨æ„åŠ›çš„å±€é™æ€§ã€‚InfiniteVLåœ¨ä½¿ç”¨ä¸åˆ°2%çš„è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„çº¿æ€§å¤æ‚åº¦æ¨¡å‹ï¼Œå¹¶ä¸é¡¶å°–çš„Transformeræ¨¡å‹æ€§èƒ½ç›¸åŒ¹é…ã€‚å®ƒåœ¨æµåª’ä½“è§†é¢‘ç†è§£åœºæ™¯ä¸­ï¼Œä¿æŒç¨³å®šçš„24å¸§æ¯ç§’çš„å®æ—¶é¢„å¡«å……é€Ÿåº¦ï¼ŒåŒæ—¶æœ‰æ•ˆä¿ç•™é•¿æœŸè®°å¿†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.09928",
            "title": "HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models",
            "url": "https://huggingface.co/papers/2512.09928",
            "abstract": "HiF-VLA integrates motion for bidirectional temporal reasoning in VLA models, improving long-horizon manipulation performance with minimal additional latency.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.",
            "score": 10,
            "issue_id": 5,
            "pub_date": "2025-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "352c0679b6cd9022",
            "authors": [
                "Minghui Lin",
                "Pengxiang Ding",
                "Shu Wang",
                "Zifeng Zhuang",
                "Yang Liu",
                "Xinyang Tong",
                "Wenxuan Song",
                "Shangke Lyu",
                "Siteng Huang",
                "Donglin Wang"
            ],
            "affiliations": [
                "HKUST(GZ)",
                "Nanjing University",
                "Westlake Robotics",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.09928.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#robotics",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ’Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ HiF-VLA â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ° (VLA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº ĞºĞ°Ğº ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ĞµĞµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ: Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ³Ğ¾ (hindsight), Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾ (foresight) Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¾Ğ±Ğ¾Ğ¸Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… LIBERO-Long Ğ¸ CALVIN ABC-D Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Enhancing Robotic Manipulation with Motion-Aware Temporal Reasoning",
                    "desc": "HiF-VLA is a new framework that enhances Vision-Language-Action (VLA) models by incorporating motion for better understanding of time in robotic tasks. Traditional VLA models often struggle with long-term actions because they only consider the current state, leading to poor performance over time. By using motion as a key element, HiF-VLA captures changes in the environment and improves decision-making through a process called bidirectional temporal reasoning. This approach allows robots to think ahead and learn from past actions, resulting in better performance in complex manipulation tasks with minimal delay."
                },
                "zh": {
                    "title": "HiF-VLAï¼šæå‡é•¿æ—¶é—´æ“ä½œçš„æ™ºèƒ½æœºå™¨äºº",
                    "desc": "HiF-VLAæ˜¯ä¸€ç§æ–°å‹çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆè¿åŠ¨ä¿¡æ¯æ¥å¢å¼ºåŒå‘æ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œä»è€Œæé«˜é•¿æ—¶é—´æ“ä½œçš„è¡¨ç°ã€‚è¯¥æ¨¡å‹å…‹æœäº†ä¼ ç»ŸVLAæ¨¡å‹çš„æ—¶é—´çŸ­è§†é—®é¢˜ï¼Œåˆ©ç”¨è¿åŠ¨ä½œä¸ºæ›´ç´§å‡‘å’Œä¿¡æ¯ä¸°å¯Œçš„æ—¶é—´ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚HiF-VLAé€šè¿‡å›é¡¾è¿‡å»çš„åŠ¨æ€å’Œé¢„æµ‹æœªæ¥çš„è¿åŠ¨ï¼Œç»“åˆäº†å›é¡¾è°ƒåˆ¶çš„è”åˆä¸“å®¶ï¼Œå®ç°äº†â€œè¾¹æ€è€ƒè¾¹è¡ŒåŠ¨â€çš„æ“ä½œæ¨¡å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHiF-VLAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶åœ¨å®é™…çš„é•¿æ—¶é—´æ“ä½œä»»åŠ¡ä¸­ä¹Ÿå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02892",
            "title": "Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules",
            "url": "https://huggingface.co/papers/2512.02892",
            "abstract": "SchED, a training-free early-exit algorithm, accelerates diffusion large language model decoding with minimal performance loss across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion large language models (dLLMs) offer a promising alternative to autoregressive models, but their practical utility is severely hampered by slow, iterative sampling. We present SchED, a training-free, model-agnostic early-exit algorithm that aggregates full-span logit margins and halts decoding once a smooth, progress-dependent confidence threshold is met. We evaluated SchED on two dLLM families (Dream and LLaDA), in base and instruction-tuned variants across ten benchmarks spanning downstream tasks including multiple-choice question answering (MCQ), math, long-form QA/summarization, and translation. SchED delivers large, stable accelerations: on instruction-tuned models, it achieves 3.8-4.0times speedups while retaining 99.8-100% of the baseline score on average. On base models, SchED yields consistent speedup gains with 99.1-100% performance retention, with up to 2.34times under more aggressive settings. Using a conservative speed metric that heavily penalizes quality loss (QPS, Î³{=}4), we show that SchED is robust and clearly outperforms prior confidence-based early-exit methods, which break down on long-form generation. An entropy analysis of the model's token predictions reveals that instruction tuning speeds up the decay of predictive entropy. By turning genuine confidence stabilization into computational savings, SchED makes dLLM decoding substantially more efficient.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "855bb2b6a35ff192",
            "authors": [
                "Amr Mohamed",
                "Yang Zhang",
                "Michalis Vazirgiannis",
                "Guokan Shang"
            ],
            "affiliations": [
                "Ecole Polytechnique",
                "MBZUAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.02892.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#diffusion",
                    "#machine_translation",
                    "#optimization"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¼Ğ½ÑƒÑ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºÑƒ",
                    "desc": "SchED â€” ÑÑ‚Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ñ€Ğ°Ğ½Ğ½ĞµĞ¹ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ ĞµÑ‘ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ğ³Ğ´Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ°Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´ĞµÑÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² 3.8-4.0 Ñ€Ğ°Ğ·Ğ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ 99.8-100% ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². SchED ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€Ğ°Ğ½Ğ½ĞµĞ¹ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², Ğ¸ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ°Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼."
                },
                "en": {
                    "title": "Accelerating dLLM Decoding with SchED: Fast and Efficient!",
                    "desc": "SchED is an innovative early-exit algorithm designed to enhance the efficiency of diffusion large language models (dLLMs) during decoding without requiring additional training. It works by monitoring the confidence of predictions and stopping the decoding process once a certain confidence level is reached, thus saving time. The algorithm has been tested on various tasks and shows significant speed improvements, achieving up to 4 times faster decoding while maintaining nearly perfect performance. This approach not only accelerates the process but also demonstrates robustness compared to previous methods, especially in long-form generation tasks."
                },
                "zh": {
                    "title": "SchEDï¼šé«˜æ•ˆè§£ç çš„æ—©æœŸé€€å‡ºç®—æ³•",
                    "desc": "SchEDæ˜¯ä¸€ç§æ— è®­ç»ƒçš„æ—©æœŸé€€å‡ºç®—æ³•ï¼Œæ—¨åœ¨åŠ é€Ÿæ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹çš„è§£ç è¿‡ç¨‹ï¼ŒåŒæ—¶ä¿æŒè¾ƒå°çš„æ€§èƒ½æŸå¤±ã€‚è¯¥ç®—æ³•é€šè¿‡èšåˆå…¨è·¨åº¦çš„logitè¾¹é™…ï¼Œå¹¶åœ¨è¾¾åˆ°å¹³æ»‘çš„ã€ä¾èµ–è¿›åº¦çš„ç½®ä¿¡åº¦é˜ˆå€¼æ—¶åœæ­¢è§£ç ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆDreamå’ŒLLaDAï¼‰ä¸Šè¯„ä¼°äº†SchEDï¼Œç»“æœæ˜¾ç¤ºåœ¨æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ä¸Šï¼Œé€Ÿåº¦æå‡å¯è¾¾3.8-4.0å€ï¼ŒåŒæ—¶ä¿æŒ99.8-100%çš„åŸºçº¿å¾—åˆ†ã€‚SchEDé€šè¿‡å°†çœŸå®çš„ç½®ä¿¡åº¦ç¨³å®šæ€§è½¬åŒ–ä¸ºè®¡ç®—èŠ‚çœï¼Œä½¿å¾—æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹çš„è§£ç æ•ˆç‡æ˜¾è‘—æé«˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.09616",
            "title": "Rethinking Chain-of-Thought Reasoning for Videos",
            "url": "https://huggingface.co/papers/2512.09616",
            "abstract": "Efficient video reasoning can be achieved using concise chains of thought and reduced visual tokens without manual annotations or supervised fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video.",
            "score": 7,
            "issue_id": 11,
            "pub_date": "2025-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "d8512985124eab08",
            "authors": [
                "Yiwu Zhong",
                "Zi-Yuan Hu",
                "Yin Li",
                "Liwei Wang"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.09616.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#inference",
                    "#optimization",
                    "#open_source",
                    "#training",
                    "#reasoning",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞšÑ€Ğ°Ñ‚ĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆÑ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¶Ğ°Ñ‚Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ´ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ¼. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ, Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ğ½Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğµ ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ° ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ ĞºĞ°Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‚Ğ°Ğº Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Streamlining Video Reasoning with Concise Thought Chains",
                    "desc": "This paper presents a new approach to video reasoning that emphasizes the use of concise chains of thought and fewer visual tokens. The authors argue that traditional methods, which rely on lengthy reasoning and numerous visual inputs, can be improved by simplifying these processes. They introduce a framework that allows video models to work with compressed visual tokens and generate shorter reasoning paths, enhancing efficiency without the need for manual annotations or extensive fine-tuning. The results show that this streamlined approach maintains competitive performance while significantly improving inference speed."
                },
                "zh": {
                    "title": "ç®€æ´æ¨ç†ï¼Œæå‡è§†é¢‘ç†è§£æ•ˆç‡",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†ä¸€ç§é«˜æ•ˆçš„è§†é¢‘æ¨ç†æ–¹æ³•ï¼Œåˆ©ç”¨ç®€æ´çš„æ¨ç†é“¾å’Œå‡å°‘çš„è§†è§‰æ ‡è®°ï¼Œè€Œæ— éœ€æ‰‹åŠ¨æ³¨é‡Šæˆ–ç›‘ç£å¾®è°ƒã€‚ç ”ç©¶è¡¨æ˜ï¼Œç®€æ´çš„æ¨ç†ç»“åˆè¾ƒå°‘çš„è§†è§‰è¾“å…¥å¯ä»¥æœ‰æ•ˆåœ°è¿›è¡Œè§†é¢‘æ¨ç†ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§é«˜æ•ˆçš„åè®­ç»ƒå’Œæ¨ç†æ¡†æ¶ï¼Œæå‡äº†è§†é¢‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ¨ç†æ•ˆç‡ä¸Šæ˜¾è‘—æé«˜ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.04753",
            "title": "EtCon: Edit-then-Consolidate for Reliable Knowledge Editing",
            "url": "https://huggingface.co/papers/2512.04753",
            "abstract": "A novel knowledge editing framework, Edit-then-Consolidate, addresses overfitting and lack of knowledge integration in large language models through targeted fine-tuning and policy optimization, enhancing reliability and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Knowledge editing aims to update specific facts in large language models (LLMs) without full retraining. Prior efforts sought to tune the knowledge layers of LLMs, proving effective for making selective edits. However, a significant gap exists between their performance in controlled, teacher-forcing evaluations and their real-world effectiveness in lifelong learning scenarios, which greatly limits their practical applicability. This work's empirical analysis reveals two recurring issues associated with this gap: (1) Most traditional methods lead the edited model to overfit to the new fact, thereby degrading pre-trained capabilities; (2) There is a critical absence of a knowledge consolidation stage, leaving new facts insufficiently integrated into LLMs' inference-time behavior under autoregressive generation, thereby leading to a mismatch between parametric knowledge and actual generation behavior. To this end, we propose Edit-then-Consolidate, a novel knowledge editing paradigm that aims to bridge the gap between theoretical knowledge editing methods and their real-world applicability. Specifically, (1) our framework mitigates overfitting via Targeted Proximal Supervised Fine-Tuning (TPSFT) that localizes the edit via a trust-region objective to limit policy drift; (2) Then, a consolidation stage using Group Relative Policy Optimization (GRPO) aligns the edited knowledge with CoT-based inference policy by optimizing trajectory-level behavior under comprehensive reward signals. Extensive experiments demonstrate our framework consistently improves editing reliability and generalization under real-world evaluations, while better preserving locality and pre-trained capabilities.",
            "score": 7,
            "issue_id": 5,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "ccb065e424ee2067",
            "authors": [
                "Ruilin Li",
                "Yibin Wang",
                "Wenhong Zhu",
                "Chenglin Li",
                "Jinghao Zhang",
                "Chenliang Li",
                "Junchi Yan",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "University of Science and Technology of China",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.04753.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#rlhf",
                    "#optimization",
                    "#training"
                ],
                "emoji": "âœï¸",
                "ru": {
                    "title": "ĞÑ‚ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸: Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² LLM",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Edit-then-Consolidate, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ (TPSFT) Ñ trust-region Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ (GRPO) Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Edit-then-Consolidate: Bridging Knowledge Gaps in Language Models",
                    "desc": "The paper introduces a new framework called Edit-then-Consolidate for knowledge editing in large language models (LLMs). This framework addresses two main issues: overfitting to new facts and poor integration of knowledge during inference. It employs Targeted Proximal Supervised Fine-Tuning to ensure that updates do not degrade existing capabilities, followed by Group Relative Policy Optimization to align the edited knowledge with the model's inference behavior. The results show that this approach enhances the reliability and generalization of LLMs in real-world applications."
                },
                "zh": {
                    "title": "ç¼–è¾‘åæ•´åˆï¼šæå‡çŸ¥è¯†ç¼–è¾‘çš„å¯é æ€§ä¸æ³›åŒ–èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„çŸ¥è¯†ç¼–è¾‘æ¡†æ¶â€”â€”ç¼–è¾‘åæ•´åˆï¼ˆEdit-then-Consolidateï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è¿‡æ‹Ÿåˆå’ŒçŸ¥è¯†æ•´åˆä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡é’ˆå¯¹æ€§çš„å¾®è°ƒå’Œç­–ç•¥ä¼˜åŒ–ï¼Œå¢å¼ºäº†æ¨¡å‹çš„å¯é æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¼ ç»Ÿæ–¹æ³•å¾€å¾€å¯¼è‡´æ¨¡å‹å¯¹æ–°çŸ¥è¯†çš„è¿‡æ‹Ÿåˆï¼ŒæŸå®³äº†é¢„è®­ç»ƒèƒ½åŠ›ï¼ŒåŒæ—¶ç¼ºä¹çŸ¥è¯†æ•´åˆé˜¶æ®µï¼Œä½¿å¾—æ–°çŸ¥è¯†åœ¨æ¨ç†æ—¶çš„è¡¨ç°ä¸ä½³ã€‚é€šè¿‡å¼•å…¥ç›®æ ‡é‚»è¿‘ç›‘ç£å¾®è°ƒå’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œæœ¬æ–‡çš„æ¡†æ¶æœ‰æ•ˆæå‡äº†çŸ¥è¯†ç¼–è¾‘çš„å¯é æ€§å’Œå®é™…åº”ç”¨æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.09864",
            "title": "UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving",
            "url": "https://huggingface.co/papers/2512.09864",
            "abstract": "A unified framework combines vision-language models and video generation to improve autonomous driving in complex scenarios by enhancing reasoning, trajectory planning, and video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.",
            "score": 6,
            "issue_id": 5,
            "pub_date": "2025-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "3f3f20fcab48cc05",
            "authors": [
                "Hao Lu",
                "Ziyang Liu",
                "Guangfeng Jiang",
                "Yuanfei Luo",
                "Sheng Chen",
                "Yangang Zhang",
                "Ying-Cong Chen"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2512.09864.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#video",
                    "#robotics",
                    "#multimodal"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ‚ĞµĞ· Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ UniUGP â€” ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ², Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ VLM Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑÑŒ Ğ½Ğ° Ñ€ĞµĞ´ĞºĞ¸Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Enhancing Autonomous Driving with Unified Vision-Language and Video Generation",
                    "desc": "This paper presents a new framework called UniUGP that combines vision-language models with video generation to enhance autonomous driving systems. It addresses the challenges faced in complex driving scenarios by improving reasoning, trajectory planning, and video generation capabilities. The framework utilizes specialized datasets for training, allowing it to learn from both labeled and unlabeled data, which helps in understanding visual dynamics and semantic context. The results show that UniUGP outperforms existing methods in perception and decision-making, especially in difficult situations that are often overlooked by traditional models."
                },
                "zh": {
                    "title": "ç»Ÿä¸€æ¡†æ¶æå‡è‡ªåŠ¨é©¾é©¶èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¡†æ¶ï¼Œç»“åˆäº†è§†è§‰-è¯­è¨€æ¨¡å‹å’Œè§†é¢‘ç”ŸæˆæŠ€æœ¯ï¼Œä»¥æå‡å¤æ‚åœºæ™¯ä¸‹çš„è‡ªåŠ¨é©¾é©¶èƒ½åŠ›ã€‚é€šè¿‡æ„å»ºå¤šä¸ªä¸“é—¨çš„æ•°æ®é›†ï¼Œæä¾›æ¨ç†å’Œè§„åˆ’çš„æ³¨é‡Šï¼Œå¢å¼ºäº†ç³»ç»Ÿåœ¨é•¿å°¾åœºæ™¯ä¸­çš„è¡¨ç°ã€‚æå‡ºçš„UniUGPæ¡†æ¶é€šè¿‡æ··åˆä¸“å®¶æ¶æ„ï¼Œæ•´åˆäº†é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹å’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå®ç°äº†åœºæ™¯æ¨ç†ã€æœªæ¥è§†é¢‘ç”Ÿæˆå’Œè½¨è¿¹è§„åˆ’çš„ååŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ„ŸçŸ¥ã€æ¨ç†å’Œå†³ç­–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åº”å¯¹å…·æœ‰æŒ‘æˆ˜æ€§çš„é•¿å°¾æƒ…å†µã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.09164",
            "title": "WonderZoom: Multi-Scale 3D World Generation",
            "url": "https://huggingface.co/papers/2512.09164",
            "abstract": "WonderZoom generates multi-scale 3D scenes from a single image using scale-adaptive Gaussian surfels and a progressive detail synthesizer, outperforming existing models in quality and alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t We present WonderZoom, a novel approach to generating 3D scenes with contents across multiple spatial scales from a single image. Existing 3D world generation models remain limited to single-scale synthesis and cannot produce coherent scene contents at varying granularities. The fundamental challenge is the lack of a scale-aware 3D representation capable of generating and rendering content with largely different spatial sizes. WonderZoom addresses this through two key innovations: (1) scale-adaptive Gaussian surfels for generating and real-time rendering of multi-scale 3D scenes, and (2) a progressive detail synthesizer that iteratively generates finer-scale 3D contents. Our approach enables users to \"zoom into\" a 3D region and auto-regressively synthesize previously non-existent fine details from landscapes to microscopic features. Experiments demonstrate that WonderZoom significantly outperforms state-of-the-art video and 3D models in both quality and alignment, enabling multi-scale 3D world creation from a single image. We show video results and an interactive viewer of generated multi-scale 3D worlds in https://wonderzoom.github.io/",
            "score": 6,
            "issue_id": 5,
            "pub_date": "2025-12-09",
            "pub_date_card": {
                "ru": "9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 9",
                "zh": "12æœˆ9æ—¥"
            },
            "hash": "ab1a27df70b89f35",
            "authors": [
                "Jin Cao",
                "Hong-Xing Yu",
                "Jiajun Wu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2512.09164.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#cv"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞÑ‚ Ğ¼Ğ¸ĞºÑ€Ğ¾ Ğº Ğ¼Ğ°ĞºÑ€Ğ¾: Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· 3D-Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞ½Ğ¸Ğ¼ĞºĞ°",
                    "desc": "WonderZoom Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑĞ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ â€” Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾-Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Gaussian surfels Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… 3D-ÑÑ†ĞµĞ½. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼ĞµĞ»ĞºĞ¸Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ 3D-ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¸Ñ€Ñ‹."
                },
                "en": {
                    "title": "Zoom into Reality: Multi-Scale 3D Scene Generation from a Single Image",
                    "desc": "WonderZoom is a new method for creating 3D scenes from just one image, allowing for details at different sizes. It uses special techniques called scale-adaptive Gaussian surfels to generate and display these scenes in real-time. Additionally, it features a progressive detail synthesizer that adds finer details step by step, making the 3D world richer and more realistic. This approach outperforms existing models by producing higher quality and better-aligned 3D content, enabling users to explore intricate details in their generated scenes."
                },
                "zh": {
                    "title": "ä»å•å›¾åƒç”Ÿæˆå¤šå°ºåº¦3Dåœºæ™¯çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "WonderZoomæ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå¯ä»¥ä»å•å¼ å›¾åƒç”Ÿæˆå¤šå°ºåº¦çš„3Dåœºæ™¯ã€‚ç°æœ‰çš„3Dä¸–ç•Œç”Ÿæˆæ¨¡å‹é€šå¸¸åªèƒ½å¤„ç†å•ä¸€å°ºåº¦ï¼Œæ— æ³•åœ¨ä¸åŒç²’åº¦ä¸‹ç”Ÿæˆä¸€è‡´çš„åœºæ™¯å†…å®¹ã€‚WonderZoomé€šè¿‡ä¸¤ä¸ªå…³é”®åˆ›æ–°æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼šä½¿ç”¨é€‚åº”å°ºåº¦çš„é«˜æ–¯è¡¨é¢ç”Ÿæˆå’Œå®æ—¶æ¸²æŸ“å¤šå°ºåº¦3Dåœºæ™¯ï¼Œä»¥åŠé€æ­¥ç»†èŠ‚åˆæˆå™¨è¿­ä»£ç”Ÿæˆæ›´ç²¾ç»†çš„3Då†…å®¹ã€‚å®éªŒè¡¨æ˜ï¼ŒWonderZoomåœ¨è´¨é‡å’Œå¯¹é½æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„3Dæ¨¡å‹ï¼Œèƒ½å¤Ÿå®ç°ä»å•å¼ å›¾åƒåˆ›å»ºå¤šå°ºåº¦çš„3Dä¸–ç•Œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.09106",
            "title": "Learning Unmasking Policies for Diffusion Language Models",
            "url": "https://huggingface.co/papers/2512.09106",
            "abstract": "Reinforcement learning is used to train sampling procedures for masked discrete diffusion language models, improving token throughput and quality compared to heuristic strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion (Large) Language Models (dLLMs) now match the downstream performance of their autoregressive counterparts on many tasks, while holding the promise of being more efficient during inference. One particularly successful variant is masked discrete diffusion, in which a buffer filled with special mask tokens is progressively replaced with tokens sampled from the model's vocabulary. Efficiency can be gained by unmasking several tokens in parallel, but doing too many at once risks degrading the generation quality. Thus, one critical design aspect of dLLMs is the sampling procedure that selects, at each step of the diffusion process, which tokens to replace. Indeed, recent work has found that heuristic strategies such as confidence thresholding lead to both higher quality and token throughput compared to random unmasking. However, such heuristics have downsides: they require manual tuning, and we observe that their performance degrades with larger buffer sizes. In this work, we instead propose to train sampling procedures using reinforcement learning. Specifically, we formalize masked diffusion sampling as a Markov decision process in which the dLLM serves as the environment, and propose a lightweight policy architecture based on a single-layer transformer that maps dLLM token confidences to unmasking decisions. Our experiments show that these trained policies match the performance of state-of-the-art heuristics when combined with semi-autoregressive generation, while outperforming them in the full diffusion setting. We also examine the transferability of these policies, finding that they can generalize to new underlying dLLMs and longer sequence lengths. However, we also observe that their performance degrades when applied to out-of-domain data, and that fine-grained tuning of the accuracy-efficiency trade-off can be challenging with our approach.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2025-12-09",
            "pub_date_card": {
                "ru": "9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 9",
                "zh": "12æœˆ9æ—¥"
            },
            "hash": "bb2057653f823d81",
            "authors": [
                "Metod Jazbec",
                "Theo X. Olausson",
                "Louis BÃ©thune",
                "Pierre Ablin",
                "Michael Kirchhof",
                "Joao Monterio",
                "Victor Turrisi",
                "Jason Ramapuram",
                "Marco Cuturi"
            ],
            "affiliations": [
                "Apple",
                "Massachusetts Institute of Technology",
                "University of Amsterdam"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.09106.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#diffusion",
                    "#architecture",
                    "#rl",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·masking'Ğ° ĞºĞ°Ğº Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼Ğ¸, Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¸Ñ… Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¾Ğ´Ğ½Ğ°ĞºĞ¾ Ñ‚ĞµÑ€ÑÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ½Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Reinforcement Learning Boosts Token Sampling in Diffusion Models",
                    "desc": "This paper explores the use of reinforcement learning to enhance the sampling procedures in masked discrete diffusion language models (dLLMs). By treating the sampling process as a Markov decision process, the authors develop a policy that optimally decides which tokens to unmask, improving both token throughput and generation quality. Their approach outperforms traditional heuristic methods, particularly in full diffusion settings, while also demonstrating the ability to generalize across different dLLMs and sequence lengths. However, the study notes challenges in maintaining performance on out-of-domain data and fine-tuning the balance between accuracy and efficiency."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ æå‡æ©è”½æ‰©æ•£è¯­è¨€æ¨¡å‹çš„é‡‡æ ·æ•ˆç‡",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥è®­ç»ƒæ©è”½ç¦»æ•£æ‰©æ•£è¯­è¨€æ¨¡å‹çš„é‡‡æ ·è¿‡ç¨‹ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°é€‰æ‹©æ›¿æ¢çš„æ ‡è®°ï¼Œä»è€Œæé«˜äº†æ ‡è®°çš„é€šé‡å’Œç”Ÿæˆè´¨é‡ã€‚ä¸ä¼ ç»Ÿçš„å¯å‘å¼ç­–ç•¥ç›¸æ¯”ï¼Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†æ›´å¤§ç¼“å†²åŒºæ—¶è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæ¨¡å‹åœ¨å¤„ç†åŸŸå¤–æ•°æ®æ—¶çš„è¡¨ç°ä»ç„¶æœ‰æ‰€ä¸‹é™ï¼Œä¸”åœ¨å‡†ç¡®æ€§ä¸æ•ˆç‡ä¹‹é—´çš„å¾®è°ƒä¹Ÿè¾ƒä¸ºå¤æ‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.08296",
            "title": "Towards a Science of Scaling Agent Systems",
            "url": "https://huggingface.co/papers/2512.08296",
            "abstract": "A quantitative framework for agent system scaling using empirical coordination metrics identifies optimal multi-agent strategies based on task properties.  \t\t\t\t\tAI-generated summary \t\t\t\t Agents, language model (LM)-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored, leaving practitioners to rely on heuristics rather than principled design choices. We address this gap by deriving quantitative scaling principles for agent systems. We evaluate this across four diverse benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. Using five canonical architectures (Single, Independent, Centralized, Decentralized, Hybrid) instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations with standardized tools and token budgets. We derive a predictive model using empirical coordination metrics, including efficiency, overhead, error amplification, and redundancy, that achieves cross-validated R^2=0.513. We identify three dominant effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns (beta=-0.408, p<0.001) once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x through unchecked propagation, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.9% on parallelizable tasks like financial reasoning, while decentralized coordination excels on dynamic web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, all multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations, providing a predictive principle of agentic scaling based on measurable task properties.",
            "score": 4,
            "issue_id": 14,
            "pub_date": "2025-12-09",
            "pub_date_card": {
                "ru": "9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 9",
                "zh": "12æœˆ9æ—¥"
            },
            "hash": "97c50a5e944e26c0",
            "authors": [
                "Yubin Kim",
                "Ken Gu",
                "Chanwoo Park",
                "Chunjong Park",
                "Samuel Schmidgall",
                "A. Ali Heydari",
                "Yao Yan",
                "Zhihan Zhang",
                "Yuchen Zhuang",
                "Mark Malhotra",
                "Paul Pu Liang",
                "Hae Won Park",
                "Yuzhe Yang",
                "Xuhai Xu",
                "Yilun Du",
                "Shwetak Patel",
                "Tim Althoff",
                "Daniel McDuff",
                "Xin Liu"
            ],
            "affiliations": [
                "Google DeepMind",
                "Google Research",
                "Massachusetts Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.08296.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "ĞĞ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¿ÑÑ‚ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ²ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ (ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ°ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ñ‹, ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ 51.3% Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ñ‹ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ°: ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° ÑĞ²Ñ‹ÑˆĞµ 45%, Ğ¸ ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ 87% ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸."
                },
                "en": {
                    "title": "Optimizing Multi-Agent Systems with Coordination Metrics",
                    "desc": "This paper presents a quantitative framework for optimizing multi-agent systems by using empirical coordination metrics to identify the best strategies based on specific task characteristics. It evaluates various architectures and configurations across multiple benchmarks, revealing key insights into how coordination impacts performance. The study finds that while centralized coordination can significantly enhance performance in certain tasks, it can also lead to diminishing returns in others, particularly in sequential reasoning. Ultimately, the framework offers a predictive model that helps practitioners choose the most effective coordination strategy for their agent systems."
                },
                "zh": {
                    "title": "æ™ºèƒ½ä½“ç³»ç»Ÿæ‰©å±•çš„æœ€ä½³åè°ƒç­–ç•¥",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å®šé‡æ¡†æ¶ï¼Œç”¨äºé€šè¿‡ç»éªŒåè°ƒæŒ‡æ ‡æ¥è¯†åˆ«å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æœ€ä½³ç­–ç•¥ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ€§èƒ½å—ä»»åŠ¡ç‰¹æ€§å½±å“ï¼Œä¸”åœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹ï¼Œå·¥å…·å¯†é›†å‹ä»»åŠ¡åœ¨å¤šæ™ºèƒ½ä½“åè°ƒä¸­ä¼šé­å—æ›´å¤§çš„å¼€é”€ã€‚é€šè¿‡å¯¹å¤šç§æ¶æ„å’Œä»»åŠ¡çš„è¯„ä¼°ï¼Œå‘ç°åè°ƒçš„æ”¶ç›Šä¼šéšç€å•æ™ºèƒ½ä½“åŸºçº¿çš„æé«˜è€Œé€’å‡ï¼Œä¸”ç‹¬ç«‹æ™ºèƒ½ä½“åœ¨é”™è¯¯ä¼ æ’­æ–¹é¢ä¼šæ”¾å¤§é”™è¯¯ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿé¢„æµ‹87%çš„é…ç½®çš„æœ€ä½³åè°ƒç­–ç•¥ï¼Œä¸ºæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ‰©å±•æä¾›äº†å¯é‡åŒ–çš„åŸåˆ™ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.09663",
            "title": "IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting",
            "url": "https://huggingface.co/papers/2512.09663",
            "abstract": "The introduction of IF-Bench evaluates multimodal large language models' performance on infrared images using diverse assessment strategies and proposes a training-free method to improve comprehension.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal large language models (MLLMs) have led to impressive progress across various benchmarks. However, their capability in understanding infrared images remains unexplored. To address this gap, we introduce IF-Bench, the first high-quality benchmark designed for evaluating multimodal understanding of infrared images. IF-Bench consists of 499 images sourced from 23 infrared datasets and 680 carefully curated visual question-answer pairs, covering 10 essential dimensions of image understanding. Based on this benchmark, we systematically evaluate over 40 open-source and closed-source MLLMs, employing cyclic evaluation, bilingual assessment, and hybrid judgment strategies to enhance the reliability of the results. Our analysis reveals how model scale, architecture, and inference paradigms affect infrared image comprehension, providing valuable insights for this area. Furthermore, we propose a training-free generative visual prompting (GenViP) method, which leverages advanced image editing models to translate infrared images into semantically and spatially aligned RGB counterparts, thereby mitigating domain distribution shifts. Extensive experiments demonstrate that our method consistently yields significant performance improvements across a wide range of MLLMs. The benchmark and code are available at https://github.com/casiatao/IF-Bench.",
            "score": 3,
            "issue_id": 5,
            "pub_date": "2025-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "6aae414f0ccaeb81",
            "authors": [
                "Tao Zhang",
                "Yuyang Hong",
                "Yang Xia",
                "Kun Ding",
                "Zeyu Zhang",
                "Ying Wang",
                "Shiming Xiang",
                "Chunhong Pan"
            ],
            "affiliations": [
                "MAIS, Institute of Automation",
                "Research Center of Aerospace Information, Institute of Automation",
                "School of Artificial Intelligence, UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.09663.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#benchmark",
                    "#open_source",
                    "#survey",
                    "#multimodal"
                ],
                "emoji": "ğŸŒ¡ï¸",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ñ€Ğ°ĞºÑ€Ğ°ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ IF-Bench â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ñ€Ğ°ĞºÑ€Ğ°ÑĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 499 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ 680 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ĞµĞµ 40 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ, Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑÑƒĞ´ĞµĞ¹ÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ½Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ñ€Ğ°ĞºÑ€Ğ°ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ-Ğ±ĞµĞ·-Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ GenViP, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ñ€Ğ°ĞºÑ€Ğ°ÑĞ½Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ğ² Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞµ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ñƒ RGB-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²ÑĞµÑ… Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Unlocking Infrared Insights with IF-Bench",
                    "desc": "The paper introduces IF-Bench, a benchmark specifically designed to evaluate how well multimodal large language models (MLLMs) understand infrared images. It includes a diverse set of 499 infrared images and 680 visual question-answer pairs to assess comprehension across ten key dimensions. The authors systematically test over 40 MLLMs using various evaluation strategies to ensure reliable results, revealing insights into how model architecture and size influence performance. Additionally, they propose a novel training-free method called generative visual prompting (GenViP) that enhances understanding by converting infrared images into RGB images, leading to significant performance improvements."
                },
                "zh": {
                    "title": "æå‡çº¢å¤–å›¾åƒç†è§£çš„å¤šæ¨¡æ€åŸºå‡†",
                    "desc": "IF-Benchæ˜¯ä¸€ä¸ªæ–°æå‡ºçš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨çº¢å¤–å›¾åƒç†è§£æ–¹é¢çš„è¡¨ç°ã€‚è¯¥åŸºå‡†åŒ…å«499å¼ æ¥è‡ª23ä¸ªçº¢å¤–æ•°æ®é›†çš„å›¾åƒå’Œ680ä¸ªç²¾å¿ƒç­–åˆ’çš„è§†è§‰é—®ç­”å¯¹ï¼Œæ¶µç›–äº†å›¾åƒç†è§£çš„10ä¸ªé‡è¦ç»´åº¦ã€‚é€šè¿‡å¯¹40å¤šä¸ªå¼€æºå’Œé—­æºçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼Œç ”ç©¶æ­ç¤ºäº†æ¨¡å‹è§„æ¨¡ã€æ¶æ„å’Œæ¨ç†èŒƒå¼å¯¹çº¢å¤–å›¾åƒç†è§£çš„å½±å“ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„ç”Ÿæˆè§†è§‰æç¤ºæ–¹æ³•ï¼Œåˆ©ç”¨å…ˆè¿›çš„å›¾åƒç¼–è¾‘æ¨¡å‹å°†çº¢å¤–å›¾åƒè½¬æ¢ä¸ºè¯­ä¹‰å’Œç©ºé—´å¯¹é½çš„RGBå›¾åƒï¼Œä»è€Œæ”¹å–„æ¨¡å‹çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.05446",
            "title": "TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression",
            "url": "https://huggingface.co/papers/2512.05446",
            "abstract": "TED-4DGS combines sparse anchor-based 3D Gaussian Splatting with temporal activation and embedding for efficient and high-quality compression of dynamic 3D scenes.  \t\t\t\t\tAI-generated summary \t\t\t\t Building on the success of 3D Gaussian Splatting (3DGS) in static 3D scene representation, its extension to dynamic scenes, commonly referred to as 4DGS or dynamic 3DGS, has attracted increasing attention. However, designing more compact and efficient deformation schemes together with rate-distortion-optimized compression strategies for dynamic 3DGS representations remains an underexplored area. Prior methods either rely on space-time 4DGS with overspecified, short-lived Gaussian primitives or on canonical 3DGS with deformation that lacks explicit temporal control. To address this, we present TED-4DGS, a temporally activated and embedding-based deformation scheme for rate-distortion-optimized 4DGS compression that unifies the strengths of both families. TED-4DGS is built on a sparse anchor-based 3DGS representation. Each canonical anchor is assigned learnable temporal-activation parameters to specify its appearance and disappearance transitions over time, while a lightweight per-anchor temporal embedding queries a shared deformation bank to produce anchor-specific deformation. For rate-distortion compression, we incorporate an implicit neural representation (INR)-based hyperprior to model anchor attribute distributions, along with a channel-wise autoregressive model to capture intra-anchor correlations. With these novel elements, our scheme achieves state-of-the-art rate-distortion performance on several real-world datasets. To the best of our knowledge, this work represents one of the first attempts to pursue a rate-distortion-optimized compression framework for dynamic 3DGS representations.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-12-05",
            "pub_date_card": {
                "ru": "5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 5",
                "zh": "12æœˆ5æ—¥"
            },
            "hash": "8db3443155844ec4",
            "authors": [
                "Cheng-Yuan Ho",
                "He-Bi Yang",
                "Jui-Chiu Chiang",
                "Yu-Lun Liu",
                "Wen-Hsiao Peng"
            ],
            "affiliations": [
                "National Chung Cheng University, Taiwan",
                "National Yang Ming Chiao Tung University, Taiwan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.05446.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#video",
                    "#3d"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 3D ÑÑ†ĞµĞ½ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ÑĞºĞ¾Ñ€ĞµĞ¹",
                    "desc": "TED-4DGS Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 3D ÑÑ†ĞµĞ½, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞºĞ¾Ñ€ĞµĞ¹ 3D Gaussian Splatting Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞºĞ¾Ñ€ÑŒ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ ĞµĞ³Ğ¾ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¸ÑÑ‡ĞµĞ·Ğ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğµ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ±Ğ°Ğ½ĞºĞ° Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¹. Ğ”Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğµ-ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ (rate-distortion) Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² ÑĞºĞ¾Ñ€ĞµĞ¹ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¹ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ ÑĞºĞ¾Ñ€ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾-ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¸ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¸Ğ· Ğ¿ĞµÑ€Ğ²Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸Ñ… rate-distortion Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ 3DGS Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Efficient Compression of Dynamic 3D Scenes with TED-4DGS",
                    "desc": "TED-4DGS is a novel approach that enhances the compression of dynamic 3D scenes by integrating sparse anchor-based 3D Gaussian Splatting with temporal activation and embedding techniques. This method addresses the limitations of previous dynamic 3D Gaussian Splatting approaches by providing a more efficient deformation scheme and optimizing for rate-distortion performance. By utilizing learnable temporal-activation parameters and a lightweight temporal embedding, TED-4DGS effectively manages the appearance and deformation of 3D anchors over time. The incorporation of an implicit neural representation for modeling anchor attributes further improves compression efficiency, achieving state-of-the-art results on various datasets."
                },
                "zh": {
                    "title": "åŠ¨æ€3Dåœºæ™¯çš„é«˜æ•ˆå‹ç¼©æ–°æ–¹æ³•",
                    "desc": "TED-4DGSç»“åˆäº†ç¨€ç–é”šç‚¹åŸºç¡€çš„3Dé«˜æ–¯ç‚¹äº‘å’Œæ—¶é—´æ¿€æ´»ä¸åµŒå…¥æŠ€æœ¯ï¼Œæ—¨åœ¨é«˜æ•ˆä¸”é«˜è´¨é‡åœ°å‹ç¼©åŠ¨æ€3Dåœºæ™¯ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„æ—¶é—´æ¿€æ´»å‚æ•°ï¼Œä¼˜åŒ–äº†åŠ¨æ€3Dé«˜æ–¯ç‚¹äº‘çš„å˜å½¢æ–¹æ¡ˆï¼Œå¹¶å®ç°äº†æ›´å¥½çš„å‹ç¼©æ•ˆæœã€‚TED-4DGSåˆ©ç”¨éšå¼ç¥ç»è¡¨ç¤ºå’Œé€šé“è‡ªå›å½’æ¨¡å‹æ¥æ•æ‰é”šç‚¹ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä»è€Œæé«˜äº†å‹ç¼©æ€§èƒ½ã€‚è¯¥ç ”ç©¶åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‹ç¼©æ•ˆæœï¼Œæ ‡å¿—ç€åŠ¨æ€3Dé«˜æ–¯ç‚¹äº‘å‹ç¼©é¢†åŸŸçš„é‡è¦è¿›å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.08006",
            "title": "Beyond Unified Models: A Service-Oriented Approach to Low Latency, Context Aware Phonemization for Real Time TTS",
            "url": "https://huggingface.co/papers/2512.08006",
            "abstract": "A framework is proposed to improve phonemization quality in TTS systems without sacrificing real-time performance through lightweight context-aware phonemization and a service-oriented architecture.  \t\t\t\t\tAI-generated summary \t\t\t\t Lightweight, real-time text-to-speech systems are crucial for accessibility. However, the most efficient TTS models often rely on lightweight phonemizers that struggle with context-dependent challenges. In contrast, more advanced phonemizers with a deeper linguistic understanding typically incur high computational costs, which prevents real-time performance.   This paper examines the trade-off between phonemization quality and inference speed in G2P-aided TTS systems, introducing a practical framework to bridge this gap. We propose lightweight strategies for context-aware phonemization and a service-oriented TTS architecture that executes these modules as independent services. This design decouples heavy context-aware components from the core TTS engine, effectively breaking the latency barrier and enabling real-time use of high-quality phonemization models. Experimental results confirm that the proposed system improves pronunciation soundness and linguistic accuracy while maintaining real-time responsiveness, making it well-suited for offline and end-device TTS applications.",
            "score": 2,
            "issue_id": 5,
            "pub_date": "2025-12-08",
            "pub_date_card": {
                "ru": "8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 8",
                "zh": "12æœˆ8æ—¥"
            },
            "hash": "6e8b6b05272b394a",
            "authors": [
                "Mahta Fetrat",
                "Donya Navabi",
                "Zahra Dehghanian",
                "Morteza Abolghasemi",
                "Hamid R. Rabiee"
            ],
            "affiliations": [
                "Sharif University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.08006.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#architecture",
                    "#audio",
                    "#inference"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "Ğ’Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ„Ğ¾Ğ½ĞµĞ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ„Ğ¾Ğ½ĞµĞ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ¹ Ñ„Ğ¾Ğ½ĞµĞ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞµÑ€Ğ²Ğ¸Ñ-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‰ÑƒÑ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ TTS Ğ½Ğ° Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğµ ÑĞµÑ€Ğ²Ğ¸ÑÑ‹. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ¾Ñ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ´Ñ€Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing TTS Phonemization Quality Without Slowing Down",
                    "desc": "This paper presents a new framework aimed at enhancing the quality of phonemization in text-to-speech (TTS) systems while ensuring they remain fast and efficient. It addresses the common issue where lightweight phonemizers, which are quick, often fail to handle context-dependent phonetic challenges effectively. The proposed solution involves a service-oriented architecture that allows complex phonemization tasks to be processed as separate services, thus reducing the load on the main TTS engine. Experimental results demonstrate that this approach significantly improves pronunciation accuracy and linguistic quality without compromising real-time performance, making it ideal for various applications."
                },
                "zh": {
                    "title": "æå‡TTSç³»ç»ŸéŸ³ç´ åŒ–è´¨é‡çš„å®æ—¶è§£å†³æ–¹æ¡ˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿä¸­çš„éŸ³ç´ åŒ–è´¨é‡ï¼ŒåŒæ—¶ä¸å½±å“å®æ—¶æ€§èƒ½ã€‚è¯¥æ¡†æ¶é‡‡ç”¨è½»é‡çº§çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥éŸ³ç´ åŒ–ç­–ç•¥å’Œé¢å‘æœåŠ¡çš„æ¶æ„ï¼Œä½¿å¾—éŸ³ç´ åŒ–æ¨¡å—å¯ä»¥ç‹¬ç«‹è¿è¡Œã€‚é€šè¿‡å°†å¤æ‚çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç»„ä»¶ä¸æ ¸å¿ƒTTSå¼•æ“è§£è€¦ï¼Œçªç ´äº†å»¶è¿Ÿç“¶é¢ˆï¼Œå®ç°äº†é«˜è´¨é‡éŸ³ç´ åŒ–æ¨¡å‹çš„å®æ—¶ä½¿ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å‘éŸ³å‡†ç¡®æ€§å’Œè¯­è¨€å‡†ç¡®æ€§æ–¹é¢æœ‰æ‰€æå‡ï¼Œé€‚åˆç¦»çº¿å’Œç»ˆç«¯è®¾å¤‡çš„TTSåº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.04519",
            "title": "VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory",
            "url": "https://huggingface.co/papers/2512.04519",
            "abstract": "VideoSSM, a hybrid state-space memory model combining AR diffusion, achieves state-of-the-art temporal consistency and motion stability in long-video generation by coordinating short- and long-term context.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive (AR) diffusion enables streaming, interactive long-video generation by producing frames causally, yet maintaining coherence over minute-scale horizons remains challenging due to accumulated errors, motion drift, and content repetition. We approach this problem from a memory perspective, treating video synthesis as a recurrent dynamical process that requires coordinated short- and long-term context. We propose VideoSSM, a Long Video Model that unifies AR diffusion with a hybrid state-space memory. The state-space model (SSM) serves as an evolving global memory of scene dynamics across the entire sequence, while a context window provides local memory for motion cues and fine details. This hybrid design preserves global consistency without frozen, repetitive patterns, supports prompt-adaptive interaction, and scales in linear time with sequence length. Experiments on short- and long-range benchmarks demonstrate state-of-the-art temporal consistency and motion stability among autoregressive video generator especially at minute-scale horizons, enabling content diversity and interactive prompt-based control, thereby establishing a scalable, memory-aware framework for long video generation.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "c841edb85058f835",
            "authors": [
                "Yifei Yu",
                "Xiaoshan Wu",
                "Xinting Hu",
                "Tao Hu",
                "Yangtian Sun",
                "Xiaoyang Lyu",
                "Bo Wang",
                "Lin Ma",
                "Yuewen Ma",
                "Zhongrui Wang",
                "Xiaojuan Qi"
            ],
            "affiliations": [
                "HKU",
                "PICO, ByteDance",
                "SUSTech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.04519.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#architecture",
                    "#long_context",
                    "#benchmark"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "VideoSSM â€” ÑÑ‚Ğ¾ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ state-space memory, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğµ Ñ‚Ğ¸Ğ¿Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸: Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· SSM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ ÑÑ†ĞµĞ½Ñ‹ Ğ½Ğ° Ğ²ÑĞµĞ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾ĞºĞ½Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ´Ñ€ĞµĞ¹Ñ„Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‚ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ‚ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¸Ğ½ÑƒÑ‚. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ VideoSSM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹."
                },
                "en": {
                    "title": "VideoSSM: Mastering Long Video Generation with Memory Magic!",
                    "desc": "VideoSSM is a new model for generating long videos that combines autoregressive (AR) diffusion with a hybrid state-space memory. This approach helps maintain temporal consistency and motion stability by effectively managing both short-term and long-term context. By treating video synthesis as a dynamic process, VideoSSM uses a global memory to track scene changes while also keeping local details in check. The model shows impressive results in generating diverse and coherent videos, especially over longer time frames, making it suitable for interactive applications."
                },
                "zh": {
                    "title": "VideoSSMï¼šé•¿è§†é¢‘ç”Ÿæˆçš„æ–°çºªå…ƒ",
                    "desc": "VideoSSMæ˜¯ä¸€ç§æ··åˆçŠ¶æ€ç©ºé—´è®°å¿†æ¨¡å‹ï¼Œç»“åˆäº†è‡ªå›å½’æ‰©æ•£ï¼ˆAR diffusionï¼‰ï¼Œåœ¨é•¿è§†é¢‘ç”Ÿæˆä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ—¶é—´ä¸€è‡´æ€§å’Œè¿åŠ¨ç¨³å®šæ€§ã€‚è¯¥æ¨¡å‹é€šè¿‡åè°ƒçŸ­æœŸå’Œé•¿æœŸä¸Šä¸‹æ–‡ï¼Œè§£å†³äº†åœ¨ç”Ÿæˆé•¿è§†é¢‘æ—¶å› é”™è¯¯ç§¯ç´¯ã€è¿åŠ¨æ¼‚ç§»å’Œå†…å®¹é‡å¤è€Œå¯¼è‡´çš„æŒ‘æˆ˜ã€‚VideoSSMå°†è‡ªå›å½’æ‰©æ•£ä¸æ··åˆçŠ¶æ€ç©ºé—´è®°å¿†ç›¸ç»“åˆï¼ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ä½œä¸ºæ•´ä¸ªåºåˆ—åœºæ™¯åŠ¨æ€çš„å…¨çƒè®°å¿†ï¼Œè€Œä¸Šä¸‹æ–‡çª—å£åˆ™æä¾›è¿åŠ¨çº¿ç´¢å’Œç»†èŠ‚çš„å±€éƒ¨è®°å¿†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨çŸ­æœŸå’Œé•¿æœŸåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ”¯æŒå†…å®¹å¤šæ ·æ€§å’ŒåŸºäºæç¤ºçš„äº¤äº’æ§åˆ¶ï¼Œå»ºç«‹äº†ä¸€ä¸ªå¯æ‰©å±•çš„ã€å…³æ³¨è®°å¿†çš„é•¿è§†é¢‘ç”Ÿæˆæ¡†æ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.09112",
            "title": "GimbalDiffusion: Gravity-Aware Camera Control for Video Generation",
            "url": "https://huggingface.co/papers/2512.09112",
            "abstract": "GimbalDiffusion framework enables precise camera control in text-to-video generation using absolute coordinates and gravity as a reference, enhancing controllability and robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in text-to-video generation has achieved remarkable realism, yet fine-grained control over camera motion and orientation remains elusive. Existing approaches typically encode camera trajectories through relative or ambiguous representations, limiting explicit geometric control. We introduce GimbalDiffusion, a framework that enables camera control grounded in physical-world coordinates, using gravity as a global reference. Instead of describing motion relative to previous frames, our method defines camera trajectories in an absolute coordinate system, allowing precise and interpretable control over camera parameters without requiring an initial reference frame. We leverage panoramic 360-degree videos to construct a wide variety of camera trajectories, well beyond the predominantly straight, forward-facing trajectories seen in conventional video data. To further enhance camera guidance, we introduce null-pitch conditioning, an annotation strategy that reduces the model's reliance on text content when conflicting with camera specifications (e.g., generating grass while the camera points towards the sky). Finally, we establish a benchmark for camera-aware video generation by rebalancing SpatialVID-HQ for comprehensive evaluation under wide camera pitch variation. Together, these contributions advance the controllability and robustness of text-to-video models, enabling precise, gravity-aligned camera manipulation within generative frameworks.",
            "score": 1,
            "issue_id": 14,
            "pub_date": "2025-12-09",
            "pub_date_card": {
                "ru": "9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 9",
                "zh": "12æœˆ9æ—¥"
            },
            "hash": "724d62fb5d09fba8",
            "authors": [
                "FrÃ©dÃ©ric Fortier-Chouinard",
                "Yannick Hold-Geoffroy",
                "Valentin Deschaintre",
                "Matheus Gadelha",
                "Jean-FranÃ§ois Lalonde"
            ],
            "affiliations": [
                "Adobe",
                "Universite Laval"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.09112.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#benchmark",
                    "#video",
                    "#dataset"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞĞ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€Ğ°Ğ²Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºÑƒ",
                    "desc": "GimbalDiffusion â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹ Ñ Ğ³Ñ€Ğ°Ğ²Ğ¸Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¾Ñ‚ÑÑ‡ĞµÑ‚Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑĞ²Ğ½Ğ¾Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ğµ 360-Ğ³Ñ€Ğ°Ğ´ÑƒÑĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ null-pitch conditioning Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ÑĞ¼Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒĞ³Ğ»Ğ°Ñ… Ğ½Ğ°ĞºĞ»Ğ¾Ğ½Ğ° ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "GimbalDiffusion: Mastering Camera Control in Text-to-Video Generation",
                    "desc": "The GimbalDiffusion framework improves text-to-video generation by allowing precise control of camera movements using absolute coordinates and gravity as a reference point. This method overcomes limitations of previous approaches that relied on relative camera trajectories, enabling clearer and more interpretable camera control. By utilizing panoramic 360-degree videos, GimbalDiffusion can create diverse camera paths, enhancing the realism of generated videos. Additionally, the introduction of null-pitch conditioning helps the model maintain camera specifications even when text prompts conflict with visual context."
                },
                "zh": {
                    "title": "ç²¾ç¡®æ§åˆ¶ï¼šGimbalDiffusionæ¡†æ¶çš„åŠ›é‡",
                    "desc": "GimbalDiffusionæ¡†æ¶é€šè¿‡ä½¿ç”¨ç»å¯¹åæ ‡å’Œé‡åŠ›ä½œä¸ºå‚è€ƒï¼Œå¢å¼ºäº†æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä¸­çš„ç›¸æœºæ§åˆ¶èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å…è®¸åœ¨ç»å¯¹åæ ‡ç³»ç»Ÿä¸­å®šä¹‰ç›¸æœºè½¨è¿¹ï¼Œä»è€Œå®ç°å¯¹ç›¸æœºå‚æ•°çš„ç²¾ç¡®æ§åˆ¶ï¼Œè€Œæ— éœ€åˆå§‹å‚è€ƒå¸§ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†æ— ä¿¯ä»°æ¡ä»¶åŒ–çš„æ³¨é‡Šç­–ç•¥ï¼Œä»¥å‡å°‘æ¨¡å‹åœ¨ç›¸æœºè§„æ ¼ä¸æ–‡æœ¬å†…å®¹å†²çªæ—¶çš„ä¾èµ–ã€‚é€šè¿‡è¿™äº›åˆ›æ–°ï¼ŒGimbalDiffusionæå‡äº†æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹çš„å¯æ§æ€§å’Œé²æ£’æ€§ï¼Œä½¿å¾—ç”Ÿæˆçš„ç›¸æœºæ“ä½œæ›´åŠ ç²¾ç¡®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.07222",
            "title": "Pay Less Attention to Function Words for Free Robustness of Vision-Language Models",
            "url": "https://huggingface.co/papers/2512.07222",
            "abstract": "Function-word De-Attention (FDA) mitigates adversarial attacks on robust VLMs by differentially subtracting function-word cross-attention, improving robustness with minimal performance trade-offs.  \t\t\t\t\tAI-generated summary \t\t\t\t To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18/13/53% ASR drop with only 0.2/0.3/0.6% performance drops on the 3 tested models on retrieval, and a 90% ASR drop with a 0.3% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly at https://github.com/michaeltian108/FDA.",
            "score": 1,
            "issue_id": 5,
            "pub_date": "2025-12-08",
            "pub_date_card": {
                "ru": "8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 8",
                "zh": "12æœˆ8æ—¥"
            },
            "hash": "de0263c031804577",
            "authors": [
                "Qiwei Tian",
                "Chenhao Lin",
                "Zhengyu Zhao",
                "Chao Shen"
            ],
            "affiliations": [
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.07222.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#open_source"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ»ÑƒĞ¶ĞµĞ±Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Function-word De-Attention (FDA) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ vision-language Ğº adversarial Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ»ÑƒĞ¶ĞµĞ±Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ° (function words) Ğ² ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´ĞµĞ»Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹Ğ¼Ğ¸ Ğº adversarial Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ FDA Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñƒ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ñ‚ĞµĞ»Ñ: Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ attention Ğ½Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸ Ğ½Ğ° ÑĞ»ÑƒĞ¶ĞµĞ±Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ²Ğ°Ñ… Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²Ñ‹Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¸Ğ· Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ adversarial Ğ°Ñ‚Ğ°Ğº (Ğ½Ğ° 18-90% Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸) Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (0.2-0.6%)."
                },
                "en": {
                    "title": "Enhancing VLM Robustness with Function-word De-Attention",
                    "desc": "The paper introduces Function-word De-Attention (FDA), a technique designed to enhance the robustness of Vision-Language Models (VLMs) against adversarial attacks. FDA works by identifying and subtracting the attention given to function words, which are often responsible for vulnerabilities in VLMs. Through extensive experiments, the authors demonstrate that FDA significantly reduces the attack success rate (ASR) while maintaining minimal performance loss across various tasks and datasets. The results indicate that FDA not only improves robustness but also shows promising scalability and generalization capabilities."
                },
                "zh": {
                    "title": "åŠŸèƒ½è¯å»æ³¨æ„åŠ›ï¼šæå‡æ¨¡å‹é²æ£’æ€§çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºåŠŸèƒ½è¯å»æ³¨æ„åŠ›ï¼ˆFDAï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨å‡è½»å¯¹å¼ºå¥è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å¯¹æŠ—æ”»å‡»ã€‚ç ”ç©¶å‘ç°ï¼ŒåŠŸèƒ½è¯åœ¨è·¨æ¨¡æ€å¯¹æŠ—æ”»å‡»ä¸­å¯èƒ½å¯¼è‡´VLMçš„è„†å¼±æ€§ï¼Œå› æ­¤é€šè¿‡å·®å¼‚æ€§åœ°å‡å»åŠŸèƒ½è¯çš„äº¤å‰æ³¨æ„åŠ›æ¥æé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚FDAæ–¹æ³•ç±»ä¼¼äºå·®åˆ†æ”¾å¤§å™¨ï¼Œè®¡ç®—åŸå§‹æ³¨æ„åŠ›å’ŒåŠŸèƒ½è¯äº¤å‰æ³¨æ„åŠ›ï¼Œå¹¶ä»å‰è€…ä¸­å‡å»åè€…ï¼Œä»¥å®ç°æ›´å¼ºçš„æ¨¡å‹å¯¹é½å’Œé²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFDAåœ¨å¤šä¸ªæ”»å‡»ä¸‹è¡¨ç°å‡ºæ˜¾è‘—çš„é²æ£’æ€§æå‡ï¼ŒåŒæ—¶ä»…å¸¦æ¥å¾®å°çš„æ€§èƒ½æŸå¤±ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.05402",
            "title": "Smart Timing for Mining: A Deep Learning Framework for Bitcoin Hardware ROI Prediction",
            "url": "https://huggingface.co/papers/2512.05402",
            "abstract": "MineROI-Net, a Transformer-based model, predicts profitability for ASIC mining hardware acquisitions, enhancing decision-making in the volatile mining industry.  \t\t\t\t\tAI-generated summary \t\t\t\t Bitcoin mining hardware acquisition requires strategic timing due to volatile markets, rapid technological obsolescence, and protocol-driven revenue cycles. Despite mining's evolution into a capital-intensive industry, there is little guidance on when to purchase new Application-Specific Integrated Circuit (ASIC) hardware, and no prior computational frameworks address this decision problem. We address this gap by formulating hardware acquisition as a time series classification task, predicting whether purchasing ASIC machines yields profitable (Return on Investment (ROI) >= 1), marginal (0 < ROI < 1), or unprofitable (ROI <= 0) returns within one year. We propose MineROI-Net, an open source Transformer-based architecture designed to capture multi-scale temporal patterns in mining profitability. Evaluated on data from 20 ASIC miners released between 2015 and 2024 across diverse market regimes, MineROI-Net outperforms LSTM-based and TSLANet baselines, achieving 83.7% accuracy and 83.1% macro F1-score. The model demonstrates strong economic relevance, achieving 93.6% precision in detecting unprofitable periods and 98.5% precision for profitable ones, while avoiding misclassification of profitable scenarios as unprofitable and vice versa. These results indicate that MineROI-Net offers a practical, data-driven tool for timing mining hardware acquisitions, potentially reducing financial risk in capital-intensive mining operations. The model is available through: https://github.com/AMAAI-Lab/MineROI-Net.",
            "score": 1,
            "issue_id": 12,
            "pub_date": "2025-12-05",
            "pub_date_card": {
                "ru": "5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 5",
                "zh": "12æœˆ5æ—¥"
            },
            "hash": "f5ec690791b92e1b",
            "authors": [
                "Sithumi Wickramasinghe",
                "Bikramjit Das",
                "Dorien Herremans"
            ],
            "affiliations": [
                "Singapore University of Technology and Design, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.05402.jpg",
            "data": {
                "categories": [],
                "emoji": "â›ï¸",
                "ru": {
                    "title": "Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ±Ñ‹Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ğ¹Ğ½Ğ¸Ğ½Ğ³Ğ°: ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ² Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¿Ğ¾ĞºÑƒĞ¿ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MineROI-Net â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ±Ñ‹Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ASIC Ğ´Ğ»Ñ Ğ¼Ğ°Ğ¹Ğ½Ğ¸Ğ½Ğ³Ğ° Ğ±Ğ¸Ñ‚ĞºĞ¾Ğ¸Ğ½Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¿Ğ¾ĞºÑƒĞ¿ĞºĞ¸ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ², Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ğ±ÑƒĞ´ĞµÑ‚ Ğ»Ğ¸ Ğ¸Ğ½Ğ²ĞµÑÑ‚Ğ¸Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ±Ñ‹Ğ»ÑŒĞ½Ğ¾Ğ¹, Ğ¼Ğ°Ñ€Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ»Ğ¸ ÑƒĞ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ² Ñ‚ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ³Ğ¾Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 83.7% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LSTM. Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ 93.6% ÑƒĞ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸ 98.5% Ğ¿Ñ€Ğ¸Ğ±Ñ‹Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ°Ğ¹Ğ½ĞµÑ€Ğ°Ğ¼ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ²Ğ¾Ğ»Ğ°Ñ‚Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ñ‹Ğ½ĞºĞ°."
                },
                "en": {
                    "title": "Smart Timing for ASIC Mining Investments with MineROI-Net",
                    "desc": "MineROI-Net is a Transformer-based model designed to predict the profitability of acquiring ASIC mining hardware, which is crucial in the volatile cryptocurrency mining market. The model treats hardware acquisition as a time series classification problem, categorizing potential returns into profitable, marginal, or unprofitable within a year. By analyzing data from various ASIC miners, MineROI-Net outperforms traditional models like LSTM, achieving high accuracy and precision in identifying profitable and unprofitable periods. This innovative approach provides miners with a data-driven strategy to make informed purchasing decisions, thereby minimizing financial risks associated with hardware investments."
                },
                "zh": {
                    "title": "MineROI-Netï¼šæ™ºèƒ½å†³ç­–ï¼Œä¼˜åŒ–çŸ¿æœºé‡‡è´­æ—¶æœº",
                    "desc": "MineROI-Net æ˜¯ä¸€ç§åŸºäº Transformer çš„æ¨¡å‹ï¼Œæ—¨åœ¨é¢„æµ‹ ASIC çŸ¿æœºçš„ç›ˆåˆ©èƒ½åŠ›ï¼Œä»è€Œå¢å¼ºåœ¨æ³¢åŠ¨çš„çŸ¿ä¸šè¡Œä¸šä¸­çš„å†³ç­–èƒ½åŠ›ã€‚è¯¥æ¨¡å‹å°†ç¡¬ä»¶é‡‡è´­é—®é¢˜è§†ä¸ºæ—¶é—´åºåˆ—åˆ†ç±»ä»»åŠ¡ï¼Œèƒ½å¤Ÿé¢„æµ‹åœ¨ä¸€å¹´å†…è´­ä¹° ASIC çŸ¿æœºçš„æŠ•èµ„å›æŠ¥ç‡ï¼ˆROIï¼‰æ˜¯å¦ä¸ºç›ˆåˆ©ã€è¾¹é™…æˆ–äºæŸã€‚é€šè¿‡åˆ†æ 2015 è‡³ 2024 å¹´é—´ 20 æ¬¾ ASIC çŸ¿æœºçš„æ•°æ®ï¼ŒMineROI-Net åœ¨å‡†ç¡®æ€§å’Œå®è§‚ F1 åˆ†æ•°ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„ LSTM æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨è¯†åˆ«ç›ˆåˆ©å’ŒäºæŸæ—¶æœŸæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸ºçŸ¿ä¸šç¡¬ä»¶é‡‡è´­æä¾›äº†å®ç”¨çš„æ•°æ®é©±åŠ¨å·¥å…·ï¼Œå¸®åŠ©é™ä½èµ„æœ¬å¯†é›†å‹çŸ¿ä¸šæ“ä½œä¸­çš„è´¢åŠ¡é£é™©ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01453",
            "title": "Reinventing Clinical Dialogue: Agentic Paradigms for LLM Enabled Healthcare Communication",
            "url": "https://huggingface.co/papers/2512.01453",
            "abstract": "The survey analyzes the cognitive architecture of medical AI systems, focusing on the shift from generative text prediction to agentic autonomy, and categorizes methods into four archetypes based on knowledge source and agency objective.  \t\t\t\t\tAI-generated summary \t\t\t\t Clinical dialogue represents a complex duality requiring both the empathetic fluency of natural conversation and the rigorous precision of evidence-based medicine. While Large Language Models possess unprecedented linguistic capabilities, their architectural reliance on reactive and stateless processing often favors probabilistic plausibility over factual veracity. This structural limitation has catalyzed a paradigm shift in medical AI from generative text prediction to agentic autonomy, where the model functions as a central reasoning engine capable of deliberate planning and persistent memory. Moving beyond existing reviews that primarily catalog downstream applications, this survey provides a first-principles analysis of the cognitive architecture underpinning this shift. We introduce a novel taxonomy structured along the orthogonal axes of knowledge source and agency objective to delineate the provenance of clinical knowledge against the system's operational scope. This framework facilitates a systematic analysis of the intrinsic trade-offs between creativity and reliability by categorizing methods into four archetypes: Latent Space Clinicians, Emergent Planners, Grounded Synthesizers, and Verifiable Workflow Automators. For each paradigm, we deconstruct the technical realization across the entire cognitive pipeline, encompassing strategic planning, memory management, action execution, collaboration, and evolution to reveal how distinct architectural choices balance the tension between autonomy and safety.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "2df8c54fe7f317e9",
            "authors": [
                "Xiaoquan Zhi",
                "Hongke Zhao",
                "Likang Wu",
                "Chuang Zhao",
                "Hengshu Zhu"
            ],
            "affiliations": [
                "College of Management and Economics, Laboratory of Computation and Analytics of Complex Management Systems(CACMS), Tianjin University, China",
                "Computer Network Information Center, Chinese Academy of Sciences, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.01453.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#reasoning",
                    "#agents",
                    "#architecture",
                    "#science",
                    "#survey"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼: ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ³Ğ¾ AI Ğ² ĞºĞ»Ğ¸Ğ½Ğ¸ĞºĞµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… AI ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ñ†ĞµĞ»ÑÑ… ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ°Ñ€Ñ…ĞµÑ‚Ğ¸Ğ¿Ğ°: Latent Space Clinicians, Emergent Planners, Grounded Synthesizers Ğ¸ Verifiable Workflow Automators. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾Ñ‚Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğµ. Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ±Ğ¸Ñ€Ğ°ÑÑ‚ÑÑ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "From Text Prediction to Autonomous Medical AI: A New Paradigm",
                    "desc": "This paper surveys the cognitive architecture of medical AI systems, highlighting the transition from generative text prediction to agentic autonomy. It categorizes AI methods into four archetypes based on their knowledge sources and objectives, providing a framework for understanding their operational capabilities. The authors emphasize the importance of balancing creativity and reliability in clinical dialogue, where both empathetic communication and evidence-based precision are crucial. By analyzing the cognitive pipeline of these systems, the paper reveals how different architectural choices impact the autonomy and safety of medical AI applications."
                },
                "zh": {
                    "title": "åŒ»ç–—AIçš„è®¤çŸ¥æ¶æ„è½¬å˜ï¼šä»ç”Ÿæˆåˆ°è‡ªä¸»æ™ºèƒ½",
                    "desc": "è¿™ç¯‡è®ºæ–‡åˆ†æäº†åŒ»ç–—äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„è®¤çŸ¥æ¶æ„ï¼Œé‡ç‚¹è®¨è®ºäº†ä»ç”Ÿæˆæ–‡æœ¬é¢„æµ‹åˆ°è‡ªä¸»æ™ºèƒ½çš„è½¬å˜ã€‚ä½œè€…å°†æ–¹æ³•åˆ†ä¸ºå››ç§åŸå‹ï¼ŒåŸºäºçŸ¥è¯†æ¥æºå’Œä»£ç†ç›®æ ‡è¿›è¡Œåˆ†ç±»ã€‚è®ºæ–‡æŒ‡å‡ºï¼Œå°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯­è¨€èƒ½åŠ›ä¸Šå…·æœ‰å‰æ‰€æœªæœ‰çš„ä¼˜åŠ¿ï¼Œä½†å…¶ç»“æ„é™åˆ¶ä½¿å¾—å®ƒä»¬æ›´å€¾å‘äºæ¦‚ç‡ä¸Šçš„åˆç†æ€§è€Œéäº‹å®çš„å‡†ç¡®æ€§ã€‚é€šè¿‡å¼•å…¥æ–°çš„åˆ†ç±»æ³•ï¼Œè®ºæ–‡ç³»ç»Ÿåœ°åˆ†æäº†åˆ›é€ åŠ›ä¸å¯é æ€§ä¹‹é—´çš„å†…åœ¨æƒè¡¡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-12-10.html",
    "link_next": "2025-12-12.html",
    "link_month": "2025-12.html",
    "short_date_prev": {
        "ru": "10.12",
        "en": "12/10",
        "zh": "12æœˆ10æ—¥"
    },
    "short_date_next": {
        "ru": "12.12",
        "en": "12/12",
        "zh": "12æœˆ12æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 0,
        "#benchmark": 5,
        "#agents": 2,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 4,
        "#3d": 3,
        "#audio": 1,
        "#video": 8,
        "#multimodal": 8,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 7,
        "#healthcare": 1,
        "#training": 6,
        "#robotics": 2,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 7,
        "#survey": 2,
        "#diffusion": 6,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0
    }
}