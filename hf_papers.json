{
    "date": {
        "ru": "11 –º–∞—Ä—Ç–∞",
        "en": "March 11",
        "zh": "3Êúà11Êó•"
    },
    "time_utc": "2025-03-11 08:14",
    "weekday": 1,
    "issue_id": 2636,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.03601",
            "title": "Feature-Level Insights into Artificial Text Detection with Sparse\n  Autoencoders",
            "url": "https://huggingface.co/papers/2503.03601",
            "abstract": "Artificial Text Detection (ATD) is becoming increasingly important with the rise of advanced Large Language Models (LLMs). Despite numerous efforts, no single algorithm performs consistently well across different types of unseen text or guarantees effective generalization to new LLMs. Interpretability plays a crucial role in achieving this goal. In this study, we enhance ATD interpretability by using Sparse Autoencoders (SAE) to extract features from Gemma-2-2b residual stream. We identify both interpretable and efficient features, analyzing their semantics and relevance through domain- and model-specific statistics, a steering approach, and manual or LLM-based interpretation. Our methods offer valuable insights into how texts from various models differ from human-written content. We show that modern LLMs have a distinct writing style, especially in information-dense domains, even though they can produce human-like outputs with personalized prompts.",
            "score": 63,
            "issue_id": 2634,
            "pub_date": "2025-03-05",
            "pub_date_card": {
                "ru": "5 –º–∞—Ä—Ç–∞",
                "en": "March 5",
                "zh": "3Êúà5Êó•"
            },
            "hash": "d045a8e2a39262c9",
            "authors": [
                "Kristian Kuznetsov",
                "Laida Kushnareva",
                "Polina Druzhinina",
                "Anton Razzhigaev",
                "Anastasia Voznyuk",
                "Irina Piontkovskaya",
                "Evgeny Burnaev",
                "Serguei Barannikov"
            ],
            "affiliations": [
                "AI Foundation and Algorithm Lab",
                "Artificial Intelligence Research Institute (AIRI)",
                "CNRS, Universit√© Paris Cit√©, France",
                "Moscow Institute of Physics and Technology",
                "Skolkovo Institute of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.03601.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#interpretability",
                    "#data"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–†–∞—Å–∫—Ä—ã–≤–∞—è —Å–µ–∫—Ä–µ—Ç—ã –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ–º—É –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—é",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —É–ª—É—á—à–µ–Ω–∏—é –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (ATD) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞ –º–æ–¥–µ–ª–∏ Gemma-2-2b. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Å–µ–º–∞–Ω—Ç–∏–∫—É –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –≤—ã–¥–µ–ª–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤ –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏–º–µ—é—Ç –æ—Ç–ª–∏—á–∏—Ç–µ–ª—å–Ω—ã–π —Å—Ç–∏–ª—å –ø–∏—Å—å–º–∞, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ-–Ω–∞—Å—ã—â–µ–Ω–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ü–µ–Ω–Ω—ã–µ insights –æ —Ç–æ–º, –∫–∞–∫ —Ç–µ–∫—Å—Ç—ã, —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –æ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –Ω–∞–ø–∏—Å–∞–Ω–Ω–æ–≥–æ –ª—é–¥—å–º–∏."
                },
                "en": {
                    "title": "Enhancing Text Detection with Interpretability in AI Models",
                    "desc": "This paper focuses on improving Artificial Text Detection (ATD) by enhancing its interpretability using Sparse Autoencoders (SAE). The authors extract features from the residual stream of the Gemma-2-2b model to identify both interpretable and efficient characteristics of text. They analyze these features through various statistical methods and interpretations to understand how machine-generated texts differ from human-written ones. The study reveals that modern Large Language Models (LLMs) exhibit a unique writing style, particularly in information-dense areas, despite their ability to generate human-like text."
                },
                "zh": {
                    "title": "ÊèêÂçá‰∫∫Â∑•ÊñáÊú¨Ê£ÄÊµãÁöÑÂèØËß£ÈáäÊÄß",
                    "desc": "ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂèëÂ±ïÔºå‰∫∫Â∑•ÊñáÊú¨Ê£ÄÊµãÔºàATDÔºâÂèòÂæóË∂äÊù•Ë∂äÈáçË¶Å„ÄÇÂ∞ΩÁÆ°Â∑≤ÊúâËÆ∏Â§öÂä™ÂäõÔºå‰ΩÜÊ≤°ÊúâÂçï‰∏ÄÁÆóÊ≥ïËÉΩÂ§üÂú®‰∏çÂêåÁ±ªÂûãÁöÑÊú™ËßÅÊñáÊú¨‰∏≠ÂßãÁªàË°®Áé∞ËâØÂ•ΩÔºå‰πüÊó†Ê≥ï‰øùËØÅÂØπÊñ∞LLMÁöÑÊúâÊïàÊ≥õÂåñ„ÄÇÂèØËß£ÈáäÊÄßÂú®ÂÆûÁé∞Ëøô‰∏ÄÁõÆÊ†á‰∏≠Ëµ∑ÁùÄÂÖ≥ÈîÆ‰ΩúÁî®„ÄÇÊàë‰ª¨ÈÄöËøá‰ΩøÁî®Á®ÄÁñèËá™ÁºñÁ†ÅÂô®ÔºàSAEÔºâ‰ªéGemma-2-2bÊÆãÂ∑ÆÊµÅ‰∏≠ÊèêÂèñÁâπÂæÅÔºåÂ¢ûÂº∫‰∫ÜATDÁöÑÂèØËß£ÈáäÊÄßÔºåÂàÜÊûê‰∫ÜÊñáÊú¨‰∏é‰∫∫Á±ªÂÜô‰ΩúÂÜÖÂÆπÁöÑÂ∑ÆÂºÇ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07365",
            "title": "MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2503.07365",
            "abstract": "We present MM-Eureka, a multimodal reasoning model that successfully extends large-scale rule-based reinforcement learning (RL) to multimodal reasoning. While rule-based RL has shown remarkable success in improving LLMs' reasoning abilities in text domains, its application to multimodal settings has remained challenging. Our work reproduces key characteristics of text-based RL systems like DeepSeek-R1 in the multimodal space, including steady increases in accuracy reward and response length, and the emergence of reflection behaviors. We demonstrate that both instruction-tuned and pre-trained models can develop strong multimodal reasoning capabilities through rule-based RL without supervised fine-tuning, showing superior data efficiency compared to alternative approaches. We open-source our complete pipeline to foster further research in this area. We release all our codes, models, data, etc. at https://github.com/ModalMinds/MM-EUREKA",
            "score": 33,
            "issue_id": 2630,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 –º–∞—Ä—Ç–∞",
                "en": "March 10",
                "zh": "3Êúà10Êó•"
            },
            "hash": "765d475b38d9f289",
            "authors": [
                "Fanqing Meng",
                "Lingxiao Du",
                "Zongkai Liu",
                "Zhixiang Zhou",
                "Quanfeng Lu",
                "Daocheng Fu",
                "Botian Shi",
                "Wenhai Wang",
                "Junjun He",
                "Kaipeng Zhang",
                "Ping Luo",
                "Yu Qiao",
                "Qiaosheng Zhang",
                "Wenqi Shao"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07365.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#multimodal",
                    "#reasoning",
                    "#rag",
                    "#open_source"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º",
                    "desc": "MM-Eureka ‚Äì —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å—à–∏—Ä—è–µ—Ç –º–∞—Å—à—Ç–∞–±–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –≤–∫–ª—é—á–∞—è —É—Å—Ç–æ–π—á–∏–≤–æ–µ –ø–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –¥–ª–∏–Ω—ã –æ—Ç–≤–µ—Ç–æ–≤, –∞ —Ç–∞–∫–∂–µ –ø–æ—è–≤–ª–µ–Ω–∏–µ —Ä–µ—Ñ–ª–µ–∫—Å–∏–≤–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∫–∞–∫ –º–æ–¥–µ–ª–∏, –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, —Ç–∞–∫ –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç —Ä–∞–∑–≤–∏–≤–∞—Ç—å —Å–∏–ª—å–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–µ–∑ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏. –ê–≤—Ç–æ—Ä—ã –æ—Ç–∫—Ä—ã–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –≤—Å–µ–≥–æ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏."
                },
                "en": {
                    "title": "Unlocking Multimodal Reasoning with MM-Eureka!",
                    "desc": "MM-Eureka is a new model that enhances reasoning across different types of data, like text and images, using a method called rule-based reinforcement learning (RL). This approach builds on successful techniques from text-based RL, allowing the model to improve its accuracy and response quality in multimodal contexts. The model shows that it can learn effectively without needing extra labeled data, making it more efficient than other methods. By sharing our tools and findings, we aim to encourage more research in multimodal reasoning."
                },
                "zh": {
                    "title": "MM-EurekaÔºöÂ§öÊ®°ÊÄÅÊé®ÁêÜÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "Êàë‰ª¨ÊèêÂá∫‰∫ÜMM-EurekaÔºåËøôÊòØ‰∏Ä‰∏™Â§öÊ®°ÊÄÅÊé®ÁêÜÊ®°ÂûãÔºåÊàêÂäüÂú∞Â∞ÜÂ§ßËßÑÊ®°Âü∫‰∫éËßÑÂàôÁöÑÂº∫ÂåñÂ≠¶‰π†Êâ©Â±ïÂà∞Â§öÊ®°ÊÄÅÊé®ÁêÜÈ¢ÜÂüü„ÄÇËôΩÁÑ∂Âü∫‰∫éËßÑÂàôÁöÑÂº∫ÂåñÂ≠¶‰π†Âú®ÊñáÊú¨È¢ÜÂüüÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóÊàêÂäüÔºå‰ΩÜÂú®Â§öÊ®°ÊÄÅÁéØÂ¢É‰∏≠ÁöÑÂ∫îÁî®‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊàòÊÄß„ÄÇÊàë‰ª¨ÁöÑÂ∑•‰ΩúÂú®Â§öÊ®°ÊÄÅÁ©∫Èó¥‰∏≠ÈáçÁé∞‰∫ÜÊñáÊú¨Âü∫Á°ÄÂº∫ÂåñÂ≠¶‰π†Á≥ªÁªüÁöÑÂÖ≥ÈîÆÁâπÂæÅÔºåÂåÖÊã¨ÂáÜÁ°ÆÊÄßÂ•ñÂä±ÂíåÂìçÂ∫îÈïøÂ∫¶ÁöÑÁ®≥ÂÆöÂ¢ûÂä†Ôºå‰ª•ÂèäÂèçÊÄùË°å‰∏∫ÁöÑÂá∫Áé∞„ÄÇÊàë‰ª¨Â±ïÁ§∫‰∫ÜÊó†ÁõëÁù£ÂæÆË∞ÉÁöÑÊÉÖÂÜµ‰∏ãÔºåÊåá‰ª§Ë∞É‰ºòÂíåÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÈÉΩËÉΩÈÄöËøáÂü∫‰∫éËßÑÂàôÁöÑÂº∫ÂåñÂ≠¶‰π†ÂèëÂ±ïÂá∫Âº∫Â§ßÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜËÉΩÂäõÔºå‰∏îÊï∞ÊçÆÊïàÁéá‰ºò‰∫éÂÖ∂‰ªñÊñπÊ≥ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07605",
            "title": "SEAP: Training-free Sparse Expert Activation Pruning Unlock the\n  Brainpower of Large Language Models",
            "url": "https://huggingface.co/papers/2503.07605",
            "abstract": "Large Language Models have achieved remarkable success across various natural language processing tasks, yet their high computational cost during inference remains a major bottleneck. This paper introduces Sparse Expert Activation Pruning (SEAP), a training-free pruning method that selectively retains task-relevant parameters to reduce inference overhead. Inspired by the clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and prunes the model while preserving task performance and enhancing computational efficiency. Experimental results demonstrate that SEAP significantly reduces computational overhead while maintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both WandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2% performance drop compared to the dense model. These findings highlight SEAP's scalability and effectiveness, making it a promising approach for optimizing large-scale LLMs.",
            "score": 32,
            "issue_id": 2631,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 –º–∞—Ä—Ç–∞",
                "en": "March 10",
                "zh": "3Êúà10Êó•"
            },
            "hash": "eaf9c07e3673cecc",
            "authors": [
                "Xun Liang",
                "Hanyu Wang",
                "Huayi Lai",
                "Simin Niu",
                "Shichao Song",
                "Jiawei Yang",
                "Jihao Zhao",
                "Feiyu Xiong",
                "Bo Tang",
                "Zhiyu Li"
            ],
            "affiliations": [
                "Institute for Advanced Algorithms Research, Shanghai, China",
                "School of Information, Renmin University of China, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07605.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#training"
                ],
                "emoji": "‚úÇÔ∏è",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Sparse Expert Activation Pruning (SEAP) –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. SEAP –≤—ã–±–æ—Ä–æ—á–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–Ω–∏–∑–∏—Ç—å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ. –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –≤—ã—è–≤–ª–µ–Ω–∏–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –¥–ª—è –∑–∞–¥–∞—á–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ SEAP –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏."
                },
                "en": {
                    "title": "Optimize LLMs with Sparse Expert Activation Pruning!",
                    "desc": "This paper presents Sparse Expert Activation Pruning (SEAP), a novel method designed to reduce the computational cost of large language models (LLMs) during inference without requiring additional training. SEAP works by identifying and retaining only the parameters that are relevant to specific tasks, effectively pruning the model while maintaining its performance. The method leverages the clustering patterns of hidden states and activations to optimize the model's efficiency. Experimental results show that SEAP can significantly lower computational overhead while achieving competitive accuracy, making it a scalable solution for optimizing LLMs."
                },
                "zh": {
                    "title": "Á®ÄÁñè‰∏ìÂÆ∂ÊøÄÊ¥ªÂâ™ÊûùÔºö‰ºòÂåñÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÊàêÂäüÔºå‰ΩÜÊé®ÁêÜÊó∂ÁöÑÈ´òËÆ°ÁÆóÊàêÊú¨‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™‰∏ªË¶ÅÁì∂È¢à„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Á®ÄÁñè‰∏ìÂÆ∂ÊøÄÊ¥ªÂâ™ÊûùÔºàSEAPÔºâÁöÑÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ïÂú®‰∏çÈúÄË¶ÅËÆ≠ÁªÉÁöÑÊÉÖÂÜµ‰∏ãÔºåÈÄâÊã©ÊÄßÂú∞‰øùÁïô‰∏é‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÂèÇÊï∞Ôºå‰ª•ÂáèÂ∞ëÊé®ÁêÜÂºÄÈîÄ„ÄÇSEAPÈÄöËøáÂàÜÊûêÈöêËóèÁä∂ÊÄÅÂíåÊøÄÊ¥ªÁöÑËÅöÁ±ªÊ®°ÂºèÔºåËØÜÂà´ÁâπÂÆö‰ªªÂä°ÁöÑ‰∏ìÂÆ∂ÊøÄÊ¥ªÊ®°ÂºèÔºå‰ªéËÄåÂú®‰øùÊåÅ‰ªªÂä°ÊÄßËÉΩÁöÑÂêåÊó∂‰ºòÂåñËÆ°ÁÆóÊïàÁéá„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSEAPÂú®ÂáèÂ∞ëËÆ°ÁÆóÂºÄÈîÄÁöÑÂêåÊó∂Ôºå‰øùÊåÅ‰∫ÜÁ´û‰∫âÂäõÁöÑÂáÜÁ°ÆÊÄßÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®‰ºòÂåñÂ§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÊñπÈù¢ÁöÑÊΩúÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07002",
            "title": "Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue\n  Learning",
            "url": "https://huggingface.co/papers/2503.07002",
            "abstract": "Multimodal large language models (MLLMs), built on large-scale pre-trained vision towers and language models, have shown great capabilities in multimodal understanding. However, most existing MLLMs are trained on single-turn vision question-answering tasks, which do not accurately reflect real-world human conversations. In this paper, we introduce MMDiag, a multi-turn multimodal dialogue dataset. This dataset is collaboratively generated through deliberately designed rules and GPT assistance, featuring strong correlations between questions, between questions and images, and among different image regions; thus aligning more closely with real-world scenarios. MMDiag serves as a strong benchmark for multi-turn multimodal dialogue learning and brings more challenges to the grounding and reasoning capabilities of MLLMs. Further, inspired by human vision processing, we present DiagNote, an MLLM equipped with multimodal grounding and reasoning capabilities. DiagNote consists of two modules (Deliberate and Gaze) interacting with each other to perform Chain-of-Thought and annotations respectively, throughout multi-turn dialogues. We empirically demonstrate the advantages of DiagNote in both grounding and jointly processing and reasoning with vision and language information over existing MLLMs.",
            "score": 29,
            "issue_id": 2631,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 –º–∞—Ä—Ç–∞",
                "en": "March 10",
                "zh": "3Êúà10Êó•"
            },
            "hash": "1f57fb5c5398efbc",
            "authors": [
                "Jiazheng Liu",
                "Sipeng Zheng",
                "B√∂rje F. Karlsson",
                "Zongqing Lu"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence",
                "School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07002.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#games",
                    "#multimodal",
                    "#dataset",
                    "#architecture"
                ],
                "emoji": "üó£Ô∏è",
                "ru": {
                    "title": "–ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –¥–∏–∞–ª–æ–≥–∞–º: –æ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∫ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º –º–æ–¥–µ–ª—è–º",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MMDiag - –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤, —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Å –ø–æ–º–æ—â—å—é GPT. –≠—Ç–æ—Ç –¥–∞—Ç–∞—Å–µ—Ç –ª—É—á—à–µ –æ—Ç—Ä–∞–∂–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –æ–±—â–µ–Ω–∏—è, —á–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ–¥–Ω–æ—Ç—É—Ä–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö –∑–∞–¥–∞—á. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç DiagNote - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ –∑–∞–∑–µ–º–ª–µ–Ω–∏—è –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. DiagNote –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥—É–ª—è –¥–ª—è —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –≤ —Ö–æ–¥–µ –¥–∏–∞–ª–æ–≥–∞."
                },
                "en": {
                    "title": "Enhancing Multimodal Dialogue with MMDiag and DiagNote",
                    "desc": "This paper presents MMDiag, a new dataset designed for multi-turn multimodal dialogue, which enhances the training of multimodal large language models (MLLMs). Unlike previous datasets that focus on single-turn tasks, MMDiag captures the complexities of real-world conversations by incorporating strong correlations between questions and images. The authors introduce DiagNote, an MLLM that features two interactive modules for improved grounding and reasoning in dialogues. The results show that DiagNote outperforms existing MLLMs in processing and reasoning with both visual and textual information."
                },
                "zh": {
                    "title": "ÊèêÂçáÂ§öÊ®°ÊÄÅÂØπËØùÁöÑÊô∫ËÉΩÂåñ",
                    "desc": "ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ§öÊ®°ÊÄÅÂØπËØùÊï∞ÊçÆÈõÜMMDiagÔºåÊó®Âú®ÊîπÂñÑÁé∞ÊúâÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®Â§öËΩÆÂØπËØù‰∏≠ÁöÑË°®Áé∞„ÄÇMMDiagÈÄöËøáÁ≤æÂøÉËÆæËÆ°ÁöÑËßÑÂàôÂíåGPTÁöÑËæÖÂä©ÁîüÊàêÔºåÂåÖÂê´‰∫ÜÈóÆÈ¢ò‰πãÈó¥„ÄÅÈóÆÈ¢ò‰∏éÂõæÂÉè‰πãÈó¥‰ª•Âèä‰∏çÂêåÂõæÂÉèÂå∫Âüü‰πãÈó¥ÁöÑÂº∫Áõ∏ÂÖ≥ÊÄßÔºåÊõ¥Ë¥¥ËøëÁúüÂÆûÁöÑ‰∫∫Á±ªÂØπËØùÂú∫ÊôØ„ÄÇËÆ∫ÊñáËøòÊèêÂá∫‰∫ÜDiagNoteÔºå‰∏Ä‰∏™ÂÖ∑Â§áÂ§öÊ®°ÊÄÅÂü∫Á°ÄÂíåÊé®ÁêÜËÉΩÂäõÁöÑMLLMÔºåÂåÖÂê´‰∏§‰∏™Áõ∏‰∫í‰ΩúÁî®ÁöÑÊ®°ÂùóÔºàDeliberateÂíåGazeÔºâÔºåÁî®‰∫éÂú®Â§öËΩÆÂØπËØù‰∏≠ËøõË°åÊÄùÁª¥ÈìæÂíåÊ≥®Èáä„ÄÇÈÄöËøáÂÆûÈ™åËØÅÊòéÔºåDiagNoteÂú®Âü∫Á°ÄÂíåÂÖ±ÂêåÂ§ÑÁêÜËßÜËßâ‰∏éËØ≠Ë®Ä‰ø°ÊÅØÁöÑÊé®ÁêÜËÉΩÂäõ‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑMLLMs„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07314",
            "title": "Automated Movie Generation via Multi-Agent CoT Planning",
            "url": "https://huggingface.co/papers/2503.07314",
            "abstract": "Existing long-form video generation frameworks lack automated planning, requiring manual input for storylines, scenes, cinematography, and character interactions, resulting in high costs and inefficiencies. To address these challenges, we present MovieAgent, an automated movie generation via multi-agent Chain of Thought (CoT) planning. MovieAgent offers two key advantages: 1) We firstly explore and define the paradigm of automated movie/long-video generation. Given a script and character bank, our MovieAgent can generates multi-scene, multi-shot long-form videos with a coherent narrative, while ensuring character consistency, synchronized subtitles, and stable audio throughout the film. 2) MovieAgent introduces a hierarchical CoT-based reasoning process to automatically structure scenes, camera settings, and cinematography, significantly reducing human effort. By employing multiple LLM agents to simulate the roles of a director, screenwriter, storyboard artist, and location manager, MovieAgent streamlines the production pipeline. Experiments demonstrate that MovieAgent achieves new state-of-the-art results in script faithfulness, character consistency, and narrative coherence. Our hierarchical framework takes a step forward and provides new insights into fully automated movie generation. The code and project website are available at: https://github.com/showlab/MovieAgent and https://weijiawu.github.io/MovieAgent.",
            "score": 23,
            "issue_id": 2631,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 –º–∞—Ä—Ç–∞",
                "en": "March 10",
                "zh": "3Êúà10Êó•"
            },
            "hash": "2e5c000925250863",
            "authors": [
                "Weijia Wu",
                "Zeyu Zhu",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07314.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#multimodal",
                    "#story_generation",
                    "#video",
                    "#agents"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–ª—å–º–æ–≤ —Å –ø–æ–º–æ—â—å—é –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤",
                    "desc": "MovieAgent - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π (Chain of Thought). –°–∏—Å—Ç–µ–º–∞ —Å–ø–æ—Å–æ–±–Ω–∞ —Å–æ–∑–¥–∞–≤–∞—Ç—å –º–Ω–æ–≥–æ—Å—Ü–µ–Ω–Ω—ã–µ —Ñ–∏–ª—å–º—ã —Å —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–º —Å—é–∂–µ—Ç–æ–º, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Å—Ç–æ—è–Ω—Å—Ç–≤–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é —Å—É–±—Ç–∏—Ç—Ä–æ–≤. MovieAgent –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è —Å—Ü–µ–Ω –∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –∫–∞–º–µ—Ä—ã, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ —É—Å–∏–ª–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ MovieAgent –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–æ–≤—ã—Ö –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –≤–µ—Ä–Ω–æ—Å—Ç–∏ —Å—Ü–µ–Ω–∞—Ä–∏—é, –ø–æ—Å—Ç–æ—è–Ω—Å—Ç–≤–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è."
                },
                "en": {
                    "title": "Automating Movie Magic with MovieAgent!",
                    "desc": "This paper introduces MovieAgent, a novel framework for automated long-form video generation that eliminates the need for manual planning of storylines and scenes. It utilizes a multi-agent Chain of Thought (CoT) reasoning process to create coherent narratives while maintaining character consistency and synchronized audio. By simulating various production roles, MovieAgent streamlines the filmmaking process, significantly reducing the effort required from human creators. Experimental results show that MovieAgent sets new benchmarks in script adherence, character consistency, and overall narrative quality."
                },
                "zh": {
                    "title": "Ëá™Âä®ÂåñÁîµÂΩ±ÁîüÊàêÁöÑÊñ∞Á∫™ÂÖÉ",
                    "desc": "Áé∞ÊúâÁöÑÈïøËßÜÈ¢ëÁîüÊàêÊ°ÜÊû∂Áº∫‰πèËá™Âä®ÂåñËßÑÂàíÔºåÈúÄË¶ÅÊâãÂä®ËæìÂÖ•ÊïÖ‰∫ãÊÉÖËäÇ„ÄÅÂú∫ÊôØ„ÄÅÊëÑÂΩ±ÂíåËßíËâ≤‰∫íÂä®ÔºåÂØºËá¥È´òÊàêÊú¨Âíå‰ΩéÊïàÁéá„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜMovieAgentÔºåÈÄöËøáÂ§öÊô∫ËÉΩ‰ΩìÁöÑÊÄùÁª¥ÈìæÔºàCoTÔºâËßÑÂàíÂÆûÁé∞Ëá™Âä®ÂåñÁîµÂΩ±ÁîüÊàê„ÄÇMovieAgentÁöÑ‰∏§‰∏™‰∏ªË¶Å‰ºòÂäøÊòØÔºöÈ¶ñÂÖàÔºåÊàë‰ª¨Êé¢Á¥¢Âπ∂ÂÆö‰πâ‰∫ÜËá™Âä®ÂåñÁîµÂΩ±/ÈïøËßÜÈ¢ëÁîüÊàêÁöÑËåÉÂºèÔºõÂÖ∂Ê¨°ÔºåMovieAgentÂºïÂÖ•‰∫ÜÂü∫‰∫éÂ±ÇÊ¨°ÁöÑCoTÊé®ÁêÜËøáÁ®ãÔºåËá™Âä®ÊûÑÂª∫Âú∫ÊôØ„ÄÅÊëÑÂÉèÊú∫ËÆæÁΩÆÂíåÊëÑÂΩ±ÔºåÂ§ßÂ§ßÂáèÂ∞ë‰∫Ü‰∫∫ÂäõÊäïÂÖ•„ÄÇÂÆûÈ™åË°®ÊòéÔºåMovieAgentÂú®ÂâßÊú¨Âø†ÂÆûÂ∫¶„ÄÅËßíËâ≤‰∏ÄËá¥ÊÄßÂíåÂèô‰∫ãËøûË¥ØÊÄßÊñπÈù¢ËææÂà∞‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÊ∞¥Âπ≥„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07216",
            "title": "FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA\n  Subparameter Updates",
            "url": "https://huggingface.co/papers/2503.07216",
            "abstract": "Federated Learning (FL) is a widely used framework for training models in a decentralized manner, ensuring that the central server does not have direct access to data from local clients. However, this approach may still fail to fully preserve data privacy, as models from local clients are exposed to the central server during the aggregation process. This issue becomes even more critical when training vision-language models (VLMs) with FL, as VLMs can easily memorize training data instances, making them vulnerable to membership inference attacks (MIAs). To address this challenge, we propose the FedRand framework, which avoids disclosing the full set of client parameters. In this framework, each client randomly selects subparameters of Low-Rank Adaptation (LoRA) from the server and keeps the remaining counterparts of the LoRA weights as private parameters. After training both parameters on the client's private dataset, only the non-private <PRE_TAG>client parameters</POST_TAG> are sent back to the server for aggregation. This approach mitigates the risk of exposing client-side VLM parameters, thereby enhancing data privacy. We empirically validate that FedRand improves robustness against MIAs compared to relevant baselines while achieving accuracy comparable to methods that communicate full LoRA parameters across several benchmark datasets.",
            "score": 21,
            "issue_id": 2631,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 –º–∞—Ä—Ç–∞",
                "en": "March 10",
                "zh": "3Êúà10Êó•"
            },
            "hash": "3d23c61b24a599ee",
            "authors": [
                "Sangwoo Park",
                "Seanie Lee",
                "Byungjoo Kim",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "Graduate School of AI, KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07216.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#benchmark",
                    "#data",
                    "#ethics",
                    "#multimodal",
                    "#rl"
                ],
                "emoji": "üîê",
                "ru": {
                    "title": "FedRand: –ó–∞—â–∏—Ç–∞ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏ –≤ —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é (FL) –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VLM), –Ω–∞–∑—ã–≤–∞–µ–º—ã–π FedRand. –≠—Ç–∞ –º–µ—Ç–æ–¥–∏–∫–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∞ –Ω–∞ –ø–æ–≤—ã—à–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –ø—É—Ç–µ–º —Å–ª—É—á–∞–π–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ –ø–æ–¥–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ Low-Rank Adaptation (LoRA) –¥–ª—è –æ–±–º–µ–Ω–∞ –º–µ–∂–¥—É –∫–ª–∏–µ–Ω—Ç–∞–º–∏ –∏ —Å–µ—Ä–≤–µ—Ä–æ–º. FedRand –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —á–∞—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ LoRA –ø—Ä–∏–≤–∞—Ç–Ω—ã–º–∏, —Å–Ω–∏–∂–∞—è —Ä–∏—Å–∫ —É—Ç–µ—á–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–∞–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ FedRand –ø–æ–≤—ã—à–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –∞—Ç–∞–∫–∞–º –ø–æ –≤—ã–≤–æ–¥—É —á–ª–µ–Ω—Å—Ç–≤–∞ (MIA) –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—É—é —Ç–æ—á–Ω–æ—Å—Ç—å."
                },
                "en": {
                    "title": "Enhancing Privacy in Federated Learning with FedRand",
                    "desc": "Federated Learning (FL) allows models to be trained on local data without sharing it with a central server, but it can still risk data privacy during model aggregation. This paper introduces the FedRand framework, which enhances privacy by having clients send only a subset of their model parameters back to the server. Specifically, it utilizes Low-Rank Adaptation (LoRA) to keep certain parameters private while still allowing effective model training. The results show that FedRand not only protects against membership inference attacks but also maintains accuracy similar to traditional methods that share full parameters."
                },
                "zh": {
                    "title": "FedRandÔºö‰øùÊä§Êï∞ÊçÆÈöêÁßÅÁöÑËÅîÈÇ¶Â≠¶‰π†Êñ∞Ê°ÜÊû∂",
                    "desc": "ËÅîÈÇ¶Â≠¶‰π†ÔºàFLÔºâÊòØ‰∏ÄÁßçÂéª‰∏≠ÂøÉÂåñÁöÑÊ®°ÂûãËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÁ°Æ‰øù‰∏≠Â§ÆÊúçÂä°Âô®Êó†Ê≥ïÁõ¥Êé•ËÆøÈóÆÊú¨Âú∞ÂÆ¢Êà∑Á´ØÁöÑÊï∞ÊçÆ„ÄÇÁÑ∂ËÄåÔºåÂú®ËÅöÂêàËøáÁ®ã‰∏≠ÔºåÊú¨Âú∞ÂÆ¢Êà∑Á´ØÁöÑÊ®°Âûã‰ªçÂèØËÉΩÊö¥Èú≤Áªô‰∏≠Â§ÆÊúçÂä°Âô®Ôºå‰ªéËÄåÂΩ±ÂìçÊï∞ÊçÆÈöêÁßÅ„ÄÇÁâπÂà´ÊòØÂú®ËÆ≠ÁªÉËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÊó∂ÔºåËøôÁßçÈ£éÈô©Êõ¥‰∏∫‰∏•ÈáçÔºåÂõ†‰∏∫VLMsÂÆπÊòìËÆ∞‰ΩèËÆ≠ÁªÉÊï∞ÊçÆÂÆû‰æãÔºåÂÆπÊòìÂèóÂà∞ÊàêÂëòÊé®Êñ≠ÊîªÂáªÔºàMIAsÔºâ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜFedRandÊ°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂ÈÄöËøáÈöèÊú∫ÈÄâÊã©‰ΩéÁß©ÈÄÇÂ∫îÔºàLoRAÔºâÁöÑÂ≠êÂèÇÊï∞ÔºåÈÅøÂÖçÊ≥ÑÈú≤ÂÆåÊï¥ÁöÑÂÆ¢Êà∑Á´ØÂèÇÊï∞Ôºå‰ªéËÄåÂ¢ûÂº∫Êï∞ÊçÆÈöêÁßÅ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07067",
            "title": "DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs",
            "url": "https://huggingface.co/papers/2503.07067",
            "abstract": "Despite the success of distillation in large language models (LLMs), most prior work applies identical loss functions to both teacher- and student-generated data. These strategies overlook the synergy between loss formulations and data types, leading to a suboptimal performance boost in student models. To address this, we propose DistiLLM-2, a contrastive approach that simultaneously increases the likelihood of teacher responses and decreases that of student responses by harnessing this synergy. Our extensive experiments show that DistiLLM-2 not only builds high-performing student models across a wide range of tasks, including instruction-following and code generation, but also supports diverse applications, such as preference alignment and vision-language extensions. These findings highlight the potential of a contrastive approach to enhance the efficacy of LLM distillation by effectively aligning teacher and student models across varied data types.",
            "score": 18,
            "issue_id": 2632,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 –º–∞—Ä—Ç–∞",
                "en": "March 10",
                "zh": "3Êúà10Êó•"
            },
            "hash": "8eb39990e619611e",
            "authors": [
                "Jongwoo Ko",
                "Tianyi Chen",
                "Sungnyun Kim",
                "Tianyu Ding",
                "Luming Liang",
                "Ilya Zharkov",
                "Se-Young Yun"
            ],
            "affiliations": [
                "KAIST AI",
                "Microsoft"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07067.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#alignment",
                    "#training",
                    "#optimization"
                ],
                "emoji": "üî¨",
                "ru": {
                    "title": "–ö–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "DistiLLM-2 - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ. –û–Ω –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤ —É—á–∏—Ç–µ–ª—è –∏ —É–º–µ–Ω—å—à–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤ —É—á–µ–Ω–∏–∫–∞, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—è —Å–∏–Ω–µ—Ä–≥–∏—é –º–µ–∂–¥—É —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ —Ç–∏–ø–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ DistiLLM-2 —Å–æ–∑–¥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏-—É—á–µ–Ω–∏–∫–∏ –¥–ª—è —à–∏—Ä–æ–∫–æ–≥–æ —Å–ø–µ–∫—Ç—Ä–∞ –∑–∞–¥–∞—á, –≤–∫–ª—é—á–∞—è —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ–¥–∞. –ú–µ—Ç–æ–¥ —Ç–∞–∫–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏."
                },
                "en": {
                    "title": "Enhancing LLM Distillation with Contrastive Learning",
                    "desc": "This paper introduces DistiLLM-2, a novel approach to improve the distillation process in large language models (LLMs). Unlike previous methods that use the same loss functions for both teacher and student models, DistiLLM-2 employs a contrastive strategy that boosts the likelihood of correct teacher responses while reducing the likelihood of incorrect student responses. The results demonstrate that this method significantly enhances the performance of student models on various tasks, including instruction-following and code generation. Additionally, DistiLLM-2 shows versatility in applications such as preference alignment and vision-language tasks, emphasizing the importance of tailored loss functions in model distillation."
                },
                "zh": {
                    "title": "ÂØπÊØîÊñπÊ≥ïÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËí∏È¶èÊïàÊûú",
                    "desc": "Â∞ΩÁÆ°Ëí∏È¶èÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠ÂèñÂæó‰∫ÜÊàêÂäüÔºå‰ΩÜÂ§ßÂ§öÊï∞ÂÖàÂâçÁöÑÁ†îÁ©∂ÂØπÊïôÂ∏àÂíåÂ≠¶ÁîüÁîüÊàêÁöÑÊï∞ÊçÆ‰ΩøÁî®Áõ∏ÂêåÁöÑÊçüÂ§±ÂáΩÊï∞„ÄÇËøôÁßçÁ≠ñÁï•ÂøΩËßÜ‰∫ÜÊçüÂ§±ÂÖ¨Âºè‰∏éÊï∞ÊçÆÁ±ªÂûã‰πãÈó¥ÁöÑÂçèÂêå‰ΩúÁî®ÔºåÂØºËá¥Â≠¶ÁîüÊ®°ÂûãÁöÑÊÄßËÉΩÊèêÂçá‰∏çÁêÜÊÉ≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜDistiLLM-2ÔºåËøôÊòØ‰∏ÄÁßçÂØπÊØîÊñπÊ≥ïÔºåËÉΩÂ§üÂêåÊó∂ÊèêÈ´òÊïôÂ∏àÂìçÂ∫îÁöÑÂèØËÉΩÊÄßÂπ∂Èôç‰ΩéÂ≠¶ÁîüÂìçÂ∫îÁöÑÂèØËÉΩÊÄß„ÄÇÊàë‰ª¨ÁöÑÂπøÊ≥õÂÆûÈ™åË°®ÊòéÔºåDistiLLM-2‰∏ç‰ªÖÂú®Â§öÁßç‰ªªÂä°‰∏≠ÊûÑÂª∫‰∫ÜÈ´òÊÄßËÉΩÁöÑÂ≠¶ÁîüÊ®°ÂûãÔºåËøòÊîØÊåÅÂ§öÊ†∑ÂåñÁöÑÂ∫îÁî®ÔºåÂ¶ÇÂÅèÂ•ΩÂØπÈΩêÂíåËßÜËßâ-ËØ≠Ë®ÄÊâ©Â±ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.06680",
            "title": "FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation\n  for Feature Implementation",
            "url": "https://huggingface.co/papers/2503.06680",
            "abstract": "Implementing new features in repository-level codebases is a crucial application of code generation models. However, current benchmarks lack a dedicated evaluation framework for this capability. To fill this gap, we introduce FEA-Bench, a benchmark designed to assess the ability of large language models (LLMs) to perform incremental development within code repositories. We collect pull requests from 83 GitHub repositories and use rule-based and intent-based filtering to construct task instances focused on new feature development. Each task instance containing code changes is paired with relevant unit test files to ensure that the solution can be verified. The feature implementation requires LLMs to simultaneously possess code completion capabilities for new components and code editing abilities for other relevant parts in the code repository, providing a more comprehensive evaluation method of LLMs' automated software engineering capabilities. Experimental results show that LLMs perform significantly worse in the FEA-Bench, highlighting considerable challenges in such repository-level incremental code development.",
            "score": 14,
            "issue_id": 2630,
            "pub_date": "2025-03-09",
            "pub_date_card": {
                "ru": "9 –º–∞—Ä—Ç–∞",
                "en": "March 9",
                "zh": "3Êúà9Êó•"
            },
            "hash": "4394ce17a18696a3",
            "authors": [
                "Wei Li",
                "Xin Zhang",
                "Zhongxin Guo",
                "Shaoguang Mao",
                "Wen Luo",
                "Guangyue Peng",
                "Yangyu Huang",
                "Houfeng Wang",
                "Scarlett Li"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "State Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.06680.jpg",
            "data": {
                "categories": [
                    "#plp",
                    "#dataset",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "üß™",
                "ru": {
                    "title": "FEA-Bench: –Ω–æ–≤—ã–π –≤—ã–∑–æ–≤ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ü–û",
                    "desc": "FEA-Bench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤—ã–ø–æ–ª–Ω—è—Ç—å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—É—é —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è—Ö –∫–æ–¥–∞. –û–Ω –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –ø—É–ª-—Ä–µ–∫–≤–µ—Å—Ç–∞—Ö –∏–∑ 83 —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤ GitHub –∏ –≤–∫–ª—é—á–∞–µ—Ç –∑–∞–¥–∞—á–∏ –ø–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –Ω–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π. –ö–∞–∂–¥–∞—è –∑–∞–¥–∞—á–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫–æ–¥–∞ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥—É–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ LLM –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Ö—É–∂–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –∑–∞–¥–∞—á–∞–º–∏ FEA-Bench, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Å–µ—Ä—å–µ–∑–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤."
                },
                "en": {
                    "title": "FEA-Bench: Evaluating Code Generation in Repositories",
                    "desc": "This paper introduces FEA-Bench, a new benchmark for evaluating large language models (LLMs) in the context of code generation for new features in software repositories. It addresses the lack of dedicated evaluation frameworks by collecting pull requests from 83 GitHub repositories and creating task instances that focus on incremental development. Each task is designed to test the LLM's ability to generate code changes while ensuring that these changes can be verified through associated unit tests. The findings reveal that LLMs struggle with this type of repository-level code development, indicating significant challenges in their automated software engineering capabilities."
                },
                "zh": {
                    "title": "ËØÑ‰º∞‰ª£Á†ÅÁîüÊàêÊ®°ÂûãÁöÑÊñ∞Âü∫ÂáÜÔºöFEA-Bench",
                    "desc": "Âú®‰ª£Á†ÅÁîüÊàêÊ®°Âûã‰∏≠ÔºåÂÆûÁé∞Êñ∞ÁâπÊÄßÊòØ‰∏Ä‰∏™ÈáçË¶ÅÁöÑÂ∫îÁî®„ÄÇÂΩìÂâçÁöÑÂü∫ÂáÜÊµãËØïÁº∫‰πè‰∏ìÈó®ËØÑ‰º∞Ëøô‰∏ÄËÉΩÂäõÁöÑÊ°ÜÊû∂„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜFEA-BenchÔºåËøôÊòØ‰∏Ä‰∏™Êó®Âú®ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®‰ª£Á†ÅÂ∫ì‰∏≠ËøõË°åÂ¢ûÈáèÂºÄÂèëËÉΩÂäõÁöÑÂü∫ÂáÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLLMsÂú®FEA-Bench‰∏≠ÁöÑË°®Áé∞ÊòæËëóËæÉÂ∑ÆÔºåÁ™ÅÊòæ‰∫ÜÂú®‰ª£Á†ÅÂ∫ìÁ∫ßÂà´Â¢ûÈáèÂºÄÂèë‰∏≠Èù¢‰∏¥ÁöÑÈáçÂ§ßÊåëÊàò„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07027",
            "title": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer",
            "url": "https://huggingface.co/papers/2503.07027",
            "abstract": "Recent advancements in Unet-based diffusion models, such as ControlNet and IP-Adapter, have introduced effective spatial and subject control mechanisms. However, the DiT (Diffusion Transformer) architecture still struggles with efficient and flexible control. To tackle this issue, we propose EasyControl, a novel framework designed to unify condition-guided diffusion transformers with high efficiency and flexibility. Our framework is built on three key innovations. First, we introduce a lightweight <PRE_TAG>Condition Injection LoRA Module</POST_TAG>. This module processes conditional signals in isolation, acting as a plug-and-play solution. It avoids modifying the base model weights, ensuring compatibility with customized models and enabling the flexible injection of diverse conditions. Notably, this module also supports harmonious and robust zero-shot multi-condition generalization, even when trained only on single-condition data. Second, we propose a <PRE_TAG>Position-Aware Training Paradigm</POST_TAG>. This approach standardizes input conditions to fixed resolutions, allowing the generation of images with arbitrary aspect ratios and flexible resolutions. At the same time, it optimizes computational efficiency, making the framework more practical for real-world applications. Third, we develop a Causal Attention Mechanism combined with the KV Cache technique, adapted for conditional generation tasks. This innovation significantly reduces the latency of image synthesis, improving the overall efficiency of the framework. Through extensive experiments, we demonstrate that EasyControl achieves exceptional performance across various application scenarios. These innovations collectively make our framework highly efficient, flexible, and suitable for a wide range of tasks.",
            "score": 12,
            "issue_id": 2630,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 –º–∞—Ä—Ç–∞",
                "en": "March 10",
                "zh": "3Êúà10Êó•"
            },
            "hash": "1b1589f9873720c7",
            "authors": [
                "Yuxuan Zhang",
                "Yirui Yuan",
                "Yiren Song",
                "Haofan Wang",
                "Jiaming Liu"
            ],
            "affiliations": [
                "Liblib AI",
                "National University of Singapore",
                "ShanghaiTech University",
                "Tiamat AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07027.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#architecture",
                    "#training",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "üé®",
                "ru": {
                    "title": "EasyControl: –≥–∏–±–∫–æ–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–º–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞–º–∏",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç EasyControl - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–æ–ª—è –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö. –û—Å–Ω–æ–≤–Ω—ã–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ –≤–∫–ª—é—á–∞—é—Ç –º–æ–¥—É–ª—å –≤–Ω–µ–¥—Ä–µ–Ω–∏—è —É—Å–ª–æ–≤–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ LoRA, –ø–∞—Ä–∞–¥–∏–≥–º—É –æ–±—É—á–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –ø–æ–∑–∏—Ü–∏–∏ –∏ –º–µ—Ö–∞–Ω–∏–∑–º –ø—Ä–∏—á–∏–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è —Å KV-–∫—ç—à–µ–º. EasyControl –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏ –≥–∏–±–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø—Ä–æ–ø–æ—Ä—Ü–∏—è–º–∏ –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è."
                },
                "en": {
                    "title": "EasyControl: Unifying Efficiency and Flexibility in Diffusion Transformers",
                    "desc": "This paper presents EasyControl, a new framework that enhances the efficiency and flexibility of condition-guided diffusion transformers, particularly addressing the limitations of the DiT architecture. It introduces a lightweight Condition Injection LoRA Module that allows for the independent processing of conditional signals without altering the base model, enabling easy integration of various conditions. Additionally, the Position-Aware Training Paradigm standardizes input conditions, facilitating the generation of images with different aspect ratios while optimizing computational resources. Lastly, the Causal Attention Mechanism with KV Cache significantly reduces image synthesis latency, making EasyControl a robust solution for diverse applications in machine learning."
                },
                "zh": {
                    "title": "È´òÊïàÁÅµÊ¥ªÁöÑÊù°‰ª∂ÁîüÊàêÊ°ÜÊû∂EasyControl",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫EasyControlÁöÑÊñ∞Ê°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òÂü∫‰∫éÊâ©Êï£ÂèòÊç¢Âô®ÁöÑÊù°‰ª∂ÁîüÊàêÊ®°ÂûãÁöÑÊïàÁéáÂíåÁÅµÊ¥ªÊÄß„ÄÇËØ•Ê°ÜÊû∂ÂåÖÂê´‰∏â‰∏™ÂÖ≥ÈîÆÂàõÊñ∞ÔºöÈ¶ñÂÖàÔºåËΩªÈáèÁ∫ßÁöÑÊù°‰ª∂Ê≥®ÂÖ•LoRAÊ®°ÂùóÂèØ‰ª•Áã¨Á´ãÂ§ÑÁêÜÊù°‰ª∂‰ø°Âè∑ÔºåÈÅøÂÖç‰øÆÊîπÂü∫Á°ÄÊ®°ÂûãÊùÉÈáçÔºå‰ªéËÄåÂÆûÁé∞‰∏éÂÆöÂà∂Ê®°ÂûãÁöÑÂÖºÂÆπÊÄß„ÄÇÂÖ∂Ê¨°Ôºå‰ΩçÁΩÆÊÑüÁü•ËÆ≠ÁªÉËåÉÂºèÊ†áÂáÜÂåñËæìÂÖ•Êù°‰ª∂Ôºå‰ΩøÂæóÁîüÊàê‰ªªÊÑèÈïøÂÆΩÊØîÂíåÁÅµÊ¥ªÂàÜËæ®ÁéáÁöÑÂõæÂÉèÊàê‰∏∫ÂèØËÉΩÔºåÂêåÊó∂‰ºòÂåñËÆ°ÁÆóÊïàÁéá„ÄÇÊúÄÂêéÔºåÁªìÂêàKVÁºìÂ≠òÊäÄÊúØÁöÑÂõ†ÊûúÊ≥®ÊÑèÊú∫Âà∂ÊòæËëóÈôç‰Ωé‰∫ÜÂõæÂÉèÂêàÊàêÁöÑÂª∂ËøüÔºåÊèêÂçá‰∫ÜÊï¥‰ΩìÊïàÁéá„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.06580",
            "title": "Agent models: Internalizing Chain-of-Action Generation into Reasoning\n  models",
            "url": "https://huggingface.co/papers/2503.06580",
            "abstract": "Traditional agentic workflows rely on external prompts to manage interactions with tools and the environment, which limits the autonomy of reasoning models. We position Large Agent Models (LAMs) that internalize the generation of Chain-of-Action (CoA), enabling the model to autonomously decide when and how to use external tools. Our proposed AutoCoA framework combines supervised fine-tuning (SFT) and reinforcement learning (RL), allowing the model to seamlessly switch between reasoning and action while efficiently managing environment interactions. Main components include step-level action triggering, trajectory-level CoA optimization, and an internal world model to reduce real-environment interaction costs. Evaluations on open-domain QA tasks demonstrate that AutoCoA-trained agent models significantly outperform ReAct-based workflows in task completion, especially in tasks that require long-term reasoning and multi-step actions. Code and dataset are available at https://github.com/ADaM-BJTU/AutoCoA",
            "score": 10,
            "issue_id": 2631,
            "pub_date": "2025-03-09",
            "pub_date_card": {
                "ru": "9 –º–∞—Ä—Ç–∞",
                "en": "March 9",
                "zh": "3Êúà9Êó•"
            },
            "hash": "01588376bab86ceb",
            "authors": [
                "Yuxiang Zhang",
                "Yuqi Yang",
                "Jiangming Shu",
                "Xinyan Wen",
                "Jitao Sang"
            ],
            "affiliations": [
                "School of Computer Science and Technology, Beijing Jiaotong University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.06580.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#agents",
                    "#rl",
                    "#training"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –∞–≥–µ–Ω—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏: –Ω–æ–≤—ã–π —à–∞–≥ –∫ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ–º—É –ò–ò",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∞–≥–µ–Ω—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –Ω–∞–∑—ã–≤–∞–µ–º—ã—Ö Large Agent Models (LAM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ AutoCoA, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ü–µ–ø–æ—á–∫–∏ –¥–µ–π—Å—Ç–≤–∏–π –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫. –§—Ä–µ–π–º–≤–æ—Ä–∫ —Å–æ—á–µ—Ç–∞–µ—Ç –≤ —Å–µ–±–µ –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º –∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–æ–¥–µ–ª–∏ —Å –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥–æ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é AutoCoA, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –≤ –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π."
                },
                "en": {
                    "title": "Empowering Autonomous Reasoning with AutoCoA",
                    "desc": "This paper introduces Large Agent Models (LAMs) that enhance the autonomy of reasoning models by allowing them to generate their own Chain-of-Action (CoA) without relying on external prompts. The AutoCoA framework integrates supervised fine-tuning (SFT) and reinforcement learning (RL) to enable models to effectively alternate between reasoning and taking actions in their environment. Key features of this framework include mechanisms for triggering actions at each step, optimizing CoA over entire trajectories, and utilizing an internal world model to minimize the costs associated with real-world interactions. Evaluations show that models trained with AutoCoA outperform traditional ReAct-based workflows, particularly in complex tasks requiring extended reasoning and multiple actions."
                },
                "zh": {
                    "title": "Ëá™‰∏ªÂÜ≥Á≠ñÁöÑÊô∫ËÉΩ‰ª£ÁêÜÊ®°Âûã",
                    "desc": "‰º†ÁªüÁöÑÊô∫ËÉΩÂ∑•‰ΩúÊµÅÁ®ã‰æùËµñÂ§ñÈÉ®ÊèêÁ§∫Êù•ÁÆ°ÁêÜ‰∏éÂ∑•ÂÖ∑ÂíåÁéØÂ¢ÉÁöÑ‰∫§‰∫íÔºåËøôÈôêÂà∂‰∫ÜÊé®ÁêÜÊ®°ÂûãÁöÑËá™‰∏ªÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜÂ§ßÂûã‰ª£ÁêÜÊ®°ÂûãÔºàLAMsÔºâÔºå‰ΩøÂÖ∂ËÉΩÂ§üÂÜÖÈÉ®ÁîüÊàêË°åÂä®ÈìæÔºàCoAÔºâÔºå‰ªéËÄåËá™‰∏ªÂÜ≥ÂÆö‰ΩïÊó∂‰ª•ÂèäÂ¶Ç‰Ωï‰ΩøÁî®Â§ñÈÉ®Â∑•ÂÖ∑„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑAutoCoAÊ°ÜÊû∂ÁªìÂêà‰∫ÜÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÂíåÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÔºå‰ΩøÊ®°ÂûãËÉΩÂ§üÂú®Êé®ÁêÜÂíåË°åÂä®‰πãÈó¥Êó†ÁºùÂàáÊç¢ÔºåÂêåÊó∂ÊúâÊïàÁÆ°ÁêÜ‰∏éÁéØÂ¢ÉÁöÑ‰∫§‰∫í„ÄÇËØÑ‰º∞ÁªìÊûúË°®ÊòéÔºåÁªèËøáAutoCoAËÆ≠ÁªÉÁöÑ‰ª£ÁêÜÊ®°ÂûãÂú®ÂºÄÊîæÈ¢ÜÂüüÈóÆÁ≠î‰ªªÂä°‰∏≠ÊòæËëó‰ºò‰∫éÂü∫‰∫éReActÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÈïøÊúüÊé®ÁêÜÂíåÂ§öÊ≠•È™§Ë°åÂä®ÁöÑ‰ªªÂä°‰∏≠„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04629",
            "title": "SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and\n  Multi-dimensional Evaluation for Automated Survey Writing",
            "url": "https://huggingface.co/papers/2503.04629",
            "abstract": "Survey paper plays a crucial role in scientific research, especially given the rapid growth of research publications. Recently, researchers have begun using LLMs to automate survey generation for better efficiency. However, the quality gap between LLM-generated surveys and those written by human remains significant, particularly in terms of outline quality and citation accuracy. To close these gaps, we introduce SurveyForge, which first generates the outline by analyzing the logical structure of human-written outlines and referring to the retrieved domain-related articles. Subsequently, leveraging high-quality papers retrieved from memory by our scholar navigation agent, SurveyForge can automatically generate and refine the content of the generated article. Moreover, to achieve a comprehensive evaluation, we construct SurveyBench, which includes 100 human-written survey papers for win-rate comparison and assesses AI-generated survey papers across three dimensions: reference, outline, and content quality. Experiments demonstrate that SurveyForge can outperform previous works such as AutoSurvey.",
            "score": 9,
            "issue_id": 2633,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 –º–∞—Ä—Ç–∞",
                "en": "March 6",
                "zh": "3Êúà6Êó•"
            },
            "hash": "a229855316ab195d",
            "authors": [
                "Xiangchao Yan",
                "Shiyang Feng",
                "Jiakang Yuan",
                "Renqiu Xia",
                "Bin Wang",
                "Bo Zhang",
                "Lei Bai"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04629.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#multimodal",
                    "#agents",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "üìä",
                "ru": {
                    "title": "SurveyForge: –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–±–∑–æ—Ä–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π —Å –ø–æ–º–æ—â—å—é –ò–ò",
                    "desc": "SurveyForge - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º—É —Å–æ–∑–¥–∞–Ω–∏—é –æ–±–∑–æ—Ä–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –°–∏—Å—Ç–µ–º–∞ —Å–Ω–∞—á–∞–ª–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—Ç–∞—Ç—å–∏, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—è –ª–æ–≥–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ–±–∑–æ—Ä–æ–≤, –Ω–∞–ø–∏—Å–∞–Ω–Ω—ã—Ö –ª—é–¥—å–º–∏, –∏ –æ–±—Ä–∞—â–∞—è—Å—å –∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º —Å—Ç–∞—Ç—å—è–º –≤ –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏. –ó–∞—Ç–µ–º, –∏—Å–ø–æ–ª—å–∑—É—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏, –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –∏–∑ –ø–∞–º—è—Ç–∏ —Å –ø–æ–º–æ—â—å—é –∞–≥–µ–Ω—Ç–∞ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏, SurveyForge –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏ —É–ª—É—á—à–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏. –î–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–æ–∑–¥–∞–Ω –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö SurveyBench, –≤–∫–ª—é—á–∞—é—â–∏–π 100 –æ–±–∑–æ—Ä–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π, –Ω–∞–ø–∏—Å–∞–Ω–Ω—ã—Ö –ª—é–¥—å–º–∏, –∏ –æ—Ü–µ–Ω–∏–≤–∞—é—â–∏–π —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ò–ò –æ–±–∑–æ—Ä—ã –ø–æ —Ç—Ä–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º: –∫–∞—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫, —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è."
                },
                "en": {
                    "title": "Enhancing Automated Survey Generation with SurveyForge",
                    "desc": "This paper introduces SurveyForge, a tool designed to improve the quality of automated survey generation using large language models (LLMs). It addresses the significant quality gap between LLM-generated surveys and those created by humans, particularly in outline structure and citation accuracy. SurveyForge first generates an outline by analyzing human-written surveys and retrieving relevant articles, then it refines the content using high-quality papers. The authors also present SurveyBench, a benchmark for evaluating survey papers, which shows that SurveyForge outperforms previous methods like AutoSurvey."
                },
                "zh": {
                    "title": "SurveyForgeÔºöÊèêÂçáÊñáÁåÆÁªºËø∞ÁîüÊàêË¥®ÈáèÁöÑÂà©Âô®",
                    "desc": "ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫ÜSurveyForgeÔºå‰∏Ä‰∏™Áî®‰∫éËá™Âä®ÁîüÊàêÊñáÁåÆÁªºËø∞ÁöÑÂ∑•ÂÖ∑„ÄÇÂÆÉÈÄöËøáÂàÜÊûê‰∫∫Á±ªÊí∞ÂÜôÁöÑÁªºËø∞Â§ßÁ∫≤ÁöÑÈÄªËæëÁªìÊûÑÔºåÂπ∂ÂèÇËÄÉÁõ∏ÂÖ≥È¢ÜÂüüÁöÑÊñáÁ´†ÔºåÈ¶ñÂÖàÁîüÊàêÂ§ßÁ∫≤„ÄÇÁÑ∂ÂêéÔºåSurveyForgeÂà©Áî®È´òË¥®ÈáèÁöÑËÆ∫ÊñáÊù•Ëá™Âä®ÁîüÊàêÂíåÂÆåÂñÑÊñáÁ´†ÂÜÖÂÆπ„ÄÇÁ†îÁ©∂ËøòÊûÑÂª∫‰∫ÜSurveyBenchÔºåÁî®‰∫éËØÑ‰º∞AIÁîüÊàêÁöÑÁªºËø∞ËÆ∫ÊñáÂú®ÂèÇËÄÉÊñáÁåÆ„ÄÅÁªìÊûÑÂíåÂÜÖÂÆπË¥®ÈáèÁ≠â‰∏â‰∏™Áª¥Â∫¶ÁöÑË°®Áé∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04812",
            "title": "LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted\n  Contrastive Learning",
            "url": "https://huggingface.co/papers/2503.04812",
            "abstract": "Universal multimodal embedding models play a critical role in tasks such as interleaved image-text retrieval, multimodal RAG, and multimodal clustering. However, our empirical results indicate that existing LMM-based embedding models trained with the standard InfoNCE loss exhibit a high degree of overlap in similarity distribution between positive and negative pairs, making it challenging to distinguish hard negative pairs effectively. To deal with this issue, we propose a simple yet effective framework that dynamically improves the embedding model's representation learning for negative pairs based on their discriminative difficulty. Within this framework, we train a series of models, named LLaVE, and evaluate them on the MMEB benchmark, which covers 4 meta-tasks and 36 datasets. Experimental results show that LLaVE establishes stronger baselines that achieve state-of-the-art (SOTA) performance while demonstrating strong scalability and efficiency. Specifically, LLaVE-2B surpasses the previous SOTA 7B models, while LLaVE-7B achieves a further performance improvement of 6.2 points. Although LLaVE is trained on image-text data, it can generalize to text-video retrieval tasks in a zero-shot manner and achieve strong performance, demonstrating its remarkable potential for transfer to other embedding tasks.",
            "score": 9,
            "issue_id": 2631,
            "pub_date": "2025-03-04",
            "pub_date_card": {
                "ru": "4 –º–∞—Ä—Ç–∞",
                "en": "March 4",
                "zh": "3Êúà4Êó•"
            },
            "hash": "a1fec2227e343e88",
            "authors": [
                "Zhibin Lan",
                "Liqiang Niu",
                "Fandong Meng",
                "Jie Zhou",
                "Jinsong Su"
            ],
            "affiliations": [
                "Pattern Recognition Center, WeChat AI, Tencent Inc, China",
                "School of Informatics, Xiamen University, China",
                "Shanghai Artificial Intelligence Laboratory, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04812.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#benchmark",
                    "#rag",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "üîÄ",
                "ru": {
                    "title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —Ä–∞–∑–ª–∏—á–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø–∞—Ä –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥-–º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ –ø—Ä–æ–±–ª–µ–º—É –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø–∞—Ä –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª—è—Ö. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø–∞—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–µ–Ω–∏—è. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ LLaVE –¥–æ—Å—Ç–∏–≥–∞—é—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ MMEB –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Ö–æ—Ä–æ—à—É—é –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å."
                },
                "en": {
                    "title": "Enhancing Multimodal Embeddings with LLaVE for Better Performance",
                    "desc": "This paper discusses the development of a new framework called LLaVE for improving multimodal embedding models, which are used for tasks involving both images and text. The authors identify a problem with existing models that struggle to differentiate between similar positive and negative pairs due to overlapping similarity distributions. To address this, LLaVE dynamically enhances the learning of negative pairs based on their difficulty, leading to better representation learning. The results show that LLaVE outperforms previous state-of-the-art models and can also be applied effectively to other tasks like text-video retrieval."
                },
                "zh": {
                    "title": "LLaVEÔºöÊèêÂçáÂ§öÊ®°ÊÄÅÂµåÂÖ•ÁöÑÂº∫Â§ßÂ∑•ÂÖ∑",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ§öÊ®°ÊÄÅÂµåÂÖ•Ê®°ÂûãLLaVEÔºåÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÊ®°ÂûãÂú®Â§ÑÁêÜÊ≠£Ë¥üÊ†∑Êú¨Êó∂Áõ∏‰ººÂ∫¶ÂàÜÂ∏ÉÈáçÂè†ÁöÑÈóÆÈ¢ò„ÄÇÈÄöËøáÂä®ÊÄÅË∞ÉÊï¥Ë¥üÊ†∑Êú¨ÁöÑË°®Á§∫Â≠¶‰π†ÔºåLLaVEËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âå∫ÂàÜÂõ∞ÈöæÁöÑË¥üÊ†∑Êú¨„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLLaVEÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåË∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑÊúÄÂÖàËøõÊ®°ÂûãÔºåÂπ∂Âú®ÂõæÂÉè-ÊñáÊú¨Êï∞ÊçÆ‰∏äËÆ≠ÁªÉÂêéÔºåËÉΩÂ§üÂú®Èõ∂Ê†∑Êú¨ÊÉÖÂÜµ‰∏ãÊé®ÂπøÂà∞ÊñáÊú¨-ËßÜÈ¢ëÊ£ÄÁ¥¢‰ªªÂä°„ÄÇËØ•Ê®°ÂûãÂ±ïÁ§∫‰∫ÜÂº∫Â§ßÁöÑÂèØÊâ©Â±ïÊÄßÂíåÊïàÁéáÔºåÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07608",
            "title": "AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via\n  Reinforcement Learning and Reasoning",
            "url": "https://huggingface.co/papers/2503.07608",
            "abstract": "OpenAI o1 and DeepSeek R1 achieve or even surpass human expert-level performance in complex domains like mathematics and science, with reinforcement learning (RL) and reasoning playing a crucial role. In autonomous driving, recent end-to-end models have greatly improved planning performance but still struggle with long-tailed problems due to limited common sense and reasoning abilities. Some studies integrate vision-language models (VLMs) into autonomous driving, but they typically rely on pre-trained models with simple supervised fine-tuning (SFT) on driving data, without further exploration of training strategies or optimizations specifically tailored for planning. In this paper, we propose AlphaDrive, a RL and reasoning framework for VLMs in autonomous driving. AlphaDrive introduces four GRPO-based RL rewards tailored for planning and employs a two-stage planning <PRE_TAG>reasoning training strategy</POST_TAG> that combines SFT with RL. As a result, AlphaDrive significantly improves both planning performance and training efficiency compared to using only SFT or without reasoning. Moreover, we are also excited to discover that, following RL training, AlphaDrive exhibits some emergent multimodal planning capabilities, which is critical for improving driving safety and efficiency. To the best of our knowledge, AlphaDrive is the first to integrate GRPO-based RL with planning reasoning into autonomous driving. Code will be released to facilitate future research.",
            "score": 7,
            "issue_id": 2632,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 –º–∞—Ä—Ç–∞",
                "en": "March 10",
                "zh": "3Êúà10Êó•"
            },
            "hash": "74dfc85ba9f7bcc7",
            "authors": [
                "Bo Jiang",
                "Shaoyu Chen",
                "Qian Zhang",
                "Wenyu Liu",
                "Xinggang Wang"
            ],
            "affiliations": [
                "Horizon Robotics",
                "Huazhong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07608.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#optimization",
                    "#multimodal",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "üöó",
                "ru": {
                    "title": "AlphaDrive: –ò–ò –∑–∞ —Ä—É–ª–µ–º —Å –æ–±—É—á–µ–Ω–∏–µ–º –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º",
                    "desc": "AlphaDrive - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –æ–±—É—á–µ–Ω–∏–µ–º —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏. –û–Ω–∞ –≤–≤–æ–¥–∏—Ç —á–µ—Ç—ã—Ä–µ –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ GRPO, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è, –∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è —Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏. AlphaDrive –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏. –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å–∏—Å—Ç–µ–º–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≤–æ–∑–Ω–∏–∫–∞—é—â–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è."
                },
                "en": {
                    "title": "AlphaDrive: Revolutionizing Autonomous Driving with RL and Reasoning",
                    "desc": "This paper presents AlphaDrive, a novel framework that enhances autonomous driving by integrating reinforcement learning (RL) and reasoning with vision-language models (VLMs). AlphaDrive employs four GRPO-based RL rewards specifically designed for planning tasks and utilizes a two-stage training strategy that combines supervised fine-tuning (SFT) with RL. The results show that AlphaDrive significantly outperforms traditional methods that rely solely on SFT, leading to improved planning performance and training efficiency. Additionally, the framework demonstrates emergent multimodal planning capabilities post-RL training, which are essential for enhancing driving safety and efficiency."
                },
                "zh": {
                    "title": "AlphaDriveÔºöÊèêÂçáËá™Âä®È©æÈ©∂ÁöÑÊô∫ËÉΩËßÑÂàí‰∏éÊé®ÁêÜ",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫ÜAlphaDriveÔºåËøôÊòØ‰∏Ä‰∏™Áî®‰∫éËá™Âä®È©æÈ©∂ÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂíåÊé®ÁêÜÊ°ÜÊû∂„ÄÇAlphaDriveÂºïÂÖ•‰∫ÜÂõõÁßçÂü∫‰∫éGRPOÁöÑRLÂ•ñÂä±Ôºå‰∏ìÈó®ÈíàÂØπËßÑÂàí‰ªªÂä°ÔºåÂπ∂ÈááÁî®‰∫ÜÁªìÂêàÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÂíåRLÁöÑ‰∏§Èò∂ÊÆµËßÑÂàíÊé®ÁêÜËÆ≠ÁªÉÁ≠ñÁï•„ÄÇ‰∏é‰ªÖ‰ΩøÁî®SFTÊàñ‰∏çËøõË°åÊé®ÁêÜÁöÑÊÉÖÂÜµÁõ∏ÊØîÔºåAlphaDriveÊòæËëóÊèêÈ´ò‰∫ÜËßÑÂàíÊÄßËÉΩÂíåËÆ≠ÁªÉÊïàÁéá„ÄÇÊ≠§Â§ñÔºåÁªèËøáRLËÆ≠ÁªÉÂêéÔºåAlphaDriveËøòÂ±ïÁé∞Âá∫‰∏Ä‰∫õÊñ∞ÂÖ¥ÁöÑÂ§öÊ®°ÊÄÅËßÑÂàíËÉΩÂäõÔºåËøôÂØπÊèêÈ´òÈ©æÈ©∂ÂÆâÂÖ®ÊÄßÂíåÊïàÁéáËá≥ÂÖ≥ÈáçË¶Å„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07602",
            "title": "DreamRelation: Relation-Centric Video Customization",
            "url": "https://huggingface.co/papers/2503.07602",
            "abstract": "Relational video customization refers to the creation of personalized videos that depict user-specified relations between two subjects, a crucial task for comprehending real-world visual content. While existing methods can personalize subject appearances and motions, they still struggle with complex relational video customization, where precise relational modeling and high generalization across subject categories are essential. The primary challenge arises from the intricate spatial arrangements, layout variations, and nuanced temporal dynamics inherent in relations; consequently, current models tend to overemphasize irrelevant visual details rather than capturing meaningful interactions. To address these challenges, we propose DreamRelation, a novel approach that personalizes relations through a small set of exemplar videos, leveraging two key components: Relational Decoupling Learning and Relational Dynamics Enhancement. First, in Relational Decoupling Learning, we disentangle relations from subject appearances using relation LoRA triplet and hybrid mask training strategy, ensuring better generalization across diverse relationships. Furthermore, we determine the optimal design of relation LoRA triplet by analyzing the distinct roles of the query, key, and value features within MM-DiT's attention mechanism, making DreamRelation the first relational video generation framework with explainable components. Second, in Relational Dynamics Enhancement, we introduce space-time relational contrastive loss, which prioritizes relational dynamics while minimizing the reliance on detailed subject appearances. Extensive experiments demonstrate that DreamRelation outperforms state-of-the-art methods in relational video customization. Code and models will be made publicly available.",
            "score": 7,
            "issue_id": 2632,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 –º–∞—Ä—Ç–∞",
                "en": "March 10",
                "zh": "3Êúà10Êó•"
            },
            "hash": "a32c943630a808fc",
            "authors": [
                "Yujie Wei",
                "Shiwei Zhang",
                "Hangjie Yuan",
                "Biao Gong",
                "Longxiang Tang",
                "Xiang Wang",
                "Haonan Qiu",
                "Hengjia Li",
                "Shuai Tan",
                "Yingya Zhang",
                "Hongming Shan"
            ],
            "affiliations": [
                "Alibaba Group",
                "Ant Group",
                "Fudan University",
                "Nanyang Technological University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07602.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#architecture",
                    "#open_source",
                    "#video",
                    "#interpretability",
                    "#games"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "DreamRelation: –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–Ω–æ—à–µ–Ω–∏–π –≤ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
                    "desc": "DreamRelation - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –æ—Ç–Ω–æ—à–µ–Ω–∏–π –≤ –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ–±–æ–ª—å—à–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –≤–∏–¥–µ–æ-–ø—Ä–∏–º–µ—Ä–æ–≤. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –¥–≤–∞ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: Relational Decoupling Learning –∏ Relational Dynamics Enhancement. Relational Decoupling Learning —Ä–∞–∑–¥–µ–ª—è–µ—Ç –æ—Ç–Ω–æ—à–µ–Ω–∏—è –∏ –≤–Ω–µ—à–Ω–∏–π –≤–∏–¥ —Å—É–±—ä–µ–∫—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è relation LoRA triplet –∏ –≥–∏–±—Ä–∏–¥–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è –º–∞—Å–∫–∞–º. Relational Dynamics Enhancement –≤–≤–æ–¥–∏—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—É—é –ø–æ—Ç–µ—Ä—é –æ—Ç–Ω–æ—à–µ–Ω–∏–π, —á—Ç–æ–±—ã —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è –Ω–∞ –¥–∏–Ω–∞–º–∏–∫–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–π, –∞ –Ω–µ –Ω–∞ –¥–µ—Ç–∞–ª—è—Ö –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞ —Å—É–±—ä–µ–∫—Ç–æ–≤."
                },
                "en": {
                    "title": "DreamRelation: Personalizing Video Relationships with Precision",
                    "desc": "This paper introduces DreamRelation, a new method for creating personalized videos that focus on the relationships between two subjects. Current techniques struggle with complex relational dynamics, often missing meaningful interactions due to an overemphasis on irrelevant details. DreamRelation addresses this by using Relational Decoupling Learning to separate relationships from appearances and Relational Dynamics Enhancement to focus on the dynamics of these relationships. The approach shows significant improvements over existing methods in generating customized relational videos, with the added benefit of explainable components in its design."
                },
                "zh": {
                    "title": "DreamRelationÔºö‰∏™ÊÄßÂåñËßÜÈ¢ëÂÖ≥Á≥ªÂª∫Ê®°ÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫DreamRelationÁöÑÊñ∞ÊñπÊ≥ïÔºåÁî®‰∫é‰∏™ÊÄßÂåñËßÜÈ¢ë‰∏≠ÁöÑÂÖ≥Á≥ªÂª∫Ê®°„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂÖ≥Á≥ªËß£ËÄ¶Â≠¶‰π†ÂíåÂÖ≥Á≥ªÂä®ÊÄÅÂ¢ûÂº∫‰∏§‰∏™ÂÖ≥ÈîÆÁªÑ‰ª∂ÔºåËß£ÂÜ≥‰∫ÜÂ§çÊùÇÂÖ≥Á≥ªËßÜÈ¢ëÂÆöÂà∂‰∏≠ÁöÑÊåëÊàò„ÄÇÈÄöËøáÂàÜÊûêÊü•ËØ¢„ÄÅÈîÆÂíåÂÄºÁâπÂæÅÂú®Ê≥®ÊÑèÂäõÊú∫Âà∂‰∏≠ÁöÑ‰ΩúÁî®ÔºåDreamRelationÂÆûÁé∞‰∫ÜÂèØËß£ÈáäÁöÑÂÖ≥Á≥ªËßÜÈ¢ëÁîüÊàê„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDreamRelationÂú®ÂÖ≥Á≥ªËßÜÈ¢ëÂÆöÂà∂ÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07459",
            "title": "MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for\n  Complex Medical Reasoning",
            "url": "https://huggingface.co/papers/2503.07459",
            "abstract": "Large Language Models (LLMs) have shown impressive performance on existing medical question-answering benchmarks. This high performance makes it increasingly difficult to meaningfully evaluate and differentiate advanced methods. We present <PRE_TAG>MedAgentsBench</POST_TAG>, a benchmark that focuses on challenging medical questions requiring multi-step clinical reasoning, diagnosis formulation, and treatment planning-scenarios where current models still struggle despite their strong performance on standard tests. Drawing from seven established medical datasets, our benchmark addresses three key limitations in existing evaluations: (1) the prevalence of straightforward questions where even base models achieve high performance, (2) inconsistent sampling and evaluation protocols across studies, and (3) lack of systematic analysis of the interplay between performance, cost, and inference time. Through experiments with various base models and reasoning methods, we demonstrate that the latest thinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in complex medical reasoning tasks. Additionally, advanced search-based agent methods offer promising performance-to-cost ratios compared to traditional approaches. Our analysis reveals substantial performance gaps between model families on complex questions and identifies optimal model selections for different computational constraints. Our benchmark and evaluation framework are publicly available at https://github.com/gersteinlab/medagents-benchmark.",
            "score": 7,
            "issue_id": 2634,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 –º–∞—Ä—Ç–∞",
                "en": "March 10",
                "zh": "3Êúà10Êó•"
            },
            "hash": "4d9aba5593231609",
            "authors": [
                "Xiangru Tang",
                "Daniel Shao",
                "Jiwoong Sohn",
                "Jiapeng Chen",
                "Jiayi Zhang",
                "Jinyu Xiang",
                "Fang Wu",
                "Yilun Zhao",
                "Chenglin Wu",
                "Wenqi Shi",
                "Arman Cohan",
                "Mark Gerstein"
            ],
            "affiliations": [
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07459.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#survey",
                    "#healthcare"
                ],
                "emoji": "ü©∫",
                "ru": {
                    "title": "–ù–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Å–ª–æ–∂–Ω—ã—Ö –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ <PRE_TAG>MedAgentsBench</POST_TAG> –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –æ–±–ª–∞—Å—Ç–∏ –º–µ–¥–∏—Ü–∏–Ω—ã. –ë–µ–Ω—á–º–∞—Ä–∫ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã—Ö –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ, —á—Ç–æ –Ω–æ–≤–µ–π—à–∏–µ –º–æ–¥–µ–ª–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ DeepSeek R1 –∏ OpenAI o3, –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Å–ª–æ–∂–Ω—ã—Ö –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö. –ê–Ω–∞–ª–∏–∑ —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ –º–µ—Ç–æ–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–∏—Å–∫–∞ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–Ω–æ–≥–æ–æ–±–µ—â–∞—é—â–µ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏."
                },
                "en": {
                    "title": "MedAgentsBench: Elevating Medical Question-Answering Evaluation",
                    "desc": "This paper introduces MedAgentsBench, a new benchmark designed to evaluate Large Language Models (LLMs) on complex medical questions that require multi-step reasoning. It addresses limitations in current evaluations, such as the prevalence of simple questions and inconsistent testing protocols. The authors conduct experiments with various models, revealing that advanced models like DeepSeek R1 and OpenAI o3 perform well on challenging tasks, while search-based agents show better performance-to-cost ratios. The study highlights significant performance gaps among different model families and provides insights for selecting models based on computational resources."
                },
                "zh": {
                    "title": "ÂåªÂ≠¶ÈóÆÁ≠îÁöÑÊñ∞Âü∫ÂáÜÔºöÊåëÊàòÂ§çÊùÇÊé®ÁêÜ",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂåªÂ≠¶ÈóÆÁ≠îÂü∫ÂáÜÊµãËØï‚Äî‚ÄîMedAgentsBenchÔºåÊó®Âú®ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Â§çÊùÇÂåªÂ≠¶ÈóÆÈ¢ò‰∏äÁöÑË°®Áé∞„ÄÇËØ•Âü∫ÂáÜÊµãËØï‰∏ìÊ≥®‰∫éÈúÄË¶ÅÂ§öÊ≠•È™§‰∏¥Â∫äÊé®ÁêÜ„ÄÅËØäÊñ≠Âà∂ÂÆöÂíåÊ≤ªÁñóËÆ°ÂàíÁöÑÈóÆÈ¢òÔºåËøô‰∫õÈóÆÈ¢òÊòØÂΩìÂâçÊ®°Âûã‰ªçÁÑ∂Èù¢‰∏¥ÊåëÊàòÁöÑÈ¢ÜÂüü„ÄÇÈÄöËøáÂØπ‰∏É‰∏™Â∑≤Âª∫Á´ãÁöÑÂåªÂ≠¶Êï∞ÊçÆÈõÜËøõË°åÂàÜÊûêÔºåÊú¨ÊñáËß£ÂÜ≥‰∫ÜÁé∞ÊúâËØÑ‰º∞‰∏≠ÁöÑ‰∏â‰∏™ÂÖ≥ÈîÆÈôêÂà∂ÔºåÂåÖÊã¨ÁÆÄÂçïÈóÆÈ¢òÁöÑÊôÆÈÅçÊÄßÂíåËØÑ‰º∞ÂçèËÆÆÁöÑ‰∏ç‰∏ÄËá¥ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊúÄÊñ∞ÁöÑÊÄùÁª¥Ê®°ÂûãÂú®Â§çÊùÇÂåªÂ≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂπ∂‰∏îÂü∫‰∫éÊêúÁ¥¢ÁöÑ‰ª£ÁêÜÊñπÊ≥ïÂú®ÊÄßËÉΩ‰∏éÊàêÊú¨ÊØîÊñπÈù¢ÂÖ∑ÊúâËâØÂ•ΩÁöÑÂâçÊôØ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.06749",
            "title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large\n  Language Models",
            "url": "https://huggingface.co/papers/2503.06749",
            "abstract": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. Specifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the model's ability to learn correct and complex reasoning processes on a 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of sim6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. The datasets and code will be released in: https://github.com/Osilly/Vision-R1 .",
            "score": 6,
            "issue_id": 2632,
            "pub_date": "2025-03-09",
            "pub_date_card": {
                "ru": "9 –º–∞—Ä—Ç–∞",
                "en": "March 9",
                "zh": "3Êúà9Êó•"
            },
            "hash": "403914241aa8967c",
            "authors": [
                "Wenxuan Huang",
                "Bohan Jia",
                "Zijie Zhai",
                "Shaosheng Cao",
                "Zheyu Ye",
                "Fei Zhao",
                "Yao Hu",
                "Shaohui Lin"
            ],
            "affiliations": [
                "East China Normal University",
                "Xiaohongshu Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.06749.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#optimization",
                    "#multimodal",
                    "#open_source",
                    "#benchmark",
                    "#reasoning",
                    "#dataset",
                    "#rag"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "Vision-R1: –ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –ò–ò",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ Vision-R1, –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –û–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é MLLM –∏ DeepSeek-R1. –î–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–ª–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é Progressive Thinking Suppression Training –∏ Group Relative Policy Optimization. Vision-R1-7B –¥–æ—Å—Ç–∏–≥–ª–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ 73.5% –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ MathVista, —á—Ç–æ –ª–∏—à—å –Ω–∞ 0.4% –Ω–∏–∂–µ –≤–µ–¥—É—â–µ–π –º–æ–¥–µ–ª–∏ OpenAI O1."
                },
                "en": {
                    "title": "Enhancing Reasoning in MLLMs with Reinforcement Learning",
                    "desc": "The paper introduces DeepSeek-R1-Zero, which shows that large language models (LLMs) can develop reasoning skills through Reinforcement Learning (RL). It highlights the challenge of training LLMs directly with RL due to a lack of high-quality multimodal reasoning data. To overcome this, the authors propose a new model called Vision-R1, which utilizes a self-generated multimodal dataset for cold-start training. They also introduce a training strategy called Progressive Thinking Suppression Training (PTST) to enhance the model's reasoning capabilities, achieving significant improvements in multimodal math reasoning tasks."
                },
                "zh": {
                    "title": "ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÊèêÂçáÂ§öÊ®°ÊÄÅÊé®ÁêÜËÉΩÂäõ",
                    "desc": "DeepSeek-R1-ZeroÂ±ïÁ§∫‰∫ÜÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâ‰ΩøÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂÖ∑Â§áÊé®ÁêÜËÉΩÂäõÁöÑÂèØËÉΩÊÄß„ÄÇÂü∫‰∫éËøô‰∏ÄÁ™ÅÁ†¥ÔºåÊú¨ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂà©Áî®Âº∫ÂåñÂ≠¶‰π†ÊèêÂçáÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÁî±‰∫éÁº∫‰πèÈ´òË¥®ÈáèÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜÊï∞ÊçÆÔºåÁõ¥Êé•‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÈù¢‰∏¥ÊåëÊàòÔºåÂõ†Ê≠§Êàë‰ª¨ÊèêÂá∫‰∫ÜVision-R1Ê®°ÂûãÔºå‰ª•ÊîπÂñÑÂ§öÊ®°ÊÄÅÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™È´òË¥®ÈáèÁöÑÂ§öÊ®°ÊÄÅÈìæÂºèÊÄùÁª¥ÔºàCoTÔºâÊï∞ÊçÆÈõÜÔºåÂπ∂ÈÄöËøáÊ∏êËøõÊÄùÁª¥ÊäëÂà∂ËÆ≠ÁªÉÔºàPTSTÔºâÂíåÁæ§‰ΩìÁõ∏ÂØπÁ≠ñÁï•‰ºòÂåñÔºàGRPOÔºâÁ≠ñÁï•Êù•‰ºòÂåñÊ®°ÂûãÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07507",
            "title": "PE3R: Perception-Efficient 3D Reconstruction",
            "url": "https://huggingface.co/papers/2503.07507",
            "abstract": "Recent advancements in 2D-to-3D perception have significantly improved the understanding of 3D scenes from 2D images. However, existing methods face critical challenges, including limited generalization across scenes, suboptimal perception accuracy, and slow reconstruction speeds. To address these limitations, we propose Perception-Efficient 3D Reconstruction (PE3R), a novel framework designed to enhance both accuracy and efficiency. PE3R employs a feed-forward architecture to enable rapid 3D semantic field reconstruction. The framework demonstrates robust zero-shot generalization across diverse scenes and objects while significantly improving reconstruction speed. Extensive experiments on 2D-to-3D open-vocabulary segmentation and 3D reconstruction validate the effectiveness and versatility of PE3R. The framework achieves a minimum 9-fold speedup in 3D semantic field reconstruction, along with substantial gains in perception accuracy and reconstruction precision, setting new benchmarks in the field. The code is publicly available at: https://github.com/hujiecpp/PE3R.",
            "score": 5,
            "issue_id": 2631,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 –º–∞—Ä—Ç–∞",
                "en": "March 10",
                "zh": "3Êúà10Êó•"
            },
            "hash": "6a53d341839fc41f",
            "authors": [
                "Jie Hu",
                "Shizun Wang",
                "Xinchao Wang"
            ],
            "affiliations": [
                "xML Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07507.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#architecture",
                    "#benchmark"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–ë—ã—Å—Ç—Ä–∞—è –∏ —Ç–æ—á–Ω–∞—è 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∏–∑ 2D-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
                    "desc": "PE3R - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑ 2D-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä—è–º—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è 3D —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–ª–µ–π. PE3R –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–∞–¥–µ–∂–Ω—É—é –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ö –∏ –æ–±—ä–µ–∫—Ç–∞—Ö –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ –ø–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏."
                },
                "en": {
                    "title": "Revolutionizing 3D Scene Understanding with PE3R",
                    "desc": "This paper introduces Perception-Efficient 3D Reconstruction (PE3R), a new framework that enhances the process of understanding 3D scenes from 2D images. PE3R addresses key challenges in existing methods, such as limited generalization and slow reconstruction speeds, by utilizing a feed-forward architecture. The framework achieves impressive zero-shot generalization across various scenes and objects, while also significantly speeding up the reconstruction process. Experimental results show that PE3R offers at least a 9-fold increase in reconstruction speed and improved accuracy, establishing new benchmarks in 2D-to-3D perception."
                },
                "zh": {
                    "title": "ÊÑüÁü•È´òÊïà3DÈáçÂª∫ÔºåÈÄüÂ∫¶‰∏éÁ≤æÂ∫¶ÁöÑÂèåÈáçÊèêÂçá",
                    "desc": "ÊúÄËøëÂú®2DÂà∞3DÊÑüÁü•ÊñπÈù¢ÁöÑËøõÂ±ïÊòæËëóÊèêÈ´ò‰∫Ü‰ªé2DÂõæÂÉèÁêÜËß£3DÂú∫ÊôØÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ïÈù¢‰∏¥ÁùÄÂú∫ÊôØÊ≥õÂåñËÉΩÂäõÊúâÈôê„ÄÅÊÑüÁü•Á≤æÂ∫¶‰∏ç‰Ω≥ÂíåÈáçÂª∫ÈÄüÂ∫¶ÊÖ¢Á≠âÂÖ≥ÈîÆÊåëÊàò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÊÑüÁü•È´òÊïà3DÈáçÂª∫ÔºàPE3RÔºâÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇPE3RÈááÁî®ÂâçÈ¶àÊû∂ÊûÑÔºåËÉΩÂ§üÂø´ÈÄüÈáçÂª∫3DËØ≠‰πâÂú∫ÔºåÂπ∂Âú®Â§öÊ†∑Âú∫ÊôØÂíåÁâ©‰Ωì‰∏äÂ±ïÁ§∫Âá∫Âº∫Â§ßÁöÑÈõ∂Ê†∑Êú¨Ê≥õÂåñËÉΩÂäõÔºåÂêåÊó∂ÊòæËëóÊèêÈ´òÈáçÂª∫ÈÄüÂ∫¶„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.06520",
            "title": "Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive\n  Reinforcement",
            "url": "https://huggingface.co/papers/2503.06520",
            "abstract": "Traditional methods for reasoning segmentation rely on supervised fine-tuning with categorical labels and simple descriptions, limiting its out-of-domain generalization and lacking explicit reasoning processes. To address these limitations, we propose Seg-Zero, a novel framework that demonstrates remarkable generalizability and derives explicit chain-of-thought reasoning through cognitive reinforcement. Seg-Zero introduces a decoupled architecture consisting of a reasoning model and a segmentation model. The reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precious pixel-level masks. We design a sophisticated reward mechanism that integrates both format and accuracy rewards to effectively guide optimization directions. Trained exclusively via reinforcement learning with GRPO and without explicit reasoning data, Seg-Zero achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. Experiments show that Seg-Zero-7B achieves a zero-shot performance of 57.5 on the ReasonSeg benchmark, surpassing the prior LISA-7B by 18\\%. This significant improvement highlights Seg-Zero's ability to generalize across domains while presenting an explicit reasoning process. Code is available at https://github.com/dvlab-research/Seg-Zero.",
            "score": 5,
            "issue_id": 2630,
            "pub_date": "2025-03-09",
            "pub_date_card": {
                "ru": "9 –º–∞—Ä—Ç–∞",
                "en": "March 9",
                "zh": "3Êúà9Êó•"
            },
            "hash": "b21eb23a448d282e",
            "authors": [
                "Yuqi Liu",
                "Bohao Peng",
                "Zhisheng Zhong",
                "Zihao Yue",
                "Fanbin Lu",
                "Bei Yu",
                "Jiaya Jia"
            ],
            "affiliations": [
                "CUHK",
                "HKUST",
                "RUC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.06520.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#architecture",
                    "#reasoning",
                    "#rlhf",
                    "#optimization"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º: –ò–ò —É—á–∏—Ç—Å—è –æ–±—ä—è—Å–Ω—è—Ç—å —Å–≤–æ–∏ —Ä–µ—à–µ–Ω–∏—è",
                    "desc": "Seg-Zero - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–µ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏ —è–≤–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—â–µ–π —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏, –∏ –º–æ–¥–µ–ª–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏, —Å–æ–∑–¥–∞—é—â–µ–π –º–∞—Å–∫–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–∏–∫—Å–µ–ª–µ–π. –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π. Seg-Zero-7B –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ 18% –≤ –∑–∞–¥–∞—á–µ zero-shot —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ ReasonSeg."
                },
                "en": {
                    "title": "Seg-Zero: Revolutionizing Reasoning Segmentation with Zero-Shot Generalization",
                    "desc": "The paper introduces Seg-Zero, a new framework for reasoning segmentation that overcomes the limitations of traditional supervised methods. It features a decoupled architecture with a reasoning model that interprets user intentions and generates reasoning chains, which are then used by a segmentation model to create detailed pixel-level masks. The framework employs a unique reward mechanism that balances format and accuracy to optimize performance through reinforcement learning. Seg-Zero demonstrates impressive zero-shot generalization capabilities, achieving a significant performance boost on the ReasonSeg benchmark compared to previous models."
                },
                "zh": {
                    "title": "Seg-ZeroÔºöÁ™ÅÁ†¥ÊÄßÊé®ÁêÜ‰∏éÂàÜÂâ≤ÁöÑÁªìÂêà",
                    "desc": "‰º†ÁªüÁöÑÂàÜÂâ≤Êé®ÁêÜÊñπÊ≥ï‰æùËµñ‰∫éÂ∏¶ÊúâÁ±ªÂà´Ê†áÁ≠æÁöÑÁõëÁù£ÂæÆË∞ÉÔºåËøôÈôêÂà∂‰∫ÜÂÖ∂Âú®‰∏çÂêåÈ¢ÜÂüüÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂπ∂Áº∫‰πèÊòéÁ°ÆÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜSeg-ZeroÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÂ±ïÁ§∫‰∫ÜÊòæËëóÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂπ∂ÈÄöËøáËÆ§Áü•Âº∫ÂåñÊé®ÂØºÂá∫ÊòéÁ°ÆÁöÑÊé®ÁêÜÈìæ„ÄÇSeg-ZeroÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Ëß£ËÄ¶Êû∂ÊûÑÔºåÂåÖÊã¨Êé®ÁêÜÊ®°ÂûãÂíåÂàÜÂâ≤Ê®°ÂûãÔºåÊé®ÁêÜÊ®°ÂûãËß£ÈáäÁî®Êà∑ÊÑèÂõæÔºåÁîüÊàêÊòéÁ°ÆÁöÑÊé®ÁêÜÈìæÔºåÂπ∂‰∫ßÁîü‰ΩçÁΩÆÊèêÁ§∫ÔºåÈöèÂêéÁî±ÂàÜÂâ≤Ê®°ÂûãÁîüÊàêÁ≤æÁ°ÆÁöÑÂÉèÁ¥†Á∫ßÊé©Á†Å„ÄÇÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÔºåSeg-ZeroÂú®Ê≤°ÊúâÊòéÁ°ÆÊé®ÁêÜÊï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ãÔºåÂÆûÁé∞‰∫ÜÂº∫Â§ßÁöÑÈõ∂Ê†∑Êú¨Ê≥õÂåñËÉΩÂäõÔºåÂπ∂Â±ïÁé∞Âá∫Á™ÅÂá∫ÁöÑÊµãËØïÊó∂Êé®ÁêÜËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.06121",
            "title": "BlackGoose Rimer: Harnessing RWKV-7 as a Simple yet Superior Replacement\n  for Transformers in Large-Scale Time Series Modeling",
            "url": "https://huggingface.co/papers/2503.06121",
            "abstract": "Time series models face significant challenges in scaling to handle large and complex datasets, akin to the scaling achieved by large language models (LLMs). The unique characteristics of time series data and the computational demands of model scaling necessitate innovative approaches. While researchers have explored various architectures such as Transformers, LSTMs, and GRUs to address these challenges, we propose a novel solution using RWKV-7, which incorporates meta-learning into its state update mechanism. By integrating RWKV-7's time mix and channel mix components into the transformer-based time series model Timer, we achieve a substantial performance improvement of approximately 1.13 to 43.3x and a 4.5x reduction in training time with 1/23 parameters, all while utilizing fewer parameters. Our code and model weights are publicly available for further research and development at https://github.com/Alic-Li/BlackGoose_Rimer.",
            "score": 5,
            "issue_id": 2631,
            "pub_date": "2025-03-08",
            "pub_date_card": {
                "ru": "8 –º–∞—Ä—Ç–∞",
                "en": "March 8",
                "zh": "3Êúà8Êó•"
            },
            "hash": "3f03abe6317e5bba",
            "authors": [
                "Li weile",
                "Liu Xiao"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2503.06121.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#training",
                    "#open_source"
                ],
                "emoji": "‚è≥",
                "ru": {
                    "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å RWKV-7",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é –º–æ–¥–µ–ª–µ–π –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã RWKV-7. –ê–≤—Ç–æ—Ä—ã –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã RWKV-7 –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä-–º–æ–¥–µ–ª—å Timer –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏ –º–µ–Ω—å—à–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ —Ç–µ–º, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ —Å—Ç–∞–ª–∫–∏–≤–∞—é—Ç—Å—è –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏."
                },
                "en": {
                    "title": "Revolutionizing Time Series with RWKV-7: Efficiency Meets Performance",
                    "desc": "This paper addresses the challenges of scaling time series models to manage large and complex datasets, similar to the advancements seen in large language models. It introduces RWKV-7, a novel approach that integrates meta-learning into the state update mechanism of time series models. By combining RWKV-7's time mix and channel mix components with the Timer model, the authors report significant performance enhancements and reduced training times. The proposed method achieves impressive results with fewer parameters, making it a promising solution for time series analysis."
                },
                "zh": {
                    "title": "ÂàõÊñ∞Êó∂Èó¥Â∫èÂàóÊ®°ÂûãÔºåÊèêÂçáÊÄßËÉΩ‰∏éÊïàÁéá",
                    "desc": "Êó∂Èó¥Â∫èÂàóÊ®°ÂûãÂú®Â§ÑÁêÜÂ§ßÂûãÂ§çÊùÇÊï∞ÊçÆÈõÜÊó∂Èù¢‰∏¥ÊòæËëóÊåëÊàòÔºåÁ±ª‰ºº‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊâ©Â±ïËÉΩÂäõ„ÄÇÊó∂Èó¥Â∫èÂàóÊï∞ÊçÆÁöÑÁã¨ÁâπÁâπÊÄßÂíåÊ®°ÂûãÊâ©Â±ïÁöÑËÆ°ÁÆóÈúÄÊ±ÇÈúÄË¶ÅÂàõÊñ∞ÁöÑÊñπÊ≥ï„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËß£ÂÜ≥ÊñπÊ°àÔºå‰ΩøÁî®RWKV-7Â∞ÜÂÖÉÂ≠¶‰π†ËûçÂÖ•Áä∂ÊÄÅÊõ¥Êñ∞Êú∫Âà∂„ÄÇÈÄöËøáÂ∞ÜRWKV-7ÁöÑÊó∂Èó¥Ê∑∑ÂêàÂíåÈÄöÈÅìÊ∑∑ÂêàÁªÑ‰ª∂Êï¥ÂêàÂà∞Âü∫‰∫éÂèòÊç¢Âô®ÁöÑÊó∂Èó¥Â∫èÂàóÊ®°ÂûãTimer‰∏≠ÔºåÊàë‰ª¨ÂÆûÁé∞‰∫ÜÁ∫¶1.13Âà∞43.3ÂÄçÁöÑÊÄßËÉΩÊèêÂçáÔºåÂπ∂Â∞ÜËÆ≠ÁªÉÊó∂Èó¥ÂáèÂ∞ë‰∫Ü4.5ÂÄçÔºåÂêåÊó∂ÂèÇÊï∞Êï∞Èáè‰ªÖ‰∏∫ÂéüÊù•ÁöÑ1/23„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07197",
            "title": "Effective and Efficient Masked Image Generation Models",
            "url": "https://huggingface.co/papers/2503.07197",
            "abstract": "Although masked image generation models and masked diffusion models are designed with different motivations and objectives, we observe that they can be unified within a single framework. Building upon this insight, we carefully explore the design space of training and sampling, identifying key factors that contribute to both performance and efficiency. Based on the improvements observed during this exploration, we develop our model, referred to as eMIGM. Empirically, eMIGM demonstrates strong performance on ImageNet generation, as measured by Fr\\'echet Inception Distance (FID). In particular, on ImageNet 256x256, with similar number of function evaluations (NFEs) and model parameters, eMIGM outperforms the seminal VAR. Moreover, as NFE and model parameters increase, eMIGM achieves performance comparable to the state-of-the-art continuous diffusion models while requiring less than 40% of the NFE. Additionally, on ImageNet 512x512, with only about 60% of the NFE, eMIGM outperforms the state-of-the-art continuous diffusion models.",
            "score": 4,
            "issue_id": 2631,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 –º–∞—Ä—Ç–∞",
                "en": "March 10",
                "zh": "3Êúà10Êó•"
            },
            "hash": "4740cc0178bbf099",
            "authors": [
                "Zebin You",
                "Jingyang Ou",
                "Xiaolu Zhang",
                "Jun Hu",
                "Jun Zhou",
                "Chongxuan Li"
            ],
            "affiliations": [
                "Ant Group",
                "Beijing Key Laboratory of Big Data Management and Analysis Method",
                "Gaoling School of AI, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07197.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#training",
                    "#cv"
                ],
                "emoji": "üñºÔ∏è",
                "ru": {
                    "title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±—ä–µ–¥–∏–Ω–∏–ª–∏ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –µ–¥–∏–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É. –û–Ω–∏ –∏–∑—É—á–∏–ª–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –∏ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è, –≤—ã—è–≤–∏–≤ –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã, –≤–ª–∏—è—é—â–∏–µ –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –±—ã–ª–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –º–æ–¥–µ–ª—å eMIGM, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∫–∞–∑–∞–ª–∞ –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ImageNet. eMIGM –ø—Ä–µ–≤–∑–æ—à–ª–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ –º–µ—Ç—Ä–∏–∫–µ FID –ø—Ä–∏ –º–µ–Ω—å—à–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."
                },
                "en": {
                    "title": "Unifying Masked Models for Efficient Image Generation",
                    "desc": "This paper presents a unified framework for masked image generation models and masked diffusion models, highlighting their similarities despite different goals. The authors explore various design choices in training and sampling, which significantly impact the model's performance and efficiency. They introduce a new model called eMIGM, which shows superior results on ImageNet generation tasks, particularly in terms of Fr√©chet Inception Distance (FID). Notably, eMIGM achieves competitive performance with fewer function evaluations compared to existing state-of-the-art models, demonstrating its efficiency and effectiveness in image generation."
                },
                "zh": {
                    "title": "Áªü‰∏ÄÊé©ËîΩÊ®°ÂûãÔºåÊèêÂçáÂõæÂÉèÁîüÊàêÊÄßËÉΩ",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÊé©ËîΩÂõæÂÉèÁîüÊàêÊ®°ÂûãÂíåÊé©ËîΩÊâ©Êï£Ê®°ÂûãÁöÑÁªü‰∏ÄÊ°ÜÊû∂„ÄÇÊàë‰ª¨ÂàÜÊûê‰∫ÜËÆ≠ÁªÉÂíåÈááÊ†∑ÁöÑËÆæËÆ°Á©∫Èó¥ÔºåËØÜÂà´Âá∫ÂΩ±ÂìçÊÄßËÉΩÂíåÊïàÁéáÁöÑÂÖ≥ÈîÆÂõ†Á¥†„ÄÇÂü∫‰∫éËøô‰∫õÊîπËøõÔºåÊàë‰ª¨ÂºÄÂèë‰∫ÜÂêç‰∏∫eMIGMÁöÑÊ®°Âûã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåeMIGMÂú®ImageNetÁîüÊàê‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÂ∞§ÂÖ∂Âú®ËæÉ‰ΩéÁöÑÂáΩÊï∞ËØÑ‰º∞Ê¨°Êï∞‰∏ãË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊ®°Âûã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.03499",
            "title": "State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for\n  State Space Models",
            "url": "https://huggingface.co/papers/2503.03499",
            "abstract": "State Space Models (SSMs) have emerged as efficient alternatives to Transformers, mitigating their quadratic computational cost. However, the application of Parameter-Efficient Fine-Tuning (PEFT) methods to SSMs remains largely unexplored. In particular, prompt-based methods like Prompt Tuning and Prefix-Tuning, which are widely used in Transformers, do not perform well on SSMs. To address this, we propose state-based methods as a superior alternative to prompt-based methods. This new family of methods naturally stems from the architectural characteristics of SSMs. State-based methods adjust state-related features directly instead of depending on external prompts. Furthermore, we introduce a novel state-based PEFT method: State-offset Tuning. At every timestep, our method directly affects the state at the current step, leading to more effective adaptation. Through extensive experiments across diverse datasets, we demonstrate the effectiveness of our method. Code is available at https://github.com/furiosa-ai/ssm-state-tuning.",
            "score": 3,
            "issue_id": 2630,
            "pub_date": "2025-03-05",
            "pub_date_card": {
                "ru": "5 –º–∞—Ä—Ç–∞",
                "en": "March 5",
                "zh": "3Êúà5Êó•"
            },
            "hash": "80ab6abd822f2976",
            "authors": [
                "Wonjun Kang",
                "Kevin Galim",
                "Yuchen Zeng",
                "Minjae Lee",
                "Hyung Il Koo",
                "Nam Ik Cho"
            ],
            "affiliations": [
                "UW-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.03499.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ç–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–µ–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π –±–µ–∑ –ø—Ä–æ–º–ø—Ç–æ–≤",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –º–æ–¥–µ–ª–µ–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π (SSM) –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥—ã, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è—Ö, –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É –º–µ—Ç–æ–¥–∞–º, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º –Ω–∞ –ø—Ä–æ–º–ø—Ç–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—é—Ç –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, –Ω–æ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã –¥–ª—è SSM. –û–Ω–∏ –≤–≤–æ–¥—è—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º State-offset Tuning, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–ø—Ä—è–º—É—é –≤–ª–∏—è–µ—Ç –Ω–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –∫–∞–∂–¥–æ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–º —à–∞–≥–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞."
                },
                "en": {
                    "title": "Revolutionizing Fine-Tuning with State-Based Methods for SSMs",
                    "desc": "This paper discusses the limitations of using traditional prompt-based fine-tuning methods on State Space Models (SSMs), which are more efficient than Transformers. The authors propose a new approach called state-based methods that leverage the unique structure of SSMs to improve performance. Specifically, they introduce State-offset Tuning, a novel parameter-efficient fine-tuning technique that modifies the state at each timestep directly. Experimental results show that this method outperforms existing prompt-based techniques, highlighting its effectiveness in adapting SSMs for various tasks."
                },
                "zh": {
                    "title": "Âü∫‰∫éÁä∂ÊÄÅÁöÑÂæÆË∞ÉÔºöË∂ÖË∂äÊèêÁ§∫ÁöÑÊñπÊ≥ï",
                    "desc": "Áä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÔºàSSMsÔºâ‰Ωú‰∏∫ÂèòÊç¢Âô®ÁöÑÈ´òÊïàÊõø‰ª£ÊñπÊ°àÔºåËÉΩÂ§üÂáèËΩªÂÖ∂‰∫åÊ¨°ËÆ°ÁÆóÊàêÊú¨„ÄÇÁÑ∂ËÄåÔºåÂèÇÊï∞È´òÊïàÂæÆË∞ÉÔºàPEFTÔºâÊñπÊ≥ïÂú®SSMs‰∏äÁöÑÂ∫îÁî®‰ªçÁÑ∂Êú™Ë¢´ÂÖÖÂàÜÊé¢Á¥¢„ÄÇÊàë‰ª¨ÊèêÂá∫Âü∫‰∫éÁä∂ÊÄÅÁöÑÊñπÊ≥ïÔºå‰Ωú‰∏∫‰ºò‰∫éÂü∫‰∫éÊèêÁ§∫ÁöÑÊñπÊ≥ïÁöÑÊñ∞ÈÄâÊã©ÔºåËøô‰∫õÊñπÊ≥ïÁõ¥Êé•Ë∞ÉÊï¥‰∏éÁä∂ÊÄÅÁõ∏ÂÖ≥ÁöÑÁâπÂæÅÔºåËÄå‰∏çÊòØ‰æùËµñÂ§ñÈÉ®ÊèêÁ§∫„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂü∫‰∫éÁä∂ÊÄÅÁöÑPEFTÊñπÊ≥ïÔºöÁä∂ÊÄÅÂÅèÁßªÂæÆË∞ÉÔºåËÉΩÂ§üÂú®ÊØè‰∏™Êó∂Èó¥Ê≠•Áõ¥Êé•ÂΩ±ÂìçÂΩìÂâçÁä∂ÊÄÅÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÊúâÊïàÁöÑÈÄÇÂ∫î„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07603",
            "title": "Should VLMs be Pre-trained with Image Data?",
            "url": "https://huggingface.co/papers/2503.07603",
            "abstract": "Pre-trained LLMs that are further trained with image data perform well on vision-language tasks. While adding images during a second training phase effectively unlocks this capability, it is unclear how much of a gain or loss this two-step pipeline gives over VLMs which integrate images earlier into the training process. To investigate this, we train models spanning various datasets, scales, image-text ratios, and amount of pre-training done before introducing vision tokens. We then fine-tune these models and evaluate their downstream performance on a suite of vision-language and text-only tasks. We find that pre-training with a mixture of image and text data allows models to perform better on vision-language tasks while maintaining strong performance on text-only evaluations. On an average of 6 diverse tasks, we find that for a 1B model, introducing visual tokens 80% of the way through pre-training results in a 2% average improvement over introducing visual tokens to a fully pre-trained model.",
            "score": 2,
            "issue_id": 2633,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 –º–∞—Ä—Ç–∞",
                "en": "March 10",
                "zh": "3Êúà10Êó•"
            },
            "hash": "16f17eb6db67a418",
            "authors": [
                "Sedrick Keh",
                "Jean Mercat",
                "Samir Yitzhak Gadre",
                "Kushal Arora",
                "Igor Vasiljevic",
                "Benjamin Burchfiel",
                "Shuran Song",
                "Russ Tedrake",
                "Thomas Kollar",
                "Ludwig Schmidt",
                "Achal Dave"
            ],
            "affiliations": [
                "Columbia University",
                "MIT",
                "Stanford",
                "Toyota Research Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07603.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#benchmark",
                    "#transfer_learning",
                    "#optimization"
                ],
                "emoji": "üî¨",
                "ru": {
                    "title": "–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (—Å–Ω–∞—á–∞–ª–∞ –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö, –∑–∞—Ç–µ–º –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö) —Å –º–æ–¥–µ–ª—è–º–∏, –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –æ–±—É—á–∞–µ–º—ã–º–∏ –Ω–∞ —Å–º–µ—à–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –Ω–∞–±–æ—Ä–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö, –º–∞—Å—à—Ç–∞–±–∞–º–∏ –º–æ–¥–µ–ª–µ–π –∏ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–º–µ—à–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –ª—É—á—à–µ —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —Å –∑–∞–¥–∞—á–∞–º–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–º–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –î–ª—è –º–æ–¥–µ–ª–∏ —Ä–∞–∑–º–µ—Ä–æ–º 1 –º–∏–ª–ª–∏–∞—Ä–¥ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–≤–µ–¥–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ 80% —ç—Ç–∞–ø–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –¥–∞–µ—Ç –≤ —Å—Ä–µ–¥–Ω–µ–º 2% —É–ª—É—á—à–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∫ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏."
                },
                "en": {
                    "title": "Unlocking Vision-Language Synergy: Timing Matters!",
                    "desc": "This paper explores the effectiveness of training large language models (LLMs) with image data in a two-step process compared to integrating images earlier in the training. The authors conduct experiments with various datasets and training configurations to assess the impact of when visual tokens are introduced. Their findings indicate that pre-training with both image and text data enhances performance on vision-language tasks while still performing well on text-only tasks. Specifically, they observe a 2% improvement in performance when visual tokens are added later in the pre-training phase for a 1B model."
                },
                "zh": {
                    "title": "ÂõæÂÉè‰∏éÊñáÊú¨Ê∑∑ÂêàÈ¢ÑËÆ≠ÁªÉÊèêÂçáËßÜËßâËØ≠Ë®Ä‰ªªÂä°Ë°®Áé∞",
                    "desc": "ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÈ¢ÑËÆ≠ÁªÉÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®Âä†ÂÖ•ÂõæÂÉèÊï∞ÊçÆÂêéÂú®ËßÜËßâËØ≠Ë®Ä‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂú®Á¨¨‰∫åÈò∂ÊÆµËÆ≠ÁªÉ‰∏≠Âä†ÂÖ•ÂõæÂÉèÊï∞ÊçÆÂèØ‰ª•ÊúâÊïàÊèêÂçáÊ®°ÂûãÁöÑËÉΩÂäõÔºå‰ΩÜ‰∏çÊ∏ÖÊ•öËøôÁßç‰∏§Ê≠•ËÆ≠ÁªÉÊµÅÁ®ã‰∏éÊó©ÊúüÊï¥ÂêàÂõæÂÉèÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁõ∏ÊØîÔºåÁ©∂Á´üÊòØÂ¢ûÁõäËøòÊòØÊçüÂ§±„ÄÇÈÄöËøáËÆ≠ÁªÉ‰∏çÂêåÊï∞ÊçÆÈõÜ„ÄÅËßÑÊ®°ÂíåÂõæÂÉèÊñáÊú¨ÊØî‰æãÁöÑÊ®°ÂûãÔºåÁ†îÁ©∂ËÄÖÂèëÁé∞Ê∑∑ÂêàÂõæÂÉèÂíåÊñáÊú¨Êï∞ÊçÆÁöÑÈ¢ÑËÆ≠ÁªÉÂèØ‰ª•ÊèêÈ´òËßÜËßâËØ≠Ë®Ä‰ªªÂä°ÁöÑË°®Áé∞ÔºåÂêåÊó∂Âú®‰ªÖÊñáÊú¨ÁöÑËØÑ‰º∞‰∏≠‰πü‰øùÊåÅËâØÂ•ΩË°®Áé∞„ÄÇÁªìÊûúÊòæÁ§∫ÔºåÂØπ‰∫é‰∏Ä‰∏™10‰∫øÂèÇÊï∞ÁöÑÊ®°ÂûãÔºåÂú®È¢ÑËÆ≠ÁªÉÁöÑ80%Êó∂ÂºïÂÖ•ËßÜËßâÊ†áËÆ∞ÔºåÂπ≥ÂùáÊèêÂçá‰∫Ü2%ÁöÑÊÄßËÉΩ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.06885",
            "title": "ProBench: Judging Multimodal Foundation Models on Open-ended\n  Multi-domain Expert Tasks",
            "url": "https://huggingface.co/papers/2503.06885",
            "abstract": "Solving expert-level multimodal tasks is a key milestone towards general intelligence. As the capabilities of multimodal large language models (MLLMs) continue to improve, evaluation of such advanced multimodal intelligence becomes necessary yet challenging. In this work, we introduce ProBench, a benchmark of open-ended user queries that require professional expertise and advanced reasoning. ProBench consists of 4,000 high-quality samples independently submitted by professionals based on their daily productivity demands. It spans across 10 fields and 56 sub-fields, including science, arts, humanities, coding, mathematics, and creative writing. Experimentally, we evaluate and compare 24 latest models using MLLM-as-a-Judge. Our results reveal that although the best open-source models rival the proprietary ones, ProBench presents significant challenges in visual perception, textual understanding, domain knowledge and advanced reasoning, thus providing valuable directions for future multimodal AI research efforts.",
            "score": 2,
            "issue_id": 2633,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 –º–∞—Ä—Ç–∞",
                "en": "March 10",
                "zh": "3Êúà10Êó•"
            },
            "hash": "196c83578a251f8e",
            "authors": [
                "Yan Yang",
                "Dongxu Li",
                "Haoning Wu",
                "Bei Chen",
                "Liu Liu",
                "Liyuan Pan",
                "Junnan Li"
            ],
            "affiliations": [
                "ANU",
                "BITSZ & School of CSAT, BIT",
                "KooMap, Huawei",
                "NTU",
                "Salesforce AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.06885.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#multimodal",
                    "#agi",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "ProBench: –∏—Å–ø—ã—Ç–∞–Ω–∏–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π",
                    "desc": "ProBench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM). –û–Ω —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 4000 –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö 10 –æ–±–ª–∞—Å—Ç–µ–π –∏ 56 –ø–æ–¥–æ–±–ª–∞—Å—Ç–µ–π, –≤–∫–ª—é—á–∞—è –Ω–∞—É–∫—É, –∏—Å–∫—É—Å—Å—Ç–≤–æ, –≥—É–º–∞–Ω–∏—Ç–∞—Ä–Ω—ã–µ –Ω–∞—É–∫–∏ –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ. –ó–∞–¥–∞—á–∏ –±—ã–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—ã—Ö —Ä–∞–±–æ—á–∏—Ö –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è 24 –Ω–æ–≤–µ–π—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–µ—Ç–æ–¥–∞ MLLM-as-a-Judge –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ ProBench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ–º –≤–æ—Å–ø—Ä–∏—è—Ç–∏–∏, –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞, –ø—Ä–µ–¥–º–µ—Ç–Ω—ã—Ö –∑–Ω–∞–Ω–∏—è—Ö –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö."
                },
                "en": {
                    "title": "ProBench: Benchmarking Multimodal Intelligence for Expert Tasks",
                    "desc": "This paper presents ProBench, a new benchmark designed to evaluate multimodal large language models (MLLMs) on expert-level tasks. ProBench includes 4,000 user queries that require advanced reasoning and professional expertise across various fields such as science, arts, and coding. The study compares 24 state-of-the-art models using MLLM-as-a-Judge, highlighting the challenges these models face in visual perception, textual understanding, and domain knowledge. The findings indicate that while some open-source models perform comparably to proprietary ones, ProBench identifies critical areas for improvement in multimodal AI capabilities."
                },
                "zh": {
                    "title": "Â§öÊ®°ÊÄÅÊô∫ËÉΩËØÑ‰º∞ÁöÑÊñ∞Âü∫ÂáÜÔºöProBench",
                    "desc": "Êú¨ËÆ∫Êñá‰ªãÁªç‰∫ÜProBenchÔºåËøôÊòØ‰∏Ä‰∏™ÈíàÂØπÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÁöÑÂü∫ÂáÜÊµãËØïÔºåÊó®Âú®ËØÑ‰º∞ÂÖ∂Âú®‰∏ì‰∏öÈ¢ÜÂüüÁöÑÊô∫ËÉΩË°®Áé∞„ÄÇProBenchÂåÖÂê´4000‰∏™È´òË¥®ÈáèÊ†∑Êú¨ÔºåÊ∂µÁõñÁßëÂ≠¶„ÄÅËâ∫ÊúØ„ÄÅ‰∫∫ÊñáÂ≠¶Áßë„ÄÅÁºñÁ®ã„ÄÅÊï∞Â≠¶ÂíåÂàõÊÑèÂÜô‰ΩúÁ≠â10‰∏™È¢ÜÂüüÂíå56‰∏™Â≠êÈ¢ÜÂüü„ÄÇÈÄöËøáÂØπ24‰∏™ÊúÄÊñ∞Ê®°ÂûãÁöÑÂÆûÈ™åËØÑ‰º∞ÔºåÁªìÊûúÊòæÁ§∫Â∞ΩÁÆ°‰∏Ä‰∫õÂºÄÊ∫êÊ®°ÂûãÂú®ÊÄßËÉΩ‰∏ä‰∏é‰∏ìÊúâÊ®°ÂûãÁõ∏ÂΩìÔºå‰ΩÜÂú®ËßÜËßâÊÑüÁü•„ÄÅÊñáÊú¨ÁêÜËß£„ÄÅÈ¢ÜÂüüÁü•ËØÜÂíåÈ´òÁ∫ßÊé®ÁêÜÊñπÈù¢ÔºåProBench‰ªçÁÑ∂ÊèêÂá∫‰∫ÜÊòæËëóÁöÑÊåëÊàò„ÄÇËØ•Á†îÁ©∂‰∏∫Êú™Êù•Â§öÊ®°ÊÄÅ‰∫∫Â∑•Êô∫ËÉΩÁöÑÁ†îÁ©∂ÊñπÂêëÊèê‰æõ‰∫ÜÈáçË¶ÅÁöÑÂèÇËÄÉ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.02199",
            "title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?",
            "url": "https://huggingface.co/papers/2503.02199",
            "abstract": "Vision-Language Models (VLMs) excel in integrating visual and textual information for vision-centric tasks, but their handling of inconsistencies between modalities is underexplored. We investigate VLMs' modality preferences when faced with visual data and varied textual inputs in vision-centered settings. By introducing textual variations to four vision-centric tasks and evaluating ten Vision-Language Models (VLMs), we discover a ``blind faith in text'' phenomenon: VLMs disproportionately trust textual data over visual data when inconsistencies arise, leading to significant performance drops under corrupted text and raising safety concerns. We analyze factors influencing this text bias, including instruction prompts, language model size, text relevance, token order, and the interplay between visual and textual certainty. While certain factors, such as scaling up the language model size, slightly mitigate text bias, others like token order can exacerbate it due to positional biases inherited from language models. To address this issue, we explore supervised fine-tuning with text augmentation and demonstrate its effectiveness in reducing text bias. Additionally, we provide a theoretical analysis suggesting that the blind faith in text phenomenon may stem from an imbalance of pure text and multi-modal data during training. Our findings highlight the need for balanced training and careful consideration of modality interactions in VLMs to enhance their robustness and reliability in handling multi-modal data inconsistencies.",
            "score": 2,
            "issue_id": 2630,
            "pub_date": "2025-03-04",
            "pub_date_card": {
                "ru": "4 –º–∞—Ä—Ç–∞",
                "en": "March 4",
                "zh": "3Êúà4Êó•"
            },
            "hash": "a354f8de058f0f84",
            "authors": [
                "Ailin Deng",
                "Tri Cao",
                "Zhirui Chen",
                "Bryan Hooi"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.02199.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#alignment",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–û–ø–∞—Å–Ω–æ—Å—Ç—å —Å–ª–µ–ø–æ–π –≤–µ—Ä—ã –≤ —Ç–µ–∫—Å—Ç: –ø—Ä–æ–±–ª–µ–º–∞ –∏ —Ä–µ—à–µ–Ω–∏—è –¥–ª—è VLM –º–æ–¥–µ–ª–µ–π",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (VLM) —Å–∫–ª–æ–Ω–Ω—ã —á—Ä–µ–∑–º–µ—Ä–Ω–æ –¥–æ–≤–µ—Ä—è—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º –ø—Ä–∏ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ –º–µ–∂–¥—É –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π. –≠—Ç–æ —è–≤–ª–µ–Ω–∏–µ, –Ω–∞–∑–≤–∞–Ω–Ω–æ–µ '—Å–ª–µ–ø–æ–π –≤–µ—Ä–æ–π –≤ —Ç–µ–∫—Å—Ç', –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º—É —Å–Ω–∏–∂–µ–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∏ –≤—ã–∑—ã–≤–∞–µ—Ç –æ–ø–∞—Å–µ–Ω–∏—è –ø–æ –ø–æ–≤–æ–¥—É –∏—Ö –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Ñ–∞–∫—Ç–æ—Ä—ã, –≤–ª–∏—è—é—â–∏–µ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–º–µ—â–µ–Ω–∏–µ, –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥—ã –µ–≥–æ —É–º–µ–Ω—å—à–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –¥–æ–æ–±—É—á–µ–Ω–∏–µ —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π —Ç–µ–∫—Å—Ç–∞. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç, —á—Ç–æ –ø—Ä–æ–±–ª–µ–º–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–≤—è–∑–∞–Ω–∞ —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º —á–∏—Å—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π."
                },
                "en": {
                    "title": "Balancing Vision and Text: Overcoming Bias in Vision-Language Models",
                    "desc": "This paper investigates how Vision-Language Models (VLMs) manage inconsistencies between visual and textual information in tasks that rely heavily on vision. The authors identify a phenomenon called 'blind faith in text,' where VLMs tend to rely more on textual data than visual data when faced with conflicting inputs, which can lead to performance issues. They analyze various factors that contribute to this text bias, such as the size of the language model and the order of tokens in the text. To mitigate this bias, the paper proposes supervised fine-tuning with text augmentation and emphasizes the importance of balanced training for improving the reliability of VLMs in multi-modal contexts."
                },
                "zh": {
                    "title": "Âπ≥Ë°°ËÆ≠ÁªÉÔºåÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÂèØÈù†ÊÄß",
                    "desc": "ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®Â§ÑÁêÜËßÜËßâÂíåÊñáÊú¨‰ø°ÊÅØÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÆÉ‰ª¨Âú®Èù¢ÂØπÊ®°ÊÄÅ‰∏ç‰∏ÄËá¥Êó∂ÁöÑË°®Áé∞Â∞öÊú™ÂæóÂà∞ÂÖÖÂàÜÁ†îÁ©∂„ÄÇÊàë‰ª¨Êé¢ËÆ®‰∫ÜVLMsÂú®ËßÜËßâÊï∞ÊçÆÂíå‰∏çÂêåÊñáÊú¨ËæìÂÖ•‰∏ãÁöÑÊ®°ÊÄÅÂÅèÂ•ΩÔºåÂèëÁé∞‰∫Ü‚ÄúÂØπÊñáÊú¨ÁöÑÁõ≤ÁõÆ‰ø°‰ªª‚ÄùÁé∞Ë±°ÔºöÂΩìÂá∫Áé∞‰∏ç‰∏ÄËá¥Êó∂ÔºåVLMsËøáÂ∫¶‰æùËµñÊñáÊú¨Êï∞ÊçÆÔºåÂØºËá¥ÊÄßËÉΩÊòæËëó‰∏ãÈôç„ÄÇÊàë‰ª¨ÂàÜÊûê‰∫ÜÂΩ±ÂìçËøôÁßçÊñáÊú¨ÂÅèËßÅÁöÑÂõ†Á¥†ÔºåÂåÖÊã¨Êåá‰ª§ÊèêÁ§∫„ÄÅËØ≠Ë®ÄÊ®°ÂûãÂ§ßÂ∞è„ÄÅÊñáÊú¨Áõ∏ÂÖ≥ÊÄß„ÄÅÊ†áËÆ∞È°∫Â∫è‰ª•ÂèäËßÜËßâÂíåÊñáÊú¨Á°ÆÂÆöÊÄß‰πãÈó¥ÁöÑÁõ∏‰∫í‰ΩúÁî®„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨Êé¢Á¥¢‰∫ÜÂ∏¶ÊúâÊñáÊú¨Â¢ûÂº∫ÁöÑÁõëÁù£ÂæÆË∞ÉÔºåÂπ∂ËØÅÊòéÂÖ∂Âú®ÂáèÂ∞ëÊñáÊú¨ÂÅèËßÅÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07595",
            "title": "Detection Avoidance Techniques for Large Language Models",
            "url": "https://huggingface.co/papers/2503.07595",
            "abstract": "The increasing popularity of large language models has not only led to widespread use but has also brought various risks, including the potential for systematically spreading fake news. Consequently, the development of classification systems such as DetectGPT has become vital. These detectors are vulnerable to evasion techniques, as demonstrated in an experimental series: Systematic changes of the generative models' temperature proofed shallow learning-detectors to be the least reliable. Fine-tuning the generative model via reinforcement learning circumvented BERT-based-detectors. Finally, rephrasing led to a >90\\% evasion of zero-shot-detectors like DetectGPT, although texts stayed highly similar to the original. A comparison with existing work highlights the better performance of the presented methods. Possible implications for society and further research are discussed.",
            "score": 1,
            "issue_id": 2631,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 –º–∞—Ä—Ç–∞",
                "en": "March 10",
                "zh": "3Êúà10Êó•"
            },
            "hash": "0b6929d80e047189",
            "authors": [
                "Sinclair Schneider",
                "Florian Steuber",
                "Joao A. G. Schneider",
                "Gabi Dreo Rodosek"
            ],
            "affiliations": [
                "Research Institute CODE, Bundeswehr University Munich, Munich, 81739, Bavaria, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07595.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#benchmark",
                    "#rlhf",
                    "#data",
                    "#ethics",
                    "#rl",
                    "#hallucinations"
                ],
                "emoji": "üïµÔ∏è",
                "ru": {
                    "title": "–û–±–º–∞–Ω –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤: –∫–∞–∫ LLM –æ–±—Ö–æ–¥—è—Ç —Å–∏—Å—Ç–µ–º—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ò–ò-—Ç–µ–∫—Å—Ç–æ–≤",
                    "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —É—è–∑–≤–∏–º–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ —Å–µ—Ä–∏—é —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–∏—Ö, –∫–∞–∫ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –º–æ–≥—É—Ç –æ–±–æ–π—Ç–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ DetectGPT. –ë—ã–ª–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω—ã —Ç–µ—Ö–Ω–∏–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏, –¥–æ–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–±—Ö–æ–¥–∞ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã, —á–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã, —á—Ç–æ –ø–æ–¥–Ω–∏–º–∞–µ—Ç –≤–æ–ø—Ä–æ—Å—ã –æ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —Ç–µ–∫—É—â–∏—Ö —Å–∏—Å—Ç–µ–º –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ò–ò-–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞."
                },
                "en": {
                    "title": "Enhancing Evasion Techniques Against Language Model Detectors",
                    "desc": "This paper discusses the risks associated with large language models, particularly their potential to spread misinformation. It introduces DetectGPT, a classification system designed to identify generated text, but reveals its vulnerabilities to evasion techniques. The study shows that adjusting the generative model's temperature and fine-tuning it with reinforcement learning can effectively bypass existing detectors. Additionally, rephrasing generated content can achieve over 90% evasion rates while maintaining similarity to the original text, highlighting the need for improved detection methods."
                },
                "zh": {
                    "title": "Â∫îÂØπÂÅáÊñ∞ÈóªÁöÑÊô∫ËÉΩÊ£ÄÊµãÊåëÊàò",
                    "desc": "ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊôÆÂèäÔºåÂÅáÊñ∞Èóª‰º†Êí≠ÁöÑÈ£éÈô©‰πüÈöè‰πãÂ¢ûÂä†„ÄÇÂõ†Ê≠§ÔºåÂºÄÂèëÂÉèDetectGPTËøôÊ†∑ÁöÑÂàÜÁ±ªÁ≥ªÁªüÂèòÂæóËá≥ÂÖ≥ÈáçË¶Å„ÄÇËøô‰∫õÊ£ÄÊµãÂô®ÂÆπÊòìÂèóÂà∞ËßÑÈÅøÊäÄÊúØÁöÑÂΩ±ÂìçÔºåÂÆûÈ™åË°®ÊòéÔºåÁîüÊàêÊ®°ÂûãÊ∏©Â∫¶ÁöÑÁ≥ªÁªüÊÄßÂèòÂåñ‰ΩøÂæóÊµÖÂ±ÇÂ≠¶‰π†Ê£ÄÊµãÂô®ÁöÑÂèØÈù†ÊÄßÊúÄ‰Ωé„ÄÇÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉÁîüÊàêÊ®°ÂûãÂèØ‰ª•ÁªïËøáÂü∫‰∫éBERTÁöÑÊ£ÄÊµãÂô®ÔºåËÄåÈáçÊñ∞Ë°®Ëø∞ÊñáÊú¨Âàô‰ΩøÂæóÂÉèDetectGPTËøôÊ†∑ÁöÑÈõ∂Ê†∑Êú¨Ê£ÄÊµãÂô®ÁöÑËßÑÈÅøÁéáË∂ÖËøá90%ÔºåÂ∞ΩÁÆ°ÊñáÊú¨‰∏éÂéüÊñáÈ´òÂ∫¶Áõ∏‰ºº„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.03511",
            "title": "NeuGrasp: Generalizable Neural Surface Reconstruction with Background\n  Priors for Material-Agnostic Object Grasp Detection",
            "url": "https://huggingface.co/papers/2503.03511",
            "abstract": "Robotic grasping in scenes with transparent and specular objects presents great challenges for methods relying on accurate depth information. In this paper, we introduce NeuGrasp, a neural surface reconstruction method that leverages background priors for material-agnostic grasp detection. NeuGrasp integrates transformers and global prior volumes to aggregate multi-view features with spatial encoding, enabling robust surface reconstruction in narrow and sparse viewing conditions. By focusing on foreground objects through residual feature enhancement and refining spatial perception with an occupancy-prior volume, NeuGrasp excels in handling objects with transparent and specular surfaces. Extensive experiments in both simulated and real-world scenarios show that NeuGrasp outperforms state-of-the-art methods in grasping while maintaining comparable reconstruction quality. More details are available at https://neugrasp.github.io/.",
            "score": 0,
            "issue_id": 2630,
            "pub_date": "2025-03-05",
            "pub_date_card": {
                "ru": "5 –º–∞—Ä—Ç–∞",
                "en": "March 5",
                "zh": "3Êúà5Êó•"
            },
            "hash": "ba660b9e0f676df1",
            "authors": [
                "Qingyu Fan",
                "Yinghao Cai",
                "Chao Li",
                "Wenzhe He",
                "Xudong Zheng",
                "Tao Lu",
                "Bin Liang",
                "Shuo Wang"
            ],
            "affiliations": [
                "Qiyuan Lab",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.03511.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#agents",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "NeuGrasp: –Ω–µ–π—Ä–æ–Ω–Ω—ã–π –∑–∞—Ö–≤–∞—Ç –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–µ–π",
                    "desc": "NeuGrasp - —ç—Ç–æ –Ω–µ–π—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –æ–±—ä–µ–∫—Ç–æ–≤ —Å –ø—Ä–æ–∑—Ä–∞—á–Ω—ã–º–∏ –∏ –∑–µ—Ä–∫–∞–ª—å–Ω—ã–º–∏ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—è–º–∏. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø—Ä–∏–æ—Ä—ã –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –º—É–ª—å—Ç–∏—Ä–∞–∫—É—Ä—Å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º. NeuGrasp —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –æ–±—ä–µ–∫—Ç–∞—Ö –ø–µ—Ä–µ–¥–Ω–µ–≥–æ –ø–ª–∞–Ω–∞ –∏ —É—Ç–æ—á–Ω—è–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ —Å –ø–æ–º–æ—â—å—é –æ–±—ä–µ–º–∞ –ø—Ä–∏–æ—Ä–æ–≤ –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ NeuGrasp –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≤ –∑–∞–¥–∞—á–µ –∑–∞—Ö–≤–∞—Ç–∞ –æ–±—ä–µ–∫—Ç–æ–≤."
                },
                "en": {
                    "title": "NeuGrasp: Mastering Grasping with Transparent and Specular Objects",
                    "desc": "This paper presents NeuGrasp, a novel approach for robotic grasping that effectively deals with transparent and specular objects, which are challenging for traditional depth-based methods. NeuGrasp utilizes a neural surface reconstruction technique that incorporates background priors to enhance grasp detection without being limited by material properties. By combining transformers and global prior volumes, it aggregates multi-view features with spatial encoding, improving surface reconstruction even in difficult viewing conditions. The method demonstrates superior performance in grasping tasks compared to existing techniques, while also achieving high-quality surface reconstruction in both simulated and real-world environments."
                },
                "zh": {
                    "title": "NeuGraspÔºöÈÄèÊòéÁâ©‰ΩìÊäìÂèñÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫NeuGraspÁöÑÁ•ûÁªèË°®Èù¢ÈáçÂª∫ÊñπÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥ÈÄèÊòéÂíåÈïúÈù¢Áâ©‰ΩìÊäìÂèñ‰∏≠ÁöÑÊåëÊàò„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®ËÉåÊôØÂÖàÈ™åËøõË°åÊùêÊñôÊó†ÂÖ≥ÁöÑÊäìÂèñÊ£ÄÊµãÔºåÁªìÂêà‰∫ÜÂèòÊç¢Âô®ÂíåÂÖ®Â±ÄÂÖàÈ™å‰ΩìÁßØÔºå‰ª•ËÅöÂêàÂ§öËßÜËßíÁâπÂæÅÂπ∂ËøõË°åÁ©∫Èó¥ÁºñÁ†Å„ÄÇNeuGraspÈÄöËøáÊÆãÂ∑ÆÁâπÂæÅÂ¢ûÂº∫ËÅöÁÑ¶‰∫éÂâçÊôØÁâ©‰ΩìÔºåÂπ∂Âà©Áî®Âç†Áî®ÂÖàÈ™å‰ΩìÁßØÊù•ÊîπÂñÑÁ©∫Èó¥ÊÑüÁü•ÔºåËÉΩÂ§üÊúâÊïàÂ§ÑÁêÜÈÄèÊòéÂíåÈïúÈù¢Ë°®Èù¢ÁöÑÁâ©‰Ωì„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåNeuGraspÂú®ÊäìÂèñÊÄßËÉΩ‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÁõ∏‰ººÁöÑÈáçÂª∫Ë¥®Èáè„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-03-10.html",
    "link_next": "2025-03-12.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "10.03",
        "en": "03/10",
        "zh": "3Êúà10Êó•"
    },
    "short_date_next": {
        "ru": "12.03",
        "en": "03/12",
        "zh": "3Êúà12Êó•"
    },
    "categories": {
        "#dataset": 4,
        "#data": 3,
        "#benchmark": 12,
        "#agents": 5,
        "#cv": 4,
        "#rl": 7,
        "#rlhf": 2,
        "#rag": 3,
        "#plp": 1,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 14,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 8,
        "#healthcare": 1,
        "#training": 12,
        "#robotics": 1,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 3,
        "#reasoning": 9,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 2,
        "#security": 2,
        "#optimization": 13,
        "#survey": 2,
        "#diffusion": 2,
        "#alignment": 2,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "ÊúÄËøë‰∫∫Á±ªÂÅèÂ•ΩÂØπÈΩêÁöÑËøõÂ±ïÊòæËëóÊèêÂçá‰∫ÜÂ§öÊ®°ÊÄÅÁîüÊàêÂíåÁêÜËß£„ÄÇÂÖ≥ÈîÆÊñπÊ≥ïÊòØËÆ≠ÁªÉÂ•ñÂä±Ê®°ÂûãÊåáÂØºÂÅèÂ•Ω‰ºòÂåñ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊ®°ÂûãÈÄöÂ∏∏ÊòØ‰ªªÂä°ÁâπÂÆöÁöÑÔºåÈôêÂà∂‰∫ÜÂÆÉ‰ª¨Âú®‰∏çÂêåËßÜËßâÂ∫îÁî®‰∏≠ÁöÑÈÄÇÂ∫îÊÄß„ÄÇÊàë‰ª¨ËÆ§‰∏∫ÔºåËÅîÂêàÂ≠¶‰π†ËØÑ‰º∞Â§ö‰ªªÂä°ÂèØËÉΩ‰∫ßÁîüÂçèÂêåÊïàÂ∫îÔºåÊîπÂñÑÂõæÂÉèÁêÜËß£ÂíåÁîüÊàêËØÑ‰º∞ÔºåÂπ∂ÈÄöËøáÊõ¥Â•ΩÁöÑÂ∏ßÂàÜÊûêÊèêÂçáËßÜÈ¢ëËØÑ‰º∞„ÄÇÂõ†Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜUnifiedRewardÔºåÁ¨¨‰∏Ä‰∏™Áî®‰∫éÂ§öÊ®°ÊÄÅÁêÜËß£ÂíåÁîüÊàêËØÑ‰º∞ÁöÑÁªü‰∏ÄÂ•ñÂä±Ê®°Âûã„ÄÇ",
        "title": "Unified Reward Model for Multimodal Understanding and Generation",
        "pinyin": "ÊúÄËøë‰∫∫Á±ªÂÅèÂ•ΩÂØπÈΩêÁöÑËøõÂ±ïÊòæËëóÊèêÂçá‰∫ÜÂ§öÊ®°ÊÄÅÁîüÊàêÂíåÁêÜËß£„ÄÇÂÖ≥ÈîÆÊñπÊ≥ïÊòØËÆ≠ÁªÉÂ•ñÂä±Ê®°ÂûãÊåáÂØºÂÅèÂ•Ω‰ºòÂåñ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊ®°ÂûãÈÄöÂ∏∏ÊòØ‰ªªÂä°ÁâπÂÆöÁöÑÔºåÈôêÂà∂‰∫ÜÂÆÉ‰ª¨Âú®‰∏çÂêåËßÜËßâÂ∫îÁî®‰∏≠ÁöÑÈÄÇÂ∫îÊÄß„ÄÇÊàë‰ª¨ËÆ§‰∏∫ÔºåËÅîÂêàÂ≠¶‰π†ËØÑ‰º∞Â§ö‰ªªÂä°ÂèØËÉΩ‰∫ßÁîüÂçèÂêåÊïàÂ∫îÔºåÊîπÂñÑÂõæÂÉèÁêÜËß£ÂíåÁîüÊàêËØÑ‰º∞ÔºåÂπ∂ÈÄöËøáÊõ¥Â•ΩÁöÑÂ∏ßÂàÜÊûêÊèêÂçáËßÜÈ¢ëËØÑ‰º∞„ÄÇÂõ†Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜUnifiedRewardÔºåÁ¨¨‰∏Ä‰∏™Áî®‰∫éÂ§öÊ®°ÊÄÅÁêÜËß£ÂíåÁîüÊàêËØÑ‰º∞ÁöÑÁªü‰∏ÄÂ•ñÂä±Ê®°Âûã„ÄÇ\n\nZu√¨j√¨n r√©nl√®i piƒÅnh√†o du√¨q√≠ de j√¨nzh«én xi«énzh√π t√≠shƒìng le du≈ç m√≥sh√π shƒìngch√©ng h√© l«êjiƒõ. Gu«énji√†n fƒÅngf«é sh√¨ x√πnli√†n ji«éngl√¨ m√≥x√≠ng zh«êd«éo piƒÅnh√†o y≈çuhu√†. R√°n'√©r, xi√†ny«íu m√≥x√≠ng t≈çngch√°ng sh√¨ r√®nw√π t√®d√¨ng de, xi√†nzh√¨ le tƒÅmen z√†i b√πt√≥ng sh√¨juƒì y√¨ngy√≤ng zh≈çng de sh√¨y√¨ngx√¨ng. W«ímen r√®nw√©i, li√°nh√© xu√©x√≠ p√≠ngji√† du≈ç r√®nw√π kƒõn√©ng ch«énshƒìng xi√©t√≥ng xi√†oy√¨ng, g«éish√†n t√∫xi√†ng l«êjiƒõ h√© shƒìngch√©ng p√≠ngji√†, b√¨ng t≈çnggu√≤ g√®ng h«éo de zhƒìn fƒìnxi t√≠shƒìng sh√¨p√≠n p√≠ngji√†. Yƒ´nc«ê, bƒõnw√©n t√≠ch≈´ le UnifiedReward, d√¨-yƒ´g√® y√≤ngy√∫ du≈ç m√≥sh√π l«êjiƒõ h√© shƒìngch√©ng p√≠ngji√† de t«íngyƒ´ ji«éngl√¨ m√≥x√≠ng.",
        "vocab": "[\n    {\"word\": \"ÂÅèÂ•Ω\", \"pinyin\": \"piƒÅn h√†o\", \"trans\": \"preference\"},\n    {\"word\": \"ÂØπÈΩê\", \"pinyin\": \"du√¨ q√≠\", \"trans\": \"alignment\"},\n    {\"word\": \"ËøõÂ±ï\", \"pinyin\": \"j√¨n zh«én\", \"trans\": \"progress\"},\n    {\"word\": \"ÊòæËëó\", \"pinyin\": \"xi«én zh√π\", \"trans\": \"significant\"},\n    {\"word\": \"ÊèêÂçá\", \"pinyin\": \"t√≠ shƒìng\", \"trans\": \"improve\"},\n    {\"word\": \"Â§öÊ®°ÊÄÅ\", \"pinyin\": \"du≈ç m√≥ t√†i\", \"trans\": \"multimodal\"},\n    {\"word\": \"ÁîüÊàê\", \"pinyin\": \"shƒìng ch√©ng\", \"trans\": \"generation\"},\n    {\"word\": \"ÁêÜËß£\", \"pinyin\": \"l«ê jiƒõ\", \"trans\": \"understanding\"},\n    {\"word\": \"ÂÖ≥ÈîÆ\", \"pinyin\": \"gu«én ji√†n\", \"trans\": \"key\"},\n    {\"word\": \"ÊñπÊ≥ï\", \"pinyin\": \"fƒÅng f«é\", \"trans\": \"method\"},\n    {\"word\": \"ËÆ≠ÁªÉ\", \"pinyin\": \"x√πn li√†n\", \"trans\": \"training\"},\n    {\"word\": \"Â•ñÂä±\", \"pinyin\": \"ji«éng l√¨\", \"trans\": \"reward\"},\n    {\"word\": \"Ê®°Âûã\", \"pinyin\": \"m√≥ x√≠ng\", \"trans\": \"model\"},\n    {\"word\": \"ÊåáÂØº\", \"pinyin\": \"zh«ê d«éo\", \"trans\": \"guide\"},\n    {\"word\": \"‰ºòÂåñ\", \"pinyin\": \"y≈çu hu√†\", \"trans\": \"optimization\"},\n    {\"word\": \"‰ªªÂä°\", \"pinyin\": \"r√®n wu\", \"trans\": \"task\"},\n    {\"word\": \"ÁâπÂÆö\", \"pinyin\": \"t√® d√¨ng\", \"trans\": \"specific\"},\n    {\"word\": \"ÈôêÂà∂\", \"pinyin\": \"xi√†n zh√¨\", \"trans\": \"limit\"},\n    {\"word\": \"ÈÄÇÂ∫îÊÄß\", \"pinyin\": \"sh√¨ y√¨ng x√¨ng\", \"trans\": \"adaptability\"},\n    {\"word\": \"ËßÜËßâ\", \"pinyin\": \"sh√¨ ju√©\", \"trans\": \"visual\"},\n    {\"word\": \"Â∫îÁî®\", \"pinyin\": \"y√¨ng y√≤ng\", \"trans\": \"application\"},\n    {\"word\": \"ËÅîÂêà\", \"pinyin\": \"li√°n h√©\", \"trans\": \"joint\"},\n    {\"word\": \"Â≠¶‰π†\", \"pinyin\": \"xu√© x√≠\", \"trans\": \"learning\"},\n    {\"word\": \"ËØÑ‰º∞\", \"pinyin\": \"p√≠ng g≈´\", \"trans\": \"evaluation\"},\n    {\"word\": \"ÂçèÂêå\", \"pinyin\": \"xi√© t√≥ng\", \"trans\": \"synergy\"},\n    {\"word\": \"ÊîπÂñÑ\", \"pinyin\": \"g«éi sh√†n\", \"trans\": \"improve\"},\n    {\"word\": \"ÂõæÂÉè\", \"pinyin\": \"t√∫ xi√†ng\", \"trans\": \"image\"},\n    {\"word\": \"Â∏ß\", \"pinyin\": \"zhƒìn\", \"trans\": \"frame\"},\n    {\"word\": \"ÂàÜÊûê\", \"pinyin\": \"fƒìn xƒ´\", \"trans\": \"analysis\"},\n    {\"word\": \"ËßÜÈ¢ë\", \"pinyin\": \"sh√¨ p√≠n\", \"trans\": \"video\"},\n    {\"word\": \"ÊèêÂá∫\", \"pinyin\": \"t√≠ ch≈´\", \"trans\": \"propose\"},\n    {\"word\": \"Áªü‰∏Ä\", \"pinyin\": \"t«íng yƒ´\", \"trans\": \"unified\"},\n    {\"word\": \"Á¨¨‰∏Ä‰∏™\", \"pinyin\": \"d√¨ yƒ´ g√®\", \"trans\": \"first\"}\n]",
        "trans": "Recent advancements in human preference alignment have significantly enhanced multimodal generation and understanding. The key method involves training reward models to guide preference optimization. However, existing models are typically task-specific, limiting their adaptability across different visual applications. We believe that jointly learning to evaluate multiple tasks may produce synergistic effects, improving image understanding and generation evaluation, and enhancing video evaluation through better frame analysis. Therefore, this paper introduces UnifiedReward, the first unified reward model for multimodal understanding and generation evaluation.",
        "update_ts": "2025-03-10 09:10"
    }
}