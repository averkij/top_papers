{
    "date": {
        "ru": "21 апреля",
        "en": "April 21",
        "zh": "4月21日"
    },
    "time_utc": "2025-04-21 07:12",
    "weekday": 0,
    "issue_id": 3339,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.13835",
            "title": "MIG: Automatic Data Selection for Instruction Tuning by Maximizing\n  Information Gain in Semantic Space",
            "url": "https://huggingface.co/papers/2504.13835",
            "abstract": "Data quality and diversity are key to the construction of effective instruction-tuning datasets. % With the increasing availability of open-source instruction-tuning datasets, it is advantageous to automatically select high-quality and diverse subsets from a vast amount of data. % Existing methods typically prioritize instance quality and use heuristic rules to maintain diversity. % However, this absence of a comprehensive view of the entire collection often leads to suboptimal results. % Moreover, heuristic rules generally focus on distance or clustering within the embedding space, which fails to accurately capture the intent of complex instructions in the semantic space. % To bridge this gap, we propose a unified method for quantifying the information content of datasets. This method models the semantic space by constructing a label graph and quantifies diversity based on the distribution of information within the graph. % Based on such a measurement, we further introduce an efficient sampling method that selects data samples iteratively to Maximize the Information Gain (MIG) in semantic space. % Experiments on various datasets and base models demonstrate that MIG consistently outperforms state-of-the-art methods. % Notably, the model fine-tuned with 5\\% Tulu3 data sampled by MIG achieves comparable performance to the official SFT model trained on the full dataset, with improvements of +5.73\\% on AlpacaEval and +6.89\\% on Wildbench.",
            "score": 24,
            "issue_id": 3335,
            "pub_date": "2025-04-18",
            "pub_date_card": {
                "ru": "18 апреля",
                "en": "April 18",
                "zh": "4月18日"
            },
            "hash": "12926d762a03519c",
            "authors": [
                "Yicheng Chen",
                "Yining Li",
                "Kai Hu",
                "Zerun Ma",
                "Haochen Ye",
                "Kai Chen"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Fudan University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13835.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#open_source",
                    "#training",
                    "#dataset",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Максимизация информационного прироста для эффективного обучения языковых моделей",
                    "desc": "Статья представляет новый метод для отбора высококачественных и разнообразных подмножеств данных для обучения языковых моделей с инструкциями. Авторы предлагают унифицированный подход к измерению информационного содержания наборов данных, моделируя семантическое пространство с помощью графа меток. На основе этого измерения разработан эффективный метод выборки, максимизирующий прирост информации в семантическом пространстве. Эксперименты показывают, что предложенный метод превосходит современные подходы, позволяя достичь сопоставимой производительности при использовании лишь 5% данных."
                },
                "en": {
                    "title": "Maximizing Information for Better Instruction-Tuning Datasets",
                    "desc": "This paper addresses the importance of data quality and diversity in creating effective instruction-tuning datasets for machine learning. It critiques existing methods that rely on heuristic rules for maintaining diversity, which often leads to suboptimal dataset selections. The authors propose a new approach that quantifies the information content of datasets by modeling the semantic space with a label graph, allowing for a more comprehensive understanding of data diversity. Their method, called Maximize the Information Gain (MIG), iteratively selects samples that enhance the dataset's information content, showing significant performance improvements in experiments compared to traditional methods."
                },
                "zh": {
                    "title": "提升数据集质量与多样性的统一方法",
                    "desc": "数据质量和多样性是构建有效指令调优数据集的关键。随着开源指令调优数据集的增加，自动选择高质量和多样化的子集变得尤为重要。现有方法通常优先考虑实例质量，并使用启发式规则来维持多样性，但缺乏对整个数据集的全面视角，导致结果不理想。我们提出了一种统一的方法，通过构建标签图来量化数据集的信息内容，并基于信息分布来量化多样性，从而引入了一种高效的采样方法，以最大化语义空间中的信息增益。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13837",
            "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in\n  LLMs Beyond the Base Model?",
            "url": "https://huggingface.co/papers/2504.13837",
            "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning capabilities of LLMs, particularly in mathematics and programming tasks. It is widely believed that RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning abilities that exceed corresponding base models' capacity. In this study, however, we critically re-examines this assumption by measuring the pass@k metric with large values of k to explore the reasoning capability boundary of the models across a wide range of model families and benchmarks. Surprisingly, the RL does not, in fact, elicit fundamentally new reasoning patterns. While RL-trained models outperform their base models at smaller values of k (\\eg, k=1), base models can achieve a comparable or even higher pass@k score compared to their RL counterparts at large k values. The reasoning paths generated by RL-trained models are already included in the base models' sampling distribution, suggesting that most reasoning abilities manifested in RL-trained models are already obtained by base models. Further analysis shows that RL training boosts the performance by biasing the model's output distribution toward paths that are more likely to yield rewards, therefore sampling correct responses more efficiently. But this also results in a narrower reasoning capability boundary compared to base models. Similar results are observed in visual reasoning tasks trained with RLVR. Moreover, we find that distillation can genuinely introduce new knowledge into the model, different from RLVR. These findings underscore a critical limitation of RLVR in advancing LLM reasoning abilities which requires us to fundamentally rethink the impact of RL training in reasoning LLMs and the need of a better paradigm. Project Page: https://limit-of-RLVR.github.io",
            "score": 19,
            "issue_id": 3335,
            "pub_date": "2025-04-18",
            "pub_date_card": {
                "ru": "18 апреля",
                "en": "April 18",
                "zh": "4月18日"
            },
            "hash": "2fe56493fe3aec80",
            "authors": [
                "Yang Yue",
                "Zhiqi Chen",
                "Rui Lu",
                "Andrew Zhao",
                "Zhaokai Wang",
                "Yang Yue",
                "Shiji Song",
                "Gao Huang"
            ],
            "affiliations": [
                "LeapLab, Tsinghua University",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13837.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Обучение с подкреплением не расширяет границы рассуждений ИИ",
                    "desc": "Исследование показывает, что обучение с подкреплением с верифицируемыми наградами (RLVR) не приводит к появлению принципиально новых способностей рассуждения у языковых моделей. Хотя модели, обученные с помощью RL, превосходят базовые модели при небольших значениях k в метрике pass@k, базовые модели могут достичь сопоставимых или даже более высоких показателей при больших k. Анализ выявил, что RL-обучение смещает распределение выходных данных модели в сторону путей, которые с большей вероятностью приносят награды, но это также приводит к сужению границ способностей рассуждения. Результаты подчеркивают ограниченность RLVR в улучшении способностей рассуждения языковых моделей."
                },
                "en": {
                    "title": "Rethinking RLVR: Limits of Reinforcement Learning in Reasoning",
                    "desc": "This paper critically evaluates the effectiveness of Reinforcement Learning with Verifiable Rewards (RLVR) in enhancing the reasoning capabilities of large language models (LLMs). The authors find that while RLVR improves performance at lower complexity tasks, it does not introduce fundamentally new reasoning patterns compared to base models when evaluated at higher complexity levels. Instead, RL-trained models tend to sample reasoning paths that are already present in base models, leading to a narrower range of reasoning capabilities. The study suggests that distillation may be a more effective method for introducing new knowledge into models, highlighting the limitations of RLVR in advancing LLM reasoning."
                },
                "zh": {
                    "title": "重新思考强化学习在推理中的作用",
                    "desc": "强化学习与可验证奖励（RLVR）在提升大型语言模型（LLM）推理能力方面取得了一定成功，尤其是在数学和编程任务中。然而，本研究重新审视了这一假设，发现RLVR并未真正引入新的推理模式。尽管RL训练的模型在小的k值下表现优于基础模型，但在较大的k值下，基础模型的表现可以与RL模型相媲美，甚至更好。这表明，RL训练模型的推理路径实际上已经包含在基础模型的采样分布中，强调了RLVR在提升LLM推理能力方面的局限性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11833",
            "title": "Could Thinking Multilingually Empower LLM Reasoning?",
            "url": "https://huggingface.co/papers/2504.11833",
            "abstract": "Previous work indicates that large language models exhibit a significant \"English bias\", i.e. they often perform better when tasks are presented in English. Interestingly, we have observed that using certain other languages in reasoning tasks can yield better performance than English. However, this phenomenon remains under-explored. In this paper, we explore the upper bound of harnessing multilingualism in reasoning tasks, suggesting that multilingual reasoning promises significantly (by nearly 10 Acc@k points) and robustly (tolerance for variations in translation quality and language choice) higher upper bounds than English-only reasoning. Besides analyzing the reason behind the upper bound and challenges in reaching it, we also find that common answer selection methods cannot achieve this upper bound, due to their limitations and biases. These insights could pave the way for future research aimed at fully harnessing the potential of multilingual reasoning in LLMs.",
            "score": 13,
            "issue_id": 3335,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 апреля",
                "en": "April 16",
                "zh": "4月16日"
            },
            "hash": "463f5ddc1d75970e",
            "authors": [
                "Changjiang Gao",
                "Xu Huang",
                "Wenhao Zhu",
                "Shujian Huang",
                "Lei Li",
                "Fei Yuan"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "National Key Laboratory for Novel Software Technology, Nanjing University",
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11833.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#low_resource",
                    "#multilingual"
                ],
                "emoji": "🌍",
                "ru": {
                    "title": "Многоязычное рассуждение - ключ к улучшению больших языковых моделей",
                    "desc": "Это исследование показывает, что использование нескольких языков в задачах рассуждения может значительно повысить эффективность больших языковых моделей по сравнению с использованием только английского языка. Авторы обнаружили, что многоязычное рассуждение может улучшить точность на 10 процентных пунктов. Однако существующие методы выбора ответов не могут полностью реализовать этот потенциал из-за своих ограничений и предвзятостей. Эти выводы открывают новые направления для исследований в области многоязычного рассуждения в больших языковых моделях."
                },
                "en": {
                    "title": "Unlocking the Power of Multilingual Reasoning in LLMs",
                    "desc": "This paper investigates the performance of large language models (LLMs) in reasoning tasks across multiple languages. It reveals that using certain non-English languages can lead to better outcomes than relying solely on English, highlighting a significant opportunity for multilingual reasoning. The authors suggest that multilingual approaches can achieve higher accuracy and robustness compared to English-only methods, even when translation quality varies. Additionally, they identify limitations in current answer selection methods that prevent reaching the full potential of multilingual reasoning, setting the stage for future research in this area."
                },
                "zh": {
                    "title": "多语言推理的潜力超越英语",
                    "desc": "本论文探讨了大型语言模型在推理任务中的多语言能力，发现某些语言在推理任务中的表现优于英语。研究表明，多语言推理的上限比仅使用英语的推理高出近10个准确率点，并且对翻译质量和语言选择的变化具有更强的容忍度。我们分析了达到这一上限的原因和挑战，并指出常见的答案选择方法由于其局限性和偏见，无法实现这一上限。这些发现为未来研究充分利用大型语言模型的多语言推理潜力铺平了道路。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11544",
            "title": "NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes",
            "url": "https://huggingface.co/papers/2504.11544",
            "abstract": "Retrieval-augmented generation (RAG) empowers large language models to access external and private corpus, enabling factually consistent responses in specific domains. By exploiting the inherent structure of the corpus, graph-based RAG methods further enrich this process by building a knowledge graph index and leveraging the structural nature of graphs. However, current graph-based RAG approaches seldom prioritize the design of graph structures. Inadequately designed graph not only impede the seamless integration of diverse graph algorithms but also result in workflow inconsistencies and degraded performance. To further unleash the potential of graph for RAG, we propose NodeRAG, a graph-centric framework introducing heterogeneous graph structures that enable the seamless and holistic integration of graph-based methodologies into the RAG workflow. By aligning closely with the capabilities of LLMs, this framework ensures a fully cohesive and efficient end-to-end process. Through extensive experiments, we demonstrate that NodeRAG exhibits performance advantages over previous methods, including GraphRAG and LightRAG, not only in indexing time, query time, and storage efficiency but also in delivering superior question-answering performance on multi-hop benchmarks and open-ended head-to-head evaluations with minimal retrieval tokens. Our GitHub repository could be seen at https://github.com/Terry-Xu-666/NodeRAG.",
            "score": 4,
            "issue_id": 3335,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 апреля",
                "en": "April 15",
                "zh": "4月15日"
            },
            "hash": "86dd4da356ad5ef0",
            "authors": [
                "Tianyang Xu",
                "Haojie Zheng",
                "Chengze Li",
                "Haoxiang Chen",
                "Yixin Liu",
                "Ruoxi Chen",
                "Lichao Sun"
            ],
            "affiliations": [
                "Columbia University",
                "Lehigh University",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11544.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#graphs",
                    "#open_source",
                    "#rag",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "NodeRAG: Графовый подход к улучшению генерации с дополнением из источников",
                    "desc": "NodeRAG - это новый подход к генерации с дополнением из источников (RAG), использующий гетерогенные графовые структуры для улучшения работы больших языковых моделей. Эта система позволяет эффективно интегрировать графовые алгоритмы в процесс RAG, обеспечивая более согласованный и производительный рабочий процесс. Эксперименты показывают, что NodeRAG превосходит предыдущие методы по скорости индексации, времени запросов и эффективности хранения. Кроме того, система демонстрирует улучшенные результаты в задачах ответов на вопросы и открытых сравнениях, используя минимальное количество токенов для извлечения информации."
                },
                "en": {
                    "title": "NodeRAG: Enhancing RAG with Smart Graph Structures",
                    "desc": "This paper introduces NodeRAG, a new framework that enhances retrieval-augmented generation (RAG) by using heterogeneous graph structures. By focusing on the design of graph structures, NodeRAG improves the integration of various graph algorithms into the RAG workflow, leading to better performance. The framework aligns with the capabilities of large language models (LLMs), ensuring a smooth and efficient process for generating responses. Experimental results show that NodeRAG outperforms existing methods like GraphRAG and LightRAG in terms of indexing time, query time, storage efficiency, and question-answering accuracy."
                },
                "zh": {
                    "title": "NodeRAG：图结构助力检索增强生成",
                    "desc": "检索增强生成（RAG）使大型语言模型能够访问外部和私有语料库，从而在特定领域提供事实一致的响应。通过利用语料库的内在结构，基于图的RAG方法通过构建知识图谱索引进一步丰富了这一过程。然而，目前的基于图的RAG方法很少重视图结构的设计。为了解放图在RAG中的潜力，我们提出了NodeRAG，一个以图为中心的框架，引入异构图结构，实现图方法与RAG工作流程的无缝整合。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13157",
            "title": "AerialMegaDepth: Learning Aerial-Ground Reconstruction and View\n  Synthesis",
            "url": "https://huggingface.co/papers/2504.13157",
            "abstract": "We explore the task of geometric reconstruction of images captured from a mixture of ground and aerial views. Current state-of-the-art learning-based approaches fail to handle the extreme viewpoint variation between aerial-ground image pairs. Our hypothesis is that the lack of high-quality, co-registered aerial-ground datasets for training is a key reason for this failure. Such data is difficult to assemble precisely because it is difficult to reconstruct in a scalable way. To overcome this challenge, we propose a scalable framework combining pseudo-synthetic renderings from 3D city-wide meshes (e.g., Google Earth) with real, ground-level crowd-sourced images (e.g., MegaDepth). The pseudo-synthetic data simulates a wide range of aerial viewpoints, while the real, crowd-sourced images help improve visual fidelity for ground-level images where mesh-based renderings lack sufficient detail, effectively bridging the domain gap between real images and pseudo-synthetic renderings. Using this hybrid dataset, we fine-tune several state-of-the-art algorithms and achieve significant improvements on real-world, zero-shot aerial-ground tasks. For example, we observe that baseline DUSt3R localizes fewer than 5% of aerial-ground pairs within 5 degrees of camera rotation error, while fine-tuning with our data raises accuracy to nearly 56%, addressing a major failure point in handling large viewpoint changes. Beyond camera estimation and scene reconstruction, our dataset also improves performance on downstream tasks like novel-view synthesis in challenging aerial-ground scenarios, demonstrating the practical value of our approach in real-world applications.",
            "score": 3,
            "issue_id": 3335,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "bbd51434265e3614",
            "authors": [
                "Khiem Vuong",
                "Anurag Ghosh",
                "Deva Ramanan",
                "Srinivasa Narasimhan",
                "Shubham Tulsiani"
            ],
            "affiliations": [
                "Carnegie Mellon University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13157.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#transfer_learning",
                    "#synthetic",
                    "#dataset",
                    "#3d"
                ],
                "emoji": "🏙️",
                "ru": {
                    "title": "Преодоление разрыва между землей и небом в компьютерном зрении",
                    "desc": "Статья посвящена геометрической реконструкции изображений с наземных и аэросъемок. Авторы предлагают масштабируемый подход, комбинирующий псевдо-синтетические рендеры из 3D-моделей городов с реальными наземными изображениями. Этот гибридный набор данных используется для дообучения современных алгоритмов компьютерного зрения. Результаты показывают значительное улучшение в задачах локализации камеры и реконструкции сцены при экстремальных изменениях ракурса между наземными и аэроснимками."
                },
                "en": {
                    "title": "Bridging the Viewpoint Gap: Enhanced Aerial-Ground Image Reconstruction",
                    "desc": "This paper addresses the challenge of reconstructing images from both ground and aerial perspectives, which current machine learning methods struggle with due to significant viewpoint differences. The authors suggest that the lack of high-quality datasets that pair aerial and ground images is a major obstacle. To tackle this, they introduce a scalable framework that combines pseudo-synthetic images generated from 3D city models with real ground-level images, effectively bridging the gap between these two domains. By fine-tuning existing algorithms with this hybrid dataset, they achieve substantial improvements in accuracy for aerial-ground tasks, demonstrating the framework's effectiveness in real-world applications."
                },
                "zh": {
                    "title": "打破视角限制，实现图像几何重建",
                    "desc": "本文探讨了从地面和空中视角捕获的图像进行几何重建的任务。现有的基于学习的方法在处理空中与地面图像对之间的极端视角变化时表现不佳。我们认为，缺乏高质量的、共同注册的空中-地面数据集是导致这一失败的关键原因。为了解决这个问题，我们提出了一种可扩展的框架，结合了来自3D城市网格的伪合成渲染和真实的地面众包图像，从而有效地缩小了真实图像与伪合成渲染之间的领域差距。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13072",
            "title": "HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation",
            "url": "https://huggingface.co/papers/2504.13072",
            "abstract": "Scene-level 3D generation represents a critical frontier in multimedia and computer graphics, yet existing approaches either suffer from limited object categories or lack editing flexibility for interactive applications. In this paper, we present HiScene, a novel hierarchical framework that bridges the gap between 2D image generation and 3D object generation and delivers high-fidelity scenes with compositional identities and aesthetic scene content. Our key insight is treating scenes as hierarchical \"objects\" under isometric views, where a room functions as a complex object that can be further decomposed into manipulatable items. This hierarchical approach enables us to generate 3D content that aligns with 2D representations while maintaining compositional structure. To ensure completeness and spatial alignment of each decomposed instance, we develop a video-diffusion-based amodal completion technique that effectively handles occlusions and shadows between objects, and introduce shape prior injection to ensure spatial coherence within the scene. Experimental results demonstrate that our method produces more natural object arrangements and complete object instances suitable for interactive applications, while maintaining physical plausibility and alignment with user inputs.",
            "score": 3,
            "issue_id": 3337,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "6eb45708f6cb0c26",
            "authors": [
                "Wenqi Dong",
                "Bangbang Yang",
                "Zesong Yang",
                "Yuan Li",
                "Tao Hu",
                "Hujun Bao",
                "Yuewen Ma",
                "Zhaopeng Cui"
            ],
            "affiliations": [
                "ByteDance",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13072.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "🏠",
                "ru": {
                    "title": "HiScene: Иерархическая 3D-генерация сцен с композиционной структурой",
                    "desc": "HiScene - это новый иерархический подход к генерации 3D-сцен, который объединяет 2D-генерацию изображений и 3D-генерацию объектов. Метод рассматривает сцены как иерархические 'объекты' в изометрической проекции, где комната функционирует как сложный объект, который можно разложить на отдельные предметы. Используется видео-диффузионная техника для заполнения скрытых частей объектов и инъекция априорной информации о форме для обеспечения пространственной согласованности. Результаты показывают, что HiScene создает более естественные компоновки объектов, подходящие для интерактивных приложений."
                },
                "en": {
                    "title": "HiScene: Bridging 2D and 3D for Interactive Scene Generation",
                    "desc": "This paper introduces HiScene, a new framework for generating 3D scenes that combines the strengths of 2D image generation with 3D object creation. It treats scenes as hierarchical structures, allowing for detailed manipulation of individual elements within a room. The method employs a video-diffusion-based technique for amodal completion, addressing issues like occlusions and shadows to ensure realistic object interactions. Experimental results show that HiScene produces coherent and aesthetically pleasing 3D scenes that are well-suited for interactive applications."
                },
                "zh": {
                    "title": "HiScene：层次化的3D场景生成新方法",
                    "desc": "本论文提出了一种名为HiScene的层次框架，用于场景级3D生成，旨在解决现有方法在对象类别和编辑灵活性方面的局限。我们将场景视为在等距视图下的层次“对象”，使得房间可以被进一步分解为可操作的物品。通过这种层次化的方法，我们能够生成与2D表示相一致的3D内容，同时保持组合结构。我们还开发了一种基于视频扩散的模态补全技术，以处理对象之间的遮挡和阴影，确保每个分解实例的完整性和空间对齐。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13173",
            "title": "It's All Connected: A Journey Through Test-Time Memorization,\n  Attentional Bias, Retention, and Online Optimization",
            "url": "https://huggingface.co/papers/2504.13173",
            "abstract": "Designing efficient and effective architectural backbones has been in the core of research efforts to enhance the capability of foundation models. Inspired by the human cognitive phenomenon of attentional bias-the natural tendency to prioritize certain events or stimuli-we reconceptualize neural architectures, including Transformers, Titans, and modern linear recurrent neural networks as associative memory modules that learn a mapping of keys and values using an internal objective, referred to as attentional bias. Surprisingly, we observed that most existing sequence models leverage either (1) dot-product similarity, or (2) L2 regression objectives as their attentional bias. Going beyond these objectives, we present a set of alternative attentional bias configurations along with their effective approximations to stabilize their training procedure. We then reinterpret forgetting mechanisms in modern deep learning architectures as a form of retention regularization, providing a novel set of forget gates for sequence models. Building upon these insights, we present Miras, a general framework to design deep learning architectures based on four choices of: (i) associative memory architecture, (ii) attentional bias objective, (iii) retention gate, and (iv) memory learning algorithm. We present three novel sequence models-Moneta, Yaad, and Memora-that go beyond the power of existing linear RNNs while maintaining a fast parallelizable training process. Our experiments show different design choices in Miras yield models with varying strengths. For example, certain instances of Miras achieve exceptional performance in special tasks such as language modeling, commonsense reasoning, and recall intensive tasks, even outperforming Transformers and other modern linear recurrent models.",
            "score": 2,
            "issue_id": 3336,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "809d2f1facd3aed9",
            "authors": [
                "Ali Behrouz",
                "Meisam Razaviyayn",
                "Peilin Zhong",
                "Vahab Mirrokni"
            ],
            "affiliations": [
                "Google Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13173.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Переосмысление архитектур нейронных сетей через призму селективного внимания",
                    "desc": "Статья представляет новый подход к проектированию архитектур нейронных сетей, вдохновленный когнитивным феноменом селективного внимания. Авторы предлагают framework Miras для создания моделей на основе ассоциативной памяти с различными конфигурациями внимания и механизмами забывания. Представлены три новые модели последовательностей - Moneta, Yaad и Memora, которые превосходят существующие линейные RNN. Эксперименты показывают, что некоторые варианты Miras достигают исключительной производительности в задачах языкового моделирования, здравого смысла и интенсивного запоминания."
                },
                "en": {
                    "title": "Revolutionizing Neural Architectures with Attentional Bias",
                    "desc": "This paper explores new ways to design neural network architectures, particularly focusing on how they can mimic human attention. It introduces the concept of attentional bias, which helps models prioritize important information, and critiques existing methods that rely on simple similarity measures. The authors propose a framework called Miras, which allows for flexible design choices in memory architecture and training objectives. They also present new models that outperform traditional approaches in specific tasks, demonstrating the effectiveness of their innovative strategies."
                },
                "zh": {
                    "title": "基于注意偏向的深度学习架构设计",
                    "desc": "本文探讨了如何设计高效且有效的基础模型架构，灵感来源于人类的注意偏向现象。我们将神经网络架构重新概念化为关联记忆模块，利用内部目标（注意偏向）来学习键值映射。研究发现，现有序列模型主要依赖点积相似性或L2回归目标，而我们提出了一系列替代的注意偏向配置及其有效近似，以稳定训练过程。基于这些见解，我们提出了Miras框架，设计深度学习架构，并展示了三种新型序列模型，超越了现有线性RNN的能力。"
                }
            }
        }
    ],
    "link_prev": "2025-04-18.html",
    "link_next": "2025-04-22.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "18.04",
        "en": "04/18",
        "zh": "4月18日"
    },
    "short_date_next": {
        "ru": "22.04",
        "en": "04/22",
        "zh": "4月22日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 2,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章讨论了预训练数据集的收集和优化问题。现有数据集通常来自网络内容，缺乏明确的领域划分。文章提出了一种名为CLIMB的自动化框架，通过嵌入和聚类大规模数据集，迭代搜索最佳数据混合。使用这种方法，1B模型在400B tokens上的训练效果超过了Llama-3.2-1B。文章还介绍了ClimbLab和ClimbMix两个数据集，并分析了最佳数据混合的特征。",
        "title": "CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for\n  Language Model Pre-training",
        "pinyin": "这篇文章讨论了预训练数据集的收集和优化问题。\nZhè piān wén zhāng tǎo lùn le yù xùn liàn shù jù jí de shōu jí hé yōu huà wèn tí.\n\n现有数据集通常来自网络内容，缺乏明确的领域划分。\nXiàn yǒu shù jù jí tōng cháng lái zì wǎng luò nèi róng, quē fá míng què de lǐng yù huà fēn.\n\n文章提出了一种名为CLIMB的自动化框架，通过嵌入和聚类大规模数据集，迭代搜索最佳数据混合。\nWén zhāng tí chū le yī zhǒng míng wéi CLIMB de zì dòng huà kuàng jià, tōng guò qiàn rù hé jù lèi dà guī mó shù jù jí, dié dài sōu suǒ zuì jiā shù jù hùn hé.\n\n使用这种方法，1B模型在400B tokens上的训练效果超过了Llama-3.2-1B。\nShǐ yòng zhè zhǒng fāng fǎ, 1B mó xíng zài 400B tokens shàng de xùn liàn xiào guǒ chāo guò le Llama-3.2-1B.\n\n文章还介绍了ClimbLab和ClimbMix两个数据集，并分析了最佳数据混合的特征。\nWén zhāng hái jiè shào le ClimbLab hé ClimbMix liǎng gè shù jù jí, bìng fēn xī le zuì jiā shù jù hùn hé de tè zhēng.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"预训练\", \"pinyin\": \"yù xùn liàn\", \"trans\": \"pre-train\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shù jù jí\", \"trans\": \"dataset\"},\n    {\"word\": \"收集\", \"pinyin\": \"shōu jí\", \"trans\": \"collect\"},\n    {\"word\": \"优化\", \"pinyin\": \"yōu huà\", \"trans\": \"optimize\"},\n    {\"word\": \"现有\", \"pinyin\": \"xiàn yǒu\", \"trans\": \"existing\"},\n    {\"word\": \"来自\", \"pinyin\": \"lái zì\", \"trans\": \"come from\"},\n    {\"word\": \"网络\", \"pinyin\": \"wǎng luò\", \"trans\": \"network\"},\n    {\"word\": \"内容\", \"pinyin\": \"nèi róng\", \"trans\": \"content\"},\n    {\"word\": \"缺乏\", \"pinyin\": \"quē fá\", \"trans\": \"lack\"},\n    {\"word\": \"明确\", \"pinyin\": \"míng què\", \"trans\": \"clear\"},\n    {\"word\": \"领域\", \"pinyin\": \"lǐng yù\", \"trans\": \"domain\"},\n    {\"word\": \"划分\", \"pinyin\": \"huà fēn\", \"trans\": \"divide\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"名为\", \"pinyin\": \"míng wéi\", \"trans\": \"named\"},\n    {\"word\": \"自动化\", \"pinyin\": \"zì dòng huà\", \"trans\": \"automate\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàng jià\", \"trans\": \"framework\"},\n    {\"word\": \"嵌入\", \"pinyin\": \"qiàn rù\", \"trans\": \"embed\"},\n    {\"word\": \"聚类\", \"pinyin\": \"jù lèi\", \"trans\": \"cluster\"},\n    {\"word\": \"大规模\", \"pinyin\": \"dà guī mó\", \"trans\": \"large-scale\"},\n    {\"word\": \"迭代\", \"pinyin\": \"dié dài\", \"trans\": \"iterate\"},\n    {\"word\": \"搜索\", \"pinyin\": \"sōu suǒ\", \"trans\": \"search\"},\n    {\"word\": \"最佳\", \"pinyin\": \"zuì jiā\", \"trans\": \"optimal\"},\n    {\"word\": \"混合\", \"pinyin\": \"hùn hé\", \"trans\": \"mix\"},\n    {\"word\": \"使用\", \"pinyin\": \"shǐ yòng\", \"trans\": \"use\"},\n    {\"word\": \"效果\", \"pinyin\": \"xiào guǒ\", \"trans\": \"effect\"},\n    {\"word\": \"超过\", \"pinyin\": \"chāo guò\", \"trans\": \"exceed\"},\n    {\"word\": \"介绍\", \"pinyin\": \"jiè shào\", \"trans\": \"introduce\"},\n    {\"word\": \"分析\", \"pinyin\": \"fēn xī\", \"trans\": \"analyze\"},\n    {\"word\": \"特征\", \"pinyin\": \"tè zhēng\", \"trans\": \"feature\"}\n]",
        "trans": "This article discusses the collection and optimization of pretraining datasets. Existing datasets typically come from web content and lack clear domain distinctions. The article proposes an automated framework called CLIMB, which embeds and clusters large-scale datasets to iteratively search for the optimal data mix. Using this method, the 1B model's training performance on 400B tokens surpassed that of Llama-3.2-1B. The article also introduces the ClimbLab and ClimbMix datasets and analyzes the characteristics of the optimal data mix.",
        "update_ts": "2025-04-20 12:41"
    }
}