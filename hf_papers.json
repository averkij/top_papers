{
    "date": {
        "ru": "9 Ğ¸ÑĞ½Ñ",
        "en": "June 9",
        "zh": "6æœˆ9æ—¥"
    },
    "time_utc": "2025-06-09 16:14",
    "weekday": 0,
    "issue_id": 4199,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.21115",
            "title": "Will It Still Be True Tomorrow? Multilingual Evergreen Question\n  Classification to Improve Trustworthy QA",
            "url": "https://huggingface.co/papers/2505.21115",
            "abstract": "EverGreenQA, a multilingual QA dataset with evergreen labels, is introduced to benchmark LLMs on temporality encoding and assess their performance through verbalized judgments and uncertainty signals.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) often hallucinate in question answering (QA) tasks. A key yet underexplored factor contributing to this is the temporality of questions -- whether they are evergreen (answers remain stable over time) or mutable (answers change). In this work, we introduce EverGreenQA, the first multilingual QA dataset with evergreen labels, supporting both evaluation and training. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they encode question temporality explicitly (via verbalized judgments) or implicitly (via uncertainty signals). We also train EG-E5, a lightweight multilingual classifier that achieves SoTA performance on this task. Finally, we demonstrate the practical utility of evergreen classification across three applications: improving self-knowledge estimation, filtering QA datasets, and explaining GPT-4o retrieval behavior.",
            "score": 72,
            "issue_id": 4192,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ",
                "en": "May 27",
                "zh": "5æœˆ27æ—¥"
            },
            "hash": "cbbff6b511a277fa",
            "authors": [
                "Sergey Pletenev",
                "Maria Marina",
                "Nikolay Ivanov",
                "Daria Galimzianova",
                "Nikita Krayko",
                "Mikhail Salnikov",
                "Vasily Konovalov",
                "Alexander Panchenko",
                "Viktor Moskvoretskii"
            ],
            "affiliations": [
                "AIRI",
                "HSE University",
                "MIPT",
                "MTS AI",
                "Skoltech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21115.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#low_resource",
                    "#dataset",
                    "#multilingual",
                    "#long_context",
                    "#hallucinations"
                ],
                "emoji": "ğŸŒ³",
                "ru": {
                    "title": "EverGreenQA: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…",
                    "desc": "EverGreenQA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚ĞºĞ¸ 'Ğ²ĞµÑ‡Ğ½Ğ¾Ğ·ĞµĞ»ĞµĞ½Ñ‹Ñ…' Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 12 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ Ğ¸Ñ… ÑĞ²Ğ½Ğ¾Ğµ Ğ¸ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ EG-E5, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ 'Ğ²ĞµÑ‡Ğ½Ğ¾Ğ·ĞµĞ»ĞµĞ½Ñ‹Ñ…' Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Understanding Question Timeliness with EverGreenQA",
                    "desc": "The paper introduces EverGreenQA, a new multilingual question answering (QA) dataset designed to evaluate how well large language models (LLMs) understand the concept of temporality in questions. It distinguishes between evergreen questions, which have stable answers, and mutable questions, which can change over time. The authors benchmark 12 LLMs using this dataset to see if they can explicitly or implicitly recognize question temporality through verbalized judgments and uncertainty signals. Additionally, they present EG-E5, a lightweight multilingual classifier that achieves state-of-the-art performance and demonstrate its usefulness in various applications, such as enhancing self-knowledge estimation and filtering QA datasets."
                },
                "zh": {
                    "title": "æ­ç¤ºé—®ç­”ä¸­çš„æ—¶é—´æ€§ï¼šEverGreenQAæ•°æ®é›†",
                    "desc": "EverGreenQAæ˜¯ä¸€ä¸ªå¤šè¯­è¨€é—®ç­”æ•°æ®é›†ï¼Œä¸“æ³¨äºæ—¶é—´æ€§ç¼–ç ï¼Œç‰¹åˆ«æ˜¯é—®é¢˜çš„æŒä¹…æ€§ã€‚è¯¥æ•°æ®é›†é€šè¿‡æ°¸æ’æ ‡ç­¾æ¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é—®ç­”ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé—®é¢˜çš„æ—¶é—´æ€§ï¼ˆå¦‚æ°¸æ’æ€§æˆ–å¯å˜æ€§ï¼‰å¯¹LLMsçš„å›ç­”å‡†ç¡®æ€§æœ‰é‡è¦å½±å“ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†EG-E5ï¼Œä¸€ä¸ªè½»é‡çº§çš„å¤šè¯­è¨€åˆ†ç±»å™¨ï¼Œåœ¨è¿™ä¸€ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01111",
            "title": "FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal\n  Contextual Fusion",
            "url": "https://huggingface.co/papers/2506.01111",
            "abstract": "A novel two-stage pipeline using specialized pretrained models and a large language model enhances audio caption quality by integrating diverse multimodal cues and contextual information.  \t\t\t\t\tAI-generated summary \t\t\t\t High-quality, large-scale audio captioning is crucial for advancing audio understanding, yet current automated methods often generate captions that lack fine-grained detail and contextual accuracy, primarily due to their reliance on limited unimodal or superficial multimodal information. Drawing inspiration from human auditory perception, which adeptly integrates cross-modal cues and performs sophisticated auditory scene analysis, we introduce a novel two-stage automated pipeline. This pipeline first employs specialized pretrained models to extract diverse contextual cues (e.g., speech, music, general sounds, and visual information from associated video). A large language model (LLM) then synthesizes these rich, multimodal inputs to generate detailed and context-aware audio captions. Key contributions of this work include: (1) the proposed scalable method for fine-grained audio caption generation; (2) FusionAudio, a new large-scale dataset comprising 1.2 million such detailed captions, combined with 6 million QA pairs; and (3) enhanced audio models developed using FusionAudio, specifically a CLAP-based audio encoder with superior audio-text alignment and instruction following. This paper paves the way for more nuanced and accurate automated understanding of complex audio environments. Code and data can be found in https://github.com/satsuki2486441738/FusionAudio.",
            "score": 23,
            "issue_id": 4186,
            "pub_date": "2025-06-01",
            "pub_date_card": {
                "ru": "1 Ğ¸ÑĞ½Ñ",
                "en": "June 1",
                "zh": "6æœˆ1æ—¥"
            },
            "hash": "a649684de588a812",
            "authors": [
                "Shunian Chen",
                "Xinyuan Xie",
                "Zheshu Chen",
                "Liyan Zhao",
                "Owen Lee",
                "Zhan Su",
                "Qilin Sun",
                "Benyou Wang"
            ],
            "affiliations": [
                "South China University of Technology",
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01111.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#audio",
                    "#multimodal",
                    "#optimization",
                    "#data",
                    "#games"
                ],
                "emoji": "ğŸ§",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑÑ…: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (LLM). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ FusionAudio - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 1,2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğ¸ 6 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ FusionAudio, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ CLAP Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "Enhancing Audio Captions with Multimodal Insights",
                    "desc": "This paper presents a two-stage pipeline that improves the quality of audio captions by using specialized pretrained models alongside a large language model (LLM). The first stage extracts various contextual cues from audio, such as speech and music, as well as visual information from related videos. In the second stage, the LLM synthesizes these multimodal inputs to create detailed and contextually accurate captions. The work introduces a new dataset, FusionAudio, which contains 1.2 million detailed captions and enhances audio models for better audio-text alignment."
                },
                "zh": {
                    "title": "æå‡éŸ³é¢‘å­—å¹•è´¨é‡çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µç®¡é“ï¼Œåˆ©ç”¨ä¸“é—¨çš„é¢„è®­ç»ƒæ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹æ¥æé«˜éŸ³é¢‘å­—å¹•çš„è´¨é‡ã€‚è¯¥æ–¹æ³•é€šè¿‡æå–å¤šæ ·çš„ä¸Šä¸‹æ–‡çº¿ç´¢ï¼Œå¦‚è¯­éŸ³ã€éŸ³ä¹å’Œè§†è§‰ä¿¡æ¯ï¼Œæ¥å¢å¼ºéŸ³é¢‘ç†è§£ã€‚ç„¶åï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç»¼åˆè¿™äº›å¤šæ¨¡æ€è¾“å…¥ï¼Œç”Ÿæˆè¯¦ç»†ä¸”å…·æœ‰ä¸Šä¸‹æ–‡æ„è¯†çš„éŸ³é¢‘å­—å¹•ã€‚æ­¤ç ”ç©¶çš„å…³é”®è´¡çŒ®åŒ…æ‹¬å¯æ‰©å±•çš„ç»†ç²’åº¦éŸ³é¢‘å­—å¹•ç”Ÿæˆæ–¹æ³•å’Œä¸€ä¸ªæ–°çš„å¤§è§„æ¨¡æ•°æ®é›†FusionAudioï¼ŒåŒ…å«120ä¸‡æ¡è¯¦ç»†å­—å¹•å’Œ600ä¸‡å¯¹é—®ç­”ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05629",
            "title": "Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs",
            "url": "https://huggingface.co/papers/2506.05629",
            "abstract": "A new method using input-dependent soft prompting with a self-attention mechanism improves parameter-efficient fine-tuning for large language models, enhancing zero-shot domain transfer.  \t\t\t\t\tAI-generated summary \t\t\t\t The performance of large language models in domain-specific tasks necessitates fine-tuning, which is computationally expensive and technically challenging. This paper focuses on parameter-efficient fine-tuning using soft prompting, a promising approach that adapts pre-trained models to downstream tasks by learning a small set of parameters. We propose a novel Input Dependent Soft Prompting technique with a self-Attention Mechanism (ID-SPAM) that generates soft prompts based on the input tokens and attends different tokens with varying importance. Our method is simple and efficient, keeping the number of trainable parameters small. We show the merits of the proposed approach compared to state-of-the-art techniques on various tasks and show the improved zero shot domain transfer capability.",
            "score": 14,
            "issue_id": 4186,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 Ğ¸ÑĞ½Ñ",
                "en": "June 5",
                "zh": "6æœˆ5æ—¥"
            },
            "hash": "c88ec16aee1c43d5",
            "authors": [
                "Ananth Muppidi",
                "Abhilash Nandy",
                "Sambaran Bandyopadhyay"
            ],
            "affiliations": [
                "Adobe Research, India",
                "IIIT Hyderabad, India",
                "IIT Kharagpur, India"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05629.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#transfer_learning",
                    "#small_models"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ´Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸-ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Input Dependent Soft Prompting with self-Attention Mechanism (ID-SPAM) Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ÑĞ³ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸ÑĞ²Ğ¾ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼. ID-SPAM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² zero-shot Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞµ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹."
                },
                "en": {
                    "title": "Efficient Fine-Tuning with Input-Dependent Soft Prompts",
                    "desc": "This paper introduces a new technique called Input Dependent Soft Prompting with a Self-Attention Mechanism (ID-SPAM) to enhance fine-tuning of large language models. It focuses on making the fine-tuning process more efficient by using a small set of parameters that adapt the model to specific tasks. The self-attention mechanism allows the model to weigh the importance of different input tokens when generating soft prompts. The results demonstrate that ID-SPAM outperforms existing methods, particularly in zero-shot domain transfer scenarios."
                },
                "zh": {
                    "title": "è¾“å…¥ä¾èµ–çš„è½¯æç¤ºï¼Œæå‡å¾®è°ƒæ•ˆç‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåˆ©ç”¨è¾“å…¥ä¾èµ–çš„è½¯æç¤ºå’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ¥æé«˜å¤§è¯­è¨€æ¨¡å‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒèƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•é€šè¿‡å­¦ä¹ ä¸€å°ç»„å‚æ•°ï¼Œé€‚åº”é¢„è®­ç»ƒæ¨¡å‹åˆ°ä¸‹æ¸¸ä»»åŠ¡ï¼Œå‡å°‘äº†è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç”ŸæˆåŸºäºè¾“å…¥æ ‡è®°çš„è½¯æç¤ºï¼Œå¹¶å¯¹ä¸åŒçš„é‡è¦æ€§æ ‡è®°è¿›è¡Œå…³æ³¨ï¼Œä»è€Œå®ç°äº†é«˜æ•ˆçš„å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶æå‡äº†é›¶-shoté¢†åŸŸè¿ç§»èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01872",
            "title": "Is Extending Modality The Right Path Towards Omni-Modality?",
            "url": "https://huggingface.co/papers/2506.01872",
            "abstract": "Research investigates the impact of extending modality and model merging on maintaining language abilities and generalization in omni-modal language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Omni-modal language models (OLMs) aim to integrate and reason over diverse input modalities--such as text, images, video, and audio--while maintaining strong language capabilities. Despite recent advancements, existing models, especially open-source ones, remain far from true omni-modality, struggling to generalize beyond the specific modality pairs they are trained on or to achieve strong performance when processing multi-modal inputs. We study the effect of extending modality, the dominant technique for training multimodal models, where an off-the-shelf language model is fine-tuned on target-domain and language data. Specifically, we investigate three key questions: (1) Does modality extension compromise core language abilities? (2) Can model merging effectively integrate independently fine-tuned modality-specific models to achieve omni-modality? (3) Does omni-modality extension lead to better knowledge sharing and generalization compared to sequential extension? Through extensive experiments, we analyze these trade-offs and provide insights into the feasibility of achieving true omni-modality using current approaches.",
            "score": 14,
            "issue_id": 4193,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "f876844db3f1bfbd",
            "authors": [
                "Tinghui Zhu",
                "Kai Zhang",
                "Muhao Chen",
                "Yu Su"
            ],
            "affiliations": [
                "The Ohio State University",
                "University of California, Davis"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01872.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#agi",
                    "#multimodal",
                    "#open_source",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑƒÑ‚ÑŒ Ğº Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¼Ğ½Ğ¸-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ² Ğ¾Ğ¼Ğ½Ğ¸-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚, ĞºĞ°Ğº Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ¼Ğ½Ğ¸-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ğ¼Ğ½Ğ¸-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ¾ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¼Ğ½Ğ¸-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ²."
                },
                "en": {
                    "title": "Unlocking True Omni-Modality in Language Models",
                    "desc": "This research explores how extending the types of data (modalities) and merging different models can help omni-modal language models (OLMs) maintain their language skills and improve their ability to generalize across various inputs. OLMs are designed to work with multiple forms of data, like text and images, but often struggle to perform well when faced with new combinations of these inputs. The study examines whether adding new modalities affects the language capabilities of these models, if merging models trained on different modalities can create a more effective omni-modal model, and whether this approach enhances knowledge sharing and generalization. Through experiments, the paper provides valuable insights into the challenges and potential solutions for achieving true omni-modality in language models."
                },
                "zh": {
                    "title": "å®ç°çœŸæ­£çš„å…¨æ¨¡æ€èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†æ‰©å±•æ¨¡æ€å’Œæ¨¡å‹åˆå¹¶å¯¹å…¨æ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨ä¿æŒè¯­è¨€èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢çš„å½±å“ã€‚å…¨æ¨¡æ€è¯­è¨€æ¨¡å‹æ—¨åœ¨æ•´åˆå’Œæ¨ç†å¤šç§è¾“å…¥æ¨¡æ€ï¼Œå¦‚æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ã€‚å°½ç®¡å·²æœ‰è¿›å±•ï¼Œç°æœ‰æ¨¡å‹åœ¨çœŸæ­£çš„å…¨æ¨¡æ€èƒ½åŠ›ä¸Šä»ç„¶å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤šæ¨¡æ€è¾“å…¥æ—¶çš„æ³›åŒ–èƒ½åŠ›è¾ƒå¼±ã€‚æˆ‘ä»¬é€šè¿‡å®éªŒåˆ†æäº†æ‰©å±•æ¨¡æ€å¯¹æ ¸å¿ƒè¯­è¨€èƒ½åŠ›çš„å½±å“ï¼Œä»¥åŠæ¨¡å‹åˆå¹¶æ˜¯å¦èƒ½æœ‰æ•ˆæ•´åˆç‹¬ç«‹å¾®è°ƒçš„æ¨¡æ€ç‰¹å®šæ¨¡å‹ï¼Œä»¥å®ç°å…¨æ¨¡æ€èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05984",
            "title": "Audio-Aware Large Language Models as Judges for Speaking Styles",
            "url": "https://huggingface.co/papers/2506.05984",
            "abstract": "Audio-aware large language models can assess speaking styles in audio inputs, demonstrating performance comparable to human judges in evaluating synthesized speech along dimensions like emotion, volume, and pitch.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-aware large language models (ALLMs) can understand the textual and non-textual information in the audio input. In this paper, we explore using ALLMs as an automatic judge to assess the speaking styles of speeches. We use ALLM judges to evaluate the speeches generated by SLMs on two tasks: voice style instruction following and role-playing. The speaking style we consider includes emotion, volume, speaking pace, word emphasis, pitch control, and non-verbal elements. We use four spoken language models (SLMs) to complete the two tasks and use humans and ALLMs to judge the SLMs' responses. We compare two ALLM judges, GPT-4o-audio and Gemini-2.5-pro, with human evaluation results and show that the agreement between Gemini and human judges is comparable to the agreement between human evaluators. These promising results show that ALLMs can be used as a judge to evaluate SLMs. Our results also reveal that current SLMs, even GPT-4o-audio, still have room for improvement in controlling the speaking style and generating natural dialogues.",
            "score": 10,
            "issue_id": 4185,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 Ğ¸ÑĞ½Ñ",
                "en": "June 6",
                "zh": "6æœˆ6æ—¥"
            },
            "hash": "10dcc4567ff634c1",
            "authors": [
                "Cheng-Han Chiang",
                "Xiaofei Wang",
                "Chung-Ching Lin",
                "Kevin Lin",
                "Linjie Li",
                "Radu Kopetz",
                "Yao Qian",
                "Zhendong Wang",
                "Zhengyuan Yang",
                "Hung-yi Lee",
                "Lijuan Wang"
            ],
            "affiliations": [
                "Microsoft",
                "National Taiwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05984.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#audio",
                    "#interpretability",
                    "#games"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "ĞĞĞ‘Ğ›Ğœ ĞºĞ°Ğº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑƒĞ´ÑŒĞ¸ ÑÑ‚Ğ¸Ğ»Ñ Ñ€ĞµÑ‡Ğ¸",
                    "desc": "ĞÑƒĞ´Ğ¸Ğ¾-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (ĞĞĞ‘Ğ›Ğœ) ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ¸Ğ»Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ñ…Ğ¾Ğ´Ğ°Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ÑƒÑ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ»ÑĞ´ĞµĞ¹-ÑÑƒĞ´ĞµĞ¹. Ğ’ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ĞĞĞ‘Ğ›Ğœ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµÑ‡ĞµĞ¹, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (Ğ Ğ¯Ğœ) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¿Ğ¾ ÑÑ‚Ğ¸Ğ»Ñ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ğ¸ Ñ€Ğ¾Ğ»ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ³Ñ€Ñ‹. ĞÑ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹, ĞºĞ°Ğº ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¸, Ğ³Ñ€Ğ¾Ğ¼ĞºĞ¾ÑÑ‚ÑŒ, Ñ‚ĞµĞ¼Ğ¿ Ñ€ĞµÑ‡Ğ¸, Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ², ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ²Ñ‹ÑĞ¾Ñ‚Ñ‹ Ñ‚Ğ¾Ğ½Ğ° Ğ¸ Ğ½ĞµĞ²ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Gemini Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… ÑÑƒĞ´ĞµĞ¹ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ° Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ»ÑĞ´ĞµĞ¹."
                },
                "en": {
                    "title": "Evaluating Speech Styles with AI: ALLMs vs. Human Judges",
                    "desc": "This paper discusses the capabilities of audio-aware large language models (ALLMs) in evaluating speaking styles from audio inputs. The authors demonstrate that ALLMs can assess synthesized speech similarly to human judges, focusing on aspects like emotion, volume, and pitch. They compare the performance of two ALLMs, GPT-4o-audio and Gemini-2.5-pro, against human evaluations in tasks involving voice style instruction and role-playing. The findings indicate that while ALLMs can effectively judge speaking styles, there is still potential for improvement in the speaking style control of current spoken language models (SLMs)."
                },
                "zh": {
                    "title": "éŸ³é¢‘æ„ŸçŸ¥æ¨¡å‹ï¼šè¯„ä¼°è¯´è¯é£æ ¼çš„æ–°å·¥å…·",
                    "desc": "éŸ³é¢‘æ„ŸçŸ¥çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆALLMsï¼‰èƒ½å¤Ÿè¯„ä¼°éŸ³é¢‘è¾“å…¥ä¸­çš„è¯´è¯é£æ ¼ï¼Œå…¶è¡¨ç°ä¸äººç±»è¯„å®¡åœ¨æƒ…æ„Ÿã€éŸ³é‡å’ŒéŸ³è°ƒç­‰ç»´åº¦ä¸Šçš„è¯„ä¼°ç›¸å½“ã€‚æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨ALLMsä½œä¸ºè‡ªåŠ¨è¯„å®¡è€…æ¥è¯„ä¼°æ¼”è®²çš„è¯´è¯é£æ ¼ã€‚æˆ‘ä»¬ä½¿ç”¨å››ä¸ªå£è¯­è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰å®Œæˆä¸¤ä¸ªä»»åŠ¡ï¼Œå¹¶é€šè¿‡äººç±»å’ŒALLMså¯¹SLMsçš„å“åº”è¿›è¡Œè¯„ä¼°ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒALLMså¯ä»¥ä½œä¸ºè¯„å®¡å·¥å…·æ¥è¯„ä¼°SLMsï¼Œä½†å½“å‰çš„SLMsåœ¨æ§åˆ¶è¯´è¯é£æ ¼å’Œç”Ÿæˆè‡ªç„¶å¯¹è¯æ–¹é¢ä»æœ‰æ”¹è¿›ç©ºé—´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05573",
            "title": "PartCrafter: Structured 3D Mesh Generation via Compositional Latent\n  Diffusion Transformers",
            "url": "https://huggingface.co/papers/2506.05573",
            "abstract": "PartCrafter generates complex 3D scenes from single images using a unified compositional architecture with a diffusion transformer, enabling part-aware generation and hierarchical attention.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce PartCrafter, the first structured 3D generative model that jointly synthesizes multiple semantically meaningful and geometrically distinct 3D meshes from a single RGB image. Unlike existing methods that either produce monolithic 3D shapes or follow two-stage pipelines, i.e., first segmenting an image and then reconstructing each segment, PartCrafter adopts a unified, compositional generation architecture that does not rely on pre-segmented inputs. Conditioned on a single image, it simultaneously denoises multiple 3D parts, enabling end-to-end part-aware generation of both individual objects and complex multi-object scenes. PartCrafter builds upon a pretrained 3D mesh diffusion transformer (DiT) trained on whole objects, inheriting the pretrained weights, encoder, and decoder, and introduces two key innovations: (1) A compositional latent space, where each 3D part is represented by a set of disentangled latent tokens; (2) A hierarchical attention mechanism that enables structured information flow both within individual parts and across all parts, ensuring global coherence while preserving part-level detail during generation. To support part-level supervision, we curate a new dataset by mining part-level annotations from large-scale 3D object datasets. Experiments show that PartCrafter outperforms existing approaches in generating decomposable 3D meshes, including parts that are not directly visible in input images, demonstrating the strength of part-aware generative priors for 3D understanding and synthesis. Code and training data will be released.",
            "score": 10,
            "issue_id": 4191,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 Ğ¸ÑĞ½Ñ",
                "en": "June 5",
                "zh": "6æœˆ5æ—¥"
            },
            "hash": "af089266a7086a2d",
            "authors": [
                "Yuchen Lin",
                "Chenguo Lin",
                "Panwang Pan",
                "Honglei Yan",
                "Yiqiang Feng",
                "Yadong Mu",
                "Katerina Fragkiadaki"
            ],
            "affiliations": [
                "ByteDance",
                "Carnegie Mellon University",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05573.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#diffusion",
                    "#open_source",
                    "#architecture",
                    "#dataset"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… 3D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ",
                    "desc": "PartCrafter - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… 3D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹. PartCrafter Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‡Ğ°ÑÑ‚Ğ¸, Ğ½Ğµ Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ½Ğ° Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Transforming Images into 3D Worlds with PartCrafter!",
                    "desc": "PartCrafter is a novel 3D generative model that creates detailed 3D scenes from a single RGB image without needing pre-segmented inputs. It uses a unified architecture that combines part-aware generation with hierarchical attention, allowing it to generate multiple distinct 3D meshes simultaneously. The model leverages a pretrained diffusion transformer to enhance the quality of the generated parts and maintains coherence across the entire scene. By introducing a compositional latent space and a new dataset for part-level supervision, PartCrafter significantly improves the generation of complex 3D structures, even including parts not visible in the original image."
                },
                "zh": {
                    "title": "PartCrafterï¼šä»å•å›¾åƒç”Ÿæˆå¤æ‚3Dåœºæ™¯çš„åˆ›æ–°æ¨¡å‹",
                    "desc": "PartCrafteræ˜¯ä¸€ç§æ–°å‹çš„3Dç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥ä»å•å¼ RGBå›¾åƒç”Ÿæˆå¤šä¸ªè¯­ä¹‰æ˜ç¡®ä¸”å‡ ä½•ä¸Šä¸åŒçš„3Dç½‘æ ¼ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒPartCrafteré‡‡ç”¨ç»Ÿä¸€çš„ç”Ÿæˆæ¶æ„ï¼Œæ— éœ€é¢„å…ˆåˆ†å‰²è¾“å…¥å›¾åƒï¼Œèƒ½å¤ŸåŒæ—¶å»å™ªå¤šä¸ª3Déƒ¨åˆ†ã€‚è¯¥æ¨¡å‹åˆ©ç”¨é¢„è®­ç»ƒçš„3Dç½‘æ ¼æ‰©æ•£å˜æ¢å™¨ï¼Œå¹¶å¼•å…¥äº†ç»„åˆæ½œåœ¨ç©ºé—´å’Œå±‚æ¬¡æ³¨æ„æœºåˆ¶ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„3Dåœºæ™¯åœ¨å…¨å±€ä¸€è‡´æ€§çš„åŒæ—¶ä¿ç•™éƒ¨åˆ†ç»†èŠ‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPartCrafteråœ¨ç”Ÿæˆå¯åˆ†è§£çš„3Dç½‘æ ¼æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨3Dç†è§£å’Œåˆæˆä¸­çš„ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05523",
            "title": "MORSE-500: A Programmatically Controllable Video Benchmark to\n  Stress-Test Multimodal Reasoning",
            "url": "https://huggingface.co/papers/2506.05523",
            "abstract": "MORSE-500, a video benchmark with 500 scripted clips, evaluates multimodal reasoning across six categories, highlighting performance gaps in abstract and planning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid advances in vision-language models (VLMs), current benchmarks for multimodal reasoning fall short in three key dimensions. First, they overwhelmingly rely on static images, failing to capture the temporal complexity of real-world environments. Second, they narrowly focus on mathematical problem-solving, neglecting the broader spectrum of reasoning skills -- including abstract, physical, planning, spatial, and temporal capabilities -- required for robust multimodal intelligence. Third, many benchmarks quickly saturate, offering limited headroom for diagnosing failure modes or measuring continued progress. We introduce MORSE-500 (Multimodal Reasoning Stress-test Environment), a video benchmark composed of 500 fully scripted clips with embedded questions spanning six complementary reasoning categories. Each instance is programmatically generated using deterministic Python scripts (via Manim, Matplotlib, MoviePy), generative video models, and curated real footage. This script-driven design allows fine-grained control over visual complexity, distractor density, and temporal dynamics -- enabling difficulty to be scaled systematically as models improve. Unlike static benchmarks that become obsolete once saturated, MORSE-500 is built to evolve: its controllable generation pipeline supports the creation of arbitrarily challenging new instances, making it ideally suited for stress-testing next-generation models. Initial experiments with state-of-the-art systems -- including various Gemini 2.5 Pro and OpenAI o3 which represent the strongest available at the time, alongside strong open-source models -- reveal substantial performance gaps across all categories, with particularly large deficits in abstract and planning tasks. We release the full dataset, generation scripts, and evaluation harness to support transparent, reproducible, and forward-looking multimodal reasoning research.",
            "score": 10,
            "issue_id": 4188,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 Ğ¸ÑĞ½Ñ",
                "en": "June 5",
                "zh": "6æœˆ5æ—¥"
            },
            "hash": "524394ef06ba3ba1",
            "authors": [
                "Zikui Cai",
                "Andrew Wang",
                "Anirudh Satheesh",
                "Ankit Nakhawa",
                "Hyunwoo Jae",
                "Keenan Powell",
                "Minghui Liu",
                "Neel Jay",
                "Sungbin Oh",
                "Xiyao Wang",
                "Yongyuan Liang",
                "Tom Goldstein",
                "Furong Huang"
            ],
            "affiliations": [
                "Capital One",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05523.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#survey",
                    "#video",
                    "#open_source",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "MORSE-500: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "MORSE-500 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 500 ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¿Ğ¾ ÑˆĞµÑÑ‚Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "MORSE-500: Evolving Benchmark for Multimodal Reasoning",
                    "desc": "MORSE-500 is a new video benchmark designed to evaluate multimodal reasoning in AI across six different categories. It addresses limitations in existing benchmarks by incorporating dynamic video clips instead of static images, allowing for a more realistic assessment of reasoning skills. The benchmark includes a variety of reasoning tasks, such as abstract thinking and planning, which are essential for advanced multimodal intelligence. By providing a scalable and evolving dataset, MORSE-500 aims to facilitate ongoing research and development in multimodal reasoning capabilities."
                },
                "zh": {
                    "title": "MORSE-500ï¼šå¤šæ¨¡æ€æ¨ç†çš„æ–°åŸºå‡†",
                    "desc": "MORSE-500æ˜¯ä¸€ä¸ªåŒ…å«500ä¸ªè„šæœ¬åŒ–è§†é¢‘ç‰‡æ®µçš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†æ¶µç›–å…­ä¸ªäº’è¡¥çš„æ¨ç†ç±»åˆ«ï¼Œå¼ºè°ƒäº†åœ¨æŠ½è±¡å’Œè§„åˆ’ä»»åŠ¡ä¸­çš„æ€§èƒ½å·®è·ã€‚ä¸é™æ€å›¾åƒåŸºå‡†ä¸åŒï¼ŒMORSE-500èƒ½å¤Ÿæ•æ‰ç°å®ç¯å¢ƒçš„æ—¶é—´å¤æ‚æ€§ï¼Œå¹¶æ”¯æŒç”Ÿæˆå…·æœ‰ä¸åŒéš¾åº¦çš„æ–°å®ä¾‹ã€‚é€šè¿‡æä¾›å®Œæ•´çš„æ•°æ®é›†å’Œç”Ÿæˆè„šæœ¬ï¼ŒMORSE-500ä¸ºå¤šæ¨¡æ€æ¨ç†ç ”ç©¶æä¾›äº†é€æ˜å’Œå¯é‡å¤çš„æ”¯æŒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05446",
            "title": "Sentinel: SOTA model to protect against prompt injections",
            "url": "https://huggingface.co/papers/2506.05446",
            "abstract": "Sentinel, a detection model based on ModernBERT-large, effectively identifies prompt injection attacks with high accuracy and outperforms existing baselines.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are increasingly powerful but remain vulnerable to prompt injection attacks, where malicious inputs cause the model to deviate from its intended instructions. This paper introduces Sentinel, a novel detection model, qualifire/prompt-injection-sentinel, based on the \\answerdotai/ModernBERT-large architecture. By leveraging ModernBERT's advanced features and fine-tuning on an extensive and diverse dataset comprising a few open-source and private collections, Sentinel achieves state-of-the-art performance. This dataset amalgamates varied attack types, from role-playing and instruction hijacking to attempts to generate biased content, alongside a broad spectrum of benign instructions, with private datasets specifically targeting nuanced error correction and real-world misclassifications. On a comprehensive, unseen internal test set, Sentinel demonstrates an average accuracy of 0.987 and an F1-score of 0.980. Furthermore, when evaluated on public benchmarks, it consistently outperforms strong baselines like protectai/deberta-v3-base-prompt-injection-v2. This work details Sentinel's architecture, its meticulous dataset curation, its training methodology, and a thorough evaluation, highlighting its superior detection capabilities.",
            "score": 10,
            "issue_id": 4199,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 Ğ¸ÑĞ½Ñ",
                "en": "June 5",
                "zh": "6æœˆ5æ—¥"
            },
            "hash": "293cbf4b92765fe2",
            "authors": [
                "Dror Ivry",
                "Oran Nahum"
            ],
            "affiliations": [
                "Qualiï¬re, Tel Aviv, IL"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05446.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#data",
                    "#security",
                    "#ethics",
                    "#dataset",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Sentinel: ĞŸĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ Ğ°Ñ‚Ğ°Ğº Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Sentinel - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ñ‚Ğ°Ğº Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ ModernBERT-large. Sentinel Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ (0.987) Ğ¸ F1-Ğ¼ĞµÑ€Ñƒ (0.980) Ğ½Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¼ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ°Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ¾Ğ±Ñ€Ğ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Sentinel, Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Sentinel: Safeguarding LLMs from Prompt Injection Attacks",
                    "desc": "The paper presents Sentinel, a detection model built on the ModernBERT-large architecture, designed to identify prompt injection attacks in large language models (LLMs). Prompt injection attacks can manipulate LLMs into producing unintended outputs, making effective detection crucial. Sentinel is trained on a diverse dataset that includes various attack types and benign instructions, achieving high accuracy and F1-scores in its evaluations. The results show that Sentinel outperforms existing models, demonstrating its potential as a robust solution for enhancing the security of LLMs against such vulnerabilities."
                },
                "zh": {
                    "title": "Sentinelï¼šé«˜æ•ˆè¯†åˆ«æç¤ºæ³¨å…¥æ”»å‡»çš„æ£€æµ‹æ¨¡å‹",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSentinelçš„æ£€æµ‹æ¨¡å‹ï¼ŒåŸºäºModernBERT-largeæ¶æ„ï¼Œèƒ½å¤Ÿé«˜æ•ˆè¯†åˆ«æç¤ºæ³¨å…¥æ”»å‡»ã€‚æç¤ºæ³¨å…¥æ”»å‡»æ˜¯æŒ‡æ¶æ„è¾“å…¥å¯¼è‡´å¤§å‹è¯­è¨€æ¨¡å‹åç¦»é¢„æœŸæŒ‡ä»¤çš„æƒ…å†µã€‚Sentinelé€šè¿‡åœ¨å¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œè¾¾åˆ°äº†å…ˆè¿›çš„æ£€æµ‹æ€§èƒ½ï¼Œå¹³å‡å‡†ç¡®ç‡ä¸º0.987ï¼ŒF1åˆ†æ•°ä¸º0.980ã€‚è¯¥æ¨¡å‹åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿè¡¨ç°ä¼˜äºç°æœ‰çš„å¼ºåŸºçº¿ï¼Œå±•ç¤ºäº†å…¶å“è¶Šçš„æ£€æµ‹èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.06276",
            "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image\n  Synthesis",
            "url": "https://huggingface.co/papers/2506.06276",
            "abstract": "STARFlow, a generative model combining normalizing flows with autoregressive Transformers, achieves competitive image synthesis performance with innovations in architecture and latent space modeling.  \t\t\t\t\tAI-generated summary \t\t\t\t We present STARFlow, a scalable generative model based on normalizing flows that achieves strong performance in high-resolution image synthesis. The core of STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the expressive power of normalizing flows with the structured modeling capabilities of Autoregressive Transformers. We first establish the theoretical universality of TARFlow for modeling continuous distributions. Building on this foundation, we introduce several key architectural and algorithmic innovations to significantly enhance scalability: (1) a deep-shallow design, wherein a deep Transformer block captures most of the model representational capacity, complemented by a few shallow Transformer blocks that are computationally efficient yet substantially beneficial; (2) modeling in the latent space of pretrained autoencoders, which proves more effective than direct pixel-level modeling; and (3) a novel guidance algorithm that significantly boosts sample quality. Crucially, our model remains an end-to-end normalizing flow, enabling exact maximum likelihood training in continuous spaces without discretization. STARFlow achieves competitive performance in both class-conditional and text-conditional image generation tasks, approaching state-of-the-art diffusion models in sample quality. To our knowledge, this work is the first successful demonstration of normalizing flows operating effectively at this scale and resolution.",
            "score": 9,
            "issue_id": 4189,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 Ğ¸ÑĞ½Ñ",
                "en": "June 6",
                "zh": "6æœˆ6æ—¥"
            },
            "hash": "14f98c6826d7c6bb",
            "authors": [
                "Jiatao Gu",
                "Tianrong Chen",
                "David Berthelot",
                "Huangjie Zheng",
                "Yuyang Wang",
                "Ruixiang Zhang",
                "Laurent Dinh",
                "Miguel Angel Bautista",
                "Josh Susskind",
                "Shuangfei Zhai"
            ],
            "affiliations": [
                "Apple"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.06276.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ¿Ğ¾ĞºĞ¾Ñ€ÑÑÑ‚ Ğ²Ñ‹ÑĞ¾Ñ‚Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "STARFlow - ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾-Ğ¼ĞµĞ»ĞºÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑÑĞ¼Ğ¿Ğ»Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. STARFlow Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ĞºĞ»Ğ°ÑÑÑƒ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ÑÑÑŒ Ğº ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼."
                },
                "en": {
                    "title": "STARFlow: Merging Flows and Transformers for High-Quality Image Generation",
                    "desc": "STARFlow is a generative model that merges normalizing flows with autoregressive Transformers to create high-quality images. It introduces the Transformer Autoregressive Flow (TARFlow), which effectively models continuous distributions while maintaining scalability. Key innovations include a deep-shallow architecture for efficient computation, latent space modeling using pretrained autoencoders, and a novel guidance algorithm to enhance sample quality. This model achieves competitive results in both class-conditional and text-conditional image generation, marking a significant advancement in the use of normalizing flows for high-resolution image synthesis."
                },
                "zh": {
                    "title": "STARFlowï¼šé«˜æ•ˆå›¾åƒåˆæˆçš„æ–°çºªå…ƒ",
                    "desc": "STARFlowæ˜¯ä¸€ç§ç»“åˆäº†å½’ä¸€åŒ–æµå’Œè‡ªå›å½’å˜æ¢å™¨çš„ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆä¸­å®ç°å¼ºå¤§çš„æ€§èƒ½ã€‚å…¶æ ¸å¿ƒæ˜¯å˜æ¢å™¨è‡ªå›å½’æµï¼ˆTARFlowï¼‰ï¼Œå°†å½’ä¸€åŒ–æµçš„è¡¨è¾¾èƒ½åŠ›ä¸è‡ªå›å½’å˜æ¢å™¨çš„ç»“æ„å»ºæ¨¡èƒ½åŠ›ç›¸ç»“åˆã€‚é€šè¿‡æ·±æµ…è®¾è®¡ã€åœ¨é¢„è®­ç»ƒè‡ªç¼–ç å™¨çš„æ½œåœ¨ç©ºé—´å»ºæ¨¡ä»¥åŠæ–°é¢–çš„å¼•å¯¼ç®—æ³•ï¼ŒSTARFlowæ˜¾è‘—æé«˜äº†å¯æ‰©å±•æ€§å’Œæ ·æœ¬è´¨é‡ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨è¿ç»­ç©ºé—´ä¸­è¿›è¡Œç²¾ç¡®çš„æœ€å¤§ä¼¼ç„¶è®­ç»ƒï¼Œä¸”åœ¨ç±»æ¡ä»¶å’Œæ–‡æœ¬æ¡ä»¶çš„å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ¥è¿‘æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.06253",
            "title": "Bridging Perspectives: A Survey on Cross-view Collaborative Intelligence\n  with Egocentric-Exocentric Vision",
            "url": "https://huggingface.co/papers/2506.06253",
            "abstract": "A survey reviews advancements in video understanding from both egocentric and exocentric perspectives, highlighting applications, tasks, joint learning frameworks, and limitations, with the aim of enhancing machine perception.  \t\t\t\t\tAI-generated summary \t\t\t\t Perceiving the world from both egocentric (first-person) and exocentric (third-person) perspectives is fundamental to human cognition, enabling rich and complementary understanding of dynamic environments. In recent years, allowing the machines to leverage the synergistic potential of these dual perspectives has emerged as a compelling research direction in video understanding. In this survey, we provide a comprehensive review of video understanding from both exocentric and egocentric viewpoints. We begin by highlighting the practical applications of integrating egocentric and exocentric techniques, envisioning their potential collaboration across domains. We then identify key research tasks to realize these applications. Next, we systematically organize and review recent advancements into three main research directions: (1) leveraging egocentric data to enhance exocentric understanding, (2) utilizing exocentric data to improve egocentric analysis, and (3) joint learning frameworks that unify both perspectives. For each direction, we analyze a diverse set of tasks and relevant works. Additionally, we discuss benchmark datasets that support research in both perspectives, evaluating their scope, diversity, and applicability. Finally, we discuss limitations in current works and propose promising future research directions. By synthesizing insights from both perspectives, our goal is to inspire advancements in video understanding and artificial intelligence, bringing machines closer to perceiving the world in a human-like manner. A GitHub repo of related works can be found at https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision.",
            "score": 5,
            "issue_id": 4192,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 Ğ¸ÑĞ½Ñ",
                "en": "June 6",
                "zh": "6æœˆ6æ—¥"
            },
            "hash": "7ddc25331945068e",
            "authors": [
                "Yuping He",
                "Yifei Huang",
                "Guo Chen",
                "Lidong Lu",
                "Baoqi Pei",
                "Jilan Xu",
                "Tong Lu",
                "Yoichi Sato"
            ],
            "affiliations": [
                "Fudan University, Shanghai 200433, China",
                "State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China",
                "University of Tokyo, Tokyo, Japan",
                "Zhejiang University, Zhejiang 310027, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.06253.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video",
                    "#survey",
                    "#benchmark"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ ÑĞºĞ·Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ²Ğ° Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸ ÑĞºĞ·Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ¾Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞºĞ·Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞºĞ·Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ±Ğ·Ğ¾Ñ€ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Bridging Perspectives for Enhanced Video Understanding",
                    "desc": "This paper surveys the progress in video understanding by examining both egocentric (first-person) and exocentric (third-person) perspectives. It emphasizes the importance of combining these viewpoints to enhance machine perception of dynamic environments. The authors categorize recent advancements into three main research directions: improving exocentric understanding with egocentric data, enhancing egocentric analysis with exocentric data, and developing joint learning frameworks. The survey also discusses practical applications, key research tasks, benchmark datasets, and identifies limitations in current research while suggesting future directions."
                },
                "zh": {
                    "title": "èåˆè§†è§’ï¼Œæå‡è§†é¢‘ç†è§£",
                    "desc": "è¿™ç¯‡è®ºæ–‡ç»¼è¿°äº†è§†é¢‘ç†è§£é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œé‡ç‚¹å…³æ³¨è‡ªæˆ‘ä¸­å¿ƒï¼ˆç¬¬ä¸€äººç§°ï¼‰å’Œå¤–éƒ¨ä¸­å¿ƒï¼ˆç¬¬ä¸‰äººç§°ï¼‰è§†è§’çš„ç»“åˆã€‚é€šè¿‡æ•´åˆè¿™ä¸¤ç§è§†è§’ï¼Œç ”ç©¶è€…ä»¬å¸Œæœ›æå‡æœºå™¨å¯¹åŠ¨æ€ç¯å¢ƒçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚è®ºæ–‡è¿˜è¯†åˆ«äº†å®ç°è¿™äº›åº”ç”¨çš„å…³é”®ç ”ç©¶ä»»åŠ¡ï¼Œå¹¶ç³»ç»Ÿåœ°ç»„ç»‡äº†æœ€è¿‘çš„ç ”ç©¶è¿›å±•ã€‚æœ€åï¼Œä½œè€…è®¨è®ºäº†å½“å‰å·¥ä½œçš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œä»¥æ¨åŠ¨è§†é¢‘ç†è§£å’Œäººå·¥æ™ºèƒ½çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05433",
            "title": "Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward",
            "url": "https://huggingface.co/papers/2506.05433",
            "abstract": "Prefix Grouper reduces computational overhead in GRPO by encoding shared prefixes only once, improving scalability in long-context scenarios without altering training dynamics or policy performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Group Relative Policy Optimization (GRPO) enhances policy learning by computing gradients from relative comparisons among candidate outputs that share a common input prefix. Despite its effectiveness, GRPO introduces substantial computational overhead when processing long shared prefixes, which must be redundantly encoded for each group member. This inefficiency becomes a major scalability bottleneck in long-context learning scenarios. We propose Prefix Grouper, an efficient GRPO training algorithm that eliminates redundant prefix computation via a Shared-Prefix Forward strategy. In particular, by restructuring self-attention into two parts, our method enables the shared prefix to be encoded only once, while preserving full differentiability and compatibility with end-to-end training. We provide both theoretical and empirical evidence that Prefix Grouper is training-equivalent to standard GRPO: it yields identical forward outputs and backward gradients, ensuring that the optimization dynamics and final policy performance remain unchanged. Empirically, our experiments confirm that Prefix Grouper achieves consistent results while significantly reducing the computational cost of training, particularly in long-prefix scenarios. The proposed method is fully plug-and-play: it is compatible with existing GRPO-based architectures and can be seamlessly integrated into current training pipelines as a drop-in replacement, requiring no structural modifications and only minimal changes to input construction and attention computation. Prefix Grouper enables the use of larger group sizes under the same computational budget, thereby improving the scalability of GRPO to more complex tasks and larger models. Code is now available at https://github.com/johncaged/PrefixGrouper",
            "score": 4,
            "issue_id": 4192,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 Ğ¸ÑĞ½Ñ",
                "en": "June 5",
                "zh": "6æœˆ5æ—¥"
            },
            "hash": "fe9abfe1e500f3e2",
            "authors": [
                "Zikang Liu",
                "Tongtian Yue",
                "Yepeng Tang",
                "Longteng Guo",
                "Junxian Cai",
                "Qingbin Liu",
                "Xi Chen",
                "Jing Liu"
            ],
            "affiliations": [
                "Basic Algorithm Center, Tencent",
                "Institute of Automation, Chinese Academy of Sciences",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "School of Computer Science and Technology, Beijing Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05433.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#optimization",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "ĞŸÑ€ĞµÑ„Ğ¸ĞºÑ Ğ“Ñ€ÑƒĞ¿Ğ¿ĞµÑ€: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ GRPO Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²",
                    "desc": "ĞŸÑ€ĞµÑ„Ğ¸ĞºÑ Ğ“Ñ€ÑƒĞ¿Ğ¿ĞµÑ€ - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ GRPO (Group Relative Policy Optimization), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ´Ğ²Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ¸Ğ½ Ñ€Ğ°Ğ·, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ´Ğ¾ ĞºĞ¾Ğ½Ñ†Ğ°. ĞŸÑ€ĞµÑ„Ğ¸ĞºÑ Ğ“Ñ€ÑƒĞ¿Ğ¿ĞµÑ€ Ğ½Ğµ Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ½ĞµÑ‡Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ°Ğ¼Ğ¸, Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GRPO."
                },
                "en": {
                    "title": "Efficiently Scaling GRPO with Prefix Grouper",
                    "desc": "The paper introduces Prefix Grouper, a novel algorithm designed to enhance the efficiency of Group Relative Policy Optimization (GRPO) by reducing computational overhead associated with encoding shared prefixes. By implementing a Shared-Prefix Forward strategy, it allows the shared prefix to be encoded only once, which significantly improves scalability in long-context scenarios without compromising the training dynamics or policy performance. The method maintains full differentiability and is compatible with end-to-end training, ensuring that the optimization process remains unchanged. Empirical results demonstrate that Prefix Grouper not only achieves consistent performance but also allows for larger group sizes within the same computational budget, making it a valuable addition to GRPO-based architectures."
                },
                "zh": {
                    "title": "Prefix Grouperï¼šæå‡ GRPO çš„å¯æ‰©å±•æ€§",
                    "desc": "Prefix Grouper æ˜¯ä¸€ç§é«˜æ•ˆçš„ GRPO è®­ç»ƒç®—æ³•ï¼Œé€šè¿‡å…±äº«å‰ç¼€çš„å‰å‘ç­–ç•¥ï¼Œæ¶ˆé™¤äº†å†—ä½™çš„å‰ç¼€è®¡ç®—ï¼Œä»è€Œå‡å°‘äº†è®¡ç®—å¼€é”€ã€‚è¯¥æ–¹æ³•å°†è‡ªæ³¨æ„åŠ›ç»“æ„é‡ç»„ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼Œä½¿å¾—å…±äº«å‰ç¼€åªéœ€ç¼–ç ä¸€æ¬¡ï¼ŒåŒæ—¶ä¿æŒå®Œå…¨çš„å¯å¾®æ€§å’Œä¸ç«¯åˆ°ç«¯è®­ç»ƒçš„å…¼å®¹æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPrefix Grouper åœ¨é•¿å‰ç¼€åœºæ™¯ä¸­æ˜¾è‘—é™ä½äº†è®­ç»ƒçš„è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ç¡®ä¿ä¼˜åŒ–åŠ¨æ€å’Œæœ€ç»ˆç­–ç•¥æ€§èƒ½ä¸å˜ã€‚è¯¥æ–¹æ³•å¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„ GRPO æ¶æ„ä¸­ï¼Œæ”¯æŒæ›´å¤§çš„ç»„å¤§å°ï¼Œä»è€Œæé«˜ GRPO åœ¨å¤æ‚ä»»åŠ¡å’Œå¤§æ¨¡å‹ä¸­çš„å¯æ‰©å±•æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.06199",
            "title": "3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World\n  Model",
            "url": "https://huggingface.co/papers/2506.06199",
            "abstract": "A 3D flow world model learned from human and robot manipulation data, using video diffusion and GPT-4o, enables robots to perform diverse manipulation tasks with strong generalization and cross-embodiment adaptation.  \t\t\t\t\tAI-generated summary \t\t\t\t Manipulation has long been a challenging task for robots, while humans can effortlessly perform complex interactions with objects, such as hanging a cup on the mug rack. A key reason is the lack of a large and uniform dataset for teaching robots manipulation skills. Current robot datasets often record robot action in different action spaces within a simple scene. This hinders the robot to learn a unified and robust action representation for different robots within diverse scenes. Observing how humans understand a manipulation task, we find that understanding how the objects should move in the 3D space is a critical clue for guiding actions. This clue is embodiment-agnostic and suitable for both humans and different robots. Motivated by this, we aim to learn a 3D flow world model from both human and robot manipulation data. This model predicts the future movement of the interacting objects in 3D space, guiding action planning for manipulation. Specifically, we synthesize a large-scale 3D optical flow dataset, named ManiFlow-110k, through a moving object auto-detect pipeline. A video diffusion-based world model then learns manipulation physics from these data, generating 3D optical flow trajectories conditioned on language instructions. With the generated 3D object optical flow, we propose a flow-guided rendering mechanism, which renders the predicted final state and leverages GPT-4o to assess whether the predicted flow aligns with the task description. This equips the robot with a closed-loop planning ability. Finally, we consider the predicted 3D optical flow as constraints for an optimization policy to determine a chunk of robot actions for manipulation. Extensive experiments demonstrate strong generalization across diverse robotic manipulation tasks and reliable cross-embodiment adaptation without hardware-specific training.",
            "score": 3,
            "issue_id": 4186,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 Ğ¸ÑĞ½Ñ",
                "en": "June 6",
                "zh": "6æœˆ6æ—¥"
            },
            "hash": "c1b9d0a9c29bdf3b",
            "authors": [
                "Hongyan Zhi",
                "Peihao Chen",
                "Siyuan Zhou",
                "Yubo Dong",
                "Quanxi Wu",
                "Lei Han",
                "Mingkui Tan"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Pazhou Laboratory",
                "South China University of Technology",
                "Tencent Robotics"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.06199.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#synthetic",
                    "#robotics",
                    "#optimization",
                    "#3d",
                    "#games"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ 3D-Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ 3D-Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ»Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ¸ GPT-4o Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² 3D-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ManiFlow-110k Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering Robots with 3D Flow for Versatile Manipulation",
                    "desc": "This paper presents a novel 3D flow world model that enables robots to learn manipulation tasks by leveraging both human and robot data. The model predicts how objects move in 3D space, which helps robots plan their actions more effectively. By creating a large dataset called ManiFlow-110k and using a video diffusion approach, the researchers teach robots to understand manipulation physics and generate action plans based on language instructions. The results show that this method allows robots to generalize well across various tasks and adapt to different robotic embodiments without needing specific training for each hardware type."
                },
                "zh": {
                    "title": "å­¦ä¹ 3DæµåŠ¨æ¨¡å‹ï¼Œæå‡æœºå™¨äººæ“ä½œèƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§ä»äººç±»å’Œæœºå™¨äººæ“ä½œæ•°æ®ä¸­å­¦ä¹ çš„3DæµåŠ¨ä¸–ç•Œæ¨¡å‹ï¼Œæ—¨åœ¨å¸®åŠ©æœºå™¨äººæ‰§è¡Œå¤šæ ·åŒ–çš„æ“ä½œä»»åŠ¡ã€‚é€šè¿‡åˆæˆä¸€ä¸ªåä¸ºManiFlow-110kçš„å¤§è§„æ¨¡3Då…‰æµæ•°æ®é›†ï¼Œæ¨¡å‹èƒ½å¤Ÿé¢„æµ‹äº¤äº’å¯¹è±¡åœ¨3Dç©ºé—´ä¸­çš„æœªæ¥è¿åŠ¨ã€‚åˆ©ç”¨è§†é¢‘æ‰©æ•£æŠ€æœ¯å’ŒGPT-4oï¼Œæ¨¡å‹ç”Ÿæˆçš„3Då…‰æµè½¨è¿¹å¯ä»¥æŒ‡å¯¼æœºå™¨äººçš„æ“ä½œè§„åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä¸åŒçš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œè·¨å®ä½“é€‚åº”æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05817",
            "title": "CodeContests+: High-Quality Test Case Generation for Competitive\n  Programming",
            "url": "https://huggingface.co/papers/2506.05817",
            "abstract": "An LLM-based system generates high-quality test cases for competitive programming problems, enhancing the accuracy of model evaluation and RL performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Competitive programming, due to its high reasoning difficulty and precise correctness feedback, has become a key task for both training and evaluating the reasoning capabilities of large language models (LLMs). However, while a large amount of public problem data, such as problem statements and solutions, is available, the test cases of these problems are often difficult to obtain. Therefore, test case generation is a necessary task for building large-scale datasets, and the quality of the test cases directly determines the accuracy of the evaluation. In this paper, we introduce an LLM-based agent system that creates high-quality test cases for competitive programming problems. We apply this system to the CodeContests dataset and propose a new version with improved test cases, named CodeContests+. We evaluated the quality of test cases in CodeContestsPlus. First, we used 1.72 million submissions with pass/fail labels to examine the accuracy of these test cases in evaluation. The results indicated that CodeContests+ achieves significantly higher accuracy than CodeContests, particularly with a notably higher True Positive Rate (TPR). Subsequently, our experiments in LLM Reinforcement Learning (RL) further confirmed that improvements in test case quality yield considerable advantages for RL.",
            "score": 2,
            "issue_id": 4195,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 Ğ¸ÑĞ½Ñ",
                "en": "June 6",
                "zh": "6æœˆ6æ—¥"
            },
            "hash": "f0f3a151758192f0",
            "authors": [
                "Zihan Wang",
                "Siyao Liu",
                "Yang Sun",
                "Hongyan Li",
                "Kai Shen"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05817.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rl",
                    "#optimization",
                    "#reasoning",
                    "#dataset",
                    "#data"
                ],
                "emoji": "ğŸ†",
                "ru": {
                    "title": "Ğ˜Ğ˜ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ‚ĞµÑÑ‚Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğº Ğ½Ğ°Ğ±Ğ¾Ñ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… CodeContests, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ CodeContests+. ĞÑ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ CodeContests+, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ° (TPR). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ´Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ RL."
                },
                "en": {
                    "title": "Enhancing Competitive Programming Evaluation with LLM-Generated Test Cases",
                    "desc": "This paper presents a system that uses large language models (LLMs) to generate high-quality test cases for competitive programming problems. The generation of these test cases is crucial because they enhance the evaluation accuracy of models and improve reinforcement learning (RL) performance. The authors introduce a new dataset, CodeContests+, which contains improved test cases derived from the original CodeContests dataset. Their evaluation shows that the new test cases significantly increase the True Positive Rate (TPR) and overall accuracy, demonstrating the benefits of high-quality test case generation for model training and assessment."
                },
                "zh": {
                    "title": "åŸºäºLLMçš„é«˜è´¨é‡æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿä¸ºç«äº‰ç¼–ç¨‹é—®é¢˜ç”Ÿæˆé«˜è´¨é‡çš„æµ‹è¯•ç”¨ä¾‹ã€‚è¿™é¡¹æŠ€æœ¯æé«˜äº†æ¨¡å‹è¯„ä¼°çš„å‡†ç¡®æ€§ï¼Œå¹¶å¢å¼ºäº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ€§èƒ½ã€‚ç”±äºç«äº‰ç¼–ç¨‹é—®é¢˜çš„æµ‹è¯•ç”¨ä¾‹éš¾ä»¥è·å–ï¼Œç”Ÿæˆæµ‹è¯•ç”¨ä¾‹æˆä¸ºæ„å»ºå¤§è§„æ¨¡æ•°æ®é›†çš„å¿…è¦ä»»åŠ¡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ”¹è¿›åçš„æµ‹è¯•ç”¨ä¾‹åœ¨å‡†ç¡®æ€§ä¸Šæ˜¾è‘—ä¼˜äºåŸå§‹æ•°æ®é›†ï¼Œå°¤å…¶åœ¨çœŸå®æ­£ä¾‹ç‡ï¼ˆTPRï¼‰æ–¹é¢è¡¨ç°çªå‡ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04120",
            "title": "Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot\n  Data",
            "url": "https://huggingface.co/papers/2506.04120",
            "abstract": "A hybrid real-to-sim framework combining 3D Gaussian Splatting and physics simulation with MuJoCo allows simultaneous high-fidelity object reconstruction and accurate robot pose calibration from raw trajectories.  \t\t\t\t\tAI-generated summary \t\t\t\t Creating accurate, physical simulations directly from real-world robot motion holds great value for safe, scalable, and affordable robot learning, yet remains exceptionally challenging. Real robot data suffers from occlusions, noisy camera poses, dynamic scene elements, which hinder the creation of geometrically accurate and photorealistic digital twins of unseen objects. We introduce a novel real-to-sim framework tackling all these challenges at once. Our key insight is a hybrid scene representation merging the photorealistic rendering of 3D Gaussian Splatting with explicit object meshes suitable for physics simulation within a single representation. We propose an end-to-end optimization pipeline that leverages differentiable rendering and differentiable physics within MuJoCo to jointly refine all scene components - from object geometry and appearance to robot poses and physical parameters - directly from raw and imprecise robot trajectories. This unified optimization allows us to simultaneously achieve high-fidelity object mesh reconstruction, generate photorealistic novel views, and perform annotation-free robot pose calibration. We demonstrate the effectiveness of our approach both in simulation and on challenging real-world sequences using an ALOHA 2 bi-manual manipulator, enabling more practical and robust real-to-simulation pipelines.",
            "score": 2,
            "issue_id": 4194,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 Ğ¸ÑĞ½Ñ",
                "en": "June 4",
                "zh": "6æœˆ4æ—¥"
            },
            "hash": "d808acd50e3bfd0c",
            "authors": [
                "Ben Moran",
                "Mauro Comi",
                "Steven Bohez",
                "Tom Erez",
                "Zhibin Li",
                "Leonard Hasenclever"
            ],
            "affiliations": [
                "Google DeepMind",
                "University College London",
                "University of Bristol"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04120.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ ÑÑ†ĞµĞ½ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ 3D Gaussian Splatting Ğ´Ğ»Ñ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ² MuJoCo. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ğ¹, ÑˆÑƒĞ¼Ğ° Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² ÑÑ†ĞµĞ½Ñ‹, Ğ´ĞµĞ»Ğ°Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¸ĞºĞ¾Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼."
                },
                "en": {
                    "title": "Bridging Reality and Simulation for Enhanced Robot Learning",
                    "desc": "This paper presents a new framework that combines 3D Gaussian Splatting and physics simulation to improve robot learning from real-world data. It addresses challenges like occlusions and noisy camera poses that make it hard to create accurate digital models of objects. The authors introduce a hybrid representation that merges photorealistic rendering with physics-compatible object meshes, allowing for better simulations. Their end-to-end optimization process refines object geometry, appearance, and robot poses from raw data, achieving high-fidelity reconstructions and accurate pose calibration without needing annotations."
                },
                "zh": {
                    "title": "å®ç°çœŸå®ä¸ä»¿çœŸçš„å®Œç¾ç»“åˆ",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ··åˆçš„çœŸå®åˆ°ä»¿çœŸæ¡†æ¶ï¼Œç»“åˆäº†3Dé«˜æ–¯ç‚¹äº‘å’ŒMuJoCoç‰©ç†ä»¿çœŸï¼Œèƒ½å¤Ÿä»åŸå§‹è½¨è¿¹ä¸­åŒæ—¶å®ç°é«˜ä¿çœŸç‰©ä½“é‡å»ºå’Œå‡†ç¡®çš„æœºå™¨äººå§¿æ€æ ¡å‡†ã€‚è¯¥æ¡†æ¶è§£å†³äº†çœŸå®æœºå™¨äººæ•°æ®ä¸­çš„é®æŒ¡ã€å™ªå£°ç›¸æœºå§¿æ€å’ŒåŠ¨æ€åœºæ™¯å…ƒç´ ç­‰æŒ‘æˆ˜ï¼Œåˆ›å»ºå‡ ä½•ä¸Šå‡†ç¡®ä¸”é€¼çœŸçš„æ•°å­—åŒèƒèƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œå°†3Dé«˜æ–¯ç‚¹äº‘çš„å…‰ç…§çœŸå®æ¸²æŸ“ä¸é€‚åˆç‰©ç†ä»¿çœŸçš„ç‰©ä½“ç½‘æ ¼ç»“åˆåœ¨ä¸€èµ·ã€‚é€šè¿‡ç«¯åˆ°ç«¯çš„ä¼˜åŒ–æµç¨‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿç›´æ¥ä»ä¸ç²¾ç¡®çš„æœºå™¨äººè½¨è¿¹ä¸­ä¼˜åŒ–æ‰€æœ‰åœºæ™¯ç»„ä»¶ï¼Œæå‡äº†ç‰©ä½“é‡å»ºçš„ç²¾åº¦å’Œæœºå™¨äººå§¿æ€çš„æ ¡å‡†æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04255",
            "title": "HASHIRU: Hierarchical Agent System for Hybrid Intelligent Resource\n  Utilization",
            "url": "https://huggingface.co/papers/2506.04255",
            "abstract": "HASHIRU, a novel MAS framework, enhances flexibility, resource efficiency, and adaptability by dynamically managing specialized agents and using a hybrid intelligence approach with smaller, local LLMs and external APIs.  \t\t\t\t\tAI-generated summary \t\t\t\t Rapid Large Language Model (LLM) advancements are fueling autonomous Multi-Agent System (MAS) development. However, current frameworks often lack flexibility, resource awareness, model diversity, and autonomous tool creation. This paper introduces HASHIRU (Hierarchical Agent System for Hybrid Intelligent Resource Utilization), a novel MAS framework enhancing flexibility, resource efficiency, and adaptability. HASHIRU features a \"CEO\" agent dynamically managing specialized \"employee\" agents, instantiated based on task needs and resource constraints (cost, memory). Its hybrid intelligence prioritizes smaller, local LLMs (via Ollama) while flexibly using external APIs and larger models when necessary. An economic model with hiring/firing costs promotes team stability and efficient resource allocation. The system also includes autonomous API tool creation and a memory function. Evaluations on tasks like academic paper review (58% success), safety assessments (100% on a JailbreakBench subset), and complex reasoning (outperforming Gemini 2.0 Flash on GSM8K: 96% vs. 61%; JEEBench: 80% vs. 68.3%; SVAMP: 92% vs. 84%) demonstrate HASHIRU's capabilities. Case studies illustrate its self-improvement via autonomous cost model generation, tool integration, and budget management. HASHIRU offers a promising approach for more robust, efficient, and adaptable MAS through dynamic hierarchical control, resource-aware hybrid intelligence, and autonomous functional extension. Source code and benchmarks are available at https://github.com/HASHIRU-AI/HASHIRU and https://github.com/HASHIRU-AI/HASHIRUBench respectively, and a live demo is available at https://hashiruagentx-hashiruai.hf.space upon request.",
            "score": 2,
            "issue_id": 4189,
            "pub_date": "2025-06-01",
            "pub_date_card": {
                "ru": "1 Ğ¸ÑĞ½Ñ",
                "en": "June 1",
                "zh": "6æœˆ1æ—¥"
            },
            "hash": "d3d7d73af3533148",
            "authors": [
                "Kunal Pai",
                "Parth Shah",
                "Harshil Patel"
            ],
            "affiliations": [
                "Independent Researcher",
                "UC Davis"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04255.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#agi",
                    "#optimization",
                    "#benchmark",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±ĞºĞ°Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "HASHIRU - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ API. HASHIRU Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ½Ğ°Ğ¹Ğ¼/ÑƒĞ²Ğ¾Ğ»ÑŒĞ½ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "HASHIRU: Dynamic Intelligence for Efficient Multi-Agent Systems",
                    "desc": "HASHIRU is a new framework for Multi-Agent Systems (MAS) that improves flexibility and resource efficiency by managing specialized agents dynamically. It uses a hybrid intelligence approach, combining smaller local Large Language Models (LLMs) with external APIs to adapt to different tasks. The framework includes a 'CEO' agent that oversees 'employee' agents based on the specific needs and available resources, promoting efficient team management. Evaluations show HASHIRU's strong performance in various tasks, highlighting its ability to autonomously create tools and manage resources effectively."
                },
                "zh": {
                    "title": "HASHIRUï¼šçµæ´»é«˜æ•ˆçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ–°æ¡†æ¶",
                    "desc": "HASHIRUæ˜¯ä¸€ä¸ªæ–°é¢–çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜çµæ´»æ€§ã€èµ„æºæ•ˆç‡å’Œé€‚åº”æ€§ã€‚å®ƒé€šè¿‡åŠ¨æ€ç®¡ç†ä¸“é—¨çš„ä»£ç†ï¼ˆå¦‚â€œCEOâ€ä»£ç†å’Œâ€œå‘˜å·¥â€ä»£ç†ï¼‰æ¥æ»¡è¶³ä»»åŠ¡éœ€æ±‚å’Œèµ„æºé™åˆ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ··åˆæ™ºèƒ½ï¼Œä¼˜å…ˆä½¿ç”¨è¾ƒå°çš„æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¹¶åœ¨å¿…è¦æ—¶çµæ´»è°ƒç”¨å¤–éƒ¨APIå’Œæ›´å¤§çš„æ¨¡å‹ã€‚HASHIRUçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºå…¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶åœ¨åŠ¨æ€æ§åˆ¶å’Œèµ„æºæ„ŸçŸ¥æ–¹é¢çš„ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04755",
            "title": "Truth in the Few: High-Value Data Selection for Efficient Multi-Modal\n  Reasoning",
            "url": "https://huggingface.co/papers/2506.04755",
            "abstract": "A new data selection paradigm, Reasoning Activation Potential (RAP), enhances multi-modal reasoning in large language models using minimal high-value datasets, improving performance and reducing computational costs.  \t\t\t\t\tAI-generated summary \t\t\t\t While multi-modal large language models (MLLMs) have made significant progress in complex reasoning tasks via reinforcement learning, it is commonly believed that extensive training data is necessary for improving multi-modal reasoning ability, inevitably leading to data redundancy and substantial computational costs. However, can smaller high-value datasets match or outperform full corpora for multi-modal reasoning in MLLMs? In this work, we challenge this assumption through a key observation: meaningful multi-modal reasoning is triggered by only a sparse subset of training samples, termed cognitive samples, whereas the majority contribute marginally. Building on this insight, we propose a novel data selection paradigm termed Reasoning Activation Potential (RAP), which identifies cognitive samples by estimating each sample's potential to stimulate genuine multi-modal reasoning by two complementary estimators: 1) Causal Discrepancy Estimator (CDE) based on the potential outcome model principle, eliminates samples that overly rely on language priors by comparing outputs between multi-modal and text-only inputs; 2) Attention Confidence Estimator (ACE), which exploits token-level self-attention to discard samples dominated by irrelevant but over-emphasized tokens in intermediate reasoning stages. Moreover, we introduce a Difficulty-aware Replacement Module (DRM) to substitute trivial instances with cognitively challenging ones, thereby ensuring complexity for robust multi-modal reasoning. Experiments on six datasets show that our RAP method consistently achieves superior performance using only 9.3% of the training data, while reducing computational costs by over 43%. Our code is available at https://github.com/Leo-ssl/RAP.",
            "score": 1,
            "issue_id": 4194,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 Ğ¸ÑĞ½Ñ",
                "en": "June 5",
                "zh": "6æœˆ5æ—¥"
            },
            "hash": "e4a5cd694d56b2a0",
            "authors": [
                "Shenshen Li",
                "Kaiyuan Deng",
                "Lei Wang",
                "Hao Yang",
                "Chong Peng",
                "Peng Yan",
                "Fumin Shen",
                "Heng Tao Shen",
                "Xing Xu"
            ],
            "affiliations": [
                "Meituan",
                "Salesforce AI Research",
                "School of Computer Science and Technology, Tongji University",
                "University of Electronic Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04755.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#data",
                    "#optimization",
                    "#dataset",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Reasoning Activation Potential (RAP) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. RAP Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ 'ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ' Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ²Ğ° Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ°: Causal Discrepancy Estimator Ğ¸ Attention Confidence Estimator. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Difficulty-aware Replacement Ğ´Ğ»Ñ Ğ·Ğ°Ğ¼ĞµĞ½Ñ‹ Ñ‚Ñ€Ğ¸Ğ²Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RAP Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 9.3% Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° 43%."
                },
                "en": {
                    "title": "Unlocking Multi-Modal Reasoning with Less Data: The RAP Approach",
                    "desc": "The paper introduces a new method called Reasoning Activation Potential (RAP) to improve multi-modal reasoning in large language models (MLLMs) using smaller, high-value datasets. It challenges the belief that large amounts of training data are necessary for effective reasoning, showing that only a small subset of samples, known as cognitive samples, can trigger meaningful reasoning. RAP employs two estimators: the Causal Discrepancy Estimator (CDE) to filter out less relevant samples and the Attention Confidence Estimator (ACE) to focus on important tokens during reasoning. The results demonstrate that RAP can enhance performance while significantly reducing the amount of data and computational resources needed."
                },
                "zh": {
                    "title": "æ¨ç†æ¿€æ´»æ½œåŠ›ï¼šé«˜æ•ˆçš„å¤šæ¨¡æ€æ¨ç†æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é€‰æ‹©èŒƒå¼ï¼Œç§°ä¸ºæ¨ç†æ¿€æ´»æ½œåŠ›ï¼ˆRAPï¼‰ï¼Œæ—¨åœ¨é€šè¿‡ä½¿ç”¨æœ€å°çš„é«˜ä»·å€¼æ•°æ®é›†æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒçœŸæ­£çš„å¤šæ¨¡æ€æ¨ç†åªéœ€å°‘é‡å…³é”®æ ·æœ¬ï¼Œè€Œå¤§å¤šæ•°æ ·æœ¬çš„è´¡çŒ®å¾®ä¹å…¶å¾®ã€‚RAPé€šè¿‡ä¸¤ä¸ªäº’è¡¥çš„ä¼°è®¡å™¨æ¥è¯†åˆ«è¿™äº›å…³é”®æ ·æœ¬ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€§èƒ½å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRAPæ–¹æ³•åœ¨ä»…ä½¿ç”¨9.3%çš„è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬é™ä½è¶…è¿‡43%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.00649",
            "title": "GuideX: Guided Synthetic Data Generation for Zero-Shot Information\n  Extraction",
            "url": "https://huggingface.co/papers/2506.00649",
            "abstract": "GUIDEX enhances zero-shot Named Entity Recognition by automatically defining schemas and inferring guidelines, setting new benchmarks without extensive human-labeled data.  \t\t\t\t\tAI-generated summary \t\t\t\t Information Extraction (IE) systems are traditionally domain-specific, requiring costly adaptation that involves expert schema design, data annotation, and model training. While Large Language Models have shown promise in zero-shot IE, performance degrades significantly in unseen domains where label definitions differ. This paper introduces GUIDEX, a novel method that automatically defines domain-specific schemas, infers guidelines, and generates synthetically labeled instances, allowing for better out-of-domain generalization. Fine-tuning Llama 3.1 with GUIDEX sets a new state-of-the-art across seven zeroshot Named Entity Recognition benchmarks. Models trained with GUIDEX gain up to 7 F1 points over previous methods without humanlabeled data, and nearly 2 F1 points higher when combined with it. Models trained on GUIDEX demonstrate enhanced comprehension of complex, domain-specific annotation schemas. Code, models, and synthetic datasets are available at neilus03.github.io/guidex.com",
            "score": 1,
            "issue_id": 4194,
            "pub_date": "2025-05-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ",
                "en": "May 31",
                "zh": "5æœˆ31æ—¥"
            },
            "hash": "32639eb393594459",
            "authors": [
                "Neil De La Fuente",
                "Oscar Sainz",
                "Iker GarcÃ­a-Ferrero",
                "Eneko Agirre"
            ],
            "affiliations": [
                "HiTZ Basque Center for Language Technology - Ixa NLP Group",
                "Technical University of Munich (TUM)",
                "University of the Basque Country (UPV/EHU)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.00649.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#dataset",
                    "#synthetic",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ·ï¸",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑÑ…ĞµĞ¼ Ğ¸ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ zero-shot NER",
                    "desc": "GUIDEX - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ ÑÑ…ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ², Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ (NER). ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ”Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Llama 3.1 Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ GUIDEX ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ½Ğ° ÑĞµĞ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ zero-shot NER. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ GUIDEX, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ…, ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² ÑÑ…ĞµĞ¼ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Zero-Shot NER with GUIDEX!",
                    "desc": "GUIDEX is a new approach that improves zero-shot Named Entity Recognition (NER) by automatically creating schemas and guidelines for different domains. Traditional NER systems need a lot of human effort to design schemas and label data, which can be expensive and time-consuming. GUIDEX uses Large Language Models to generate synthetic labeled data, helping models perform better in new, unseen domains. By fine-tuning Llama 3.1 with GUIDEX, researchers achieved significant improvements in NER performance, setting new records without relying on extensive human-labeled datasets."
                },
                "zh": {
                    "title": "GUIDEXï¼šé›¶-shot NERçš„æ–°çªç ´",
                    "desc": "GUIDEXæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºé›¶-shotå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰çš„èƒ½åŠ›ã€‚å®ƒé€šè¿‡è‡ªåŠ¨å®šä¹‰é¢†åŸŸç‰¹å®šçš„æ¨¡å¼å’Œæ¨æ–­æŒ‡å¯¼æ–¹é’ˆï¼Œå‡å°‘äº†å¯¹å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚ä½¿ç”¨GUIDEXå¾®è°ƒLlama 3.1æ¨¡å‹ï¼Œåœ¨ä¸ƒä¸ªé›¶-shot NERåŸºå‡†æµ‹è¯•ä¸­åˆ›é€ äº†æ–°çš„æœ€ä½³æˆç»©ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œä½¿ç”¨GUIDEXè®­ç»ƒçš„æ¨¡å‹åœ¨æ²¡æœ‰äººå·¥æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼ŒF1åˆ†æ•°æé«˜äº†7åˆ†ï¼Œç»“åˆä½¿ç”¨æ—¶ä¹Ÿæé«˜äº†è¿‘2åˆ†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05579",
            "title": "When Models Know More Than They Can Explain: Quantifying Knowledge\n  Transfer in Human-AI Collaboration",
            "url": "https://huggingface.co/papers/2506.05579",
            "abstract": "Research investigates human-AI knowledge transfer through a large-scale study, revealing that AI performance does not consistently correlate with human understanding, requiring dedicated optimization for effective communication.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in AI reasoning have driven substantial improvements across diverse tasks. A critical open question is whether these improvements also yields better knowledge transfer: the ability of models to communicate reasoning in ways humans can understand, apply, and learn from. To investigate this, we introduce Knowledge Integration and Transfer Evaluation (KITE), a conceptual and experimental framework for Human-AI knowledge transfer capabilities and conduct the first large-scale human study (N=118) explicitly designed to measure it. In our two-phase setup, humans first ideate with an AI on problem-solving strategies, then independently implement solutions, isolating model explanations' influence on human understanding. Our findings reveal that although model benchmark performance correlates with collaborative outcomes, this relationship is notably inconsistent, featuring significant outliers, indicating that knowledge transfer requires dedicated optimization. Our analysis identifies behavioral and strategic factors mediating successful knowledge transfer. We release our code, dataset, and evaluation framework to support future work on communicatively aligned models.",
            "score": 0,
            "issue_id": 4199,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 Ğ¸ÑĞ½Ñ",
                "en": "June 5",
                "zh": "6æœˆ5æ—¥"
            },
            "hash": "451fe5ddfc2da95f",
            "authors": [
                "Quan Shi",
                "Carlos E. Jimenez",
                "Shunyu Yao",
                "Nick Haber",
                "Diyi Yang",
                "Karthik Narasimhan"
            ],
            "affiliations": [
                "OpenAI",
                "Princeton Language and Intelligence",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05579.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#data",
                    "#dataset",
                    "#benchmark",
                    "#reasoning",
                    "#rlhf",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ˜Ğ˜ Ğ½Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼. ĞŸÑ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ 118 Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ĞµĞµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº KITE Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ˜Ğ˜ Ğ² Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ ĞµĞ³Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±ÑŠÑÑĞ½ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ»ÑĞ´ÑĞ¼. Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Optimizing AI for Better Human Understanding",
                    "desc": "This paper explores how well artificial intelligence (AI) can share knowledge with humans, focusing on the transfer of understanding between the two. The researchers developed a framework called Knowledge Integration and Transfer Evaluation (KITE) to assess how effectively AI can communicate its reasoning to humans. They conducted a large study with 118 participants to see how AI explanations affected human problem-solving. The results showed that while AI performance on benchmarks is related to collaborative success, it does not always guarantee effective knowledge transfer, highlighting the need for specific optimizations in AI communication."
                },
                "zh": {
                    "title": "ä¼˜åŒ–äººæœºçŸ¥è¯†è½¬ç§»ï¼Œæå‡æ²Ÿé€šæ•ˆæœ",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†äººç±»ä¸äººå·¥æ™ºèƒ½ä¹‹é—´çš„çŸ¥è¯†è½¬ç§»ï¼Œå‘ç°äººå·¥æ™ºèƒ½çš„è¡¨ç°ä¸äººç±»ç†è§£å¹¶ä¸æ€»æ˜¯ç›¸å…³ï¼Œå› æ­¤éœ€è¦ä¸“é—¨çš„ä¼˜åŒ–ä»¥å®ç°æœ‰æ•ˆçš„æ²Ÿé€šã€‚æˆ‘ä»¬æå‡ºäº†çŸ¥è¯†æ•´åˆä¸è½¬ç§»è¯„ä¼°ï¼ˆKITEï¼‰æ¡†æ¶ï¼Œè¿›è¡Œäº†ä¸€é¡¹å¤§è§„æ¨¡çš„äººç±»ç ”ç©¶ï¼Œæ—¨åœ¨æµ‹é‡äººç±»ä¸äººå·¥æ™ºèƒ½çš„çŸ¥è¯†è½¬ç§»èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°½ç®¡æ¨¡å‹çš„åŸºå‡†æ€§èƒ½ä¸åˆä½œç»“æœæœ‰ä¸€å®šçš„ç›¸å…³æ€§ï¼Œä½†è¿™ç§å…³ç³»å¹¶ä¸ç¨³å®šï¼Œå­˜åœ¨æ˜¾è‘—çš„å¼‚å¸¸å€¼ï¼Œè¡¨æ˜çŸ¥è¯†è½¬ç§»éœ€è¦ä¸“é—¨çš„ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„åˆ†æè¿˜è¯†åˆ«äº†å½±å“æˆåŠŸçŸ¥è¯†è½¬ç§»çš„è¡Œä¸ºå’Œç­–ç•¥å› ç´ ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-06.html",
    "link_next": "2025-06-10.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "06.06",
        "en": "06/06",
        "zh": "6æœˆ6æ—¥"
    },
    "short_date_next": {
        "ru": "10.06",
        "en": "06/10",
        "zh": "6æœˆ10æ—¥"
    },
    "categories": {
        "#dataset": 10,
        "#data": 5,
        "#benchmark": 8,
        "#agents": 1,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 3,
        "#audio": 2,
        "#video": 2,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 2,
        "#agi": 2,
        "#games": 3,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 4,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 9,
        "#survey": 2,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„ä¸¤é˜¶æ®µç®¡é“ï¼Œä½¿ç”¨ä¸“é—¨çš„é¢„è®­ç»ƒæ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹æ¥æé«˜éŸ³é¢‘å­—å¹•è´¨é‡ã€‚é€šè¿‡æ•´åˆå¤šç§å¤šæ¨¡æ€çº¿ç´¢å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¿™ç§æ–¹æ³•èƒ½ç”Ÿæˆç»†è‡´ä¸”å‡†ç¡®çš„éŸ³é¢‘å­—å¹•ã€‚æ–‡ç« è¿˜æå‡ºäº†ä¸€ä¸ªæ–°çš„å¤§è§„æ¨¡æ•°æ®é›†FusionAudioï¼Œå¹¶å¼€å‘äº†æ”¹è¿›çš„éŸ³é¢‘æ¨¡å‹ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚",
        "title": "FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal\n  Contextual Fusion",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„ä¸¤é˜¶æ®µç®¡é“ï¼Œä½¿ç”¨ä¸“é—¨çš„é¢„è®­ç»ƒæ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹æ¥æé«˜éŸ³é¢‘å­—å¹•è´¨é‡ã€‚é€šè¿‡æ•´åˆå¤šç§å¤šæ¨¡æ€çº¿ç´¢å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¿™ç§æ–¹æ³•èƒ½ç”Ÿæˆç»†è‡´ä¸”å‡†ç¡®çš„éŸ³é¢‘å­—å¹•ã€‚æ–‡ç« è¿˜æå‡ºäº†ä¸€ä¸ªæ–°çš„å¤§è§„æ¨¡æ•°æ®é›†FusionAudioï¼Œå¹¶å¼€å‘äº†æ”¹è¿›çš„éŸ³é¢‘æ¨¡å‹ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ« zhÇ’ng xÄ«n de liÇng jiÄ“duÃ n guÇndÃ o, shÇyÃ²ng zhuÄnmÃ©n de yÃ¹xÃ¹nliÃ n mÃ³xÃ­ng hÃ© dÃ  yÇ”yÃ¡n mÃ³xÃ­ng lÃ¡i tÃ­gÄo yÄ«npiÃ n zÃ¬mÇ” zhÃ¬liÃ ng. TÅngguÃ² zhÄ›nghÃ© duÅzhÇ’ng duÅmÃ³ tÃ i xiÃ n hÃ© shÃ ngxiÃ wÃ©n xÃ¬nxÄ«, zhÃ¨ zhÇ’ng fÄngfÇ nÃ©ng shÄ“ngchÃ©ng xÃ¬zhÃ¬ qiÄ› zhÇ”nquÃ¨ de yÄ«npiÃ n zÃ¬mÇ”. WÃ©nzhÄng hÃ¡i tÃ­chÅ« le yÄ«gÃ¨ xÄ«n de dÃ  guÄ«mÃ³ shÃ¹jÃ¹jÃ­ FusionAudio, bÃ¬ng kÄifÄ le gÇijÃ¬n de yÄ«npiÃ n mÃ³xÃ­ng. DÃ imÇ hÃ© shÃ¹jÃ¹ kÄ› zÃ i GitHub shÃ ng zhÇo dÃ o.",
        "vocab": "[\n    {\"word\": \"ä¸¤é˜¶æ®µ\", \"pinyin\": \"liÇng jiÄ“ duÃ n\", \"trans\": \"two-stage\"},\n    {\"word\": \"ç®¡é“\", \"pinyin\": \"guÇn dÃ o\", \"trans\": \"pipeline\"},\n    {\"word\": \"é¢„è®­ç»ƒ\", \"pinyin\": \"yÃ¹ xÃ¹n liÃ n\", \"trans\": \"pre-trained\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"å¤§è¯­è¨€æ¨¡å‹\", \"pinyin\": \"dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"large language model\"},\n    {\"word\": \"å­—å¹•\", \"pinyin\": \"zÃ¬ mÃ¹\", \"trans\": \"subtitle\"},\n    {\"word\": \"æ•´åˆ\", \"pinyin\": \"zhÄ›ng hÃ©\", \"trans\": \"integrate\"},\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³ tÃ i\", \"trans\": \"multimodal\"},\n    {\"word\": \"çº¿ç´¢\", \"pinyin\": \"xiÃ n suÇ’\", \"trans\": \"clue\"},\n    {\"word\": \"ä¸Šä¸‹æ–‡\", \"pinyin\": \"shÃ ng xiÃ  wÃ©n\", \"trans\": \"context\"},\n    {\"word\": \"ç»†è‡´\", \"pinyin\": \"xÃ¬ zhÃ¬\", \"trans\": \"detailed\"},\n    {\"word\": \"å‡†ç¡®\", \"pinyin\": \"zhÇ”n quÃ¨\", \"trans\": \"accurate\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­ chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"å¤§è§„æ¨¡\", \"pinyin\": \"dÃ  guÄ« mÃ³\", \"trans\": \"large-scale\"},\n    {\"word\": \"æ•°æ®é›†\", \"pinyin\": \"shÃ¹ jÃ¹ jÃ­\", \"trans\": \"dataset\"},\n    {\"word\": \"æ”¹è¿›\", \"pinyin\": \"gÇi jÃ¬n\", \"trans\": \"improved\"},\n    {\"word\": \"GitHub\", \"pinyin\": \"GitHub\", \"trans\": \"GitHub\"}\n]",
        "trans": "This article introduces a new two-stage pipeline that utilizes specialized pre-trained models and large language models to enhance the quality of audio captions. By integrating various multimodal cues and contextual information, this method can generate detailed and accurate audio captions. The article also proposes a new large-scale dataset, FusionAudio, and develops improved audio models. The code and data can be found on GitHub.",
        "update_ts": "2025-06-09 09:13"
    }
}