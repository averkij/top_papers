{
    "date": {
        "ru": "2 Ğ¸ÑĞ»Ñ",
        "en": "July 2",
        "zh": "7æœˆ2æ—¥"
    },
    "time_utc": "2025-07-02 08:16",
    "weekday": 2,
    "issue_id": 4598,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.01006",
            "title": "GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2507.01006",
            "abstract": "A vision-language model, GLM-4.1V-Thinking, enhances general-purpose multimodal reasoning through large-scale pre-training and reinforcement learning, achieving state-of-the-art performance across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to advance general-purpose multimodal reasoning. In this report, we share our key findings in the development of the reasoning-centric training framework. We first develop a capable vision foundation model with significant potential through large-scale pre-training, which arguably sets the upper bound for the final performance. Reinforcement Learning with Curriculum Sampling (RLCS) then unlocks the full potential of the model, leading to comprehensive capability enhancement across a diverse range of tasks, including STEM problem solving, video understanding, content recognition, coding, grounding, GUI-based agents, and long document understanding, among others. To facilitate research in this field, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art performance among models of comparable size. In a comprehensive evaluation across 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all tasks and achieves comparable or even superior performance on 18 benchmarks relative to the significantly larger Qwen2.5-VL-72B. Notably, GLM-4.1V-9B-Thinking also demonstrates competitive or superior performance compared to closed-source models such as GPT-4o on challenging tasks including long document understanding and STEM reasoning, further underscoring its strong capabilities. Code, models and more information are released at https://github.com/THUDM/GLM-4.1V-Thinking.",
            "score": 82,
            "issue_id": 4595,
            "pub_date": "2025-07-01",
            "pub_date_card": {
                "ru": "1 Ğ¸ÑĞ»Ñ",
                "en": "July 1",
                "zh": "7æœˆ1æ—¥"
            },
            "hash": "174c869b64ed9ae7",
            "authors": [
                "Wenyi Hong",
                "Wenmeng Yu",
                "Xiaotao Gu",
                "Guo Wang",
                "Guobing Gan",
                "Haomiao Tang",
                "Jiale Cheng",
                "Ji Qi",
                "Junhui Ji",
                "Lihang Pan",
                "Shuaiqi Duan",
                "Weihan Wang",
                "Yan Wang",
                "Yean Cheng",
                "Zehai He",
                "Zhe Su",
                "Zhen Yang",
                "Ziyang Pan",
                "Aohan Zeng",
                "Baoxu Wang",
                "Boyan Shi",
                "Changyu Pang",
                "Chenhui Zhang",
                "Da Yin",
                "Fan Yang",
                "Guoqing Chen",
                "Jiazheng Xu",
                "Jiali Chen",
                "Jing Chen",
                "Jinhao Chen",
                "Jinghao Lin",
                "Jinjiang Wang",
                "Junjie Chen",
                "Leqi Lei",
                "Leyi Pan",
                "Mingzhi Zhang",
                "Qinkai Zheng",
                "Sheng Yang",
                "Shi Zhong",
                "Shiyu Huang",
                "Shuyuan Zhao",
                "Siyan Xue",
                "Shangqin Tu",
                "Shengbiao Meng",
                "Tianshu Zhang",
                "Tianwei Luo",
                "Tianxiang Hao",
                "Tianle Gong",
                "Wenkai Li",
                "Wei Jia",
                "Xin Lyu",
                "Xuancheng Huang",
                "Yanling Wang",
                "Yadong Xue",
                "Yanfeng Wang",
                "Yifan An",
                "Yifan Du",
                "Yiming Shi",
                "Yiheng Huang",
                "Yilin Niu",
                "Yuan Wang",
                "Yuanchang Yue",
                "Yuchen Li",
                "Yutao Zhang",
                "Yuxuan Zhang",
                "Zhanxiao Du",
                "Zhenyu Hou",
                "Zhao Xue",
                "Zhengxiao Du",
                "Zihan Wang",
                "Peng Zhang",
                "Debing Liu",
                "Bin Xu",
                "Juanzi Li",
                "Minlie Huang",
                "Yuxiao Dong",
                "Jie Tang"
            ],
            "affiliations": [
                "Tsinghua University",
                "Zhipu AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01006.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#rl",
                    "#architecture",
                    "#reasoning",
                    "#multimodal",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ GLM-4.1V-Thinking - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° 28 Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. GLM-4.1V-9B-Thinking Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ GPT-4 Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Unlocking Multimodal Reasoning with GLM-4.1V-Thinking",
                    "desc": "GLM-4.1V-Thinking is a vision-language model that enhances multimodal reasoning through extensive pre-training and reinforcement learning. The model is built on a strong vision foundation, which is crucial for achieving high performance across various tasks. By employing Reinforcement Learning with Curriculum Sampling, it maximizes its capabilities in areas like STEM problem solving and video understanding. The model has been open-sourced and shows superior performance compared to other models of similar size, making it a significant contribution to the field of AI."
                },
                "zh": {
                    "title": "GLM-4.1V-Thinkingï¼šå¤šæ¨¡æ€æ¨ç†çš„æ–°é«˜åº¦",
                    "desc": "GLM-4.1V-Thinking æ˜¯ä¸€ç§è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æå‡é€šç”¨å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¼ºå¤§çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œå¹¶é€šè¿‡è¯¾ç¨‹é‡‡æ ·çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•è¿›ä¸€æ­¥æå‡äº†æ¨¡å‹çš„èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨28ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†è®¸å¤šåŒç±»æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶åœ¨é•¿æ–‡æ¡£ç†è§£å’ŒSTEMæ¨ç†ç­‰å¤æ‚ä»»åŠ¡ä¸­çš„ç«äº‰åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.23115",
            "title": "MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional\n  Multimodal Embeddings",
            "url": "https://huggingface.co/papers/2506.23115",
            "abstract": "MoCa, a two-stage framework, enhances pre-trained causal vision-language models for multimodal embedding by introducing bidirectional attention, scaling with unlabeled data, and diverse training objectives.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal embedding models, built upon causal Vision Language Models (VLMs), have shown promise in various tasks. However, current approaches face three key limitations: the use of causal attention in VLM backbones is suboptimal for embedding tasks; scalability issues due to reliance on high-quality labeled paired data for contrastive learning; and limited diversity in training objectives and data. To address these issues, we propose MoCa, a two-stage framework for transforming pre-trained VLMs into effective bidirectional multimodal embedding models. The first stage, Modality-aware Continual Pre-training, introduces a joint reconstruction objective that simultaneously denoises interleaved text and image inputs, enhancing bidirectional context-aware reasoning. The second stage, Heterogeneous Contrastive Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple image-caption pairs to enhance generalization and alignment. Our method addresses the stated limitations by introducing bidirectional attention through continual pre-training, scaling effectively with massive unlabeled datasets via joint reconstruction objectives, and utilizing diverse multimodal data for enhanced representation robustness. Experiments demonstrate that MoCa consistently improves performance across MMEB and ViDoRe-v2 benchmarks, achieving new state-of-the-art results, and exhibits strong scalability with both model size and training data on MMEB.",
            "score": 28,
            "issue_id": 4592,
            "pub_date": "2025-06-29",
            "pub_date_card": {
                "ru": "29 Ğ¸ÑĞ½Ñ",
                "en": "June 29",
                "zh": "6æœˆ29æ—¥"
            },
            "hash": "d7fecdae218ccf8e",
            "authors": [
                "Haonan Chen",
                "Hong Liu",
                "Yuping Luo",
                "Liang Wang",
                "Nan Yang",
                "Furu Wei",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Microsoft Corporation",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.23115.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#training",
                    "#optimization",
                    "#multimodal",
                    "#alignment",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "MoCa: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "MoCa - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ†ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "MoCa: Enhancing Multimodal Embedding with Bidirectional Attention",
                    "desc": "MoCa is a two-stage framework designed to improve pre-trained causal vision-language models for better multimodal embedding. It addresses limitations in current models, such as the inefficiency of causal attention and the need for high-quality labeled data. The first stage focuses on modality-aware continual pre-training, which enhances understanding by denoising both text and image inputs. The second stage employs heterogeneous contrastive fine-tuning, using diverse multimodal data to improve model generalization and alignment, leading to state-of-the-art performance in various benchmarks."
                },
                "zh": {
                    "title": "MoCaï¼šåŒå‘å¤šæ¨¡æ€åµŒå…¥çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "MoCaæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºé¢„è®­ç»ƒçš„å› æœè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€åµŒå…¥ä¸­çš„è¡¨ç°ã€‚å®ƒé€šè¿‡å¼•å…¥åŒå‘æ³¨æ„åŠ›æœºåˆ¶ã€åˆ©ç”¨æœªæ ‡è®°æ•°æ®è¿›è¡Œæ‰©å±•ä»¥åŠå¤šæ ·åŒ–çš„è®­ç»ƒç›®æ ‡æ¥è§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡è”åˆé‡å»ºç›®æ ‡æ¥æé«˜æ–‡æœ¬å’Œå›¾åƒè¾“å…¥çš„å»å™ªèƒ½åŠ›ï¼Œå¢å¼ºåŒå‘ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨ç†ã€‚ç¬¬äºŒé˜¶æ®µåˆ™åˆ©ç”¨ä¸°å¯Œçš„å¤šæ¨¡æ€æ•°æ®è¿›è¡Œå¼‚æ„å¯¹æ¯”å¾®è°ƒï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå¯¹é½æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01001",
            "title": "SciArena: An Open Evaluation Platform for Foundation Models in\n  Scientific Literature Tasks",
            "url": "https://huggingface.co/papers/2507.01001",
            "abstract": "SciArena is a community-driven platform for evaluating foundation models on scientific literature tasks, using collective voter judgments to rank models and address the need for reliable automated evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SciArena, an open and collaborative platform for evaluating foundation models on scientific literature tasks. Unlike traditional benchmarks for scientific literature understanding and synthesis, SciArena engages the research community directly, following the Chatbot Arena evaluation approach of community voting on model comparisons. By leveraging collective intelligence, SciArena offers a community-driven evaluation of model performance on open-ended scientific tasks that demand literature-grounded, long-form responses. The platform currently supports 23 open-source and proprietary foundation models and has collected over 13,000 votes from trusted researchers across diverse scientific domains. We analyze the data collected so far and confirm that the submitted questions are diverse, aligned with real-world literature needs, and that participating researchers demonstrate strong self-consistency and inter-annotator agreement in their evaluations. We discuss the results and insights based on the model ranking leaderboard. To further promote research in building model-based automated evaluation systems for literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based on our collected preference data. The benchmark measures the accuracy of models in judging answer quality by comparing their pairwise assessments with human votes. Our experiments highlight the benchmark's challenges and emphasize the need for more reliable automated evaluation methods.",
            "score": 24,
            "issue_id": 4593,
            "pub_date": "2025-07-01",
            "pub_date_card": {
                "ru": "1 Ğ¸ÑĞ»Ñ",
                "en": "July 1",
                "zh": "7æœˆ1æ—¥"
            },
            "hash": "f3c20682e2dcf410",
            "authors": [
                "Yilun Zhao",
                "Kaiyan Zhang",
                "Tiansheng Hu",
                "Sihong Wu",
                "Ronan Le Bras",
                "Taira Anderson",
                "Jonathan Bragg",
                "Joseph Chee Chang",
                "Jesse Dodge",
                "Matt Latzke",
                "Yixin Liu",
                "Charles McGrady",
                "Xiangru Tang",
                "Zihang Wang",
                "Chen Zhao",
                "Hannaneh Hajishirzi",
                "Doug Downey",
                "Arman Cohan"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "New York University",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01001.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#science",
                    "#benchmark",
                    "#survey",
                    "#dataset"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "ĞšĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·ÑƒĞ¼ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹",
                    "desc": "SciArena - ÑÑ‚Ğ¾ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ¾Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ 23 Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¾Ğ±Ñ€Ğ°Ğ² Ğ±Ğ¾Ğ»ĞµĞµ 13 000 Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ² Ğ¾Ñ‚ Ğ´Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. ĞĞ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ¸Ñ… ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚ÑĞ¼ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº ÑƒÑ‡Ğ°ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ¿ÑƒÑÑ‚Ğ¸Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SciArena-Eval Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Scientific Model Evaluation through Community Collaboration",
                    "desc": "SciArena is a collaborative platform designed to evaluate foundation models specifically for tasks related to scientific literature. It utilizes community voting to rank models, moving away from traditional benchmarks and fostering direct engagement from researchers. The platform supports a variety of models and has gathered extensive voting data, demonstrating strong agreement among participants in their evaluations. Additionally, SciArena introduces a meta-evaluation benchmark, SciArena-Eval, to enhance automated evaluation systems by comparing model assessments with human judgments."
                },
                "zh": {
                    "title": "SciArenaï¼šç§‘å­¦æ–‡çŒ®ä»»åŠ¡çš„ç¤¾åŒºè¯„ä¼°å¹³å°",
                    "desc": "SciArenaæ˜¯ä¸€ä¸ªç¤¾åŒºé©±åŠ¨çš„å¹³å°ï¼Œç”¨äºè¯„ä¼°åŸºç¡€æ¨¡å‹åœ¨ç§‘å­¦æ–‡çŒ®ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ä¸ä¼ ç»Ÿçš„ç§‘å­¦æ–‡çŒ®ç†è§£åŸºå‡†ä¸åŒï¼ŒSciArenaé€šè¿‡ç¤¾åŒºæŠ•ç¥¨çš„æ–¹å¼ç›´æ¥å‚ä¸ç ”ç©¶è€…ï¼Œåˆ©ç”¨é›†ä½“æ™ºæ…§å¯¹æ¨¡å‹æ€§èƒ½è¿›è¡Œè¯„ä¼°ã€‚è¯¥å¹³å°æ”¯æŒ23ä¸ªå¼€æºå’Œä¸“æœ‰çš„åŸºç¡€æ¨¡å‹ï¼Œå¹¶æ”¶é›†äº†æ¥è‡ªä¸åŒç§‘å­¦é¢†åŸŸçš„ç ”ç©¶è€…çš„è¶…è¿‡13,000ä¸ªæŠ•ç¥¨ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†SciArena-Evalï¼Œä¸€ä¸ªåŸºäºæ”¶é›†çš„åå¥½æ•°æ®çš„å…ƒè¯„ä¼°åŸºå‡†ï¼Œæ—¨åœ¨ä¿ƒè¿›æ–‡çŒ®ä»»åŠ¡çš„è‡ªåŠ¨è¯„ä¼°ç³»ç»Ÿçš„ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.00432",
            "title": "Does Math Reasoning Improve General LLM Capabilities? Understanding\n  Transferability of LLM Reasoning",
            "url": "https://huggingface.co/papers/2507.00432",
            "abstract": "Reinforcement learning-tuned models outperform supervised fine-tuned models in generalizing mathematical problem-solving abilities to other domains, indicating a need to re-evaluate training methods for reasoning models.  \t\t\t\t\tAI-generated summary \t\t\t\t Math reasoning has become the poster child of progress in large language models (LLMs), with new models rapidly surpassing human-level performance on benchmarks like MATH and AIME. But as math leaderboards improve week by week, it is worth asking: do these gains reflect broader problem-solving ability or just narrow overfitting? To answer this question, we evaluate over 20 open-weight reasoning-tuned models across a broad suite of tasks, including math, scientific QA, agent planning, coding, and standard instruction-following. We surprisingly find that most models that succeed in math fail to transfer their gains to other domains. To rigorously study this phenomenon, we conduct controlled experiments on Qwen3-14B models using math-only data but different tuning methods. We find that reinforcement learning (RL)-tuned models generalize well across domains, while supervised fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space representation and token-space distribution shift analyses reveal that SFT induces substantial representation and output drift, while RL preserves general-domain structure. Our results suggest a need to rethink standard post-training recipes, particularly the reliance on SFT-distilled data for advancing reasoning models.",
            "score": 20,
            "issue_id": 4592,
            "pub_date": "2025-07-01",
            "pub_date_card": {
                "ru": "1 Ğ¸ÑĞ»Ñ",
                "en": "July 1",
                "zh": "7æœˆ1æ—¥"
            },
            "hash": "c4a7e4dd11865858",
            "authors": [
                "Maggie Huan",
                "Yuetai Li",
                "Tuney Zheng",
                "Xiaoyu Xu",
                "Seungone Kim",
                "Minxin Du",
                "Radha Poovendran",
                "Graham Neubig",
                "Xiang Yue"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "M-A-P",
                "The Hong Kong Polytechnic University",
                "University of Pennsylvania",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.00432.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#rl",
                    "#transfer_learning",
                    "#optimization",
                    "#math"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ² Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL), Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ñ‡ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ (SFT). ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ñ‹ÑĞ²Ğ¸Ğ», Ñ‡Ñ‚Ğ¾ SFT Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ´Ğ²Ğ¸Ğ³ Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº RL ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¾Ğ±Ñ‰ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Qwen3-14B Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ SFT."
                },
                "en": {
                    "title": "Rethinking Training: Reinforcement Learning for Better Generalization",
                    "desc": "This paper investigates the effectiveness of different training methods for reasoning models, particularly in the context of mathematical problem-solving. It finds that models trained with reinforcement learning (RL) outperform those fine-tuned with supervised learning (SFT) when applied to a variety of tasks beyond mathematics. The study reveals that while SFT models excel in math, they struggle to generalize their skills to other domains due to significant representation drift. In contrast, RL-tuned models maintain their general problem-solving abilities, suggesting a need to reconsider current training approaches for reasoning models."
                },
                "zh": {
                    "title": "é‡æ–°æ€è€ƒæ¨ç†æ¨¡å‹çš„è®­ç»ƒæ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è°ƒä¼˜æ¨¡å‹åœ¨æ•°å­¦é—®é¢˜è§£å†³èƒ½åŠ›ä¸Šçš„è¡¨ç°ï¼Œå‘ç°å…¶åœ¨å…¶ä»–é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ä¼˜äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¨¡å‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè™½ç„¶è®¸å¤šæ¨¡å‹åœ¨æ•°å­¦ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨å…¶ä»–ä»»åŠ¡ä¸Šçš„è¿ç§»èƒ½åŠ›å´è¾ƒå·®ã€‚é€šè¿‡å¯¹Qwen3-14Bæ¨¡å‹çš„å®éªŒï¼Œå‘ç°RLè°ƒä¼˜æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šä¸ªé¢†åŸŸä¸­ä¿æŒè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œè€ŒSFTæ¨¡å‹åˆ™å®¹æ˜“é—å¿˜å…¶é€šç”¨èƒ½åŠ›ã€‚ç»“æœæç¤ºæˆ‘ä»¬éœ€è¦é‡æ–°å®¡è§†ç°æœ‰çš„è®­ç»ƒæ–¹æ³•ï¼Œå°¤å…¶æ˜¯å¯¹SFTæ•°æ®çš„ä¾èµ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.19852",
            "title": "Radial Attention: O(nlog n) Sparse Attention with Energy Decay for\n  Long Video Generation",
            "url": "https://huggingface.co/papers/2506.19852",
            "abstract": "Radial Attention, a scalable sparse attention mechanism, improves efficiency and preserves video quality in diffusion models by leveraging spatiotemporal energy decay.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion models have enabled high-quality video generation, but the additional temporal dimension significantly increases computational costs, making training and inference on long videos prohibitively expensive. In this paper, we identify a phenomenon we term Spatiotemporal Energy Decay in video diffusion models: post-softmax attention scores diminish as spatial and temporal distance between tokens increase, akin to the physical decay of signal or waves over space and time in nature. Motivated by this, we propose Radial Attention, a scalable sparse attention mechanism with O(n log n) complexity that translates energy decay into exponentially decaying compute density, which is significantly more efficient than standard O(n^2) dense attention and more expressive than linear attention. Specifically, Radial Attention employs a simple, static attention mask where each token attends to spatially nearby tokens, with the attention window size shrinking with temporal distance. Moreover, it allows pre-trained video diffusion models to extend their generation length with efficient LoRA-based fine-tuning. Extensive experiments show that Radial Attention maintains video quality across Wan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9times speedup over the original dense attention. With minimal tuning, it enables video generation up to 4times longer while reducing training costs by up to 4.4times compared to direct fine-tuning and accelerating inference by up to 3.7times compared to dense attention inference.",
            "score": 12,
            "issue_id": 4594,
            "pub_date": "2025-06-24",
            "pub_date_card": {
                "ru": "24 Ğ¸ÑĞ½Ñ",
                "en": "June 24",
                "zh": "6æœˆ24æ—¥"
            },
            "hash": "c195d9c32370fcf1",
            "authors": [
                "Xingyang Li",
                "Muyang Li",
                "Tianle Cai",
                "Haocheng Xi",
                "Shuo Yang",
                "Yujun Lin",
                "Lvmin Zhang",
                "Songlin Yang",
                "Jinbo Hu",
                "Kelly Peng",
                "Maneesh Agrawala",
                "Ion Stoica",
                "Kurt Keutzer",
                "Song Han"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2506.19852.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#diffusion",
                    "#architecture",
                    "#inference",
                    "#training"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ Ğ°Ğ´Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Ğ Ğ°Ğ´Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ' Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ñ ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ Ğ°Ğ´Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ¼ĞµĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ O(n log n) Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Radial Attention: Efficient Video Generation with Sparse Attention",
                    "desc": "This paper introduces Radial Attention, a new sparse attention mechanism designed to enhance the efficiency of video generation in diffusion models. It leverages the concept of Spatiotemporal Energy Decay, which explains how attention scores decrease as the distance between tokens increases, similar to how signals weaken over distance. By using a static attention mask that focuses on nearby tokens and reduces the attention window with temporal distance, Radial Attention achieves a computational complexity of O(n log n), making it much faster than traditional O(n^2) dense attention. The results show that this method not only speeds up training and inference but also maintains high video quality, allowing for longer video generation with reduced costs."
                },
                "zh": {
                    "title": "å¾„å‘æ³¨æ„åŠ›ï¼šé«˜æ•ˆè§†é¢‘ç”Ÿæˆçš„æ–°æœºåˆ¶",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºå¾„å‘æ³¨æ„åŠ›ï¼ˆRadial Attentionï¼‰çš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ—¨åœ¨æé«˜æ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘ç”Ÿæˆä¸­çš„æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒè§†é¢‘è´¨é‡ã€‚æˆ‘ä»¬å‘ç°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­å­˜åœ¨ä¸€ç§ç°è±¡ï¼Œç§°ä¸ºæ—¶ç©ºèƒ½é‡è¡°å‡ï¼Œéšç€ç©ºé—´å’Œæ—¶é—´è·ç¦»çš„å¢åŠ ï¼Œæ³¨æ„åŠ›å¾—åˆ†ä¼šå‡å°ã€‚å¾„å‘æ³¨æ„åŠ›é€šè¿‡å°†èƒ½é‡è¡°å‡è½¬åŒ–ä¸ºæŒ‡æ•°è¡°å‡çš„è®¡ç®—å¯†åº¦ï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ï¼Œå¤æ‚åº¦ä¸ºO(n log n)ï¼Œè¿œä¼˜äºä¼ ç»Ÿçš„O(n^2)å¯†é›†æ³¨æ„åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¾„å‘æ³¨æ„åŠ›åœ¨å¤šä¸ªè§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­ä¿æŒäº†è§†é¢‘è´¨é‡ï¼Œå¹¶å®ç°äº†è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦çš„æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.20639",
            "title": "DiffuCoder: Understanding and Improving Masked Diffusion Models for Code\n  Generation",
            "url": "https://huggingface.co/papers/2506.20639",
            "abstract": "Diffusion large language models are applied to code generation, revealing their unique denoising processes and benefiting from a novel reinforcement learning sampling scheme.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion large language models (dLLMs) are compelling alternatives to autoregressive (AR) models because their denoising models operate over the entire sequence. The global planning and iterative refinement features of dLLMs are particularly useful for code generation. However, current training and inference mechanisms for dLLMs in coding are still under-explored. To demystify the decoding behavior of dLLMs and unlock their potential for coding, we systematically investigate their denoising processes and reinforcement learning (RL) methods. We train a 7B dLLM, DiffuCoder, on 130B tokens of code. Using this model as a testbed, we analyze its decoding behavior, revealing how it differs from that of AR models: (1) dLLMs can decide how causal their generation should be without relying on semi-AR decoding, and (2) increasing the sampling temperature diversifies not only token choices but also their generation order. This diversity creates a rich search space for RL rollouts. For RL training, to reduce the variance of token log-likelihood estimates and maintain training efficiency, we propose coupled-GRPO, a novel sampling scheme that constructs complementary mask noise for completions used in training. In our experiments, coupled-GRPO significantly improves DiffuCoder's performance on code generation benchmarks (+4.4\\% on EvalPlus) and reduces reliance on AR causal during decoding. Our work provides deeper insight into the machinery of dLLM generation and offers an effective, diffusion-native RL training framework. https://github.com/apple/ml-diffucoder.",
            "score": 10,
            "issue_id": 4593,
            "pub_date": "2025-06-25",
            "pub_date_card": {
                "ru": "25 Ğ¸ÑĞ½Ñ",
                "en": "June 25",
                "zh": "6æœˆ25æ—¥"
            },
            "hash": "20d886d0a4cd5bb6",
            "authors": [
                "Shansan Gong",
                "Ruixiang Zhang",
                "Huangjie Zheng",
                "Jiatao Gu",
                "Navdeep Jaitly",
                "Lingpeng Kong",
                "Yizhe Zhang"
            ],
            "affiliations": [
                "Apple",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.20639.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#architecture",
                    "#optimization",
                    "#dataset",
                    "#diffusion"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (dLLM) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² dLLM Ğ¸ Ğ¸Ñ… Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¾Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ coupled-GRPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Unlocking Code Generation with Diffusion Models",
                    "desc": "This paper explores the use of diffusion large language models (dLLMs) for code generation, highlighting their unique denoising processes compared to traditional autoregressive models. The authors introduce a novel reinforcement learning sampling scheme called coupled-GRPO, which enhances the training efficiency and performance of the dLLM named DiffuCoder. By analyzing the decoding behavior of DiffuCoder, the study reveals that dLLMs can flexibly adjust their generation strategies and improve diversity in output. The findings demonstrate that dLLMs have significant potential for coding tasks, providing a new framework for effective code generation."
                },
                "zh": {
                    "title": "æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼šä»£ç ç”Ÿæˆçš„æ–°é€‰æ‹©",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œæ­ç¤ºäº†å…¶ç‹¬ç‰¹çš„å»å™ªè¿‡ç¨‹ã€‚dLLMsä¸è‡ªå›å½’æ¨¡å‹ç›¸æ¯”ï¼Œèƒ½å¤Ÿåœ¨æ•´ä¸ªåºåˆ—ä¸Šè¿›è¡Œå»å™ªï¼Œå…·æœ‰å…¨çƒè§„åˆ’å’Œè¿­ä»£ä¼˜åŒ–çš„ç‰¹ç‚¹ï¼Œç‰¹åˆ«é€‚åˆä»£ç ç”Ÿæˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ é‡‡æ ·æ–¹æ¡ˆcoupled-GRPOï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡å¹¶å‡å°‘æ ‡è®°å¯¹æ•°ä¼°è®¡çš„æ–¹å·®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œcoupled-GRPOæ˜¾è‘—æå‡äº†DiffuCoderåœ¨ä»£ç ç”ŸæˆåŸºå‡†ä¸Šçš„è¡¨ç°ï¼Œå¹¶å‡å°‘äº†å¯¹è‡ªå›å½’å› æœè§£ç çš„ä¾èµ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.00951",
            "title": "Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive\n  Foundations for Artificial General Intelligence and its Societal Impact",
            "url": "https://huggingface.co/papers/2507.00951",
            "abstract": "The paper synthesizes the interdisciplinary approach to achieving Artificial General Intelligence, emphasizing modular reasoning, memory, multi-agent coordination, and the integration of neurosymbolic systems and reinforcement learning to overcome current model limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Can machines truly think, reason and act in domains like humans? This enduring question continues to shape the pursuit of Artificial General Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5, DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal fluency and partial reasoning, these systems remain fundamentally limited by their reliance on token-level prediction and lack of grounded agency. This paper offers a cross-disciplinary synthesis of AGI development, spanning artificial intelligence, cognitive neuroscience, psychology, generative models, and agent-based systems. We analyze the architectural and cognitive foundations of general intelligence, highlighting the role of modular reasoning, persistent memory, and multi-agent coordination. In particular, we emphasize the rise of Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use to enable more adaptive behavior. We discuss generalization strategies, including information compression, test-time adaptation, and training-free methods, as critical pathways toward flexible, domain-agnostic intelligence. Vision-Language Models (VLMs) are reexamined not just as perception modules but as evolving interfaces for embodied understanding and collaborative task completion. We also argue that true intelligence arises not from scale alone but from the integration of memory and reasoning: an orchestration of modular, interactive, and self-improving components where compression enables adaptive behavior. Drawing on advances in neurosymbolic systems, reinforcement learning, and cognitive scaffolding, we explore how recent architectures begin to bridge the gap between statistical learning and goal-directed cognition. Finally, we identify key scientific, technical, and ethical challenges on the path to AGI.",
            "score": 7,
            "issue_id": 4593,
            "pub_date": "2025-07-01",
            "pub_date_card": {
                "ru": "1 Ğ¸ÑĞ»Ñ",
                "en": "July 1",
                "zh": "7æœˆ1æ—¥"
            },
            "hash": "056b6a5007ee5fc2",
            "authors": [
                "Rizwan Qureshi",
                "Ranjan Sapkota",
                "Abbas Shah",
                "Amgad Muneer",
                "Anas Zafar",
                "Ashmal Vayani",
                "Maged Shoman",
                "Abdelrahman B. M. Eldaly",
                "Kai Zhang",
                "Ferhat Sadak",
                "Shaina Raza",
                "Xinqi Fan",
                "Ravid Shwartz-Ziv",
                "Hong Yan",
                "Vinjia Jain",
                "Aman Chadha",
                "Manoj Karkee",
                "Jia Wu",
                "Philip Torr",
                "Seyedali Mirjalili"
            ],
            "affiliations": [
                "Amazon Research (Work done outside Amazon)",
                "Center for Data Science, New York University, NYU, NY, USA",
                "Center for research in Computer Vision, University of Central Florida, Orlando, FL, USA",
                "Centre for Artificial Intelligence Research and Optimization, Torrens University Australia, Fortitude Valley, Brisbane, QLD 4006, Australia",
                "Cornell University, Department of Biological and Environmental Engineering, Ithaca, NY 14853, USA",
                "Department of Electrical Engineering, City University of Hong Kong, SAR China",
                "Department of Electronics Engineering, Mehran University of Engineering & Technology, Jamshoro, Sindh, Pakistan",
                "Department of Engineering Science, University of Oxford, UK",
                "Department of Imaging Physics, The University of Texas MD Anderson Cancer Center, Houston, TX, USA",
                "Department of Mechanical Engineering, Bartin University, Bartin Turkey",
                "Intelligent Transportation Systems, University of Tennessee, Oakridge, TN, USA",
                "Manchester Metropolitan University, Manchester, UK",
                "Meta Research (Work done outside Meta)",
                "University Research and Innovation Center, Obuda University, 1034 Budapest, Hungary",
                "Vector Institute, Toronto Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.00951.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#agi",
                    "#rl",
                    "#architecture",
                    "#multimodal",
                    "#rag",
                    "#ethics",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑƒÑ‚ÑŒ Ğº AGI: Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµĞ¶Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° (AGI). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ÑĞµÑ‚ÑÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ¾Ğ»ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğ¸ AGI."
                },
                "en": {
                    "title": "Towards True Intelligence: Integrating Memory, Reasoning, and Adaptation for AGI",
                    "desc": "This paper explores the interdisciplinary approach to achieving Artificial General Intelligence (AGI) by integrating concepts from various fields such as cognitive neuroscience and psychology. It emphasizes the importance of modular reasoning, persistent memory, and multi-agent coordination in developing intelligent systems. The authors propose that true intelligence is not just about scaling models but involves the orchestration of memory and reasoning capabilities. They also highlight the potential of neurosymbolic systems and reinforcement learning to create more adaptive and flexible AI agents."
                },
                "zh": {
                    "title": "è·¨å­¦ç§‘æ¨åŠ¨äººå·¥é€šç”¨æ™ºèƒ½çš„å®ç°",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å®ç°äººå·¥é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰çš„è·¨å­¦ç§‘æ–¹æ³•ï¼Œå¼ºè°ƒäº†æ¨¡å—åŒ–æ¨ç†ã€æŒä¹…è®°å¿†å’Œå¤šæ™ºèƒ½ä½“åè°ƒçš„é‡è¦æ€§ã€‚è®ºæ–‡åˆ†æäº†é€šç”¨æ™ºèƒ½çš„æ¶æ„å’Œè®¤çŸ¥åŸºç¡€ï¼Œæå‡ºäº†ç»“åˆæ£€ç´¢ã€è§„åˆ’å’ŒåŠ¨æ€å·¥å…·ä½¿ç”¨çš„Agentic RAGæ¡†æ¶ï¼Œä»¥å®ç°æ›´çµæ´»çš„è¡Œä¸ºã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†ä¿¡æ¯å‹ç¼©ã€æµ‹è¯•æ—¶é€‚åº”å’Œæ— è®­ç»ƒæ–¹æ³•ç­‰æ³›åŒ–ç­–ç•¥ï¼Œä½œä¸ºå®ç°çµæ´»ã€é¢†åŸŸæ— å…³æ™ºèƒ½çš„å…³é”®è·¯å¾„ã€‚æœ€åï¼Œè®ºæ–‡æŒ‡å‡ºäº†åœ¨å®ç°AGIè¿‡ç¨‹ä¸­é¢ä¸´çš„ç§‘å­¦ã€æŠ€æœ¯å’Œä¼¦ç†æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21277",
            "title": "HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context",
            "url": "https://huggingface.co/papers/2506.21277",
            "abstract": "A reinforcement learning-based approach enhances multimodal reasoning by addressing context understanding and shortcut problems, using context, format, accuracy, and logical rewards, and achieving superior performance on the IntentBench benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid evolution of multimodal large language models, the capacity to deeply understand and interpret human intentions has emerged as a critical capability, which demands detailed and thoughtful reasoning. In recent studies, Reinforcement Learning (RL) has demonstrated potential in enhancing the reasoning capabilities of Large Language Models (LLMs). Nonetheless, the challenges associated with adapting RL to multimodal data and formats remain largely unaddressed. In this paper, we identify two issues in existing multimodal reasoning models: insufficient global context understanding and shortcut problems. Insufficient context understanding can happen when a model misinterprets multimodal context, resulting in incorrect answers. The shortcut problem occurs when the model overlooks crucial clues in multimodal inputs, directly addressing the query without considering the multimodal information. To tackle these issues, we emphasize the necessity for the model to reason with a clear understanding of the global context within multimodal inputs. This global context understanding can effectively prevent the model from overlooking key multimodal cues and ensure a thorough reasoning process. To ensure the accurate interpretation of multimodal context information, we implement a context reward judged by a large language model, alongside format and accuracy rewards. Additionally, to improve complex reasoning capability, we employ the LLM to assess the logical reward, determining whether the reasoning process successfully integrates multimodal information with logical methods. We also introduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating models in understanding complex human intentions and emotions. Our proposed method demonstrates advanced performance across multiple omni-modal benchmarks compared to other open-source omni-modal models.",
            "score": 7,
            "issue_id": 4592,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 Ğ¸ÑĞ½Ñ",
                "en": "June 26",
                "zh": "6æœˆ26æ—¥"
            },
            "hash": "15a38ef84e7820fa",
            "authors": [
                "Qize Yang",
                "Shimin Yao",
                "Weixuan Chen",
                "Shenghao Fu",
                "Detao Bai",
                "Jiaxing Zhao",
                "Boyuan Sun",
                "Bowen Yin",
                "Xihan Wei",
                "Jingren Zhou"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21277.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#games",
                    "#rl",
                    "#multimodal",
                    "#survey"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ, Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ½Ñ‹Ğµ, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº IntentBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° IntentBench Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning with Reinforcement Learning",
                    "desc": "This paper presents a reinforcement learning approach to improve multimodal reasoning in large language models. It addresses two main challenges: insufficient understanding of global context and the shortcut problem, where models fail to consider important multimodal cues. By implementing context, format, accuracy, and logical rewards, the model enhances its reasoning capabilities and interprets multimodal inputs more effectively. The proposed method outperforms existing models on the IntentBench benchmark, showcasing its ability to understand complex human intentions and emotions."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œä»¥å¢å¼ºå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œè§£å†³ä¸Šä¸‹æ–‡ç†è§£å’Œæ·å¾„é—®é¢˜ã€‚æˆ‘ä»¬å‘ç°ç°æœ‰å¤šæ¨¡æ€æ¨ç†æ¨¡å‹å­˜åœ¨å…¨çƒä¸Šä¸‹æ–‡ç†è§£ä¸è¶³å’Œæ·å¾„é—®é¢˜ï¼Œè¿™ä¼šå¯¼è‡´æ¨¡å‹é”™è¯¯è§£è¯»å¤šæ¨¡æ€ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼ºè°ƒæ¨¡å‹éœ€è¦åœ¨å¤šæ¨¡æ€è¾“å…¥ä¸­æ¸…æ™°ç†è§£å…¨çƒä¸Šä¸‹æ–‡ï¼Œå¹¶é€šè¿‡ä¸Šä¸‹æ–‡å¥–åŠ±ã€æ ¼å¼å¥–åŠ±å’Œå‡†ç¡®æ€§å¥–åŠ±æ¥ç¡®ä¿å¯¹å¤šæ¨¡æ€ä¿¡æ¯çš„å‡†ç¡®è§£è¯»ã€‚æˆ‘ä»¬çš„ç ”ç©¶åœ¨IntentBenchåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç¤ºäº†åœ¨ç†è§£å¤æ‚äººç±»æ„å›¾å’Œæƒ…æ„Ÿæ–¹é¢çš„å…ˆè¿›æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21545",
            "title": "Data Efficacy for Language Model Training",
            "url": "https://huggingface.co/papers/2506.21545",
            "abstract": "DELT, a paradigm for enhancing language model performance through data efficacy, consists of data scoring, selection, and ordering, demonstrating significant improvements without increasing data scale or model size.  \t\t\t\t\tAI-generated summary \t\t\t\t Data is fundamental to the training of language models (LM). Recent research has been dedicated to data efficiency, which aims to maximize performance by selecting a minimal or optimal subset of training data. Techniques such as data filtering, sampling, and selection play a crucial role in this area. To complement it, we define Data Efficacy, which focuses on maximizing performance by optimizing the organization of training data and remains relatively underexplored. This work introduces a general paradigm, DELT, for considering data efficacy in LM training, which highlights the significance of training data organization. DELT comprises three components: Data Scoring, Data Selection, and Data Ordering. Among these components, we design Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which considers both the learnability and quality of each data sample from the gradient consistency perspective. We also devise Folding Ordering (FO), as a novel instance of Data Ordering, which addresses issues such as model forgetting and data distribution bias. Comprehensive experiments validate the data efficacy in LM training, which demonstrates the following: Firstly, various instances of the proposed DELT enhance LM performance to varying degrees without increasing the data scale and model size. Secondly, among these instances, the combination of our proposed LQS for data scoring and Folding for data ordering achieves the most significant improvement. Lastly, data efficacy can be achieved together with data efficiency by applying data selection. Therefore, we believe that data efficacy is a promising foundational area in LM training.",
            "score": 3,
            "issue_id": 4596,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 Ğ¸ÑĞ½Ñ",
                "en": "June 26",
                "zh": "6æœˆ26æ—¥"
            },
            "hash": "b19a54a5dd4e35c8",
            "authors": [
                "Yalun Dai",
                "Yangyu Huang",
                "Xin Zhang",
                "Wenshan Wu",
                "Chong Li",
                "Wenhui Lu",
                "Shijie Cao",
                "Li Dong",
                "Scarlett Li"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2506.21545.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#data"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… - ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "DELT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… LQS, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… FO Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DELT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Maximizing Language Model Performance through Data Efficacy",
                    "desc": "The paper introduces DELT, a new approach to improve language model performance by focusing on data efficacy, which is about how well training data is organized. It consists of three main components: Data Scoring, Data Selection, and Data Ordering, which work together to enhance model training without needing more data or larger models. A key innovation is the Learnability-Quality Scoring (LQS), which evaluates data samples based on their learnability and quality. The results show that using DELT can significantly boost performance, especially when combining LQS with Folding Ordering, while also achieving data efficiency."
                },
                "zh": {
                    "title": "æå‡è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•ï¼šDELT",
                    "desc": "DELTæ˜¯ä¸€ç§æé«˜è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•ï¼Œä¸“æ³¨äºæ•°æ®çš„æœ‰æ•ˆæ€§ã€‚å®ƒåŒ…æ‹¬æ•°æ®è¯„åˆ†ã€é€‰æ‹©å’Œæ’åºä¸‰ä¸ªéƒ¨åˆ†ï¼Œæ—¨åœ¨ä¼˜åŒ–è®­ç»ƒæ•°æ®çš„ç»„ç»‡æ–¹å¼ã€‚é€šè¿‡è®¾è®¡å­¦ä¹ è´¨é‡è¯„åˆ†ï¼ˆLQSï¼‰å’ŒæŠ˜å æ’åºï¼ˆFOï¼‰ï¼ŒDELTèƒ½å¤Ÿåœ¨ä¸å¢åŠ æ•°æ®è§„æ¨¡æˆ–æ¨¡å‹å¤§å°çš„æƒ…å†µä¸‹æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ•°æ®æœ‰æ•ˆæ€§ä¸æ•°æ®æ•ˆç‡å¯ä»¥ç»“åˆä½¿ç”¨ï¼Œä»è€Œä¸ºè¯­è¨€æ¨¡å‹è®­ç»ƒæä¾›æ–°çš„æ€è·¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.22960",
            "title": "Peccavi: Visual Paraphrase Attack Safe and Distortion Free Image\n  Watermarking Technique for AI-Generated Images",
            "url": "https://huggingface.co/papers/2506.22960",
            "abstract": "PECCAVI is a robust image watermarking technique that is resistant to visual paraphrase attacks and distortions, utilizing NMPs and multi-channel frequency domain watermarking.  \t\t\t\t\tAI-generated summary \t\t\t\t A report by the European Union Law Enforcement Agency predicts that by 2026, up to 90 percent of online content could be synthetically generated, raising concerns among policymakers, who cautioned that \"Generative AI could act as a force multiplier for political disinformation. The combined effect of generative text, images, videos, and audio may surpass the influence of any single modality.\" In response, California's Bill AB 3211 mandates the watermarking of AI-generated images, videos, and audio. However, concerns remain regarding the vulnerability of invisible watermarking techniques to tampering and the potential for malicious actors to bypass them entirely. Generative AI-powered de-watermarking attacks, especially the newly introduced visual paraphrase attack, have shown an ability to fully remove watermarks, resulting in a paraphrase of the original image. This paper introduces PECCAVI, the first visual paraphrase attack-safe and distortion-free image watermarking technique. In visual paraphrase attacks, an image is altered while preserving its core semantic regions, termed Non-Melting Points (NMPs). PECCAVI strategically embeds watermarks within these NMPs and employs multi-channel frequency domain watermarking. It also incorporates noisy burnishing to counter reverse-engineering efforts aimed at locating NMPs to disrupt the embedded watermark, thereby enhancing durability. PECCAVI is model-agnostic. All relevant resources and codes will be open-sourced.",
            "score": 1,
            "issue_id": 4593,
            "pub_date": "2025-06-28",
            "pub_date_card": {
                "ru": "28 Ğ¸ÑĞ½Ñ",
                "en": "June 28",
                "zh": "6æœˆ28æ—¥"
            },
            "hash": "c946e3ac9bf6133a",
            "authors": [
                "Shreyas Dixit",
                "Ashhar Aziz",
                "Shashwat Bajpai",
                "Vasu Sharma",
                "Aman Chadha",
                "Vinija Jain",
                "Amitava Das"
            ],
            "affiliations": [
                "AI Institute, University of South Carolina, USA",
                "Amazon GenAI, USA",
                "BITS Pilani Hyderabad, India",
                "IIIT Delhi, India",
                "Meta AI, USA",
                "Stanford University, USA",
                "VIIT Pune, India"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.22960.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#synthetic",
                    "#data",
                    "#open_source",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞµĞ¿Ğ¾Ğ±ĞµĞ´Ğ¸Ğ¼Ñ‹Ğµ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ ÑĞ¿Ğ¾Ñ…Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "PECCAVI - ÑÑ‚Ğ¾ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµÑ„Ñ€Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ¸Ğ·Ğ¼ĞµĞ½ÑĞµĞ¼Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ (NMP) Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. PECCAVI ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°ĞºĞ¸ Ğ² NMP Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑˆÑƒĞ¼Ğ¾Ğ²ÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ğ¶Ğ¸Ğ½Ğ¸Ñ€Ğ¸Ğ½Ğ³Ñƒ. Ğ­Ñ‚Ğ° Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ¹ Ğ¸ Ğ¾Ğ±ĞµÑ‰Ğ°ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼."
                },
                "en": {
                    "title": "PECCAVI: Watermarking Resilience Against Visual Paraphrase Attacks",
                    "desc": "PECCAVI is a novel image watermarking technique designed to withstand visual paraphrase attacks and various distortions. It utilizes Non-Melting Points (NMPs) to strategically embed watermarks, ensuring that the essential features of the image remain intact. The method employs multi-channel frequency domain watermarking and incorporates noisy burnishing to protect against reverse-engineering attempts. This approach is model-agnostic, making it applicable across different systems, and all resources will be made available to the public."
                },
                "zh": {
                    "title": "PECCAVIï¼šæŠµå¾¡è§†è§‰æ”¹å†™çš„æ°´å°æ–°æŠ€æœ¯",
                    "desc": "PECCAVIæ˜¯ä¸€ç§å¼ºå¤§çš„å›¾åƒæ°´å°æŠ€æœ¯ï¼Œèƒ½å¤ŸæŠµå¾¡è§†è§‰æ”¹å†™æ”»å‡»å’Œå¤±çœŸã€‚å®ƒåˆ©ç”¨éç†”åŒ–ç‚¹ï¼ˆNMPsï¼‰å’Œå¤šé€šé“é¢‘åŸŸæ°´å°æŠ€æœ¯ï¼Œå°†æ°´å°åµŒå…¥å›¾åƒçš„æ ¸å¿ƒè¯­ä¹‰åŒºåŸŸã€‚è¯¥æŠ€æœ¯è¿˜é‡‡ç”¨äº†å™ªå£°çƒ§ç¼æ–¹æ³•ï¼Œä»¥é˜²æ­¢é€†å‘å·¥ç¨‹æ”»å‡»ï¼Œå¢å¼ºæ°´å°çš„è€ä¹…æ€§ã€‚PECCAVIä¸ä¾èµ–äºç‰¹å®šæ¨¡å‹ï¼Œæ‰€æœ‰ç›¸å…³èµ„æºå’Œä»£ç å°†å¼€æºã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-07-01.html",
    "link_next": "2025-07-03.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "01.07",
        "en": "07/01",
        "zh": "7æœˆ1æ—¥"
    },
    "short_date_next": {
        "ru": "03.07",
        "en": "07/03",
        "zh": "7æœˆ3æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 4,
        "#agents": 1,
        "#cv": 1,
        "#rl": 5,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 5,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 4,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 5,
        "#survey": 2,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}