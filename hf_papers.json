{
    "date": {
        "ru": "29 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 29",
        "zh": "11æœˆ29æ—¥"
    },
    "time_utc": "2024-11-29 06:23",
    "weekday": 4,
    "issue_id": 853,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.18203",
            "title": "Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning",
            "url": "https://huggingface.co/papers/2411.18203",
            "abstract": "Vision-language models~(VLMs) have shown remarkable advancements in multimodal reasoning tasks. However, they still often generate inaccurate or irrelevant responses due to issues like hallucinated image understandings or unrefined reasoning paths. To address these challenges, we introduce Critic-V, a novel framework inspired by the Actor-Critic paradigm to boost the reasoning capability of VLMs. This framework decouples the reasoning process and critic process by integrating two independent components: the Reasoner, which generates reasoning paths based on visual and textual inputs, and the Critic, which provides constructive critique to refine these paths. In this approach, the Reasoner generates reasoning responses according to text prompts, which can evolve iteratively as a policy based on feedback from the Critic. This interaction process was theoretically driven by a reinforcement learning framework where the Critic offers natural language critiques instead of scalar rewards, enabling more nuanced feedback to boost the Reasoner's capability on complex reasoning tasks. The Critic model is trained using Direct Preference Optimization (DPO), leveraging a preference dataset of critiques ranked by Rule-based Reward(RBR) to enhance its critic capabilities. Evaluation results show that the Critic-V framework significantly outperforms existing methods, including GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning accuracy and efficiency. Combining a dynamic text-based policy for the Reasoner and constructive feedback from the preference-optimized Critic enables a more reliable and context-sensitive multimodal reasoning process. Our approach provides a promising solution to enhance the reliability of VLMs, improving their performance in real-world reasoning-heavy multimodal applications such as autonomous driving and embodied intelligence.",
            "score": 7,
            "issue_id": 853,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 27",
                "zh": "11æœˆ27æ—¥"
            },
            "hash": "13bce174f2b29a74",
            "authors": [
                "Di Zhang",
                "Jingdi Lei",
                "Junxian Li",
                "Xunzhi Wang",
                "Yujie Liu",
                "Zonglin Yang",
                "Jiatong Li",
                "Weida Wang",
                "Suorong Yang",
                "Jianbo Wu",
                "Peng Ye",
                "Wanli Ouyang",
                "Dongzhan Zhou"
            ],
            "affiliations": [
                "Beijing Institute of Technology",
                "Fudan University",
                "Hong Kong Polytechnic University",
                "Nanjing University",
                "Nankai University",
                "Nanyang Technological University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiaotong University",
                "Shanghai University",
                "Tongji University",
                "University of California, Merced"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18203.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rlhf",
                    "#reasoning",
                    "#hallucinations",
                    "#rl",
                    "#multimodal",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Critic-V: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Critic-V Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM). Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Reasoner, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¸ Critic, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰ĞµĞ³Ğ¾ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ´Ğ»Ñ Ğ¸Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ³Ğ´Ğµ Critic Ğ´Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Critic-V Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-4V, Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğµ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing VLMs with Critic-V: A New Era in Multimodal Reasoning",
                    "desc": "This paper presents Critic-V, a new framework designed to improve the reasoning abilities of vision-language models (VLMs) in multimodal tasks. It separates the reasoning and critique processes by using two components: the Reasoner, which creates reasoning paths from visual and textual data, and the Critic, which evaluates and refines these paths. The Critic provides feedback in the form of natural language critiques, rather than simple rewards, allowing for more detailed guidance in the reasoning process. The results demonstrate that Critic-V significantly enhances reasoning accuracy and efficiency compared to existing models, making it a valuable advancement for applications requiring complex reasoning."
                },
                "zh": {
                    "title": "Critic-Vï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¡†æ¶Critic-Vï¼Œæ—¨åœ¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å€Ÿé‰´äº†æ¼”å‘˜-è¯„è®ºå®¶ï¼ˆActor-Criticï¼‰èŒƒå¼ï¼Œå°†æ¨ç†è¿‡ç¨‹ä¸è¯„è®ºè¿‡ç¨‹è§£è€¦ï¼Œåˆ†åˆ«ç”±æ¨ç†å™¨å’Œè¯„è®ºå™¨ä¸¤ä¸ªç‹¬ç«‹ç»„ä»¶å®Œæˆã€‚æ¨ç†å™¨æ ¹æ®è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ç”Ÿæˆæ¨ç†è·¯å¾„ï¼Œè€Œè¯„è®ºå™¨åˆ™æä¾›å»ºè®¾æ€§çš„åé¦ˆä»¥ä¼˜åŒ–è¿™äº›è·¯å¾„ã€‚é€šè¿‡è¿™ç§äº’åŠ¨è¿‡ç¨‹ï¼ŒCritic-Våœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†VLMsçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå±•ç¤ºäº†åœ¨å¤šæ¨¡æ€æ¨ç†åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14951",
            "title": "Morph: A Motion-free Physics Optimization Framework for Human Motion Generation",
            "url": "https://huggingface.co/papers/2411.14951",
            "abstract": "Human motion generation plays a vital role in applications such as digital humans and humanoid robot control. However, most existing approaches disregard physics constraints, leading to the frequent production of physically implausible motions with pronounced artifacts such as floating and foot sliding. In this paper, we propose Morph, a Motion-free physics optimization framework, comprising a Motion Generator and a Motion Physics Refinement module, for enhancing physical plausibility without relying on costly real-world motion data. Specifically, the Motion Generator is responsible for providing large-scale synthetic motion data, while the Motion Physics Refinement Module utilizes these synthetic data to train a motion imitator within a physics simulator, enforcing physical constraints to project the noisy motions into a physically-plausible space. These physically refined motions, in turn, are used to fine-tune the Motion Generator, further enhancing its capability. Experiments on both text-to-motion and music-to-dance generation tasks demonstrate that our framework achieves state-of-the-art motion generation quality while improving physical plausibility drastically.",
            "score": 1,
            "issue_id": 853,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 22",
                "zh": "11æœˆ22æ—¥"
            },
            "hash": "c2f59b10a563bc5d",
            "authors": [
                "Zhuo Li",
                "Mingshuang Luo",
                "Ruibing Hou",
                "Xin Zhao",
                "Hao Liu",
                "Hong Chang",
                "Zimo Liu",
                "Chen Li"
            ],
            "affiliations": [
                "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, China",
                "MoE Key Laboratory of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University",
                "Peng Cheng Laboratory, China",
                "University of Chinese Academy of Sciences, China",
                "WeChat, Tencent Inc"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14951.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#training",
                    "#optimization",
                    "#agents",
                    "#3d"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Morph - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. ĞĞ½ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ”Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‰ĞµĞ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¸ ĞœĞ¾Ğ´ÑƒĞ»Ñ Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ£Ñ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Morph Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Enhancing Human Motion with Physics-Driven Optimization",
                    "desc": "This paper introduces Morph, a framework designed to improve the physical realism of human motion generation for applications like digital humans and robots. It consists of two main components: a Motion Generator that creates large-scale synthetic motion data, and a Motion Physics Refinement module that uses this data to train a motion imitator within a physics simulator. By enforcing physical constraints, the framework reduces artifacts such as floating and foot sliding, resulting in more believable motions. The approach is validated through experiments in text-to-motion and music-to-dance tasks, showing significant improvements in both motion quality and physical plausibility."
                },
                "zh": {
                    "title": "æå‡è¿åŠ¨ç”Ÿæˆçš„ç‰©ç†åˆç†æ€§",
                    "desc": "äººç±»è¿åŠ¨ç”Ÿæˆåœ¨æ•°å­—äººç±»å’Œç±»äººæœºå™¨äººæ§åˆ¶ç­‰åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•å¤§å¤šå¿½è§†ç‰©ç†çº¦æŸï¼Œå¯¼è‡´ç”Ÿæˆçš„è¿åŠ¨å¸¸å¸¸ä¸ç¬¦åˆç‰©ç†è§„å¾‹ï¼Œå‡ºç°æ¼‚æµ®å’Œæ»‘åŠ¨ç­‰æ˜æ˜¾ä¼ªå½±ã€‚æœ¬æ–‡æå‡ºäº†Morphï¼Œä¸€ä¸ªæ— è¿åŠ¨çš„ç‰©ç†ä¼˜åŒ–æ¡†æ¶ï¼ŒåŒ…æ‹¬è¿åŠ¨ç”Ÿæˆå™¨å’Œè¿åŠ¨ç‰©ç†ç²¾ç‚¼æ¨¡å—ï¼Œæ—¨åœ¨åœ¨ä¸ä¾èµ–æ˜‚è´µçš„çœŸå®è¿åŠ¨æ•°æ®çš„æƒ…å†µä¸‹å¢å¼ºç‰©ç†åˆç†æ€§ã€‚é€šè¿‡åˆæˆè¿åŠ¨æ•°æ®è®­ç»ƒè¿åŠ¨æ¨¡ä»¿å™¨ï¼Œå¼ºåˆ¶ç‰©ç†çº¦æŸï¼Œå°†å™ªå£°è¿åŠ¨æŠ•å½±åˆ°ç‰©ç†åˆç†çš„ç©ºé—´ï¼Œä»è€Œå®ç°é«˜è´¨é‡çš„è¿åŠ¨ç”Ÿæˆã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-28.html",
    "link_next": "2024-12-02.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "28.11",
        "en": "11/28",
        "zh": "11æœˆ28æ—¥"
    },
    "short_date_next": {
        "ru": "02.12",
        "en": "12/02",
        "zh": "12æœˆ2æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†è‡ªç„¶è¯­è¨€åœ¨å¤„ç†å¤šå®ä¾‹çš„ä½ç½®å’Œå±æ€§ä¿¡æ¯æ—¶çš„å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºåŒºåŸŸå®ä¾‹æ§åˆ¶çš„æ‰©æ•£æ¨¡å‹ã€‚ä»–ä»¬å¼•å…¥äº†ROI-Unpoolæ“ä½œï¼Œä¸ROI-Alignç»“åˆï¼Œå®ç°äº†é«˜æ•ˆå’Œå‡†ç¡®çš„åŒºåŸŸæ“ä½œã€‚åŸºäºè¿™ä¸€æŠ€æœ¯ï¼Œä»–ä»¬æå‡ºäº†ROICtrlï¼Œä¸€ä¸ªé€‚é…å™¨ï¼Œèƒ½å¤Ÿç²¾ç¡®æ§åˆ¶åŒºåŸŸå®ä¾‹ã€‚å®éªŒè¡¨æ˜ï¼ŒROICtrlåœ¨åŒºåŸŸå®ä¾‹æ§åˆ¶æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚",
        "title": "ROICtrl: Boosting Instance Control for Visual Generation",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†è‡ªç„¶è¯­è¨€åœ¨å¤„ç†å¤šå®ä¾‹çš„ä½ç½®å’Œå±æ€§ä¿¡æ¯æ—¶çš„å±€é™æ€§ã€‚\nZhÃ¨ piÄn wÃ©n zhÄng tÇo lÃ¹n le zÃ¬ rÃ¡n yÇ” yÃ¡n zÃ i chÇ” lÇ duÅ shÃ­ lÃ¬ de wÃ¨i zhÃ¬ hÃ© shÇ” xÃ¬ng xÃ¬n xÄ« shÃ­ de jÃº xiÃ n xÃ¬ng.\n\nä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºåŒºåŸŸå®ä¾‹æ§åˆ¶çš„æ‰©æ•£æ¨¡å‹ã€‚\nWÃ¨i le jiÄ› juÃ© zhÃ¨ gÃ¨ wÃ¨n tÃ­, zuÃ² zhÄ› tÃ­ chÅ« le yÄ« zhÇ’ng jÄ« yÃº qÅ« yÃ¹ shÃ­ lÃ¬ kÃ²ng zhÃ¬ de kuÃ² sÃ n mÃ³ xÃ­ng.\n\nä»–ä»¬å¼•å…¥äº†ROI-Unpoolæ“ä½œï¼Œä¸ROI-Alignç»“åˆï¼Œå®ç°äº†é«˜æ•ˆå’Œå‡†ç¡®çš„åŒºåŸŸæ“ä½œã€‚\nTÄ men yÇn rÃ¹ le ROI-Unpool cÄo zuÃ², yÇ” ROI-Align jiÃ© hÃ©, shÃ­ xiÃ n le gÄo xiÃ o hÃ© zhÇ”n quÃ¨ de qÅ« yÃ¹ cÄo zuÃ².\n\nåŸºäºè¿™ä¸€æŠ€æœ¯ï¼Œä»–ä»¬æå‡ºäº†ROICtrlï¼Œä¸€ä¸ªé€‚é…å™¨ï¼Œèƒ½å¤Ÿç²¾ç¡®æ§åˆ¶åŒºåŸŸå®ä¾‹ã€‚\nJÄ« yÃº zhÃ¨ yÄ« jÃ¬ shÃ¹, tÄ men tÃ­ chÅ« le ROICtrl, yÄ« gÃ¨ shÃ¬ pÃ¨i qÃ¬, nÃ©ng gÃ²u jÄ«ng quÃ¨ kÃ²ng zhÃ¬ qÅ« yÃ¹ shÃ­ lÃ¬.\n\nå®éªŒè¡¨æ˜ï¼ŒROICtrlåœ¨åŒºåŸŸå®ä¾‹æ§åˆ¶æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚\nShÃ­ yÃ n biÇo mÃ­ng, ROICtrl zÃ i qÅ« yÃ¹ shÃ­ lÃ¬ kÃ²ng zhÃ¬ fÄng miÃ n biÇo xiÃ n yÅu yÃ¬, bÃ¬ng xiÇn zhÃ¹ jiÃ ng dÄ« le jÃ¬ suÃ n chÃ©ng bÄ›n.",
        "vocab": "[\n    {\"word\": \"è®¨è®º\", \"pinyin\": \"tÇo lÃ¹n\", \"trans\": \"discuss\"},\n    {\"word\": \"è‡ªç„¶è¯­è¨€\", \"pinyin\": \"zÃ¬ rÃ¡n yÇ” yÃ¡n\", \"trans\": \"natural language\"},\n    {\"word\": \"å¤„ç†\", \"pinyin\": \"chÇ” lÇ\", \"trans\": \"process\"},\n    {\"word\": \"å¤šå®ä¾‹\", \"pinyin\": \"duÅ shÃ­ lÃ¬\", \"trans\": \"multiple instances\"},\n    {\"word\": \"ä½ç½®\", \"pinyin\": \"wÃ¨i zhÃ¬\", \"trans\": \"position\"},\n    {\"word\": \"å±æ€§\", \"pinyin\": \"shÇ” xÃ¬ng\", \"trans\": \"attribute\"},\n    {\"word\": \"ä¿¡æ¯\", \"pinyin\": \"xÃ¬n xÄ«\", \"trans\": \"information\"},\n    {\"word\": \"å±€é™æ€§\", \"pinyin\": \"jÃº xiÃ n xÃ¬ng\", \"trans\": \"limitation\"},\n    {\"word\": \"è§£å†³\", \"pinyin\": \"jiÄ› juÃ©\", \"trans\": \"solve\"},\n    {\"word\": \"é—®é¢˜\", \"pinyin\": \"wÃ¨n tÃ­\", \"trans\": \"problem\"},\n    {\"word\": \"ä½œè€…\", \"pinyin\": \"zuÃ² zhÄ›\", \"trans\": \"author\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­ chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"åŸºäº\", \"pinyin\": \"jÄ« yÃº\", \"trans\": \"based on\"},\n    {\"word\": \"åŒºåŸŸ\", \"pinyin\": \"qÅ« yÃ¹\", \"trans\": \"region\"},\n    {\"word\": \"å®ä¾‹\", \"pinyin\": \"shÃ­ lÃ¬\", \"trans\": \"instance\"},\n    {\"word\": \"æ§åˆ¶\", \"pinyin\": \"kÃ²ng zhÃ¬\", \"trans\": \"control\"},\n    {\"word\": \"æ‰©æ•£\", \"pinyin\": \"kuÃ² sÃ n\", \"trans\": \"diffusion\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"å¼•å…¥\", \"pinyin\": \"yÇn rÃ¹\", \"trans\": \"introduce\"},\n    {\"word\": \"ROI-Unpool\", \"pinyin\": \"ROI-Unpool\", \"trans\": \"ROI-Unpool\"},\n    {\"word\": \"æ“ä½œ\", \"pinyin\": \"cÄo zuÃ²\", \"trans\": \"operation\"},\n    {\"word\": \"ç»“åˆ\", \"pinyin\": \"jiÃ© hÃ©\", \"trans\": \"combine\"},\n    {\"word\": \"å®ç°\", \"pinyin\": \"shÃ­ xiÃ n\", \"trans\": \"achieve\"},\n    {\"word\": \"é«˜æ•ˆ\", \"pinyin\": \"gÄo xiÃ o\", \"trans\": \"efficient\"},\n    {\"word\": \"å‡†ç¡®\", \"pinyin\": \"zhÇ”n quÃ¨\", \"trans\": \"accurate\"},\n    {\"word\": \"é€‚é…å™¨\", \"pinyin\": \"shÃ¬ pÃ¨i qÃ¬\", \"trans\": \"adapter\"},\n    {\"word\": \"ç²¾ç¡®\", \"pinyin\": \"jÄ«ng quÃ¨\", \"trans\": \"precise\"},\n    {\"word\": \"è¡¨æ˜\", \"pinyin\": \"biÇo mÃ­ng\", \"trans\": \"indicate\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇo xiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"ä¼˜å¼‚\", \"pinyin\": \"yÅu yÃ¬\", \"trans\": \"excellent\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇn zhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"é™ä½\", \"pinyin\": \"jiÃ ng dÄ«\", \"trans\": \"reduce\"},\n    {\"word\": \"è®¡ç®—\", \"pinyin\": \"jÃ¬ suÃ n\", \"trans\": \"computation\"},\n    {\"word\": \"æˆæœ¬\", \"pinyin\": \"chÃ©ng bÄ›n\", \"trans\": \"cost\"}\n]",
        "trans": "This article discusses the limitations of natural language in handling multi-instance positional and attribute information. To address this issue, the authors propose a diffusion model based on region instance control. They introduce the ROI-Unpool operation, which, when combined with ROI-Align, achieves efficient and accurate regional operations. Based on this technology, they present ROICtrl, an adapter that can precisely control regional instances. Experiments show that ROICtrl performs exceptionally well in regional instance control and significantly reduces computational costs.",
        "update_ts": "2024-11-28 09:11"
    }
}