{
    "date": {
        "ru": "26 июня",
        "en": "June 26",
        "zh": "6月26日"
    },
    "time_utc": "2025-06-26 07:13",
    "weekday": 3,
    "issue_id": 4498,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.18095",
            "title": "ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image\n  Generation",
            "url": "https://huggingface.co/papers/2506.18095",
            "abstract": "ShareGPT-4o-Image and Janus-4o enable open research in photorealistic, instruction-aligned image generation through a large dataset and multimodal model.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal generative models have unlocked photorealistic, instruction-aligned image generation, yet leading systems like GPT-4o-Image remain proprietary and inaccessible. To democratize these capabilities, we present ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and 46K text-and-image-to-image data, all synthesized using GPT-4o's image generation capabilities for distilling its advanced image generation abilities. Leveraging this dataset, we develop Janus-4o, a multimodal large language model capable of both text-to-image and text-and-image-to-image generation. Janus-4o not only significantly improves text-to-image generation over its predecessor, Janus-Pro, but also newly supports text-and-image-to-image generation. Notably, it achieves impressive performance in text-and-image-to-image generation from scratch, using only 91K synthetic samples and 6 hours of training on an 8 A800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will foster open research in photorealistic, instruction-aligned image generation.",
            "score": 38,
            "issue_id": 4496,
            "pub_date": "2025-06-22",
            "pub_date_card": {
                "ru": "22 июня",
                "en": "June 22",
                "zh": "6月22日"
            },
            "hash": "ea0d767800ce404b",
            "authors": [
                "Junying Chen",
                "Zhenyang Cai",
                "Pengcheng Chen",
                "Shunian Chen",
                "Ke Ji",
                "Xidong Wang",
                "Yunjin Yang",
                "Benyou Wang"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18095.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#open_source",
                    "#multimodal",
                    "#synthetic"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Демократизация фотореалистичной генерации изображений с помощью открытых данных и моделей",
                    "desc": "Статья представляет ShareGPT-4o-Image - первый набор данных, содержащий 45 тысяч примеров генерации изображений по тексту и 46 тысяч примеров генерации изображений по тексту и изображению, созданных с помощью GPT-4o. На основе этого датасета разработана мультимодальная языковая модель Janus-4o, способная генерировать изображения как по тексту, так и по тексту с изображением. Janus-4o значительно улучшает генерацию изображений по сравнению с предшественником Janus-Pro и достигает впечатляющих результатов в генерации изображений по тексту и изображению, используя всего 91 тысячу синтетических примеров. Авторы надеются, что публикация ShareGPT-4o-Image и Janus-4o будет способствовать открытым исследованиям в области фотореалистичной генерации изображений, согласованной с инструкциями."
                },
                "en": {
                    "title": "Democratizing Photorealistic Image Generation with Open Datasets and Models",
                    "desc": "This paper introduces ShareGPT-4o-Image, a comprehensive dataset designed to enhance photorealistic image generation aligned with user instructions. It includes 45,000 text-to-image and 46,000 text-and-image-to-image samples, all generated using the advanced capabilities of GPT-4o. The authors also present Janus-4o, a multimodal large language model that improves upon previous models by enabling both text-to-image and text-and-image-to-image generation. With only 91,000 synthetic samples and minimal training time, Janus-4o demonstrates significant advancements in generating high-quality images, promoting open research in this field."
                },
                "zh": {
                    "title": "开放研究，真实感图像生成的新纪元",
                    "desc": "本论文介绍了ShareGPT-4o-Image数据集和Janus-4o模型，旨在推动开放研究在真实感图像生成领域的发展。ShareGPT-4o-Image包含45K文本到图像和46K文本与图像到图像的数据，利用GPT-4o的图像生成能力进行合成。Janus-4o是一个多模态大语言模型，能够进行文本到图像和文本与图像到图像的生成，显著提升了生成效果。我们希望这些工具能够促进真实感、指令对齐的图像生成研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.19697",
            "title": "Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large\n  Language Models",
            "url": "https://huggingface.co/papers/2506.19697",
            "abstract": "Outlier-Safe Pre-Training improves large language model quantization performance by preventing extreme activation outliers through innovative training techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Extreme activation outliers in Large Language Models (LLMs) critically degrade quantization performance, hindering efficient on-device deployment. While channel-wise operations and adaptive gradient scaling are recognized causes, practical mitigation remains challenging. We introduce Outlier-Safe Pre-Training (OSP), a practical guideline that proactively prevents outlier formation rather than relying on post-hoc mitigation. OSP combines three key innovations: (1) the Muon optimizer, eliminating privileged bases while maintaining training efficiency; (2) Single-Scale RMSNorm, preventing channel-wise amplification; and (3) a learnable embedding projection, redistributing activation magnitudes originating from embedding matrices. We validate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is the first production-scale LLM trained without such outliers. Under aggressive 4-bit quantization, our OSP model achieves a 35.7 average score across 10 benchmarks (compared to 26.5 for an Adam-trained model), with only a 2% training overhead. Remarkably, OSP models exhibit near-zero excess kurtosis (0.04) compared to extreme values (1818.56) in standard models, fundamentally altering LLM quantization behavior. Our work demonstrates that outliers are not inherent to LLMs but are consequences of training strategies, paving the way for more efficient LLM deployment. The source code and pretrained checkpoints are available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.",
            "score": 26,
            "issue_id": 4495,
            "pub_date": "2025-06-24",
            "pub_date_card": {
                "ru": "24 июня",
                "en": "June 24",
                "zh": "6月24日"
            },
            "hash": "3a6d3af4578d26f6",
            "authors": [
                "Jungwoo Park",
                "Taewhoo Lee",
                "Chanwoong Yoon",
                "Hyeon Hwang",
                "Jaewoo Kang"
            ],
            "affiliations": [
                "AIGEN Sciences",
                "Korea University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.19697.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Безопасное предобучение LLM для эффективного квантования",
                    "desc": "Статья представляет новый метод предварительного обучения больших языковых моделей (LLM), называемый Outlier-Safe Pre-Training (OSP). OSP предотвращает образование экстремальных выбросов активации, которые ухудшают производительность квантования моделей. Метод включает в себя три ключевые инновации: оптимизатор Muon, однопараметрическую нормализацию RMSNorm и обучаемую проекцию эмбеддингов. Эксперименты показывают, что OSP значительно улучшает производительность 4-битного квантования LLM, делая их более эффективными для развертывания на устройствах."
                },
                "en": {
                    "title": "Preventing Outliers for Better LLM Performance",
                    "desc": "This paper presents Outlier-Safe Pre-Training (OSP), a novel approach to enhance the quantization performance of large language models (LLMs) by preventing extreme activation outliers during training. The authors identify that these outliers significantly impair the efficiency of LLMs when deployed on devices, and propose a proactive strategy rather than relying on post-training fixes. OSP incorporates three innovations: the Muon optimizer for efficient training, Single-Scale RMSNorm to control channel-wise amplification, and a learnable embedding projection to manage activation magnitudes. The results show that OSP-trained models achieve superior performance in quantization benchmarks while maintaining low training overhead, indicating that outliers can be effectively managed through improved training techniques."
                },
                "zh": {
                    "title": "创新训练技术，提升模型量化性能",
                    "desc": "本论文提出了一种名为Outlier-Safe Pre-Training（OSP）的新方法，旨在改善大型语言模型（LLM）的量化性能。OSP通过创新的训练技术，主动防止极端激活异常值的形成，从而提高模型在设备上的高效部署。该方法结合了三项关键创新：Muon优化器、单尺度RMSNorm和可学习的嵌入投影，显著降低了训练过程中的异常值。实验结果表明，OSP模型在4位量化下的表现优于传统模型，展示了训练策略对异常值的影响。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18403",
            "title": "The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs",
            "url": "https://huggingface.co/papers/2506.18403",
            "abstract": "The Debugging Decay Index (DDI) quantifies and optimizes the effectiveness of iterative AI debugging by predicting intervention points to revive and enhance debugging capability.  \t\t\t\t\tAI-generated summary \t\t\t\t The effectiveness of AI debugging follows a predictable exponential decay pattern; most models lose 60-80% of their debugging capability within just 2-3 attempts, despite iterative debugging being a critical capability for practical code generation systems. We introduce the Debugging Decay Index (DDI), a mathematical framework that quantifies when debugging becomes ineffective and predicts intervention points. Our strategic fresh start approach shifts from exploitation to exploration at strategic points in the debugging process, demonstrating that well-timed interventions can rescue the effectiveness of debugging. DDI reveals a fundamental limitation in current AI debugging and provides the first quantitative framework for optimising iterative code generation strategies.",
            "score": 2,
            "issue_id": 4494,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 июня",
                "en": "June 23",
                "zh": "6月23日"
            },
            "hash": "30aa788d94afe45d",
            "authors": [
                "Muntasir Adnan",
                "Carlos C. N. Kuhn"
            ],
            "affiliations": [
                "Open Source Institute, University of Canberra, Bruce, Canberra, Australia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18403.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#math"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Оптимизация отладки ИИ: измерение и преодоление затухания эффективности",
                    "desc": "Статья представляет Индекс Затухания Отладки (DDI), который количественно оценивает эффективность итеративной отладки ИИ. Авторы обнаружили, что способность моделей к отладке быстро снижается, следуя экспоненциальному паттерну затухания. DDI позволяет предсказать оптимальные точки вмешательства для восстановления эффективности отладки. Предложенный подход стратегического свежего старта демонстрирует, что своевременные вмешательства могут значительно улучшить процесс отладки кода."
                },
                "en": {
                    "title": "Reviving AI Debugging with Strategic Interventions",
                    "desc": "The Debugging Decay Index (DDI) is a new method that measures how quickly AI debugging abilities decline over time. It shows that most AI models lose a significant portion of their debugging skills after just a few attempts, which is a major issue for systems that generate code. By using DDI, developers can identify the best moments to intervene and improve the debugging process, shifting from refining existing solutions to exploring new ones. This approach not only highlights a key limitation in current AI debugging methods but also offers a way to enhance the effectiveness of iterative code generation."
                },
                "zh": {
                    "title": "优化AI调试的关键：调试衰减指数",
                    "desc": "调试衰减指数（DDI）量化并优化了迭代AI调试的有效性，通过预测干预点来恢复和增强调试能力。研究表明，AI调试的有效性遵循可预测的指数衰减模式，大多数模型在仅仅2-3次尝试后就会失去60-80%的调试能力。我们提出的DDI框架可以量化调试失效的时机，并预测干预点，从而在调试过程中实现从利用到探索的战略转变。DDI揭示了当前AI调试的基本局限性，并提供了优化迭代代码生成策略的首个定量框架。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.20544",
            "title": "When Life Gives You Samples: The Benefits of Scaling up Inference\n  Compute for Multilingual LLMs",
            "url": "https://huggingface.co/papers/2506.20544",
            "abstract": "The study examines and proposes new sampling and selection strategies to enhance inference-time compute for multilingual and multi-task large language models, demonstrating significant improvements in win-rates across various languages and tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in large language models (LLMs) have shifted focus toward scaling inference-time compute, improving performance without retraining the model. A common approach is to sample multiple outputs in parallel, and select one of these as the final output. However, work to date has focused on English and a handful of domains such as math and code. In contrast, we are most interested in techniques that generalize across open-ended tasks, formally verifiable tasks, and across languages. In this work, we study how to robustly scale inference-time compute for open-ended generative tasks in a multilingual, multi-task setting.   Our findings show that both sampling strategy based on temperature variation and selection strategy must be adapted to account for diverse domains and varied language settings. We evaluate existing selection methods, revealing that strategies effective in English often fail to generalize across languages. We propose novel sampling and selection strategies specifically adapted for multilingual and multi-task inference scenarios, and show they yield notable gains across languages and tasks. In particular, our combined sampling and selection methods lead to an average +6.8 jump in win-rates for our 8B models on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At larger scale, Command-A (111B model) equipped with our methods, shows +9.0 improvement in win-rates on the same benchmark with just five samples against single-sample decoding, a substantial increase at minimal cost. Our results underscore the need for language- and task-aware approaches to inference-time compute, aiming to democratize performance improvements in underrepresented languages.",
            "score": 1,
            "issue_id": 4497,
            "pub_date": "2025-06-25",
            "pub_date_card": {
                "ru": "25 июня",
                "en": "June 25",
                "zh": "6月25日"
            },
            "hash": "8c4af7dcfe82a334",
            "authors": [
                "Ammar Khairi",
                "Daniel D'souza",
                "Ye Shen",
                "Julia Kreutzer",
                "Sara Hooker"
            ],
            "affiliations": [
                "Cohere",
                "Cohere Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.20544.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#inference",
                    "#low_resource"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Повышение эффективности многоязычных LLM через оптимизацию вывода",
                    "desc": "Исследование посвящено новым стратегиям выборки и отбора для улучшения вычислений во время вывода многоязычных и многозадачных больших языковых моделей (LLM). Авторы предлагают методы, адаптированные для различных языков и задач, которые показывают значительное улучшение результатов. Особое внимание уделяется обобщению на открытые генеративные задачи в многоязычном контексте. Результаты демонстрируют необходимость учета специфики языков и задач при оптимизации вычислений LLM на этапе вывода."
                },
                "en": {
                    "title": "Enhancing Multilingual Performance in Large Language Models",
                    "desc": "This paper explores new methods for sampling and selecting outputs from large language models (LLMs) during inference to improve their performance across multiple languages and tasks. The authors highlight that traditional strategies often focus on English and specific domains, which limits their effectiveness in diverse settings. They propose innovative sampling techniques that adjust for temperature variations and selection methods tailored to different languages and tasks. The results demonstrate significant improvements in win-rates, showcasing the importance of adapting inference strategies to enhance multilingual and multi-task capabilities of LLMs."
                },
                "zh": {
                    "title": "提升多语言模型推理效率的新策略",
                    "desc": "本研究探讨并提出了新的采样和选择策略，以增强多语言和多任务大语言模型的推理时间计算效率。我们发现，基于温度变化的采样策略和选择策略必须适应不同领域和语言环境。通过评估现有的选择方法，我们揭示了在英语中有效的策略往往无法跨语言推广。我们提出的创新方法在多语言和多任务推理场景中表现出显著的性能提升，特别是在m-ArenaHard-v2.0基准测试中，8B模型的胜率平均提高了6.8个百分点。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.19502",
            "title": "MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility\n  Applications",
            "url": "https://huggingface.co/papers/2506.19502",
            "abstract": "MATE, a multimodal accessibility multi-agent system, converts data into understandable formats based on user needs, supporting various disabilities and integrating with institutional technologies.  \t\t\t\t\tAI-generated summary \t\t\t\t Accessibility remains a critical concern in today's society, as many technologies are not developed to support the full range of user needs. Existing multi-agent systems (MAS) often cannot provide comprehensive assistance for users in need due to the lack of customization stemming from closed-source designs. Consequently, individuals with disabilities frequently encounter significant barriers when attempting to interact with digital environments. We introduce MATE, a multimodal accessibility MAS, which performs the modality conversions based on the user's needs. The system is useful for assisting people with disabilities by ensuring that data will be converted to an understandable format. For instance, if the user cannot see well and receives an image, the system converts this image to its audio description. MATE can be applied to a wide range of domains, industries, and areas, such as healthcare, and can become a useful assistant for various groups of users. The system supports multiple types of models, ranging from LLM API calling to using custom machine learning (ML) classifiers. This flexibility ensures that the system can be adapted to various needs and is compatible with a wide variety of hardware. Since the system is expected to run locally, it ensures the privacy and security of sensitive information. In addition, the framework can be effectively integrated with institutional technologies (e.g., digital healthcare service) for real-time user assistance. Furthermore, we introduce ModCon-Task-Identifier, a model that is capable of extracting the precise modality conversion task from the user input. Numerous experiments show that ModCon-Task-Identifier consistently outperforms other LLMs and statistical models on our custom data. Our code and data are publicly available at https://github.com/AlgazinovAleksandr/Multi-Agent-MATE.",
            "score": 1,
            "issue_id": 4494,
            "pub_date": "2025-06-24",
            "pub_date_card": {
                "ru": "24 июня",
                "en": "June 24",
                "zh": "6月24日"
            },
            "hash": "85d844ff6061cc93",
            "authors": [
                "Aleksandr Algazinov",
                "Matt Laing",
                "Paul Laban"
            ],
            "affiliations": [
                "Dept. of Comp. Sci. & Tech. Tsinghua University Beijing, China",
                "Dept. of Psych. & Cog. Sci. Tsinghua University Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.19502.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#agents",
                    "#multimodal",
                    "#ethics",
                    "#open_source"
                ],
                "emoji": "♿",
                "ru": {
                    "title": "MATE: Интеллектуальная система для преодоления барьеров доступности",
                    "desc": "MATE - это мультимодальная система мультиагентов для обеспечения доступности, которая преобразует данные в понятные форматы в зависимости от потребностей пользователя. Система поддерживает различные виды инвалидности и может интегрироваться с институциональными технологиями. MATE использует широкий спектр моделей, от вызовов API больших языковых моделей до пользовательских классификаторов машинного обучения. Система включает в себя ModCon-Task-Identifier - модель, способную точно определять задачу преобразования модальности из пользовательского ввода."
                },
                "en": {
                    "title": "Empowering Accessibility Through Intelligent Data Conversion",
                    "desc": "MATE is a multimodal accessibility multi-agent system designed to convert data into formats that are understandable for users with disabilities. It addresses the limitations of existing multi-agent systems by providing customizable solutions that adapt to individual user needs. The system can perform tasks like converting images to audio descriptions, making digital content more accessible. Additionally, MATE integrates with institutional technologies and ensures user privacy by running locally, while its ModCon-Task-Identifier model excels in identifying specific modality conversion tasks."
                },
                "zh": {
                    "title": "MATE：为每个人提供无障碍的智能助手",
                    "desc": "MATE是一个多模态无障碍多代理系统，旨在根据用户需求将数据转换为可理解的格式，以支持各种残疾人士。该系统通过执行模态转换，帮助用户更好地与数字环境互动，例如将图像转换为音频描述，以满足视觉障碍者的需求。MATE具有灵活性，支持多种模型和硬件，确保能够适应不同的用户需求，并保护敏感信息的隐私和安全。通过与机构技术的有效集成，MATE能够提供实时的用户支持，提升无障碍服务的质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18674",
            "title": "Is There a Case for Conversation Optimized Tokenizers in Large Language\n  Models?",
            "url": "https://huggingface.co/papers/2506.18674",
            "abstract": "Optimizing tokenizers for chatbot conversations reduces computational costs and energy usage with minimal impact on training corpus performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The computational and energy costs of Large Language Models (LLMs) have increased exponentially driven by the growing model sizes and the massive adoption of LLMs by hundreds of millions of users. The unit cost of an LLM is the computation of a token. Therefore, the tokenizer plays an important role in the efficiency of a model, and they are carefully optimized to minimize the number of tokens for the text in their training corpus. One of the most popular applications of LLMs are chatbots that interact with users. A key observation is that, for those chatbots, what is important is the performance of the tokenizer in the user text input and the chatbot responses. Those are most likely different from the text in the training corpus. So, a question that immediately arises is whether there is a potential benefit in optimizing tokenizers for chatbot conversations. In this paper, this idea is explored for different tokenizers by using a publicly available corpus of chatbot conversations to redesign their vocabularies and evaluate their performance in this domain. The results show that conversation-optimized tokenizers consistently reduce the number of tokens in chatbot dialogues, which can lead to meaningful energy savings, in the range of 5% to 10% while having minimal or even slightly positive impact on tokenization efficiency for the original training corpus.",
            "score": 1,
            "issue_id": 4497,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 июня",
                "en": "June 23",
                "zh": "6月23日"
            },
            "hash": "8daec89854af9338",
            "authors": [
                "Raquel Ferrando",
                "Javier Conde",
                "Gonzalo Martínez",
                "Pedro Reviriego"
            ],
            "affiliations": [
                "ETSI de Telecomunicación Universidad Politécnica de Madrid 28040 Madrid, Spain"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18674.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#data"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Экономия энергии через оптимизацию токенизации для чат-ботов",
                    "desc": "Исследование показывает, что оптимизация токенизаторов для чат-ботов может снизить вычислительные затраты и энергопотребление на 5-10%. Авторы перестроили словари токенизаторов, используя корпус диалогов с чат-ботами. Оптимизированные токенизаторы сократили количество токенов в диалогах, при этом минимально влияя на эффективность для оригинального обучающего корпуса. Это важно, так как токенизация - ключевой фактор эффективности больших языковых моделей (LLM)."
                },
                "en": {
                    "title": "Optimize Tokenizers, Save Energy!",
                    "desc": "This paper explores the optimization of tokenizers specifically for chatbot conversations to enhance efficiency and reduce costs. It highlights that the computational expense of Large Language Models (LLMs) is closely tied to the number of tokens processed, making tokenizer performance crucial. By redesigning vocabularies based on a corpus of chatbot dialogues, the study demonstrates that conversation-optimized tokenizers can decrease token counts by 5% to 10%. Importantly, this optimization has little to no negative effect on the performance of the original training corpus, suggesting a dual benefit of energy savings and maintained efficiency."
                },
                "zh": {
                    "title": "优化分词器，节省能源与成本",
                    "desc": "本论文探讨了为聊天机器人对话优化分词器的潜在好处。随着大型语言模型（LLMs）的广泛应用，计算和能源成本急剧上升，而分词器在模型效率中扮演着重要角色。研究表明，针对聊天对话优化的分词器能够显著减少对话中的标记数量，从而节省5%到10%的能源消耗，同时对原始训练语料的标记化效率影响较小或略有正面效果。通过重新设计分词器的词汇，本文展示了在聊天机器人领域优化分词器的有效性。"
                }
            }
        }
    ],
    "link_prev": "2025-06-25.html",
    "link_next": "2025-06-27.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "25.06",
        "en": "06/25",
        "zh": "6月25日"
    },
    "short_date_next": {
        "ru": "27.06",
        "en": "06/27",
        "zh": "6月27日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 0,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 0,
        "#healthcare": 1,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    }
}