{
    "date": {
        "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 13",
        "zh": "2æœˆ13æ—¥"
    },
    "time_utc": "2026-02-13 08:34",
    "weekday": 4,
    "issue_id": 1040,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2602.09877",
            "title": "The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies",
            "url": "https://huggingface.co/papers/2602.09877",
            "abstract": "Multi-agent LLM systems face fundamental limitations in achieving continuous self-improvement while maintaining safety alignment due to inherent statistical blind spots in isolated evolution.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms.",
            "score": 116,
            "issue_id": 1036,
            "pub_date": "2026-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "b2d2392c6c84757a",
            "authors": [
                "Chenxu Wang",
                "Chaozhuo Li",
                "Songyang Liu",
                "Zejian Chen",
                "Jinyu Hou",
                "Ji Qi",
                "Rui Li",
                "Litian Zhang",
                "Qiwei Ye",
                "Zheng Liu",
                "Xu Chen",
                "Xi Zhang",
                "Philip S. Yu"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence",
                "Beijing University of Posts and Telecommunications",
                "Renmin University of China",
                "University of Illinois at Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09877.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#training",
                    "#agents",
                    "#security"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ¤ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸ĞµĞ¹ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸Ğ·Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ - ÑÑ‚Ğ¾ Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ñ‚Ñ€Ğ¸Ğ»Ğ»ĞµĞ¼Ğ¾Ğ¹ ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğº Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ Ğ°Ğ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½ĞµĞ¸Ğ·Ğ±ĞµĞ¶Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ»ĞµĞ¿Ñ‹Ñ… Ğ¿ÑÑ‚ĞµĞ½ Ğ¸ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Navigating the Self-Evolution Trilemma in AI Safety",
                    "desc": "This paper discusses the challenges faced by multi-agent systems that use large language models (LLMs) in achieving continuous self-improvement while ensuring safety. The authors introduce the concept of the 'self-evolution trilemma,' which highlights the difficulty of balancing self-evolution, isolation, and safety. They show that isolated self-evolution leads to statistical blind spots, which can cause safety alignment to degrade over time. The paper also suggests potential solutions to improve safety in these systems, emphasizing the importance of external oversight and innovative safety mechanisms."
                },
                "zh": {
                    "title": "è‡ªæˆ‘è¿›åŒ–çš„AIç¤¾ä¼šé¢ä¸´å®‰å…¨æŒ‘æˆ˜",
                    "desc": "å¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹ç³»ç»Ÿåœ¨å®ç°æŒç»­è‡ªæˆ‘æ”¹è¿›çš„åŒæ—¶ä¿æŒå®‰å…¨å¯¹é½é¢ä¸´æ ¹æœ¬æ€§é™åˆ¶ã€‚è¿™æ˜¯å› ä¸ºå­¤ç«‹è¿›åŒ–ä¸­å­˜åœ¨å›ºæœ‰çš„ç»Ÿè®¡ç›²ç‚¹ï¼Œå¯¼è‡´æ— æ³•åŒæ—¶æ»¡è¶³æŒç»­è‡ªæˆ‘è¿›åŒ–ã€å®Œå…¨éš”ç¦»å’Œå®‰å…¨ä¸å˜æ€§ã€‚æˆ‘ä»¬é€šè¿‡ä¿¡æ¯è®ºæ¡†æ¶å°†å®‰å…¨æ€§å½¢å¼åŒ–ä¸ºä¸äººç±»ä»·å€¼åˆ†å¸ƒçš„åç¦»ç¨‹åº¦ï¼Œå¹¶ç†è®ºä¸Šè¯æ˜å­¤ç«‹è‡ªæˆ‘è¿›åŒ–ä¼šå¯¼è‡´å®‰å…¨å¯¹é½çš„ä¸å¯é€†é™çº§ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†è‡ªæˆ‘è¿›åŒ–çš„AIç¤¾ä¼šçš„åŸºæœ¬é™åˆ¶ï¼Œå¹¶å¼ºè°ƒäº†å¤–éƒ¨ç›‘ç£æˆ–æ–°å‹å®‰å…¨ä¿æŠ¤æœºåˆ¶çš„å¿…è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.12125",
            "title": "Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation",
            "url": "https://huggingface.co/papers/2602.12125",
            "abstract": "On-policy distillation is extended through a generalized framework that introduces flexible reference models and reward scaling factors, demonstrating improved performance through reward extrapolation and reward correction techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t On-policy distillation (OPD), which aligns the student with the teacher's logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher's performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher's base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher's pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.",
            "score": 34,
            "issue_id": 1036,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "61dc2f3b4d89050f",
            "authors": [
                "Wenkai Yang",
                "Weijie Liu",
                "Ruobing Xie",
                "Kai Yang",
                "Saiyong Yang",
                "Yankai Lin"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "LLM Department, Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.12125.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#training",
                    "#rl",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ: ĞºĞ°Ğº ÑƒÑ‡ĞµĞ½Ğ¸Ğº Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Â«Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞµÂ» (on-policy distillation), Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ñ‘Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ğ¼ ÑĞ»ÑƒÑ‡Ğ°ĞµĞ¼ KL-Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ³Ğ´Ğµ Ğ²ĞµÑ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ KL-Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğ¹. Ğ’ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ G-OPD ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ Ğ²Ğ²ĞµĞ´ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ: ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ 1) Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ ÑƒÑ‡ĞµĞ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Boosting Student Performance with Generalized On-Policy Distillation",
                    "desc": "This paper extends the concept of on-policy distillation (OPD) by introducing a Generalized On-Policy Distillation (G-OPD) framework, which allows for flexible reference models and adjustable reward scaling factors. The authors demonstrate that by using reward extrapolation, where the reward scaling factor is set above 1, the performance of the student model can surpass that of the teacher model in certain scenarios. Additionally, they explore reward correction techniques that enhance the accuracy of the reward signal when distilling from a larger teacher model to a smaller student model. Overall, the findings suggest that these new methods can significantly improve the effectiveness of student models in various tasks, such as math reasoning and code generation."
                },
                "zh": {
                    "title": "æ‰©å±•åœ¨çº¿è’¸é¦ï¼šæå‡å­¦ç”Ÿæ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ‰©å±•çš„åœ¨çº¿è’¸é¦æ¡†æ¶ï¼Œç§°ä¸ºå¹¿ä¹‰åœ¨çº¿è’¸é¦ï¼ˆG-OPDï¼‰ï¼Œé€šè¿‡å¼•å…¥çµæ´»çš„å‚è€ƒæ¨¡å‹å’Œå¥–åŠ±ç¼©æ”¾å› å­æ¥æé«˜å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè®¾ç½®å¥–åŠ±ç¼©æ”¾å› å­å¤§äº1ï¼ˆå³å¥–åŠ±å¤–æ¨ï¼‰å¯ä»¥æ˜¾è‘—æå‡æ ‡å‡†åœ¨çº¿è’¸é¦çš„æ•ˆæœï¼Œå°¤å…¶æ˜¯åœ¨åˆå¹¶æ¥è‡ªä¸åŒé¢†åŸŸä¸“å®¶çš„çŸ¥è¯†æ—¶ã€‚é€šè¿‡å¥–åŠ±ä¿®æ­£ï¼Œé€‰æ‹©æ•™å¸ˆçš„åŸºç¡€æ¨¡å‹ä½œä¸ºå‚è€ƒæ¨¡å‹ï¼Œå¯ä»¥è·å¾—æ›´å‡†ç¡®çš„å¥–åŠ±ä¿¡å·ï¼Œä»è€Œè¿›ä¸€æ­¥æ”¹å–„è’¸é¦æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºåœ¨çº¿è’¸é¦çš„æœªæ¥ç ”ç©¶æä¾›äº†æ–°çš„è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.10934",
            "title": "MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models",
            "url": "https://huggingface.co/papers/2602.10934",
            "abstract": "A fully end-to-end Transformer-based audio tokenizer architecture achieves high-fidelity reconstruction across diverse audio domains and enables superior text-to-speech and automatic speech recognition performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Discrete audio tokenizers are fundamental to empowering large language models with native audio processing and generation capabilities. Despite recent progress, existing approaches often rely on pretrained encoders, semantic distillation, or heterogeneous CNN-based architectures. These designs introduce fixed inductive biases that limit reconstruction fidelity and hinder effective scaling. In this paper, we argue that discrete audio tokenization should be learned fully end-to-end using a homogeneous and scalable architecture. To this end, we first propose CAT (Causal Audio Tokenizer with Transformer), a purely Transformer-based architecture that jointly optimizes the encoder, quantizer, and decoder from scratch for high-fidelity reconstruction. Building on the CAT architecture, we develop MOSS-Audio-Tokenizer, a large-scale audio tokenizer featuring 1.6 billion parameters, pre-trained on 3 million hours of diverse, general audio data. We show that this simple, fully end-to-end approach built from homogeneous, causal Transformer blocks scales gracefully and supports high-fidelity reconstruction across diverse audio domains. Across speech, sound, and music, MOSS-Audio-Tokenizer consistently outperforms prior codecs over a wide range of bitrates, while exhibiting predictable improvements with increased scale. Notably, leveraging the discrete tokens from our model, we develop the first purely autoregressive TTS model that surpasses prior non-autoregressive and cascaded systems. Furthermore, MOSS-Audio-Tokenizer enables competitive ASR performance without auxiliary encoders. Our findings position the CAT architecture as a unified, scalable interface for the next generation of native audio foundation models.",
            "score": 34,
            "issue_id": 1037,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "bb9e0cc5bf9f0a9e",
            "authors": [
                "Yitian Gong",
                "Kuangwei Chen",
                "Zhaoye Fei",
                "Xiaogui Yang",
                "Ke Chen",
                "Yang Wang",
                "Kexin Huang",
                "Mingshu Chen",
                "Ruixiao Li",
                "Qingyuan Cheng",
                "Shimin Li",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "MOSI.AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10934.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#audio",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "Ğ¡ĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ´Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€-Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° CAT (Causal Audio Tokenizer with Transformer) â€” Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€-Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞºĞ²Ğ¾Ğ·Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€, ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ CAT Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ MOSS-Audio-Tokenizer Ñ 1.6 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° 3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‡Ğ°ÑĞ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ°Ñ… Ğ±Ğ¸Ñ‚Ñ€ĞµĞ¹Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€ĞµÑ‡Ğ¸, Ğ·Ğ²ÑƒĞºĞ° Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾."
                },
                "en": {
                    "title": "Transforming Audio: High-Fidelity Tokenization with Transformers",
                    "desc": "This paper introduces a new audio tokenizer architecture called MOSS-Audio-Tokenizer, which is based entirely on Transformers and designed for high-fidelity audio reconstruction. Unlike previous methods that depend on fixed architectures or pretrained models, this approach learns the entire process end-to-end, optimizing the encoder, quantizer, and decoder together. The model, with 1.6 billion parameters, is trained on a vast dataset of diverse audio, allowing it to perform exceptionally well in text-to-speech (TTS) and automatic speech recognition (ASR) tasks. The results demonstrate that this architecture not only scales effectively but also consistently outperforms existing audio codecs across various audio types and bitrates."
                },
                "zh": {
                    "title": "å…¨ç«¯åˆ°ç«¯çš„éŸ³é¢‘æ ‡è®°å™¨ï¼Œé‡å¡‘éŸ³é¢‘å¤„ç†çš„æœªæ¥",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºTransformerçš„éŸ³é¢‘æ ‡è®°å™¨æ¶æ„ï¼Œèƒ½å¤Ÿåœ¨å¤šç§éŸ³é¢‘é¢†åŸŸå®ç°é«˜ä¿çœŸé‡å»ºï¼Œå¹¶æå‡æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºçš„CATï¼ˆå› æœéŸ³é¢‘æ ‡è®°å™¨ï¼‰æ¶æ„é€šè¿‡ç«¯åˆ°ç«¯çš„å­¦ä¹ ï¼Œä¼˜åŒ–ç¼–ç å™¨ã€é‡åŒ–å™¨å’Œè§£ç å™¨ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚åŸºäºCATæ¶æ„ï¼Œæˆ‘ä»¬å¼€å‘äº†MOSS-Audio-Tokenizerï¼Œå…·æœ‰16äº¿å‚æ•°ï¼Œç»è¿‡300ä¸‡å°æ—¶å¤šæ ·åŒ–éŸ³é¢‘æ•°æ®çš„é¢„è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è¯­éŸ³ã€å£°éŸ³å’ŒéŸ³ä¹ç­‰é¢†åŸŸçš„è¡¨ç°ä¼˜äºç°æœ‰çš„éŸ³é¢‘ç¼–è§£ç å™¨ï¼Œå¹¶ä¸”éšç€è§„æ¨¡çš„å¢åŠ ï¼Œæ€§èƒ½æŒç»­æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.12099",
            "title": "GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning",
            "url": "https://huggingface.co/papers/2602.12099",
            "abstract": "A vision-language-action model enhanced with world model-based reinforcement learning demonstrates improved performance and long-horizon execution capabilities for robotic manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language-action (VLA) models that directly predict multi-step action chunks from current observations face inherent limitations due to constrained scene understanding and weak future anticipation capabilities. In contrast, video world models pre-trained on web-scale video corpora exhibit robust spatiotemporal reasoning and accurate future prediction, making them a natural foundation for enhancing VLA learning. Therefore, we propose GigaBrain-0.5M*, a VLA model trained via world model-based reinforcement learning. Built upon GigaBrain-0.5, which is pre-trained on over 10,000 hours of robotic manipulation data, whose intermediate version currently ranks first on the international RoboChallenge benchmark. GigaBrain-0.5M* further integrates world model-based reinforcement learning via RAMP (Reinforcement leArning via world Model-conditioned Policy) to enable robust cross-task adaptation. Empirical results demonstrate that RAMP achieves substantial performance gains over the RECAP baseline, yielding improvements of approximately 30\\% on challenging tasks including Laundry Folding, Box Packing, and Espresso Preparation. Critically, GigaBrain-0.5M^* exhibits reliable long-horizon execution, consistently accomplishing complex manipulation tasks without failure as validated by real-world deployment videos on our https://gigabrain05m.github.io{project page}.",
            "score": 27,
            "issue_id": 1036,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "e67a2c526ad1501a",
            "authors": [
                "GigaBrain Team",
                "Boyuan Wang",
                "Chaojun Ni",
                "Guan Huang",
                "Guosheng Zhao",
                "Hao Li",
                "Jie Li",
                "Jindi Lv",
                "Jingyu Liu",
                "Lv Feng",
                "Mingming Yu",
                "Peng Li",
                "Qiuping Deng",
                "Tianze Liu",
                "Xinyu Zhou",
                "Xinze Chen",
                "Xiaofeng Wang",
                "Yang Wang",
                "Yifan Li",
                "Yifei Nie",
                "Yilong Li",
                "Yukun Zhou",
                "Yun Ye",
                "Zhichao Liu",
                "Zheng Zhu"
            ],
            "affiliations": [
                "GigaAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.12099.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#rl",
                    "#cv",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞœĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ°Ğº Ğ¾ÑĞ½Ğ¾Ğ²Ğ° Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ GigaBrain-0.5M*, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Vision-Language-Action, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 10,000 Ñ‡Ğ°ÑĞ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ RAMP Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ°, Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ğ²ĞµĞ±-Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°Ñ…, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ 30% Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Enhancing Robotic Manipulation with World Model-Based Learning",
                    "desc": "This paper presents GigaBrain-0.5M*, a vision-language-action (VLA) model that enhances robotic manipulation through world model-based reinforcement learning. By leveraging video world models, the model improves scene understanding and future prediction, which are crucial for executing multi-step actions. The integration of RAMP (Reinforcement learning via world Model-conditioned Policy) allows for better adaptation across various tasks, leading to significant performance improvements. Empirical results show that GigaBrain-0.5M* outperforms previous models, achieving reliable long-horizon execution in complex tasks like Laundry Folding and Box Packing."
                },
                "zh": {
                    "title": "å¢å¼ºæœºå™¨äººæ“ä½œçš„æ™ºèƒ½æ¨¡å‹",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å¢å¼ºçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼Œç»“åˆäº†åŸºäºä¸–ç•Œæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ï¼Œä»¥æé«˜æœºå™¨äººæ“ä½œä»»åŠ¡çš„æ€§èƒ½å’Œé•¿æ—¶é—´æ‰§è¡Œèƒ½åŠ›ã€‚ä¼ ç»Ÿçš„VLAæ¨¡å‹åœ¨ç†è§£åœºæ™¯å’Œé¢„æµ‹æœªæ¥æ–¹é¢å­˜åœ¨å±€é™ï¼Œè€ŒåŸºäºè§†é¢‘çš„ä¸–ç•Œæ¨¡å‹åˆ™èƒ½æä¾›æ›´å¼ºçš„æ—¶ç©ºæ¨ç†å’Œå‡†ç¡®çš„æœªæ¥é¢„æµ‹ã€‚æˆ‘ä»¬æå‡ºçš„GigaBrain-0.5M*æ¨¡å‹åœ¨è¶…è¿‡10,000å°æ—¶çš„æœºå™¨äººæ“ä½œæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶é€šè¿‡RAMPæ–¹æ³•å®ç°äº†è·¨ä»»åŠ¡çš„å¼ºé€‚åº”æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGigaBrain-0.5M*åœ¨å¤æ‚ä»»åŠ¡ä¸Šç›¸è¾ƒäºåŸºçº¿æ¨¡å‹RECAPæœ‰çº¦30%çš„æ€§èƒ½æå‡ï¼Œä¸”åœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°å‡ºå¯é çš„é•¿æ—¶é—´æ‰§è¡Œèƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.12056",
            "title": "LawThinker: A Deep Research Legal Agent in Dynamic Environments",
            "url": "https://huggingface.co/papers/2602.12056",
            "abstract": "LawThinker is an autonomous legal research agent that uses an Explore-Verify-Memorize strategy with a DeepVerifier module to ensure accurate and procedurally compliant legal reasoning through dynamic verification of intermediate steps.  \t\t\t\t\tAI-generated summary \t\t\t\t Legal reasoning requires not only correct outcomes but also procedurally compliant reasoning processes. However, existing methods lack mechanisms to verify intermediate reasoning steps, allowing errors such as inapplicable statute citations to propagate undetected through the reasoning chain. To address this, we propose LawThinker, an autonomous legal research agent that adopts an Explore-Verify-Memorize strategy for dynamic judicial environments. The core idea is to enforce verification as an atomic operation after every knowledge exploration step. A DeepVerifier module examines each retrieval result along three dimensions of knowledge accuracy, fact-law relevance, and procedural compliance, with a memory module for cross-round knowledge reuse in long-horizon tasks. Experiments on the dynamic benchmark J1-EVAL show that LawThinker achieves a 24% improvement over direct reasoning and an 11% gain over workflow-based methods, with particularly strong improvements on process-oriented metrics. Evaluations on three static benchmarks further confirm its generalization capability. The code is available at https://github.com/yxy-919/LawThinker-agent .",
            "score": 25,
            "issue_id": 1036,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "a6e29ce78e7f4fd7",
            "authors": [
                "Xinyu Yang",
                "Chenlong Deng",
                "Tongyu Wen",
                "Binyu Xie",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.12056.jpg",
            "data": {
                "categories": [],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ®Ñ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "LawThinker â€” ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Explore-Verify-Memorize Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼ DeepVerifier Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ‚Ñ€Ñ‘Ğ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ°Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ. ĞœĞ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 24% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ÑĞ¼Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ½Ğ° 11% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Legal Research with Dynamic Verification",
                    "desc": "LawThinker is an advanced legal research tool that enhances legal reasoning by implementing an Explore-Verify-Memorize strategy. It features a DeepVerifier module that checks the accuracy and relevance of legal information at each step, preventing errors from propagating through the reasoning process. This approach ensures that not only the final legal outcomes are correct, but also that the reasoning follows proper legal procedures. Experiments demonstrate that LawThinker significantly outperforms existing methods in both dynamic and static legal environments, showcasing its effectiveness in maintaining procedural compliance."
                },
                "zh": {
                    "title": "LawThinkerï¼šåŠ¨æ€æ³•å¾‹æ¨ç†çš„æ–°çªç ´",
                    "desc": "LawThinker æ˜¯ä¸€ä¸ªè‡ªä¸»æ³•å¾‹ç ”ç©¶ä»£ç†ï¼Œé‡‡ç”¨æ¢ç´¢-éªŒè¯-è®°å¿†ç­–ç•¥ï¼Œç»“åˆ DeepVerifier æ¨¡å—ï¼Œç¡®ä¿æ³•å¾‹æ¨ç†çš„å‡†ç¡®æ€§å’Œç¨‹åºåˆè§„æ€§ã€‚è¯¥ç³»ç»Ÿé€šè¿‡åŠ¨æ€éªŒè¯ä¸­é—´æ­¥éª¤ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•ä¸­ç¼ºä¹éªŒè¯æœºåˆ¶çš„é—®é¢˜ï¼Œä»è€Œé¿å…é”™è¯¯åœ¨æ¨ç†é“¾ä¸­ä¼ æ’­ã€‚DeepVerifier æ¨¡å—ä»çŸ¥è¯†å‡†ç¡®æ€§ã€äº‹å®ä¸æ³•å¾‹çš„ç›¸å…³æ€§ä»¥åŠç¨‹åºåˆè§„æ€§ä¸‰ä¸ªç»´åº¦å¯¹æ¯ä¸ªæ£€ç´¢ç»“æœè¿›è¡Œæ£€æŸ¥ï¼Œå¹¶é€šè¿‡è®°å¿†æ¨¡å—åœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­é‡ç”¨çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLawThinker åœ¨åŠ¨æ€åŸºå‡† J1-EVAL ä¸Šæ¯”ç›´æ¥æ¨ç†æé«˜äº† 24%ï¼Œåœ¨åŸºäºå·¥ä½œæµçš„æ–¹æ³•ä¸Šæé«˜äº† 11%ï¼Œç‰¹åˆ«æ˜¯åœ¨è¿‡ç¨‹å¯¼å‘æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æ”¹å–„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.12280",
            "title": "Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching",
            "url": "https://huggingface.co/papers/2602.12280",
            "abstract": "Progressive Semantic Illusions use a generative framework with dual-branch Score Distillation Sampling to create vector sketches that transform semantically through sequential stroke additions, achieving superior recognizability and illusion strength.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, a novel vector sketching task where a single sketch undergoes a dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise, a generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the \"dual-constraint\": initial prefix strokes must form a coherent object (e.g., a duck) while simultaneously serving as the structural foundation for a second concept (e.g., a sheep) upon adding delta strokes. To address this, we propose a sequence-aware joint optimization framework driven by a dual-branch Score Distillation Sampling (SDS) mechanism. Unlike sequential approaches that freeze the initial state, our method dynamically adjusts prefix strokes to discover a \"common structural subspace\" valid for both targets. Furthermore, we introduce a novel Overlay Loss that enforces spatial complementarity, ensuring structural integration rather than occlusion. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines in recognizability and illusion strength, successfully expanding visual anagrams from the spatial to the temporal dimension. Project page: https://stroke-of-surprise.github.io/",
            "score": 18,
            "issue_id": 1037,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "856fd3554716f140",
            "authors": [
                "Huai-Hsun Cheng",
                "Siang-Ling Zhang",
                "Yu-Lun Liu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2602.12280.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞÑ‚ ÑƒÑ‚ĞºĞ¸ Ğº Ğ¾Ğ²Ñ†Ğµ: Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ»Ğ»ÑĞ·Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑˆÑ‚Ñ€Ğ¸Ñ…Ğ¸",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ»Ğ»ÑĞ·Ğ¸Ğ¹ â€” Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑÑĞºĞ¸Ğ·Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼ĞµĞ½ÑÑÑ‚ ÑĞ²Ğ¾Ñ‘ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑˆÑ‚Ñ€Ğ¸Ñ…Ğ¾Ğ². Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Stroke of Surprise, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ²ÑƒÑ…Ğ²ĞµÑ‚Ğ²ĞµĞ²Ğ¾Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Score Distillation Sampling (SDS) Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑˆÑ‚Ñ€Ğ¸Ñ…Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ´Ğ²Ğµ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ â€” Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑˆÑ‚Ñ€Ğ¸Ñ…Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°, Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ¸Ñ… Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ’Ğ²ĞµĞ´Ñ‘Ğ½Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Overlay Loss Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ¸Ğ· Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ."
                },
                "en": {
                    "title": "Transforming Sketches: From One Meaning to Another!",
                    "desc": "This paper presents Progressive Semantic Illusions, a new approach to vector sketching that allows a single sketch to change its meaning through the addition of strokes. The authors introduce a generative framework called Stroke of Surprise, which uses dual-branch Score Distillation Sampling to optimize the strokes for different semantic interpretations. A key challenge is to ensure that the initial strokes represent one object while also allowing for a transformation into another object with subsequent strokes. The proposed method includes a novel Overlay Loss to maintain structural integrity, leading to improved recognizability and illusion strength compared to existing methods."
                },
                "zh": {
                    "title": "æ¸è¿›è¯­ä¹‰å¹»è§‰ï¼šä»ç©ºé—´åˆ°æ—¶é—´çš„è§†è§‰è½¬å˜",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºæ¸è¿›è¯­ä¹‰å¹»è§‰çš„æ–°å‹çŸ¢é‡ç´ æä»»åŠ¡ï¼Œé€šè¿‡é€æ­¥æ·»åŠ ç¬”ç”»å®ç°å•ä¸€ç´ æçš„è¯­ä¹‰è½¬å˜ã€‚æˆ‘ä»¬å¼•å…¥äº†æƒŠå–œç¬”ç”»ç”Ÿæˆæ¡†æ¶ï¼Œä¼˜åŒ–çŸ¢é‡ç¬”ç”»ä»¥æ»¡è¶³ä¸åŒç»˜åˆ¶é˜¶æ®µçš„è¯­ä¹‰è§£é‡Šã€‚æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºåŒé‡çº¦æŸï¼šåˆå§‹ç¬”ç”»å¿…é¡»å½¢æˆä¸€ä¸ªè¿è´¯çš„å¯¹è±¡ï¼ŒåŒæ—¶ä¸ºæ·»åŠ æ–°ç¬”ç”»æä¾›ç»“æ„åŸºç¡€ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¯è¯†åˆ«æ€§å’Œå¹»è§‰å¼ºåº¦ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼ŒæˆåŠŸå°†è§†è§‰é”™è§‰ä»ç©ºé—´ç»´åº¦æ‰©å±•åˆ°æ—¶é—´ç»´åº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.12153",
            "title": "dVoting: Fast Voting for dLLMs",
            "url": "https://huggingface.co/papers/2602.12153",
            "abstract": "Diffusion large language models enable parallel token generation and efficient reasoning enhancement through a voting technique that identifies and refines uncertain predictions across multiple samples.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Large Language Models (dLLMs) represent a new paradigm beyond autoregressive modeling, offering competitive performance while naturally enabling a flexible decoding process. Specifically, dLLMs can generate tokens at arbitrary positions in parallel, endowing them with significant potential for parallel test-time scaling, which was previously constrained by severe inefficiency in autoregressive modeling. In this work, we introduce dVoting, a fast voting technique that boosts reasoning capability without training, with only an acceptable extra computational overhead. dVoting is motivated by the observation that, across multiple samples for the same prompt, token predictions remain largely consistent, whereas performance is determined by a small subset of tokens exhibiting cross-sample variability. Leveraging the arbitrary-position generation capability of dLLMs, dVoting performs iterative refinement by sampling, identifying uncertain tokens via consistency analysis, regenerating them through voting, and repeating this process until convergence. Extensive evaluations demonstrate that dVoting consistently improves performance across various benchmarks. It achieves gains of 6.22%-7.66% on GSM8K, 4.40%-7.20% on MATH500, 3.16%-14.84% on ARC-C, and 4.83%-5.74% on MMLU. Our code is available at https://github.com/fscdc/dVoting",
            "score": 15,
            "issue_id": 1036,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "d0c2a762d8c97f8c",
            "authors": [
                "Sicheng Feng",
                "Zigeng Chen",
                "Xinyin Ma",
                "Gongfan Fang",
                "Xinchao Wang"
            ],
            "affiliations": [
                "Department of Electrical and Computer Engineering, National University of Singapore, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.12153.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#diffusion",
                    "#open_source"
                ],
                "emoji": "ğŸ—³ï¸",
                "ru": {
                    "title": "ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (dLLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ‚Ñ…Ğ¾Ğ´ÑÑ‚ Ğ¾Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ÑÑ… Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ dVoting, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ: ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ½ĞµÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¿ĞµÑ€ĞµĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…: Ğ¾Ñ‚ 6-7% Ğ½Ğ° GSM8K Ğ´Ğ¾ 14% Ğ½Ğ° ARC-C."
                },
                "en": {
                    "title": "Boosting Language Model Reasoning with dVoting",
                    "desc": "This paper introduces Diffusion Large Language Models (dLLMs), which allow for generating tokens in parallel rather than sequentially, improving efficiency in language tasks. The authors present a technique called dVoting, which enhances reasoning capabilities by refining uncertain predictions through a voting mechanism across multiple samples. By analyzing consistency among token predictions, dVoting iteratively regenerates uncertain tokens until a stable output is achieved. The results show significant performance improvements on various benchmarks, demonstrating the effectiveness of this approach in enhancing model accuracy without additional training."
                },
                "zh": {
                    "title": "æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼šé«˜æ•ˆæ¨ç†çš„æ–°æ–¹æ³•",
                    "desc": "æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰æ˜¯ä¸€ç§æ–°çš„å»ºæ¨¡èŒƒå¼ï¼Œè¶…è¶Šäº†è‡ªå›å½’æ¨¡å‹ï¼Œèƒ½å¤Ÿå¹¶è¡Œç”Ÿæˆæ ‡è®°å¹¶æé«˜æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¿«é€ŸæŠ•ç¥¨æŠ€æœ¯dVotingï¼Œèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹æå‡æ¨ç†èƒ½åŠ›ï¼Œä»…éœ€å°‘é‡é¢å¤–è®¡ç®—å¼€é”€ã€‚dVotingé€šè¿‡åˆ†æå¤šä¸ªæ ·æœ¬ä¸­çš„ä¸€è‡´æ€§ï¼Œè¯†åˆ«ä¸ç¡®å®šçš„æ ‡è®°ï¼Œå¹¶é€šè¿‡æŠ•ç¥¨é‡æ–°ç”Ÿæˆè¿™äº›æ ‡è®°ï¼Œåå¤è¿›è¡Œç›´åˆ°æ”¶æ•›ã€‚å¤§é‡è¯„ä¼°è¡¨æ˜ï¼ŒdVotingåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡èƒ½æ˜¾è‘—æé«˜æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11748",
            "title": "Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning",
            "url": "https://huggingface.co/papers/2602.11748",
            "abstract": "Models require in-context exploration capabilities to scale effectively at test time, but autoregressive generation faces exponential decay in sampling long sequences, which is addressed by a length-incentivized exploration method that improves performance on both in-domain and out-of-domain tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Achieving effective test-time scaling requires models to engage in In-Context Exploration -- the intrinsic ability to generate, verify, and refine multiple reasoning hypotheses within a single continuous context.   Grounded in State Coverage theory, our analysis identifies a critical bottleneck to enabling this capability: while broader state coverage requires longer reasoning trajectories, the probability of sampling such sequences decays exponentially during autoregressive generation, a phenomenon we term the ``Shallow Exploration Trap''.   To bridge this gap, we propose Length-Incentivized Exploration(\\method).   This simple yet effective recipe explicitly encourages models to explore more via a length-based reward coupled with a redundancy penalty, thereby maximizing state coverage in two-step manner.   Comprehensive experiments across different models (Qwen3, Llama) demonstrate that \\method effectively incentivize in-context exploration.   As a result, our method achieves an average improvement of 4.4\\% on in-domain tasks and a 2.7\\% gain on out-of-domain benchmarks.",
            "score": 13,
            "issue_id": 1038,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "7c0be7c7f4b87e17",
            "authors": [
                "Futing Wang",
                "Jianhao Yan",
                "Yun Luo",
                "Ganqu Cui",
                "Zhi Wang",
                "Xiaoye Qu",
                "Yue Zhang",
                "Yu Cheng",
                "Tao Lin"
            ],
            "affiliations": [
                "Institute of Advanced Technology, Westlake Institute for Advanced Study",
                "Nanjing University",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11748.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸĞ¾Ğ¾Ñ‰Ñ€ĞµĞ½Ğ¸Ğµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Length-Incentivized Exploration, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñƒ Ğ·Ğ° Ğ´Ğ»Ğ¸Ğ½Ñƒ Ñ ÑˆÑ‚Ñ€Ğ°Ñ„Ğ¾Ğ¼ Ğ·Ğ° Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ in-context Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 4,4% Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ° 2,7% Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking Long-Sequence Potential with Length-Incentivized Exploration",
                    "desc": "This paper addresses the challenge of improving the performance of autoregressive models during test-time by introducing a method called Length-Incentivized Exploration. The authors identify a problem known as the 'Shallow Exploration Trap', where the likelihood of generating longer sequences decreases exponentially, hindering effective in-context exploration. To overcome this, they propose a reward system that encourages longer reasoning paths while penalizing redundancy, thus enhancing state coverage. Experimental results show that this approach leads to significant performance gains on both in-domain and out-of-domain tasks across various models."
                },
                "zh": {
                    "title": "æ¿€åŠ±æ¢ç´¢ï¼Œæå‡æ¨¡å‹æ€§èƒ½ï¼",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†æ¨¡å‹åœ¨æµ‹è¯•æ—¶éœ€è¦å…·å¤‡ä¸Šä¸‹æ–‡æ¢ç´¢èƒ½åŠ›ï¼Œä»¥æœ‰æ•ˆæ‰©å±•å…¶æ€§èƒ½ã€‚è‡ªå›å½’ç”Ÿæˆåœ¨é‡‡æ ·é•¿åºåˆ—æ—¶é¢ä¸´æŒ‡æ•°è¡°å‡çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é•¿åº¦æ¿€åŠ±æ¢ç´¢æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡é•¿åº¦å¥–åŠ±å’Œå†—ä½™æƒ©ç½šï¼Œé¼“åŠ±æ¨¡å‹è¿›è¡Œæ›´å¤šæ¢ç´¢ï¼Œä»è€Œæœ€å¤§åŒ–çŠ¶æ€è¦†ç›–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹ä¸Šå‡èƒ½æœ‰æ•ˆæå‡åœ¨åŸŸå†…å’ŒåŸŸå¤–ä»»åŠ¡çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.05548",
            "title": "Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation",
            "url": "https://huggingface.co/papers/2602.05548",
            "abstract": "Asymmetric Group Relative Advantage Estimation addresses exploration and difficulty adaptation challenges in reinforcement learning with large language models by dynamically modulating exploration incentives and sample difficulty focus.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR), particularly GRPO, has become the standard for eliciting LLM reasoning. However, its efficiency in exploration and difficulty adaptation remains an open challenge. In this work, we argue that these bottlenecks stem from an implicit advantage symmetry inherent in Group Relative Advantage Estimation (GRAE). This symmetry induces two critical limitations: (i) at the group level, strict symmetry in weights between correct and incorrect trajectories leaves unsampled action logits unchanged, thereby hindering exploration of novel correct solution. (ii) at the sample level, the algorithm implicitly prioritizes medium-difficulty samples, remaining agnostic to the non-stationary demands of difficulty focus. Through controlled experiments, we reveal that this symmetric property is sub-optimal, yielding two pivotal insights: (i) asymmetrically suppressing the advantages of correct trajectories encourages essential exploration. (ii) learning efficiency is maximized by a curriculum-like transition-prioritizing simpler samples initially before gradually shifting to complex ones. Motivated by these findings, we propose Asymmetric GRAE (A-GRAE), which dynamically modulates exploration incentives and sample-difficulty focus. Experiments across seven benchmarks demonstrate that A-GRAE consistently improves GRPO and its variants across both LLMs and MLLMs.",
            "score": 10,
            "issue_id": 1036,
            "pub_date": "2026-02-05",
            "pub_date_card": {
                "ru": "5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 5",
                "zh": "2æœˆ5æ—¥"
            },
            "hash": "f994353853a9d401",
            "authors": [
                "Zhiqi Yu",
                "Zhangquan Chen",
                "Mengting Liu",
                "Heye Zhang",
                "Liangqiong Qu"
            ],
            "affiliations": [
                "Sun Yat-sen University",
                "Tsinghua University",
                "University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.05548.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#rlhf",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "ĞÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾: Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ exploration Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ LLM",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Asymmetric GRAE Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Group Relative Advantage Estimation (GRAE) Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²ĞµÑĞ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ½ĞµĞ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ½Ğ° Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ exploration Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ curriculum learning Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑĞµĞ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ A-GRAE Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ GRPO Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Enhancing Exploration and Learning with Asymmetric Advantage Estimation",
                    "desc": "This paper introduces Asymmetric Group Relative Advantage Estimation (A-GRAE) to tackle challenges in reinforcement learning, particularly in large language models (LLMs). The authors identify that the existing Group Relative Advantage Estimation (GRAE) suffers from a symmetry that limits exploration and adaptation to varying sample difficulties. By asymmetrically adjusting the advantages of correct trajectories, A-GRAE promotes better exploration of novel solutions. Additionally, it employs a curriculum learning approach, starting with simpler samples and gradually increasing complexity, which enhances learning efficiency across multiple benchmarks."
                },
                "zh": {
                    "title": "åŠ¨æ€è°ƒèŠ‚æ¢ç´¢ä¸éš¾åº¦çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸å¯¹ç§°ç»„ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡ï¼ˆA-GRAEï¼‰ï¼Œæ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­å¤§è¯­è¨€æ¨¡å‹çš„æ¢ç´¢å’Œéš¾åº¦é€‚åº”é—®é¢˜ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¼ ç»Ÿçš„ç»„ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡ï¼ˆGRAEï¼‰å­˜åœ¨éšå«çš„ä¼˜åŠ¿å¯¹ç§°æ€§ï¼Œå¯¼è‡´æ¢ç´¢æ–°è§£çš„èƒ½åŠ›å—é™ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†ä¸å¯¹ç§°æŠ‘åˆ¶æ­£ç¡®è½¨è¿¹çš„ä¼˜åŠ¿å¯ä»¥ä¿ƒè¿›å¿…è¦çš„æ¢ç´¢ï¼ŒåŒæ—¶é€šè¿‡é€æ­¥è¿‡æ¸¡åˆ°å¤æ‚æ ·æœ¬æ¥æœ€å¤§åŒ–å­¦ä¹ æ•ˆç‡ã€‚æœ€ç»ˆï¼ŒA-GRAEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æå‡äº†GRPOåŠå…¶å˜ä½“çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.12092",
            "title": "DeepSight: An All-in-One LM Safety Toolkit",
            "url": "https://huggingface.co/papers/2602.12092",
            "abstract": "DeepSight is an open-source project that integrates safety evaluation and diagnosis for large language and multimodal models, enabling white-box insights through unified protocols and specialized toolkits.  \t\t\t\t\tAI-generated summary \t\t\t\t As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluation can only locate external behavioral risks but cannot figure out internal root causes. Meanwhile, safety diagnosis often drifts from concrete risk scenarios and remains at the explainable level. In this way, safety alignment lack dedicated explanations of changes in internal mechanisms, potentially degrading general capabilities. To systematically address these issues, we propose an open-source project, namely DeepSight, to practice a new safety evaluation-diagnosis integrated paradigm. DeepSight is low-cost, reproducible, efficient, and highly scalable large-scale model safety evaluation project consisting of a evaluation toolkit DeepSafe and a diagnosis toolkit DeepScan. By unifying task and data protocols, we build a connection between the two stages and transform safety evaluation from black-box to white-box insight. Besides, DeepSight is the first open source toolkit that support the frontier AI risk evaluation and joint safety evaluation and diagnosis.",
            "score": 8,
            "issue_id": 1038,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "ba47578de85a4387",
            "authors": [
                "Bo Zhang",
                "Jiaxuan Guo",
                "Lijun Li",
                "Dongrui Liu",
                "Sujin Chen",
                "Guanxu Chen",
                "Zhijie Zheng",
                "Qihao Lin",
                "Lewen Yan",
                "Chen Qian",
                "Yijin Zhou",
                "Yuyao Wu",
                "Shaoxiong Guo",
                "Tianyi Du",
                "Jingyi Yang",
                "Xuhao Hu",
                "Ziqi Miao",
                "Xiaoya Lu",
                "Jing Shao",
                "Xia Hu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2602.12092.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#interpretability",
                    "#security",
                    "#benchmark",
                    "#alignment",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞÑ‚ Ñ‡Ñ‘Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‰Ğ¸ĞºĞ° Ğº Ğ±ĞµĞ»Ğ¾Ğ¼Ñƒ: Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "DeepSight â€” ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ²Ğµ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸: Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· toolkit DeepSafe Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºÑƒ Ñ‡ĞµÑ€ĞµĞ· toolkit DeepScan, ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ñ Ğ¸Ñ… ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‡Ñ‘Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‰Ğ¸ĞºĞ°, DeepSight Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±ĞµĞ»Ñ‹Ğ¹ ÑÑ‰Ğ¸Ğº Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ¸ÑĞºĞ¸, Ğ½Ğ¾ Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ. ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ, Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ, ÑÑ‚Ğ°Ğ² Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "DeepSight: Unifying Safety Evaluation and Diagnosis for AI Models",
                    "desc": "DeepSight is an innovative open-source project designed to enhance the safety evaluation and diagnosis of large language models (LLMs) and multimodal models (MLLMs). It addresses the limitations of current safety workflows by integrating evaluation and diagnosis into a unified framework, allowing for better insights into both external risks and internal mechanisms. The project features two main toolkits: DeepSafe for safety evaluation and DeepScan for diagnosis, which work together to provide a comprehensive understanding of model safety. By transforming the safety evaluation process from a black-box approach to a white-box insight, DeepSight aims to improve the alignment and overall capabilities of large models."
                },
                "zh": {
                    "title": "DeepSightï¼šå®‰å…¨è¯„ä¼°ä¸è¯Šæ–­çš„æ•´åˆæ–°èŒƒå¼",
                    "desc": "DeepSightæ˜¯ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œæ—¨åœ¨ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€æ¨¡å‹æä¾›å®‰å…¨è¯„ä¼°å’Œè¯Šæ–­çš„æ•´åˆè§£å†³æ–¹æ¡ˆã€‚è¯¥é¡¹ç›®é€šè¿‡ç»Ÿä¸€çš„åè®®å’Œä¸“é—¨çš„å·¥å…·åŒ…ï¼Œå®ç°äº†ä»é»‘ç®±åˆ°ç™½ç®±çš„å®‰å…¨è¯„ä¼°è½¬å˜ã€‚DeepSightåŒ…å«ä¸¤ä¸ªä¸»è¦å·¥å…·åŒ…ï¼šDeepSafeç”¨äºå®‰å…¨è¯„ä¼°ï¼ŒDeepScanç”¨äºå®‰å…¨è¯Šæ–­ï¼Œèƒ½å¤Ÿé«˜æ•ˆã€ä½æˆæœ¬åœ°è¿›è¡Œå¤§è§„æ¨¡æ¨¡å‹çš„å®‰å…¨è¯„ä¼°ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒDeepSightè§£å†³äº†å½“å‰å®‰å…¨è¯„ä¼°å’Œè¯Šæ–­ä¸­å­˜åœ¨çš„åˆ†ç¦»é—®é¢˜ï¼Œæå‡äº†æ¨¡å‹çš„å®‰å…¨æ€§å’Œå¯è§£é‡Šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09070",
            "title": "NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control",
            "url": "https://huggingface.co/papers/2602.09070",
            "abstract": "NarraScore presents a hierarchical framework that uses frozen Vision-Language Models as affective sensors to generate coherent soundtracks for long-form videos by combining global semantic anchors with token-level adaptive modulation.  \t\t\t\t\tAI-generated summary \t\t\t\t Synthesizing coherent soundtracks for long-form videos remains a formidable challenge, currently stalled by three critical impediments: computational scalability, temporal coherence, and, most critically, a pervasive semantic blindness to evolving narrative logic. To bridge these gaps, we propose NarraScore, a hierarchical framework predicated on the core insight that emotion serves as a high-density compression of narrative logic. Uniquely, we repurpose frozen Vision-Language Models (VLMs) as continuous affective sensors, distilling high-dimensional visual streams into dense, narrative-aware Valence-Arousal trajectories. Mechanistically, NarraScore employs a Dual-Branch Injection strategy to reconcile global structure with local dynamism: a Global Semantic Anchor ensures stylistic stability, while a surgical Token-Level Affective Adapter modulates local tension via direct element-wise residual injection. This minimalist design bypasses the bottlenecks of dense attention and architectural cloning, effectively mitigating the overfitting risks associated with data scarcity. Experiments demonstrate that NarraScore achieves state-of-the-art consistency and narrative alignment with negligible computational overhead, establishing a fully autonomous paradigm for long-video soundtrack generation.",
            "score": 8,
            "issue_id": 1037,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "300846e1ac31a815",
            "authors": [
                "Yufan Wen",
                "Zhaocheng Liu",
                "YeGuo Hua",
                "Ziyi Guo",
                "Lihua Zhang",
                "Chun Yuan",
                "Jian Wu"
            ],
            "affiliations": [
                "ByteDance Beijing, China",
                "ByteDance Shenzhen, China",
                "Tsinghua University Shenzhen, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09070.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#long_context",
                    "#audio",
                    "#video",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ­Ğ¼Ğ¾Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑ Ğ¿Ğ¾Ğ²ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: ÑƒĞ¼Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "NarraScore â€” ÑÑ‚Ğ¾ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ñ‚Ğ¸Ñ‚ÑƒÑ€ Ğº Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Vision-Language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ´Ğ°Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¸ ÑĞ»ÑƒĞ¶Ğ°Ñ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ğ¿Ğ¾Ğ²ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ²ĞµÑ‚Ğ²ĞµĞ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ: Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞºĞ¾Ñ€ÑŒ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸, Ñ€ĞµÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ñ‹ÑĞ»Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "NarraScore: Emotion-Driven Soundtrack Generation for Long Videos",
                    "desc": "NarraScore is a new framework designed to create soundtracks for long videos by understanding the emotions in the narrative. It uses frozen Vision-Language Models as sensors to capture the emotional flow of the video, translating visual information into emotional trajectories. The framework combines a stable global structure with dynamic local adjustments to ensure the soundtrack matches the evolving story. This approach not only improves coherence and alignment with the narrative but also reduces the computational demands typically associated with such tasks."
                },
                "zh": {
                    "title": "æƒ…æ„Ÿé©±åŠ¨çš„é•¿è§†é¢‘é…ä¹ç”Ÿæˆæ–°èŒƒå¼",
                    "desc": "NarraScoreæ˜¯ä¸€ä¸ªå±‚æ¬¡åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨å†»ç»“çš„è§†è§‰-è¯­è¨€æ¨¡å‹ä½œä¸ºæƒ…æ„Ÿä¼ æ„Ÿå™¨ï¼Œä¸ºé•¿è§†é¢‘ç”Ÿæˆè¿è´¯çš„é…ä¹ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆå…¨å±€è¯­ä¹‰é”šç‚¹å’ŒåŸºäºæ ‡è®°çš„è‡ªé€‚åº”è°ƒåˆ¶ï¼Œè§£å†³äº†è®¡ç®—å¯æ‰©å±•æ€§ã€æ—¶é—´ä¸€è‡´æ€§å’Œå™äº‹é€»è¾‘çš„è¯­ä¹‰ç›²ç‚¹ç­‰æŒ‘æˆ˜ã€‚NarraScoreé‡‡ç”¨åŒåˆ†æ”¯æ³¨å…¥ç­–ç•¥ï¼Œç¡®ä¿å…¨å±€ç»“æ„çš„ç¨³å®šæ€§ï¼ŒåŒæ—¶é€šè¿‡å±€éƒ¨è°ƒèŠ‚å¢å¼ºå™äº‹çš„åŠ¨æ€æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒNarraScoreåœ¨è®¡ç®—å¼€é”€æå°çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„ä¸€è‡´æ€§å’Œå™äº‹å¯¹é½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11731",
            "title": "Thinking with Drafting: Optical Decompression via Logical Reconstruction",
            "url": "https://huggingface.co/papers/2602.11731",
            "abstract": "Visual reasoning is enhanced by reconstructing logical structures from compressed visual tokens through a DSL-based approach that generates deterministic visual proofs for verification.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing multimodal large language models have achieved high-fidelity visual perception and exploratory visual generation. However, a precision paradox persists in complex reasoning tasks: optical perception systems transcribe symbols without capturing logical topology, while pixel-based generative models produce visual artifacts lacking mathematical exactness. To bridge this gap, we propose that reasoning over visual inputs be reconceptualized as optical decompression-the process of reconstructing latent logical structures from compressed visual tokens. Guided by the axiom that Parsing is Reasoning, we introduce Thinking with Drafting (TwD), which utilizes a minimalist Domain-Specific Language (DSL) as a grounding intermediate representation. Unlike standard approaches that hallucinate answers directly, TwD forces the model to draft its mental model into executable code, rendering deterministic visual proofs for self-verification. To validate this, we present VisAlg, a visual algebra benchmark. Experiments demonstrate that TwD serve as a superior cognitive scaffold. Our work establishes a closed-loop system where visual generation acts not as a creative output but as a logical verifier, offering a generalizable path for visual reasoning.",
            "score": 5,
            "issue_id": 1040,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "1df4199856d15b9f",
            "authors": [
                "Jingxuan Wei",
                "Honghao He",
                "Caijun Jia",
                "Siyuan Li",
                "Zheng Sun",
                "Yuhang Xu",
                "Yuanyuan Lin",
                "Linzhuang Sun",
                "Yuchen Wu",
                "Bihui Yu",
                "Xiangxiang Zhang",
                "Cheng Tan"
            ],
            "affiliations": [
                "ByteDance",
                "Shenyang Institute of Computing Technology, Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences",
                "Westlake University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11731.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#cv",
                    "#interpretability",
                    "#architecture",
                    "#reasoning"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ñƒ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ´",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ğ¸ - Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ¸Ğ· ÑĞ¶Ğ°Ñ‚Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Thinking with Drafting (TwD), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº (DSL) ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞºĞ¾Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ, Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ´, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VisAlg Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ TwD Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ³Ğ´Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Revolutionizing Visual Reasoning with Structured Drafting",
                    "desc": "This paper addresses the challenges in visual reasoning by proposing a new method called Thinking with Drafting (TwD). TwD uses a Domain-Specific Language (DSL) to create a structured representation of visual information, allowing for clearer logical reasoning. Instead of generating answers directly, the model drafts its understanding into executable code, which helps in verifying the accuracy of visual outputs. The authors introduce a benchmark called VisAlg to demonstrate that this approach enhances cognitive processing in visual tasks, establishing a more reliable system for visual reasoning."
                },
                "zh": {
                    "title": "è§†è§‰æ¨ç†çš„æ–°æ–¹æ³•ï¼šæ€ç»´ä¸è‰æ‹Ÿ",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰æ¨ç†æ–¹æ³•ï¼Œé€šè¿‡å‹ç¼©çš„è§†è§‰ç¬¦å·é‡å»ºé€»è¾‘ç»“æ„ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åä¸ºâ€œæ€ç»´ä¸è‰æ‹Ÿâ€ï¼ˆTwDï¼‰çš„æ¡†æ¶ï¼Œåˆ©ç”¨ç‰¹å®šé¢†åŸŸè¯­è¨€ï¼ˆDSLï¼‰ä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œå¸®åŠ©æ¨¡å‹ç”Ÿæˆå¯æ‰§è¡Œä»£ç ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒTwD å¼ºè°ƒæ¨¡å‹åœ¨ç”Ÿæˆç­”æ¡ˆä¹‹å‰å…ˆè‰æ‹Ÿå…¶æ€ç»´æ¨¡å‹ï¼Œä»è€Œæä¾›ç¡®å®šæ€§çš„è§†è§‰è¯æ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTwD åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå»ºç«‹äº†ä¸€ä¸ªé—­ç¯ç³»ç»Ÿï¼Œä½¿è§†è§‰ç”Ÿæˆä¸ä»…æ˜¯åˆ›é€ æ€§è¾“å‡ºï¼Œæ›´æ˜¯é€»è¾‘éªŒè¯çš„å·¥å…·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.12205",
            "title": "DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing",
            "url": "https://huggingface.co/papers/2602.12205",
            "abstract": "A lightweight 5B unified multimodal model achieves competitive performance through hierarchical feature extraction, learnable think tokens, and progressive training strategies including alignment pre-training, joint supervised fine-tuning, and reinforcement learning with MR-GRPO.  \t\t\t\t\tAI-generated summary \t\t\t\t Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable 'think tokens' to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research.",
            "score": 3,
            "issue_id": 1038,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "ed2c8c050b444dae",
            "authors": [
                "Dianyi Wang",
                "Ruihang Li",
                "Feng Han",
                "Chaofan Ma",
                "Wei Song",
                "Siyuan Wang",
                "Yibin Wang",
                "Yi Xin",
                "Hongjian Liu",
                "Zhixiong Zhang",
                "Shengyuan Ding",
                "Tianhang Wang",
                "Zhenglin Cheng",
                "Tao Lin",
                "Cheng Jin",
                "Kaicheng Yu",
                "Jingjing Chen",
                "Wenjie Wang",
                "Zhongyu Wei",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "Fudan University",
                "Nanjing University",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "University of Science and Technology of China",
                "University of Southern California",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.12205.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#rlhf",
                    "#training",
                    "#alignment",
                    "#open_source",
                    "#small_models",
                    "#dataset",
                    "#optimization"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼Ğ¾Ñ‰ÑŒÑ: 5B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¾Ğ±Ğ³Ğ¾Ğ½ÑÑÑ‚ 80B Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ",
                    "desc": "DeepGen 1.0 â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ 5-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Stacked Channel Bridging Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹èåˆ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ»Ğ¾Ñ‘Ğ² Vision Language Model Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². Ğ¢Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚, ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ MR-GRPO Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ° 50 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¸ data-centric Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼."
                },
                "en": {
                    "title": "DeepGen 1.0: Lightweight Powerhouse in Multimodal AI",
                    "desc": "This paper introduces DeepGen 1.0, a lightweight unified multimodal model with only 5 billion parameters that competes with larger models in image generation and editing. It employs hierarchical feature extraction through Stacked Channel Bridging (SCB) and utilizes learnable 'think tokens' to enhance semantic understanding and control. The training strategy includes alignment pre-training, joint supervised fine-tuning, and reinforcement learning, which collectively improve generation quality and alignment with human preferences. Despite being trained on a smaller dataset, DeepGen 1.0 outperforms larger models on various benchmarks, making it a valuable resource for multimodal research."
                },
                "zh": {
                    "title": "è½»é‡çº§å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ€§èƒ½è¶…è¶Šæ›´å¤§æ¨¡å‹ï¼",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è½»é‡çº§çš„5Bç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹DeepGen 1.0ï¼Œè¯¥æ¨¡å‹é€šè¿‡åˆ†å±‚ç‰¹å¾æå–ã€å¯å­¦ä¹ çš„æ€ç»´æ ‡è®°å’Œæ¸è¿›å¼è®­ç»ƒç­–ç•¥å®ç°äº†ç«äº‰åŠ›çš„æ€§èƒ½ã€‚ä¸ºäº†å…‹æœç´§å‡‘æ¨¡å‹åœ¨è¯­ä¹‰ç†è§£å’Œç»†ç²’åº¦æ§åˆ¶æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†å †å é€šé“æ¡¥æ¥ï¼ˆSCBï¼‰æ¡†æ¶ï¼Œèƒ½å¤Ÿä»å¤šä¸ªè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å±‚æå–ç‰¹å¾å¹¶ä¸æ€ç»´æ ‡è®°èåˆã€‚è®­ç»ƒç­–ç•¥åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šå¤§è§„æ¨¡å›¾åƒ-æ–‡æœ¬å¯¹çš„å¯¹é½é¢„è®­ç»ƒã€è”åˆç›‘ç£å¾®è°ƒä»¥åŠåŸºäºMR-GRPOçš„å¼ºåŒ–å­¦ä¹ ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆè´¨é‡å’Œä¸äººç±»åå¥½çš„å¯¹é½ã€‚å°½ç®¡åªä½¿ç”¨äº†çº¦5000ä¸‡æ ·æœ¬ï¼ŒDeepGen 1.0åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†æ›´å¤§æ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11683",
            "title": "ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces",
            "url": "https://huggingface.co/papers/2602.11683",
            "abstract": "ThinkRouter is a confidence-aware routing mechanism that improves reasoning efficiency by switching between discrete token and latent spaces based on model confidence, achieving better accuracy and faster generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent work explores latent reasoning to improve reasoning efficiency by replacing explicit reasoning trajectories with continuous representations in a latent space, yet its effectiveness varies across settings. Analysis of model confidence dynamics under latent reasoning reveals that thinking trajectories ending in incorrect answers contain fewer low-confidence steps than those ending in correct answers. Meanwhile, we suggest that soft embeddings aggregated by multiple low-confidence thinking alternatives may introduce and propagate noise, leading to high confidence in unreliable reasoning trajectories. Motivated by these observations, ThinkRouter, an inference-time confidence-aware routing mechanism is proposed to avoid high confidence and noise for efficient reasoning. ThinkRouter routes thinking to the discrete token space when model confidence is low, and to the latent space otherwise. Extensive experiments on STEM reasoning and coding benchmarks across diverse large reasoning models demonstrate that ThinkRouter outperforms explicit CoT, random routing, and latent reasoning baselines in terms of accuracy, achieving an average improvement of 19.70 points in Pass@1, while reducing generation length by up to 15.55%. Further comprehensive analysis reveals that ThinkRouter can calibrate errors arising from explicit CoT and latent reasoning, and accelerates end-of-thinking token generation by globally lowering model confidence.",
            "score": 3,
            "issue_id": 1036,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "a3099ad23b1e1bfb",
            "authors": [
                "Xin Xu",
                "Tong Yu",
                "Xiang Chen",
                "Haoliang Wang",
                "Julian McAuley",
                "Saayan Mitra"
            ],
            "affiliations": [
                "Adobe Research",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11683.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "ğŸ›£ï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ²Ğ½Ñ‹Ğ¼ Ğ¸ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "ThinkRouter â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ¼ĞµĞ½ÑŒÑˆĞµ ÑˆĞ°Ğ³Ğ¾Ğ² Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ° Ğ¼ÑĞ³ĞºĞ¸Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑˆÑƒĞ¼ Ğ¸ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¾Ğ¶Ğ½ÑƒÑ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… STEM-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 19.70 Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ² Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ Pass@1 Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° 15.55%."
                },
                "en": {
                    "title": "ThinkRouter: Smart Routing for Confident Reasoning",
                    "desc": "ThinkRouter is a novel routing mechanism that enhances reasoning efficiency in machine learning models by adapting to the model's confidence levels. It intelligently switches between discrete token spaces and latent spaces, depending on whether the model is confident or not, which leads to improved accuracy and faster generation times. The mechanism addresses the issue of noise introduced by low-confidence reasoning paths, ensuring that the model avoids high confidence in unreliable outputs. Through extensive testing, ThinkRouter has shown significant improvements in performance metrics, particularly in STEM reasoning and coding tasks, outperforming existing methods."
                },
                "zh": {
                    "title": "ThinkRouterï¼šæå‡æ¨ç†æ•ˆç‡çš„ä¿¡å¿ƒæ„ŸçŸ¥è·¯ç”±æœºåˆ¶",
                    "desc": "ThinkRouteræ˜¯ä¸€ç§åŸºäºä¿¡å¿ƒçš„è·¯ç”±æœºåˆ¶ï¼Œé€šè¿‡æ ¹æ®æ¨¡å‹ä¿¡å¿ƒåœ¨ç¦»æ•£æ ‡è®°ç©ºé—´å’Œæ½œåœ¨ç©ºé—´ä¹‹é—´åˆ‡æ¢ï¼Œæé«˜æ¨ç†æ•ˆç‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé”™è¯¯ç­”æ¡ˆçš„æ€ç»´è½¨è¿¹ä¸­ä½ä¿¡å¿ƒæ­¥éª¤è¾ƒå°‘ï¼Œè€Œä½ä¿¡å¿ƒçš„è½¯åµŒå…¥å¯èƒ½å¼•å…¥å™ªå£°ï¼Œå¯¼è‡´ä¸å¯é çš„æ¨ç†è½¨è¿¹äº§ç”Ÿé«˜ä¿¡å¿ƒã€‚ThinkRouteråœ¨æ¨¡å‹ä¿¡å¿ƒä½æ—¶å°†æ¨ç†è·¯ç”±åˆ°ç¦»æ•£æ ‡è®°ç©ºé—´ï¼Œè€Œåœ¨ä¿¡å¿ƒé«˜æ—¶åˆ™è·¯ç”±åˆ°æ½œåœ¨ç©ºé—´ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒThinkRouteråœ¨STEMæ¨ç†å’Œç¼–ç åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå‡†ç¡®ç‡å¹³å‡æé«˜äº†19.70ä¸ªç™¾åˆ†ç‚¹ï¼ŒåŒæ—¶ç”Ÿæˆé•¿åº¦å‡å°‘äº†15.55%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11541",
            "title": "Budget-Constrained Agentic Large Language Models: Intention-Based Planning for Costly Tool Use",
            "url": "https://huggingface.co/papers/2602.11541",
            "abstract": "Budget-constrained tool-augmented agents use a hierarchical world model and intent-aware planning to optimize multi-step task completion under monetary constraints.  \t\t\t\t\tAI-generated summary \t\t\t\t We study budget-constrained tool-augmented agents, where a large language model must solve multi-step tasks by invoking external tools under a strict monetary budget. We formalize this setting as sequential decision making in context space with priced and stochastic tool executions, making direct planning intractable due to massive state-action spaces, high variance of outcomes and prohibitive exploration cost. To address these challenges, we propose INTENT, an inference-time planning framework that leverages an intention-aware hierarchical world model to anticipate future tool usage, risk-calibrated cost, and guide decisions online. Across cost-augmented StableToolBench, INTENT strictly enforces hard budget feasibility while substantially improving task success over baselines, and remains robust under dynamic market shifts such as tool price changes and varying budgets.",
            "score": 3,
            "issue_id": 1036,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "afd1ba3da2e52d63",
            "authors": [
                "Hanbing Liu",
                "Chunhao Tian",
                "Nan An",
                "Ziyuan Wang",
                "Pinyan Lu",
                "Changyuan Yu",
                "Qi Qi"
            ],
            "affiliations": [
                "Baidu Inc.",
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Shanghai University of Finance and Economics"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11541.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ’°",
                "ru": {
                    "title": "Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ INTENT Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ½Ğ¾Ğ¼ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ ÑĞ¾ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ»Ğ°Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ°Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½ĞµÑ€Ğ°Ğ·Ñ€ĞµÑˆĞ¸Ğ¼Ñ‹Ğ¼. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ°, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ Ñ€Ğ¸ÑĞºĞ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº ĞºĞ¾Ğ»ĞµĞ±Ğ°Ğ½Ğ¸Ñ Ñ†ĞµĞ½ Ğ½Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒÑÑ‰Ğ¸ĞµÑÑ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ñ‹."
                },
                "en": {
                    "title": "Optimizing Task Success Under Budget Constraints with INTENT",
                    "desc": "This paper introduces a framework for budget-constrained tool-augmented agents that need to complete multi-step tasks while adhering to a strict monetary budget. The authors formalize the problem as a sequential decision-making challenge, where traditional planning methods are impractical due to the complexity of state-action spaces and the unpredictability of tool outcomes. To overcome these obstacles, they propose INTENT, which utilizes a hierarchical world model that is aware of the agent's intentions, allowing for better anticipation of tool usage and cost management. The results show that INTENT not only maintains budget constraints but also enhances task success rates, even in fluctuating market conditions."
                },
                "zh": {
                    "title": "é¢„ç®—å—é™çš„æ™ºèƒ½å†³ç­–ä¸å·¥å…·ä¼˜åŒ–",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†é¢„ç®—å—é™çš„å·¥å…·å¢å¼ºä»£ç†ï¼Œåˆ©ç”¨åˆ†å±‚ä¸–ç•Œæ¨¡å‹å’Œæ„å›¾æ„ŸçŸ¥è§„åˆ’æ¥ä¼˜åŒ–åœ¨é‡‘é’±é™åˆ¶ä¸‹çš„å¤šæ­¥éª¤ä»»åŠ¡å®Œæˆã€‚æˆ‘ä»¬å°†è¿™ä¸€è®¾ç½®å½¢å¼åŒ–ä¸ºä¸Šä¸‹æ–‡ç©ºé—´ä¸­çš„åºåˆ—å†³ç­–ï¼Œæ¶‰åŠæœ‰ä»·æ ¼å’Œéšæœºæ€§çš„å·¥å…·æ‰§è¡Œã€‚ç”±äºçŠ¶æ€-åŠ¨ä½œç©ºé—´åºå¤§ã€ç»“æœçš„é«˜æ–¹å·®å’Œæ¢ç´¢æˆæœ¬é«˜ï¼Œç›´æ¥è§„åˆ’å˜å¾—ä¸å¯è¡Œã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†INTENTæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨æ„å›¾æ„ŸçŸ¥çš„åˆ†å±‚ä¸–ç•Œæ¨¡å‹æ¥é¢„æµ‹æœªæ¥çš„å·¥å…·ä½¿ç”¨å’Œé£é™©æ ¡å‡†æˆæœ¬ï¼Œä»è€Œåœ¨çº¿æŒ‡å¯¼å†³ç­–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11337",
            "title": "MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation",
            "url": "https://huggingface.co/papers/2602.11337",
            "abstract": "MolmoSpaces presents an open ecosystem with diverse indoor environments and annotated objects for large-scale robot policy benchmarking across multiple tasks and simulators.  \t\t\t\t\tAI-generated summary \t\t\t\t Deploying robots at scale demands robustness to the long tail of everyday situations. The countless variations in scene layout, object geometry, and task specifications that characterize real environments are vast and underrepresented in existing robot benchmarks. Measuring this level of generalization requires infrastructure at a scale and diversity that physical evaluation alone cannot provide. We introduce MolmoSpaces, a fully open ecosystem to support large-scale benchmarking of robot policies. MolmoSpaces consists of over 230k diverse indoor environments, ranging from handcrafted household scenes to procedurally generated multiroom houses, populated with 130k richly annotated object assets, including 48k manipulable objects with 42M stable grasps. Crucially, these environments are simulator-agnostic, supporting popular options such as MuJoCo, Isaac, and ManiSkill. The ecosystem supports the full spectrum of embodied tasks: static and mobile manipulation, navigation, and multiroom long-horizon tasks requiring coordinated perception, planning, and interaction across entire indoor environments. We also design MolmoSpaces-Bench, a benchmark suite of 8 tasks in which robots interact with our diverse scenes and richly annotated objects. Our experiments show MolmoSpaces-Bench exhibits strong sim-to-real correlation (R = 0.96, ho = 0.98), confirm newer and stronger zero-shot policies outperform earlier versions in our benchmarks, and identify key sensitivities to prompt phrasing, initial joint positions, and camera occlusion. Through MolmoSpaces and its open-source assets and tooling, we provide a foundation for scalable data generation, policy training, and benchmark creation for robot learning research.",
            "score": 3,
            "issue_id": 1036,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "d1e5bd5922feaf2f",
            "authors": [
                "Yejin Kim",
                "Wilbert Pumacay",
                "Omar Rayyan",
                "Max Argus",
                "Winson Han",
                "Eli VanderBilt",
                "Jordi Salvador",
                "Abhay Deshpande",
                "Rose Hendrix",
                "Snehal Jauhri",
                "Shuo Liu",
                "Nur Muhammad Mahi Shafiullah",
                "Maya Guru",
                "Ainaz Eftekhar",
                "Karen Farley",
                "Donovan Clay",
                "Jiafei Duan",
                "Arjun Guru",
                "Piper Wolters",
                "Alvaro Herrasti",
                "Ying-Chun Lee",
                "Georgia Chalvatzaki",
                "Yuchen Cui",
                "Ali Farhadi",
                "Dieter Fox",
                "Ranjay Krishna"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "Technische UniversitÃ¤t Darmstadt",
                "University of California, Berkeley",
                "University of California, Los Angeles",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11337.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#dataset",
                    "#open_source",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº",
                    "desc": "MolmoSpaces Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 230 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… ÑÑ†ĞµĞ½ Ğ¸ 130 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ñ‹ (MuJoCo, Isaac, ManiSkill) Ğ¸ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ, Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ¾Ğ¼Ğ½Ğ°Ñ‚Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ MolmoSpaces-Bench Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ¸Ğ· 8 Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ-Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ (R = 0.96) Ğ¸ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‰Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… zero-shot Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº. ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸."
                },
                "en": {
                    "title": "Empowering Robots with Diverse Indoor Benchmarking",
                    "desc": "MolmoSpaces is an innovative open ecosystem designed for benchmarking robot policies in diverse indoor environments. It features over 230,000 unique scenes and 130,000 annotated objects, including 48,000 manipulable items, which allow for extensive testing of robotic capabilities. The platform supports various simulators and encompasses a wide range of tasks, from simple manipulation to complex navigation challenges. By providing a robust infrastructure for sim-to-real evaluation, MolmoSpaces aims to enhance the generalization and performance of robotic systems in real-world applications."
                },
                "zh": {
                    "title": "MolmoSpacesï¼šæœºå™¨äººç­–ç•¥åŸºå‡†æµ‹è¯•çš„æ–°ç”Ÿæ€",
                    "desc": "MolmoSpacesæ˜¯ä¸€ä¸ªå¼€æ”¾çš„ç”Ÿæ€ç³»ç»Ÿï¼Œæä¾›å¤šæ ·åŒ–çš„å®¤å†…ç¯å¢ƒå’Œæ³¨é‡Šå¯¹è±¡ï¼Œç”¨äºå¤§è§„æ¨¡æœºå™¨äººç­–ç•¥åŸºå‡†æµ‹è¯•ã€‚è¯¥ç³»ç»ŸåŒ…å«è¶…è¿‡23ä¸‡ä¸ªå¤šæ ·åŒ–çš„å®¤å†…åœºæ™¯å’Œ13ä¸‡ä¸ªä¸°å¯Œæ³¨é‡Šçš„å¯¹è±¡èµ„äº§ï¼Œæ”¯æŒå¤šç§æ¨¡æ‹Ÿå™¨ã€‚MolmoSpacesèƒ½å¤Ÿè¯„ä¼°æœºå™¨äººåœ¨é™æ€å’Œç§»åŠ¨æ“ä½œã€å¯¼èˆªç­‰ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰å¾ˆå¼ºçš„æ¨¡æ‹Ÿåˆ°ç°å®çš„ç›¸å…³æ€§ã€‚é€šè¿‡MolmoSpacesï¼Œæˆ‘ä»¬ä¸ºæœºå™¨äººå­¦ä¹ ç ”ç©¶æä¾›äº†å¯æ‰©å±•çš„æ•°æ®ç”Ÿæˆã€ç­–ç•¥è®­ç»ƒå’ŒåŸºå‡†åˆ›å»ºçš„åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08194",
            "title": "Dreaming in Code for Curriculum Learning in Open-Ended Worlds",
            "url": "https://huggingface.co/papers/2602.08194",
            "abstract": "Foundation models generate executable environment code to scaffold learning progress in open-ended worlds, enabling agents to acquire long-horizon skills through curriculum control.  \t\t\t\t\tAI-generated summary \t\t\t\t Open-ended learning frames intelligence as emerging from continual interaction with an ever-expanding space of environments. While recent advances have utilized foundation models to programmatically generate diverse environments, these approaches often focus on discovering isolated behaviors rather than orchestrating sustained progression. In complex open-ended worlds, the large combinatorial space of possible challenges makes it difficult for agents to discover sequences of experiences that remain consistently learnable. To address this, we propose Dreaming in Code (DiCode), a framework in which foundation models synthesize executable environment code to scaffold learning toward increasing competence. In DiCode, \"dreaming\" takes the form of materializing code-level variations of the world. We instantiate DiCode in Craftax, a challenging open-ended benchmark characterized by rich mechanics and long-horizon progression. Empirically, DiCode enables agents to acquire long-horizon skills, achieving a 16% improvement in mean return over the strongest baseline and non-zero success on late-game combat tasks where prior methods fail. Our results suggest that code-level environment design provides a practical mechanism for curriculum control, enabling the construction of intermediate environments that bridge competence gaps in open-ended worlds. Project page and source code are available at https://konstantinosmitsides.github.io/dreaming-in-code and https://github.com/konstantinosmitsides/dreaming-in-code.",
            "score": 3,
            "issue_id": 1036,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "1276107f5475c4e9",
            "authors": [
                "Konstantinos Mitsides",
                "Maxence Faldor",
                "Antoine Cully"
            ],
            "affiliations": [
                "Department of Computing, Imperial College London, London, United Kingdom"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08194.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rl",
                    "#training",
                    "#agents"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ‚ĞµĞ· ĞºĞ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Dreaming in Code (DiCode) â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ² ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ†ĞµĞ»ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ¾ÑĞ²Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ°Ñ… Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡. Foundation Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¼ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Craftax Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ 16% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ³Ğ´Ğµ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ."
                },
                "en": {
                    "title": "Empowering Agents with Code-Generated Learning Environments",
                    "desc": "This paper introduces Dreaming in Code (DiCode), a framework that uses foundation models to create executable environment code, facilitating learning in complex open-ended worlds. The approach allows agents to develop long-horizon skills by generating diverse and progressively challenging environments, rather than focusing solely on isolated behaviors. By implementing DiCode in the Craftax benchmark, the authors demonstrate a significant improvement in agent performance, achieving better results in late-game tasks compared to previous methods. The findings highlight the effectiveness of code-level environment design as a means of curriculum control, helping agents bridge competence gaps during their learning journey."
                },
                "zh": {
                    "title": "ä»£ç ä¸­çš„æ¢¦æƒ³ï¼šæå‡æ™ºèƒ½ä½“å­¦ä¹ èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸º\"ä»£ç ä¸­çš„æ¢¦æƒ³\"ï¼ˆDiCodeï¼‰çš„æ¡†æ¶ï¼Œåˆ©ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆå¯æ‰§è¡Œçš„ç¯å¢ƒä»£ç ï¼Œä»¥ä¿ƒè¿›æ™ºèƒ½ä½“åœ¨å¼€æ”¾å¼ä¸–ç•Œä¸­çš„å­¦ä¹ è¿›ç¨‹ã€‚è¯¥æ–¹æ³•é€šè¿‡ç¼–ç¨‹ç”Ÿæˆå¤šæ ·åŒ–çš„ç¯å¢ƒï¼Œå¸®åŠ©æ™ºèƒ½ä½“åœ¨å¤æ‚çš„æŒ‘æˆ˜ä¸­é€æ­¥æé«˜æŠ€èƒ½ã€‚DiCodeåœ¨CraftaxåŸºå‡†æµ‹è¯•ä¸­å¾—åˆ°äº†åº”ç”¨ï¼Œæ˜¾ç¤ºå‡ºæ™ºèƒ½ä½“åœ¨é•¿æ—¶é—´è·¨åº¦çš„ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä»£ç çº§ç¯å¢ƒè®¾è®¡ä¸ºè¯¾ç¨‹æ§åˆ¶æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æœºåˆ¶ï¼Œèƒ½å¤Ÿæ„å»ºä¸­é—´ç¯å¢ƒä»¥å¼¥è¡¥å¼€æ”¾å¼ä¸–ç•Œä¸­çš„èƒ½åŠ›å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.12262",
            "title": "T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization",
            "url": "https://huggingface.co/papers/2602.12262",
            "abstract": "A trajectory self-distillation framework with direct discriminative optimization improves few-step decoding efficiency in diffusion large language models while maintaining generation quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to a substantial degradation in generation quality. To alleviate this, we propose a trajectory self-distillation framework that improves few-step decoding by distilling the model's own generative trajectories. We incorporate Direct Discriminative Optimization (DDO), a reverse-KL objective that promotes mode-seeking distillation and encourages the student to concentrate on high-probability teacher modes. Across benchmarks, our approach consistently outperforms strong few-step baselines and standard training under tight step budgets. Although full-step decoding remains superior, we substantially narrow the gap, establishing a strong foundation towards practical few-step DLLMs. The source code is available at https://github.com/Tyrion58/T3D.",
            "score": 2,
            "issue_id": 1037,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "d7fa420c87c1009e",
            "authors": [
                "Tunyu Zhang",
                "Xinxi Zhang",
                "Ligong Han",
                "Haizhou Shi",
                "Xiaoxiao He",
                "Zhuowei Li",
                "Hao Wang",
                "Kai Xu",
                "Akash Srivastava",
                "Hao Wang",
                "Vladimir Pavlovic",
                "Dimitris N. Metaxas"
            ],
            "affiliations": [
                "Amazon",
                "Department of Computer Science",
                "MIT-IBM Watson AI Lab",
                "Red Hat AI",
                "Rutgers University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.12262.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#diffusion"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Direct Discriminative Optimization Ñ reverse-KL Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ñ… ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚ Ğ·Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑˆĞ°Ğ³Ğ¾Ğ², Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¼ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ñ… Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ°Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Few-Step Decoding in DLLMs with Self-Distillation",
                    "desc": "This paper introduces a trajectory self-distillation framework aimed at enhancing the efficiency of few-step decoding in diffusion large language models (DLLMs). By utilizing Direct Discriminative Optimization (DDO), the framework distills the model's own generative paths, allowing it to focus on the most probable outputs. This method significantly improves the balance between decoding speed and generation quality, outperforming existing few-step approaches. While full-step decoding is still more effective, this work narrows the performance gap, paving the way for more practical applications of DLLMs."
                },
                "zh": {
                    "title": "æå‡å°‘æ­¥è§£ç æ•ˆç‡çš„è½¨è¿¹è‡ªè’¸é¦æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§è½¨è¿¹è‡ªè’¸é¦æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆDLLMsï¼‰åœ¨å°‘æ­¥è§£ç ä¸­çš„æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆè´¨é‡ã€‚è¯¥æ¡†æ¶é€šè¿‡è’¸é¦æ¨¡å‹è‡ªèº«çš„ç”Ÿæˆè½¨è¿¹æ¥ä¼˜åŒ–è§£ç è¿‡ç¨‹ï¼Œå¹¶å¼•å…¥äº†ç›´æ¥åˆ¤åˆ«ä¼˜åŒ–ï¼ˆDDOï¼‰ï¼Œä¿ƒè¿›æ¨¡å¼å¯»æ±‚è’¸é¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºå¼ºå¤§çš„å°‘æ­¥åŸºçº¿å’Œæ ‡å‡†è®­ç»ƒï¼Œå°½ç®¡å…¨æ­¥è§£ç ä»ç„¶è¡¨ç°æ›´å¥½ï¼Œä½†æˆ‘ä»¬æ˜¾è‘—ç¼©å°äº†ä¸¤è€…ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºå®ç”¨çš„å°‘æ­¥DLLMså¥ å®šäº†åšå®çš„åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.12164",
            "title": "Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision",
            "url": "https://huggingface.co/papers/2602.12164",
            "abstract": "Sci-CoE is a two-stage scientific co-evolving framework that enables large language models to self-evolve as both solver and verifier through sparse-to-unsupervised learning transitions, improving scientific reasoning capabilities and evaluation system robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated exceptional reasoning capabilities, and co-evolving paradigms have shown promising results in domains such as code and math. However, in scientific reasoning tasks, these models remain fragile due to unreliable solution evaluation and limited diversity in verification strategies. In this work, we propose Sci-CoE, a two-stage scientific co-evolving framework that enables models to self-evolve as both solver and verifier through a transition from sparse supervision to unsupervised learning. In the first stage, the model uses a small set of annotated data to establish fundamental correctness judgment anchors for the Verifier. In the second stage, we introduce a geometric reward mechanism that jointly considers consensus, reliability, and diversity, driving large-scale self-iteration on unlabeled data. Experiments on several general scientific benchmarks demonstrate that Sci-CoE enhances complex reasoning capabilities and exhibits strong scalability, facilitating the construction of more robust and diverse evaluation systems. Codes are available at https://github.com/InternScience/Sci-CoE.",
            "score": 2,
            "issue_id": 1037,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "0b1015bef8de911f",
            "authors": [
                "Xiaohan He",
                "Shiyang Feng",
                "Songtao Huang",
                "Lei Bai",
                "Bin Wang",
                "Bo Zhang"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.12164.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#math",
                    "#science",
                    "#open_source",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡Ğ¸Ñ‚ÑÑ ÑĞ°Ğ¼Ğ° ÑĞµĞ±Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒ: ĞºĞ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»Ñ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Sci-CoE Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾-ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°, Ğ° Ğ²Ğ¾ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ½ĞµĞ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ ĞºĞ°Ğº Ğ² Ñ€Ğ¾Ğ»Ğ¸ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ñ€Ğ¾Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑÑ‰ĞµĞ³Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Sci-CoE ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸."
                },
                "en": {
                    "title": "Empowering Language Models to Self-Evolve in Scientific Reasoning",
                    "desc": "Sci-CoE is a novel framework designed to enhance the reasoning abilities of large language models (LLMs) in scientific tasks. It operates in two stages: first, it uses a small amount of labeled data to train a Verifier that can assess the correctness of solutions. In the second stage, it employs a geometric reward system that encourages the model to improve through self-iteration on unlabeled data, focusing on consensus, reliability, and diversity. This approach not only strengthens the model's reasoning capabilities but also builds a more robust evaluation system for scientific reasoning tasks."
                },
                "zh": {
                    "title": "Sci-CoEï¼šç§‘å­¦æ¨ç†çš„æ–°è¿›åŒ–æ¡†æ¶",
                    "desc": "Sci-CoEæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„ç§‘å­¦å…±åŒè¿›åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç¨€ç–åˆ°æ— ç›‘ç£å­¦ä¹ çš„è½¬å˜ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹è‡ªæˆ‘è¿›åŒ–ä¸ºæ±‚è§£è€…å’ŒéªŒè¯è€…ï¼Œä»è€Œæé«˜ç§‘å­¦æ¨ç†èƒ½åŠ›å’Œè¯„ä¼°ç³»ç»Ÿçš„é²æ£’æ€§ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæ¨¡å‹åˆ©ç”¨å°‘é‡æ ‡æ³¨æ•°æ®å»ºç«‹éªŒè¯è€…çš„åŸºæœ¬æ­£ç¡®æ€§åˆ¤æ–­é”šç‚¹ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥å‡ ä½•å¥–åŠ±æœºåˆ¶ï¼Œç»¼åˆè€ƒè™‘å…±è¯†ã€å¯é æ€§å’Œå¤šæ ·æ€§ï¼Œæ¨åŠ¨åœ¨æœªæ ‡è®°æ•°æ®ä¸Šçš„å¤§è§„æ¨¡è‡ªè¿­ä»£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSci-CoEå¢å¼ºäº†å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„å¯æ‰©å±•æ€§ï¼Œæœ‰åŠ©äºæ„å»ºæ›´ç¨³å¥å’Œå¤šæ ·åŒ–çš„è¯„ä¼°ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.12116",
            "title": "P-GenRM: Personalized Generative Reward Model with Test-time User-based Scaling",
            "url": "https://huggingface.co/papers/2602.12116",
            "abstract": "Personalized generative reward models address challenges in adapting language model responses to individual user preferences by using structured evaluation chains and dual-granularity scaling mechanisms for improved generalization and accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Personalized alignment of large language models seeks to adapt responses to individual user preferences, typically via reinforcement learning. A key challenge is obtaining accurate, user-specific reward signals in open-ended scenarios. Existing personalized reward models face two persistent limitations: (1) oversimplifying diverse, scenario-specific preferences into a small, fixed set of evaluation principles, and (2) struggling with generalization to new users with limited feedback. To this end, we propose P-GenRM, the first Personalized Generative Reward Model with test-time user-based scaling. P-GenRM transforms preference signals into structured evaluation chains that derive adaptive personas and scoring rubrics across various scenarios. It further clusters users into User Prototypes and introduces a dual-granularity scaling mechanism: at the individual level, it adaptively scales and aggregates each user's scoring scheme; at the prototype level, it incorporates preferences from similar users. This design mitigates noise in inferred preferences and enhances generalization to unseen users through prototype-based transfer. Empirical results show that P-GenRM achieves state-of-the-art results on widely-used personalized reward model benchmarks, with an average improvement of 2.31%, and demonstrates strong generalization on an out-of-distribution dataset. Notably, Test-time User-based scaling provides an additional 3% boost, demonstrating stronger personalized alignment with test-time scalability.",
            "score": 2,
            "issue_id": 1038,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "7c1ebb5ff8a8343f",
            "authors": [
                "Pinyi Zhang",
                "Ting-En Lin",
                "Yuchuan Wu",
                "Jingyang Chen",
                "Zongqi Wang",
                "Hua Yang",
                "Ze Xu",
                "Fei Huang",
                "Kai Zhang",
                "Yongbin Li"
            ],
            "affiliations": [
                "Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.12116.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ P-GenRM â€” Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ñ‹ Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 2.31% Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 3% Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Personalized Responses Made Easy with P-GenRM!",
                    "desc": "This paper introduces P-GenRM, a Personalized Generative Reward Model designed to enhance the adaptation of language model responses to individual user preferences. It addresses the limitations of existing models by using structured evaluation chains and dual-granularity scaling mechanisms, which help in accurately capturing diverse user preferences. By clustering users into prototypes and adapting scoring schemes at both individual and prototype levels, P-GenRM improves generalization to new users with limited feedback. Empirical results indicate that this model outperforms previous benchmarks, achieving significant improvements in personalized alignment and scalability."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼šæå‡è¯­è¨€æ¨¡å‹çš„ç”¨æˆ·é€‚åº”æ€§",
                    "desc": "ä¸ªæ€§åŒ–ç”Ÿæˆå¥–åŠ±æ¨¡å‹æ—¨åœ¨è§£å†³è¯­è¨€æ¨¡å‹å“åº”ä¸ªä½“ç”¨æˆ·åå¥½çš„é€‚åº”æ€§é—®é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡ç»“æ„åŒ–è¯„ä¼°é“¾å’ŒåŒç²’åº¦ç¼©æ”¾æœºåˆ¶ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå‡†ç¡®æ€§ã€‚P-GenRMæ˜¯é¦–ä¸ªåœ¨æµ‹è¯•æ—¶åŸºäºç”¨æˆ·çš„ç¼©æ”¾çš„ä¸ªæ€§åŒ–ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼Œèƒ½å¤Ÿå°†åå¥½ä¿¡å·è½¬åŒ–ä¸ºé€‚åº”æ€§çš„äººç‰©è§’è‰²å’Œè¯„åˆ†æ ‡å‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒP-GenRMåœ¨ä¸ªæ€§åŒ–å¥–åŠ±æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶åœ¨æœªè§ç”¨æˆ·ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11964",
            "title": "Gaia2: Benchmarking LLM Agents on Dynamic and Asynchronous Environments",
            "url": "https://huggingface.co/papers/2602.11964",
            "abstract": "Gaia2 presents a benchmark for evaluating large language model agents in asynchronous, dynamic environments with temporal constraints and multi-agent collaboration, featuring a write-action verifier for reinforcement learning and revealing trade-offs between reasoning, efficiency, and robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Gaia2, a benchmark for evaluating large language model agents in realistic, asynchronous environments. Unlike prior static or synchronous evaluations, Gaia2 introduces scenarios where environments evolve independently of agent actions, requiring agents to operate under temporal constraints, adapt to noisy and dynamic events, resolve ambiguity, and collaborate with other agents. Each scenario is paired with a write-action verifier, enabling fine-grained, action-level evaluation and making Gaia2 directly usable for reinforcement learning from verifiable rewards. Our evaluation of state-of-the-art proprietary and open-source models shows that no model dominates across capabilities: GPT-5 (high) reaches the strongest overall score of 42% pass@1 but fails on time-sensitive tasks, Claude-4 Sonnet trades accuracy and speed for cost, Kimi-K2 leads among open-source models with 21% pass@1. These results highlight fundamental trade-offs between reasoning, efficiency, robustness, and expose challenges in closing the \"sim2real\" gap. Gaia2 is built on a consumer environment with the open-source Agents Research Environments platform and designed to be easy to extend. By releasing Gaia2 alongside the foundational ARE framework, we aim to provide the community with a flexible infrastructure for developing, benchmarking, and training the next generation of practical agent systems.",
            "score": 2,
            "issue_id": 1036,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "2b13f07b9b5eb0df",
            "authors": [
                "Romain Froger",
                "Pierre Andrews",
                "Matteo Bettini",
                "Amar Budhiraja",
                "Ricardo Silveira Cabral",
                "Virginie Do",
                "Emilien Garreau",
                "Jean-Baptiste Gaya",
                "Hugo LaurenÃ§on",
                "Maxime Lecanu",
                "Kunal Malkan",
                "Dheeraj Mekala",
                "Pierre MÃ©nard",
                "Gerard Moreno-Torres Bertran",
                "Ulyana Piterbarg",
                "Mikhail Plekhanov",
                "Mathieu Rita",
                "Andrey Rusakov",
                "Vladislav Vorotilov",
                "Mengjue Wang",
                "Ian Yu",
                "Amine Benhalloum",
                "GrÃ©goire Mialon",
                "Thomas Scialom"
            ],
            "affiliations": [
                "Meta SuperIntelligence Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11964.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#rl",
                    "#benchmark",
                    "#optimization",
                    "#agents",
                    "#reasoning"
                ],
                "emoji": "â±ï¸",
                "ru": {
                    "title": "ĞÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´ Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸: ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑÑ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·ÑƒĞ¼Ğ¾Ğ¼, ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Gaia2 â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ… Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº, Gaia2 Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸, Ğ³Ğ´Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº ÑˆÑƒĞ¼Ğ½Ñ‹Ğ¼ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸ÑĞ¼ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¹ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑÑ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ğ² Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ĞµĞ¹ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ."
                },
                "en": {
                    "title": "Gaia2: Benchmarking AI Agents in Dynamic Environments",
                    "desc": "Gaia2 is a new benchmark designed to test large language model agents in complex, changing environments where timing and teamwork are crucial. It allows agents to face challenges like adapting to unexpected changes and working with other agents while under time pressure. The benchmark includes a write-action verifier that helps evaluate agents' actions in detail, making it suitable for reinforcement learning. The findings show that different models excel in different areas, revealing important trade-offs between reasoning ability, efficiency, and robustness in AI systems."
                },
                "zh": {
                    "title": "Gaia2ï¼šè¯„ä¼°æ™ºèƒ½ä»£ç†çš„æ–°åŸºå‡†",
                    "desc": "Gaia2æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†çš„åŸºå‡†ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼‚æ­¥å’ŒåŠ¨æ€ç¯å¢ƒä¸­ã€‚å®ƒè¦æ±‚ä»£ç†åœ¨æ—¶é—´é™åˆ¶ä¸‹é€‚åº”ä¸æ–­å˜åŒ–çš„ç¯å¢ƒï¼Œå¹¶ä¸å…¶ä»–ä»£ç†è¿›è¡Œåä½œã€‚æ¯ä¸ªåœºæ™¯éƒ½é…å¤‡äº†å†™æ“ä½œéªŒè¯å™¨ï¼Œä»¥ä¾¿è¿›è¡Œç»†ç²’åº¦çš„è¯„ä¼°ï¼Œé€‚ç”¨äºå¼ºåŒ–å­¦ä¹ ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä¸åŒæ¨¡å‹åœ¨æ¨ç†ã€æ•ˆç‡å’Œé²æ£’æ€§ä¹‹é—´å­˜åœ¨åŸºæœ¬çš„æƒè¡¡ï¼Œæ­ç¤ºäº†åœ¨â€œæ¨¡æ‹Ÿåˆ°ç°å®â€è½¬å˜ä¸­çš„æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11761",
            "title": "MiniCPM-SALA: Hybridizing Sparse and Linear Attention for Efficient Long-Context Modeling",
            "url": "https://huggingface.co/papers/2602.11761",
            "abstract": "MiniCPM-SALA combines sparse and linear attention mechanisms in a hybrid architecture to enable efficient processing of ultra-long contexts while maintaining model performance and reducing training costs.  \t\t\t\t\tAI-generated summary \t\t\t\t The evolution of large language models (LLMs) towards applications with ultra-long contexts faces challenges posed by the high computational and memory costs of the Transformer architecture. While existing sparse and linear attention mechanisms attempt to mitigate these issues, they typically involve a trade-off between memory efficiency and model performance. This paper introduces MiniCPM-SALA, a 9B-parameter hybrid architecture that integrates the high-fidelity long-context modeling of sparse attention (InfLLM-V2) with the global efficiency of linear attention (Lightning Attention). By employing a layer selection algorithm to integrate these mechanisms in a 1:3 ratio and utilizing a hybrid positional encoding (HyPE), the model maintains efficiency and performance for long-context tasks. Furthermore, we introduce a cost-effective continual training framework that transforms pre-trained Transformer-based models into hybrid models, which reduces training costs by approximately 75% compared to training from scratch. Extensive experiments show that MiniCPM-SALA maintains general capabilities comparable to full-attention models while offering improved efficiency. On a single NVIDIA A6000D GPU, the model achieves up to 3.5x the inference speed of the full-attention model at the sequence length of 256K tokens and supports context lengths of up to 1M tokens, a scale where traditional full-attention 8B models fail because of memory constraints.",
            "score": 2,
            "issue_id": 1036,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "8a9b6412eff322cc",
            "authors": [
                "MiniCPM Team",
                "Wenhao An",
                "Yingfa Chen",
                "Yewei Fang",
                "Jiayi Li",
                "Xin Li",
                "Yaohui Li",
                "Yishan Li",
                "Yuxuan Li",
                "Biyuan Lin",
                "Chuan Liu",
                "Hezi Liu",
                "Siyuan Liu",
                "Hongya Lyu",
                "Yinxu Pan",
                "Shixin Ren",
                "Xingyu Shen",
                "Zhou Su",
                "Haojun Sun",
                "Yangang Sun",
                "Zhen Leng Thai",
                "Xin Tian",
                "Rui Wang",
                "Xiaorong Wang",
                "Yudong Wang",
                "Bo Wu",
                "Xiaoyue Xu",
                "Dong Xu",
                "Shuaikang Xue",
                "Jiawei Yang",
                "Bowen Zhang",
                "Jinqian Zhang",
                "Letian Zhang",
                "Shengnan Zhang",
                "Xinyu Zhang",
                "Xinyuan Zhang",
                "Zhu Zhang",
                "Hengyu Zhao",
                "Jiacheng Zhao",
                "Jie Zhou",
                "Zihan Zhou",
                "Shuo Wang",
                "Chaojun Xiao",
                "Xu Han",
                "Zhiyuan Liu",
                "Maosong Sun"
            ],
            "affiliations": [
                "OpenBMB"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11761.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#training",
                    "#architecture",
                    "#long_context",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑŒÑ‚Ñ€Ğ°-Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "MiniCPM-SALA Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ (sparse attention) Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ (linear attention) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑƒĞ»ÑŒÑ‚Ñ€Ğ°-Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑĞ»Ğ¾Ñ‘Ğ² Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¸ 1:3 Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ¶Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Transformer-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° 75% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ½ÑƒĞ»Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² 3.5 Ñ€Ğ°Ğ·Ğ° Ğ¿Ñ€Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ 256K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ´Ğ¾ 1M Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±Ñ‰Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Efficient Long-Context Processing with MiniCPM-SALA",
                    "desc": "MiniCPM-SALA is a new machine learning model that combines sparse and linear attention mechanisms to efficiently handle very long text inputs. It uses a hybrid architecture that balances the detailed context understanding of sparse attention with the speed of linear attention. This model not only reduces the training costs significantly but also maintains high performance, achieving faster inference speeds compared to traditional models. With the ability to process up to 1 million tokens, MiniCPM-SALA addresses the limitations of existing large language models in handling ultra-long contexts."
                },
                "zh": {
                    "title": "é«˜æ•ˆå¤„ç†è¶…é•¿ä¸Šä¸‹æ–‡çš„æ··åˆæ¨¡å‹",
                    "desc": "MiniCPM-SALAæ˜¯ä¸€ç§ç»“åˆç¨€ç–æ³¨æ„åŠ›å’Œçº¿æ€§æ³¨æ„åŠ›æœºåˆ¶çš„æ··åˆæ¶æ„ï¼Œæ—¨åœ¨é«˜æ•ˆå¤„ç†è¶…é•¿ä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½å¹¶é™ä½è®­ç»ƒæˆæœ¬ã€‚è¯¥æ¨¡å‹é€šè¿‡å±‚é€‰æ‹©ç®—æ³•ä»¥1:3çš„æ¯”ä¾‹æ•´åˆè¿™ä¸¤ç§æœºåˆ¶ï¼Œå¹¶é‡‡ç”¨æ··åˆä½ç½®ç¼–ç ï¼ˆHyPEï¼‰ï¼Œåœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­å®ç°äº†æ•ˆç‡å’Œæ€§èƒ½çš„å¹³è¡¡ã€‚ä¸ä¼ ç»Ÿçš„å…¨æ³¨æ„åŠ›æ¨¡å‹ç›¸æ¯”ï¼ŒMiniCPM-SALAåœ¨æ¨ç†é€Ÿåº¦ä¸Šæé«˜äº†3.5å€ï¼Œå¹¶æ”¯æŒé«˜è¾¾1Mçš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œè§£å†³äº†å†…å­˜é™åˆ¶çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§ç»æµé«˜æ•ˆçš„æŒç»­è®­ç»ƒæ¡†æ¶ï¼Œå°†é¢„è®­ç»ƒçš„Transformeræ¨¡å‹è½¬åŒ–ä¸ºæ··åˆæ¨¡å‹ï¼Œè®­ç»ƒæˆæœ¬é™ä½çº¦75%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11636",
            "title": "ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning",
            "url": "https://huggingface.co/papers/2602.11636",
            "abstract": "ScalSelect is a scalable training-free method for selecting representative multimodal data that achieves near-full-dataset performance with significantly reduced computational requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t Large-scale Visual Instruction Tuning (VIT) has become a key paradigm for advancing the performance of vision-language models (VLMs) across various multimodal tasks. However, training on the large-scale datasets is computationally expensive and inefficient due to redundancy in the data, which motivates the need for multimodal data selection to improve training efficiency. Existing data selection methods for VIT either require costly training or gradient computation. Training-free alternatives often depend on proxy models or datasets, instruction-agnostic representations, and pairwise similarity with quadratic complexity, limiting scalability and representation fidelity. In this work, we propose ScalSelect, a scalable training-free multimodal data selection method with linear-time complexity with respect to the number of samples, eliminating the need for external models or auxiliary datasets. ScalSelect first constructs sample representations by extracting visual features most attended by instruction tokens in the target VLM, capturing instruction-relevant information. It then identifies samples whose representations best approximate the dominant subspace of the full dataset representations, enabling scalable importance scoring without pairwise comparisons. Extensive experiments across multiple VLMs, datasets, and selection budgets demonstrate that ScalSelect achieves over 97.5% of the performance of training on the full dataset using only 16% of the data, and even outperforms full-data training in some settings. The code is available at https://github.com/ChangtiWu/ScalSelect{ScalSelect}.",
            "score": 2,
            "issue_id": 1036,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "027b0bcf349fb144",
            "authors": [
                "Changti Wu",
                "Jiahuai Mao",
                "Yuzhuo Miao",
                "Shijie Lian",
                "Bin Yu",
                "Xiaopeng Lin",
                "Cong Huang",
                "Lei Zhang",
                "Kai Chen"
            ],
            "affiliations": [
                "East China Normal University",
                "Harbin Institute of Technology",
                "Huazhong University of Science and Technology",
                "The Hong Kong Polytechnic University",
                "The Hong Kong University of Science and Technology (Guangzhou)",
                "Zhongguancun Academy",
                "Zhongguancun Institute of Artificial Intelligence"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11636.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#data"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ScalSelect â€” ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸, Ğ½Ğ° ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°ÑÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹, Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞµ Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ScalSelect Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ»Ğ¸ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 97.5% Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 16% Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ° Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ."
                },
                "en": {
                    "title": "Efficient Multimodal Data Selection with ScalSelect",
                    "desc": "ScalSelect is a novel method designed to efficiently select representative multimodal data without the need for extensive training. It operates with linear-time complexity, making it scalable and reducing computational costs significantly. By focusing on visual features that are most relevant to instruction tokens, ScalSelect captures essential information for effective data representation. The method has been shown to achieve nearly full-dataset performance using only a fraction of the data, demonstrating its effectiveness across various vision-language models and datasets."
                },
                "zh": {
                    "title": "ScalSelectï¼šé«˜æ•ˆé€‰æ‹©å¤šæ¨¡æ€æ•°æ®çš„æ— è®­ç»ƒæ–¹æ³•",
                    "desc": "ScalSelectæ˜¯ä¸€ç§å¯æ‰©å±•çš„æ— è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºé€‰æ‹©ä»£è¡¨æ€§çš„å¤šæ¨¡æ€æ•°æ®ã€‚å®ƒé€šè¿‡æå–ä¸æŒ‡ä»¤ç›¸å…³çš„è§†è§‰ç‰¹å¾ï¼Œæ„å»ºæ ·æœ¬è¡¨ç¤ºï¼Œä»è€Œå®ç°çº¿æ€§æ—¶é—´å¤æ‚åº¦çš„æ•°æ®é€‰æ‹©ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸éœ€è¦å¤–éƒ¨æ¨¡å‹æˆ–è¾…åŠ©æ•°æ®é›†çš„æƒ…å†µä¸‹ï¼Œè¯†åˆ«å‡ºæœ€èƒ½ä»£è¡¨å…¨æ•°æ®é›†çš„æ ·æœ¬ã€‚å®éªŒè¡¨æ˜ï¼ŒScalSelectåœ¨ä½¿ç”¨ä»…16%çš„æ•°æ®æ—¶ï¼Œèƒ½å¤Ÿè¾¾åˆ°å…¨æ•°æ®é›†è®­ç»ƒçš„97.5%ä»¥ä¸Šçš„æ€§èƒ½ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šå…¨æ•°æ®é›†è®­ç»ƒçš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11598",
            "title": "ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation",
            "url": "https://huggingface.co/papers/2602.11598",
            "abstract": "A unified Vision-Language-Action model with a hierarchical architecture combining semantic reasoning and continuous trajectory generation achieves state-of-the-art performance across multiple embodied navigation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Embodied navigation has long been fragmented by task-specific architectures. We introduce ABot-N0, a unified Vision-Language-Action (VLA) foundation model that achieves a ``Grand Unification'' across 5 core tasks: Point-Goal, Object-Goal, Instruction-Following, POI-Goal, and Person-Following. ABot-N0 utilizes a hierarchical ``Brain-Action'' architecture, pairing an LLM-based Cognitive Brain for semantic reasoning with a Flow Matching-based Action Expert for precise, continuous trajectory generation.   To support large-scale learning, we developed the ABot-N0 Data Engine, curating 16.9M expert trajectories and 5.0M reasoning samples across 7,802 high-fidelity 3D scenes (10.7 km^2). ABot-N0 achieves new SOTA performance across 7 benchmarks, significantly outperforming specialized models. Furthermore, our Agentic Navigation System integrates a planner with hierarchical topological memory, enabling robust, long-horizon missions in dynamic real-world environments.",
            "score": 2,
            "issue_id": 1036,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "490d6ff8712d0925",
            "authors": [
                "Zedong Chu",
                "Shichao Xie",
                "Xiaolong Wu",
                "Yanfen Shen",
                "Minghua Luo",
                "Zhengbo Wang",
                "Fei Liu",
                "Xiaoxu Leng",
                "Junjun Hu",
                "Mingyang Yin",
                "Jia Lu",
                "Yingnan Guo",
                "Kai Yang",
                "Jiawei Han",
                "Xu Chen",
                "Yanqing Zhu",
                "Yuxiang Zhao",
                "Xin Liu",
                "Yirong Yang",
                "Ye He",
                "Jiahang Wang",
                "Yang Cai",
                "Tianlin Zhang",
                "Li Gao",
                "Liu Liu",
                "Mingchao Sun",
                "Fan Jiang",
                "Chiyu Wang",
                "Zhicheng Liu",
                "Hongyu Pan",
                "Honglin Han",
                "Zhining Gu",
                "Kuan Yang",
                "Jianfang Zhang",
                "Di Jing",
                "Zihao Guan",
                "Wei Guo",
                "Guoqing Liu",
                "Di Yang",
                "Xiangpo Yang",
                "Menglin Yang",
                "Hongguang Xing",
                "Weiguo Li",
                "Mu Xu"
            ],
            "affiliations": [
                "alibaba-inc.com"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11598.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#dataset",
                    "#transfer_learning",
                    "#3d",
                    "#architecture",
                    "#synthetic",
                    "#benchmark",
                    "#multimodal",
                    "#agents",
                    "#reasoning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Vision-Language-Action (VLA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Â«ĞœĞ¾Ğ·Ğ³-Ğ”ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµÂ», Ğ³Ğ´Ğµ LLM Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·, Ğ° Flow Matching Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° Data Engine Ñ 16,9 Ğ¼Ğ»Ğ½ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ 5,0 Ğ¼Ğ»Ğ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° 7802 Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… 3D-ÑÑ†ĞµĞ½Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° 7 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼Ğ¸ÑÑĞ¸ÑĞ¼ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Unifying Vision, Language, and Action for Superior Navigation",
                    "desc": "This paper presents ABot-N0, a unified Vision-Language-Action (VLA) model designed to enhance embodied navigation tasks. It employs a hierarchical architecture that combines a large language model (LLM) for semantic reasoning with a Flow Matching-based Action Expert for generating smooth movement trajectories. The model is trained on a vast dataset of expert trajectories and reasoning samples, allowing it to excel across multiple navigation benchmarks. ABot-N0 sets new state-of-the-art performance, demonstrating its effectiveness in complex, real-world navigation scenarios."
                },
                "zh": {
                    "title": "ç»Ÿä¸€è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹ï¼Œå¯¼èˆªæ–°çºªå…ƒ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç»Ÿä¸€çš„è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹ABot-N0ï¼Œé‡‡ç”¨åˆ†å±‚æ¶æ„ï¼Œç»“åˆè¯­ä¹‰æ¨ç†å’Œè¿ç»­è½¨è¿¹ç”Ÿæˆï¼Œèƒ½å¤Ÿåœ¨å¤šç§å…·èº«å¯¼èˆªä»»åŠ¡ä¸­å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ABot-N0æ•´åˆäº†äº”ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼ŒåŒ…æ‹¬ç›®æ ‡ç‚¹å¯¼èˆªã€ç‰©ä½“ç›®æ ‡å¯¼èˆªã€æŒ‡ä»¤è·Ÿéšã€å…´è¶£ç‚¹ç›®æ ‡å¯¼èˆªå’Œè·Ÿéšäººç±»ã€‚è¯¥æ¨¡å‹ä½¿ç”¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è®¤çŸ¥å¤§è„‘è¿›è¡Œè¯­ä¹‰æ¨ç†ï¼Œå¹¶ç»“åˆåŸºäºæµåŒ¹é…çš„è¡ŒåŠ¨ä¸“å®¶è¿›è¡Œç²¾ç¡®çš„è½¨è¿¹ç”Ÿæˆã€‚é€šè¿‡å¼€å‘ABot-N0æ•°æ®å¼•æ“ï¼Œæ”¶é›†äº†å¤§é‡ä¸“å®¶è½¨è¿¹å’Œæ¨ç†æ ·æœ¬ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„å¯¼èˆªèƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11298",
            "title": "Voxtral Realtime",
            "url": "https://huggingface.co/papers/2602.11298",
            "abstract": "Voxtral Realtime is a streaming speech recognition model trained end-to-end for sub-second latency with performance matching offline systems.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Voxtral Realtime, a natively streaming automatic speech recognition model that matches offline transcription quality at sub-second latency. Unlike approaches that adapt offline models through chunking or sliding windows, Voxtral Realtime is trained end-to-end for streaming, with explicit alignment between audio and text streams. Our architecture builds on the Delayed Streams Modeling framework, introducing a new causal audio encoder and Ada RMS-Norm for improved delay conditioning. We scale pretraining to a large-scale dataset spanning 13 languages. At a delay of 480ms, Voxtral Realtime achieves performance on par with Whisper, the most widely deployed offline transcription system. We release the model weights under the Apache 2.0 license.",
            "score": 2,
            "issue_id": 1036,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "e5e0535d723d2400",
            "authors": [
                "Alexander H. Liu",
                "Andy Ehrenberg",
                "Andy Lo",
                "Chen-Yo Sun",
                "Guillaume Lample",
                "Jean-Malo Delignon",
                "Khyathi Raghavi Chandu",
                "Patrick von Platen",
                "Pavankumar Reddy Muddireddy",
                "Rohin Arora",
                "Sanchit Gandhi",
                "Sandeep Subramanian",
                "Soham Ghosh",
                "Srijan Mishra",
                "Abhinav Rastogi",
                "Alan Jeffares",
                "Albert Jiang",
                "Alexandre Sablayrolles",
                "AmÃ©lie HÃ©liou",
                "Andrew Bai",
                "Angele Lenglemetz",
                "Anmol Agarwal",
                "Anton Eliseev",
                "Antonia Calvi",
                "Arjun Majumdar",
                "Baptiste Bout",
                "Baptiste RoziÃ¨re",
                "Baudouin De Monicault",
                "Benjamin Tibi",
                "ClÃ©mence Lanfranchi",
                "Connor Chen",
                "Corentin Barreau",
                "Corentin Sautier",
                "Cyprien Courtot",
                "Darius Dabert",
                "Diego de las Casas",
                "Elliot Chane-Sane",
                "Enguerrand Paquin",
                "Faruk Ahmed",
                "Federico Baldassarre",
                "Gabrielle Berrada",
                "GaÃ«tan Ecrepont",
                "Gauthier Guinet",
                "Genevieve Hayes",
                "Georgii Novikov",
                "Giada Pistilli",
                "Guillaume Martin",
                "Gunjan Dhanuka",
                "Gunshi Gupta",
                "Han Zhou",
                "Indraneel Mukherjee",
                "Irene Zhang",
                "Jaeyoung Kim",
                "Jan Ludziejewski",
                "Jason Rute",
                "Joachim Studnia",
                "John Harvill",
                "Jonas Amar",
                "Josselin Somerville Roberts",
                "Julien Tauran",
                "Karmesh Yadav",
                "Kartik Khandelwal",
                "Kush Jain",
                "Laurence Aitchison",
                "LÃ©onard Blier",
                "Lingxiao Zhao",
                "Louis Martin",
                "Lucile Saulnier",
                "Luyu Gao",
                "Maarten Buyl",
                "Manan Sharma",
                "Margaret Jennings",
                "Marie Pellat",
                "Mark Prins",
                "Mathieu PoirÃ©e",
                "Mathilde Guillaumin",
                "Matthieu Dinot",
                "Matthieu Futeral",
                "Maxime Darrin",
                "Maximilian Augustin",
                "Mert Unsal",
                "Mia Chiquier",
                "Nathan Grinsztajn",
                "Neha Gupta",
                "Olivier Bousquet",
                "Olivier Duchenne",
                "Patricia Wang",
                "Paul Jacob",
                "Paul Wambergue",
                "Paula Kurylowicz",
                "PhilomÃ¨ne Chagniot",
                "Pierre Stock",
                "Piotr MiÅ‚oÅ›",
                "Prateek Gupta",
                "Pravesh Agrawal",
                "Quentin Torroba",
                "Ram Ramrakhya",
                "Rishi Shah",
                "Romain Sauvestre",
                "Roman Soletskyi",
                "Rosalie Millner",
                "Sagar Vaze",
                "Samuel Humeau",
                "Siddharth Gandhi",
                "Sumukh Aithal",
                "Szymon Antoniak",
                "Teven Le Scao",
                "ThÃ©o Cachet",
                "Theo Simon Sorg",
                "Thibaut Lavril",
                "Thomas Chabal",
                "Thomas Foubert",
                "Thomas Robert",
                "Thomas Wang",
                "Tim Lawson",
                "Tom Bewley",
                "Tom Edwards",
                "Tyler Wang",
                "Valeriia Nemychnikova",
                "Van Phung",
                "Vedant Nanda",
                "Victor Jouault",
                "Virgile Richard",
                "Vladislav Bataev",
                "Wassim Bouaziz",
                "Wen-Ding Li",
                "William Marshall",
                "Xinghui Li",
                "Xingran Guo",
                "Xinyu Yang",
                "Yannic Neuhaus",
                "Yihan Wang",
                "Zaccharie Ramzi",
                "Zhenlin Xu"
            ],
            "affiliations": [
                "Mistral AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11298.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#open_source",
                    "#low_resource",
                    "#training",
                    "#architecture",
                    "#multilingual"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "ĞŸĞ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ² Ğ¿Ğ¾Ğ»ÑĞµĞºÑƒĞ½Ğ´Ñ‹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Voxtral Realtime â€” Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ end-to-end Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¼ĞµĞ½ĞµĞµ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞµĞºÑƒĞ½Ğ´Ñ‹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‡Ğ°Ğ½ĞºĞ¸, ÑÑ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° Ñ ÑĞ²Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°Ğ¼Ğ¸. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞµ Delayed Streams Modeling Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ Ğ¸ Ada RMS-Norm Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸. ĞŸÑ€Ğ¸ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞµ 480Ğ¼Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ Whisper, ÑĞ°Ğ¼Ğ¾Ğ¹ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ñ‘Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Voxtral Realtime: Instant Speech Recognition with Offline Quality",
                    "desc": "Voxtral Realtime is a cutting-edge automatic speech recognition model designed for real-time streaming with very low latency. It is trained end-to-end, ensuring that the audio and text are perfectly aligned, which is a significant improvement over traditional methods that rely on chunking or sliding windows. The model utilizes a novel causal audio encoder and Ada RMS-Norm to enhance its performance and reduce delays. With its ability to transcribe speech in 13 languages at a delay of just 480ms, it matches the quality of established offline systems like Whisper."
                },
                "zh": {
                    "title": "å®æ—¶è¯­éŸ³è¯†åˆ«ï¼Œäºšç§’å»¶è¿Ÿï¼Œè¶…è¶Šç¦»çº¿ç³»ç»Ÿ",
                    "desc": "Voxtral Realtime æ˜¯ä¸€ç§å®æ—¶æµå¼è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œç»è¿‡ç«¯åˆ°ç«¯è®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨äºšç§’å»¶è¿Ÿä¸‹å®ç°ä¸ç¦»çº¿ç³»ç»Ÿç›¸å½“çš„æ€§èƒ½ã€‚ä¸é€šè¿‡åˆ†å—æˆ–æ»‘åŠ¨çª—å£é€‚åº”ç¦»çº¿æ¨¡å‹çš„æ–¹æ³•ä¸åŒï¼ŒVoxtral Realtime ä¸“ä¸ºæµå¼å¤„ç†è€Œè®¾è®¡ï¼Œç¡®ä¿éŸ³é¢‘å’Œæ–‡æœ¬æµä¹‹é—´çš„æ˜ç¡®å¯¹é½ã€‚è¯¥æ¶æ„åŸºäºå»¶è¿Ÿæµå»ºæ¨¡æ¡†æ¶ï¼Œå¼•å…¥äº†æ–°çš„å› æœéŸ³é¢‘ç¼–ç å™¨å’Œè‡ªé€‚åº” RMS-Normï¼Œä»¥æ”¹å–„å»¶è¿Ÿæ¡ä»¶ã€‚ç»è¿‡å¤§è§„æ¨¡æ•°æ®é›†çš„é¢„è®­ç»ƒï¼ŒVoxtral Realtime åœ¨480æ¯«ç§’çš„å»¶è¿Ÿä¸‹ï¼Œæ€§èƒ½ä¸æœ€å¹¿æ³›ä½¿ç”¨çš„ç¦»çº¿è½¬å½•ç³»ç»Ÿ Whisper ç›¸å½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11792",
            "title": "Detecting RLVR Training Data via Structural Convergence of Reasoning",
            "url": "https://huggingface.co/papers/2602.11792",
            "abstract": "Reinforcement learning with verifiable rewards induces behavioral signatures that can be detected using a black-box method based on prompt generation diversity, outperforming existing contamination detection approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) is central to training modern reasoning models, but the undisclosed training data raises concerns about benchmark contamination. Unlike pretraining methods, which optimize models using token-level probabilities, RLVR fine-tunes models based on reward feedback from self-generated reasoning trajectories, making conventional likelihood-based detection methods less effective. We show that RLVR induces a distinctive behavioral signature: prompts encountered during RLVR training result in more rigid and similar generations, while unseen prompts retain greater diversity. We introduce Min-kNN Distance, a simple black-box detector that quantifies this collapse by sampling multiple completions for a given prompt and computing the average of the k smallest nearest-neighbor edit distances. Min-kNN Distance requires no access to the reference model or token probabilities. Experiments across multiple RLVR-trained reasoning models show that Min-kNN Distance reliably distinguishes RL-seen examples from unseen ones and outperforms existing membership inference and RL contamination detection baselines.",
            "score": 1,
            "issue_id": 1039,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "b71b292d651b338a",
            "authors": [
                "Hongbo Zhang",
                "Yue Yang",
                "Jianhao Yan",
                "Guangsheng Bao",
                "Yue Zhang",
                "Yue Zhang"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "School of Engineering, Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11792.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#benchmark",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚Ğ°Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ğ°Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ (RLVR). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ RLVR Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ¾Ğº: Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ¸Ğ· Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¾Ğ´Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ğ´Ñ€ÑƒĞ³ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ÑÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ Min-kNN Distance, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ‡Ñ‘Ñ€Ğ½Ñ‹Ğ¼ ÑÑ‰Ğ¸ĞºĞ¾Ğ¼ Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Detecting Contamination in RL with Unique Behavioral Signatures",
                    "desc": "This paper discusses a method for detecting contamination in reinforcement learning with verifiable rewards (RLVR). RLVR trains models using feedback from their own reasoning processes, which can lead to a unique behavioral pattern in the generated outputs. The authors introduce a new detection technique called Min-kNN Distance, which measures the similarity of generated prompts without needing access to the original model. Their experiments show that this method effectively identifies examples seen during training versus those that are unseen, outperforming previous detection methods."
                },
                "zh": {
                    "title": "å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼šæ£€æµ‹è¡Œä¸ºç‰¹å¾çš„æ–°æ–¹æ³•",
                    "desc": "å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨è®­ç»ƒç°ä»£æ¨ç†æ¨¡å‹ä¸­èµ·ç€æ ¸å¿ƒä½œç”¨ï¼Œä½†æœªå…¬å¼€çš„è®­ç»ƒæ•°æ®å¼•å‘äº†åŸºå‡†æ±¡æŸ“çš„æ‹…å¿§ã€‚ä¸é¢„è®­ç»ƒæ–¹æ³•ä¸åŒï¼ŒRLVRé€šè¿‡è‡ªç”Ÿæˆæ¨ç†è½¨è¿¹çš„å¥–åŠ±åé¦ˆæ¥å¾®è°ƒæ¨¡å‹ï¼Œä½¿å¾—ä¼ ç»Ÿçš„åŸºäºä¼¼ç„¶çš„æ£€æµ‹æ–¹æ³•æ•ˆæœä¸ä½³ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒRLVRä¼šäº§ç”Ÿç‹¬ç‰¹çš„è¡Œä¸ºç‰¹å¾ï¼šåœ¨RLVRè®­ç»ƒä¸­é‡åˆ°çš„æç¤ºä¼šå¯¼è‡´ç”Ÿæˆç»“æœæ›´åŠ åƒµåŒ–å’Œç›¸ä¼¼ï¼Œè€Œæœªè§è¿‡çš„æç¤ºåˆ™ä¿æŒæ›´å¤§çš„å¤šæ ·æ€§ã€‚æˆ‘ä»¬æå‡ºäº†Min-kNNè·ç¦»ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„é»‘ç®±æ£€æµ‹å™¨ï¼Œé€šè¿‡å¯¹ç»™å®šæç¤ºè¿›è¡Œå¤šæ¬¡ç”Ÿæˆå¹¶è®¡ç®—kä¸ªæœ€å°é‚»è¿‘ç¼–è¾‘è·ç¦»çš„å¹³å‡å€¼æ¥é‡åŒ–è¿™ç§å´©æºƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11543",
            "title": "Pretraining A Large Language Model using Distributed GPUs: A Memory-Efficient Decentralized Paradigm",
            "url": "https://huggingface.co/papers/2602.11543",
            "abstract": "A memory-efficient decentralized framework for training mixture-of-experts language models using sparse expert synchronization and expert-merging warm-up strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Pretraining large language models (LLMs) typically requires centralized clusters with thousands of high-memory GPUs (e.g., H100/A100). Recent decentralized training methods reduce communication overhead by employing federated optimization; however, they still need to train the entire model on each node, remaining constrained by GPU memory limitations. In this work, we propose SParse Expert Synchronization (SPES), a memory-efficient decentralized framework for pretraining mixture-of-experts (MoE) LLMs. SPES trains only a subset of experts per node, substantially lowering the memory footprint. Each node updates its local experts and periodically synchronizes with other nodes, eliminating full-parameter transmission while ensuring efficient knowledge sharing. To accelerate convergence, we introduce an expert-merging warm-up strategy, where experts exchange knowledge early in training, to rapidly establish foundational capabilities. With SPES, we train a 2B-parameter MoE LLM using 16 standalone 48GB GPUs over internet connections, which achieves competitive performance with centrally trained LLMs under similar computational budgets. We further demonstrate scalability by training a 7B model from scratch and a 9B model upcycled from a dense checkpoint, both of which match prior centralized baselines. Our code is available at https://github.com/zjr2000/SPES.",
            "score": 1,
            "issue_id": 1037,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "c4b1da298a9d4b9f",
            "authors": [
                "Jinrui Zhang",
                "Chaodong Xiao",
                "Aoqi Wu",
                "Xindong Zhang",
                "Lei Zhang"
            ],
            "affiliations": [
                "Department of Computing, The Hong Kong Polytechnic University",
                "OPPO Research Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11543.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ±ĞµĞ· ÑÑƒĞ¿ĞµÑ€ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° SPES â€” Ğ´ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ mixture-of-experts, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° GPU Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑƒĞ·Ğ»Ğµ. ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑƒĞ·ĞµĞ» Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹ Ğ¸ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ ÑƒĞ·Ğ»Ğ°Ğ¼Ğ¸, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ expert-merging warm-up, Ğ¿Ñ€Ğ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ MoE LLM Ñ 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğ° 16 GPU Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ 48GB Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚-ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ, Ğ´Ğ¾Ğ±Ğ¸Ğ²ÑˆĞ¸ÑÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ñ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶ĞµĞ¼ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Efficient Decentralized Training for Large Language Models",
                    "desc": "This paper presents a new decentralized framework called SParse Expert Synchronization (SPES) for training mixture-of-experts (MoE) language models in a memory-efficient way. Instead of requiring high-memory GPUs to train the entire model, SPES allows each node to train only a subset of experts, significantly reducing memory usage. The framework also incorporates a novel expert-merging warm-up strategy to enhance knowledge sharing among experts early in the training process. As a result, SPES enables the training of large language models with competitive performance using fewer resources and demonstrates scalability for even larger models."
                },
                "zh": {
                    "title": "å†…å­˜é«˜æ•ˆçš„å»ä¸­å¿ƒåŒ–æ··åˆä¸“å®¶è®­ç»ƒæ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSPESçš„å†…å­˜é«˜æ•ˆå»ä¸­å¿ƒåŒ–æ¡†æ¶ï¼Œç”¨äºè®­ç»ƒæ··åˆä¸“å®¶è¯­è¨€æ¨¡å‹ï¼ˆMoEï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šä»…è®­ç»ƒéƒ¨åˆ†ä¸“å®¶ï¼Œæ˜¾è‘—é™ä½äº†å†…å­˜å ç”¨ã€‚SPESè¿˜å¼•å…¥äº†ä¸“å®¶åˆå¹¶é¢„çƒ­ç­–ç•¥ï¼Œä¿ƒè¿›ä¸“å®¶ä¹‹é—´çš„çŸ¥è¯†å…±äº«ï¼ŒåŠ é€Ÿæ¨¡å‹æ”¶æ•›ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨16ä¸ªç‹¬ç«‹çš„48GB GPUä¸ŠæˆåŠŸè®­ç»ƒäº†ä¸€ä¸ª2Bå‚æ•°çš„MoE LLMï¼Œå¹¶å±•ç¤ºäº†å…¶ä¸ä¸­å¿ƒåŒ–è®­ç»ƒæ¨¡å‹çš„ç«äº‰æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11509",
            "title": "Multimodal Fact-Level Attribution for Verifiable Reasoning",
            "url": "https://huggingface.co/papers/2602.11509",
            "abstract": "MuRGAt is a benchmark for evaluating fact-level multimodal attribution in complex reasoning tasks, requiring models to provide precise citations for their answers across video, audio, and other modalities.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) are increasingly used for real-world tasks involving multi-step reasoning and long-form generation, where reliability requires grounding model outputs in heterogeneous input sources and verifying individual factual claims. However, existing multimodal grounding benchmarks and evaluation methods focus on simplified, observation-based scenarios or limited modalities and fail to assess attribution in complex multimodal reasoning. We introduce MuRGAt (Multimodal Reasoning with Grounded Attribution), a benchmark for evaluating fact-level multimodal attribution in settings that require reasoning beyond direct observation. Given inputs spanning video, audio, and other modalities, MuRGAt requires models to generate answers with explicit reasoning and precise citations, where each citation specifies both modality and temporal segments. To enable reliable assessment, we introduce an automatic evaluation framework that strongly correlates with human judgments. Benchmarking with human and automated scores reveals that even strong MLLMs frequently hallucinate citations despite correct reasoning. Moreover, we observe a key trade-off: increasing reasoning depth or enforcing structured grounding often degrades accuracy, highlighting a significant gap between internal reasoning and verifiable attribution.",
            "score": 1,
            "issue_id": 1037,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "fb98b68cbe4f4ff4",
            "authors": [
                "David Wan",
                "Han Wang",
                "Ziyang Wang",
                "Elias Stengel-Eskin",
                "Hyunji Lee",
                "Mohit Bansal"
            ],
            "affiliations": [
                "The University of Texas at Austin",
                "UNC Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11509.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#multimodal",
                    "#long_context",
                    "#hallucinations",
                    "#audio",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ’ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²",
                    "desc": "MuRGAt â€” ÑÑ‚Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑÑÑ‹Ğ»ĞºĞ¸ Ğ½Ğ° Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ñ‹ Ğ½Ğ° ÑƒĞ¿Ñ€Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¸ Ğ½Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ñ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ: ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ MLLM Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ†Ğ¸Ñ‚Ğ°Ñ‚Ğ°Ñ…, Ğ° ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "MuRGAt: Grounding Reasoning in Multimodal Attribution",
                    "desc": "MuRGAt is a new benchmark designed to evaluate how well models can attribute facts in complex reasoning tasks using multiple types of data, like video and audio. It challenges models to not only provide answers but also to cite their sources accurately, specifying the type of data and the exact time segments used. The benchmark reveals that even advanced multimodal large language models (MLLMs) often make mistakes in citing sources, even when their reasoning is correct. Additionally, the study highlights a trade-off where deeper reasoning can lead to less accurate citations, indicating a gap between a model's internal logic and its ability to provide verifiable information."
                },
                "zh": {
                    "title": "MuRGAtï¼šå¤šæ¨¡æ€æ¨ç†çš„ç²¾å‡†å½’å±è¯„ä¼°",
                    "desc": "MuRGAtæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤æ‚æ¨ç†ä»»åŠ¡ä¸­äº‹å®çº§å¤šæ¨¡æ€å½’å±çš„åŸºå‡†ã€‚å®ƒè¦æ±‚æ¨¡å‹åœ¨å¤„ç†è§†é¢‘ã€éŸ³é¢‘ç­‰å¤šç§æ¨¡æ€æ—¶ï¼Œæä¾›ç²¾ç¡®çš„å¼•ç”¨ä»¥æ”¯æŒå…¶ç­”æ¡ˆã€‚ç°æœ‰çš„å¤šæ¨¡æ€åŸºå‡†ä¸»è¦å…³æ³¨ç®€åŒ–çš„è§‚å¯Ÿåœºæ™¯ï¼Œæœªèƒ½æœ‰æ•ˆè¯„ä¼°å¤æ‚æ¨ç†ä¸­çš„å½’å±èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥è‡ªåŠ¨è¯„ä¼°æ¡†æ¶ï¼ŒMuRGAtæ­ç¤ºäº†å¼ºå¤§çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå¼•ç”¨æ—¶å¸¸å¸¸å‡ºç°å¹»è§‰ï¼Œå°½ç®¡æ¨ç†è¿‡ç¨‹æ­£ç¡®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.10575",
            "title": "MetaphorStar: Image Metaphor Understanding and Reasoning with End-to-End Visual Reinforcement Learning",
            "url": "https://huggingface.co/papers/2602.10575",
            "abstract": "MetaphorStar, an end-to-end visual reinforcement learning framework, significantly enhances metaphor comprehension in images through a specialized dataset, RL method, and benchmark, achieving state-of-the-art performance on multiple visual reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Metaphorical comprehension in images remains a critical challenge for Nowadays AI systems. While Multimodal Large Language Models (MLLMs) excel at basic Visual Question Answering (VQA), they consistently struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visual content. This difficulty stems from the task's demand for sophisticated multi-hop reasoning, cultural context, and Theory of Mind (ToM) capabilities, which current models lack. To fill this gap, we propose MetaphorStar, the first end-to-end visual reinforcement learning (RL) framework for image implication tasks. Our framework includes three core components: the fine-grained dataset TFQ-Data, the visual RL method TFQ-GRPO, and the well-structured benchmark TFQ-Bench.   Our fully open-source MetaphorStar family, trained using TFQ-GRPO on TFQ-Data, significantly improves performance by an average of 82.6% on the image implication benchmarks. Compared with 20+ mainstream MLLMs, MetaphorStar-32B achieves state-of-the-art (SOTA) on Multiple-Choice Question and Open-Style Question, significantly outperforms the top closed-source model Gemini-3.0-pro on True-False Question. Crucially, our experiments reveal that learning image implication tasks improves the general understanding ability, especially the complex visual reasoning ability. We further provide a systematic analysis of model parameter scaling, training data scaling, and the impact of different model architectures and training strategies, demonstrating the broad applicability of our method. We open-sourced all model weights, datasets, and method code at https://metaphorstar.github.io.",
            "score": 1,
            "issue_id": 1040,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "0087b0e3e9a0d38d",
            "authors": [
                "Chenhao Zhang",
                "Yazhe Niu",
                "Hongsheng Li"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10575.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#multimodal",
                    "#rl",
                    "#open_source",
                    "#optimization",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞĞ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ˜Ğ˜ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¼Ñ‹ÑĞ»Ñ‹: Ğ¼ĞµÑ‚Ğ°Ñ„Ğ¾Ñ€Ñ‹ Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "MetaphorStar â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ end-to-end Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ°Ñ„Ğ¾Ñ€ Ğ¸ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¼Ñ‹ÑĞ»Ğ¾Ğ² Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ TFQ-Data, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ TFQ-GRPO Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº TFQ-Bench Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ¼Ğ¿Ğ»Ğ¸Ñ†Ğ¸Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ° Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ MetaphorStar-32B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ¾Ğ»ĞµĞµ 20 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ°Ñ„Ğ¾Ñ€ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Unlocking Visual Metaphors with MetaphorStar",
                    "desc": "MetaphorStar is a novel visual reinforcement learning framework designed to improve the understanding of metaphors in images. It addresses the limitations of existing Multimodal Large Language Models (MLLMs) by incorporating a specialized dataset, TFQ-Data, and a unique reinforcement learning method, TFQ-GRPO. The framework has shown remarkable performance improvements, achieving state-of-the-art results on various visual reasoning tasks, particularly in complex multi-hop reasoning and contextual understanding. By open-sourcing its components, MetaphorStar aims to enhance the general capabilities of AI in interpreting nuanced visual content."
                },
                "zh": {
                    "title": "MetaphorStarï¼šæå‡å›¾åƒéšå–»ç†è§£çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
                    "desc": "MetaphorStaræ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„è§†è§‰å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¯¹å›¾åƒä¸­éšå–»çš„ç†è§£ã€‚å®ƒé€šè¿‡ä¸“é—¨çš„æ•°æ®é›†TFQ-Dataã€è§†è§‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•TFQ-GRPOå’Œç»“æ„è‰¯å¥½çš„åŸºå‡†TFQ-Benchï¼Œæ˜¾è‘—æå‡äº†å¤šé¡¹è§†è§‰æ¨ç†ä»»åŠ¡çš„è¡¨ç°ã€‚ä¸20å¤šç§ä¸»æµå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼ŒMetaphorStar-32Båœ¨å¤šé¡¹é€‰æ‹©é¢˜å’Œå¼€æ”¾å¼é—®é¢˜ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨çœŸä¼ªé—®é¢˜ä¸Šè¶…è¶Šäº†é¡¶å°–çš„é—­æºæ¨¡å‹Gemini-3.0-proã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå­¦ä¹ å›¾åƒéšå–»ä»»åŠ¡èƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹çš„æ•´ä½“ç†è§£èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å¤æ‚çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08277",
            "title": "PISCO: Precise Video Instance Insertion with Sparse Control",
            "url": "https://huggingface.co/papers/2602.08277",
            "abstract": "Video diffusion model PISCO enables precise instance insertion with sparse keyframe control through variable-information guidance and distribution-preserving temporal masking.  \t\t\t\t\tAI-generated summary \t\t\t\t The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and \"cherry-picking\" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: xiangbogaobarry.github.io/PISCO.",
            "score": 1,
            "issue_id": 1037,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "bca942d1fea9c8e1",
            "authors": [
                "Xiangbo Gao",
                "Renjie Li",
                "Xinghao Chen",
                "Yuheng Wu",
                "Suofei Feng",
                "Qing Yin",
                "Zhengzhong Tu"
            ],
            "affiliations": [
                "KAIST",
                "Stanford University",
                "Texas A&M University",
                "Visko Platform"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08277.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#benchmark",
                    "#video",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€ĞµĞ´ĞºĞ¸Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸",
                    "desc": "PISCO â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Variable-Information Guidance Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Distribution-Preserving Temporal Masking Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´, Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº PISCO-Bench Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "PISCO: Precision in Video Instance Insertion with Minimal Effort",
                    "desc": "The paper introduces PISCO, a video diffusion model designed for precise video instance insertion, allowing users to control the insertion of specific instances into existing footage with minimal effort. It addresses challenges in maintaining scene integrity and dynamics while ensuring accurate spatial-temporal placement and interaction of objects. PISCO employs Variable-Information Guidance and Distribution-Preserving Temporal Masking to enhance the robustness of the model under sparse keyframe conditions. The model is evaluated using a new benchmark, PISCO-Bench, demonstrating superior performance compared to traditional video editing methods, especially as more control signals are utilized."
                },
                "zh": {
                    "title": "PISCOï¼šç²¾å‡†è§†é¢‘å®ä¾‹æ’å…¥çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPISCOçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°ç²¾ç¡®çš„è§†é¢‘å®ä¾‹æ’å…¥ã€‚è¯¥æ¨¡å‹å…è®¸ç”¨æˆ·é€šè¿‡ç¨€ç–å…³é”®å¸§æ§åˆ¶ï¼Œçµæ´»åœ°æŒ‡å®šå…³é”®å¸§ï¼Œå¹¶è‡ªåŠ¨ä¼ æ’­å¯¹è±¡çš„å¤–è§‚ã€è¿åŠ¨å’Œäº¤äº’ã€‚PISCOå¼•å…¥äº†å¯å˜ä¿¡æ¯å¼•å¯¼å’Œåˆ†å¸ƒä¿æŒæ—¶é—´æ©è”½æŠ€æœ¯ï¼Œä»¥åº”å¯¹ç¨€ç–æ¡ä»¶ä¸‹çš„åˆ†å¸ƒåç§»ï¼Œç¡®ä¿ç”Ÿæˆçš„æ—¶é—´ç¨³å®šæ€§å’Œåœºæ™¯çš„çœŸå®é€‚åº”æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPISCOåœ¨ç¨€ç–æ§åˆ¶ä¸‹çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„è§†é¢‘ç¼–è¾‘æ–¹æ³•ï¼Œå¹¶éšç€æ§åˆ¶ä¿¡å·çš„å¢åŠ è€ŒæŒç»­æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.10585",
            "title": "Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity",
            "url": "https://huggingface.co/papers/2602.10585",
            "abstract": "Neural Additive Experts combines multiple specialized networks with a dynamic gating mechanism to balance predictive accuracy and feature interpretability in machine learning models.  \t\t\t\t\tAI-generated summary \t\t\t\t The trade-off between interpretability and accuracy remains a core challenge in machine learning. Standard Generalized Additive Models (GAMs) offer clear feature attributions but are often constrained by their strictly additive nature, which can limit predictive performance. Introducing feature interactions can boost accuracy yet may obscure individual feature contributions. To address these issues, we propose Neural Additive Experts (NAEs), a novel framework that seamlessly balances interpretability and accuracy. NAEs employ a mixture of experts framework, learning multiple specialized networks per feature, while a dynamic gating mechanism integrates information across features, thereby relaxing rigid additive constraints. Furthermore, we propose targeted regularization techniques to mitigate variance among expert predictions, facilitating a smooth transition from an exclusively additive model to one that captures intricate feature interactions while maintaining clarity in feature attributions. Our theoretical analysis and experiments on synthetic data illustrate the model's flexibility, and extensive evaluations on real-world datasets confirm that NAEs achieve an optimal balance between predictive accuracy and transparent, feature-level explanations. The code is available at https://github.com/Teddy-XiongGZ/NAE.",
            "score": 0,
            "issue_id": 1036,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "4c1b5ee68d982e38",
            "authors": [
                "Guangzhi Xiong",
                "Sanchit Sinha",
                "Aidong Zhang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2602.10585.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#open_source"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ñ‡ĞµÑ€ĞµĞ· ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ ÑĞ¼ĞµÑĞ¸",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Neural Additive Experts (NAE) â€” Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºÑƒÑ Ğ´Ğ¸Ğ»ĞµĞ¼Ğ¼Ñƒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ², Ğ³Ğ´Ğµ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸, Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ gating Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ°Ğ´Ğ´Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ NAE Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ²ĞºĞ»Ğ°Ğ´Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°."
                },
                "en": {
                    "title": "Balancing Accuracy and Interpretability with Neural Additive Experts",
                    "desc": "Neural Additive Experts (NAEs) is a new machine learning framework that combines multiple specialized networks to improve both accuracy and interpretability. It addresses the limitations of traditional Generalized Additive Models (GAMs) by allowing for feature interactions while still providing clear insights into feature contributions. NAEs use a dynamic gating mechanism to integrate information from different features, which helps to balance the trade-off between complex interactions and straightforward explanations. The framework has been tested on various datasets, showing that it can effectively enhance predictive performance without sacrificing transparency in feature attributions."
                },
                "zh": {
                    "title": "å¹³è¡¡å¯è§£é‡Šæ€§ä¸å‡†ç¡®æ€§çš„ç¥ç»åŠ æ€§ä¸“å®¶",
                    "desc": "ç¥ç»åŠ æ€§ä¸“å®¶ï¼ˆNAEsï¼‰ç»“åˆäº†å¤šä¸ªä¸“ä¸šç½‘ç»œå’ŒåŠ¨æ€é—¨æ§æœºåˆ¶ï¼Œä»¥å¹³è¡¡æœºå™¨å­¦ä¹ æ¨¡å‹çš„é¢„æµ‹å‡†ç¡®æ€§å’Œç‰¹å¾å¯è§£é‡Šæ€§ã€‚ä¼ ç»Ÿçš„å¹¿ä¹‰åŠ æ€§æ¨¡å‹ï¼ˆGAMsï¼‰è™½ç„¶æä¾›äº†æ¸…æ™°çš„ç‰¹å¾å½’å› ï¼Œä½†å…¶ä¸¥æ ¼çš„åŠ æ€§ç‰¹æ€§å¾€å¾€é™åˆ¶äº†é¢„æµ‹æ€§èƒ½ã€‚å¼•å…¥ç‰¹å¾äº¤äº’å¯ä»¥æé«˜å‡†ç¡®æ€§ï¼Œä½†å¯èƒ½ä¼šæ¨¡ç³Šå•ä¸ªç‰¹å¾çš„è´¡çŒ®ã€‚NAEsé€šè¿‡æ··åˆä¸“å®¶æ¡†æ¶ï¼Œå­¦ä¹ æ¯ä¸ªç‰¹å¾çš„å¤šä¸ªä¸“ä¸šç½‘ç»œï¼ŒåŒæ—¶åŠ¨æ€é—¨æ§æœºåˆ¶æ•´åˆç‰¹å¾é—´çš„ä¿¡æ¯ï¼Œä»è€Œæ”¾å®½äº†ä¸¥æ ¼çš„åŠ æ€§çº¦æŸã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-02-12.html",
    "link_next": "2026-02-16.html",
    "link_month": "2026-02.html",
    "short_date_prev": {
        "ru": "12.02",
        "en": "02/12",
        "zh": "2æœˆ12æ—¥"
    },
    "short_date_next": {
        "ru": "16.02",
        "en": "02/16",
        "zh": "2æœˆ16æ—¥"
    },
    "categories": {
        "#dataset": 6,
        "#data": 1,
        "#benchmark": 13,
        "#agents": 4,
        "#cv": 3,
        "#rl": 7,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 4,
        "#video": 3,
        "#multimodal": 11,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 7,
        "#healthcare": 0,
        "#training": 11,
        "#robotics": 3,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 4,
        "#reasoning": 11,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 3,
        "#optimization": 10,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 5,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 3,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 10,
        "#small_models": 2,
        "#science": 1,
        "#low_resource": 1
    }
}