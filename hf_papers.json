{
    "date": {
        "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 16",
        "zh": "1æœˆ16æ—¥"
    },
    "time_utc": "2025-01-16 06:14",
    "weekday": 3,
    "issue_id": 1698,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.08994",
            "title": "RepVideo: Rethinking Cross-Layer Representation for Video Generation",
            "url": "https://huggingface.co/papers/2501.08994",
            "abstract": "Video generation has achieved remarkable progress with the introduction of diffusion models, which have significantly improved the quality of generated videos. However, recent research has primarily focused on scaling up model training, while offering limited insights into the direct impact of representations on the video generation process. In this paper, we initially investigate the characteristics of features in intermediate layers, finding substantial variations in attention maps across different layers. These variations lead to unstable semantic representations and contribute to cumulative differences between features, which ultimately reduce the similarity between adjacent frames and negatively affect temporal coherence. To address this, we propose RepVideo, an enhanced representation framework for text-to-video diffusion models. By accumulating features from neighboring layers to form enriched representations, this approach captures more stable semantic information. These enhanced representations are then used as inputs to the attention mechanism, thereby improving semantic expressiveness while ensuring feature consistency across adjacent frames. Extensive experiments demonstrate that our RepVideo not only significantly enhances the ability to generate accurate spatial appearances, such as capturing complex spatial relationships between multiple objects, but also improves temporal consistency in video generation.",
            "score": 3,
            "issue_id": 1697,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "0d164d45ba2a5c71",
            "authors": [
                "Chenyang Si",
                "Weichen Fan",
                "Zhengyao Lv",
                "Ziqi Huang",
                "Yu Qiao",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University, Singapore, 639798",
                "Shanghai Artificial Intelligence Laboratory, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08994.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "RepVideo: ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RepVideo - ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¸ Ğ² ĞºĞ°Ñ€Ñ‚Ğ°Ñ… Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². RepVideo Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ· ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RepVideo Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Enhancing Video Generation with Stable Representations",
                    "desc": "This paper presents RepVideo, a new framework designed to improve video generation using text-to-video diffusion models. It identifies issues with unstable semantic representations caused by variations in attention maps across different layers of the model. By accumulating features from neighboring layers, RepVideo creates more stable and enriched representations that enhance the model's ability to maintain consistency between adjacent frames. The results show that RepVideo significantly improves both the spatial accuracy of generated videos and their temporal coherence, leading to more realistic video outputs."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘ç”Ÿæˆè´¨é‡çš„RepVideoæ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘ç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œæå‡ºäº†RepVideoæ¡†æ¶ä»¥æ”¹å–„è§†é¢‘ç”Ÿæˆçš„è´¨é‡ã€‚ç ”ç©¶å‘ç°ä¸­é—´å±‚ç‰¹å¾çš„æ³¨æ„åŠ›å›¾å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œè¿™å¯¼è‡´è¯­ä¹‰è¡¨ç¤ºçš„ä¸ç¨³å®šæ€§ï¼Œè¿›è€Œå½±å“ç›¸é‚»å¸§ä¹‹é—´çš„ç›¸ä¼¼æ€§å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚RepVideoé€šè¿‡ä»ç›¸é‚»å±‚ç´¯ç§¯ç‰¹å¾ï¼Œå½¢æˆæ›´ä¸°å¯Œçš„è¡¨ç¤ºï¼Œä»è€Œæ•æ‰æ›´ç¨³å®šçš„è¯­ä¹‰ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRepVideoæ˜¾è‘—æé«˜äº†ç”Ÿæˆè§†é¢‘çš„ç©ºé—´è¡¨ç°èƒ½åŠ›å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08983",
            "title": "CityDreamer4D: Compositional Generative Model of Unbounded 4D Cities",
            "url": "https://huggingface.co/papers/2501.08983",
            "abstract": "3D scene generation has garnered growing attention in recent years and has made significant progress. Generating 4D cities is more challenging than 3D scenes due to the presence of structurally complex, visually diverse objects like buildings and vehicles, and heightened human sensitivity to distortions in urban environments. To tackle these issues, we propose CityDreamer4D, a compositional generative model specifically tailored for generating unbounded 4D cities. Our main insights are 1) 4D city generation should separate dynamic objects (e.g., vehicles) from static scenes (e.g., buildings and roads), and 2) all objects in the 4D scene should be composed of different types of neural fields for buildings, vehicles, and background stuff. Specifically, we propose Traffic Scenario Generator and Unbounded Layout Generator to produce dynamic traffic scenarios and static city layouts using a highly compact BEV representation. Objects in 4D cities are generated by combining stuff-oriented and instance-oriented neural fields for background stuff, buildings, and vehicles. To suit the distinct characteristics of background stuff and instances, the neural fields employ customized generative hash grids and periodic positional embeddings as scene parameterizations. Furthermore, we offer a comprehensive suite of datasets for city generation, including OSM, GoogleEarth, and CityTopia. The OSM dataset provides a variety of real-world city layouts, while the Google Earth and CityTopia datasets deliver large-scale, high-quality city imagery complete with 3D instance annotations. Leveraging its compositional design, CityDreamer4D supports a range of downstream applications, such as instance editing, city stylization, and urban simulation, while delivering state-of-the-art performance in generating realistic 4D cities.",
            "score": 2,
            "issue_id": 1698,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "39cd0826d4232170",
            "authors": [
                "Haozhe Xie",
                "Zhaoxi Chen",
                "Fangzhou Hong",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University, Singapore 637335"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08983.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#dataset"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 4D-Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸ĞºĞ¸",
                    "desc": "CityDreamer4D - ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… 4D-Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ¾Ğ². ĞĞ½Ğ° Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ°) Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½ (Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¹, Ğ´Ğ¾Ñ€Ğ¾Ğ³). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¹, Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ° Ğ¸ Ñ„Ğ¾Ğ½Ğ°, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ…ĞµÑˆ-ÑĞµÑ‚ĞºĞ¸ Ğ¸ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸. CityDreamer4D Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… 4D-Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "Revolutionizing Urban Landscapes: CityDreamer4D for Dynamic City Generation",
                    "desc": "This paper introduces CityDreamer4D, a generative model designed for creating unbounded 4D cities, which include both static and dynamic elements. The model distinguishes between dynamic objects like vehicles and static structures such as buildings, using specialized neural fields for each type. It employs a compact bird's-eye view (BEV) representation to generate realistic traffic scenarios and city layouts. Additionally, the paper provides extensive datasets for training, enabling various applications like instance editing and urban simulation while achieving high-quality results in 4D city generation."
                },
                "zh": {
                    "title": "CityDreamer4Dï¼šæ— é™4DåŸå¸‚ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "è¿‘å¹´æ¥ï¼Œ3Dåœºæ™¯ç”Ÿæˆå—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œå¹¶å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç”Ÿæˆ4DåŸå¸‚æ¯”3Dåœºæ™¯æ›´å…·æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºåŸå¸‚ç¯å¢ƒä¸­å­˜åœ¨ç»“æ„å¤æ‚ã€è§†è§‰å¤šæ ·çš„ç‰©ä½“ï¼Œå¦‚å»ºç­‘å’Œè½¦è¾†ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CityDreamer4Dï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ç”¨äºç”Ÿæˆæ— é™4DåŸå¸‚çš„ç»„åˆç”Ÿæˆæ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡å°†åŠ¨æ€ç‰©ä½“ä¸é™æ€åœºæ™¯åˆ†ç¦»ï¼Œå¹¶ä½¿ç”¨ä¸åŒç±»å‹çš„ç¥ç»åœºæ¥ç»„åˆåŸå¸‚ä¸­çš„æ‰€æœ‰ç‰©ä½“ï¼Œä»è€Œå®ç°é«˜è´¨é‡çš„åŸå¸‚ç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08828",
            "title": "MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents",
            "url": "https://huggingface.co/papers/2501.08828",
            "abstract": "Multi-modal document retrieval is designed to identify and retrieve various forms of multi-modal content, such as figures, tables, charts, and layout information from extensive documents. Despite its significance, there is a notable lack of a robust benchmark to effectively evaluate the performance of systems in multi-modal document retrieval. To address this gap, this work introduces a new benchmark, named as MMDocIR, encompassing two distinct tasks: page-level and layout-level retrieval. The former focuses on localizing the most relevant pages within a long document, while the latter targets the detection of specific layouts, offering a more fine-grained granularity than whole-page analysis. A layout can refer to a variety of elements such as textual paragraphs, equations, figures, tables, or charts. The MMDocIR benchmark comprises a rich dataset featuring expertly annotated labels for 1,685 questions and bootstrapped labels for 173,843 questions, making it a pivotal resource for advancing multi-modal document retrieval for both training and evaluation. Through rigorous experiments, we reveal that (i) visual retrievers significantly outperform their text counterparts, (ii) MMDocIR train set can effectively benefit the training process of multi-modal document retrieval and (iii) text retrievers leveraging on VLM-text perform much better than those using OCR-text. These findings underscores the potential advantages of integrating visual elements for multi-modal document retrieval.",
            "score": 2,
            "issue_id": 1698,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "bf9a6df8fecd4ec1",
            "authors": [
                "Kuicai Dong",
                "Yujing Chang",
                "Xin Deik Goh",
                "Dexun Li",
                "Ruiming Tang",
                "Yong Liu"
            ],
            "affiliations": [
                "Noahs Ark Lab, Huawei"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08828.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "MMDocIR: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MMDocIR Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ¿Ğ¾Ğ¸ÑĞº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¼Ğ°ĞºĞµÑ‚Ğ¾Ğ². Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ 1,685 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚ĞºĞ¸ Ğ´Ğ»Ñ 173,843 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²ĞµÑ€Ñ‹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ, Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ñ‡ĞµĞ¼ OCR-Ñ‚ĞµĞºÑÑ‚."
                },
                "en": {
                    "title": "Unlocking Multi-Modal Document Retrieval with MMDocIR",
                    "desc": "This paper addresses the challenge of multi-modal document retrieval, which involves finding various types of content like figures and tables in large documents. It introduces a new benchmark called MMDocIR, which includes two tasks: page-level retrieval for finding relevant pages and layout-level retrieval for identifying specific layouts within those pages. The benchmark is supported by a comprehensive dataset with thousands of annotated questions, facilitating better training and evaluation of retrieval systems. The results show that visual retrieval methods outperform text-based methods, highlighting the importance of incorporating visual information in multi-modal retrieval tasks."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€æ–‡æ¡£æ£€ç´¢çš„æ–°åŸºå‡†MMDocIR",
                    "desc": "å¤šæ¨¡æ€æ–‡æ¡£æ£€ç´¢æ—¨åœ¨ä»å¤§é‡æ–‡æ¡£ä¸­è¯†åˆ«å’Œæå–å„ç§å½¢å¼çš„å†…å®¹ï¼Œå¦‚å›¾å½¢ã€è¡¨æ ¼ã€å›¾è¡¨å’Œå¸ƒå±€ä¿¡æ¯ã€‚å°½ç®¡å…¶é‡è¦æ€§æ˜¾è‘—ï¼Œä½†ç›®å‰ç¼ºä¹æœ‰æ•ˆè¯„ä¼°å¤šæ¨¡æ€æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿæ€§èƒ½çš„åŸºå‡†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†MMDocIRï¼ŒåŒ…å«é¡µé¢çº§å’Œå¸ƒå±€çº§æ£€ç´¢ä¸¤ä¸ªä»»åŠ¡ã€‚é€šè¿‡ä¸¥æ ¼çš„å®éªŒï¼Œæˆ‘ä»¬å‘ç°è§†è§‰æ£€ç´¢å™¨çš„è¡¨ç°æ˜¾è‘—ä¼˜äºæ–‡æœ¬æ£€ç´¢å™¨ï¼Œä¸”MMDocIRè®­ç»ƒé›†èƒ½æœ‰æ•ˆä¿ƒè¿›å¤šæ¨¡æ€æ–‡æ¡£æ£€ç´¢çš„è®­ç»ƒè¿‡ç¨‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08809",
            "title": "XMusic: Towards a Generalized and Controllable Symbolic Music Generation Framework",
            "url": "https://huggingface.co/papers/2501.08809",
            "abstract": "In recent years, remarkable advancements in artificial intelligence-generated content (AIGC) have been achieved in the fields of image synthesis and text generation, generating content comparable to that produced by humans. However, the quality of AI-generated music has not yet reached this standard, primarily due to the challenge of effectively controlling musical emotions and ensuring high-quality outputs. This paper presents a generalized symbolic music generation framework, XMusic, which supports flexible prompts (i.e., images, videos, texts, tags, and humming) to generate emotionally controllable and high-quality symbolic music. XMusic consists of two core components, XProjector and XComposer. XProjector parses the prompts of various modalities into symbolic music elements (i.e., emotions, genres, rhythms and notes) within the projection space to generate matching music. XComposer contains a Generator and a Selector. The Generator generates emotionally controllable and melodious music based on our innovative symbolic music representation, whereas the Selector identifies high-quality symbolic music by constructing a multi-task learning scheme involving quality assessment, emotion recognition, and genre recognition tasks. In addition, we build XMIDI, a large-scale symbolic music dataset that contains 108,023 MIDI files annotated with precise emotion and genre labels. Objective and subjective evaluations show that XMusic significantly outperforms the current state-of-the-art methods with impressive music quality. Our XMusic has been awarded as one of the nine Highlights of Collectibles at WAIC 2023. The project homepage of XMusic is https://xmusic-project.github.io.",
            "score": 1,
            "issue_id": 1697,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "d4d018c9adb2579c",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#audio",
                    "#story_generation",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "XMusic: Ğ˜Ğ˜-ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸ÑĞ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ XMusic - Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². XMusic ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: XProjector Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ XComposer Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ XMIDI, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 100 Ñ‚Ñ‹ÑÑÑ‡ MIDI-Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¶Ğ°Ğ½Ñ€Ğ¾Ğ². Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼, XMusic Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸."
                },
                "en": {
                    "title": "XMusic: Emotionally Controlled Music Generation Made Easy!",
                    "desc": "This paper introduces XMusic, a new framework for generating symbolic music that can be controlled by emotional prompts. It includes two main components: XProjector, which converts various input types into musical elements, and XComposer, which generates and selects high-quality music. The framework uses a multi-task learning approach to ensure the generated music meets quality, emotional, and genre standards. Additionally, the authors created a large dataset, XMIDI, to support their research and demonstrate that XMusic outperforms existing methods in music generation."
                },
                "zh": {
                    "title": "XMusicï¼šæƒ…æ„Ÿå¯æ§çš„é«˜è´¨é‡éŸ³ä¹ç”Ÿæˆ",
                    "desc": "è¿‘å¹´æ¥ï¼Œäººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰åœ¨å›¾åƒåˆæˆå’Œæ–‡æœ¬ç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨éŸ³ä¹ç”Ÿæˆæ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨çš„ç¬¦å·éŸ³ä¹ç”Ÿæˆæ¡†æ¶XMusicï¼Œèƒ½å¤Ÿé€šè¿‡çµæ´»çš„æç¤ºç”Ÿæˆå¯æ§æƒ…æ„Ÿå’Œé«˜è´¨é‡çš„ç¬¦å·éŸ³ä¹ã€‚XMusicç”±ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ç»„æˆï¼šXProjectorå’ŒXComposerï¼Œå‰è€…å°†å¤šç§æ¨¡æ€çš„æç¤ºè§£æä¸ºéŸ³ä¹å…ƒç´ ï¼Œåè€…åˆ™ç”Ÿæˆå’Œé€‰æ‹©é«˜è´¨é‡çš„éŸ³ä¹ã€‚é€šè¿‡æ„å»ºå¤§è§„æ¨¡çš„XMIDIæ•°æ®é›†å’Œå¤šä»»åŠ¡å­¦ä¹ æ–¹æ¡ˆï¼ŒXMusicåœ¨éŸ³ä¹è´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09019",
            "title": "Ouroboros-Diffusion: Exploring Consistent Content Generation in Tuning-free Long Video Diffusion",
            "url": "https://huggingface.co/papers/2501.09019",
            "abstract": "The first-in-first-out (FIFO) video diffusion, built on a pre-trained text-to-video model, has recently emerged as an effective approach for tuning-free long video generation. This technique maintains a queue of video frames with progressively increasing noise, continuously producing clean frames at the queue's head while Gaussian noise is enqueued at the tail. However, FIFO-Diffusion often struggles to keep long-range temporal consistency in the generated videos due to the lack of correspondence modeling across frames. In this paper, we propose Ouroboros-Diffusion, a novel video denoising framework designed to enhance structural and content (subject) consistency, enabling the generation of consistent videos of arbitrary length. Specifically, we introduce a new latent sampling technique at the queue tail to improve structural consistency, ensuring perceptually smooth transitions among frames. To enhance subject consistency, we devise a Subject-Aware Cross-Frame Attention (SACFA) mechanism, which aligns subjects across frames within short segments to achieve better visual coherence. Furthermore, we introduce self-recurrent guidance. This technique leverages information from all previous cleaner frames at the front of the queue to guide the denoising of noisier frames at the end, fostering rich and contextual global information interaction. Extensive experiments of long video generation on the VBench benchmark demonstrate the superiority of our Ouroboros-Diffusion, particularly in terms of subject consistency, motion smoothness, and temporal consistency.",
            "score": 0,
            "issue_id": 1697,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "c4c991699f684865",
            "authors": [
                "Jingyuan Chen",
                "Fuchen Long",
                "Jie An",
                "Zhaofan Qiu",
                "Ting Yao",
                "Jiebo Luo",
                "Tao Mei"
            ],
            "affiliations": [
                "HiDream.ai Inc.",
                "University of Rochester, Rochester, NY USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09019.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#video",
                    "#long_context",
                    "#diffusion"
                ],
                "emoji": "ğŸ",
                "ru": {
                    "title": "Ğ‘ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾: Ouroboros-Diffusion Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ouroboros-Diffusion. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ğ¸ ÑÑĞ¶ĞµÑ‚Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Subject-Aware Cross-Frame Attention. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ°Ğ¼Ğ¾Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰ĞµĞµÑÑ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ĞµĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ VBench Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ouroboros-Diffusion Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Ouroboros-Diffusion: Enhancing Long Video Consistency and Coherence",
                    "desc": "The paper introduces Ouroboros-Diffusion, a new framework for improving long video generation using a pre-trained text-to-video model. It addresses the limitations of FIFO-Diffusion, particularly in maintaining long-range temporal consistency across video frames. The proposed method enhances structural consistency through a novel latent sampling technique and improves subject consistency with a Subject-Aware Cross-Frame Attention mechanism. Additionally, self-recurrent guidance is implemented to utilize information from previous frames, resulting in videos with better visual coherence and smoother transitions."
                },
                "zh": {
                    "title": "Ouroboros-Diffusionï¼šæå‡è§†é¢‘ç”Ÿæˆä¸€è‡´æ€§çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "FIFOè§†é¢‘æ‰©æ•£æ˜¯ä¸€ç§åŸºäºé¢„è®­ç»ƒæ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹çš„é•¿è§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼Œä½†åœ¨ç”Ÿæˆè§†é¢‘æ—¶å¸¸å¸¸ç¼ºä¹é•¿æ—¶é—´çš„ä¸€è‡´æ€§ã€‚æœ¬æ–‡æå‡ºäº†Ouroboros-Diffusionæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥æ–°çš„æ½œåœ¨é‡‡æ ·æŠ€æœ¯å’Œä¸»é¢˜æ„ŸçŸ¥è·¨å¸§æ³¨æ„æœºåˆ¶ï¼Œå¢å¼ºäº†è§†é¢‘çš„ç»“æ„å’Œå†…å®¹ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•ç¡®ä¿äº†å¸§ä¹‹é—´çš„å¹³æ»‘è¿‡æ¸¡ï¼Œå¹¶é€šè¿‡è‡ªé€’å½’å¼•å¯¼æŠ€æœ¯åˆ©ç”¨å‰é¢æ¸…æ™°å¸§çš„ä¿¡æ¯æ¥æ”¹å–„åé¢å™ªå£°å¸§çš„å»å™ªæ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOuroboros-Diffusionåœ¨ä¸»é¢˜ä¸€è‡´æ€§ã€è¿åŠ¨å¹³æ»‘æ€§å’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-15.html",
    "link_next": "2025-01-17.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "15.01",
        "en": "01/15",
        "zh": "1æœˆ15æ—¥"
    },
    "short_date_next": {
        "ru": "17.01",
        "en": "01/17",
        "zh": "1æœˆ17æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "æˆ‘ä»¬ä»‹ç»äº† MiniMax-01 ç³»åˆ—ï¼ŒåŒ…æ‹¬ MiniMax-Text-01 å’Œ MiniMax-VL-01ã€‚è¿™äº›æ¨¡å‹åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ–¹é¢å…·æœ‰å“è¶Šèƒ½åŠ›ã€‚æ ¸å¿ƒåœ¨äºé—ªç”µæ³¨æ„åŠ›å’Œå…¶é«˜æ•ˆæ‰©å±•ã€‚æˆ‘ä»¬å°†å…¶ä¸æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰é›†æˆï¼Œåˆ›å»ºäº†ä¸€ä¸ªå…·æœ‰ 32 ä¸ªä¸“å®¶å’Œ 4560 äº¿æ€»å‚æ•°çš„æ¨¡å‹ã€‚æˆ‘ä»¬å¼€å‘äº†ä¼˜åŒ–çš„å¹¶è¡Œç­–ç•¥å’Œé«˜æ•ˆçš„è®¡ç®—é€šä¿¡é‡å æŠ€æœ¯ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨æ•°ç™¾äº¿å‚æ•°çš„æ¨¡å‹ä¸Šè¿›è¡Œé«˜æ•ˆè®­ç»ƒå’Œæ¨ç†ã€‚MiniMax-Text-01 çš„ä¸Šä¸‹æ–‡çª—å£åœ¨è®­ç»ƒæœŸé—´å¯è¾¾åˆ° 100 ä¸‡ä¸ªæ ‡è®°ï¼Œå¹¶åœ¨æ¨ç†æœŸé—´æ‰©å±•åˆ° 400 ä¸‡ä¸ªæ ‡è®°ã€‚MiniMax-VL-01 é€šè¿‡ä½¿ç”¨ 5120 äº¿è§†è§‰è¯­è¨€æ ‡è®°è¿›è¡ŒæŒç»­è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ ‡å‡†å’Œå†…éƒ¨åŸºå‡†ä¸Šçš„æ€§èƒ½ä¸ GPT-4o å’Œ Claude-3.5-Sonnet ç›¸å½“ï¼ŒåŒæ—¶æä¾› 20-32 å€çš„ä¸Šä¸‹æ–‡çª—å£ã€‚æˆ‘ä»¬åœ¨ https://github.com/MiniMax-AI å…¬å¼€å‘å¸ƒäº† MiniMax-01ã€‚",
        "title": "MiniMax-01: Scaling Foundation Models with Lightning Attention",
        "pinyin": "WÇ’men jiÃ¨shÃ o le MiniMax-01 xÃ¬liÃ¨, bÄokuÃ² MiniMax-Text-01 hÃ© MiniMax-VL-01. ZhÃ¨xiÄ“ mÃ³xÃ­ng zÃ i chÇ”lÇ chÃ¡ng shÃ ngxÃ¬awÃ©n fÄngmiÃ n jÃ¹yÇ’u zhuÃ³yuÃ¨ nÃ©nglÃ¬. HÃ©xÄ«n zÃ iyÃº shÇndiÇn zhÃ¹yÃ¬lÃ¬ hÃ© qÃ­ gÄoxiÃ o kuÃ²zhÇn. WÇ’men jiÄng qÃ­ yÇ” hÃ¹n hÃ© zhuÄnjiÄ mÃ³xÃ­ng (MoE) jÃ­chÃ©ng, chuÃ ngjiÃ n le yÄ«gÃ¨ jÃ¹yÇ’u 32 gÃ¨ zhuÄnjiÄ hÃ© 4560 yÃ¬ zÇ’ng cÄnshÃ¹ de mÃ³xÃ­ng. WÇ’men kÄifÄ le yÅuhuÃ  de bÃ¬ngxÃ­ng cÃ¨lÃ¼Ã¨ hÃ© gÄoxiÃ o de jÃ¬suÃ n tÅngxÃ¬n zhÃ²ngdiÃ© jÃ¬shÃ¹. ZhÃ¨ shÇ wÇ’men nÃ©nggÃ²u zÃ i shÃ¹bÇiyÃ¬ cÄnshÃ¹ de mÃ³xÃ­ng shÃ ng jÃ¬nxÃ­ng gÄoxiÃ o xÃ¹nliÃ n hÃ© tuÃ¬lÇ. MiniMax-Text-01 de shÃ ngxÃ¬awÃ©n chuÄngkÇ’u zÃ i xÃ¹nliÃ n qÄ«jiÄn kÄ› dÃ¡dÃ o 100 wÃ n gÃ¨ biÄojÃ¬, bÃ¬ng zÃ i tuÃ¬lÇ qÄ«jiÄn kuÃ²zhÇn dÃ o 400 wÃ n gÃ¨ biÄojÃ¬. MiniMax-VL-01 tÅngguÃ² shÇyÃ²ng 5120 yÃ¬ shÃ¬juÃ© yÇ”yÃ¡n biÄojÃ¬ jÃ¬nxÃ­ng chÃ­xÃ¹ xÃ¹nliÃ n. ShÃ¬yÃ n biÇomÃ­ng, wÇ’men de mÃ³xÃ­ng zÃ i biÄozhÇ”n hÃ© nÃ¨ibÃ¹ jÄ«zhÇ”n shÃ ng de xiÃ onÃ©nglÃ¬ yÇ” GPT-4o hÃ© Claude-3.5-Sonnet xiÄngdÄng, tÃ³ngshÃ­ tÃ­gÅng 20-32 bÃ¨i de shÃ ngxÃ¬awÃ©n chuÄngkÇ’u. WÇ’men zÃ i https://github.com/MiniMax-AI gÅngkÄi fÄbÃ¹ le MiniMax-01.",
        "vocab": "[\n    {\"word\": \"ä»‹ç»\", \"pinyin\": \"jiÃ¨ shÃ o\", \"trans\": \"introduce\"},\n    {\"word\": \"ç³»åˆ—\", \"pinyin\": \"xÃ¬ liÃ¨\", \"trans\": \"series\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"å¤„ç†\", \"pinyin\": \"chÇ” lÇ\", \"trans\": \"process\"},\n    {\"word\": \"ä¸Šä¸‹æ–‡\", \"pinyin\": \"shÃ ng xiÃ  wÃ©n\", \"trans\": \"context\"},\n    {\"word\": \"å“è¶Š\", \"pinyin\": \"zhuÃ³ yuÃ¨\", \"trans\": \"outstanding\"},\n    {\"word\": \"èƒ½åŠ›\", \"pinyin\": \"nÃ©ng lÃ¬\", \"trans\": \"ability\"},\n    {\"word\": \"æ ¸å¿ƒ\", \"pinyin\": \"hÃ© xÄ«n\", \"trans\": \"core\"},\n    {\"word\": \"é—ªç”µ\", \"pinyin\": \"shÇn diÃ n\", \"trans\": \"lightning\"},\n    {\"word\": \"æ³¨æ„åŠ›\", \"pinyin\": \"zhÃ¹ yÃ¬ lÃ¬\", \"trans\": \"attention\"},\n    {\"word\": \"é«˜æ•ˆ\", \"pinyin\": \"gÄo xiÃ o\", \"trans\": \"efficient\"},\n    {\"word\": \"æ‰©å±•\", \"pinyin\": \"kuÃ² zhÇn\", \"trans\": \"expand\"},\n    {\"word\": \"æ··åˆ\", \"pinyin\": \"hÃ¹n hÃ©\", \"trans\": \"hybrid\"},\n    {\"word\": \"ä¸“å®¶\", \"pinyin\": \"zhuÄn jiÄ\", \"trans\": \"expert\"},\n    {\"word\": \"é›†æˆ\", \"pinyin\": \"jÃ­ chÃ©ng\", \"trans\": \"integrate\"},\n    {\"word\": \"å¹¶è¡Œ\", \"pinyin\": \"bÃ¬ng xÃ­ng\", \"trans\": \"parallel\"},\n    {\"word\": \"ç­–ç•¥\", \"pinyin\": \"cÃ¨ lÃ¼Ã¨\", \"trans\": \"strategy\"},\n    {\"word\": \"é€šä¿¡\", \"pinyin\": \"tÅng xÃ¬n\", \"trans\": \"communication\"},\n    {\"word\": \"é‡å \", \"pinyin\": \"chÃ³ng diÃ©\", \"trans\": \"overlap\"},\n    {\"word\": \"æŠ€æœ¯\", \"pinyin\": \"jÃ¬ shÃ¹\", \"trans\": \"technology\"},\n    {\"word\": \"è®­ç»ƒ\", \"pinyin\": \"xÃ¹n liÃ n\", \"trans\": \"train\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"inference\"},\n    {\"word\": \"çª—å£\", \"pinyin\": \"chuÄng kÇ’u\", \"trans\": \"window\"},\n    {\"word\": \"æ ‡è®°\", \"pinyin\": \"biÄo jÃ¬\", \"trans\": \"token\"},\n    {\"word\": \"è§†è§‰\", \"pinyin\": \"shÃ¬ juÃ©\", \"trans\": \"visual\"},\n    {\"word\": \"è¯­è¨€\", \"pinyin\": \"yÇ” yÃ¡n\", \"trans\": \"language\"},\n    {\"word\": \"æŒç»­\", \"pinyin\": \"chÃ­ xÃ¹\", \"trans\": \"continuous\"},\n    {\"word\": \"å®éªŒ\", \"pinyin\": \"shÃ­ yÃ n\", \"trans\": \"experiment\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ¬ng nÃ©ng\", \"trans\": \"performance\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ« zhÇ”n\", \"trans\": \"benchmark\"},\n    {\"word\": \"å…¬å¼€\", \"pinyin\": \"gÅng kÄi\", \"trans\": \"public\"},\n    {\"word\": \"å‘å¸ƒ\", \"pinyin\": \"fÄ bÃ¹\", \"trans\": \"release\"}\n]",
        "trans": "We introduced the MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01. These models excel in handling long contexts, with a core focus on flash attention and its efficient scaling. We integrated them with a Mixture of Experts (MoE) model, creating a model with 32 experts and a total of 4560 billion parameters. We developed optimized parallel strategies and efficient computation-communication overlap techniques. This enables us to perform efficient training and inference on models with hundreds of billions of parameters. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and expands to 4 million tokens during inference. MiniMax-VL-01 undergoes continuous training using 5120 billion vision-language tokens. Experiments show that our models perform comparably to GPT-4o and Claude-3.5-Sonnet on standard and internal benchmarks while providing a 20-32 times larger context window. We have made MiniMax-01 publicly available at https://github.com/MiniMax-AI.",
        "update_ts": "2025-01-15 09:11"
    }
}