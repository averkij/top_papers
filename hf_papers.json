{
    "date": {
        "ru": "15 Ğ¼Ğ°Ñ",
        "en": "May 15",
        "zh": "5æœˆ15æ—¥"
    },
    "time_utc": "2025-05-15 03:37",
    "weekday": 3,
    "issue_id": 3769,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.09568",
            "title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,\n  Training and Dataset",
            "url": "https://huggingface.co/papers/2505.09568",
            "abstract": "Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain underexplored. Motivated by the strong potential of autoregressive and diffusion models for high-quality generation and scalability, we conduct a comprehensive study of their use in unified multimodal settings, with emphasis on image representations, modeling objectives, and training strategies. Grounded in these investigations, we introduce a novel approach that employs a diffusion transformer to generate semantically rich CLIP image features, in contrast to conventional VAE-based representations. This design yields both higher training efficiency and improved generative quality. Furthermore, we demonstrate that a sequential pretraining strategy for unified models-first training on image understanding and subsequently on image generation-offers practical advantages by preserving image understanding capability while developing strong image generation ability. Finally, we carefully curate a high-quality instruction-tuning dataset BLIP3o-60k for image generation by prompting GPT-4o with a diverse set of captions covering various scenes, objects, human gestures, and more. Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models. BLIP3-o achieves superior performance across most of the popular benchmarks spanning both image understanding and generation tasks. To facilitate future research, we fully open-source our models, including code, model weights, training scripts, and pretraining and instruction tuning datasets.",
            "score": 4,
            "issue_id": 3768,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ",
                "en": "May 14",
                "zh": "5æœˆ14æ—¥"
            },
            "hash": "822f8dd79d39211b",
            "authors": [
                "Jiuhai Chen",
                "Zhiyang Xu",
                "Xichen Pan",
                "Yushi Hu",
                "Can Qin",
                "Tom Goldstein",
                "Lifu Huang",
                "Tianyi Zhou",
                "Saining Xie",
                "Silvio Savarese",
                "Le Xue",
                "Caiming Xiong",
                "Ran Xu"
            ],
            "affiliations": [
                "New York University",
                "Salesforce Research",
                "UC Davis",
                "University of Maryland",
                "University of Washington",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09568.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#diffusion",
                    "#open_source",
                    "#multimodal",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… CLIP-Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… VAE-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ BLIP3-o, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ°Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unifying Image Understanding and Generation with BLIP3-o",
                    "desc": "This paper explores the integration of image understanding and generation in multimodal models, focusing on the use of autoregressive and diffusion models. The authors propose a new architecture that utilizes a diffusion transformer to create high-quality CLIP image features, which enhances both training efficiency and generative quality compared to traditional VAE methods. They introduce a sequential pretraining strategy that first develops image understanding before transitioning to image generation, ensuring that both capabilities are effectively preserved. Additionally, they present a curated dataset, BLIP3o-60k, for instruction tuning, which supports the development of their state-of-the-art unified multimodal model, BLIP3-o, achieving top performance on various benchmarks."
                },
                "zh": {
                    "title": "ç»Ÿä¸€å›¾åƒç†è§£ä¸ç”Ÿæˆçš„åˆ›æ–°æ¨¡å‹",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å›¾åƒç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€æ¨¡å‹ï¼Œå¼ºè°ƒäº†è‡ªå›å½’å’Œæ‰©æ•£æ¨¡å‹åœ¨é«˜è´¨é‡ç”Ÿæˆä¸­çš„æ½œåŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œä½¿ç”¨æ‰©æ•£å˜æ¢å™¨ç”Ÿæˆè¯­ä¹‰ä¸°å¯Œçš„CLIPå›¾åƒç‰¹å¾ï¼Œæå‡äº†è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆè´¨é‡ã€‚é€šè¿‡å…ˆè¿›è¡Œå›¾åƒç†è§£è®­ç»ƒï¼Œå†è¿›è¡Œå›¾åƒç”Ÿæˆè®­ç»ƒçš„é¡ºåºé¢„è®­ç»ƒç­–ç•¥ï¼Œä¿æŒäº†å›¾åƒç†è§£èƒ½åŠ›çš„åŒæ—¶å¢å¼ºäº†ç”Ÿæˆèƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ›å»ºäº†é«˜è´¨é‡çš„æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†BLIP3o-60kï¼Œä»¥æ”¯æŒå›¾åƒç”Ÿæˆä»»åŠ¡çš„ç ”ç©¶ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-14.html",
    "link_next": "2025-05-16.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "14.05",
        "en": "05/14",
        "zh": "5æœˆ14æ—¥"
    },
    "short_date_next": {
        "ru": "16.05",
        "en": "05/16",
        "zh": "5æœˆ16æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "æˆ‘ä»¬ä»‹ç»äº†MiniMax-Speechï¼Œä¸€ç§åŸºäºTransformerçš„æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ã€‚å®ƒèƒ½ç”Ÿæˆé«˜è´¨é‡çš„è¯­éŸ³ã€‚æ¨¡å‹çš„å…³é”®åˆ›æ–°æ˜¯å¯å­¦ä¹ çš„æ¼”è®²è€…ç¼–ç å™¨ï¼Œä»å‚è€ƒéŸ³é¢‘ä¸­æå–éŸ³è‰²ç‰¹å¾ï¼Œæ— éœ€è½¬å½•ã€‚è¿™ä½¿å¾—MiniMax-Speechèƒ½åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹ç”Ÿæˆè¡¨ç°åŠ›å¼ºã€éŸ³è‰²ä¸€è‡´çš„è¯­éŸ³ï¼Œå¹¶æ”¯æŒä¸€æ ·æœ¬å£°éŸ³å…‹éš†ã€‚é€šè¿‡Flow-VAEï¼ŒåˆæˆéŸ³é¢‘çš„æ•´ä½“è´¨é‡å¾—åˆ°æå‡ã€‚æ¨¡å‹æ”¯æŒ32ç§è¯­è¨€ï¼Œåœ¨å¤šç§è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚",
        "title": "MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with a Learnable\n  Speaker Encoder",
        "pinyin": "æˆ‘ä»¬ä»‹ç»äº†MiniMax-Speechï¼Œä¸€ç§åŸºäºTransformerçš„æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ã€‚å®ƒèƒ½ç”Ÿæˆé«˜è´¨é‡çš„è¯­éŸ³ã€‚æ¨¡å‹çš„å…³é”®åˆ›æ–°æ˜¯å¯å­¦ä¹ çš„æ¼”è®²è€…ç¼–ç å™¨ï¼Œä»å‚è€ƒéŸ³é¢‘ä¸­æå–éŸ³è‰²ç‰¹å¾ï¼Œæ— éœ€è½¬å½•ã€‚è¿™ä½¿å¾—MiniMax-Speechèƒ½åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹ç”Ÿæˆè¡¨ç°åŠ›å¼ºã€éŸ³è‰²ä¸€è‡´çš„è¯­éŸ³ï¼Œå¹¶æ”¯æŒä¸€æ ·æœ¬å£°éŸ³å…‹éš†ã€‚é€šè¿‡Flow-VAEï¼ŒåˆæˆéŸ³é¢‘çš„æ•´ä½“è´¨é‡å¾—åˆ°æå‡ã€‚æ¨¡å‹æ”¯æŒ32ç§è¯­è¨€ï¼Œåœ¨å¤šç§è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚\n\nWÇ’men jiÃ¨shÃ o le MiniMax-Speech, yÄ« zhÇ’ng jÄ«yÃº Transformer de wÃ©nbÄ›n zhuÇn yÇ”yÄ«n mÃ³xÃ­ng. TÄ nÃ©ng shÄ“ngchÃ©ng gÄo zhÃ¬liÃ ng de yÇ”yÄ«n. MÃ³xÃ­ng de guÇnjiÃ n chuÃ ngxÄ«n shÃ¬ kÄ› xuÃ©xÃ­ de yÇnjiÇngzhÄ› biÄnmÇqÃ¬, cÃ³ng cÄnkÇo yÄ«npÃ­n zhÅng tÃ­quÄn yÄ«nsÃ¨ tÃ¨zhÄ“ng, wÃºxÅ« zhuÇnlÃ¹. ZhÃ¨ shÇ de MiniMax-Speech nÃ©ng zÃ i lÃ­ng yÃ ngbÄ›n qÃ­ngkuÃ ng xiÃ  shÄ“ngchÃ©ng biÇoxiÃ nlÃ¬ qiÃ¡ng, yÄ«nsÃ¨ yÄ«zhÃ¬ de yÇ”yÄ«n, bÃ¬ng zhÄ«chÃ­ yÄ« yÃ ngbÄ›n shÄ“ngyÄ«n kÃ¨lÃ³ng. TÅngguÃ² Flow-VAE, hÃ©chÃ©ng yÄ«npÃ­n de zhÄ›ngtÇ zhÃ¬liÃ ng dÃ©dÃ o tÃ­shÄ“ng. MÃ³xÃ­ng zhÄ«chÃ­ 32 zhÇ’ng yÇ”yÃ¡n, zÃ i duÅ zhÇ’ng pÃ­nggÅ« zhÇbiÄo shÃ ng biÇoxiÃ n chÅ«sÃ¨, dÃ¡dÃ o le zuÃ¬ xiÄnjÃ¬n de jiÃ©guÇ’.",
        "vocab": "[\n    {\"word\": \"ä»‹ç»\", \"pinyin\": \"jiÃ¨ shÃ o\", \"trans\": \"introduce\"},\n    {\"word\": \"MiniMax-Speech\", \"pinyin\": \"MiniMax-Speech\", \"trans\": \"MiniMax-Speech\"},\n    {\"word\": \"åŸºäº\", \"pinyin\": \"jÄ« yÃº\", \"trans\": \"based on\"},\n    {\"word\": \"Transformer\", \"pinyin\": \"Transformer\", \"trans\": \"Transformer\"},\n    {\"word\": \"æ–‡æœ¬è½¬è¯­éŸ³\", \"pinyin\": \"wÃ©n bÄ›n zhuÇn yÇ” yÄ«n\", \"trans\": \"text-to-speech\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ng chÃ©ng\", \"trans\": \"generate\"},\n    {\"word\": \"é«˜è´¨é‡\", \"pinyin\": \"gÄo zhÃ¬ liÃ ng\", \"trans\": \"high quality\"},\n    {\"word\": \"è¯­éŸ³\", \"pinyin\": \"yÇ” yÄ«n\", \"trans\": \"speech\"},\n    {\"word\": \"å…³é”®\", \"pinyin\": \"guÇn jiÃ n\", \"trans\": \"key\"},\n    {\"word\": \"åˆ›æ–°\", \"pinyin\": \"chuÃ ng xÄ«n\", \"trans\": \"innovation\"},\n    {\"word\": \"å¯å­¦ä¹ \", \"pinyin\": \"kÄ› xuÃ© xÃ­\", \"trans\": \"learnable\"},\n    {\"word\": \"æ¼”è®²è€…\", \"pinyin\": \"yÇn jiÇng zhÄ›\", \"trans\": \"speaker\"},\n    {\"word\": \"ç¼–ç å™¨\", \"pinyin\": \"biÄn mÇ qÃ¬\", \"trans\": \"encoder\"},\n    {\"word\": \"ä»\", \"pinyin\": \"cÃ³ng\", \"trans\": \"from\"},\n    {\"word\": \"å‚è€ƒ\", \"pinyin\": \"cÄn kÇo\", \"trans\": \"reference\"},\n    {\"word\": \"éŸ³é¢‘\", \"pinyin\": \"yÄ«n pÃ­n\", \"trans\": \"audio\"},\n    {\"word\": \"æå–\", \"pinyin\": \"tÃ­ qu\", \"trans\": \"extract\"},\n    {\"word\": \"éŸ³è‰²\", \"pinyin\": \"yÄ«n sÃ¨\", \"trans\": \"timbre\"},\n    {\"word\": \"ç‰¹å¾\", \"pinyin\": \"tÃ¨ zhÄ“ng\", \"trans\": \"features\"},\n    {\"word\": \"æ— éœ€\", \"pinyin\": \"wÃº xÅ«\", \"trans\": \"no need\"},\n    {\"word\": \"è½¬å½•\", \"pinyin\": \"zhuÇn lÃ¹\", \"trans\": \"transcription\"},\n    {\"word\": \"é›¶æ ·æœ¬\", \"pinyin\": \"lÃ­ng yÃ ng bÄ›n\", \"trans\": \"zero-shot\"},\n    {\"word\": \"æƒ…å†µ\", \"pinyin\": \"qÃ­ng kuÃ ng\", \"trans\": \"situation\"},\n    {\"word\": \"è¡¨ç°åŠ›\", \"pinyin\": \"biÇo xiÃ n lÃ¬\", \"trans\": \"expressiveness\"},\n    {\"word\": \"å¼º\", \"pinyin\": \"qiÃ¡ng\", \"trans\": \"strong\"},\n    {\"word\": \"ä¸€è‡´\", \"pinyin\": \"yÄ« zhÃ¬\", \"trans\": \"consistent\"},\n    {\"word\": \"æ”¯æŒ\", \"pinyin\": \"zhÄ« chÃ­\", \"trans\": \"support\"},\n    {\"word\": \"ä¸€æ ·æœ¬\", \"pinyin\": \"yÄ« yÃ ng bÄ›n\", \"trans\": \"one-shot\"},\n    {\"word\": \"å£°éŸ³\", \"pinyin\": \"shÄ“ng yÄ«n\", \"trans\": \"voice\"},\n    {\"word\": \"å…‹éš†\", \"pinyin\": \"kÃ¨ lÃ³ng\", \"trans\": \"clone\"},\n    {\"word\": \"é€šè¿‡\", \"pinyin\": \"tÅng guÃ²\", \"trans\": \"through\"},\n    {\"word\": \"Flow-VAE\", \"pinyin\": \"Flow-VAE\", \"trans\": \"Flow-VAE\"},\n    {\"word\": \"åˆæˆ\", \"pinyin\": \"hÃ© chÃ©ng\", \"trans\": \"synthesis\"},\n    {\"word\": \"æ•´ä½“\", \"pinyin\": \"zhÄ›ng tÇ\", \"trans\": \"overall\"},\n    {\"word\": \"è´¨é‡\", \"pinyin\": \"zhÃ¬ liÃ ng\", \"trans\": \"quality\"},\n    {\"word\": \"å¾—åˆ°\", \"pinyin\": \"dÃ© dÃ o\", \"trans\": \"obtain\"},\n    {\"word\": \"æå‡\", \"pinyin\": \"tÃ­ shÄ“ng\", \"trans\": \"improvement\"},\n    {\"word\": \"å¤šç§\", \"pinyin\": \"duÅ zhÇ’ng\", \"trans\": \"various\"},\n    {\"word\": \"è¯„ä¼°\", \"pinyin\": \"pÃ­ng gÅ«\", \"trans\": \"evaluation\"},\n    {\"word\": \"æŒ‡æ ‡\", \"pinyin\": \"zhÇ biÄo\", \"trans\": \"metrics\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇo xiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"å‡ºè‰²\", \"pinyin\": \"chÅ« sÃ¨\", \"trans\": \"outstanding\"},\n    {\"word\": \"è¾¾åˆ°\", \"pinyin\": \"dÃ¡ dÃ o\", \"trans\": \"achieve\"},\n    {\"word\": \"æœ€å…ˆè¿›\", \"pinyin\": \"zuÃ¬ xiÄn jÃ¬n\", \"trans\": \"state-of-the-art\"}\n]",
        "trans": "We introduce MiniMax-Speech, a Transformer-based text-to-speech model capable of generating high-quality speech. The key innovation of the model is a learnable speaker encoder that extracts voice characteristics from reference audio without the need for transcription. This enables MiniMax-Speech to generate expressive and voice-consistent speech in zero-shot scenarios and supports one-shot voice cloning. Through Flow-VAE, the overall quality of the synthesized audio is enhanced. The model supports 32 languages and performs excellently across various evaluation metrics, achieving state-of-the-art results.",
        "update_ts": "2025-05-14 09:12"
    }
}