{
    "date": {
        "ru": "4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
        "en": "August 4",
        "zh": "8æœˆ4æ—¥"
    },
    "time_utc": "2025-08-04 08:18",
    "weekday": 0,
    "issue_id": 5159,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.00819",
            "title": "Beyond Fixed: Variable-Length Denoising for Diffusion Large Language\n  Models",
            "url": "https://huggingface.co/papers/2508.00819",
            "abstract": "DAEDAL, a novel training-free denoising strategy, enables dynamic length adaptation in Diffusion Large Language Models, improving performance and computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Large Language Models (DLLMs) are emerging as a powerful alternative to the dominant Autoregressive Large Language Models, offering efficient parallel generation and capable global context modeling. However, the practical application of DLLMs is hindered by a critical architectural constraint: the need for a statically predefined generation length. This static length allocation leads to a problematic trade-off: insufficient lengths cripple performance on complex tasks, while excessive lengths incur significant computational overhead and sometimes result in performance degradation. While the inference framework is rigid, we observe that the model itself possesses internal signals that correlate with the optimal response length for a given task. To bridge this gap, we leverage these latent signals and introduce DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive Length Expansion for Diffusion Large Language Models. DAEDAL operates in two phases: 1) Before the denoising process, DAEDAL starts from a short initial length and iteratively expands it to a coarse task-appropriate length, guided by a sequence completion metric. 2) During the denoising process, DAEDAL dynamically intervenes by pinpointing and expanding insufficient generation regions through mask token insertion, ensuring the final output is fully developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves performance comparable, and in some cases superior, to meticulously tuned fixed-length baselines, while simultaneously enhancing computational efficiency by achieving a higher effective token ratio. By resolving the static length constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap with their Autoregressive counterparts and paving the way for more efficient and capable generation.",
            "score": 21,
            "issue_id": 5154,
            "pub_date": "2025-08-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 1",
                "zh": "8æœˆ1æ—¥"
            },
            "hash": "2241beba3b69f1fd",
            "authors": [
                "Jinsong Li",
                "Xiaoyi Dong",
                "Yuhang Zang",
                "Yuhang Cao",
                "Jiaqi Wang",
                "Dahua Lin"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.00819.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization",
                    "#diffusion",
                    "#long_context"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "DAEDAL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (DLLM). ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ DLLM. DAEDAL Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºÑƒÑ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ´Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DAEDAL Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Dynamic Length Adaptation for Enhanced DLLM Performance",
                    "desc": "DAEDAL is a new method that improves Diffusion Large Language Models (DLLMs) by allowing them to adapt their output length dynamically without needing additional training. Traditional DLLMs require a fixed length for generation, which can limit their performance on complex tasks or waste computational resources. DAEDAL addresses this issue by using internal signals from the model to determine the optimal length for responses, expanding the generation length as needed. This approach not only enhances the quality of the generated text but also increases efficiency, making DLLMs more competitive with Autoregressive models."
                },
                "zh": {
                    "title": "DAEDALï¼šåŠ¨æ€é€‚åº”é•¿åº¦çš„å»å™ªæ–°ç­–ç•¥",
                    "desc": "DAEDALæ˜¯ä¸€ç§æ–°é¢–çš„æ— è®­ç»ƒå»å™ªç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å®ç°åŠ¨æ€é•¿åº¦é€‚åº”ï¼Œä»è€Œæé«˜æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆDLLMsï¼‰åœ¨ç”Ÿæˆæ•ˆç‡å’Œå…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶é™æ€ç”Ÿæˆé•¿åº¦é™åˆ¶äº†å®é™…åº”ç”¨ã€‚DAEDALé€šè¿‡åˆ©ç”¨æ¨¡å‹å†…éƒ¨ä¿¡å·ï¼ŒåŠ¨æ€è°ƒæ•´ç”Ÿæˆé•¿åº¦ï¼Œè§£å†³äº†é™æ€é•¿åº¦å¸¦æ¥çš„æ€§èƒ½å’Œè®¡ç®—å¼€é”€é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒDAEDALåœ¨æ€§èƒ½ä¸Šä¸å›ºå®šé•¿åº¦åŸºçº¿ç›¸å½“ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹æ›´ä¼˜ï¼ŒåŒæ—¶æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.23268",
            "title": "PixNerd: Pixel Neural Field Diffusion",
            "url": "https://huggingface.co/papers/2507.23268",
            "abstract": "Pixel Neural Field Diffusion (PixNerd) achieves high-quality image generation in a single-scale, single-stage process without VAEs or complex pipelines, and extends to text-to-image applications with competitive performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The current success of diffusion transformers heavily depends on the compressed latent space shaped by the pre-trained variational autoencoder(VAE). However, this two-stage training paradigm inevitably introduces accumulated errors and decoding artifacts. To address the aforementioned problems, researchers return to pixel space at the cost of complicated cascade pipelines and increased token complexity. In contrast to their efforts, we propose to model the patch-wise decoding with neural field and present a single-scale, single-stage, efficient, end-to-end solution, coined as pixel neural field diffusion~(PixelNerd). Thanks to the efficient neural field representation in PixNerd, we directly achieved 2.15 FID on ImageNet 256times256 and 2.84 FID on ImageNet 512times512 without any complex cascade pipeline or VAE. We also extend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16 achieved a competitive 0.73 overall score on the GenEval benchmark and 80.9 overall score on the DPG benchmark.",
            "score": 16,
            "issue_id": 5154,
            "pub_date": "2025-07-31",
            "pub_date_card": {
                "ru": "31 Ğ¸ÑĞ»Ñ",
                "en": "July 31",
                "zh": "7æœˆ31æ—¥"
            },
            "hash": "f035699955568725",
            "authors": [
                "Shuai Wang",
                "Ziteng Gao",
                "Chenhui Zhu",
                "Weilin Huang",
                "Limin Wang"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Nanjing University",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.23268.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#benchmark"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€",
                    "desc": "PixNerd (Pixel Neural Field Diffusion) - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ². ĞĞ½ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€. PixNerd Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ImageNet, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ FID. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… GenEval Ğ¸ DPG."
                },
                "en": {
                    "title": "Efficient Image Generation with PixNerd: No VAEs, No Hassle!",
                    "desc": "Pixel Neural Field Diffusion (PixNerd) introduces a novel approach to image generation that operates in a single-scale and single-stage manner, eliminating the need for variational autoencoders (VAEs) and complex pipelines. This method addresses the issues of accumulated errors and artifacts that arise from traditional two-stage training processes. By utilizing a patch-wise decoding strategy with neural fields, PixNerd achieves impressive performance metrics, such as a 2.15 FID score on ImageNet 256x256. Additionally, it extends its capabilities to text-to-image generation, demonstrating competitive results on various benchmarks."
                },
                "zh": {
                    "title": "é«˜æ•ˆå›¾åƒç”Ÿæˆçš„æ–°æ–¹æ³•ï¼šPixNerd",
                    "desc": "Pixel Neural Field Diffusionï¼ˆPixNerdï¼‰æ˜¯ä¸€ç§é«˜æ•ˆçš„å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œé‡‡ç”¨å•å°ºåº¦ã€å•é˜¶æ®µçš„æµç¨‹ï¼Œæ— éœ€å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æˆ–å¤æ‚çš„ç®¡é“ã€‚è¯¥æ–¹æ³•é€šè¿‡ç¥ç»åœºæ¨¡å‹å®ç°äº†è¡¥ä¸çº§è§£ç ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å¸¸è§çš„ç´¯ç§¯è¯¯å·®å’Œè§£ç ä¼ªå½±ã€‚PixNerdåœ¨ImageNetæ•°æ®é›†ä¸Šå–å¾—äº†2.15çš„FIDåˆ†æ•°ï¼Œæ˜¾ç¤ºå‡ºå…¶ä¼˜è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å°†PixNerdæ‰©å±•åˆ°æ–‡æœ¬ç”Ÿæˆå›¾åƒçš„åº”ç”¨ä¸­ï¼Œå–å¾—äº†åœ¨GenEvalå’ŒDPGåŸºå‡†æµ‹è¯•ä¸­çš„ç«äº‰æ€§æˆç»©ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.23361",
            "title": "SWE-Exp: Experience-Driven Software Issue Resolution",
            "url": "https://huggingface.co/papers/2507.23361",
            "abstract": "SWE-Exp enhances software issue resolution by systematically accumulating and leveraging repair expertise from past agent experiences, improving resolution rates.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language model (LLM) agents have shown remarkable progress in software issue resolution, leveraging advanced techniques such as multi-agent collaboration and Monte Carlo Tree Search (MCTS). However, current agents act as memoryless explorers - treating each problem separately without retaining or reusing knowledge from previous repair experiences. This leads to redundant exploration of failed trajectories and missed chances to adapt successful issue resolution methods to similar problems. To address this problem, we introduce SWE-Exp, an experience - enhanced approach that distills concise and actionable experience from prior agent trajectories, enabling continuous learning across issues. Our method introduces a multi-faceted experience bank that captures both successful and failed repair attempts. Specifically, it extracts reusable issue resolution knowledge at different levels - from high-level problem comprehension to specific code changes. Experiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6% Pass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach establishes a new paradigm in which automated software engineering agents systematically accumulate and leverage repair expertise, fundamentally shifting from trial-and-error exploration to strategic, experience-driven issue resolution.",
            "score": 5,
            "issue_id": 5154,
            "pub_date": "2025-07-31",
            "pub_date_card": {
                "ru": "31 Ğ¸ÑĞ»Ñ",
                "en": "July 31",
                "zh": "7æœˆ31æ—¥"
            },
            "hash": "e16fe4dad5f61553",
            "authors": [
                "Silin Chen",
                "Shaoxin Lin",
                "Xiaodong Gu",
                "Yuling Shi",
                "Heng Lian",
                "Longfei Yun",
                "Dong Chen",
                "Weiguo Sun",
                "Lin Cao",
                "Qianxiang Wang"
            ],
            "affiliations": [
                "Huawei, China",
                "Shanghai Jiao Tong University, China",
                "UC San Diego, United States",
                "Xidian University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.23361.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ¿Ñ‹Ñ‚ - ĞºĞ»ÑÑ‡ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² ĞŸĞ",
                    "desc": "SWE-Exp - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¹ Ğ±Ğ°Ğ½Ğº Ğ¾Ğ¿Ñ‹Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ°Ğº ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼. SWE-Exp Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ - Ğ¾Ñ‚ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ¾Ğ´Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SWE-Exp Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ SWE-bench-Verified ÑÑ€ĞµĞ´Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Transforming Software Issue Resolution with Experience-Driven Learning",
                    "desc": "SWE-Exp is a novel approach that enhances software issue resolution by utilizing past experiences of agents to improve their performance. Unlike traditional agents that do not retain knowledge, SWE-Exp builds an experience bank that captures both successful and failed attempts at resolving issues. This allows the system to learn from previous repairs and apply that knowledge to new problems, leading to more efficient and effective resolutions. The method has demonstrated a significant improvement in resolution rates, showcasing a shift from random exploration to a more strategic, experience-driven process."
                },
                "zh": {
                    "title": "ç»éªŒé©±åŠ¨çš„è½¯ä»¶é—®é¢˜è§£å†³æ–°æ–¹æ³•",
                    "desc": "SWE-Expæ˜¯ä¸€ç§å¢å¼ºè½¯ä»¶é—®é¢˜è§£å†³èƒ½åŠ›çš„æ–¹æ³•ï¼Œé€šè¿‡ç³»ç»Ÿåœ°ç§¯ç´¯å’Œåˆ©ç”¨è¿‡å»ä»£ç†çš„ä¿®å¤ç»éªŒï¼Œæé«˜äº†è§£å†³ç‡ã€‚å½“å‰çš„ä»£ç†åœ¨å¤„ç†é—®é¢˜æ—¶ç¼ºä¹è®°å¿†ï¼Œæ— æ³•é‡ç”¨ä¹‹å‰çš„çŸ¥è¯†ï¼Œå¯¼è‡´é‡å¤æ¢ç´¢å¤±è´¥çš„è·¯å¾„ã€‚SWE-Expé€šè¿‡å»ºç«‹ä¸€ä¸ªå¤šå±‚æ¬¡çš„ç»éªŒåº“ï¼Œæå–æˆåŠŸå’Œå¤±è´¥çš„ä¿®å¤å°è¯•ä¸­çš„å¯é‡ç”¨çŸ¥è¯†ï¼Œä»è€Œå®ç°è·¨é—®é¢˜çš„æŒç»­å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒSWE-Expåœ¨å¼€æºä»£ç†æ¡†æ¶ä¸‹çš„SWE-bench-Verifiedä¸Šè¾¾åˆ°äº†41.6%çš„æœ€ä½³è§£å†³ç‡ï¼Œæ ‡å¿—ç€è‡ªåŠ¨åŒ–è½¯ä»¶å·¥ç¨‹ä»£ç†çš„ä¸€ä¸ªæ–°èŒƒå¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.23348",
            "title": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution",
            "url": "https://huggingface.co/papers/2507.23348",
            "abstract": "SWE-Debate, a competitive multi-agent framework, enhances issue resolution in software engineering by promoting diverse reasoning and achieving better issue localization and fix planning.  \t\t\t\t\tAI-generated summary \t\t\t\t Issue resolution has made remarkable progress thanks to the advanced reasoning capabilities of large language models (LLMs). Recently, agent-based frameworks such as SWE-agent have further advanced this progress by enabling autonomous, tool-using agents to tackle complex software engineering tasks. While existing agent-based issue resolution approaches are primarily based on agents' independent explorations, they often get stuck in local solutions and fail to identify issue patterns that span across different parts of the codebase. To address this limitation, we propose SWE-Debate, a competitive multi-agent debate framework that encourages diverse reasoning paths and achieves more consolidated issue localization. SWE-Debate first creates multiple fault propagation traces as localization proposals by traversing a code dependency graph. Then, it organizes a three-round debate among specialized agents, each embodying distinct reasoning perspectives along the fault propagation trace. This structured competition enables agents to collaboratively converge on a consolidated fix plan. Finally, this consolidated fix plan is integrated into an MCTS-based code modification agent for patch generation. Experiments on the SWE-bench benchmark show that SWE-Debate achieves new state-of-the-art results in open-source agent frameworks and outperforms baselines by a large margin.",
            "score": 4,
            "issue_id": 5154,
            "pub_date": "2025-07-31",
            "pub_date_card": {
                "ru": "31 Ğ¸ÑĞ»Ñ",
                "en": "July 31",
                "zh": "7æœˆ31æ—¥"
            },
            "hash": "28b58ecd36ac995b",
            "authors": [
                "Han Li",
                "Yuling Shi",
                "Shaoxin Lin",
                "Xiaodong Gu",
                "Heng Lian",
                "Xin Wang",
                "Yantao Jia",
                "Tao Huang",
                "Qianxiang Wang"
            ],
            "affiliations": [
                "Huawei China",
                "Shanghai Jiao Tong University China",
                "Xidian University China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.23348.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#agents",
                    "#open_source",
                    "#reasoning",
                    "#games",
                    "#benchmark"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ”ĞµĞ±Ğ°Ñ‚Ñ‹ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞŸĞ",
                    "desc": "SWE-Debate - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´ĞµĞ±Ğ°Ñ‚Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑĞ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ SWE-Debate Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Empowering Software Issue Resolution through Competitive Multi-Agent Debate",
                    "desc": "SWE-Debate is a multi-agent framework designed to improve issue resolution in software engineering by fostering diverse reasoning among agents. It utilizes large language models to enhance the reasoning capabilities of autonomous agents, allowing them to tackle complex tasks more effectively. By organizing a structured debate among agents with different perspectives, SWE-Debate helps identify broader issue patterns and achieve better localization of software faults. The framework has demonstrated significant improvements in performance on the SWE-bench benchmark, setting new standards in open-source agent frameworks."
                },
                "zh": {
                    "title": "SWE-Debateï¼šå¤šæ ·åŒ–æ¨ç†ä¿ƒè¿›è½¯ä»¶é—®é¢˜è§£å†³",
                    "desc": "SWE-Debateæ˜¯ä¸€ä¸ªç«äº‰æ€§çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¿ƒè¿›å¤šæ ·åŒ–çš„æ¨ç†æ¥å¢å¼ºè½¯ä»¶å·¥ç¨‹ä¸­çš„é—®é¢˜è§£å†³èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¸®åŠ©æ™ºèƒ½ä½“åœ¨å¤æ‚çš„è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­è¿›è¡Œè‡ªä¸»æ¢ç´¢ã€‚ä¸ä»¥å¾€çš„ç‹¬ç«‹æ¢ç´¢æ–¹æ³•ä¸åŒï¼ŒSWE-Debateé€šè¿‡ç»„ç»‡æ™ºèƒ½ä½“ä¹‹é—´çš„è¾©è®ºï¼Œé¼“åŠ±ä¸åŒçš„æ¨ç†è·¯å¾„ï¼Œä»è€Œæ›´å¥½åœ°å®šä½é—®é¢˜å¹¶åˆ¶å®šä¿®å¤è®¡åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSWE-Debateåœ¨å¼€æºæ™ºèƒ½ä½“æ¡†æ¶ä¸­è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.00265",
            "title": "Multimodal Referring Segmentation: A Survey",
            "url": "https://huggingface.co/papers/2508.00265",
            "abstract": "A survey of multimodal referring segmentation techniques, covering advancements in convolutional neural networks, transformers, and large language models for segmenting objects in images, videos, and 3D scenes based on text or audio instructions.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal referring segmentation aims to segment target objects in visual scenes, such as images, videos, and 3D scenes, based on referring expressions in text or audio format. This task plays a crucial role in practical applications requiring accurate object perception based on user instructions. Over the past decade, it has gained significant attention in the multimodal community, driven by advances in convolutional neural networks, transformers, and large language models, all of which have substantially improved multimodal perception capabilities. This paper provides a comprehensive survey of multimodal referring segmentation. We begin by introducing this field's background, including problem definitions and commonly used datasets. Next, we summarize a unified meta architecture for referring segmentation and review representative methods across three primary visual scenes, including images, videos, and 3D scenes. We further discuss Generalized Referring Expression (GREx) methods to address the challenges of real-world complexity, along with related tasks and practical applications. Extensive performance comparisons on standard benchmarks are also provided. We continually track related works at https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.",
            "score": 3,
            "issue_id": 5154,
            "pub_date": "2025-08-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 1",
                "zh": "8æœˆ1æ—¥"
            },
            "hash": "1604e587f6dc8177",
            "authors": [
                "Henghui Ding",
                "Song Tang",
                "Shuting He",
                "Chang Liu",
                "Zuxuan Wu",
                "Yu-Gang Jiang"
            ],
            "affiliations": [
                "ByteDance Inc.",
                "Fudan University",
                "Shanghai University of Finance and Economics"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.00265.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#3d",
                    "#survey",
                    "#video",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ: Ğ¾Ñ‚ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑÑ‹Ğ»ĞºĞ°Ğ¼, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ² 3D-ÑÑ†ĞµĞ½Ğ°Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ğ°-Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑÑ‹Ğ»ĞºĞ°Ğ¼ Ğ¸ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ÑÑÑ‹Ğ»Ğ¾Ğº (GREx) Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°."
                },
                "en": {
                    "title": "Enhancing Object Segmentation with Multimodal Instructions",
                    "desc": "This paper surveys the field of multimodal referring segmentation, which focuses on identifying and segmenting objects in visual content based on textual or audio instructions. It highlights the advancements made through convolutional neural networks, transformers, and large language models that enhance the ability to understand and process multimodal data. The authors present a unified meta architecture for referring segmentation and review various methods applicable to images, videos, and 3D scenes. Additionally, they discuss challenges in real-world applications and provide performance comparisons on standard benchmarks to evaluate the effectiveness of different approaches."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€æŒ‡å‘åˆ†å‰²çš„å…¨é¢è°ƒæŸ¥",
                    "desc": "å¤šæ¨¡æ€æŒ‡å‘åˆ†å‰²æ—¨åœ¨æ ¹æ®æ–‡æœ¬æˆ–éŸ³é¢‘æŒ‡ä»¤åœ¨è§†è§‰åœºæ™¯ä¸­åˆ†å‰²ç›®æ ‡ç‰©ä½“ï¼Œå¦‚å›¾åƒã€è§†é¢‘å’Œ3Dåœºæ™¯ã€‚è¯¥ä»»åŠ¡åœ¨éœ€è¦æ ¹æ®ç”¨æˆ·æŒ‡ä»¤è¿›è¡Œå‡†ç¡®ç‰©ä½“æ„ŸçŸ¥çš„å®é™…åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚è¿‘å¹´æ¥ï¼Œå·ç§¯ç¥ç»ç½‘ç»œã€å˜æ¢å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥æ˜¾è‘—æå‡äº†å¤šæ¨¡æ€æ„ŸçŸ¥èƒ½åŠ›ã€‚æœ¬æ–‡æä¾›äº†å¤šæ¨¡æ€æŒ‡å‘åˆ†å‰²çš„å…¨é¢è°ƒæŸ¥ï¼Œæ¶µç›–äº†èƒŒæ™¯ä»‹ç»ã€ç»Ÿä¸€çš„å…ƒæ¶æ„ã€ä»£è¡¨æ€§æ–¹æ³•åŠå…¶åœ¨ä¸åŒè§†è§‰åœºæ™¯ä¸­çš„åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.23478",
            "title": "3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding",
            "url": "https://huggingface.co/papers/2507.23478",
            "abstract": "3D-R1 enhances 3D scene understanding through a high-quality synthetic dataset, reinforcement learning with GRPO, and dynamic view selection, achieving significant improvements in reasoning and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large vision-language models (VLMs) have made significant strides in 2D visual understanding tasks, sparking interest in extending these capabilities to 3D scene understanding. However, current 3D VLMs often struggle with robust reasoning and generalization due to limitations in high-quality spatial data and the static nature of viewpoint assumptions. To address these challenges, we propose 3D-R1, a foundation model that enhances the reasoning capabilities of 3D VLMs. Specifically, we first construct a high-quality synthetic dataset with CoT, named Scene-30K, leveraging existing 3D-VL datasets and a data engine based on Gemini 2.5 Pro. It serves as cold-start initialization data for 3D-R1. Moreover, we leverage RLHF policy such as GRPO in the reinforcement learning training process to enhance reasoning capabilities and introduce three reward functions: a perception reward, a semantic similarity reward and a format reward to maintain detection accuracy and answer semantic precision. Furthermore, we introduce a dynamic view selection strategy that adaptively chooses the most informative perspectives for 3D scene understanding. Extensive experiments demonstrate that 3D-R1 delivers an average improvement of 10% across various 3D scene benchmarks, highlighting its effectiveness in enhancing reasoning and generalization in 3D scene understanding. Code: https://github.com/AIGeeksGroup/3D-R1. Website: https://aigeeksgroup.github.io/3D-R1.",
            "score": 3,
            "issue_id": 5155,
            "pub_date": "2025-07-31",
            "pub_date_card": {
                "ru": "31 Ğ¸ÑĞ»Ñ",
                "en": "July 31",
                "zh": "7æœˆ31æ—¥"
            },
            "hash": "f5e99fc10e8b9ad5",
            "authors": [
                "Ting Huang",
                "Zeyu Zhang",
                "Hao Tang"
            ],
            "affiliations": [
                "School of Computer Science, Peking University",
                "Shanghai University of Engineering Science"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.23478.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#3d",
                    "#dataset",
                    "#reasoning",
                    "#rlhf",
                    "#synthetic"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "3D-R1: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "ĞœĞ¾Ğ´ĞµĞ»ÑŒ 3D-R1 ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° 3D-ÑÑ†ĞµĞ½. 3D-R1 Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½."
                },
                "en": {
                    "title": "Enhancing 3D Scene Understanding with 3D-R1",
                    "desc": "The paper presents 3D-R1, a model designed to improve 3D scene understanding by addressing the limitations of existing vision-language models (VLMs). It introduces a high-quality synthetic dataset called Scene-30K, which is used to enhance the model's reasoning capabilities. The training process incorporates reinforcement learning with a GRPO policy and utilizes multiple reward functions to ensure accuracy and semantic precision. Additionally, a dynamic view selection strategy is implemented to optimize the perspectives used for analyzing 3D scenes, resulting in a notable average improvement of 10% in performance across various benchmarks."
                },
                "zh": {
                    "title": "3D-R1ï¼šæå‡3Dåœºæ™¯ç†è§£çš„æ™ºèƒ½æ¨¡å‹",
                    "desc": "3D-R1 æ˜¯ä¸€ä¸ªå¢å¼º 3D åœºæ™¯ç†è§£çš„åŸºç¡€æ¨¡å‹ï¼Œåˆ©ç”¨é«˜è´¨é‡çš„åˆæˆæ•°æ®é›†å’Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•æ¥æå‡æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåä¸º Scene-30K çš„åˆæˆæ•°æ®é›†ï¼Œä½œä¸º 3D-R1 çš„å†·å¯åŠ¨åˆå§‹åŒ–æ•°æ®ã€‚é€šè¿‡å¼•å…¥åŠ¨æ€è§†è§’é€‰æ‹©ç­–ç•¥ï¼Œ3D-R1 èƒ½å¤Ÿè‡ªé€‚åº”é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„è§†è§’è¿›è¡Œ 3D åœºæ™¯ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ3D-R1 åœ¨å¤šä¸ª 3D åœºæ™¯åŸºå‡†æµ‹è¯•ä¸­å¹³å‡æå‡äº† 10%ï¼Œæœ‰æ•ˆå¢å¼ºäº†æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22720",
            "title": "Investigating Hallucination in Conversations for Low Resource Languages",
            "url": "https://huggingface.co/papers/2507.22720",
            "abstract": "LLMs generate fewer hallucinations in Mandarin compared to Hindi and Farsi across multiple models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated remarkable proficiency in generating text that closely resemble human writing. However, they often generate factually incorrect statements, a problem typically referred to as 'hallucination'. Addressing hallucination is crucial for enhancing the reliability and effectiveness of LLMs. While much research has focused on hallucinations in English, our study extends this investigation to conversational data in three languages: Hindi, Farsi, and Mandarin. We offer a comprehensive analysis of a dataset to examine both factual and linguistic errors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0, DeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated responses in Mandarin but generate a significantly higher number of hallucinations in Hindi and Farsi.",
            "score": 2,
            "issue_id": 5155,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 Ğ¸ÑĞ»Ñ",
                "en": "July 30",
                "zh": "7æœˆ30æ—¥"
            },
            "hash": "c7f5db5f58895f4f",
            "authors": [
                "Amit Das",
                "Md. Najib Hasan",
                "Souvika Sarkar",
                "Zheng Zhang",
                "Fatemeh Jamshidi",
                "Tathagata Bhattacharya",
                "Nilanjana Raychawdhury",
                "Dongji Feng",
                "Vinija Jain",
                "Aman Chadha"
            ],
            "affiliations": [
                "Amazon GenAI",
                "Auburn University",
                "Auburn University at Montgomery",
                "California State Polytechnic University Pomona",
                "Gustavus Adolphus College",
                "Meta",
                "Murray State University",
                "Stanford University",
                "University of North Alabama",
                "Wichita State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.22720.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multilingual",
                    "#hallucinations"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾-ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ñ‚Ñ€ĞµÑ… ÑĞ·Ñ‹ĞºĞ¾Ğ²: Ñ…Ğ¸Ğ½Ğ´Ğ¸, Ñ„Ğ°Ñ€ÑĞ¸ Ğ¸ Ğ¼Ğ°Ğ½Ğ´Ğ°Ñ€Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ³Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² ÑÑ‚Ğ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-3.5, GPT-4, Llama-3.1 Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ LLM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¼Ğ°Ğ½Ğ´Ğ°Ñ€Ğ¸Ğ½ÑĞºĞ¾Ğ¼ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ…Ğ¸Ğ½Ğ´Ğ¸ Ğ¸ Ñ„Ğ°Ñ€ÑĞ¸. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ñ‡Ñ‚Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ LLM."
                },
                "en": {
                    "title": "Mandarin LLMs: Fewer Hallucinations, More Accuracy!",
                    "desc": "This paper investigates the phenomenon of hallucinations in Large Language Models (LLMs) across three languages: Mandarin, Hindi, and Farsi. Hallucinations refer to instances where the models generate incorrect or misleading information. The study analyzes conversational data from various LLMs, including GPT-3.5 and GPT-4o, to compare the frequency of these errors. The findings reveal that LLMs exhibit fewer hallucinations in Mandarin compared to the higher rates observed in Hindi and Farsi, highlighting the need for language-specific improvements in model training."
                },
                "zh": {
                    "title": "æ™®é€šè¯ä¸­çš„å¹»è§‰ç°è±¡è¾ƒå°‘",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆæ–‡æœ¬æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬æœ‰æ—¶ä¼šäº§ç”Ÿä¸å‡†ç¡®çš„ä¿¡æ¯ï¼Œè¿™è¢«ç§°ä¸ºâ€œå¹»è§‰â€ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨æ™®é€šè¯ã€å°åœ°è¯­å’Œæ³•å°”è¥¿è¯­ä¸­ï¼ŒLLMsçš„å¹»è§‰ç°è±¡ã€‚æˆ‘ä»¬åˆ†æäº†å¤šä¸ªæ¨¡å‹ï¼ˆå¦‚GPT-3.5ã€GPT-4oç­‰ï¼‰åœ¨è¿™ä¸‰ç§è¯­è¨€ä¸­çš„äº‹å®å’Œè¯­è¨€é”™è¯¯ã€‚ç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨æ™®é€šè¯ä¸­äº§ç”Ÿçš„å¹»è§‰å“åº”è¾ƒå°‘ï¼Œè€Œåœ¨å°åœ°è¯­å’Œæ³•å°”è¥¿è¯­ä¸­åˆ™æ˜¾è‘—æ›´å¤šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.00823",
            "title": "IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation",
            "url": "https://huggingface.co/papers/2508.00823",
            "abstract": "IGL-Nav uses an incremental 3D Gaussian representation for efficient and accurate image-goal navigation in 3D space, outperforming existing methods and applicable in real-world settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual navigation with an image as goal is a fundamental and challenging problem. Conventional methods either rely on end-to-end RL learning or modular-based policy with topological graph or BEV map as memory, which cannot fully model the geometric relationship between the explored 3D environment and the goal image. In order to efficiently and accurately localize the goal image in 3D space, we build our navigation system upon the renderable 3D gaussian (3DGS) representation. However, due to the computational intensity of 3DGS optimization and the large search space of 6-DoF camera pose, directly leveraging 3DGS for image localization during agent exploration process is prohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D Gaussian Localization framework for efficient and 3D-aware image-goal navigation. Specifically, we incrementally update the scene representation as new images arrive with feed-forward monocular prediction. Then we coarsely localize the goal by leveraging the geometric information for discrete space matching, which can be equivalent to efficient 3D convolution. When the agent is close to the goal, we finally solve the fine target pose with optimization via differentiable rendering. The proposed IGL-Nav outperforms existing state-of-the-art methods by a large margin across diverse experimental configurations. It can also handle the more challenging free-view image-goal setting and be deployed on real-world robotic platform using a cellphone to capture goal image at arbitrary pose. Project page: https://gwxuan.github.io/IGL-Nav/.",
            "score": 1,
            "issue_id": 5154,
            "pub_date": "2025-08-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 1",
                "zh": "8æœˆ1æ—¥"
            },
            "hash": "a4651adceaac80f7",
            "authors": [
                "Wenxuan Guo",
                "Xiuwei Xu",
                "Hang Yin",
                "Ziwei Wang",
                "Jianjiang Feng",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.00823.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#3d",
                    "#optimization",
                    "#agents",
                    "#games"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "ĞĞ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ² 3D Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ²",
                    "desc": "IGL-Nav - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ-Ñ†ĞµĞ»Ğ¸ Ğ² Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ 3D Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğµ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³. IGL-Nav Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Efficient 3D Navigation with Incremental Gaussian Localization",
                    "desc": "IGL-Nav introduces an innovative approach to image-goal navigation in 3D environments using an incremental 3D Gaussian representation. This method enhances localization accuracy by updating the scene representation as new images are processed, allowing for efficient navigation. Unlike traditional methods that struggle with geometric relationships, IGL-Nav utilizes geometric information for effective discrete space matching and fine target pose optimization. The framework demonstrates significant improvements over existing techniques and is suitable for real-world applications, including robotic platforms."
                },
                "zh": {
                    "title": "å¢é‡å¼3Dé«˜æ–¯å¯¼èˆªï¼šé«˜æ•ˆå‡†ç¡®çš„å›¾åƒç›®æ ‡å®šä½",
                    "desc": "IGL-Navæ˜¯ä¸€ç§å¢é‡å¼3Dé«˜æ–¯å®šä½æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å›¾åƒç›®æ ‡å¯¼èˆªçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯æ¸²æŸ“çš„3Dé«˜æ–¯è¡¨ç¤ºæ¥å»ºæ¨¡3Dç¯å¢ƒä¸ç›®æ ‡å›¾åƒä¹‹é—´çš„å‡ ä½•å…³ç³»ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚IGL-Navé€šè¿‡å‰é¦ˆå•ç›®é¢„æµ‹é€æ­¥æ›´æ–°åœºæ™¯è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨å‡ ä½•ä¿¡æ¯è¿›è¡Œç²—ç•¥å®šä½ï¼Œæœ€ç»ˆé€šè¿‡å¯å¾®æ¸²æŸ“ä¼˜åŒ–ç²¾ç¡®ç¡®å®šç›®æ ‡ä½ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIGL-Navåœ¨å¤šç§é…ç½®ä¸‹æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶èƒ½å¤Ÿåœ¨çœŸå®ä¸–ç•Œçš„æœºå™¨äººå¹³å°ä¸Šåº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.00454",
            "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges",
            "url": "https://huggingface.co/papers/2508.00454",
            "abstract": "An efficient multi-turn dialogue evaluator aggregates multiple LLM judgments into a single model to assess dialogue quality with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating the conversational abilities of large language models (LLMs) remains a challenging task. Current mainstream approaches primarily rely on the ``LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator to assess dialogue quality. However, such methods often suffer from various biases, which undermine the reliability and consistency of the evaluation results. To mitigate these biases, recent methods employ multiple LLMs as judges and aggregate their judgments to select the optimal assessment. Although effective, this multi-judge approach incurs significant computational overhead during inference. In this paper, we propose an efficient multi-turn dialogue evaluator that captures the collective wisdom of multiple LLM judges by aggregating their preference knowledge into a single model. Our approach preserves the advantages of diverse multi-judge feedback while drastically reducing the evaluation cost, enabling fast and flexible dialogue quality assessment. Extensive experiments on seven single rating and pairwise comparison dialogue evaluation benchmarks demonstrate that our method outperforms existing baselines across diverse scenarios, showcasing its efficiency and robustness.",
            "score": 1,
            "issue_id": 5154,
            "pub_date": "2025-08-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 1",
                "zh": "8æœˆ1æ—¥"
            },
            "hash": "7d97f0b64c1261dd",
            "authors": [
                "Yuqi Tang",
                "Kehua Feng",
                "Yunfeng Wang",
                "Zhiwen Chen",
                "Chengfei Lv",
                "Gang Yu",
                "Qiang Zhang",
                "Keyan Ding"
            ],
            "affiliations": [
                "Alibaba Group",
                "College of Computer Science and Technology, Zhejiang University",
                "ZJU-Hangzhou Global Scientific and Technological Innovation Center, Zhejiang University",
                "ZJU-UIUC Institute, Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.00454.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#interpretability",
                    "#inference",
                    "#alignment",
                    "#benchmark"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²: Ğ¼ÑƒĞ´Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ñ… Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… LLM Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº, Ğ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑĞµĞ¼Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ Ğ¸ Ğ³Ğ¸Ğ±ĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Efficient Dialogue Evaluation: Harnessing Collective Wisdom of LLMs",
                    "desc": "This paper introduces an efficient multi-turn dialogue evaluator that combines the judgments of multiple large language models (LLMs) to assess dialogue quality. Traditional methods using a single LLM as a judge often face biases that affect evaluation reliability. The proposed method aggregates the preferences of several LLMs into one model, maintaining the benefits of diverse feedback while significantly lowering computational costs. Experiments show that this new approach outperforms existing methods in various evaluation scenarios, proving its effectiveness and efficiency."
                },
                "zh": {
                    "title": "é«˜æ•ˆçš„å¤šè½®å¯¹è¯è¯„ä¼°å™¨ï¼šèšåˆæ™ºæ…§ï¼Œé™ä½æˆæœ¬",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å¤šè½®å¯¹è¯è¯„ä¼°å™¨ï¼Œé€šè¿‡å°†å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åˆ¤æ–­æ±‡èšæˆä¸€ä¸ªå•ä¸€æ¨¡å‹æ¥è¯„ä¼°å¯¹è¯è´¨é‡ï¼Œä»è€Œé™ä½è®¡ç®—æˆæœ¬ã€‚å½“å‰çš„è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–äºâ€œLLMä½œä¸ºè¯„å®¡â€çš„æ¨¡å¼ï¼Œä½†è¿™ç§æ–¹æ³•å¸¸å¸¸å—åˆ°åè§çš„å½±å“ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœçš„ä¸å¯é æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡çš„æ–¹æ³•åˆ©ç”¨å¤šä¸ªLLMä½œä¸ºè¯„å®¡ï¼Œå¹¶å°†å®ƒä»¬çš„åå¥½çŸ¥è¯†æ±‡èšåˆ°ä¸€ä¸ªæ¨¡å‹ä¸­ï¼Œä»è€Œä¿ç•™å¤šè¯„å®¡åé¦ˆçš„ä¼˜åŠ¿ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘è¯„ä¼°æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§å¯¹è¯è¯„ä¼°åŸºå‡†ä¸Šä¼˜äºç°æœ‰çš„åŸºçº¿ï¼Œå±•ç¤ºäº†å…¶é«˜æ•ˆæ€§å’Œé²æ£’æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-08-01.html",
    "link_next": "2025-08-05.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "01.08",
        "en": "08/01",
        "zh": "8æœˆ1æ—¥"
    },
    "short_date_next": {
        "ru": "05.08",
        "en": "08/05",
        "zh": "8æœˆ5æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 5,
        "#agents": 3,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 3,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 1,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}