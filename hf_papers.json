{
    "date": {
        "ru": "9 декабря",
        "en": "December 9",
        "zh": "12月9日"
    },
    "time_utc": "2024-12-09 03:32",
    "weekday": 0,
    "issue_id": 1011,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.04445",
            "title": "Moto: Latent Motion Token as the Bridging Language for Robot Manipulation",
            "url": "https://huggingface.co/papers/2412.04445",
            "abstract": "Recent developments in Large Language Models pre-trained on extensive corpora have shown significant success in various natural language processing tasks with minimal fine-tuning. This success offers new promise for robotics, which has long been constrained by the high cost of action-labeled data. We ask: given the abundant video data containing interaction-related knowledge available as a rich \"corpus\", can a similar generative pre-training approach be effectively applied to enhance robot learning? The key challenge is to identify an effective representation for autoregressive pre-training that benefits robot manipulation tasks. Inspired by the way humans learn new skills through observing dynamic environments, we propose that effective robotic learning should emphasize motion-related knowledge, which is closely tied to low-level actions and is hardware-agnostic, facilitating the transfer of learned motions to actual robot actions. To this end, we introduce Moto, which converts video content into latent Motion Token sequences by a Latent Motion Tokenizer, learning a bridging \"language\" of motion from videos in an unsupervised manner. We pre-train Moto-GPT through motion token autoregression, enabling it to capture diverse visual motion knowledge. After pre-training, Moto-GPT demonstrates the promising ability to produce semantically interpretable motion tokens, predict plausible motion trajectories, and assess trajectory rationality through output likelihood. To transfer learned motion priors to real robot actions, we implement a co-fine-tuning strategy that seamlessly bridges latent motion token prediction and real robot control. Extensive experiments show that the fine-tuned Moto-GPT exhibits superior robustness and efficiency on robot manipulation benchmarks, underscoring its effectiveness in transferring knowledge from video data to downstream visual manipulation tasks.",
            "score": 11,
            "issue_id": 1011,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 декабря",
                "en": "December 5",
                "zh": "12月5日"
            },
            "hash": "a5ac6d786500ef9f",
            "authors": [
                "Yi Chen",
                "Yuying Ge",
                "Yizhuo Li",
                "Yixiao Ge",
                "Mingyu Ding",
                "Ying Shan",
                "Xihui Liu"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "The University of Hong Kong",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04445.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#transfer_learning",
                    "#games",
                    "#robotics",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Обучение роботов движению через предобучение на видеоданных",
                    "desc": "В статье представлен новый подход к обучению роботов на основе предобученных языковых моделей. Авторы предлагают метод Moto, который преобразует видеоконтент в последовательности латентных токенов движения с помощью Latent Motion Tokenizer. Moto-GPT обучается на основе авторегрессии токенов движения, что позволяет ей захватывать разнообразные знания о визуальном движении. После предобучения и дообучения Moto-GPT демонстрирует высокую эффективность в задачах манипуляции роботов."
                },
                "en": {
                    "title": "Bridging Video Knowledge to Robot Actions with Moto-GPT",
                    "desc": "This paper explores the application of Large Language Models to enhance robot learning by leveraging abundant video data. It introduces Moto, a system that converts video content into Motion Token sequences, allowing robots to learn motion-related knowledge in an unsupervised manner. The approach emphasizes the importance of motion understanding for effective robotic manipulation, enabling the transfer of learned skills to real-world actions. Through extensive experiments, Moto-GPT demonstrates improved performance in robot manipulation tasks, showcasing its ability to bridge the gap between video knowledge and practical robot control."
                },
                "zh": {
                    "title": "利用视频数据提升机器人学习能力",
                    "desc": "这篇论文探讨了如何利用大规模视频数据来提升机器人学习能力。研究者提出了一种名为Moto的模型，通过将视频内容转换为潜在的运动标记序列，来学习运动知识。Moto-GPT经过预训练后，能够生成可解释的运动标记，并预测合理的运动轨迹。最终，研究表明，经过微调的Moto-GPT在机器人操作基准测试中表现出色，证明了从视频数据转移知识的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04440",
            "title": "GenMAC: Compositional Text-to-Video Generation with Multi-Agent Collaboration",
            "url": "https://huggingface.co/papers/2412.04440",
            "abstract": "Text-to-video generation models have shown significant progress in the recent years. However, they still struggle with generating complex dynamic scenes based on compositional text prompts, such as attribute binding for multiple objects, temporal dynamics associated with different objects, and interactions between objects. Our key motivation is that complex tasks can be decomposed into simpler ones, each handled by a role-specialized MLLM agent. Multiple agents can collaborate together to achieve collective intelligence for complex goals. We propose GenMAC, an iterative, multi-agent framework that enables compositional text-to-video generation. The collaborative workflow includes three stages: Design, Generation, and Redesign, with an iterative loop between the Generation and Redesign stages to progressively verify and refine the generated videos. The Redesign stage is the most challenging stage that aims to verify the generated videos, suggest corrections, and redesign the text prompts, frame-wise layouts, and guidance scales for the next iteration of generation. To avoid hallucination of a single MLLM agent, we decompose this stage to four sequentially-executed MLLM-based agents: verification agent, suggestion agent, correction agent, and output structuring agent. Furthermore, to tackle diverse scenarios of compositional text-to-video generation, we design a self-routing mechanism to adaptively select the proper correction agent from a collection of correction agents each specialized for one scenario. Extensive experiments demonstrate the effectiveness of GenMAC, achieving state-of-the art performance in compositional text-to-video generation.",
            "score": 8,
            "issue_id": 1011,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 декабря",
                "en": "December 5",
                "zh": "12月5日"
            },
            "hash": "28dc2191ba71c4ea",
            "authors": [
                "Kaiyi Huang",
                "Yukun Huang",
                "Xuefei Ning",
                "Zinan Lin",
                "Yu Wang",
                "Xihui Liu"
            ],
            "affiliations": [
                "Microsoft Research",
                "The University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04440.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#agents",
                    "#games",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Коллективный интеллект агентов для создания сложных видео по тексту",
                    "desc": "Статья представляет GenMAC - итеративную мультиагентную систему для генерации видео по текстовому описанию. Система использует несколько специализированных агентов на основе мультимодальных языковых моделей (MLLM) для декомпозиции сложных задач. Процесс включает этапы проектирования, генерации и перепроектирования, с итеративным циклом между генерацией и перепроектированием для постепенного улучшения результата. GenMAC демонстрирует передовые результаты в генерации композиционных видео по текстовому описанию."
                },
                "en": {
                    "title": "Collaborative Intelligence for Text-to-Video Mastery",
                    "desc": "This paper introduces GenMAC, a multi-agent framework designed to improve text-to-video generation by breaking down complex tasks into simpler components. It utilizes specialized machine learning agents that collaborate through three main stages: Design, Generation, and Redesign, with an iterative process to refine video outputs. The Redesign stage is particularly crucial as it involves multiple agents that verify, suggest corrections, and restructure prompts to enhance the generated videos. The framework also includes a self-routing mechanism to select the most suitable correction agent for various scenarios, leading to significant advancements in generating dynamic scenes from text prompts."
                },
                "zh": {
                    "title": "GenMAC：协作生成复杂视频的智能框架",
                    "desc": "文本到视频生成模型近年来取得了显著进展，但在生成复杂动态场景时仍面临挑战。我们提出了一种名为GenMAC的多代理框架，通过将复杂任务分解为简单任务来实现协作生成。该框架包括设计、生成和重新设计三个阶段，生成和重新设计之间存在迭代循环，以逐步验证和优化生成的视频。通过自适应选择专门化的修正代理，GenMAC能够有效应对多样化的文本到视频生成场景。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04862",
            "title": "EXAONE 3.5: Series of Large Language Models for Real-world Use Cases",
            "url": "https://huggingface.co/papers/2412.04862",
            "abstract": "This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8B, and 2.4B. These models feature several standout capabilities: 1) exceptional instruction following capabilities in real-world scenarios, achieving the highest scores across seven benchmarks, 2) outstanding long-context comprehension, attaining the top performance in four benchmarks, and 3) competitive results compared to state-of-the-art open models of similar sizes across nine general benchmarks. The EXAONE 3.5 language models are open to anyone for research purposes and can be downloaded from https://huggingface.co/LGAI-EXAONE. For commercial use, please reach out to the official contact point of LG AI Research: contact_us@lgresearch.ai.",
            "score": 1,
            "issue_id": 1011,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 декабря",
                "en": "December 6",
                "zh": "12月6日"
            },
            "hash": "83e6f957e42ebb5e",
            "authors": [
                "LG AI Research",
                "Soyoung An",
                "Kyunghoon Bae",
                "Eunbi Choi",
                "Kibong Choi",
                "Stanley Jungkyu Choi",
                "Seokhee Hong",
                "Junwon Hwang",
                "Hyojin Jeon",
                "Gerrard Jeongwon Jo",
                "Hyunjik Jo",
                "Jiyeon Jung",
                "Yountae Jung",
                "Hyosang Kim",
                "Joonkee Kim",
                "Seonghwan Kim",
                "Soyeon Kim",
                "Sunkyoung Kim",
                "Yireun Kim",
                "Yongil Kim",
                "Youchul Kim",
                "Edward Hwayoung Lee",
                "Haeju Lee",
                "Honglak Lee",
                "Jinsik Lee",
                "Kyungmin Lee",
                "Woohyung Lim",
                "Sangha Park",
                "Sooyoun Park",
                "Yongmin Park",
                "Sihoon Yang",
                "Heuiyeen Yeen",
                "Hyeongu Yun"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2412.04862.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#open_source",
                    "#small_models",
                    "#long_context",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "EXAONE 3.5: Новое слово в языковых моделях от LG AI Research",
                    "desc": "Компания LG AI Research представила языковые модели EXAONE 3.5, обученные выполнению инструкций. Модели доступны в трех конфигурациях: 32B, 7.8B и 2.4B параметров. Они демонстрируют исключительные способности в следовании инструкциям, понимании длинного контекста и показывают конкурентоспособные результаты по сравнению с современными открытыми моделями аналогичного размера. Модели EXAONE 3.5 доступны для исследовательских целей и могут быть загружены с платформы Hugging Face."
                },
                "en": {
                    "title": "EXAONE 3.5: Leading the Way in Instruction-Tuned Language Models",
                    "desc": "The EXAONE 3.5 language models, created by LG AI Research, are advanced instruction-tuned models available in three sizes: 32B, 7.8B, and 2.4B. They excel in following instructions in real-world applications, achieving top scores on seven different benchmarks. Additionally, these models demonstrate superior long-context understanding, ranking first in four benchmarks. They are accessible for research purposes and show competitive performance against leading open models of similar sizes across nine general benchmarks."
                },
                "zh": {
                    "title": "EXAONE 3.5：指令跟随与长上下文理解的先锋",
                    "desc": "EXAONE 3.5 是由 LG AI 研究所开发的指令调优语言模型，提供三种配置：32B、7.8B 和 2.4B。这些模型在实际场景中具有卓越的指令跟随能力，在七个基准测试中取得了最高分。它们在长上下文理解方面表现出色，在四个基准测试中也达到了最佳性能。此外，与同类开源模型相比，EXAONE 3.5 在九个通用基准测试中也展现了竞争力的结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04301",
            "title": "SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step Diffusion",
            "url": "https://huggingface.co/papers/2412.04301",
            "abstract": "Recent advances in text-guided image editing enable users to perform image edits through simple text inputs, leveraging the extensive priors of multi-step diffusion-based text-to-image models. However, these methods often fall short of the speed demands required for real-world and on-device applications due to the costly multi-step inversion and sampling process involved. In response to this, we introduce SwiftEdit, a simple yet highly efficient editing tool that achieve instant text-guided image editing (in 0.23s). The advancement of SwiftEdit lies in its two novel contributions: a one-step inversion framework that enables one-step image reconstruction via inversion and a mask-guided editing technique with our proposed attention rescaling mechanism to perform localized image editing. Extensive experiments are provided to demonstrate the effectiveness and efficiency of SwiftEdit. In particular, SwiftEdit enables instant text-guided image editing, which is extremely faster than previous multi-step methods (at least 50 times faster) while maintain a competitive performance in editing results. Our project page is at: https://swift-edit.github.io/",
            "score": 1,
            "issue_id": 1011,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 декабря",
                "en": "December 5",
                "zh": "12月5日"
            },
            "hash": "a4cea89a59a9a3c0",
            "authors": [
                "Trong-Tung Nguyen",
                "Quang Nguyen",
                "Khoi Nguyen",
                "Anh Tran",
                "Cuong Pham"
            ],
            "affiliations": [
                "Posts & Telecom. Inst. of Tech., Vietnam",
                "VinAI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04301.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Мгновенное редактирование изображений текстом",
                    "desc": "SwiftEdit - это новый инструмент для быстрого редактирования изображений с помощью текстовых запросов. В отличие от существующих многошаговых методов, SwiftEdit использует одношаговую инверсию и механизм масштабирования внимания для локального редактирования. Это позволяет выполнять изменения изображений практически мгновенно - за 0.23 секунды. SwiftEdit работает как минимум в 50 раз быстрее аналогов, сохраняя при этом высокое качество результатов."
                },
                "en": {
                    "title": "SwiftEdit: Instant Text-Guided Image Editing at Lightning Speed!",
                    "desc": "This paper presents SwiftEdit, a new tool for fast text-guided image editing that significantly improves the speed of image modifications. Traditional methods rely on multi-step diffusion processes, which can be slow and inefficient for real-time applications. SwiftEdit introduces a one-step inversion framework that allows for quick image reconstruction and a mask-guided editing technique that uses attention rescaling for precise edits. The results show that SwiftEdit is at least 50 times faster than previous methods while still delivering high-quality editing outcomes."
                },
                "zh": {
                    "title": "SwiftEdit：瞬时文本引导图像编辑的革命",
                    "desc": "最近的文本引导图像编辑技术使用户能够通过简单的文本输入进行图像编辑，利用了多步扩散模型的丰富先验知识。然而，这些方法在实际应用中往往速度较慢，无法满足实时和设备端的需求。为此，我们提出了SwiftEdit，这是一种简单而高效的编辑工具，可以实现瞬时的文本引导图像编辑，速度达到0.23秒。SwiftEdit的创新在于其一体化的反演框架和基于掩码的编辑技术，能够快速进行局部图像编辑，同时保持与传统多步方法相当的编辑效果。"
                }
            }
        }
    ],
    "link_prev": "2024-12-06.html",
    "link_next": "2024-12-10.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "06.12",
        "en": "12/06",
        "zh": "12月6日"
    },
    "short_date_next": {
        "ru": "10.12",
        "en": "12/10",
        "zh": "12月10日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 1,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "最近的视觉-语言模型通过增加视觉标记的长度提高了性能，但也增加了计算成本。我们发现，流行的视觉编码器生成的视觉标记存在大量冗余。为解决这个问题，我们引入了VisionZip，一种选择信息丰富的视觉标记的方法，减少冗余并提高效率。VisionZip可应用于图像和视频理解任务，适用于多轮对话。实验结果显示，VisionZip在几乎所有设置中都优于之前的最佳方法，并显著提高了模型推理速度。",
        "title": "VisionZip: Longer is Better but Not Necessary in Vision Language Models",
        "pinyin": "最近的视觉-语言模型通过增加视觉标记的长度提高了性能，但也增加了计算成本。我们发现，流行的视觉编码器生成的视觉标记存在大量冗余。为解决这个问题，我们引入了VisionZip，一种选择信息丰富的视觉标记的方法，减少冗余并提高效率。VisionZip可应用于图像和视频理解任务，适用于多轮对话。实验结果显示，VisionZip在几乎所有设置中都优于之前的最佳方法，并显著提高了模型推理速度。\n\nZuìjìn de shìjué-yǔyán móxíng tōngguò zēngjiā shìjué biāojì de chángdù tígāo le xìngnéng, dàn yě zēngjiā le jìsuàn chéngběn. Wǒmen fāxiàn, liúxíng de shìjué biānmǎqì shēngchéng de shìjué biāojì cúnzài dàliàng rǒngyù. Wèi jiějué zhègè wèntí, wǒmen yǐnrù le VisionZip, yīzhǒng xuǎnzé xìnxī fēngfù de shìjué biāojì de fāngfǎ, jiǎnshǎo rǒngyù bìng tígāo xiàolǜ. VisionZip kě yìngyòng yú túxiàng hé shìpǐn lǐjiě rènwù, shìyòng yú duōlún duìhuà. Shíyàn jiéguǒ xiǎnshì, VisionZip zài jīhuā suǒyǒu shèzhì zhōng dōu yōu zhīqián de zuìjiā fāngfǎ, bìng xiǎnzhù tígāo le móxíng tuīlǐ sùdù.",
        "vocab": "[{'word': '视觉', 'pinyin': 'shìjué', 'trans': 'vision'}, {'word': '语言', 'pinyin': 'yǔyán', 'trans': 'language'}, {'word': '模型', 'pinyin': 'móxíng', 'trans': 'model'}, {'word': '增加', 'pinyin': 'zēngjiā', 'trans': 'increase'}, {'word': '长度', 'pinyin': 'chángdù', 'trans': 'length'}, {'word': '性能', 'pinyin': 'xìngnéng', 'trans': 'performance'}, {'word': '计算', 'pinyin': 'jìsuàn', 'trans': 'calculation'}, {'word': '成本', 'pinyin': 'chéngběn', 'trans': 'cost'}, {'word': '发现', 'pinyin': 'fāxiàn', 'trans': 'discover'}, {'word': '流行', 'pinyin': 'liúxíng', 'trans': 'popular'}, {'word': '编码器', 'pinyin': 'biānmǎqì', 'trans': 'encoder'}, {'word': '生成', 'pinyin': 'shēngchéng', 'trans': 'generate'}, {'word': '标记', 'pinyin': 'biāojì', 'trans': 'token'}, {'word': '存在', 'pinyin': 'cúnzài', 'trans': 'exist'}, {'word': '冗余', 'pinyin': 'róngyú', 'trans': 'redundancy'}, {'word': '解决', 'pinyin': 'jiějué', 'trans': 'solve'}, {'word': '引入', 'pinyin': 'yǐnrù', 'trans': 'introduce'}, {'word': '选择', 'pinyin': 'xuǎnzé', 'trans': 'select'}, {'word': '丰富', 'pinyin': 'fēngfù', 'trans': 'rich'}, {'word': '方法', 'pinyin': 'fāngfǎ', 'trans': 'method'}, {'word': '减少', 'pinyin': 'jiǎnshǎo', 'trans': 'reduce'}, {'word': '效率', 'pinyin': 'xiàolǜ', 'trans': 'efficiency'}, {'word': '应用', 'pinyin': 'yìngyòng', 'trans': 'apply'}, {'word': '图像', 'pinyin': 'túxiàng', 'trans': 'image'}, {'word': '视频', 'pinyin': 'shìpín', 'trans': 'video'}, {'word': '理解', 'pinyin': 'lǐjiě', 'trans': 'understanding'}, {'word': '任务', 'pinyin': 'rènwù', 'trans': 'task'}, {'word': '适用', 'pinyin': 'shìyòng', 'trans': 'applicable'}, {'word': '多轮', 'pinyin': 'duōlún', 'trans': 'multi-turn'}, {'word': '对话', 'pinyin': 'duìhuà', 'trans': 'dialogue'}, {'word': '实验', 'pinyin': 'shíyàn', 'trans': 'experiment'}, {'word': '结果', 'pinyin': 'jiéguǒ', 'trans': 'result'}, {'word': '显示', 'pinyin': 'xiǎnshì', 'trans': 'show'}, {'word': '优于', 'pinyin': 'yōuyú', 'trans': 'superior to'}, {'word': '之前', 'pinyin': 'zhīqián', 'trans': 'previous'}, {'word': '最佳', 'pinyin': 'zuìjiā', 'trans': 'best'}, {'word': '设置', 'pinyin': 'shèzhì', 'trans': 'setting'}, {'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'}, {'word': '提高', 'pinyin': 'tígāo', 'trans': 'improve'}, {'word': '推理', 'pinyin': 'tuīlǐ', 'trans': 'inference'}, {'word': '速度', 'pinyin': 'sùdù', 'trans': 'speed'}]",
        "trans": "Recent vision-language models have improved performance by increasing the length of visual tokens, but this has also increased computational costs. We have discovered that popular visual encoders generate visual tokens with a significant amount of redundancy. To address this issue, we introduce VisionZip, a method for selecting information-rich visual tokens to reduce redundancy and enhance efficiency. VisionZip can be applied to both image and video understanding tasks and is suitable for multi-turn conversations. Experimental results show that VisionZip outperforms previous best methods in almost all settings and significantly speeds up model inference.",
        "update_ts": "2024-12-08 12:42"
    }
}