{
    "date": {
        "ru": "23 июля",
        "en": "July 23",
        "zh": "7月23日"
    },
    "time_utc": "2025-07-23 02:59",
    "weekday": 2,
    "issue_id": 4959,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.16815",
            "title": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent\n  Planning",
            "url": "https://huggingface.co/papers/2507.16815",
            "abstract": "ThinkAct, a dual-system framework, uses reinforced visual latent planning to enable high-level reasoning and robust action execution in vision-language-action tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language-action (VLA) reasoning tasks require agents to interpret multimodal instructions, perform long-horizon planning, and act adaptively in dynamic environments. Existing approaches typically train VLA models in an end-to-end fashion, directly mapping inputs to actions without explicit reasoning, which hinders their ability to plan over multiple steps or adapt to complex task variations. In this paper, we propose ThinkAct, a dual-system framework that bridges high-level reasoning with low-level action execution via reinforced visual latent planning. ThinkAct trains a multimodal LLM to generate embodied reasoning plans guided by reinforcing action-aligned visual rewards based on goal completion and trajectory consistency. These reasoning plans are compressed into a visual plan latent that conditions a downstream action model for robust action execution on target environments. Extensive experiments on embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct enables few-shot adaptation, long-horizon planning, and self-correction behaviors in complex embodied AI tasks.",
            "score": 5,
            "issue_id": 4959,
            "pub_date": "2025-07-22",
            "pub_date_card": {
                "ru": "22 июля",
                "en": "July 22",
                "zh": "7月22日"
            },
            "hash": "a0c0c8cc661a52b5",
            "authors": [
                "Chi-Pin Huang",
                "Yueh-Hua Wu",
                "Min-Hung Chen",
                "Yu-Chiang Frank Wang",
                "Fu-En Yang"
            ],
            "affiliations": [
                "NVIDIA",
                "National Taiwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.16815.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#robotics",
                    "#training",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Думай и действуй: интеллектуальное планирование для воплощенного ИИ",
                    "desc": "ThinkAct - это двухсистемная архитектура для задач визуально-языкового взаимодействия. Она использует подкрепленное визуальное латентное планирование для высокоуровневых рассуждений и надежного выполнения действий. ThinkAct обучает мультимодальную языковую модель генерировать планы рассуждений, основываясь на визуальных наградах, связанных с действиями. Эти планы сжимаются в визуальный латентный план, который используется для управления моделью действий в целевой среде."
                },
                "en": {
                    "title": "ThinkAct: Bridging Reasoning and Action in AI Tasks",
                    "desc": "ThinkAct is a novel framework designed for vision-language-action tasks that combines high-level reasoning with low-level action execution. It utilizes reinforced visual latent planning to create effective plans that guide agents in dynamic environments. By training a multimodal large language model (LLM), ThinkAct generates reasoning plans that are optimized through visual rewards, ensuring actions align with goals. The framework shows significant improvements in few-shot adaptation and long-horizon planning, making it suitable for complex AI tasks like robot manipulation."
                },
                "zh": {
                    "title": "ThinkAct：高效推理与动作执行的双系统框架",
                    "desc": "ThinkAct是一个双系统框架，旨在通过强化视觉潜在规划实现高水平推理和稳健的动作执行。该框架能够处理视觉-语言-动作（VLA）任务，帮助智能体理解多模态指令并进行长远规划。与传统的端到端训练方法不同，ThinkAct通过生成与动作对齐的视觉奖励来指导推理计划，从而提高了智能体在复杂环境中的适应能力。实验结果表明，ThinkAct在少量样本适应、长远规划和自我修正行为方面表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.16812",
            "title": "MegaScience: Pushing the Frontiers of Post-Training Datasets for Science\n  Reasoning",
            "url": "https://huggingface.co/papers/2507.16812",
            "abstract": "MegaScience, a large-scale dataset of scientific reasoning questions, enhances the performance and training efficiency of AI models compared to existing datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Scientific reasoning is critical for developing AI scientists and supporting human researchers in advancing the frontiers of natural science discovery. However, the open-source community has primarily focused on mathematics and coding while neglecting the scientific domain, largely due to the absence of open, large-scale, high-quality, verifiable scientific reasoning datasets. To bridge this gap, we first present TextbookReasoning, an open dataset featuring truthful reference answers extracted from 12k university-level scientific textbooks, comprising 650k reasoning questions spanning 7 scientific disciplines. We further introduce MegaScience, a large-scale mixture of high-quality open-source datasets totaling 1.25 million instances, developed through systematic ablation studies that evaluate various data selection methodologies to identify the optimal subset for each publicly available scientific dataset. Meanwhile, we build a comprehensive evaluation system covering diverse subjects and question types across 15 benchmarks, incorporating comprehensive answer extraction strategies to ensure accurate evaluation metrics. Our experiments demonstrate that our datasets achieve superior performance and training efficiency with more concise response lengths compared to existing open-source scientific datasets. Furthermore, we train Llama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which significantly outperform the corresponding official instruct models in average performance. In addition, MegaScience exhibits greater effectiveness for larger and stronger models, suggesting a scaling benefit for scientific tuning. We release our data curation pipeline, evaluation system, datasets, and seven trained models to the community to advance scientific reasoning research.",
            "score": 3,
            "issue_id": 4959,
            "pub_date": "2025-07-22",
            "pub_date_card": {
                "ru": "22 июля",
                "en": "July 22",
                "zh": "7月22日"
            },
            "hash": "e6653e3f0a1b904f",
            "authors": [
                "Run-Ze Fan",
                "Zengzhi Wang",
                "Pengfei Liu"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University, SII, GAIR Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.16812.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#science",
                    "#data",
                    "#open_source",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "MegaScience: прорыв в научном мышлении ИИ",
                    "desc": "MegaScience - это крупномасштабный набор данных научных рассуждений, улучшающий производительность и эффективность обучения моделей ИИ по сравнению с существующими наборами данных. Он включает в себя 1,25 миллиона примеров из 7 научных дисциплин, полученных из учебников университетского уровня и других высококачественных источников. Авторы разработали комплексную систему оценки, охватывающую различные предметы и типы вопросов по 15 эталонным тестам. Эксперименты показывают, что модели, обученные на MegaScience, значительно превосходят официальные инструктивные модели по средней производительности."
                },
                "en": {
                    "title": "Empowering AI with MegaScience for Superior Scientific Reasoning",
                    "desc": "This paper introduces MegaScience, a large-scale dataset designed to improve AI models' performance in scientific reasoning tasks. It addresses the lack of high-quality, open-source datasets in the scientific domain by providing 1.25 million instances of reasoning questions derived from university-level textbooks. The authors conducted systematic studies to select the best data subsets, ensuring that the dataset is both comprehensive and effective for training AI models. Their experiments show that models trained on MegaScience outperform existing models, highlighting its potential to enhance scientific reasoning capabilities in AI."
                },
                "zh": {
                    "title": "MegaScience：推动科学推理的未来",
                    "desc": "MegaScience是一个大规模的科学推理问题数据集，旨在提高人工智能模型的性能和训练效率。该数据集包含从12000本大学级科学教科书中提取的真实参考答案，涵盖650000个推理问题，涉及7个科学学科。通过系统的消融研究，我们开发了1.25百万实例的高质量开放源数据集，并建立了全面的评估系统，以确保准确的评估指标。我们的实验表明，MegaScience在训练效率和响应长度上优于现有的开放源科学数据集，特别适合更大和更强的模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.16746",
            "title": "Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning",
            "url": "https://huggingface.co/papers/2507.16746",
            "abstract": "Humans often use visual aids, for example diagrams or sketches, when solving complex problems. Training multimodal models to do the same, known as Visual Chain of Thought (Visual CoT), is challenging due to: (1) poor off-the-shelf visual CoT performance, which hinders reinforcement learning, and (2) the lack of high-quality visual CoT training data. We introduce Zebra-CoT, a diverse large-scale dataset with 182,384 samples, containing logically coherent interleaved text-image reasoning traces. We focus on four categories of tasks where sketching or visual reasoning is especially natural, spanning scientific questions such as geometry, physics, and algorithms; 2D visual reasoning tasks like visual search and jigsaw puzzles; 3D reasoning tasks including 3D multi-hop inference, embodied and robot planning; visual logic problems and strategic games like chess. Fine-tuning the Anole-7B model on the Zebra-CoT training corpus results in an improvement of +12% in our test-set accuracy and yields up to +13% performance gain on standard VLM benchmark evaluations. Fine-tuning Bagel-7B yields a model that generates high-quality interleaved visual reasoning chains, underscoring Zebra-CoT's effectiveness for developing multimodal reasoning abilities. We open-source our dataset and models to support development and evaluation of visual CoT.",
            "score": 3,
            "issue_id": 4959,
            "pub_date": "2025-07-22",
            "pub_date_card": {
                "ru": "22 июля",
                "en": "July 22",
                "zh": "7月22日"
            },
            "hash": "195867d3f8c130bf",
            "authors": [
                "Ang Li",
                "Charles Wang",
                "Kaiyu Yue",
                "Zikui Cai",
                "Ollie Liu",
                "Deqing Fu",
                "Peng Guo",
                "Wang Bill Zhu",
                "Vatsal Sharan",
                "Robin Jia",
                "Willie Neiswanger",
                "Furong Huang",
                "Tom Goldstein",
                "Micah Goldblum"
            ],
            "affiliations": [
                "Columbia University",
                "New York University",
                "University of Maryland",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.16746.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#games",
                    "#optimization",
                    "#multimodal",
                    "#open_source",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "🦓",
                "ru": {
                    "title": "Zebra-CoT: Прорыв в обучении ИИ визуальному мышлению",
                    "desc": "Статья представляет Zebra-CoT - крупномасштабный набор данных для обучения мультимодальных моделей визуальному рассуждению. Набор содержит 182,384 образца с логически связанными текстово-визуальными цепочками рассуждений для различных задач, включая научные вопросы, 2D и 3D визуальное мышление, и стратегические игры. Файнтюнинг модели Anole-7B на этом корпусе улучшил точность на 12% в тестовом наборе и до 13% в стандартных бенчмарках VLM. Авторы открыли доступ к набору данных и моделям для поддержки разработки и оценки визуального цепочечного мышления."
                },
                "en": {
                    "title": "Zebra-CoT: Enhancing Visual Reasoning with a Rich Dataset",
                    "desc": "This paper presents Zebra-CoT, a large dataset designed to enhance multimodal models' ability to perform visual reasoning tasks. It addresses the challenges of poor performance in existing visual chain of thought (CoT) models and the scarcity of quality training data. The dataset includes 182,384 samples that combine text and images for various reasoning tasks, such as geometry and strategic games. Fine-tuning models like Anole-7B and Bagel-7B on this dataset significantly improves their accuracy and ability to generate coherent visual reasoning chains."
                },
                "zh": {
                    "title": "Zebra-CoT：提升多模态推理能力的关键数据集",
                    "desc": "本论文介绍了一种名为Zebra-CoT的大规模数据集，包含182,384个样本，旨在帮助多模态模型进行视觉推理。该数据集提供了逻辑连贯的文本-图像推理链，适用于科学问题、2D和3D推理任务以及视觉逻辑问题等多种任务。通过对Anole-7B模型进行微调，测试集准确率提高了12%，并在标准VLM基准评估中获得了高达13%的性能提升。我们开源了该数据集和模型，以支持视觉推理能力的开发和评估。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.16813",
            "title": "HOComp: Interaction-Aware Human-Object Composition",
            "url": "https://huggingface.co/papers/2507.16813",
            "abstract": "HOComp uses MLLMs and attention mechanisms to achieve seamless human-object interactions with consistent appearances in image compositing.  \t\t\t\t\tAI-generated summary \t\t\t\t While existing image-guided composition methods may help insert a foreground object onto a user-specified region of a background image, achieving natural blending inside the region with the rest of the image unchanged, we observe that these existing methods often struggle in synthesizing seamless interaction-aware compositions when the task involves human-object interactions. In this paper, we first propose HOComp, a novel approach for compositing a foreground object onto a human-centric background image, while ensuring harmonious interactions between the foreground object and the background person and their consistent appearances. Our approach includes two key designs: (1) MLLMs-driven Region-based Pose Guidance (MRPG), which utilizes MLLMs to identify the interaction region as well as the interaction type (e.g., holding and lefting) to provide coarse-to-fine constraints to the generated pose for the interaction while incorporating human pose landmarks to track action variations and enforcing fine-grained pose constraints; and (2) Detail-Consistent Appearance Preservation (DCAP), which unifies a shape-aware attention modulation mechanism, a multi-view appearance loss, and a background consistency loss to ensure consistent shapes/textures of the foreground and faithful reproduction of the background human. We then propose the first dataset, named Interaction-aware Human-Object Composition (IHOC), for the task. Experimental results on our dataset show that HOComp effectively generates harmonious human-object interactions with consistent appearances, and outperforms relevant methods qualitatively and quantitatively.",
            "score": 1,
            "issue_id": 4959,
            "pub_date": "2025-07-22",
            "pub_date_card": {
                "ru": "22 июля",
                "en": "July 22",
                "zh": "7月22日"
            },
            "hash": "5a513d80af038052",
            "authors": [
                "Dong Liang",
                "Jinyuan Jia",
                "Yuhao Liu",
                "Rynson W. H. Lau"
            ],
            "affiliations": [
                "CityUHK",
                "HKUST(GZ)",
                "Tongji University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.16813.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#synthetic",
                    "#games",
                    "#dataset"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Гармоничная композиция человека и объекта с помощью искусственного интеллекта",
                    "desc": "HOComp - это новый подход к композиции изображений, обеспечивающий гармоничное взаимодействие между объектом переднего плана и человеком на фоновом изображении. Он использует многоязычные языковые модели (MLLM) для определения региона и типа взаимодействия, а также механизмы внимания для сохранения деталей и согласованности внешнего вида. HOComp превосходит существующие методы в создании естественных композиций с взаимодействием человека и объекта. Авторы также представили новый набор данных IHOC для этой задачи."
                },
                "en": {
                    "title": "Seamless Human-Object Interaction in Image Compositing",
                    "desc": "HOComp is a new method that enhances how foreground objects interact with people in images, ensuring they blend naturally into the scene. It uses Multi-Layered Language Models (MLLMs) to guide the placement and pose of objects based on the type of interaction, like holding or lifting. Additionally, it employs a technique called Detail-Consistent Appearance Preservation (DCAP) to maintain the visual consistency of shapes and textures between the foreground and background. The paper also introduces a new dataset, Interaction-aware Human-Object Composition (IHOC), to evaluate the effectiveness of HOComp, which shows significant improvements over existing methods."
                },
                "zh": {
                    "title": "无缝人机互动的图像合成新方法",
                    "desc": "HOComp是一种新颖的方法，旨在将前景物体无缝地合成到以人为中心的背景图像中。该方法利用多语言大模型（MLLMs）和注意力机制，确保前景物体与背景人物之间的和谐互动及一致的外观。其核心设计包括基于区域的姿态引导和细节一致的外观保留，前者帮助识别互动区域和类型，后者确保前景和背景的一致性。实验结果表明，HOComp在生成自然的人物与物体互动方面表现优异，超越了相关方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15245",
            "title": "SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced\n  Academic Search",
            "url": "https://huggingface.co/papers/2507.15245",
            "abstract": "Recent advances in large language models (LLMs) have opened new opportunities for academic literature retrieval. However, existing systems often rely on rigid pipelines and exhibit limited reasoning capabilities. We introduce SPAR, a multi-agent framework that incorporates RefChain-based query decomposition and query evolution to enable more flexible and effective search. To facilitate systematic evaluation, we also construct SPARBench, a challenging benchmark with expert-annotated relevance labels. Experimental results demonstrate that SPAR substantially outperforms strong baselines, achieving up to +56% F1 on AutoScholar and +23% F1 on SPARBench over the best-performing baseline. Together, SPAR and SPARBench provide a scalable, interpretable, and high-performing foundation for advancing research in scholarly retrieval. Code and data will be available at: https://github.com/xiaofengShi/SPAR",
            "score": 0,
            "issue_id": 4959,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 июля",
                "en": "July 21",
                "zh": "7月21日"
            },
            "hash": "a01817799751346b",
            "authors": [
                "Xiaofeng Shi",
                "Yuduo Li",
                "Qian Kou",
                "Longbin Yu",
                "Jinxin Xie",
                "Hua Zhou"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence (BAAI)",
                "Beijing Jiaotong University (BJTU)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15245.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#survey",
                    "#interpretability",
                    "#reasoning"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "SPAR: Умный поиск научной литературы с помощью ИИ",
                    "desc": "SPAR - это мультиагентная система для поиска научной литературы, использующая большие языковые модели. Она применяет декомпозицию и эволюцию запросов на основе RefChain для более гибкого и эффективного поиска. Авторы также создали бенчмарк SPARBench с экспертной разметкой для оценки таких систем. Эксперименты показали, что SPAR значительно превосходит сильные базовые модели, достигая улучшения F1-меры до 56% на AutoScholar и 23% на SPARBench."
                },
                "en": {
                    "title": "Revolutionizing Academic Search with SPAR",
                    "desc": "This paper presents SPAR, a novel multi-agent framework designed to enhance academic literature retrieval using large language models. SPAR utilizes RefChain-based query decomposition and evolution, allowing for more adaptable and effective search strategies compared to traditional rigid systems. The authors also introduce SPARBench, a benchmark with expert-annotated relevance labels to systematically evaluate retrieval performance. Experimental results show that SPAR significantly improves retrieval accuracy, outperforming existing methods by notable margins on both AutoScholar and SPARBench datasets."
                },
                "zh": {
                    "title": "SPAR：提升学术检索的灵活性与效果",
                    "desc": "本文介绍了一种名为SPAR的多智能体框架，旨在提高学术文献检索的灵活性和有效性。SPAR通过基于RefChain的查询分解和查询演变技术，克服了现有系统的局限性。为了系统评估，我们构建了SPARBench，这是一个具有专家标注相关性标签的挑战性基准。实验结果表明，SPAR在AutoScholar和SPARBench上分别比最佳基线提高了56%和23%的F1分数，展示了其优越的性能。"
                }
            }
        }
    ],
    "link_prev": "2025-07-22.html",
    "link_next": "2025-07-24.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "22.07",
        "en": "07/22",
        "zh": "7月22日"
    },
    "short_date_next": {
        "ru": "24.07",
        "en": "07/24",
        "zh": "7月24日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 2,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 1,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}