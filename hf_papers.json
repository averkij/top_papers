{
    "date": {
        "ru": "25 марта",
        "en": "March 25",
        "zh": "3月25日"
    },
    "time_utc": "2025-03-25 04:13",
    "weekday": 1,
    "issue_id": 2876,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.17359",
            "title": "Position: Interactive Generative Video as Next-Generation Game Engine",
            "url": "https://huggingface.co/papers/2503.17359",
            "abstract": "Modern game development faces significant challenges in creativity and cost due to predetermined content in traditional game engines. Recent breakthroughs in video generation models, capable of synthesizing realistic and interactive virtual environments, present an opportunity to revolutionize game creation. In this position paper, we propose Interactive Generative Video (IGV) as the foundation for Generative Game Engines (GGE), enabling unlimited novel content generation in next-generation gaming. GGE leverages IGV's unique strengths in unlimited high-quality content synthesis, physics-aware world modeling, user-controlled interactivity, long-term memory capabilities, and causal reasoning. We present a comprehensive framework detailing GGE's core modules and a hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work charts a new course for game development in the AI era, envisioning a future where AI-powered generative systems fundamentally reshape how games are created and experienced.",
            "score": 27,
            "issue_id": 2875,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 марта",
                "en": "March 21",
                "zh": "3月21日"
            },
            "hash": "0046c940a41d8637",
            "authors": [
                "Jiwen Yu",
                "Yiran Qin",
                "Haoxuan Che",
                "Quande Liu",
                "Xintao Wang",
                "Pengfei Wan",
                "Di Zhang",
                "Xihui Liu"
            ],
            "affiliations": [
                "Kuaishou",
                "The Hong Kong University of Science and Technology",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.17359.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#architecture",
                    "#games",
                    "#multimodal"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "Революция в разработке игр: ИИ-генерируемые миры будущего",
                    "desc": "Статья предлагает концепцию Генеративных Игровых Движков (GGE), основанных на Интерактивной Генеративной Видео технологии (IGV). GGE позволяет создавать неограниченный новый контент для игр следующего поколения, используя преимущества IGV в синтезе высококачественного контента, моделировании физики мира и интерактивности. Авторы представляют комплексную структуру основных модулей GGE и иерархическую дорожную карту зрелости (L0-L4) для его развития. Это исследование открывает новые перспективы для разработки игр в эпоху искусственного интеллекта."
                },
                "en": {
                    "title": "Revolutionizing Game Development with AI-Driven Generative Engines",
                    "desc": "This paper discusses the limitations of traditional game engines that rely on fixed content, which can hinder creativity and increase costs in game development. It introduces Interactive Generative Video (IGV) as a new approach to create Generative Game Engines (GGE), which can produce endless unique game content. GGE utilizes advanced features like high-quality content synthesis, physics-aware modeling, and user interactivity to enhance the gaming experience. The authors also outline a framework and roadmap for the development of GGE, aiming to transform the future of game creation through AI technologies."
                },
                "zh": {
                    "title": "AI驱动的游戏创作新纪元",
                    "desc": "现代游戏开发面临着创造力和成本的重大挑战，传统游戏引擎的内容预设限制了创新。最近，视频生成模型的突破使得合成逼真且互动的虚拟环境成为可能，这为游戏创作带来了革命性的机会。我们提出了互动生成视频（IGV）作为生成游戏引擎（GGE）的基础，能够在下一代游戏中实现无限的新内容生成。GGE利用IGV在高质量内容合成、物理感知世界建模、用户控制互动、长期记忆能力和因果推理等方面的独特优势。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18942",
            "title": "Video-T1: Test-Time Scaling for Video Generation",
            "url": "https://huggingface.co/papers/2503.18942",
            "abstract": "With the scale capability of increasing training data, model size, and computational cost, video generation has achieved impressive results in digital creation, enabling users to express creativity across various domains. Recently, researchers in Large Language Models (LLMs) have expanded the scaling to test-time, which can significantly improve LLM performance by using more inference-time computation. Instead of scaling up video foundation models through expensive training costs, we explore the power of Test-Time Scaling (TTS) in video generation, aiming to answer the question: if a video generation model is allowed to use non-trivial amount of inference-time compute, how much can it improve generation quality given a challenging text prompt. In this work, we reinterpret the test-time scaling of video generation as a searching problem to sample better trajectories from Gaussian noise space to the target video distribution. Specifically, we build the search space with test-time verifiers to provide feedback and heuristic algorithms to guide searching process. Given a text prompt, we first explore an intuitive linear search strategy by increasing noise candidates at inference time. As full-step denoising all frames simultaneously requires heavy test-time computation costs, we further design a more efficient TTS method for video generation called Tree-of-Frames (ToF) that adaptively expands and prunes video branches in an autoregressive manner. Extensive experiments on text-conditioned video generation benchmarks demonstrate that increasing test-time compute consistently leads to significant improvements in the quality of videos. Project page: https://liuff19.github.io/Video-T1",
            "score": 19,
            "issue_id": 2876,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 марта",
                "en": "March 24",
                "zh": "3月24日"
            },
            "hash": "1d482b72d90d6136",
            "authors": [
                "Fangfu Liu",
                "Hanyang Wang",
                "Yimo Cai",
                "Kaiyan Zhang",
                "Xiaohang Zhan",
                "Yueqi Duan"
            ],
            "affiliations": [
                "Tencent",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18942.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#inference",
                    "#games",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Масштабирование времени тестирования: новый подход к улучшению генерации видео",
                    "desc": "Статья исследует применение метода масштабирования времени тестирования (TTS) для улучшения качества генерации видео по текстовому описанию. Авторы представляют этот процесс как задачу поиска лучших траекторий от гауссова шума к целевому распределению видео. Они предлагают два подхода: линейный поиск с увеличением кандидатов шума и более эффективный метод Tree-of-Frames (ToF), который адаптивно расширяет и обрезает ветви видео авторегрессивным способом. Эксперименты показывают, что увеличение вычислительных ресурсов на этапе тестирования значительно улучшает качество генерируемых видео."
                },
                "en": {
                    "title": "Unlocking Video Quality with Test-Time Scaling",
                    "desc": "This paper explores the concept of Test-Time Scaling (TTS) in video generation, which allows models to utilize additional computational resources during inference to enhance video quality. Instead of focusing solely on training larger models, the authors investigate how increasing inference-time computation can improve the generation of videos from text prompts. They propose a method called Tree-of-Frames (ToF) that efficiently manages the search for better video outputs by adaptively expanding and pruning video branches. The results show that leveraging more computational power at test time leads to significant improvements in the quality of generated videos."
                },
                "zh": {
                    "title": "测试时间扩展：提升视频生成质量的新方法",
                    "desc": "随着训练数据、模型规模和计算成本的增加，视频生成在数字创作中取得了显著成果。本文探讨了在视频生成中应用测试时间扩展（TTS）的潜力，旨在提高生成质量。我们将测试时间扩展重新解释为一个搜索问题，通过从高斯噪声空间中采样更好的轨迹来生成目标视频。实验结果表明，增加测试时间计算可以显著提升视频质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18940",
            "title": "Training-free Diffusion Acceleration with Bottleneck Sampling",
            "url": "https://huggingface.co/papers/2503.18940",
            "abstract": "Diffusion models have demonstrated remarkable capabilities in visual content generation but remain challenging to deploy due to their high computational cost during inference. This computational burden primarily arises from the quadratic complexity of self-attention with respect to image or video resolution. While existing acceleration methods often compromise output quality or necessitate costly retraining, we observe that most diffusion models are pre-trained at lower resolutions, presenting an opportunity to exploit these low-resolution priors for more efficient inference without degrading performance. In this work, we introduce Bottleneck Sampling, a training-free framework that leverages low-resolution priors to reduce computational overhead while preserving output fidelity. Bottleneck Sampling follows a high-low-high denoising workflow: it performs high-resolution denoising in the initial and final stages while operating at lower resolutions in intermediate steps. To mitigate aliasing and blurring artifacts, we further refine the resolution transition points and adaptively shift the denoising timesteps at each stage. We evaluate Bottleneck Sampling on both image and video generation tasks, where extensive experiments demonstrate that it accelerates inference by up to 3times for image generation and 2.5times for video generation, all while maintaining output quality comparable to the standard full-resolution sampling process across multiple evaluation metrics. Code is available at: https://github.com/tyfeld/Bottleneck-Sampling",
            "score": 6,
            "issue_id": 2875,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 марта",
                "en": "March 24",
                "zh": "3月24日"
            },
            "hash": "83ffcf1c20f5d4db",
            "authors": [
                "Ye Tian",
                "Xin Xia",
                "Yuxi Ren",
                "Shanchuan Lin",
                "Xing Wang",
                "Xuefeng Xiao",
                "Yunhai Tong",
                "Ling Yang",
                "Bin Cui"
            ],
            "affiliations": [
                "Bytedance",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18940.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#diffusion",
                    "#inference",
                    "#video"
                ],
                "emoji": "⏱️",
                "ru": {
                    "title": "Ускорение диффузионных моделей без потери качества",
                    "desc": "Статья представляет новый метод ускорения работы диффузионных моделей под названием Bottleneck Sampling. Этот подход использует предобученные низкоразрешающие модели для уменьшения вычислительных затрат без потери качества выходных данных. Метод следует схеме высокое-низкое-высокое разрешение при денойзинге, что позволяет ускорить вывод в 2.5-3 раза для задач генерации изображений и видео. Bottleneck Sampling не требует переобучения модели и сохраняет качество результатов на уровне стандартного полноразрешающего семплирования."
                },
                "en": {
                    "title": "Speeding Up Diffusion Models with Bottleneck Sampling",
                    "desc": "This paper presents Bottleneck Sampling, a new method to speed up diffusion models used for generating images and videos. Traditional diffusion models are slow because they use a complex self-attention mechanism that increases with image resolution. Bottleneck Sampling takes advantage of low-resolution training data to reduce the computational load during inference without sacrificing quality. By using a high-low-high denoising approach, it achieves significant speed improvements—up to 3 times faster for images and 2.5 times for videos—while still producing high-quality outputs."
                },
                "zh": {
                    "title": "瓶颈采样：高效的扩散模型推理",
                    "desc": "扩散模型在视觉内容生成方面表现出色，但在推理时由于计算成本高而难以部署。主要的计算负担来自于自注意力机制在图像或视频分辨率上的二次复杂性。我们提出了一种名为瓶颈采样的框架，利用低分辨率的先验知识来减少计算开销，同时保持输出质量。通过在高分辨率和低分辨率之间进行高低高的去噪工作流程，我们的实验表明，该方法在图像生成中加速推理速度可达3倍，在视频生成中可达2.5倍。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.17489",
            "title": "Judge Anything: MLLM as a Judge Across Any Modality",
            "url": "https://huggingface.co/papers/2503.17489",
            "abstract": "Evaluating generative foundation models on open-ended multimodal understanding (MMU) and generation (MMG) tasks across diverse modalities (e.g., images, audio, video) poses significant challenges due to the complexity of cross-modal interactions. To this end, the idea of utilizing Multimodal LLMs (MLLMs) as automated judges has emerged, with encouraging results in assessing vision-language understanding tasks. Moving further, this paper extends MLLM-as-a-Judge across modalities to a unified manner by introducing two benchmarks, TaskAnything and JudgeAnything, to respectively evaluate the overall performance and judging capabilities of MLLMs across any-to-any modality tasks. Specifically, TaskAnything evaluates the MMU and MMG capabilities across 15 any-to-any modality categories, employing 1,500 queries curated from well-established benchmarks. Furthermore, JudgeAnything evaluates the judging capabilities of 5 advanced (e.g., GPT-4o and Gemini-2.0-Flash) from the perspectives of Pair Comparison and Score Evaluation, providing a standardized testbed that incorporates human judgments and detailed rubrics. Our extensive experiments reveal that while these MLLMs show promise in assessing MMU (i.e., achieving an average of 66.55% in Pair Comparison setting and 42.79% in Score Evaluation setting), they encounter significant challenges with MMG tasks (i.e., averaging only 53.37% in Pair Comparison setting and 30.05% in Score Evaluation setting), exposing cross-modality biases and hallucination issues. To address this, we present OmniArena, an automated platform for evaluating omni-models and multimodal reward models. Our work highlights the need for fairer evaluation protocols and stronger alignment with human preferences. The source code and dataset are publicly available at: https://urrealhero.github.io/judgeanythingweb/.",
            "score": 6,
            "issue_id": 2875,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 марта",
                "en": "March 21",
                "zh": "3月21日"
            },
            "hash": "bb040618997e1b0a",
            "authors": [
                "Shu Pu",
                "Yaochen Wang",
                "Dongping Chen",
                "Yuhang Chen",
                "Guohao Wang",
                "Qi Qin",
                "Zhongyi Zhang",
                "Zhiyuan Zhang",
                "Zetong Zhou",
                "Shuang Gong",
                "Yi Gui",
                "Yao Wan",
                "Philip S. Yu"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "University of Illinois Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.17489.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#hallucinations",
                    "#benchmark",
                    "#alignment",
                    "#open_source"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Универсальная оценка мультимодальных ИИ-моделей: от понимания к генерации",
                    "desc": "Статья представляет новые бенчмарки TaskAnything и JudgeAnything для оценки мультимодальных языковых моделей (MLLM) в задачах понимания и генерации контента различных модальностей. TaskAnything оценивает способности MLLM в 15 категориях задач с различными комбинациями модальностей, используя 1500 запросов. JudgeAnything оценивает способности MLLM выступать в роли судей, сравнивая их оценки с человеческими по методикам попарного сравнения и балльной оценки. Результаты показывают, что MLLM лучше справляются с задачами понимания, чем с задачами генерации, выявляя проблемы межмодальных предубеждений и галлюцинаций."
                },
                "en": {
                    "title": "Enhancing Multimodal Evaluation with MLLMs",
                    "desc": "This paper discusses the challenges of evaluating generative foundation models in tasks that involve multiple types of data, like images and audio. It introduces Multimodal LLMs (MLLMs) as automated judges to assess these models' understanding and generation capabilities across different modalities. The authors present two benchmarks, TaskAnything and JudgeAnything, to systematically evaluate MLLMs' performance and judging abilities. The findings reveal that while MLLMs perform reasonably well in understanding tasks, they struggle with generation tasks, highlighting the need for improved evaluation methods and alignment with human preferences."
                },
                "zh": {
                    "title": "多模态评估的新视角",
                    "desc": "本论文探讨了在多模态理解（MMU）和生成（MMG）任务中评估生成基础模型的挑战，尤其是跨模态交互的复杂性。我们提出了使用多模态大语言模型（MLLMs）作为自动评估者的想法，并引入了两个基准：TaskAnything和JudgeAnything，分别用于评估MLLMs在任何模态任务中的整体性能和判断能力。实验结果显示，尽管MLLMs在MMU任务中表现出一定的潜力，但在MMG任务中面临显著挑战，暴露了跨模态偏见和幻觉问题。为了解决这些问题，我们提出了OmniArena，一个用于评估多模态模型和奖励模型的自动化平台。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.17439",
            "title": "LEMMA: Learning from Errors for MatheMatical Advancement in LLMs",
            "url": "https://huggingface.co/papers/2503.17439",
            "abstract": "Large language models (LLMs) have demonstrated remarkable reasoning capability in solving mathematical problems. However, existing approaches primarily focus on improving the quality of correct training data, e.g., distilling high-quality correct solutions from advanced models, neglecting the value contained in error data, potentially hindering the model's reflective ability. Though some studies attempt to leverage error data, they often involve complex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error nodes. In this work, we propose to enhance LLMs' reasoning ability by Learning from Errors for Mathematical Advancement (LEMMA). LEMMA constructs data consisting of an incorrect solution with an erroneous step and a reflection connection to a correct solution for fine-tuning. Specifically, we systematically analyze the model-generated error types and introduce an error-type grounded mistake augmentation method to collect diverse and representative errors. Correct solutions are either from fixing the errors or generating a fresh start. Through a model-aware smooth reflection connection, the erroneous solution is transferred to the correct one. By fine-tuning on the constructed dataset, the model is able to self-correct errors autonomously within the generation process without relying on external critique models. Experimental results demonstrate that LEMMA achieves significant performance improvements over other strong baselines.",
            "score": 5,
            "issue_id": 2875,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 марта",
                "en": "March 21",
                "zh": "3月21日"
            },
            "hash": "946d486485fedb03",
            "authors": [
                "Zhuoshi Pan",
                "Yu Li",
                "Honglin Lin",
                "Qizhi Pei",
                "Zinan Tang",
                "Wei Wu",
                "Chenlin Ming",
                "H. Vicky Zhao",
                "Conghui He",
                "Lijun Wu"
            ],
            "affiliations": [
                "Renmin University of China",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Tsinghua University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.17439.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#training",
                    "#reasoning",
                    "#dataset",
                    "#data"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Учимся на ошибках: новый подход к улучшению математических способностей ИИ",
                    "desc": "Эта статья предлагает метод LEMMA для улучшения способности больших языковых моделей (LLM) решать математические задачи путем обучения на ошибках. LEMMA создает набор данных, состоящий из неправильных решений с ошибочными шагами и связями с правильными решениями для дообучения модели. Авторы вводят метод аугментации ошибок на основе типов ошибок для сбора разнообразных и репрезентативных ошибок. Эксперименты показывают, что LEMMA значительно улучшает производительность по сравнению с другими сильными базовыми моделями."
                },
                "en": {
                    "title": "Empowering LLMs: Learning from Errors to Enhance Reasoning",
                    "desc": "This paper introduces a novel approach called Learning from Errors for Mathematical Advancement (LEMMA) to improve the reasoning capabilities of large language models (LLMs) in solving mathematical problems. Unlike traditional methods that focus solely on enhancing correct training data, LEMMA leverages the value of error data by constructing a dataset that includes incorrect solutions paired with reflections on correct solutions. The method systematically analyzes error types and employs a mistake augmentation technique to gather diverse errors, allowing the model to learn from its mistakes. By fine-tuning on this enriched dataset, LEMMA enables LLMs to autonomously correct their errors during the generation process, leading to significant performance gains compared to existing methods."
                },
                "zh": {
                    "title": "从错误中学习，提升数学推理能力",
                    "desc": "大型语言模型（LLMs）在解决数学问题时展现了出色的推理能力。现有的方法主要关注提高正确训练数据的质量，而忽视了错误数据的价值，这可能会妨碍模型的反思能力。我们提出了一种通过学习错误来提升数学推理能力的方法，称为LEMMA。该方法通过构建包含错误步骤的错误解和与正确解的反思连接的数据集，来进行模型的微调，从而使模型能够在生成过程中自主纠正错误。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18923",
            "title": "Video SimpleQA: Towards Factuality Evaluation in Large Video Language\n  Models",
            "url": "https://huggingface.co/papers/2503.18923",
            "abstract": "Recent advancements in Large Video Language Models (LVLMs) have highlighted their potential for multi-modal understanding, yet evaluating their factual grounding in video contexts remains a critical unsolved challenge. To address this gap, we introduce Video SimpleQA, the first comprehensive benchmark tailored for factuality evaluation of LVLMs. Our work distinguishes from existing video benchmarks through the following key features: 1) Knowledge required: demanding integration of external knowledge beyond the explicit narrative; 2) Fact-seeking question: targeting objective, undisputed events or relationships, avoiding subjective interpretation; 3) Definitive & short-form answer: Answers are crafted as unambiguous and definitively correct in a short format, enabling automated evaluation through LLM-as-a-judge frameworks with minimal scoring variance; 4) External-source verified: All annotations undergo rigorous validation against authoritative external references to ensure the reliability; 5) Temporal reasoning required: The annotated question types encompass both static single-frame understanding and dynamic temporal reasoning, explicitly evaluating LVLMs factuality under the long-context dependencies. We extensively evaluate 41 state-of-the-art LVLMs and summarize key findings as follows: 1) Current LVLMs exhibit notable deficiencies in factual adherence, particularly for open-source models. The best-performing model Gemini-1.5-Pro achieves merely an F-score of 54.4%; 2) Test-time compute paradigms show insignificant performance gains, revealing fundamental constraints for enhancing factuality through post-hoc computation; 3) Retrieval-Augmented Generation demonstrates consistent improvements at the cost of additional inference time overhead, presenting a critical efficiency-performance trade-off.",
            "score": 4,
            "issue_id": 2876,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 марта",
                "en": "March 24",
                "zh": "3月24日"
            },
            "hash": "599abe342d833dd0",
            "authors": [
                "Meng Cao",
                "Pengfei Hu",
                "Yingyao Wang",
                "Jihao Gu",
                "Haoran Tang",
                "Haoze Zhao",
                "Jiahua Dong",
                "Wangbo Yu",
                "Ge Zhang",
                "Ian Reid",
                "Xiaodan Liang"
            ],
            "affiliations": [
                "Alibaba Group",
                "M-A-P",
                "MBZUAI",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18923.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#interpretability",
                    "#reasoning",
                    "#long_context",
                    "#multimodal",
                    "#rag",
                    "#benchmark"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Новый стандарт оценки фактической точности видео-языковых моделей",
                    "desc": "Статья представляет Video SimpleQA - первый комплексный бенчмарк для оценки фактической точности Больших Видео-Языковых Моделей (LVLM). Бенчмарк отличается требованием интеграции внешних знаний, объективностью вопросов и верифицируемостью ответов. Оценка 41 современной LVLM показала значительные недостатки в фактической точности, при этом лучшая модель Gemini-1.5-Pro достигла F-меры всего 54.4%. Исследование выявило ограничения улучшения фактической точности через пост-обработку и компромисс между эффективностью и производительностью при использовании Retrieval-Augmented Generation."
                },
                "en": {
                    "title": "Evaluating Factual Accuracy in Video Language Models",
                    "desc": "This paper introduces Video SimpleQA, a new benchmark designed to evaluate the factual accuracy of Large Video Language Models (LVLMs). It focuses on assessing how well these models can integrate external knowledge and answer fact-based questions about video content. The benchmark emphasizes the need for definitive answers and includes rigorous validation against authoritative sources to ensure reliability. The evaluation of 41 LVLMs reveals significant shortcomings in factual adherence, particularly among open-source models, highlighting the challenges in improving factual accuracy in multi-modal contexts."
                },
                "zh": {
                    "title": "视频语言模型的事实性评估新基准",
                    "desc": "最近，大型视频语言模型（LVLMs）的进展显示了它们在多模态理解方面的潜力，但在视频上下文中评估其事实基础仍然是一个重要的未解决挑战。为了解决这个问题，我们引入了Video SimpleQA，这是第一个专门针对LVLMs事实性评估的综合基准。该基准的特点包括：需要整合外部知识、针对客观事件的问题、明确且简短的答案，以及经过外部来源验证的注释。我们对41个最先进的LVLMs进行了广泛评估，发现当前模型在事实遵循方面存在显著不足，尤其是开源模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18892",
            "title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for\n  Open Base Models in the Wild",
            "url": "https://huggingface.co/papers/2503.18892",
            "abstract": "DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can naturally emerge through a simple reinforcement learning (RL) framework with rule-based rewards, where the training may directly start from the base models-a paradigm referred to as zero RL training. Most recent efforts to reproduce zero RL training have primarily focused on the Qwen2.5 model series, which may not be representative as we find the base models already exhibit strong instruction-following and self-reflection abilities. In this work, we investigate zero RL training across 10 diverse base models, spanning different families and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B, Qwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several key design strategies-such as adjusting format reward and controlling query difficulty-we achieve substantial improvements in both reasoning accuracy and response length across most settings. However, by carefully monitoring the training dynamics, we observe that different base models exhibit distinct patterns during training. For instance, the increased response length does not always correlate with the emergence of certain cognitive behaviors such as verification (i.e., the \"aha moment\"). Notably, we observe the \"aha moment\" for the first time in small models not from the Qwen family. We share the key designs that enable successful zero RL training, along with our findings and practices. To facilitate further research, we open-source the code, models, and analysis tools.",
            "score": 3,
            "issue_id": 2876,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 марта",
                "en": "March 24",
                "zh": "3月24日"
            },
            "hash": "4c0c4ab2292562e4",
            "authors": [
                "Weihao Zeng",
                "Yuzhen Huang",
                "Qian Liu",
                "Wei Liu",
                "Keqing He",
                "Zejun Ma",
                "Junxian He"
            ],
            "affiliations": [
                "BUPT",
                "HKUST",
                "TikTok"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18892.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#rl",
                    "#training",
                    "#small_models"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Развитие рассуждений в языковых моделях через RL с нуля",
                    "desc": "Это исследование посвящено применению обучения с подкреплением (RL) без предварительной подготовки для развития способностей к рассуждениям у различных языковых моделей. Авторы провели эксперименты на 10 разных базовых моделях, включая LLama3, Mistral, DeepSeek-Math и Qwen2.5. Используя специальные стратегии, такие как настройка формата вознаграждения и контроль сложности запросов, удалось значительно улучшить точность рассуждений и длину ответов. Наблюдения показали, что разные модели демонстрируют различные паттерны в процессе обучения, причем увеличение длины ответа не всегда коррелирует с появлением определенных когнитивных способностей."
                },
                "en": {
                    "title": "Unlocking Reasoning with Zero RL Training",
                    "desc": "The paper discusses DeepSeek-R1, which demonstrates that long chain-of-thought reasoning can be developed using a simple reinforcement learning framework with rule-based rewards, starting directly from base models, a method called zero RL training. The authors explore zero RL training across ten different base models, revealing that many of these models already possess strong instruction-following and self-reflection capabilities. They implement design strategies to enhance reasoning accuracy and response length, while also noting that training dynamics vary significantly among models. Importantly, they identify the 'aha moment' in smaller models outside the Qwen family, providing insights and open-sourcing their findings to support further research."
                },
                "zh": {
                    "title": "零强化学习训练：推理与反思的新突破",
                    "desc": "DeepSeek-R1展示了通过简单的强化学习框架和基于规则的奖励，长链思维推理可以自然出现。这种训练方法被称为零强化学习训练，允许直接从基础模型开始。我们研究了10种不同的基础模型，发现它们在推理准确性和响应长度上都有显著提升。我们还观察到，不同模型在训练过程中表现出不同的模式，特别是小模型首次出现了“恍然大悟”的现象。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18769",
            "title": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and\n  Symbolic Reasoning",
            "url": "https://huggingface.co/papers/2503.18769",
            "abstract": "This paper presents AlphaSpace, a novel methodology designed to enhance the spatial reasoning capabilities of large language models (LLMs) for 3D Cartesian space navigation. AlphaSpace employs a semantics-based tokenization strategy, encoding height information through specialized semantic tokens, and integrates primarily symbolic synthetic reasoning data. This approach enables LLMs to accurately manipulate objects by positioning them at specific [x, y, z] coordinates. Experimental results demonstrate that AlphaSpace significantly outperforms existing models on manipulation subtasks, achieving a total accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5 Sonnet.",
            "score": 1,
            "issue_id": 2875,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 марта",
                "en": "March 24",
                "zh": "3月24日"
            },
            "hash": "e92ee9df78b66019",
            "authors": [
                "Alan Dao",
                "Dinh Bach Vu",
                "Bui Quang Huy"
            ],
            "affiliations": [
                "Menlo Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18769.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#synthetic",
                    "#reasoning",
                    "#3d"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "AlphaSpace: Прорыв в пространственном мышлении ИИ",
                    "desc": "AlphaSpace - это новая методология, разработанная для улучшения пространственного мышления больших языковых моделей (LLM) в навигации по 3D декартовому пространству. Она использует токенизацию на основе семантики, кодируя информацию о высоте через специальные семантические токены, и интегрирует преимущественно символические синтетические данные для рассуждений. Этот подход позволяет LLM точно манипулировать объектами, позиционируя их по конкретным координатам [x, y, z]. Экспериментальные результаты показывают, что AlphaSpace значительно превосходит существующие модели в подзадачах манипулирования, достигая общей точности 66,67%."
                },
                "en": {
                    "title": "Enhancing 3D Navigation in Language Models with AlphaSpace",
                    "desc": "This paper introduces AlphaSpace, a new method aimed at improving how large language models (LLMs) understand and navigate 3D spaces. It uses a unique tokenization method that incorporates height information through special semantic tokens, allowing for better spatial reasoning. By combining this with symbolic reasoning data, AlphaSpace enables LLMs to effectively manipulate objects in a 3D environment by placing them at precise coordinates. The results show that AlphaSpace achieves a notable accuracy of 66.67% in manipulation tasks, outperforming other models like GPT-4o and Claude 3.5 Sonnet."
                },
                "zh": {
                    "title": "AlphaSpace：提升语言模型的空间推理能力",
                    "desc": "本文介绍了一种新方法AlphaSpace，旨在提升大型语言模型（LLMs）在三维笛卡尔空间导航中的空间推理能力。AlphaSpace采用基于语义的标记化策略，通过专门的语义标记编码高度信息，并主要整合符号合成推理数据。该方法使得LLMs能够准确地通过特定的[x, y, z]坐标来操作物体。实验结果表明，AlphaSpace在操作子任务上显著优于现有模型，总准确率达到66.67%，而GPT-4o为37.5%，Claude 3.5 Sonnet为29.17%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.17735",
            "title": "RDTF: Resource-efficient Dual-mask Training Framework for Multi-frame\n  Animated Sticker Generation",
            "url": "https://huggingface.co/papers/2503.17735",
            "abstract": "Recently, great progress has been made in video generation technology, attracting the widespread attention of scholars. To apply this technology to downstream applications under resource-constrained conditions, researchers usually fine-tune the pre-trained models based on parameter-efficient tuning methods such as Adapter or Lora. Although these methods can transfer the knowledge from the source domain to the target domain, fewer training parameters lead to poor fitting ability, and the knowledge from the source domain may lead to the inference process deviating from the target domain. In this paper, we argue that under constrained resources, training a smaller video generation model from scratch using only million-level samples can outperform parameter-efficient tuning on larger models in downstream applications: the core lies in the effective utilization of data and curriculum strategy. Take animated sticker generation (ASG) as a case study, we first construct a discrete frame generation network for stickers with low frame rates, ensuring that its parameters meet the requirements of model training under constrained resources. In order to provide data support for models trained from scratch, we come up with a dual-mask based data utilization strategy, which manages to improve the availability and expand the diversity of limited data. To facilitate convergence under dual-mask situation, we propose a difficulty-adaptive curriculum learning method, which decomposes the sample entropy into static and adaptive components so as to obtain samples from easy to difficult. The experiment demonstrates that our resource-efficient dual-mask training framework is quantitatively and qualitatively superior to efficient-parameter tuning methods such as I2V-Adapter and SimDA, verifying the feasibility of our method on downstream tasks under constrained resources. Code will be available.",
            "score": 1,
            "issue_id": 2876,
            "pub_date": "2025-03-22",
            "pub_date_card": {
                "ru": "22 марта",
                "en": "March 22",
                "zh": "3月22日"
            },
            "hash": "186b92c438925eb6",
            "authors": [
                "Zhiqiang Yuan",
                "Ting Zhang",
                "Ying Deng",
                "Jiapei Zhang",
                "Yeshuang Zhu",
                "Zexi Jia",
                "Jie Zhou",
                "Jinchao Zhang"
            ],
            "affiliations": [
                "Pattern Recognition Center, WeChat AI, Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.17735.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#dataset",
                    "#optimization",
                    "#training",
                    "#transfer_learning"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Эффективная генерация видео с нуля вместо тонкой настройки больших моделей",
                    "desc": "Статья представляет новый подход к генерации видео в условиях ограниченных ресурсов. Авторы предлагают обучать с нуля небольшую модель на миллионах образцов вместо тонкой настройки больших предобученных моделей. Ключевые элементы подхода включают эффективное использование данных с помощью стратегии двойной маски и адаптивное обучение по учебной программе. Эксперименты на задаче генерации анимированных стикеров показывают превосходство предложенного метода над существующими подходами."
                },
                "en": {
                    "title": "Train Small, Win Big: Efficient Video Generation Under Constraints",
                    "desc": "This paper discusses advancements in video generation technology and its application in resource-limited environments. It challenges the effectiveness of parameter-efficient tuning methods like Adapter and Lora, suggesting that training smaller models from scratch can yield better results with limited data. The authors introduce a dual-mask data utilization strategy to enhance data diversity and a difficulty-adaptive curriculum learning method to improve model training. Their experiments show that this new approach outperforms existing tuning methods, demonstrating its potential for downstream applications."
                },
                "zh": {
                    "title": "资源受限下的视频生成新策略",
                    "desc": "最近，视频生成技术取得了显著进展，吸引了学者们的广泛关注。为了在资源受限的条件下应用这一技术，研究人员通常基于参数高效的调优方法对预训练模型进行微调。本文提出在资源受限的情况下，从头开始训练一个较小的视频生成模型，使用百万级样本，能够在下游应用中超越大型模型的参数高效调优。我们通过构建低帧率贴纸的离散帧生成网络和双掩码数据利用策略，结合难度自适应的课程学习方法，显著提高了模型的训练效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14428",
            "title": "MagicComp: Training-free Dual-Phase Refinement for Compositional Video\n  Generation",
            "url": "https://huggingface.co/papers/2503.14428",
            "abstract": "Text-to-video (T2V) generation has made significant strides with diffusion models. However, existing methods still struggle with accurately binding attributes, determining spatial relationships, and capturing complex action interactions between multiple subjects. To address these limitations, we propose MagicComp, a training-free method that enhances compositional T2V generation through dual-phase refinement. Specifically, (1) During the Conditioning Stage: We introduce the Semantic Anchor Disambiguation to reinforces subject-specific semantics and resolve inter-subject ambiguity by progressively injecting the directional vectors of semantic anchors into original text embedding; (2) During the Denoising Stage: We propose Dynamic Layout Fusion Attention, which integrates grounding priors and model-adaptive spatial perception to flexibly bind subjects to their spatiotemporal regions through masked attention modulation. Furthermore, MagicComp is a model-agnostic and versatile approach, which can be seamlessly integrated into existing T2V architectures. Extensive experiments on T2V-CompBench and VBench demonstrate that MagicComp outperforms state-of-the-art methods, highlighting its potential for applications such as complex prompt-based and trajectory-controllable video generation. Project page: https://hong-yu-zhang.github.io/MagicComp-Page/.",
            "score": 1,
            "issue_id": 2875,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 марта",
                "en": "March 18",
                "zh": "3月18日"
            },
            "hash": "1cd532518024f266",
            "authors": [
                "Hongyu Zhang",
                "Yufan Deng",
                "Shenghai Yuan",
                "Peng Jin",
                "Zesen Cheng",
                "Yian Zhao",
                "Chang Liu",
                "Jie Chen"
            ],
            "affiliations": [
                "Peng Cheng Laboratory, Shenzhen, China",
                "School of Electronic and Computer Engineering, Peking University, Shenzhen, China",
                "Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14428.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#architecture",
                    "#games",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "MagicComp: Усовершенствованная генерация видео по тексту без дополнительного обучения",
                    "desc": "MagicComp - это метод генерации видео по тексту, не требующий дополнительного обучения. Он использует двухфазовое уточнение для улучшения композиционной генерации: семантическое разрешение неоднозначности на этапе подготовки условий и динамическое слияние макетов на этапе шумоподавления. Метод решает проблемы связывания атрибутов, определения пространственных отношений и захвата сложных взаимодействий между несколькими объектами. MagicComp может быть интегрирован в существующие архитектуры генерации видео по тексту и превосходит современные методы в экспериментах."
                },
                "en": {
                    "title": "Enhancing Text-to-Video Generation with MagicComp",
                    "desc": "This paper presents MagicComp, a novel method for improving text-to-video (T2V) generation using diffusion models. It addresses challenges in accurately linking attributes and understanding spatial relationships between subjects in videos. The method consists of two main phases: the Conditioning Stage, which clarifies subject semantics using Semantic Anchor Disambiguation, and the Denoising Stage, which employs Dynamic Layout Fusion Attention to enhance spatial binding. MagicComp is designed to be adaptable and can be integrated into existing T2V systems, showing superior performance in various benchmarks."
                },
                "zh": {
                    "title": "MagicComp：提升文本到视频生成的创新方法",
                    "desc": "本文提出了一种名为MagicComp的文本到视频生成方法，旨在解决现有方法在属性绑定、空间关系确定和复杂动作交互方面的不足。该方法通过双阶段的精炼过程来增强组合式T2V生成，首先在条件阶段引入语义锚点消歧，以强化特定主题的语义并解决主题间的歧义。其次，在去噪阶段，提出动态布局融合注意力，通过掩蔽注意力调制灵活绑定主题与其时空区域。MagicComp是一种与模型无关的通用方法，可以无缝集成到现有的T2V架构中，并在多个基准测试中表现优异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18866",
            "title": "Reasoning to Learn from Latent Thoughts",
            "url": "https://huggingface.co/papers/2503.18866",
            "abstract": "Compute scaling for language model (LM) pretraining has outpaced the growth of human-written texts, leading to concerns that data will become the bottleneck to LM scaling. To continue scaling pretraining in this data-constrained regime, we propose that explicitly modeling and inferring the latent thoughts that underlie the text generation process can significantly improve pretraining data efficiency. Intuitively, our approach views web text as the compressed final outcome of a verbose human thought process and that the latent thoughts contain important contextual knowledge and reasoning steps that are critical to data-efficient learning. We empirically demonstrate the effectiveness of our approach through data-constrained continued pretraining for math. We first show that synthetic data approaches to inferring latent thoughts significantly improve data efficiency, outperforming training on the same amount of raw data (5.7\\% rightarrow 25.4\\% on MATH). Furthermore, we demonstrate latent thought inference without a strong teacher, where an LM bootstraps its own performance by using an EM algorithm to iteratively improve the capability of the trained LM and the quality of thought-augmented pretraining data. We show that a 1B LM can bootstrap its performance across at least three iterations and significantly outperform baselines trained on raw data, with increasing gains from additional inference compute when performing the E-step. The gains from inference scaling and EM iterations suggest new opportunities for scaling data-constrained pretraining.",
            "score": 0,
            "issue_id": 2876,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 марта",
                "en": "March 24",
                "zh": "3月24日"
            },
            "hash": "ab963a9dd28b0934",
            "authors": [
                "Yangjun Ruan",
                "Neil Band",
                "Chris J. Maddison",
                "Tatsunori Hashimoto"
            ],
            "affiliations": [
                "Stanford University",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18866.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#transfer_learning",
                    "#synthetic",
                    "#data",
                    "#math"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Раскрытие скрытых мыслей для эффективного обучения языковых моделей",
                    "desc": "Статья предлагает метод повышения эффективности предобучения языковых моделей в условиях ограниченных данных. Авторы предлагают моделировать и выводить скрытые мысли, лежащие в основе процесса генерации текста. Этот подход рассматривает веб-текст как сжатый результат подробного мыслительного процесса человека. Эмпирические результаты показывают значительное улучшение эффективности обучения, особенно в области математики."
                },
                "en": {
                    "title": "Unlocking Data Efficiency through Latent Thought Inference",
                    "desc": "This paper addresses the challenge of limited human-written text data for training large language models (LMs). It proposes a method to model and infer the underlying thoughts that lead to text generation, which can enhance data efficiency during pretraining. By treating web text as a condensed version of human thought processes, the authors show that inferring these latent thoughts can lead to better learning outcomes, especially in data-scarce situations. Their experiments demonstrate that this approach not only improves performance on tasks like math but also allows LMs to iteratively enhance their own capabilities without relying heavily on external data."
                },
                "zh": {
                    "title": "潜在思维推断提升语言模型预训练效率",
                    "desc": "这篇论文探讨了在语言模型预训练中，数据增长速度慢于模型规模扩展的问题。作者提出通过显式建模和推断文本生成过程中的潜在思维，可以显著提高预训练的数据效率。研究表明，合成数据方法在推断潜在思维方面的应用，能够在数据受限的情况下，提升模型的学习效果。通过迭代的EM算法，模型能够自我提升性能，并在多个迭代中显著超越仅使用原始数据训练的基线模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18013",
            "title": "Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models\n  via Vision-Guided Reinforcement Learning",
            "url": "https://huggingface.co/papers/2503.18013",
            "abstract": "Large Vision-Language Models (LVLMs) typically follow a two-stage training paradigm-pretraining and supervised fine-tuning. Recently, preference optimization, derived from the language domain, has emerged as an effective post-training reinforcement strategy to enhance capabilities of LVLMs. However, constructing high-quality human-annotated preference data and developing robust reward models to mimic these preferences are both costly and challenging. Motivated by this observation, we propose Vision-R1, a novel vision-guided R1-like reinforcement learning algorithm for LVLMs that rewards models with definitive vision feedback. It only leverages curated instruction data, eliminating the need for specialized reward models and handcrafted preference datasets. We incorporate a criterion-driven reward function that further integrates multi-dimensional feedback to evaluate model completions comprehensively based on the vision task logic. Furthermore, we introduce a progressive rule refinement strategy that dynamically adjusts the reward criteria during training, enabling continuous model improvement and mitigating reward hacking. Extensive experiments on both in-distribution and out-of-distribution benchmarks demonstrate that fine-tuning the 7B LVLMs with Vision-R1 achieves consistent performance gains, with even up to 50% improvement and surpassing the state-of-the-art 10x size model.",
            "score": 0,
            "issue_id": 2876,
            "pub_date": "2025-03-23",
            "pub_date_card": {
                "ru": "23 марта",
                "en": "March 23",
                "zh": "3月23日"
            },
            "hash": "45029d297f1b8ac9",
            "authors": [
                "Yufei Zhan",
                "Yousong Zhu",
                "Shurong Zheng",
                "Hongyin Zhao",
                "Fan Yang",
                "Ming Tang",
                "Jinqiao Wang"
            ],
            "affiliations": [
                "Foundation Model Research Center, Institute of Automation, Chinese Academy of Sciences, Beijing, China",
                "Peng Cheng Laboratory, Shenzhen, China",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China",
                "Wuhan AI Research, Wuhan, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18013.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#training",
                    "#benchmark",
                    "#rlhf"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "Vision-R1: Революция в обучении визуально-языковых моделей без ручной разметки",
                    "desc": "В статье представлен новый алгоритм обучения с подкреплением для крупных визуально-языковых моделей под названием Vision-R1. Этот метод использует обратную связь на основе зрения для улучшения моделей, не требуя специальных наборов данных о предпочтениях или моделей вознаграждения. Vision-R1 включает функцию вознаграждения на основе критериев и стратегию прогрессивного уточнения правил. Эксперименты показывают значительное улучшение производительности моделей, обученных с помощью Vision-R1, в некоторых случаях превосходящее модели в 10 раз большего размера."
                },
                "en": {
                    "title": "Reinforcing Vision with Vision-R1: Simplifying LVLM Training",
                    "desc": "This paper introduces Vision-R1, a new reinforcement learning algorithm designed for Large Vision-Language Models (LVLMs). Unlike traditional methods that require expensive human-annotated preference data, Vision-R1 uses curated instruction data to provide vision feedback directly to the models. The algorithm employs a criterion-driven reward function that assesses model outputs based on the logic of vision tasks, allowing for a more comprehensive evaluation. Additionally, it features a progressive rule refinement strategy that adapts reward criteria during training, leading to significant performance improvements in LVLMs without the need for complex reward models."
                },
                "zh": {
                    "title": "视觉引导的强化学习提升模型能力",
                    "desc": "大型视觉语言模型（LVLMs）通常采用两阶段训练方法：预训练和监督微调。最近，源自语言领域的偏好优化成为一种有效的后训练强化策略，用于提升LVLMs的能力。我们提出了一种新颖的视觉引导R1类强化学习算法Vision-R1，它通过明确的视觉反馈来奖励模型，避免了构建高质量人类标注的偏好数据和开发复杂的奖励模型的高成本。通过引入多维反馈的标准驱动奖励函数，Vision-R1能够全面评估模型的完成情况，并在训练过程中动态调整奖励标准，从而实现持续的模型改进。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.17422",
            "title": "V-Seek: Accelerating LLM Reasoning on Open-hardware Server-class RISC-V\n  Platforms",
            "url": "https://huggingface.co/papers/2503.17422",
            "abstract": "The recent exponential growth of Large Language Models (LLMs) has relied on GPU-based systems. However, CPUs are emerging as a flexible and lower-cost alternative, especially when targeting inference and reasoning workloads. RISC-V is rapidly gaining traction in this area, given its open and vendor-neutral ISA. However, the RISC-V hardware for LLM workloads and the corresponding software ecosystem are not fully mature and streamlined, given the requirement of domain-specific tuning. This paper aims at filling this gap, focusing on optimizing LLM inference on the Sophon SG2042, the first commercially available many-core RISC-V CPU with vector processing capabilities.   On two recent state-of-the-art LLMs optimized for reasoning, DeepSeek R1 Distill Llama 8B and DeepSeek R1 Distill QWEN 14B, we achieve 4.32/2.29 token/s for token generation and 6.54/3.68 token/s for prompt processing, with a speed up of up 2.9x/3.0x compared to our baseline.",
            "score": 0,
            "issue_id": 2876,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 марта",
                "en": "March 21",
                "zh": "3月21日"
            },
            "hash": "3811c1f2a2e12813",
            "authors": [
                "Javier J. Poveda Rodrigo",
                "Mohamed Amine Ahmdi",
                "Alessio Burrello",
                "Daniele Jahier Pagliari",
                "Luca Benini"
            ],
            "affiliations": [
                "DAUIN, Politecnico of Turin, Turin, Italy",
                "ETHZ, Zurich, Switzerland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.17422.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#reasoning",
                    "#inference"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение языковых моделей на RISC-V процессорах",
                    "desc": "Данная статья посвящена оптимизации инференса больших языковых моделей (LLM) на процессорах RISC-V, в частности на Sophon SG2042. Авторы исследуют возможности использования CPU как альтернативы GPU для задач обработки естественного языка. В работе представлены результаты оптимизации двух современных LLM моделей - DeepSeek R1 Distill Llama 8B и DeepSeek R1 Distill QWEN 14B. Достигнуто значительное ускорение инференса по сравнению с базовой реализацией, до 2.9-3.0 раз."
                },
                "en": {
                    "title": "Unlocking LLM Potential with RISC-V CPUs",
                    "desc": "This paper discusses the potential of using CPUs, specifically RISC-V architecture, for optimizing Large Language Model (LLM) inference. It highlights the advantages of RISC-V, such as its flexibility and cost-effectiveness, especially for reasoning tasks. The authors focus on the Sophon SG2042, a many-core RISC-V CPU, and demonstrate significant performance improvements in token generation and prompt processing for two advanced LLMs. The results show up to 3x speed improvements compared to traditional systems, indicating a promising direction for LLM deployment on CPU architectures."
                },
                "zh": {
                    "title": "用RISC-V优化大型语言模型推理",
                    "desc": "近年来，大型语言模型（LLMs）的快速发展依赖于基于GPU的系统。然而，CPU作为一种灵活且成本更低的替代方案，正在逐渐崭露头角，特别是在推理和推断工作负载方面。RISC-V因其开放和中立的指令集架构（ISA）而在这一领域迅速获得关注。本文旨在优化在Sophon SG2042上进行LLM推理，展示了在两个最新的优化推理模型上实现的显著性能提升。"
                }
            }
        }
    ],
    "link_prev": "2025-03-24.html",
    "link_next": "2025-03-26.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "24.03",
        "en": "03/24",
        "zh": "3月24日"
    },
    "short_date_next": {
        "ru": "26.03",
        "en": "03/26",
        "zh": "3月26日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 2,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 3,
        "#3d": 1,
        "#audio": 0,
        "#video": 6,
        "#multimodal": 4,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 0,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 1,
        "#reasoning": 5,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了多模态科学问题（MSPs）及其挑战。MSPs需要整合多种模态，如文本和图表。目前，MSPs面临两大问题：多模态推理和缺乏反思能力。作者提出了基于大七人格和苏格拉底指导的多代理框架（MAPS）来解决这些问题。实验结果显示，该框架在多个数据集上表现出色，超越了现有的最佳模型。",
        "title": "MAPS: A Multi-Agent Framework Based on Big Seven Personality and\n  Socratic Guidance for Multimodal Scientific Problem Solving",
        "pinyin": "这篇文章讨论了多模态科学问题（MSPs）及其挑战。\nZhè piān wénzhāng tǎolùn le duō móshuài kēxué wèntí (MSPs) jí qí tiǎozhàn.\n\nMSPs需要整合多种模态，如文本和图表。\nMSPs xūyào zhěnghé duō zhǒng móshuài, rú wénběn hé túbiǎo.\n\n目前，MSPs面临两大问题：多模态推理和缺乏反思能力。\nMùqián, MSPs miànlín liǎng dà wèntí: duō móshuài tuīlǐ hé quēfá fǎnsī nénglì.\n\n作者提出了基于大七人格和苏格拉底指导的多代理框架（MAPS）来解决这些问题。\nZuòzhě tíchū le jīyú dà qī réngé hé Sūgélādǐ zhǐdǎo de duō dàilǐ kuàngjià (MAPS) lái jiějué zhèxiē wèntí.\n\n实验结果显示，该框架在多个数据集上表现出色，超越了现有的最佳模型。\nShíyàn jiéguǒ xiǎnshì, gǎi kuàngjià zài duō gè shùjùjí shàng biǎoxiàn chūsè, chāoyuè le xiànyǒu de zuìjiā móxíng.",
        "vocab": "[\n    {\"word\": \"多模态\", \"pinyin\": \"duō mó tài\", \"trans\": \"multimodal\"},\n    {\"word\": \"科学问题\", \"pinyin\": \"kē xué wèn tí\", \"trans\": \"scientific problems\"},\n    {\"word\": \"挑战\", \"pinyin\": \"tiǎo zhàn\", \"trans\": \"challenges\"},\n    {\"word\": \"整合\", \"pinyin\": \"zhěng hé\", \"trans\": \"integrate\"},\n    {\"word\": \"模态\", \"pinyin\": \"mó tài\", \"trans\": \"modality\"},\n    {\"word\": \"文本\", \"pinyin\": \"wén běn\", \"trans\": \"text\"},\n    {\"word\": \"图表\", \"pinyin\": \"tú biǎo\", \"trans\": \"charts\"},\n    {\"word\": \"面临\", \"pinyin\": \"miàn lín\", \"trans\": \"face\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"反思\", \"pinyin\": \"fǎn sī\", \"trans\": \"reflection\"},\n    {\"word\": \"能力\", \"pinyin\": \"néng lì\", \"trans\": \"ability\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"基于\", \"pinyin\": \"jī yú\", \"trans\": \"based on\"},\n    {\"word\": \"大七人格\", \"pinyin\": \"dà qī rén gé\", \"trans\": \"Big Five personality traits\"},\n    {\"word\": \"苏格拉底\", \"pinyin\": \"sū gé lā dǐ\", \"trans\": \"Socrates\"},\n    {\"word\": \"指导\", \"pinyin\": \"zhǐ dǎo\", \"trans\": \"guidance\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàng jià\", \"trans\": \"framework\"},\n    {\"word\": \"实验\", \"pinyin\": \"shí yàn\", \"trans\": \"experiment\"},\n    {\"word\": \"结果\", \"pinyin\": \"jié guǒ\", \"trans\": \"results\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chū sè\", \"trans\": \"outstanding\"},\n    {\"word\": \"超越\", \"pinyin\": \"chāo yuè\", \"trans\": \"surpass\"},\n    {\"word\": \"现有\", \"pinyin\": \"xiàn yǒu\", \"trans\": \"existing\"},\n    {\"word\": \"最佳\", \"pinyin\": \"zuì jiā\", \"trans\": \"best\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"}\n]",
        "trans": "This article discusses multimodal scientific problems (MSPs) and their challenges. MSPs require the integration of multiple modalities, such as text and graphs. Currently, MSPs face two major issues: multimodal reasoning and a lack of reflective capabilities. The authors propose a multi-agent framework (MAPS) based on the Big Five personality traits and Socratic guidance to address these issues. Experimental results show that this framework performs excellently on multiple datasets, surpassing existing state-of-the-art models.",
        "update_ts": "2025-03-24 09:12"
    }
}