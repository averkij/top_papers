{
    "date": {
        "ru": "8 апреля",
        "en": "April 8",
        "zh": "4月8日"
    },
    "time_utc": "2025-04-08 03:27",
    "weekday": 1,
    "issue_id": 3115,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.05305",
            "title": "URECA: Unique Region Caption Anything",
            "url": "https://huggingface.co/papers/2504.05305",
            "abstract": "Region-level captioning aims to generate natural language descriptions for specific image regions while highlighting their distinguishing features. However, existing methods struggle to produce unique captions across multi-granularity, limiting their real-world applicability. To address the need for detailed region-level understanding, we introduce URECA dataset, a large-scale dataset tailored for multi-granularity region captioning. Unlike prior datasets that focus primarily on salient objects, URECA dataset ensures a unique and consistent mapping between regions and captions by incorporating a diverse set of objects, parts, and background elements. Central to this is a stage-wise data curation pipeline, where each stage incrementally refines region selection and caption generation. By leveraging Multimodal Large Language Models (MLLMs) at each stage, our pipeline produces distinctive and contextually grounded captions with improved accuracy and semantic diversity. Building upon this dataset, we present URECA, a novel captioning model designed to effectively encode multi-granularity regions. URECA maintains essential spatial properties such as position and shape through simple yet impactful modifications to existing MLLMs, enabling fine-grained and semantically rich region descriptions. Our approach introduces dynamic mask modeling and a high-resolution mask encoder to enhance caption uniqueness. Experiments show that URECA achieves state-of-the-art performance on URECA dataset and generalizes well to existing region-level captioning benchmarks.",
            "score": 0,
            "issue_id": 3115,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "6eec948e6319fc99",
            "authors": [
                "Sangbeom Lim",
                "Junwan Kim",
                "Heeji Yoon",
                "Jaewoo Jung",
                "Seungryong Kim"
            ],
            "affiliations": [
                "KAIST AI",
                "Korea University",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05305.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#games",
                    "#cv",
                    "#interpretability",
                    "#dataset",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Точное описание регионов изображений с помощью многоуровневого подхода",
                    "desc": "Статья представляет новый набор данных URECA для многоуровневого описания регионов изображений. Авторы разработали поэтапный процесс создания данных с использованием мультимодальных больших языковых моделей для генерации уникальных и контекстуально обоснованных описаний. На основе этого набора данных предложена модель URECA, которая эффективно кодирует регионы разной детализации, сохраняя их пространственные свойства. Эксперименты показывают, что URECA достигает наилучших результатов на созданном наборе данных и хорошо обобщается на существующие эталонные тесты."
                },
                "en": {
                    "title": "Enhancing Region-Level Captioning with URECA Dataset and Model",
                    "desc": "This paper presents a new approach to region-level captioning, which generates detailed descriptions for specific parts of images. The authors introduce the URECA dataset, designed to improve the uniqueness of captions by including a variety of objects and backgrounds. They propose a novel captioning model, URECA, that uses advanced techniques like dynamic mask modeling to maintain spatial properties and enhance the quality of generated captions. The results demonstrate that URECA outperforms existing methods, providing more accurate and diverse descriptions across different image regions."
                },
                "zh": {
                    "title": "多粒度区域描述的新突破",
                    "desc": "区域级描述旨在为特定图像区域生成自然语言描述，并突出其独特特征。然而，现有方法在多粒度生成独特描述方面存在困难，限制了其在实际应用中的有效性。为了解决这一问题，我们引入了URECA数据集，这是一个针对多粒度区域描述的大规模数据集，确保区域与描述之间的独特和一致的映射。基于此数据集，我们提出了URECA模型，能够有效编码多粒度区域，生成细致且语义丰富的描述。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05304",
            "title": "Gaussian Mixture Flow Matching Models",
            "url": "https://huggingface.co/papers/2504.05304",
            "abstract": "Diffusion models approximate the denoising distribution as a Gaussian and predict its mean, whereas flow matching models reparameterize the Gaussian mean as flow velocity. However, they underperform in few-step sampling due to discretization error and tend to produce over-saturated colors under classifier-free guidance (CFG). To address these limitations, we propose a novel Gaussian mixture flow matching (GMFlow) model: instead of predicting the mean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a multi-modal flow velocity distribution, which can be learned with a KL divergence loss. We demonstrate that GMFlow generalizes previous diffusion and flow matching models where a single Gaussian is learned with an L_2 denoising loss. For inference, we derive GM-SDE/ODE solvers that leverage analytic denoising distributions and velocity fields for precise few-step sampling. Furthermore, we introduce a novel probabilistic guidance scheme that mitigates the over-saturation issues of CFG and improves image generation quality. Extensive experiments demonstrate that GMFlow consistently outperforms flow matching baselines in generation quality, achieving a Precision of 0.942 with only 6 sampling steps on ImageNet 256times256.",
            "score": 0,
            "issue_id": 3115,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "b0223808c61a3545",
            "authors": [
                "Hansheng Chen",
                "Kai Zhang",
                "Hao Tan",
                "Zexiang Xu",
                "Fujun Luan",
                "Leonidas Guibas",
                "Gordon Wetzstein",
                "Sai Bi"
            ],
            "affiliations": [
                "Adobe Research, CA 95110, USA",
                "Hillbot",
                "Stanford University, CA 94305, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05304.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#diffusion",
                    "#inference"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "GMFlow: мощная генерация изображений с гауссовыми смесями",
                    "desc": "Статья представляет новую модель Gaussian mixture flow matching (GMFlow) для генерации изображений. GMFlow предсказывает параметры динамической гауссовой смеси для захвата мультимодального распределения скорости потока, что можно обучить с помощью потери KL-дивергенции. Авторы разработали специальные решатели GM-SDE/ODE для точного сэмплирования за небольшое число шагов. Также предложена новая схема вероятностного управления, улучшающая качество генерации изображений и решающая проблему пересыщенности цветов при классификационно-свободном управлении."
                },
                "en": {
                    "title": "GMFlow: Enhancing Image Generation with Dynamic Gaussian Mixtures",
                    "desc": "This paper introduces a new model called Gaussian Mixture Flow Matching (GMFlow) that improves upon traditional diffusion models and flow matching models. Instead of just predicting a single Gaussian mean, GMFlow predicts parameters for a dynamic Gaussian mixture, allowing it to better capture complex distributions in the data. The model addresses issues like discretization error and color saturation in generated images by using a novel probabilistic guidance scheme. Experimental results show that GMFlow achieves higher image generation quality with fewer sampling steps compared to existing methods."
                },
                "zh": {
                    "title": "高斯混合流匹配：提升图像生成质量的新方法",
                    "desc": "扩散模型通过高斯分布来近似去噪分布并预测其均值，而流匹配模型则将高斯均值重新参数化为流速。然而，它们在少步采样时表现不佳，主要是由于离散化误差，并且在无分类器引导下容易产生过饱和的颜色。为了解决这些问题，我们提出了一种新颖的高斯混合流匹配（GMFlow）模型：GMFlow预测动态高斯混合参数，以捕捉多模态流速分布，并通过KL散度损失进行学习。实验表明，GMFlow在生成质量上始终优于流匹配基线，在ImageNet 256x256上仅用6个采样步骤就达到了0.942的精度。"
                }
            }
        }
    ],
    "link_prev": "2025-04-07.html",
    "link_next": "2025-04-09.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "07.04",
        "en": "04/07",
        "zh": "4月7日"
    },
    "short_date_next": {
        "ru": "09.04",
        "en": "04/09",
        "zh": "4月9日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一个新的多语言问题解决基准，称为Multi-SWE-bench。它涵盖了Java、TypeScript、JavaScript、Go、Rust、C和C++等语言。该基准包含1,632个高质量实例，由68位专家注释。基于这个基准，作者评估了一系列先进的模型，并提供了详细的分析。此外，文章还宣布成立了一个开源社区Multi-SWE-RL，旨在为问题解决任务构建大规模强化学习训练数据集。",
        "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving",
        "pinyin": "这篇文章介绍了一个新的多语言问题解决基准，称为Multi-SWE-bench。它涵盖了Java、TypeScript、JavaScript、Go、Rust、C和C++等语言。该基准包含1,632个高质量实例，由68位专家注释。基于这个基准，作者评估了一系列先进的模型，并提供了详细的分析。此外，文章还宣布成立了一个开源社区Multi-SWE-RL，旨在为问题解决任务构建大规模强化学习训练数据集。\n\nzhè piān wén zhāng jiè shào le yī gè xīn de duō yǔ yán wèn tí jiě jué jī zhǔn, chēng wéi Multi-SWE-bench. tā hán gǎi le Java, TypeScript, JavaScript, Go, Rust, C hé C++ děng yǔ yán. gǎi jī zhǔn bāo hán 1,632 gè gāo zhì liàng shí lì, yǒu 68 wèi zhuān jiā zhù shì. jī yú zhè gè jī zhǔn, zuò zhě píng gū le yī xì liè xiān jìn de mó xíng, bìng tí gōng le xiáng xì de fēn xī. cǐ wài, wén zhāng hái xuān bù chéng lì le yī gè kāi yuán shè qū Multi-SWE-RL, zhǐ yú wèi wèn tí jiě jué rèn wù gòu jiàn dà guī mó qiáng huà xué xùn liàn shù jù jí.",
        "vocab": "[\n    {\"word\": \"涵盖\", \"pinyin\": \"hángài\", \"trans\": \"cover\"},\n    {\"word\": \"基准\", \"pinyin\": \"jīzhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"注释\", \"pinyin\": \"zhùshì\", \"trans\": \"annotate\"},\n    {\"word\": \"评估\", \"pinyin\": \"pínggū\", \"trans\": \"evaluate\"},\n    {\"word\": \"先进\", \"pinyin\": \"xiānjìn\", \"trans\": \"advanced\"},\n    {\"word\": \"详细\", \"pinyin\": \"xiángxì\", \"trans\": \"detailed\"},\n    {\"word\": \"分析\", \"pinyin\": \"fēnxī\", \"trans\": \"analysis\"},\n    {\"word\": \"宣布\", \"pinyin\": \"xuānbù\", \"trans\": \"announce\"},\n    {\"word\": \"成立\", \"pinyin\": \"chénglì\", \"trans\": \"establish\"},\n    {\"word\": \"开源\", \"pinyin\": \"kāiyuán\", \"trans\": \"open-source\"},\n    {\"word\": \"社区\", \"pinyin\": \"shèqū\", \"trans\": \"community\"},\n    {\"word\": \"旨在\", \"pinyin\": \"zhǐzài\", \"trans\": \"aim to\"},\n    {\"word\": \"构建\", \"pinyin\": \"gòujiàn\", \"trans\": \"build\"},\n    {\"word\": \"任务\", \"pinyin\": \"rènwù\", \"trans\": \"task\"},\n    {\"word\": \"强化学习\", \"pinyin\": \"qiáng huà xuéxí\", \"trans\": \"reinforcement learning\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùnliàn\", \"trans\": \"training\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shùjùjí\", \"trans\": \"dataset\"}\n]",
        "trans": "This article introduces a new multilingual problem-solving benchmark called Multi-SWE-bench. It covers languages such as Java, TypeScript, JavaScript, Go, Rust, C, and C++. The benchmark contains 1,632 high-quality instances annotated by 68 experts. Based on this benchmark, the authors evaluated a series of advanced models and provided detailed analyses. Additionally, the article announces the establishment of an open-source community, Multi-SWE-RL, aimed at building large-scale reinforcement learning training datasets for problem-solving tasks.",
        "update_ts": "2025-04-07 09:12"
    }
}