{
    "date": {
        "ru": "2 мая",
        "en": "May 2",
        "zh": "5月2日"
    },
    "time_utc": "2025-05-02 05:12",
    "weekday": 4,
    "issue_id": 3551,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.00662",
            "title": "DeepCritic: Deliberate Critique with Large Language Models",
            "url": "https://huggingface.co/papers/2505.00662",
            "abstract": "As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a promising solution. In this work, we focus on studying and enhancing the math critique ability of LLMs. Current LLM critics provide critiques that are too shallow and superficial on each step, leading to low judgment accuracy and struggling to offer sufficient feedback for the LLM generator to correct mistakes. To tackle this issue, we propose a novel and effective two-stage framework to develop LLM critics that are capable of deliberately critiquing on each reasoning step of math solutions. In the first stage, we utilize Qwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for supervised fine-tuning. Each seed critique consists of deliberate step-wise critiques that includes multi-perspective verifications as well as in-depth critiques of initial critiques for each reasoning step. Then, we perform reinforcement learning on the fine-tuned model with either existing human-labeled data from PRM800K or our automatically annotated data obtained via Monte Carlo sampling-based correctness estimation, to further incentivize its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct not only significantly outperforms existing LLM critics (including the same-sized DeepSeek-R1-distill models and GPT-4o) on various error identification benchmarks, but also more effectively helps the LLM generator refine erroneous steps through more detailed feedback.",
            "score": 17,
            "issue_id": 3549,
            "pub_date": "2025-05-01",
            "pub_date_card": {
                "ru": "1 мая",
                "en": "May 1",
                "zh": "5月1日"
            },
            "hash": "259dddc97137d27a",
            "authors": [
                "Wenkai Yang",
                "Jingwen Chen",
                "Yankai Lin",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "School of Computer Science and Technology, Beijing Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.00662.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#reasoning",
                    "#rlhf",
                    "#math",
                    "#training"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Усовершенствование LLM для глубокой критики математических решений",
                    "desc": "Эта статья представляет новый двухэтапный подход к улучшению способностей больших языковых моделей (LLM) критиковать математические решения. На первом этапе используется Qwen2.5-72B-Instruct для генерации подробных пошаговых критик, которые служат обучающими данными. Затем применяется обучение с подкреплением для дальнейшего улучшения критических способностей модели. Разработанная модель-критик на базе Qwen2.5-7B-Instruct превосходит существующие LLM-критики в выявлении ошибок и предоставлении детальной обратной связи."
                },
                "en": {
                    "title": "Enhancing LLMs: From Shallow Critiques to Deep Insights in Math Solutions",
                    "desc": "This paper addresses the challenge of providing effective feedback for Large Language Models (LLMs) by enhancing their ability to critique mathematical solutions. The authors propose a two-stage framework where the first stage involves generating detailed critiques using a specific LLM, which serves as seed data for further training. In the second stage, reinforcement learning is applied to improve the critique model's performance using both human-labeled and automatically annotated data. The resulting critique model demonstrates superior performance in identifying errors and providing constructive feedback compared to existing LLM critics."
                },
                "zh": {
                    "title": "提升大型语言模型的数学批评能力",
                    "desc": "随着大型语言模型（LLMs）的快速发展，提供准确的反馈和可扩展的监督变得尤为重要。本文提出了一种新颖的两阶段框架，旨在增强LLMs在数学批评方面的能力。通过生成详细的逐步批评，模型能够更准确地识别错误并提供深入的反馈，帮助生成模型纠正错误。我们的实验表明，改进后的批评模型在错误识别基准测试中显著优于现有的LLM批评者。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.21853",
            "title": "A Survey of Interactive Generative Video",
            "url": "https://huggingface.co/papers/2504.21853",
            "abstract": "Interactive Generative Video (IGV) has emerged as a crucial technology in response to the growing demand for high-quality, interactive video content across various domains. In this paper, we define IGV as a technology that combines generative capabilities to produce diverse high-quality video content with interactive features that enable user engagement through control signals and responsive feedback. We survey the current landscape of IGV applications, focusing on three major domains: 1) gaming, where IGV enables infinite exploration in virtual worlds; 2) embodied AI, where IGV serves as a physics-aware environment synthesizer for training agents in multimodal interaction with dynamically evolving scenes; and 3) autonomous driving, where IGV provides closed-loop simulation capabilities for safety-critical testing and validation. To guide future development, we propose a comprehensive framework that decomposes an ideal IGV system into five essential modules: Generation, Control, Memory, Dynamics, and Intelligence. Furthermore, we systematically analyze the technical challenges and future directions in realizing each component for an ideal IGV system, such as achieving real-time generation, enabling open-domain control, maintaining long-term coherence, simulating accurate physics, and integrating causal reasoning. We believe that this systematic analysis will facilitate future research and development in the field of IGV, ultimately advancing the technology toward more sophisticated and practical applications.",
            "score": 13,
            "issue_id": 3550,
            "pub_date": "2025-04-30",
            "pub_date_card": {
                "ru": "30 апреля",
                "en": "April 30",
                "zh": "4月30日"
            },
            "hash": "4e975f915f638955",
            "authors": [
                "Jiwen Yu",
                "Yiran Qin",
                "Haoxuan Che",
                "Quande Liu",
                "Xintao Wang",
                "Pengfei Wan",
                "Di Zhang",
                "Kun Gai",
                "Hao Chen",
                "Xihui Liu"
            ],
            "affiliations": [
                "Kuaishou Technology, Shenzhen, China",
                "The Hong Kong University of Science and Technology (HKUST), Hong Kong",
                "The University of Hong Kong, Pok Fu Lam, Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.21853.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#interpretability",
                    "#video",
                    "#optimization",
                    "#survey",
                    "#agents",
                    "#games",
                    "#multimodal"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "IGV: Будущее интерактивного видеоконтента",
                    "desc": "Статья представляет обзор технологии Интерактивного Генеративного Видео (IGV), которая сочетает генеративные возможности с интерактивными функциями. Авторы рассматривают применение IGV в трех основных областях: игры, воплощенный ИИ и автономное вождение. Предложена комплексная структура, разбивающая идеальную систему IGV на пять модулей: Генерация, Контроль, Память, Динамика и Интеллект. В работе анализируются технические проблемы и будущие направления развития каждого компонента IGV."
                },
                "en": {
                    "title": "Empowering Interactive Experiences with Generative Video",
                    "desc": "Interactive Generative Video (IGV) is a new technology that creates high-quality videos that users can interact with. It combines generative models to produce diverse video content and allows users to engage with it through control signals. The paper explores IGV applications in gaming, embodied AI, and autonomous driving, highlighting its potential for creating immersive experiences and training environments. A framework is proposed to break down IGV systems into five key modules, addressing challenges like real-time generation and accurate physics simulation to guide future advancements in the field."
                },
                "zh": {
                    "title": "推动互动生成视频技术的未来发展",
                    "desc": "互动生成视频（IGV）是一种新兴技术，旨在满足对高质量互动视频内容的需求。本文将IGV定义为一种结合生成能力和互动特性的技术，能够通过控制信号和反馈实现用户参与。我们调查了IGV在游戏、具身人工智能和自动驾驶等三个主要领域的应用，并提出了一个全面的框架，将理想的IGV系统分解为生成、控制、记忆、动态和智能五个模块。通过系统分析技术挑战和未来方向，本文旨在推动IGV领域的研究与发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.00703",
            "title": "T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level\n  and Token-level CoT",
            "url": "https://huggingface.co/papers/2505.00703",
            "abstract": "Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: https://github.com/CaraJ7/T2I-R1",
            "score": 8,
            "issue_id": 3548,
            "pub_date": "2025-05-01",
            "pub_date_card": {
                "ru": "1 мая",
                "en": "May 1",
                "zh": "5月1日"
            },
            "hash": "ca564761ff71d15e",
            "authors": [
                "Dongzhi Jiang",
                "Ziyu Guo",
                "Renrui Zhang",
                "Zhuofan Zong",
                "Hao Li",
                "Le Zhuo",
                "Shilin Yan",
                "Pheng-Ann Heng",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CUHK MMLab",
                "CUHK MiuLar Lab",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.00703.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#optimization",
                    "#rl",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Рассуждения на двух уровнях улучшают генерацию изображений по тексту",
                    "desc": "Данная статья представляет T2I-R1 - новую модель генерации изображений по тексту, улучшенную с помощью рассуждений и обучения с подкреплением. Модель использует двухуровневый процесс рассуждений: семантический уровень для планирования промпта и токенный уровень для попиксельной генерации. Авторы вводят метод BiCoT-GRPO для оптимизации обоих уровней рассуждений. Применение этих стратегий к базовой модели Janus-Pro позволило значительно улучшить результаты на бенчмарках T2I-CompBench и WISE."
                },
                "en": {
                    "title": "Revolutionizing Text-to-Image Generation with Enhanced Reasoning",
                    "desc": "This paper introduces T2I-R1, a new model for generating images from text that uses advanced reasoning techniques. It employs a bi-level chain-of-thought (CoT) approach, which includes semantic-level reasoning for planning and token-level reasoning for detailed image generation. The model is enhanced by reinforcement learning (RL) and a novel reward system called BiCoT-GRPO, which optimizes both levels of reasoning simultaneously. As a result, T2I-R1 outperforms existing models, achieving significant improvements on benchmark tests."
                },
                "zh": {
                    "title": "双层思维链提升文本到图像生成",
                    "desc": "本文介绍了一种新颖的文本到图像生成模型T2I-R1，该模型结合了强化学习和双层思维链推理过程。我们提出了两种思维链：语义层次的思维链用于高层次的提示规划，令生成过程更具逻辑性；而令牌层次的思维链则用于在逐块生成过程中进行低层次的像素处理。通过引入BiCoT-GRPO，我们能够在同一训练步骤中优化这两种思维链，从而提升生成效果。实验结果表明，T2I-R1在多个基准测试中表现优异，超越了现有的最先进模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.21659",
            "title": "AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning\n  Optimization",
            "url": "https://huggingface.co/papers/2504.21659",
            "abstract": "Recently, long-thought reasoning models achieve strong performance on complex reasoning tasks, but often incur substantial inference overhead, making efficiency a critical concern. Our empirical analysis reveals that the benefit of using Long-CoT varies across problems: while some problems require elaborate reasoning, others show no improvement, or even degraded accuracy. This motivates adaptive reasoning strategies that tailor reasoning depth to the input. However, prior work primarily reduces redundancy within long reasoning paths, limiting exploration of more efficient strategies beyond the Long-CoT paradigm. To address this, we propose a novel two-stage framework for adaptive and efficient reasoning. First, we construct a hybrid reasoning model by merging long and short CoT models to enable diverse reasoning styles. Second, we apply bi-level preference training to guide the model to select suitable reasoning styles (group-level), and prefer concise and correct reasoning within each style group (instance-level). Experiments demonstrate that our method significantly reduces inference costs compared to other baseline approaches, while maintaining performance. Notably, on five mathematical datasets, the average length of reasoning is reduced by more than 50%, highlighting the potential of adaptive strategies to optimize reasoning efficiency in large language models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1",
            "score": 3,
            "issue_id": 3549,
            "pub_date": "2025-04-30",
            "pub_date_card": {
                "ru": "30 апреля",
                "en": "April 30",
                "zh": "4月30日"
            },
            "hash": "6487e5a67faf07a5",
            "authors": [
                "Haotian Luo",
                "Haiying He",
                "Yibo Wang",
                "Jinluan Yang",
                "Rui Liu",
                "Naiqiang Tan",
                "Xiaochun Cao",
                "Dacheng Tao",
                "Li Shen"
            ],
            "affiliations": [
                "China Agricultural University",
                "Didichuxing Co. Ltd",
                "Nanyang Technological University",
                "Sun Yat-sen University",
                "Tsinghua University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.21659.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#inference",
                    "#math",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Адаптивное рассуждение: эффективность без потери качества",
                    "desc": "Статья представляет новый подход к адаптивному и эффективному рассуждению в языковых моделях. Авторы предлагают двухэтапную структуру, включающую гибридную модель рассуждений и би-уровневое обучение предпочтениям. Эксперименты показывают, что метод значительно сокращает вычислительные затраты при сохранении производительности. На пяти математических наборах данных средняя длина рассуждений сократилась более чем на 50%."
                },
                "en": {
                    "title": "Adaptive Reasoning for Efficient Inference in Complex Tasks",
                    "desc": "This paper addresses the challenge of efficiency in long-thought reasoning models used for complex tasks. It highlights that while some problems benefit from detailed reasoning, others do not, leading to the need for adaptive reasoning strategies. The authors propose a two-stage framework that combines long and short reasoning models to optimize reasoning depth based on the input. Their experiments show that this approach significantly reduces inference costs and reasoning length while maintaining performance across various mathematical datasets."
                },
                "zh": {
                    "title": "自适应推理，提升效率！",
                    "desc": "最近，长推理模型在复杂推理任务中表现出色，但常常导致推理开销大，因此效率成为一个关键问题。我们的实证分析显示，使用长链推理（Long-CoT）的好处在不同问题上差异很大：有些问题需要复杂推理，而其他问题则没有改善，甚至准确率下降。这促使我们提出自适应推理策略，根据输入调整推理深度。我们提出了一种新颖的两阶段框架，通过混合长短链推理模型和双层偏好训练，显著降低推理成本，同时保持性能。"
                }
            }
        }
    ],
    "link_prev": "2025-05-01.html",
    "link_next": "2025-05-05.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "01.05",
        "en": "05/01",
        "zh": "5月1日"
    },
    "short_date_next": {
        "ru": "05.05",
        "en": "05/05",
        "zh": "5月5日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 1,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了阿拉伯文本添加发音符号的挑战。作者提出了一种名为Sadeed的新方法，基于一个经过微调的解码器语言模型。Sadeed在精心编制的高质量数据集上进行微调，并在有限的计算资源下取得了竞争力的结果。此外，文章还介绍了一个新的评测基准SadeedDiac-25，以解决当前评测方法的局限性。这些工具共同推动了阿拉伯语自然语言处理应用的发展。",
        "title": "Sadeed: Advancing Arabic Diacritization Through Small Language Model",
        "pinyin": "这篇文章讨论了阿拉伯文本添加发音符号的挑战。\nZhè piān wénzhāng tǎolùn le Ālābó wénběn tiānjiā fāyīn fúhào de tiǎozhàn.\n\n作者提出了一种名为Sadeed的新方法，\nZuòzhě tíchū le yīzhǒng míngwèi Sadeed de xīn fāngfǎ,\n\n基于一个经过微调的解码器语言模型。\nJīyú yīgè jīngguò wēitiáo de jiěmǎqì yǔyán móxíng.\n\nSadeed在精心编制的高质量数据集上进行微调，\nSadeed zài jīngxīn biānzhì de gāo zhìliàng shùjùjí shàng jìnxíng wēitiáo,\n\n并在有限的计算资源下取得了竞争力的结果。\nBìng zài yǒuxiàn de jìsuàn zīyuán xià qǔdé le jìngzhēnglì de jiéguǒ.\n\n此外，文章还介绍了一个新的评测基准SadeedDiac-25，\nCǐwài, wénzhāng hái jièshào le yīgè xīn de píngcè jīzhǔn SadeedDiac-25,\n\n以解决当前评测方法的局限性。\nYǐ jiějué dāngqián píngcè fāngfǎ de júxiànxìng.\n\n这些工具共同推动了阿拉伯语自然语言处理应用的发展。\nZhèxiē gōngjù gòngtóng tuīdòng le Ālābóyǔ zìrán yǔyán chǔlǐ yìngyòng de fāzhǎn.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"阿拉伯\", \"pinyin\": \"ā lā bó\", \"trans\": \"Arabic\"},\n    {\"word\": \"文本\", \"pinyin\": \"wén běn\", \"trans\": \"text\"},\n    {\"word\": \"添加\", \"pinyin\": \"tiān jiā\", \"trans\": \"add\"},\n    {\"word\": \"发音\", \"pinyin\": \"fā yīn\", \"trans\": \"pronunciation\"},\n    {\"word\": \"符号\", \"pinyin\": \"fú hào\", \"trans\": \"symbol\"},\n    {\"word\": \"挑战\", \"pinyin\": \"tiǎo zhàn\", \"trans\": \"challenge\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"名为\", \"pinyin\": \"míng wéi\", \"trans\": \"named\"},\n    {\"word\": \"基于\", \"pinyin\": \"jī yú\", \"trans\": \"based on\"},\n    {\"word\": \"微调\", \"pinyin\": \"wēi tiáo\", \"trans\": \"fine-tune\"},\n    {\"word\": \"解码器\", \"pinyin\": \"jiě mǎ qì\", \"trans\": \"decoder\"},\n    {\"word\": \"语言\", \"pinyin\": \"yǔ yán\", \"trans\": \"language\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"精心\", \"pinyin\": \"jīng xīn\", \"trans\": \"carefully\"},\n    {\"word\": \"编制\", \"pinyin\": \"biān zhì\", \"trans\": \"compile\"},\n    {\"word\": \"高质量\", \"pinyin\": \"gāo zhì liàng\", \"trans\": \"high quality\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shù jù jí\", \"trans\": \"dataset\"},\n    {\"word\": \"进行\", \"pinyin\": \"jìn xíng\", \"trans\": \"conduct\"},\n    {\"word\": \"有限\", \"pinyin\": \"yǒu xiàn\", \"trans\": \"limited\"},\n    {\"word\": \"计算\", \"pinyin\": \"jì suàn\", \"trans\": \"computational\"},\n    {\"word\": \"资源\", \"pinyin\": \"zī yuán\", \"trans\": \"resources\"},\n    {\"word\": \"取得\", \"pinyin\": \"qǔ dé\", \"trans\": \"achieve\"},\n    {\"word\": \"竞争力\", \"pinyin\": \"jìng zhēng lì\", \"trans\": \"competitive\"},\n    {\"word\": \"结果\", \"pinyin\": \"jié guǒ\", \"trans\": \"results\"},\n    {\"word\": \"此外\", \"pinyin\": \"cǐ wài\", \"trans\": \"moreover\"},\n    {\"word\": \"介绍\", \"pinyin\": \"jiè shào\", \"trans\": \"introduce\"},\n    {\"word\": \"评测\", \"pinyin\": \"píng cè\", \"trans\": \"evaluation\"},\n    {\"word\": \"基准\", \"pinyin\": \"jī zhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"解决\", \"pinyin\": \"jiě jué\", \"trans\": \"address\"},\n    {\"word\": \"当前\", \"pinyin\": \"dāng qián\", \"trans\": \"current\"},\n    {\"word\": \"局限性\", \"pinyin\": \"jú xiàn xìng\", \"trans\": \"limitations\"},\n    {\"word\": \"这些\", \"pinyin\": \"zhè xiē\", \"trans\": \"these\"},\n    {\"word\": \"工具\", \"pinyin\": \"gōng jù\", \"trans\": \"tools\"},\n    {\"word\": \"共同\", \"pinyin\": \"gòng tóng\", \"trans\": \"jointly\"},\n    {\"word\": \"推动\", \"pinyin\": \"tuī dòng\", \"trans\": \"promote\"},\n    {\"word\": \"自然\", \"pinyin\": \"zì rán\", \"trans\": \"natural\"},\n    {\"word\": \"处理\", \"pinyin\": \"chǔ lǐ\", \"trans\": \"processing\"},\n    {\"word\": \"应用\", \"pinyin\": \"yìng yòng\", \"trans\": \"application\"},\n    {\"word\": \"发展\", \"pinyin\": \"fā zhǎn\", \"trans\": \"development\"}\n]",
        "trans": "This article discusses the challenges of adding diacritical marks to Arabic texts. The authors propose a new method called Sadeed, based on a fine-tuned decoder language model. Sadeed is fine-tuned on a meticulously curated high-quality dataset and achieves competitive results with limited computational resources. Additionally, the article introduces a new evaluation benchmark, SadeedDiac-25, to address the limitations of current evaluation methods. These tools collectively advance the development of Arabic natural language processing applications.",
        "update_ts": "2025-05-01 09:12"
    }
}